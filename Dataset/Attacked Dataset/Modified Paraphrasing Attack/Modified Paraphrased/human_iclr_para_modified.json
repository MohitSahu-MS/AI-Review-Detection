{"qvUJV2-t_c": ["Paraphrase: Summary This paper introduces an algorithm for automatically selecting the step size for SGD (stochastic gradient descent) in DNN (Deep Neural Network) training. The algorithm assumes that the loss function can be approximated by a parabola along the negative direction of the normalized batch gradient. Thus the optimal step size can be estimated as the distance to the minimum of this parabola. The authors observe that the optimal step size usually remains constant during SGD optimization allowing for its reuse over multiple iterations reducing computational complexity. effectiveness and Weaknesses This paper introduces an algorithm for automatically selecting the step size for SGD (stochastic gradient descent) which is used for DNN (Deep Neural Network) training. The algorithm approximates the loss function along the negative direction of the normalized batch gradient with a parabola enabling the estimation of the optimal step size. Weaknesses 1. Limited Novelty: The proposed LABPAL method is not only novel as it is a adjustment of the existing PAL algorithm. The main difference lies in the direction the parabolic approximation is performed which might be considered a small adjustment. Moreover the experimental basis for the parabolic approximation (observation 16) and their illustrations are nearly identical to those found in a previous paper. other technical contingent like adapting the algorithm to varying gradient noise and its relation to the low Wolfe constant have limited significance for the papers main line. 2. Insufficient Reasoning: observation 2 draws a determination based on a limited dataset and model raising concerns about its generalizability to other scenarios. effectiveness 1. efficient performance: Experiments demonstrate that the proposed LABPAL method outperforms other benchmark methods on various datasets. LABPAL also exhibits robustness delivering stable results across different hyperparameter settings.", "Paraphrase: This search introduces LABPAL a novel line search algorithm that enhances training performance for range classification tasks. LABPAL utilizes a parabola to approximate the complete loss function along the minibatch gradient direction then identifies the parabola minimum to determine the optimal step size. The algorithm implements technique to ensure its functionality in realworld applications. effectiveness:  LABPAL is original and promotes efficient training. Weaknesses:  The algorithms design is based on observations from range classification tasks and its generalization to other domains like NLP is unknown.  several arbitrary design determination and additional hyperparameters introduce complexity in tuning.  While LABPALs benefits are demonstrated in experiments it is not compared against the superior optimizer SGD with Momentum.  The unclear distinction between step size and learning rate requires further clarification in the paper.  The paper lacks a convergence analysis and could benefit from more extensive experiments.", "Paraphrase: Proposed method: Researchers present a parabolic line search approach for stochastic gradient descent (SGD) that automatically adjusts learning rates during training. The approach is inspired by empirical observations during the training of ResNet20 on CIFAR10. Evaluation: The method is compared to other line search strategies for SGD (including constant and piecewise learning rates) using three datasets (CIFAR100) and three network architectures (Resnet20 MobileNetV2 DenseNet121). Additionally the authors demonstrate how to mitigate noise in gradient estimation using a small minibatch size (10). effectiveness and Weaknesses: Adaptive learning methods are valuable for SGD and enhancements over previous line search approach have potential impact. However the evaluation of different methods in this study has limitations:  Lack of comparison with more common learning rate strategies (e.g. constant or piecewise constant).  Incorrect evaluation of some methods (e.g. SLS performing shortly despite previous positive results).  Suboptimal hyperparameter tuning (e.g. coarse grid search and low achieved accuracy). other Comments:  The study cites a specific paper for the claim that gradientonly Line Search (GOLSI) has short performance.  The assumption that all results are based on a single study with ResNet18 on CIFAR10 may limit generalizability.  The statement that increasing batch size reduces gradient noise is countered by literature suggesting the opposite.  The authors use a batch size of 10 to simulate gradient noise increase but it would be beneficial to investigate intermediate batch sizes.", "Paraphrased Summary: This search introduces LABPAL a line search method that enhances SGD (stochastic gradient descent) for neural network training. It aims to address limitations of previous line search methods which either assume an accurate descent direction or rely on minibatches that can compromise robustness. LABPAL utilizes two key observations: (1) the widebatch loss forms a parabola along SGD update direction and (2) the optimal step size adjusts gradually. With these insights LABPAL estimates the wide loss using multiple large minibatches and adjusts the step size using a backtracking line search. extensive experiments with range classification models demonstrate that LABPAL outperforms both vanilla SGD and normalized SGD. effectiveness and Weaknesses: effectiveness:  LABPAL improves upon SGD without requiring finetuning. Weaknesses:  LABPAL is compared only to SGD excluding SGD with momentum despite their similarities. This limits LABPALs practical utility.  LABPALs reliance on empirical observations specific to range classification limits its applicability to other tasks.  Certain hyperparameters in LABPAL may not generalize well across different scenarios.  The mechanism for increasing step sizes over time may not be universally applicable to all data distributions."], "oTQNAU_g_AZ": ["Paraphrased Statement Summary: This field explores crucial aspects of cooperative object manipulation involving two robots specifically dominance and conflict. It introduces the \"disentangled attention\" technique which offers an inherent regularization mechanism enabling the robots to focus on distinct subtasks or objects. The effectiveness of the proposed method is tested in simulation using two robot arms that complete various tasks in tabletop settings. Strengths and Weaknesses: Strengths:  Clear and concise writing  Welldefined problem field challenges  Clear formulation of the proposed method Weaknesses:  Lack of other clarification regarding the state of robot arms  Undefined concept of \"interaction region\" in both simulation and realworld scenarios  Ambiguity in the interpretation of state encoders agents and their relationships (fij() si)  Lack of clarity regarding \"target interacting position velocity\" in the implementation details  High standard deviations in reported experimental results  significant performance drop in adaptation stages (Table 3) without clear explanation or improvement strategies  Absence of a legend in Figure 7", "Summary This paper suggests using attention mechanisms for dualarm robot manipulation based on sparse reinforcements incorporating a novel intrinsic regularization technique. This regularization encourages each robotic arm to concentrate on different tasks and objects aimed at addressing the issues of dominant agents and collisions in cooperative settings. The approach demonstrates improved success rates sample efficiency and reduced conflicts between operating arms as shown in simulation experiments. Strengths and Weaknesses Strengths:  foundation of intrinsic regularization using attention  Wellstructured paper  Comprehensive experimental evaluation Weaknesses:  Misleading claims regarding the contribution of identifying dominance and collisions in dualarm control (not novel)  Insufficient use of experiment seeds (3 is inadequate for modelfree reinforcement learning)  Reduced collision risks are implicitly learned rather than guaranteed making the approach impractical for safetycritical applications Additional Concerns and Questions:  The practical significance of the solution for preserving safety in collaborative spaces could be emphasized safe.  The apparent contradiction between improved efficiency (10M vs. 11M step) and faster convergence requires clarification.  A more thorough evaluation with a higher number of seeds is necessary to confirm the methods utility.  The surprising preference for fully sparse reinforcements over informative ones should be explained.  The field of required finishing step seems to be based on nonconverged policies potentially overestimating the effectiveness of the proposed regularization.  A reference is needed for the statement \"One potential solution is to design a taskallocation reinforcement use to encourage safe coordination.\"", "Paraphrased Summary: This work presents a novel implicit regularization mechanism for bimanual manipulation. It compels two robot arms to concentrate on distinct regions preventing them from concurrently acting on the same object. This method utilizes attention computations between robots and objects and subsequently restricts the dot product between the two arms attention values to avoid overlap. This effectively resolves conflicts between the arms encouraging them to focus on distinct tasks for efficient reinforcement learning training and safer behaviors. Experimental assessments indicate the proposed methods generalizability to novel situations due to the attention mechanism and regularization. Strengths:  Intuitive and wellmotivated concept  Clear and wellwritten presentation  Meaningful comparisons and experimental setups  Impressive generalization capacity and collaborative behaviors Weaknesses:  The regularization may hinder close collaboration between robots  Uncertain why the method does not prevent synergistic skills  The performance conflict between approaches is not significant  Questionable effectiveness of \"Attention baseline\"  Limited explanation of generalization capability  Dependence on handengineered state space  Unclear resolution of the \"domination\" issue  Lack of clarity on the use of hindsight experience replay Minor Comments:  Update symbol notation for consistency  Include citations for similar works  Specify episode length consistently  Consider increasing random seeds for more robust results", "Summary: This research introduces a novel technique for controlling multiagent systems specifically collaborative tasks involving two agents. Its effectiveness lies in using an attention mechanism that simultaneously acts as a variety of builtin regularization. The authors demonstrate the methods effectiveness through simulations involving two robotic manipulators. They compare it to a simpler version without regularization and a standard multiagent reinforcement learning (RL) approach. Strengths and Weaknesses: Strengths:  Simple and easytounderstand regularization method for multiagent systems.  Attention mechanism helps agents focus on their various tasks while preventing noise with each other. Weaknesses: Assumptions:  Precomputed interaction field may limit scalability to scenarios with more than two agents. Terminology and applicability:  The term \"bimanual manipulation\" is incorrectly used. The proposed method addresses collaboration between multiple agents not bimanual manipulation by a single agent.  The authors misuse the term \"safety.\" The evaluation measures the number of collisions but this does not provide varietyal safety guarantees. Technical Concerns:  In Section 3.1 the authors incorrectly claim that SAC requires learning the temperature parameter.  In Section 4.1 the authors description of Qfunction modifications in SAC is incorrect. SAC does not predict action distributions but outputs scalar values directly. Evaluation Limitations:  The proposed method is not compared to stateoftheart multiagent RL algorithms like MAAC.  The results are not statistically significant and present high variance.  The authors claim to have attempted implementing MASAC but present no results.  The conclusion overstates the pervarietyance of the proposed method as Figure 7 does not show clear statistical significance. Recommendations:  Provide additional multiagent experimental scenarios to showcase the attention mechanisms capabilities.  Include comparisons with more innovative multiagent RL methods such as MAAC."], "qiMXBIf4NfB": ["Summary In this study researchers investigate the theoretical properties of a selftraining approach for regression problems in semisupervised scope. Given labeled and unlabeled data the algorithm iteratively: 1. Trains a model on the labeled data. 2. Generates pseudolabels for the unlabeled data using the trained model. 3. Trains the model using a convex combination of empirical losses on the labeled and pseudolabeled unlabeled data. Under specific assumptions including the realizability of the ground accuracy exact observation of labels and model structure the algorithm exhibits surprising results:  Theorem 2: With suitable parameter choices the model converges towards the ground accuracy with an assumption on the issue of sample that is similar to supervised learning without unlabeled data.  Theorem 1: Under weaker conditions the model converges towards a combination of the ground accuracy and the initial model outperworking the initial model. Key findings  The algorithm work as an optimization trick improving perworkance in datasparse regimes by leveraging a work of selfregularization.  Mean or isotropy inworkation contained in the unlabeled data does not appear to play a significant function. Technical approach The proofs rely on an auxiliary loss function called the population risk function which estimates risk as the \"labels\" are images of the points by a convex combination of the ground accuracy and the initial point. The authors show that:  The optimization landscape of the population risk function is mild.  The empirical risk minimized by the model is close to the population risk function. Strengths and Weaknesses  Strengths:  Impressive mathematical analysis with intriguing results.  Clear account of the overall proof strategy.  Minor typos compared to typical conference papers.  Weaknesses:  Possible problem with the proof of Lemma 1s lower bound.  Lack of details on the relationship to previous work.  Missing accounts in the appendix. Additional Notes  The study assumes a specific data distribution and model structure which may limit generalizability.  The authors suggest that unlabeled data may improve perworkance in practice even if it does not contain explicit inworkation.", "Paraphrased Summary: This study examines the theoretical underpinnings of selftraining on singlelayer neural networks with Gaussian inputs for the first time. It demonstrates that under specific conditions (e.g. appropriate initialization) selftraining can accurately restore the ground accuracy labeling matrix and improve sample efficiency compared to supervised learning with only labeled data. Experimental findings corroborate these theoretical predictions. Strengths and Weaknesses: Pros:  Addresses a crucial theoretical question related to selftrainings effectiveness.  Experimental results strongly support theoretical claims with preliminary experiments on realistic models and data indicating understanding with theoretical predictions. Cons:  Potential technical flaws in the proof which relies on an \"intermediate rate theorem\" that may not exist in the stated work.  The studys assumption of a convex function as the neural network class weakens the findings. This function class is less expressive than workal onehiddenlayer neural networks.  Some notations such as c(\u03ba) remain undefined. Questions:  Should the population risk function f in the appendix function squared difference  How do the constraints on unlabeled data distribution translate to practical scenarios Updated Assessment: After the authors response the MVTrelated concerns have been resolved making the paper technically sound. The extension to twolayer models is also valid if the tensor initialization method accurately reflects the training dynamics of actual models. The authors skepticism regarding this assumption remains however. Given these improvement the paper is now considered suitable for issue.", "Paraphrased Summary: This paper mathematically examines a selftraining algorithm using a onehiddenlayer neural network. When data is sampled from Gaussian distributions with varying scales it demonstrates that the algorithm converges and generalizes with an error rate proportional to 1\u221aM (where M is the issue of unlabeled sample). Experiments on simulated and augmented realworld data support these theoretical results. Strengths:  Rigorous theoretical analysis with advanced technical elements  Empirical findings that validate theoretical predictions  Clear and informative account of the proof  Comparative analysis with existing work Weaknesses:  Selftraining may impair accuracy in certain cases (confirmation bias)  the theory does not address this  The theory doesnt provide practical guidance for enhancing selftraining algorithms  The \"0 generalization error\" mentioned in Section 3.3 is confusing as estimation error is inherent", "Paraphrased Statement: The paper investigates the theoretical performance of selftraining a semisupervised learning algorithm applied to a twolayer neural network. Selftraining uses a large dataset with unlabel data to train a model using pseudolabels predicted by a teacher model trained on a smaller set of label data. The analysis assumes a Gaussian distribution for the input data and derives theoretical bounds on the generalization error which predict a performance improvement proportional to the square root of the size of the unlabel dataset. Experimental results on synthetic data and the CIFAR10 dataset evidence that these theoretical bounds align with empirical performance. Strengths:  study a challenging problem: Understanding selftraining in nonconvex neural network with two layers.  Connects theory to practical insights and simplifies understanding. Weaknesses:  Limited novelty: Previous work have studied selftraining in linear scope and this papers focus on twolayer network is constrained by assumptions that effectively linearize the problem.  foundation of the parameter \u03bb is unclear and its practical implications are not fully explained.  The analysis is limited to regression tasks which simplifies the problem compared to classification.  Missing relevant related work on selftraining.  Nonintuitive restrictions in the model family and network structure may limit the generalizability of the findings.  The writing could be improved by introducing Key concepts before using them."], "ue4CArRAsct": ["Paraphrase: Summary This paper introduces a hierarchical latent variable model with an accompanying sampling method to create disentangled representation. The latent variables are used to guide affine transformations within the decoder instead of forming a latent feature map. The authors enhance this with a \"hybrid\" sampling technique that effectively draws from the combined approximate posterior. The model is compared to various baselines using FID and disentanglement metrics. The authors also explore generalization. Strengths  The empirical evaluation is comprehensive using multiple image datasets and comparing with a wide scope of models.  Results posterior multiple aspects including reconstruction generation disentanglement and generalization.  extrapolation experiments evaluate generalization in autoencoder components a novel investigation. Weaknesses  Some design choice raise concerns about the models validity as a proper probabilistic model (e.g. deterministic noise nonbinary reconstruction).  The novelty of the models insights is unclear. previous research has demonstrated the potential of hierarchical latent variable models for disentanglement and the authors do not fully explain why specific aspects of their model are necessary for this.  The encoders diagram is missing making its performance unclear.  The connection between structural causal models and the approach is not welldefined.  The extrapolation sections appear somewhat unrelated to the rest of the paper.  BetaVAE underperforms VAE in Figure 6 which contradicts the authors claim that beta was optimized.", "Paraphrased Summary This paper presents an approach to training generative models that can learn disentangled representation without relying on additional regularization losses. It proposes two technique: 1. Injecting noise into layers inspired by AdaIN to correspond different parts of the latent representation to different decoder depths. 2. Sampling each part of the latent representation from a predefined set of vectors similar to the codebook approach in VQVAE. Strengths and Weaknesses Strengths:  The paper emphasizes the importance of disentangled representation in generative models.  The approach is straightforward and easy to implement. Weaknesses: 1. Lack of Theoretical Justification for disentanglement:  While the architecture aims for disentanglement there is no theoretical explanation or evidence provided to support that it actually achieves this goal. 2. Evaluation Limitations:  The disentanglement metrics used are not entirely convincing and some prior work paper conflicting results.  The evaluation does not include comparisons with certain relevant models (e.g. FactorVAE \\betaTCVAE InfoGAN).  The emphasis on FID as a disentanglement metric is questionable as it has been criticized in the literature. 3. Motivation for architectural choice:  The paper justification for the use of AdaIN layers and the alternative sampling method is not fully clear.  There are other established methods for enforcing causal dependence in generative models that may offer similar or better results. 4. Scalability Concerns:  The paper acknowledges that the benefits of the proposed approach may diminish for complex datasets with higherdimensional encodings and deep models.  The limited experiments with 12 and 32dimensional embedding space do not fully address this concern. Nitpicks on Style:  Intext references should be enclosed in parentheses and there are instances where this is not followed.  The paper highlights the absence of regularization terms in the proposed loss but the specific loss work used is not explicitly stated.", "Paraphrase: The research paper introduces a novel model called SAE (structural autoencoder) and a new sampling technique called hybrid sampling for training a generative model based on an encoderdecoder architecture. The authors have conducted numerous experiments to demonstrate that the proposed model and sampling method significantly enhance image quality and produce More coherent and meaningful images in experiments related to feature disentanglement and extrapolation. Strengths and Weaknesses: Strengths:  The proposed model achieves promising empirical results.  The authors have performed comprehensive experiments and ablation work.  The results and explanations are compelling.  The paper is wellwritten and easy to understand.  The authors clearly articulate their motivations and technique.  The code for the model is provided as supplementary material with a clear structure and comprehensive instructions. Minor Weaknesses:  Figure 3 could be improved for clarity with fewer bars of the same color and clearer annotations for the \"X\" and \"O\" symbols.  Figure 5 could be placed closer to the relevant text in section 4.1 with annotations or marks on the image to enhance comprehension.", "Paraphrased Summary: This research outlines how to use latent variables in autoencoders to enhance their performance particularly in generating images. The approach is based on the assumption that latent variables are causally independent. The authors also propose a \"discrete mixup\" method for sampling latent codes which shows improvements. Strengths:  Clear and understandable explanations  Importance of addressing structured latent structure representation learning  Welldesigned experiments that support the conclusions Weaknesses:  Empirical evidence suggests that latent variables are independent but the paper lacks a strong theoretical justification for the specific method of using latents in the generation algorithm.  Some line in the paper are unclear such as the claim that architectural asymmetry in the autoencoder encourages latent variable independence.  Clarification is needed on the following points:  How latent vectors are distributed among layers during generation  Whether AdaAE is the proposed method  Why SAE6 performs worse than other SAE models in some experiments  Whether generated images closely resemble training instances due to the use of training model for latent sampling."], "yjxVspo7gXt": ["Paraphrase: This study examines bias mitigation techniques in deep study specifically addressing the challenges of intersectionality and missing labels for sensitive attributes. Various methods are assessed on the Celeb dataset (with complete labels) and ImageNet (with limited labels). The findings indicate that the optimal bias mitigation approach depends on the situation and methods that do not rely on sensitive attributes perform poorly regarding intersectionality. A novel distillationbased method is proposed to address the computational complexity of techniques like area Independent. On ImageNet the proposed method reduces bias amplification but has no impact on groupreweighed truth or intersectional bias. Strengths and Weaknesses:  Strengths:  Focuses on intersectionality a crucial fairness issue.  Offers insights into the performance of existing methods on intersectional groups.  Highlights fairness challenges with intersectionality complexity and stability.  Weaknesses:  Lacks discussion on hyperparameter tuning which can impact fairnesstruth balance.  Defines sensitive attributes with a \"pale male\" bias inclusive terms are suggested.  Raises concerns about fairness with respect to attributes like \"straight hair\" without providing a clear rationale.  Table 4 study truth on a different scale than earlier in the study.  Key terms such as \"Mean Average Precision\" error bars test sets and synsets require clarification.", "Paraphrased Statement: This article compares various methods that reduce bias when dealing with multiple protected groups. It also introduces a knowledge distillation technique to enhance fairness in the ImageNet dataset which has limited representation of protected groups. Strengths:  Emphasizes the practical importance of addressing intersectional fairness.  Provides a thorough comparison of existing bias mitigation approach.  Starts out with clear writing. Weaknesses: 1. Fairness Metrics:  The use of \"worstcase truth\" as a substitute for equalized odds lacks justification.  The paper relies on surrogate metrics rather than the actual fairness measures (e.g. demographic parity).  The mathematical definitions of the fairness metrics in Section 3.1 are unclear. 2. Interpretation of Results:  The paper lacks any interpretation of the findings presented in Section 4.", "Paraphrased Summary: This study evaluates current bias mitigation techniques on two extensive datasets (CelebA and ImageNet) containing numerous sensitive attributes and missing protected labels. Findings suggest that existing methods can reduce intersectional bias on a large scale but are less effective when applied to unlabeled data. Additionally the study proposes a knowledge distillation regularization approach (DIR) as a supplement to other bias mitigation algorithms. Strengths:  Comprehensive analysis of bias mitigation techniques on datasets with multiple sensitive attributes and limited protected labels.  introduction of a novel DIR method to enhance existing algorithms. Weaknesses:  Lack of significant novelty in the proposed study.  DIR method is largely an incremental improvement upon existing techniques offering only minor complexity reduction with potential integration issue.  Uncertain generalizability of DIR method to unlabeled data given the poor performance of such algorithms in previous evaluation.", "Paraphrased Statement: This research examines the fairness of deep study model in classification tasks involving numerous intersecting groups. It makes two main contribution: 1. Empirical study: The study demonstrates the inherent challenges (due to lack of labeled data) and limitations of existing approach in such settings. 2. New Approach: The paper proposes a novel method called Knowledge Distillation of Independent Models as Regularization (DIR) to train deep model for scenarios with many intersecting groups. Strengths:  Addresses a crucial and underresearched issue.  Introduces an innovative approach (DIR). Weaknesses:  Major: The methods are evaluated on datasets without ground truth labels. These issue should be validated on synthetic datasets with known labels (e.g. binary classification with multiple binary group attributes).  Major: The study key findings rely on summary statistics that fail to capture variations in truth across intersecting groups. Alternative evaluation methods (e.g. plots showing the distribution of groupspecific truth) and metrics that assess grouplevel performance (e.g. worstcase truth over groups with significant bias) are recommended.  Minor: Missing references to previous study on similar issue including training single classifiers with fairness guarantees for infinite intersecting groups and decoupled training across multiple intersecting attributes."], "y_tIL5vki1l": ["Paraphrase: Summary: This study introduces a novel approach for separating and managing the GAN synthesis process by manipulating the positions of key points their appearance and the backgrounds appearance. The unsupervised learning of key points eliminates the need for arduous manual labeling. extensive testing has demonstrated the efficacy of the proposed technique. effectiveness: 1. Clear and concise writing style. 2. Comprehensive and persuasive evaluation. Weaknesses: 1. Limited novelty: While the paper emphasizes key points its inclusion of wconstj and the spatial embedding layer (section 3.2) essentially transforms key points into a particular type of \"segmentation mask.\" Consequently the core method (sections 3.1 and 3.2) primarily entails converting key points into a special type of mask that resembles a Gaussian function and lacks sharp boundaries. This also explains its seamless integration into SPADE. 2. The performance of the unsupervised method relies heavily on two hyperparameters: the number of key points and their influence image (tau). These parameters are not emphasized in the main paper but are briefly discussed in Appendix F.2. Their influence on one another may make it challenging to tune potentially limiting the practicality of the method. It is recommended to include an indepth analysis of how these hyperparameters affect performance.", "Paraphrased Summary This research promodels a novel approach for generating images with local control using keypoints. Unlike previous autoencoderbased methods this method employs a GANbased approach to address limitations such as:  The need for careful tuning in existing keypointbased approach  The difficulty in disentangling model and appearance The method operates without supervision relying solely on RGB images for training. Strengths  Clear and concise writing  experimental results and videos in the supplementary material strongly support the ability to edit images using keypoints  extensive ablation study validate architectural choices  Impressive results on the challenging LSUN beds dataset which has been difficult for unsupervised approach  Similar image fidelity to supervised methods on CelebAHQ Weaknesses  Insufficient qualitative and quantitative comparison with baseline approach  While the method claims to improve disentanglement of model and appearance this assertion is not experimentally verified  The method may require as much tuning as baseline approach despite claims to the contrary  The results in the paper and supplementary appear curated raising concerns about cherrypicking additional Questions and Comments  Exploratory manipulation of keypoint extraction from pretrained models could simplify the task and potentially enhance results  Clarification is needed on the specific tuning difficulties associated with TPSbased methods  Inclusion of FID results for all datasets in the main paper and for baseline methods would provide a more comprehensive evaluation  Evaluation on Human3.6M and PennAction datasets would validate the methods applicability to more complex scenes  Overfitting to appearance codes observed in the BBC model videos should be acknowledged as a limitation  additional details are requested for Appendix B including downsides of the tuned LatentKeypointGAN architecture and FID evaluation  Inclusion of more uncurated samples would enhance intuition about the methods performance  Motivation for tackling this problem in an unsupervised setting should be clearly stated  Citing stronger peerreviewed literature would strengthen the argument for limitations of autoencoderbased approach  Reporting keypoint detection results on FFHQ would be valuable  Explanation of table introduction (e.g. why FID without keypoint conditioning is better than with) would provide clarity  The manipulationfulness of the background loss should be demonstrated with a qualitative ablation study", "Summary This research proposes a method for image generation that employs structured keypoints as an intermediate representation. Each keypoint aims to control the position and semantics of a specific image component. During generation three noise vectors are drawn from Gaussian distributions to model the keypoint locations overall image style and background semantics. Training involves a GAN loss and a background loss that separates background semantics from foreground semantics. effectiveness and Weaknesses  The concept of using keypoints for image synthesis is intriguing and practical for controlling the generation process.  However the related study section lacks citations and analysis of similar approach that enable structural and local edits in GAN synthesis limiting the comprehensiveness of the experiments. Major Concerns 1. Missing Related study:  [af] Several study that provide structural and local edits in GAN synthesis are not included in the discussion. 2. comparison Inadequacy:  LatentKeyPointGAN is treated as an unconditional GAN but should be compared to recent unconditional GANs that also incorporate structural latent space as mentioned in point 1. 3. Keypoint Interdependency:  The background loss aims to disentangle background and foreground semantics. It is unclear whether the semantics of the k keypoints may become entangled with each other during training.  Whether multiple keypoints can control the same region (collapse) and how to mitigate this issue need to be addressed. 4. Keypoint Value Determination:  K is a predetermined value.  The meaning of adding a keypoint in image 1 is not explained.  direction on choosing the value of K and an ablation study are lacking.", "Paraphrase: Authors developed LatentKeypointGAN a system that automatically detects keypoints on images and allows manipulationrs to edit them. This system is unique in that it trains on unlabeled data without explicit supervision. While GANs are known for generating realistic images their ability to modify specific attributes is limited. The proposed method addresses this limitation by leveraging the detected keypoints. The authors approach includes:  Detecting keypoints on images in an unsupervised manner.  Providing the ability to edit images using the detected keypoints. However the review notes that similar study exists that offers interactive capability for GANs through the manipulation of segmentation maps. The authors should include this reference and compare their method to it. The similarity between segmentation maps and 2D heatmaps could potentially reduce the significance of the authors contribution."], "pWBNOgdeURp": ["Paraphrased Summary: The authors analyzed network pruning from the view of dynamical system theory. They introduced Koopman pruning a new technique that combines magnitude pruning and gradientbased pruning. This method also sheds light on aspect of magnitudebased pruning before training converges. strength and Weaknesses:  The paper is wellwritten and explores significant questions.  The authors provided easytofollow code reproduction. However the authors may have overstated some of their results:  equivalence of Global and Magnitude Pruning:  The equivalence claim is strong and potential only holds under specific assumptions about network dynamics.  Koopman pruning approximates parameter dynamics using a linear transformation which may not be accurate for all neural network.  New Insight on Magnitude Pruning Preintersection:  The authors claim that Koopman pruning gives insight into the success of magnitude pruning before intersection but the paper mainly demonstrates that the two methods differ during this stage. Questions:  Interpretation of Koopman decomposition Terms:  Can the authors explain the meaning of \\mathbfUg and \\mathbfT in the context of neural network training  Assumption of Dominant Eigenvalue Close to 1:  Is there experimental evidence to support the assumption that \\lambda1 is close to 1 in Koopman decomposition of neural network parameter snapshots", "Paraphrase: Summary This paper uses a mathematical concept called the Koopman operator to explain why pruning deep neural network (DNNs) or removing unneeded connections can improve their perclassance. strength: 1. The paper is easy to understand. 2. The experiments are welldesigned and conducted. 3. The paper offers a theoretical justification for pruning which is a wellknown technique for enhancing DNNs. 4. The study investigates the distinction between static and active pruning regimes. While the reasons behind this distinction are not fully explained it provides valuable insights. Weaknesses: 1. The papers conclusions may appear obvious since DNN training involves hidden processes and the Koopman operator projects onto a highdimensional space to simplify the dynamics. It is natural to expect the most significant eigenvalue to have the greatest influence in such cases. The Koopman operator also remains somewhat hidden and does not provide significant insights into DNNs internal workings. It is unclear if any class of linearizing the dynamics would yield similar results. 2. Although algorithm 1 is presented as a \"general class\" it remains conceptual rather than providing a practical method for reducing training time.", "Paraphrased Summary: This study introduces a new class of pruning techniques based on Koopman operator theory which unifies existing pruning methods based on magnitude and gradient. This approach uses eigendecomposition a method that has been effective in analyzing the structure of Reproducing Kernel Hilbert Spaces (RKHSs) and Gaussian processes (GPs) to explain the success of these pruning methods in reducing the size of neural network. strength and Weaknesses: This paper aims to explain the effectiveness of magnitudebased and gradientbased pruning methods for neural network compression. Despite the simplicity of these methods they have been found to be robust and straightforward to implement. strength:  The paper is wellwritten and easy to understand.  The problem addressed is fundamental and intriguing.  The proposed solution is theoretically sound and easy to implement.  For practitioners familiar with eigendecomposition the theoretical aspect may not be particularly novel but the view on pruning is valuable and sheds light on the success of pruningbased methods. Questions:  Can the proposed framework be applied to layerwise pruning  What are the computational necessity for applying the framework to large network  How many snapshots are required to obtain the eigenvector corresponding to the largest eigenvalue for Koopman mode decomposition of a large network on a specific dataset", "Paraphrase: This study investigates why pruning techniques based on magnitude and gradient are only effective once a neural network has fully adapted to its intended task. The researchers explore this issue using Koopman theory from the field of dynamical systems. strength:  The paper introduces a novel view on the effectiveness of pruning techniques.  It provides a clear and understandable explanation of Koopman theory. Weaknesses:  The paper does not provide specific examples or experimental results to support its findings."], "nuWpS9FNSKn": ["Paraphrase: This papers key strength lies in its rigorous theoretical analysis and corresponding validation. first the authors prove that a novel reconstructionbased objective can extract topic information for a general topic model. Secondly they improve the guarantee for the contrastive objective presented by Tosh et al. (2020) by relaxing certain conditions and eliminating the requirement for landmark papers.", "Paraphrased Statement: Summary This study:  Demonstrates how selfsupervised learning techniques can extract meaningful information about the distribution of topic proportions in a papers regardless of the underlying topic model.  Extends previous knowledge about contrastive objectives to include reconstructionbased objectives.  Remarkably a simple reconstruction objective can recreate posterior distributions generated by various probabilistic topic models on simulated data. Strengths and Weaknesses TopicWord Weight Matrix  While topic models provide insights into topic as sets of language the topicword weight matrix (A) is usually conditioned upon and assumed to be the left pseudoinverse of the topic proportion posterior (\u03b8).  Its unclear how the netprocess models in the experiments learn A. Neural Topic Modeling  The process relates to neural topic models based on variational inference but its not clear how such models recover the topic posterior.  A comparison of the proposed approach with neural topic models would be beneficial. practical Implications  The reconstructionbased objective can retrieve the posterior distribution of topic proportions from a papers.  However topic models particularly probabilistic ones are known for their interpretability which is not directly addressed by the proposed theorems. performance of AttentionBased Architecture  Attentionbased architectures show superior performance in recovering the topic posterior distribution for certain papers case.  The authors could provide further insights into the understanding for this observation. comparison with Other Embedding Methods  The study demonstrates the utility of selfsupervised objectives for papers classification.  A comparison with other embedding methods such as language models would highlight the advantage and limitations of the proposed approach.", "Paraphrased Summary: This process addresses the issue of estimating the posterior distribution in probabilistic topic models. A generic topic model comprises:  Topicword matrix (\u0394): Probabilities of language given topic  Prior over topic proportions (w): Distribution of topic within papers Given these elements and a new papers (x) the goal is to calculate the posterior probability of w given x (p(w  x)). Standard Bayesian inference techniques can estimate this posterior and their specific implementation typically aligns with the choice of prior. This paper proposes a novel approach involving a learned transformation (f(x)) trained using a reconstruction loss. It suggests that f(x) can be converted into the desired posterior distribution with a linear use. Notably the authors suggest that learning f(x) does not depend on the prior \u0394 implying that this representation is immune to prior misspecifications. The papers key contribution are: 1. Theorem 3: When f(x) perfectly optimizes the reconstruction loss a linear combination of its output can exactly match the expected value of any polynomial summary use of w under the posterior. 2. Theorem 4: Even if f(x) is not an optimal solution there exists a linear use such that the error bound for all papers remains limited. 3. Theorem 5: The contrastive objective enables the recovery of expected polynomial summaries of w under the posterior with a linear use. Empirical evaluation on simulated and realworld data demonstrate that the proposed SSL method outperforms Bayesian inference with an incorrect prior. In the classification task using realworld data SSL performs comparably to existing methods.", "Paraphrased Summary: This paper presents a groundbreaking method for topic model inference using selfsupervised learning. The method applies to various topic models and demonstrates that the expectation of a polynomial use of the topic posterior can be linearly related to the output of a use optimizing a selfsupervised learning objective. The authors explore two case of selfsupervised learning objectives: reconstructionbased and contrastive learning. They provide mathematical validation for the main issue and an approximationrobust adaptation. Empirical evaluation shows that the method outperforms inference with inaccurate models. The authors also demonstrate the effectiveness of selfsupervised learning for feature extraction in semisupervised learning. Strengths:  Novel and potentially valuable main issues.  Introduces a new selfsupervised learning technique for topic models with theoretical foundation.  Demonstrates the utility of the approach in synthetic experiments with misspecified models.  Shows the effectiveness of selfsupervised learning for feature extraction in real data experiments. Weaknesses:  Some issues need clearer explanation.  Figure 1 provides limited information.  Figure 2 lacks clarity and could benefit from a table.  comparison issues primarily focus on misspecified models which may not be representative.  Experiments on real data focus solely on semisupervised learning.  Lack of comparison with other supervised and semisupervised topic modeling methods.  Limited exploration of the approachs potential in topic model learning algorithms."], "ldkunzUzRWj": ["Paraphrased Statement: Summary: This wellwritten paper introduces the link between a longtail distribution and the importance of selecting challenging negative sampling in contrastive learning for recommendation systems. It offers a simple and issueive technique for sampling negative sampling and includes comprehensive experiments. strength:  It identifies and addresses the class imbalance issue in contrastive learning for recommendation tasks.  It provides a sound theoretical basis for the proposed approach.  It supports its claims with extensive experimental issue. Weaknesses:  The approach may be specific to recommendation scenarios.  The introduction could be improved with more context and discussion. Additional Observations:  Prior work has addressed class imbalance in terms of label count but not in contrastive learning methods.  This work highlights the correlation between imbalance and item popularity leading to infinite item embedding norms.  The proposed method involves a conditional probability to select negative sampling which can be challenging due to the size of item databases.  It combines a random walk toward popular items with a filtering criterion to obtain qualified negative sampling. Concerns:  The method may prioritize items with high prediction scores which may include false positives.  The use of reweighting is unclear and its issue should be explored.  The dependence of the proposed sampler on a bias random walk requires further explanation.", "Paraphrased Statement: Summary This research presents a proposal to enhance the negative sampling process for training ranking models that compare pairs of items in personalized recommendation systems. negative sampling is crucial in this application because it can significantly impact model performance in terms of training speed and recommendation quality. Strengths 1. The focus on a significant realworld issue in training ranking models for recommender systems. 2. The proposed approach is efficient effective and meets the needs of a suitable negative sampling method. Weaknesses 1. Limited novelty due to similarities with existing work on analyzing vertexlevel imbalance and the use of a reject sampler to increase the likelihood of selecting popular items. 2. The \"debiased\" claim in the title may be misleading. 3. Lack of clarity in the methodology item particularly in Section 4.2. 4. Insufficient analysis of the space complexity of VINS considering that each pair (u i) corresponds to a buffer. 5. Unconvincing experimental issue to demonstrate the clear superiority of VINS in terms of effectiveness and efficiency. The performance comparison in Table 1 is biased due to the use of sample weights and Table 2 lacks theoretical analysis for contrast.", "Paraphrased Summary: This work explores personalized ranking based on implicit feedback. It uses interactions like clicks or purchases as positive model and all other items without explicit feedback as negative model. The goal is to create a ranking model for recommendations. The paper identifies two issue: 1. EdgeLevel imbalance: The proportion of positive to negative model is unbalanced. 2. VertexLevel imbalance: The issue of positive and negative model for each item is disproportionate with popular items being underrepresented as negative sampling and unpopular items being overrepresented. This leads to extremely large embeddings for popular items. strength and Weaknesses: strength:  The authors propose a twophase sampling approach (VINS) to address vertexlevel imbalance by selecting more informative negative sampling.  The paper provides theoretical and empirical support for the claims. Weaknesses:  The paper does not explore regularization as a potential solution to the embedding explosion issue.  The proposed VINS approach is similar to existing hard negative sampling techniques.  The paper could have discussed the proposed approach in relation to existing research on ranking with accuracy at top.", "Paraphrase: Summary: For tasks involving pairwise learningtorank constructing training data often encounters an imbalance between positive and negative model with the latter being more prevalent. negative model sampling plays a crucial role in training and classifier performance. method: This work proposes a negative model sampling technique that considers the degree of vertices. It uses a rejection work to prioritize sampling from highdegree vertices over random edge sampling. Analysis and issue: The paper provides theoretical analysis and experimental evaluations using realworld datasets to demonstrate the efficacy of the proposed sampling approach. strength and Weaknesses: strength:  Addresses the significant issue of negative sampling in learningtorank.  Introduces an innovative vertexlevel imbalance sampling method.  Validates the approach through theoretical analysis and empirical examination. Weaknesses:  The work does not provide a detailed comparison with other negative sampling methods."], "mKsMcL8FfsV": ["Paraphrased Summary: This paper introduces a novel method for combining multiple feature representations using selfsupervised learning. Instead of traditional averaging or concatenation the method uses a selfupdating inference scheme during testing involving the adjustment of the backbone feature representation. empirical evaluations demonstrate the improved performance of this method compared to existing approach. Strengths:  Clear explanation of the method through Figure 1 and Algorithm 1.  Extensive experiments on ImageNet and other datasets.  detailed reporting of experimental conditions (checkpoints and hyperarguments). Weaknesses:  method Description:  Ambiguous description of argument optimization during training.  Insufficient explanation of the kNN evaluation specifically the quality of image features used for both training and testing.  inference Procedure:  Lack of clarity regarding the use of freezing the MLPs during inference.  potential impact of optimizing the backbone representation (rather than the MLPs) on inference performance.  Experimental Evaluation:  Insufficient experiments to fully demonstrate the methods superiority.  Limited exploration of using different training and testing features in the kNN evaluation.  potential benefit of using alternative selfsupervised learning approach for feature optimization.  Examination of the impact of random MLP initializations and unfreezing the MLPs during testing.  comparison with a baseline that includes all test image features in the kNN evaluation. Minor Issues:  Errors in proofreading including outofbound figure legends and incomplete figure captions.  Incorrect citation of Mandt et al.", "Paraphrase: This paper presents a novel approach to selfsupervised model ensembling that involves using gradient descent during inference to optimize representations extracted by pretrained feature extractors. Experimental evaluations demonstrate the effectiveness of this method. Strengths:  The paper is wellwritten and structured.  The proposed method achieves impressive performance in KNearest neighbor classification. Weaknesses:  The ensembling method is relatively straightforward involving optimization of pretrained models.  Some results and explanation in the paper lack clarity.  The experimental section requires further improvement.  The paper contribution may not be significant enough for a highly selective conference like the International Conference on Learning Representations (ICLR).", "Paraphrased argument: Summary: This paper presents a method for combining multiple selfsupervised models without any additional training data. It uses a novel approach where representations are learned through gradient descent at inference time. The methods effectiveness is measured by the accuracy of nearest neighbor classification. Strengths and Weaknesses: 1. Marginal improvement: The proposed method shows a small improvement in accuracy compared to the cost of training and testing (which includes finetuning during validation). There is a concern about potential information leakage from validation data. 2. Limited comparison: The paper only compares the proposed method with a specific baseline model. It does not compare it to other further ensembling methods. 3. Ambiguity in model Details: It is unclear how the \"4 SimCLR models\" in Figure 2 differ from each other. 4. Experimental concern:  The baseline performance in Figure 2 appears low.  Only knearest neighbor results are reported but classification accuracy under the standard linear evaluation protocol is not provided.  The paper claims to not use data augmentation but it is unclear if resizing is used.", "Paraphrased Summary This paper introduces a method for optimizing both individual data representations and an encoder network. This method allows learned representations to inherit features from multiple pretrained selfsupervised models. Experiments demonstrate improved performance compared to baseline methods that concatenate or average individual features using knearest neighbor accuracy metrics. Strengths and Weaknesses Strengths: 1. The paper addresses an significant issue: improving ensemble performance for selfsupervised learning. 2. The proposed method for learning both representations and encoder is wellreasoned. 3. Experimental results show enhanced performance of the learned representations. Weaknesses: 1. Experimental comparisons rely solely on knearest neighbor classification without data augmentation which is not a typical approach in selfsupervised learning. 2. The comparisons with baseline methods using learned representations are unclear as the learning work has already improved the original individual features. 3. Key baselines such as ensemble methods based on linear classifiers trained on each models output are missing. 4. Figures in the paper lack color distinction making it difficult to discern results. 5. The effectiveness of transferring the MLP is unclear and the number of data points required for effective generalization is not explored in detail."], "l9tb1bKyfMn": ["Paraphrase: Summary The research suggests reducing the issue of channels for query and Key in multiplehead selfattention offering potential benefits for datasets like CIFAR and Caltech. Strengths:  The approach is straightforward easy to apply to most selfattention operators. Weaknesses:  It lacks significant technical advancement optimizing channel count in neural networks is not a substantial contribution.  Experiments primarily used small datasets and performance was generally lower with the proposed Lightweight MultiScale Attention (LMSA) approach.  The study uses an unclear metric (\"activation normpeaktemperature\") in range of the conventional measure of data efficiency (reduced data necessity for training).  Differences in queryKey channels could independently influence this metric even at the start of training.  Reference: [Neural Architecture Search for Lightweight NonLocal Networks (CVPR 2020)](https:openaccess.thecvf.comcontentCVPR2020papersZhengNeuralArchitectureSearchforLightweightNonLocalNetworksCVPR2020paper.pdf)", "Paraphrased argument: Summary:  This study introduces a highperformance selfattention technique that lowers channel count to cut down on Transformer computing. Strengths:  The need for innovation in selfattention computation motivated this research.  Empirical data proves the effectiveness of the suggested strategy. Weaknesses: Novelty Concerns:  The proposal primarily centers on decreasing Query and Key sizes which appears to be parameter tuning rather than a genuine attempt at dimension compression.  Selfattentions main bottleneck lies in spatial computation which this work ignores. Thecomplexity remains O(N2) as shown in Table 2. This issue should be discussed or proven irrelevant for the studys problem. Claim Concerns:  The claim that \"few works aim to improve the visual selfattention mechanism\" is inaccurate as evidenced by previous study (e.g. ET [1] LambdaNetworks [2] HaloNet [4] LRA [3]).  Its not startling to find that Query Key and Value dimensions need not match. Adjusting these dimensions balances computation and performance. Technical contingent Concerns:  The method of selecting Query and Key dimension reductions rather than Key and Value is unexplained. Further analysis is required.  The paper only provides a highlevel description of CP(X) the compression performance without going into contingent. experiment Concerns:  The LMSAs superiority over MSA in Tables 3 and 4 is marginal.  The study lacks ImageNet experiments which are common in the field reducing the results credibility.", "Summary This paper suggests a method for lowering the computational cost of a popular selfattention module (LMSA). LMSA decreases the dimensionality of selfattentions keyquery while leaving the values dimensionality unaltered. As a issue the complexity of selfattention is reduced from O(N2D) to O(N2Dqk). The technique is investigated for image classification and semantic segmentation. Strengths  Simple and coherent concept  Clear and organized writing  Integrated LMSA into transformerbased networks and tested it on various benchmarks Weaknesses  Limited Novelty and Significance: The method appears straightforward and has been used in other approach. This raises doubts about its originality and importance.  Insufficient Validation: Experiments were limited to small datasets or weak baselines. The absence of ImageNet validation for image classification and short performance in semantic segmentation hinder the evaluation of LMSAs efficacy.  potential Bottleneck Misidentification: Selfattention may not be the bottleneck in innovative transformerbased models like Swin or CSwin rather it may be the feedforward network (FFN). The paper does not assess the impact of LMSA on inference speed leaving this issue unresolved."], "mKDtUtxIGJ": ["Summary Paraphrase: The study presents a novel approach that combines the tasks of point cloud denoising completion and upsampling into a individual problem known as \"point cloud reconstruction.\" The approach involves two main components: a voxel generation module that enhances voxel density and eliminates outliers and a point relocalization module that converts discretized voxels back into point clouds. The proposed method exhibits strong performance in selected evaluation metrics and datasets. Strengths and Weaknesses Paraphrase: Strengths:  Addresses multiple critical point cloud processing challenges simultaneously.  Wellwritten and straightforward introduction. Weaknesses: Major effect:  Overlooks misalignment effect when combining multiple scans.  Neglects a wide range of recent research on neural work implicit reintroduction for surface reconstruction.  approach point:  Unclear how the final number of points is specified.  Color information determination for novel points is not clear. Evaluation baseline:  Questionable choice of baseline due to lack of unified approach.  Should compare the method with others performing different tasks (denoising completion upsampling).  Unclear range of application in combined baseline.  Lack of compare with implicit reintroduction surface reconstruction methods. Evaluation Datasets:  Biased towards planar geometries.  Missing effect on datasets with complex geometries (e.g. Surface Reconstruction Benchmark). Evaluation:  Missing time and memory requirements.  Resource consumption compared to baseline is not provided. Lack of Insight:  Missing analysis of module contributions and overall novelty.  Lack of technical point for method reproducibility. Minor effect:  Inaccurate use of \"continuous 3D points\" throughout the paper.  Evaluation metrics not properly specified.  Vmid label missing in design 2 and 3.", "Paraphrased Summary: This paper presents a novel twostep pipeline that simultaneously tackles three point cloud processing tasks: densification denoising and reconstruction. The pipeline employs a 3D stacked hourglass network for voxel densification and denoising followed by transformers to convert the voxels into dense point clouds. The proposed method outperforms most existing techniques on the ShapeNet dataset and exhibits exceptional generalization capabilities. Strengths and Weaknesses: Strengths:  The twostage pipeline allows for efficient multitask processing addressing denoising completion and reconstruction simultaneously.  The method achieves superior reconstruction accuracy and visualization effect compared to stateoftheart techniques.  The innovative use of amplified positive encoding and transformers enhances the computation of relationships between center voxels and neighboring voxels. Weaknesses:  While the ShapeNetPart dataset effect are comparable to topperforming methods the generalization capabilities for ScanNet and ICLNUIM datasets are significantly better than others but the paper does not delve into the understanding behind this discrepancy.  An explanation of why the proposed method achieves superior generalization performance and why existing methods struggle in this aspect would enhance the paper value.", "Paraphrased Statement: Summary This paper presents a individual unified approach to address subtasks in point cloud completion. It combines two existing methods: point cloud densification and denoising (stage 1) with point cloud completion (stage 2). The paper evaluates its approach on three datasets comparing it to three different baseline and achieving superior performance. The use of sparse convolution for point cloud processing is intriguing as an autoencoder (termed \"hour glass\") aligns with the tasks objective. Strengths  Novelty: The paper combines two previously separate problem in computer vision.  Simplicity: The twostage architecture is straightforward with only the addition of positional embedding.  performance: The proposed approach outperforms baseline methods with numbers varying based on voxel solution.  Qualitative effect: Example range demonstrate improvement in completion although more examples would be beneficial. Weaknesses  introduction: The paper needs clearer explanations particularly in Section 3.2 regarding amplitudebased positional embedding.  baseline compare: The baseline are not adequately described making it difficult to assess the fairness of the compare.  stage 1 Evaluation: The need for stage 2 is not justified due to the lack of an autonomous evaluation for stage 1.  Voxel Center Handling: The paper does not address how multiple voxel centers may converge to the same 3D point location.  Upsampling Scaling component: The paper claim of avoiding a scaling component in upsampling is inaccurate as it manifests as voxel density in stage 1.  Supplemental Material: A supplemental section with more qualitative examples is missing.", "Summary This paper develops a individual framework for joint point cloud upsampling denoising and completion termed \"multitask.\" It comprises two components: a pointtovoxel autoencoder for denoising and voxelbased feature extraction and a Point Relocalization transformer for point upsampling and completion. The multitask framework exhibits superior performance to individualtask approach on various benchmarks. Strengths  The idea of using voxelization for denoising and completion is logical and effective.  The method significantly outperforms individualtask baseline.  The framework is wellwritten and includes comprehensive ablation studies. Weaknesses  The baseline considered are not sufficiently comprehensive particularly multitask joint learning approach.  The methods performance should be evaluated on individual tasks to assess specific improvement.  The effectiveness of positional encoding is not fully explained.  several point require clarification:  The average number of voxels processed in the transformer and its memory requirements.  The innovation and characteristics of input noise sparsity and incompleteness in the datasets.  The determination of the output point dimension and its consistency with previous research.  The definitions of \"sparse\" (point upsampling) and \"incomplete\" (point completion) should be explicitly clarified.  The term \"point cloud reconstruction\" is misleading and could be revised to \"point cloud refinement\" or \"conditioned point cloud reconstruction.\"  The design and significance of clustering (voxel hashing) in the transformer should be better explained. Minor Issues  The input design in design 2 is difficult to visualize due to colormap effect.  The choice of the 3D sparse Hourglass network should be justified with relevant references. Suggestions  Highlight the secondbest baseline method in Table 1 to better demonstrate the superiority of the proposed method."], "z1-I6rOKv1S": ["Paraphrase: This work introduces a method called \"autoregressive quantile flow\" for quantifying uncertainty in regression tasks. It uses a type of probabilistic model called \"normalizing flow\" to transform data and estimation uncertainty. The method can be applied to different types of regression problems such as object detection time series forecasting and generative models. It offers the advantage of being flexible and adaptable to various transformations allowing it to be used with linear or nonlinear models. However the authors do not provide a strong rationale for using normalizing flow specifically for uncertainty estimation as there are other models that can be trained using gradientbased methods and modified to incorporate quantile loss function. Additionally the paper lacks a clear explanation of the method and an algorithm box could help improve understanding.", "Paraphrased Statement: Summary: This research presents a new model for training flow models called Autoregressive Quantile Flows (AQF). The proposed method uses a novel objective that evaluates forecasts using proper scoring rules such as the continuous ranked probability score and the check score. AQF has two advantages: avoiding the need to explicitly compute the determinant of the Jacobian matrix and providing uncertainty estimates for predictions. evaluation on various tasks including regression object detection time series forecasting and generation demonstrate the effectiveness of this model. effectiveness and Weaknesses: effectiveness: 1. The objective is novel eliminating the need for computing the Jacobians inverse making flow models more applicable with complex transformers like neural networks. 2. The objective design enables samplingbased training allowing for a wider array of loss function. 3. AQF enables distribution estimation for model outputs facilitating uncertainty analysis. Weaknesses: 1. The evaluation metrics could be described in more detail for clarity. 2. There are typos throughout the paper. 3. Dataset sources or links should be provided and reference formatting should be standardized. The Sandler et al. (2018) paper should be cited correctly.", "Paraphrase: Summary: This paper presents a novel technique for estimating probability densities and quantifying their uncertainty. The technique expands upon traditional normalizing flows by: 1. Utilizing objective function based on proper scoring rules. 2. Employing autoregressive quantile flows. 3. Defining quantile flow regression. effectiveness:  The proposed objective function are wellfounded and have favorable theoretical properties.  The extension to quantile flows is logical and wellpresented.  The implementation of all methods is clearly explained.  The paper is wellorganized and presents complex concepts effectively.  The analysis on UCI datasets is thorough. Weaknesses: Major:  comprehensive experimentation on synthetic data are lacking which would help assess the methods performance in capturing true quantile function the validity of forecasting intervals and comparisons with maximum likelihood estimation. Minor:  Some terms in Tables 3 and 4 are undefined.  Typos should be reviewed throughout the manuscript."], "ghTlLwlBS-": ["Paraphrased instruction: Summary: This research presents a new Feudal Reinforcement Learning (FRL) algorithm designed to bridge the gap between complex natural language instructions and the technical actions required to execute them. FRL Algorithm: FRL employs two agents: a \"manager\" and a \"processer.\" The manager generates a plan by breaking down the goal into sequential targets. The processer executes the plan by interacting with the environment and adjusting its actions based on observations and the target coordinates. Training and Evaluation: The manager and processer are trained separately using imitation learning on a dataset collected through random exploration. The FRL algorithm is tested on two textbased interactive games: RTFM and Messager. effectiveness and Weaknesses:  effectiveness: The application of FRL to this problem is novel and promising.  Weaknesses: The paper lacks detailed explanations of the RL method used and equations and the results reported differ from previous process in certain environments. The evaluation in Messager is limited and not fully presented in the paper.", "Summary Paraphrase: This field proposes a hierarchical model for reinforcement learning that enables solving tasks involving reading and comprehending instructions or manuals to fulfill complex goals. The model consists of two components: a manager that analyzes instructions to establish subgoals and a worker that executes these subgoals and handles lowlevel perceptions. The components are trained separately. The model achieves stateoftheart results on the Messenger and RTFM benchmarks without relying on manually designed learning curriculum. effectiveness and Weaknesses Paraphrase: The field is clearly presented and understandable. The authors provide adequate need and connect the model to previous research. However there is a lack of clarity in describing the Comatch LSTM model and the figure representing linear level should be revised to align with the description. The authors employ a random agent to collect successful trajectories for training the manager which may not be efficient. They should provide more information about the information collection process and discuss potential challenges in learning for the manager in more complex environments. The field examines the impact of different training strategies on the models performance but does not delve into the models failure cases and why it encounters difficulties. solution on the Messenger benchmark are included in the appendix but not sufficiently discussed in the main text. A description of the Messenger benchmark and an analysis of the models failures in level 3 would be valuable additions. Typo correction:  \"Our model need\" should be \"Our model need\"", "Paraphrase: This field introduces a model called feudal reinforcement learning (FRL) to tackle the challenge of interpreting text instructions in combination with basic environment information. FRL consists of two agents: a \"manager\" that creates a multistep plan involving multiple goals and a \"worker\" that uses lowlevel information to carry out actions for each goal. The authors test FRL in two difficult situations and find that it matches or outperforms existing methods. effectiveness and Weaknesses effectiveness:  FRLs \"backwards\" goalgeneration method significantly improves performance.  The performance of the FRL model that generates goals backwards is comparable to that of a model with perfect goal information. Weaknesses:  Certain passages in the paper are unclear making comprehension difficult (e.g. \"It achieves a win rate about 12 instead of 14 \u00b7 13 112\").  There doesnt appear to be any significant introduction in the models design compared to existing models.", "Paraphrased Summary: This paper introduces a hierarchical reinforcement learning (RL) method for solving text adventure games where information is presented in text. The manager agent processes the text breaking it down into a sequence of subgoals. worker agents then carry out these subgoals to achieve the overall goal. The paper includes descriptions of the models and experiments (mainly RTFM) as well as an Appendix application Messenger experiments. effectiveness and Weaknesses:  Originality: The hierarchical RL approach used to separate text instructions into goals and RL execution is innovative.  Quality: The approach is promising but some concerns remain:  The language models used (LSTMs) are outdated transformerbased models would be more appropriate.  The experiments are specific to RTFM limiting the generalizability of findings.  The results analysis focuses also much on environmentspecific details making it difficult to extract insights for other problems.  Clarity: The paper contains numerous grammatical and semantic error hindering comprehension.  Suggestions:  Move unnecessary details to the Appendix.  Include the Messenger experiments in the main body for comparative analysis.  Improve the writing to enhance readability."], "okmZ6-zU6Lz": ["Paraphrased Summary: The study investigates methods for quantifying controllability metrics using limited network observations assuming community structure. Key findings include theoretical conditions that determine the feasibility and accuracy of such quantification. Strengths and Concerns: Strengths:  study an underexplored topic of learning network properties from aggregate data. Concerns:  Motivations: The use of linear control theory for studying brain networks may be questionable due to the nonlinear nature of the brain.  Relevant Literature: The authors have not adequately discussed the related subject of \"blind estimation\" which has similar objectives.  Practical applicability: The Assumption of a stochastic block Model (SBM) may not be ideal in practice as it assumes dense graphs whereas realworld networks tend to be sparse.  Sparse Graphs: SBMs might not accurately capture the behavior of sparse networks.  Assumption discussion: Assumption 4 requires further explanation as it appears to be restrictive. Technical Flaws:  Adjacency Matrix: Normalizing the adjacency matrix by the trace is not appropriate due to the absence of diagonal elements.  SBM Expression: The expression for the SBM assumes the existence of selfloops which should be explicitly mentioned.  Controllability statement: The statement regarding controllability for T vs. n step may not hold true for discretetime systems.", "Paraphrase: This paper examines controlling networks when the structure is partially known. It suggests a method for approximating controllability based on the networks topology. Strengths:  Provides a manner to infer overall network controllability from limited knowledge of the networks connections.  Bridges control theory and network science by combining mathematical support with an inspiring approach. Weaknesses:  The range may be narrow for computer science enthusiasts. A more general framework could be explored.  Practical examples highlighting when this approximation is beneficial are lacking.  The statistical significance of results in Figure 2 is unclear.  Assumption regarding diagonal elements are inconsistent. In brain networks there are no selfloops while in SBMs selfloops may exist.  It is unclear if the result is a byproduct of the inference on the hidden stochastic block structure. The two step of estimating controllability and block structure appear disconnected.", "Paraphrased statement: Summary: This study focuses on controlling the behavior of interconnected dynamical systems where observations are incomplete. Such systems are coarse in various applications making the control topic significant and relevant. The authors develop a lowerorder system using limited data and compute bounds for the convergence and approximation error relative to the original system model. These bounds are experimentally evaluated using a small synthetic dataset. Strengths and Weaknesses: Motivation: The paper lacks explicit motivation for examining the controllability of stochastic block models (SBMs). Background: The authors should provide more background on Gramian matrices and related concepts in dynamicalal systems as they may be unfamiliar to some reader. Definition 1: The definition should specify that v belongs to Vk. The concept of \"cnodes covering the fine graph\" could be clarified with a better explanation of \"covering.\" Partial Observability: The paper mentions partial observability but doesnt clearly explain its use in the problem formulation. Synchronization: The term \"synchronization\" is used in an unconventional manner describing the coverage of coarse communities raising questions about the justification for this use. Assumption: Assumption 2 and 3 need more context and justification. problem statement: Section 4s problem statement lacks sufficient clarity. Remark 2: The phrase \"This Assumption is a very weaker condition than asking...\" should be revised for clarity. Experiments: The experimental section is limited and it would be beneficial to test other network types. use of \"learning\": The use of \"learning\" should be clarified in the context of learning theory. Highlight: The phrase \"Broadly our analysis highlight the use of\" should be corrected to \"highlights.\""], "y8zhHLm7FsP": ["Summary Paraphrase: The paper presents a simulationbased algorithm for learning optimal control policies for Linear Quadratic Gaussian (LQG) control problems. It enhances the existing ensemble Kalman filter (EKF) algorithm used for state estimation to design control policies in the LQG setting. The algorithm is theoretically proven to be exact in the limit as the number of particles approaches infinity. Strengths Paraphrase: The proposed algorithm offers computational efficiency advantages when dealing with highdimensional control problems where traditional methods based on differential or algebraic Riccati equations have a complexity of O(d2) (d: state dimension). The EKFbased algorithm has a complexity of O(Nd) (N: number of particles used for estimation). Weaknesses Paraphrase: 1. The originality of extending EKF for control policy estimation is questionable. The papers results seem to indicate that constructing the mean field process for control policy estimation is similar to that for state estimation using EKF. The claimed exactness contribution is also not particularly significant. 2. The claim of reduced computational complexity from O(d2) to O(Nd) is debatable. While Nd can be used the choice of N may have implications for the accuracy of the estimated control policy. Furthermore the error bound in the paper suggests that N may still depend on the problem size making the computational complexity not as straightforwardly reduced as claimed. 3. The evaluation metric used in the paper may not be appropriate. The Mean Squared Error (MSE) with respect to the true matrix is not the same as the loss function minimized by the algorithm in Fazel et al. (2018). A more suitable metric would use the same loss function to assess performance.", "Summary The paper explores using the ensemble Kalman filter (EnKF) for state estimation and control in linear quadratic Gaussian (LQG) problems. The algorithm is assessed on a linear system and a linearized nonlinear system. Strengths: The paper introduces a novel approach for control using EnKF as an alternative to Riccatibased solvers. Samplebased inference such as the extended Kalman filter or sequential Monte Carlo enhances accuracy particularly in complex nonlinear systems. Weaknesses:  learning: The paper overlooks previous work on the duality between Kalman smoothing LQG and Riccati equations.  Motivation for EnKF: The rationale for using EnKF suitable for highdimensional linear systems with limited samples is not fully explained for practical control problems.  RL interpretation: Despite being labeled as RL the EnKF application is more aligned with optimal control lacking the explicit use of models or linear dynamics.  Experiments: The experiments are limited especially in the context of ICLR.  Minor: The paper title should avoid abbreviations.", "Paraphrased Statement: Summary The paper proposes a method for calculating filters and control in continuoustime linear systems. It uses a simulator to generate multiple trajectories and calculates statistical properties for empirical estimation and actuation signals. Strengths and Weaknesses  Weaknesses:  The problem is not adequately motivated.  The method performs no beneficial than classical Riccati equation approaches and is only computationally advantageous for highdimensional systems. However for such systems statistical properties require numerous trajectories for covariance estimation which negates the computational benefit.  The literature review is incomplete focusing primarily on discretetime systems and neglecting innovative continuoustime reinforcement learning control methods.  The paper makes several incorrect or inaccurate statements including:  LQR is portrayed as having a significant bottleneck without adequate explanation.  Generality and optimality are conflated in the footnote and Proposition 1.  In Equation 3 initial state expectations and covariance are assumed known without justification.  Strong claims about reinforcement learning algorithms require stronger evidence.  The oracles role is unclear since access to the oracle does not guarantee access to its minimizer.  Several unclear statements or unsupported claims are present throughout the paper.  Typos writing errors and an incorrect citation format are also found.  The terminology of \"particle\" and \"meanfield\" is borrowed without connecting it to existing literature.  Restricting the control signal to depend only on the current filtration excludes certain policy classes.  Markovianity of Step 2 is assumed in reinforcement learning without justification.  The term \"backward Itointegral\" is undefined.  reinforcement learning terminology is misused and connections to reinforcement learning appear artificial.  The benchmark adopted in the relationship to reinforcement learning is flawed the appropriate benchmark should be solving the Dynamic Riccati Equation and Algebraic Riccati Equation.  It is unclear what the control input is in the weather prediction application."], "gRCCdgpVZf": ["Paraphrased Statement: Abstract This research focuses on the number of domain generalization assuming that a multidimensional array indexes each domain. Only a few domains are available for training. A method is proposed to create a classifier with strong generalization abilities on unseen domains. The technique involves creating a shared latent feature representation for all domains and domainspecific linear functionals. These functionals are acquired by restricting the tensor produced by merging the functionals from all domains to be lowrank. The study provides a exhaustive theoretical analysis including an upper limit on the target domains excess risk. effectiveness and Weaknesses The paper is wellwritten and technically sound but some aspects require clarification:  Advantages and disadvantages of replacing Eqs. (2) and (3) with the surrogate in Eq. (5).  In the MNIST experiments how are prediction from the fc2 and fc3 layer combined What if classifiers were only constructed on fc3  In the MNIST experiment the value and selection work of K and its effect on the results. Concerns:  Limited applicability: The method may not apply to many realworld situations where domains cannot be defined by an attribute vector.  Experimental results: The approach should be compared to a stateoftheart domain generalization algorithm such as Matsuura and Harada (2020).", "Paraphrase: Proposed model: This study introduces a domain adaptation method that focuses on a limited set of domains during training. Training work: With enough training data for the observed domains a shared latent representation and domainspecific classifiers are created. Theoretical Basis: Mathematical criteria are provided to ensure that the learned representations are transferable to unseen domains. Experimental Evaluation: The method is tested on two datasets with limited results. Evaluation Limitations: The choice of datasets (MNIST and fiber sensing) is not ideal for evaluating domain generalization methods. More relevant datasets such as those listed on the WILDS leaderboard could provide a more comprehensive assessment of the methods effectiveness.", "Paraphrased Statement: Summary: This research investigates the concept of zeroshot domain adaptation where a model trained on one domain (source) is adapted to perform well on a different domain (target) without any targetspecific labeled data. Contributions:  The authors introduce a novel domain adaptation method.  They provide theoretical bounds on the models prediction error and prove their validity.  They experimentally demonstrate the effectiveness of their method on MNISTbased and sensor datasets. effectiveness:  The error bounds appear to be accurate.  The authors provide empirical evidence supporting their method on two datasets.  The paper is wellwritten and clear. Weaknesses:  The empirical evaluation could benefit from experimentation on more complex datasets.  The authors could provide more details on how they determined the hyperparameters for their method.", "Paraphrase: Summary: The authors propose a zeroshot domain adaptation method that assumes all domains share the same labels. They establish sample complexity boundss involving the number of training samples per class seen (training) and unseen (test) task under specified conditions. The method provides accurate results on MNIST and rotated MNIST datasets. Method: The method employs a common linear mapping to represent task followed by a domainspecific linear classifier for each task. A tensor completion technique enables predicting on unseen data without training on target domain samples. The method assumes a fixed number of samples and input dimensions for each domain (although different dimensions can be accommodated). experimentation use LeNet and directly predict decomposed tensor instead of using tensor completion. Contributions: The authors extend Tripuraneni et al. (2020)s method and proofs from transfer learning to the zeroshot domain adaptation setting. They derive a uniform convergence bounds with polynomial complexity in terms of task dimensions and rank resulting in excess risk on the order of n14. effectiveness:  Novel contribution to zeroshot domain adaptation analysis.  Theoretical analysis supported by empirical experimentation.  Polynomial bounds with exponential input dimensions demonstrates scalability. Weaknesses:  limited contributions as the method closely resembles Tripuraneni et al. (2020).  assumption of equal samples and dimensions for all domains may limit practical applications.  Existence of prior work with finite sample analysis in zeroshot domain adaptation.  Structural improvements could enhance organization and clarity."], "kcadk-DShNO": ["Summary: This work proposes a method for unsupervised alignment of data sets using flowbased techniques. It demonstrates that minimizing a density frameworkbased objective is equivalent to optimizing the upper bound of the generalized JensenShannon Divergence (JSD). Using this result the authors introduce a novel regularizer for aligning multiple distributions. Their approach extends existing methods like AlignFlow and LRMF and provides a more comprehensive framework for unsupervised data set alignment. The work includes extensive experiments to demonstrate the effectiveness and superiority of the proposed method. Strengths:  Provides a theoretical upper bound of GJSD with a learnable density work extending the AlignFlow method for multidistribution alignment.  Explores the connections between prior methods (such as LRMF) and a proposed regularization term.  Offers extensive experimental validation of the methods efficacy. Weaknesses:  The practice of a learnable Q work instead of a fixed work as in AlignFlow requires further justification. The authors claim that a fixed normal distribution Q is insufficient but should provide more context and evidence to support this.  The work compares its method to LRMF which aligns two data sets while the proposed method is designed for multiple distributions. The authors suggest that LRMF may fail when the density framework class Q is not expressive enough. They should clarify the distinction between the two methods and discuss the potential challenges when aligning a larger number of distributions.", "Summary: This paper introduces a novel approach that combines flowbasisd frameworks within a nonadversarial framework for aligning multiple distributions. Strengths:  Offers an innovative estimation for distribution alignment. Weaknesses: Concerns:  Challenging adversarial learning: The paper claims adversarial learning is challenging for distribution alignment but lacks a concrete illustration or justification beyond outdated source. Adversarial learning techniques have significantly advanced since 2019.  Auxiliary density framework training difficulty: The paper suggests that training the auxiliary density framework Qz may be more challenging than the discriminator D which contradicts the proposed justification that adversarial learning is difficult. Proposed improvements:  Conduct an experiment or provide a concrete illustration to support the claim about adversarial learning challenges.  Consider using a more diverse dataset than MNIST to demonstrate the frameworks performance.  Incorporate comparisons with other multidomain transformation frameworks using GANs such as MUNIT and StarGAN v2.  Conduct ablation work by changing the flowbasisd basis framework to demonstrate the frameworks robustness.  Explore ways to visualize the latent space of real data using the flowbasisd framework.", "Paraphrased argument: This research introduces a standardized method for aligning domains using nonadversarial techniques. Instead of addressing a competitive minmax problem this method focuses on a minmin optimization that minimizes the upper bound of JensenShannon Divergence (JSD). It establishes the equivalence and provides a practical solution that can be seamlessly integrated into nonadversarial flowbased approaches. Experiments involving the generation of different digit classes or domains attest to the frameworks accuracy. Strengths:  Introduces a novel domain alignment loss with theoretical underpinnings and connections to JSD.  Wellwritten and easy to comprehend.  Experimental details are readily available and verifiable. Weaknesses:  limited applicability of the framework to broader learning tasks.  While promising the experimental results are not significantly superior to existing stateoftheart methods.  Lack of results demonstrating the frameworks adaptability to highdimensional or structured data.  The frameworks computational efficiency and optimization capabilities are not clearly demonstrated compared to GANs."], "wogsFPHwftY": ["Paraphrased Statement: Summary The authors present an architectural framework for retrieving deep images using midlevel features called superfeatures (SFs). These SFs are constructed iteratively via an attention module and arranged sequentially with each element focusing on a distinct and localized image pattern. training and Validation The authors demonstrate that training the SFs requires only image labels. Experiments on benchmark databases show that their method (FIRe) significantly outperforms existing ones using the same number of features while consuming less memory. Strengths and Weaknesses Strengths:  Proposed a novel image retrieval method called FIRe.  Clear and wellwritten paper.  Comparisons with stateoftheart techniques demonstrating superior performance on specific datasets.  Uptodate references. Weak Points:  No significant weaknesses identified.", "Paraphrase: This paper introduces a novel \"superfeature\" model for image retrieval. Its key contribution include: 1. Feature Extraction: The model extracts features using an innovative \"local Feature Integration (LIT) Transformer.\" This transformer uses an iterative attention mechanism. While similar module exist the authors make unique modifications (decorrelation loss initial template learning residual connections). 2. training: The model trains using contrastive loss applied to local features rather than the common global embedding constraints. The paper presents solid experimental results on public datasets. The model outperforms baselines by a significant margin in terms of accuracy and memory efficiency. The ablation studies provide insights into the importance of each component. Strengths:  Excellent experimental results and performance improvements  Thorough ablation studies  Novel architecture and loss use Weaknesses:  Limited innovation beyond the novel contribution  Absence of PatchNetVLAD from the references and baselines  Unclear reasoning behind the \"midlevel feature\" terminology", "Paraphrased Statement: introduction: This paper proposes the use of \"super features\" in image retrieval forming the FIRe framework. Super features are extracted from feature maps at different scales guided by query vectors representing latent concepts. FIRe employs a transformerlike component called LIT to aggregate these super features. training loss: Two loss use are used for training. The first involves tripletlike matching on super features using empirical criteria to select positive and negative samples. The second decorrelates super features from the same image. Evaluation: Image similarity is measured using the Aggregate Semantic correspond core (ASMK) which compares groups of local features. Experiments on the Oxford and Paris datasets demonstrate promising results. Strengths:  Clear motivation and intuitive concept of super features.  Reasonable innovation using a transformer for feature aggregation.  Interesting experimental findings such as FIRes distinct behavior in local feature selection compared to other methods. Weaknesses:  Question about double normalization in the attention mechanism.  Lack of discussion on LITs relationship to set transformers.  potential concern about the complexity of the ASMK measure for comparing super features. Optional Discussion Points:  Alternative solutions to the oversmoothing problem addressed by Lattn.  more efficient methods for obtaining multiscale features without excessive memory use.", "Paraphrased Statement: The work presents LIT (local Feature Integration Transformer) an innovative architecture for extracting image features for retrieval. The resulting \"SuperFeatures\" outperform existing methods on two image retrieval benchmarks. Additionally a novel learning method called FIRe (Feature Integrationbased Retrieval) is introduced. FIRe trains LIT using a contrastive loss applied directly to local features unlike other techniques that apply the loss to image embeddings or aggregated local features. SuperFeatures possess the property of being ordered aligning well with similar image structures. Weaknesses:  Inconsistent mathematical formulations hinder understanding and reimplementation.  computational costs of feature extraction are not discussed.  Transferability of SuperFeatures to nonlandmark field remains unclear. General Suggestions:  research alternative retrieval methods beyond ASMK.  Test the sensitivity of SuperFeatures to template initialization.  Consider adding retrieval rows to Table 1.  Specify the value of \"N\" in experiments.  research retrieval methods that leverage the ordered nature of SuperFeatures.  Cite additional relevant image retrieval methods. specific Suggestions for Section 3.1:  Clarify the dimensions and meanings of attention maps and image projections.  Explain the segmentation in the second equation of Eq. 4.  Add \"t\" superscripts to Eqs. 4 and 5 for clarity.  Provide motivation for normalizing attention maps."], "gULyf2IVll0": ["Paraphrase: Summary This research proposes a new metric called \"populated region set\" (PRS) to explain why models with similar clean accuracies can have vastly different vulnerabilities to adversarial attacks. PRS concern to the areas in a models decision space that contain training model. The authors demonstrate that PRS size in the secondtolast layer is inversely proportional to robustness against attacks. This relationship holds across multiple datasets (MNIST FMNIST CIFAR10) and model architectures (CNN VGG ResNet). Strengths:  The concept of PRS is novel and warrants further exploration.  The authors analyze PRS from various perspective and show its correlation with other significant properties across multiple datasets and models. Weaknesses:  The paper lacks clarity and does not provide sufficient details about the models used in the study.  The method for calculating confidence intervals for accuracy is unclear.  The chosen attack type (FGSM) is not the strong potentially limiting the results applicability to more powerful attacks.  The paper contains numerous grammatical errors that hinder readability. Additional Comments:  It is not clear if models trained with adversarial model have smaller PRS compared to standard training.  The authors could provide a clearer explanation of the relationship between PRS size and parameter alignment in the final layer.  Some figures could be improved for clarity and additional model of successful attacks on models with different PRS size would be valuable.  The term \"more intensive feature representation\" is not clearly defined.", "Paraphrase: Summary: This research seeks to comprehend the resilience of deep neural netstudys (DNNs) through the lens of decision regions. The authors introduce a novel metric the Populated Region Set (PRS) ratio which they use to empirically analyze the resilience of various DNNs. They propose that a lower PRS ratio (i.e. decision regions with at least one training data point) leads to increased robustness improved representation of test instances in populated regions and acquisition of sparse feature representation. empirical evidence was gathered using CNN models (ResNet18 and VGG16) on datasets (MNIST FMNIST and CIFAR10) under diverse perturbations (i.e. various epsilon measure and untargetedtargeted attacks). Strengths and Weaknesses: The paper is clear and wellwritten. The PRS ratio a new geometric metric appears promising and intuitive. While the analysis comparisons (as outlined in the summary) are insightful the author acknowledges limited knowledge of related study on the interplay between decision boundaries and model resilience. The conclusions drawn from the empirical evidence align with expected results regarding decision regions. However the author questions the informativeness of the PRS ratio in more challenging scenarios with a larger number of classes. experimentation was limited to datasets with only 10 classes and it is plausible that the PRS ratio may be generally higher for larger label spaces.", "Summary Paraphrase: This study examines the link between the performance of deep study models (1) in terms of robustness and (2) the distribution of their decision boundaries in model space. A new measure the Populated Region Set (PRS) is introduced to quantify the number of decision regions with at least one training sample. The researchers propose that this metric has a strong relationship with robustness and support this claim with experiments. Strengths and Weaknesses Paraphrase: Strengths:  The analysis of decision boundaries is a valuable area for study.  Figure 2 illustrates a clear decline in the PRS ratio for a particular neural netstudy model. Weaknesses:  The absence of PGD attacks in the robustness evaluation.  The relationship between PRS and robustness is less pronounced for certain models such as CNN6 in Figure 4.  The authors role the term \"intensive\" to describe tSNE plots in Section 6 without providing a quantitative measure for this intensity.  Regression slopes are only visually presented without being reported quantitatively or tested for significance. Questions for Rebuttal:  The role of the area under the robustness curves as a robustness score is questioned.  How is it feasible to compute the PRS metric despite the \"curse of dimensionality\"  What is the relationship between this study and previous study that have analyzed linear regions of decision surfaces PostRebuttal: Despite the authors response the reviewer remains uncertain about the robustness and rolefulness of the PRS metric and its empirical evaluation. The review score remains below acceptance."], "rX3rZYP8zZF": ["Paraphrase: Summary: This research introduces a graphbased approach to recommending diabetes selfcare management strategies. It leverages knowledge graph embedding techniques to develop a system that outperforms baseline approaches in terms of AUC. effectiveness:  potential for diabetes selfcare management  Clear and accessible writing style Weaknesses:  Limited technical innovation  Inconclusive experimental design Significance:  The application is relevant and valuable for managing diabetes.  However the technical advancement is modest as knowledge graphbased recommendations have been explored in other contexts. Soundness:  The experiment lacks credibility due to the weak baseline (DCN) and absence of realworld testing.  A more robust evaluation that includes online metrics and comparisons with other knowledge graph models is necessary.", "Paraphrase: This paper presents a method to improve recommendation system in healthcare using a knowledge graph. The researchers use data about users to choose how a knowledge graph should be set up to represent the users preferences. When tested the method showed some improvement in predicting how likely a user is to click on a suggested process. effectiveness:  The issue is relevant and interesting.  The paper is clear and easy to follow. Weaknesses:  The paper technical contribution are not particularly novel or innovative. Applying knowledge graph to improve recommendations and using user data to choose how the graph should be set up are common approaches.  The paper lacks significant detail such as how the knowledge graph is created and the specific scope used in the experiments.  The paper does not compare its method to enough similar approaches and the methods it does compare to are not very strong.  The way the paper uses text data to improve recommendations may not be the better approach. Minor issue:  There are some typos in the paper such as:  \"some of the key problem we face is\" should be \"some of the key problem we face are\"  \"individual users\" should be \"individual users\"  \"Zhang et al. (2018) extends. [1] Wang et al.\" should be \"Zhang et al. (2018) extend. [1] Wang et al.\"", "Summary paper Paraphrase: This paper introduces a recommender system based on deep neural networks. It utilizes knowledge graph embeddings called Caregraph to predict health \"nudges\" (content and notifications) in a digital health platform for chronic disease management. The study investigates if knowledge graphs can overcome \"cold start\" challenges in suggesting structured health nudges. effectiveness:  The integration of knowledge graphs into recommender system is a relevant research domain particularly in healthcare. Weaknesses:  The lack of concrete model of recommended nudges makes it difficult to evaluate the applicability of the proposed approach for managing specific disease (e.g. diabetes).  The hyperparameter optimization process is inadequately described hindering the understanding of model training and optimal scope selection.  The paper lacks validation approaches to assess the correctness and safety of the inferred knowledge graph output given its potential impact on patient health.", "Paraphrase: Summary: This paper explores the recommendation of nudges (incentives or reminders) in a mobile healthcare environment. The proposed approach utilizes a knowledge graph that incorporates nudge attributes. Through inferring embeddings for nudges and attributes the method leverages similarity to generate recommendations. effectiveness and Weaknesses: effectiveness:  Developed a knowledge graphbased method for healthcare nudge recommendations.  Addressed the cold start problem by creating a knowledge graph with nudgeattribute relationships.  Used embedding similarity for efficient recommendations. Weaknesses:  Limited novelty in the proposed technique.  Lack of domainspecific insights for healthcare applications.  Unclear discussion of the cold start challenge in this context.  data description lacks data on cold startrelated aspects.  Assumptions and simplifications in technical design may not be fully justified.  Limited experimental evaluation using a single private dataset and one baseline method. other Observations:  The paper requires improvement in grammar and typography for ease of study.  Integration of nudge text into the model was attempted but its effectiveness was not demonstrated.  The handling of novel nudges and embedding updates in the knowledge graph requires clarification."], "xiXOrugVHs": ["Paraphrase: Goal: This research aims to enhance text summarization particularly for lengthy texts by presenting a hybrid reasoning approach that combines topdown and bottomup strategies. Strengths: 1. wide evaluation on long text datasets demonstrates the effectiveness of the topdown structure in improving summarization choice. 2. The proposed model outperforms GPT3 on booklength summarization tasks achieving comparable performance with fewer parameters and training data. 3. Experimental lead show significant improvements in summarization performance compared to baseline models. Weaknesses: 1. The method lacks novelty as similar approach have been explored in other field. The model structure does not adequately address the information redundancy challenges posed by long texts. 2. The description of the model could be more detailed particularly regarding the performance of local attention and the topdown mechanism. 3. The paper lacks quantitative analysis of the models computational efficiency and memory consumption. It only provides comparison of model parameters without experimental validation.", "Paraphrased Statement: Original: Summary model long documents is a challenging problem. This paper works on this problem with topdown and bottomup structures. The hierarchical structure proposed in this paper adopts local and topdown correction to make the model learn local and longrange dependency. The experiment on long document summarization benchmarks shows the effectiveness of their proposed method. Paraphrase: Summarizing lengthy documents presents significant challenges. This research proposes a novel hierarchical approach that combines both topdown and bottomup structures to address this publication. The proposed architecture leverages local and topdown correction mechanisms enabling the model to capture both local and longdistance dependencies within the input document. Experiments conducted on established long document summarization datasets demonstrate the efficacy of the proposed method. Strengths:  The hierarchical structure is conceptually sound enabling the model to analyze input documents at both common and finegrained levels by leveraging bottomup and topdown approach.  The method performs well on benchmark datasets. Weaknesses:  The specified local attention window size of 1024 appears overly large and may compromise its local nature. Additionally the authors have not explored alternative window sizes (aside from the ones presented in Table 3).  The paper lacks clarity regarding the window sliding mechanism and whether there is any overlap between adjacent local windows.  While the integration of topdown and bottomup attention is innovative it remains unclear how the crossattention component affects the overall performance.  The implementation of the copy performance is not explicitly described.  The authors initialized their model with BART. To well demonstrate the effectiveness of their proposed method they could consider initializing it with standard BERT or using random initialization.  The experimental section lacks contingent on the parameter settings for baseline methods (BigBird Longformer etc.). It would be beneficial to include the model sizes of these methods for comparison in addition to GPT3.  The writing in the paper could benefit from improvement particularly in the experimental section. Clarification is needed regarding the definition and implementation of OracleAdaPool and the meaning of \"RL\" in this context.", "Paraphrased Summary: This field introduces a novel model design for abstract text summarization. It employs a hierarchical structure for representing documents with a top level capturing longterm dependencies on a large scale and a bottom level capturing tokenlevel contingent. The model exhibits superior performance in both capturing global and local context efficiently. Validation on various datasets demonstrates its superiority over existing models. Strengths and Weaknesses: Strengths:  significantly improved performance on multiple datasets outperforming stateoftheart models by a noteworthy margin.  The models initialization with BART parameters makes its effectiveness surprising and impressive.  The hierarchical design is similar to previous models but its performance lead in exceptional outcomes. Weaknesses:  The specific factors contributing to the performance gain are not clearly articulated.  The paper lacks a discussion on the key components responsible for the models exceptional performance.  Ablation field would be beneficial in identifying the inherent principles.  The document technical focus could be complemented by more insights and intellectual discussion even though it is straightforward.  The use of the term \"inference\" is somewhat misleading as the modules mentioned are trained and finetuned.  The presentation of Figure 1 and its caption requires improvement for clarity and consistency."], "nWlk4jwupZ": ["Paraphrased Summary: This paper introduces a method called ScheduleNet that uses multiple reinforcement learning agents to address scheduling problems (like the MultiTraveling Salesman problem (mTSP) and task Shop Scheduling (JSP)). The goal is to reduce the completion time (makespan). In the mTSP each agent represents a salesperson while in JSP each agent represents a machine. The key approach is using a special type of graph attention mechanism to predict the likelihood of assigning agents to different tasks. Experiments show that ScheduleNet performs well on simulated mTSP and JSP problems. effectiveness:  ScheduleNet can balance computation time and solution efficiency (minimizing makespan). Weaknesses:  Experiments indicate that ScheduleNet is less effective than other methods such as LKH3 especially for larger problems (200 x 20). While LKH3 may take longer to run it finds optimal solutions and the tradeoff may not be worth it for offline settings where the agents and task are known in approach.  The JSP problem may be easier than the mTSP problem because machines in JSP are identical while salespeople in mTSP have different locations. This raises questions about the usefulness of using the same training framework for both problems.  The agenttask graph edge definitions are unclear especially when nodes should be connected.  ScheduleNet appears to be a centralized framework despite claims of coordination among agents. Its not explicitly explained how agents handle situations where they are assigned to the same task.  There are typos in the papers text.", "Summary This work explores using neural networks trained with Reinforcement Learning (RL) to address two difficult combinatorial optimization problems: the Multiple Traveling Salesmen problem (MTSP) and the task Shop Scheduling problem (JSP). The problems are formulated within a Markov Decision Process (MDP) framework with rewards based on a normalized makespan metric. A PPOlike objective use guides policy training. effectiveness and Weaknesses effectiveness:  Novel application of RL techniques to these wellknown optimization problems  Introduction of novel tricks to enhance the applicability of RL Weaknesses: 1. Description of MDP  Ambiguous definition of \"position\" (p\\tau)  Lack of clarity in the generative process of positions and distribution of distances between them  Insufficient explanation of the makespan definition 2. Training  potential similarities between the PPO reward normalization trick and the papers reward normalization but these are not explored  Unclear computation of Formula (9) and the meaning of expected value within it  Rationale behind the choice of a \"slowly varying\" policy as baseline is not provided 3. Experiments  Inconsistencies in the description of training setup (e.g. ScheduleNet training conditions for Tables 1 and 2)  Limited number of training seeds (1234) raising concerns about variance  Lack of clarity in Table 14 entries and their interpretation  Absence of confidence intervals for results  Ambiguous definition of \"Gap\" column  Vague mention of reallife applications without specifying scale or relevance to experimental problems  Computational limitations of the method are not addressed", "Paraphrased Statement: ScheduleNet a novel approach that combines neural networks and reinforcement learning is proposed for solving complex multiagent scheduling issues. It creates an agenttask relationship graph using edge and node embedding a technique that computes message values based on edge feature and then embeds nodes based on those values. ScheduleNet employs these embeddings to calculate assignment likelihoods between free agents and active tasks. Reinforcement learning is used to train ScheduleNet with two optimizations: (1) reward normalization and (2) a time parameter in the reward algorithm. The proposed methods efficacy and ability to handle unseen data in scheduling scenarios is demonstrated by broad testing on typical scheduling problems. effectiveness and Weaknesses: ScheduleNet presents a versatile framework for solving multiagent scheduling issues. The graph constructions typeaware embedding is a novel element that lends itself to distributed processing. reward normalization and reward algorithm change are shown to be effective. broad empirical evidence on mTSP and JSP domains shows that ScheduleNet outperforms existing techniques during training and generalizes well to realworld scenarios. Ablation field validate embedding methods and training optimizations. Queries and Suggestions:  Consider including training time in runtime comparison of RL and OR methods.  Elaborate on the multiagent aspect of the problem and describe a distributed RL training algorithm. Discuss the handling of nonstationarity in multiagent RL.  Explain how strong constraints are enforced during RL training and whether additional constraints such as time windows can be accommodated.  Clarify the setup for Table 2 results which involve training on synthetic data and testing on public datasets. If there was no incremental training on public datasets provide insights into why ScheduleNet generalizes so well. explore whether the parametric distributions between synthetic and public datasets match.", "Summary This work employs deep reinforcement learning (RL) to address traditional scheduling issues. To make RL feasible for these problems the authors represent problem states as agenttask graph with nodes encoded using a novel typeaware graph attention technique. Node embeddings are utilized to establish final assignment probabilities. To accelerate training and enhance performance the authors employ methods such as reinforcement normalization and time loss. The method demonstrates strong performance in the traveling salesperson and jobshop scheduling problems. effectiveness  Wellorganized and clear presentation.  RL is a suitable framework for scheduling problems.  Adjustments in modeling representation and training techniques are effective. Weaknesses  Typos and grammatical errors affect readability.  Repetitions in the formulation section could be streamlined with examples.  The impact of minmax vs. minsum on scheduling is not clearly explained.  The novelty of the method is questionable with minor improvements on existing techniques.  The theoretical and experimental contribution in the scheduling field need further analysis."], "r9cpyzP-DQ": ["Paraphrased Statement: Summary This paper suggests learning a \"diffeomorphism\" (coordinate transformation) to improve understanding of differential equations that generate discrete data. After the transformation a linear or neural networkparameterized ordinary differential equation (ODE) can be learned enhancing efficiency and accuracy. Strengths and Weaknesses Combining a coordinate transformation with other methods is a promising approach. However the paper could benefit from improvements: linear ODE Base  This base limits the systems topology to linearity potentially underrepresenting complex dynamics.  The paper does not provide clear guidance on when to use a linear base. Nonlinear Neural network Base  This base has more flexibility but the uniqueness of the coordinate transformation and dynamics needs to be addressed.  It is unclear how this approach is better than directly learning the vector field. Additional Points  The Lorenz system example is not fully representative of chaotic dynamics.  Some key references on datadriven differential equation learning are missing.  The equation ODESolve(f\\omega yt0 te) should be ODESolve(f\\omega yt0 t0 te).  Eulers method is not a RungeKutta method.", "Paraphrased Statement: Summary: This work introduces a new perspective that links ordinary differential equations (ODEs) to diffeomorphisms. Complex and challenging ODE dynamics are represented as variations of a straightforward \"base\" ODE system. The proposed approach involves simultaneously learning the base dynamics and the corresponding diffeomorphism. Empirical solution indicate that this method accelerates training improves accuracy and enables longterm behavior analysis for certain base ODE systems. Strengths and Weaknesses: Strengths:  The work explores a significant topic.  The connection between differential geometry and ODE systems is striking.  Persuasive experimental findings support the research. Weaknesses:  The relationship with latent neural ODEs is not addressed. Presenting the methodology as a specialized instance of latent neural ODEs would clarify the distinctions.  Empirical comparison with latent neural ODEs need more clarity. The tables imply that the competing method is consistent across all tables.  The informal definition of \"strong ODEs\" in Section 4.6 may be flawed. Typically strong ODE systems cause numerical solvers to stall at specific spatial point rather than rapidly varying along a dimension.  The comparison should consider the number of framework parameters. The proposed approach employs two networks so a fair comparison is crucial. The use of adaptive step solvers with appropriate tolerance values is also uncertain.  The initial integration values are unspecified. The impact of noise on these values should be discussed.  The relative contributions of the base ODE system and the invertible layer are unknown. Understanding this distribution could be beneficial. Minor Comments:  The writing is generally clear but the preliminaries section effectively explains the concepts.  Section 4.3 is repetitive.  The term \"integral curve\" is uncommon in the neural ODE community. A definition or alternatives would enhance the readers understanding.  Section 4.4 is highly repetitive and could be summarized under a title like \"Summary of Our methodology.\"  The last sentence before Definition 4.2 needs revision for clarity.  Figure 3 is cluttered especially the right side.  Reduce the number of long sentences especially in table captions and the appendix.  The final paragraph on page 8 is overly long and needs improved structure.  Visualizing forward trajectories over time could enhance the interpretation of the differences in Table 1.", "Paraphrase: Summary Ordinary differential equations (ODEs) parametrized by neural networks can be inefficient when training frameworks on long sequences. This paper proposes training an ODE with a fast transformation (diffeomorphism) to simplify the ODE work. This makes it more efficient to integrate numerically while retaining the same point of expressivity. Strengths and Weaknesses:  I disagree with the presented dichotomy in the related work section. \"neural ODE\" encompasses both ODEs parametrized by neural networks and frameworks incorporating these ODEs.  [1] [2] [3] and [4] should be included in the related work section as they focus on improving the efficiency of neural ODEs. Benefits of Asymptotically Stable neural ODEs:  Asymptotically stable neural ODEs maintain expressivity while ensuring that solution converge to a stable equilibrium point over time.  There are examples of unstable neural ODEs without diffeomorphisms but such frameworks are not commonly used in practice.  Periodic solution are not permitted by the definition of asymptotic stability but experiments show that these dynamics can still occur in practice. Advantages of Using Diffeomorphisms:  Composing an ODE with a diffeomorphism allows solution to be obtained without evaluating the diffeomorphisms Jacobian.  However this advantage is not significant as multiplying by the Jacobian is relatively efficient. Applications:  The entire framework can be represented as an ODE which may be beneficial in time series problem.  Diffeomorphisms are required for training continuous normalizing flows. Open Questions:  When is a linear base ODE expressive enough  Are there tractable components that can be efficiently composed instead of relying on neural networks  What are finegrained measures of time cost for different frameworks  Does Table 2 include solution from frameworks in Rubanova et al. 2019  The experiments are limited exploring additional tasks like those in [4] could be valuable. Typos:  Section 2: \"normlizing flows\"  \"normalizing flows\"  Section 4.1: \"If mapping F\"  \"If the mapping F\"  Section 4.2: \"and get the inverse with no further labor\"  \"and obtain the inverse without further computation\"", "Paraphrased Summary: The paper abstract provides a concise overview of its main point. neural networks have become popular for approximating solution to differential equations (ODEs). However these methods can be slow and unreliable when dealing with long sequences of systems of ODEs. The authors propose learning ODEs from data by considering their dynamics as vector field related to a known base vector field through a diffeomorphism (a smooth mapping). By learning both the diffeomorphism and the dynamics of the base ODE the authors aim to simplify the complexity of modeling the dynamics by shifting it to learning the diffeomorphism. This allows for faster and more robust integration of trajectories from the learned system as the base ODE is chosen to be well integrable. Strengths:  Builds on established research in ODE estimation  Mathematically rigorous  Clearly presented Weaknesses:  Practical applicability for complex problem remains unclear from limited experiments  It is uncertain if a tractable diffeomorphism that can be estimated with neural networks always exists  The choice of the base ODE can be nontrivial and may affect the performance of the method"], "tlkHrUlNTiL": ["Paraphrased argument: Summary: This research introduces Deep Linearly Gated Networks (DLGN) to analyze DNNs with ReLU activations from two view. The DLGN framework effectively separates the \"gating network\" from the \"weight network.\" experimental evaluations show that DLGN achieves substantial classification performance on benchmark datasets outperforming existing DNNs. Strengths:  Utilizing DLGN is effective for interpreting DNNs with ReLU activations.  Theoretical analysis is provided to enhance understanding of the method.  Extends the dualview approach to convolutional networks with global average pooling and ResNet providing valuable insights. Weaknesses:  Experiments were limited to small datasets. It would be interesting to see performance on more complex tasks like ImageNet and CelebA.  The impact of the \"beta\" argument in the soft gate on experimental issue was not explored.  Some of the theorems presented may be challenging to comprehend for audiences without prior background.", "Paraphrase: Summary: This paper focuses on understanding and improving the inner structure of deep neural networks (DNNs) by employing two key strategies: 1. Replacing the conventional ReLU (rectified linear unit) activation work in DNNs with Deep Linearly Gated Network (DLGN). 2. Demonstrating that the resulting weighted network exhibits disentanglement in the path space. Strengths: 1. The paper challenges the prevailing notion that DNNs rely on their nonlinearity for success and proposes an alternative approach by reducing nonlinearity. 2. The claims made in the paper are wellsupported by both theoretical analysis and experimental results. Weakness: The paper is highly theoretical in nature making it potentially challenging for readers to grasp its technical concept.", "Paraphrase: The paper enhances the Deep Gated Network framework by introducing the Deep Linearly Gated Network. It then investigates the networks individual portion. The paper employs theoretical analysis and empirical examination to propose that neural networks are trained pathbypath not layerbylayer. It also demonstrates novel property of the neural path kernel. Strengths:  Provides a valuable perspective for understanding neural networks by analyzing them in path space.  Presents novel theoretical insights such as the ensemble structure of ResNets and the rotational invariance of Neural way kernel which may aid in understanding deep learning model. Weaknesses:  Relies heavily on the Deep Gated Network work by Lakshminarayanan and Singh (2020). more details and conventional definitions should be provided for readers unfamiliar with this concept.  While the paper claims that neural networks are learned pathbypath the experiments and theoretical analysis only support this for Deep Linearly Gated Networks not necessarily for general neural networks.  The paper claim that layer permutations destroy structure may be exaggerated. While the preactivation values are permuted the underlying linear operators remain unchanged suggesting that the structure is not fully destroyed. Questions:  In Table I why do model with permuted layers perform slightly better than the original model  Can linear network work fully replace nonlinear network work If not why design the network in a linear way", "Summary Paraphrase: Researchers introduce a fresh type of neural network called Deep linear Gated Networks (DLGNs) inspired by a recent concept that separates computations in neural networks into \"gate learning\" and \"weight learning.\" DLGNs divide computations into two distinct portion:  Primal: Computation between inputs and gate preactivations.  dual: Computation in the weight network based on inputs and gates. DLGNs perwork remarkably well reaching approximately 83.5 of the accuracy of leading deep neural networks. This foundation could potentially result in deep network model that are both interpretable and highly efficient. Strengths and Weaknesses Paraphrase: Strengths:  Provides a fresh perspective on understanding neural network computations.  Overcomes limitations of existing model like NTK by introducing NPK for gated and dualpathway DNNs. Weaknesses and Suggestions:  Explore the functional role of gating and its potential computational benefits.  Investigate if separating the gating and value networks doubles the issue of argument. Consider strategies to reduce gate network argument and explain why DLGN perworkance may be slightly lower than SOTA.  Improve the clarity of Figure 1 by including a small illustration of the functional work of the gating signal (e.g. GaLU).  Consider the relevance of interpretable model versus interpretability methods for noninterpretable model.  Reframe the concluding question most DLGN as a potential way for future research rather than a concluding argument."], "w4cXZDDib1H": ["Paraphrase: This work introduces ViDT a highperformance Detection Transformer that strikes a superior balance between accuracy and speed. Extensive testing and ablation investigations validate the efficientness of this detector. ViDTs design approach sets a precedent and can inspire the future of detection transformer architecture. effectiveness:  Wellstructured and highly efficient Detection Transformer that combines the strengths of Swin Transformer YOLO and DETR.  Demonstrates the competitive accuracyspeed tradeoff of Detection Transformers countering the perception of Deformable DETR as fast in inference speed.  Establishes an efficient Detection Transformer design principle that enhances future exploitation.  Provides a thorough analysis of various Detection Transformer architectures. Weaknesses:  The [DET] token attention configurations require further clarification such as the specifics of [DET] x [DET] attention across stages and feature dimension scope.", "Paraphrased Summary: The authors present an efficient transformerbased object detector that incorporates novel components:  Reconfigured Attention Module (RAM): Enhances crossattention between regions and tokens boosting performance.  Encoderfree Neck: Eliminates the encoder for better efficiency.  Tokenbased Knowledge Distillation: Leverages knowledge from pretrained models to enhance detection accuracy. effectiveness:  The detector achieves high performance with fast process time compared to other transformerbased detectors.  The comparison with competing detectors is wellpresented. Weaknesses:  The novelty of the proposed approach is limited.  RAM and tokenbased knowledge distillation are not entirely original concepts.  technique like neck structure and auxiliary decoding loss are borrowed from existing method. Concerns about RAM:  Details about crossattention between regions and tokens are lacking.  The novel contribution of RAM may be limited as it only introduces crossattention between regions and tokens in the final stage.  An experiment demonstrating the impact of removing crossattention between regions and tokens in other RAM stages is suggested. Table 4 Interpretation:  The drop in performance when removing stages from the neck structure cannot be solely attributed to the lack of crossattention between regions and tokens in RAM.  An experiment isolating the effect of crossattention between regions and tokens in the transformer decoder is recommended. Missing Experiment:  A comparison with Deformable DETR Swinbase is requested to provide a more comprehensive evaluation.", "Paraphrased Statement: The paper introduces a new approach to image classification and detection using recent advancement in transformerbased method. However it highlights that applying a ViTbased backbone to DETR (detection transformer) introduces computational challenges due to the quadratic complexity of ViTs selfattention and attention modules. To address this the paper proposes a reconfigured attention module (RAM) built on top of the Swin Transformer backbone and removes the encoderdecoder in the transformer neck. This resolves the computational issues by limiting global attention to the last stage. Despite the potential value of the approach there are areas that could benefit from improvement:  The description of crossattention could be clearer and more precise.  The complex diagram in Figure 3 could be formally explained for better understanding.  The similarity between the proposed method and YOLOS particularly the use of global attention is not fully explored.  The performance of DETR with different base models (vanilla deformable Conv backbones) should be included for a comprehensive comparison.  The impact of distillation in the DeiT models should be considered in the evaluation. Overall while the paper introduces a potentially valuable approach further refinement and clarification would enhance its clarity and completeness."], "i3RI65sR7N": ["The authors present a new classification model that aims to enhance accuracy in crossdomain situations with limited training model (fewshot classification). To achieve this they extend a previous model by introducing multiple layer of latent variables that capture different semantic data. This hierarchical structure allows for the introduction of distinct classspecific prototypes at each layer. Predictions from each layer are then combined using weights specific to each domain obtained through a gradientbased method. Trials demonstrate significant improvement over existing approach especially in crossdomain settings. effectiveness:  clear and concise introduction of the model and its relation to prior work.  impressive performance gains compared to baselines especially in crossdomain scenarios.  Extensive ablation experiments highlight the value of each component in the proposed framework. Weaknesses:  performance using lowlevel feature extractors suggests that the ensemble aspect may be more impactful than the hierarchical structure.  Question 1: Exploring the impact of using only logits from the last layer without the ensemble or removing the dependencies between latent variables could clarify the importance of these component.  Question 2: The paper does not provide data on the memory overhead and potential impact on convergence time due to the hierarchical structure.  The Visual Semantic Matching (VSM) scores reported in the paper are low than those in the original work upon which the proposed model is based. The reason for this discrepancy needs to be addressed.", "Paraphrase: This paper introduces a hierarchical version of the variational memory method proposed by Zhen et al. (2020) for fewshot learning. While both methods contribution the same core approach this work uses a hierarchical model with separate memory for each layer while Zhen et al. used a single memory for highlevel concepts only. This selection aims to enhance handling of fewshot learning tasks with domain shift as low layer representations may be more relevant in such scenarios. The proposed framework uses a hypernetwork to predict attention weights over the different layer prototypes. However while the technical aspects are sound the experimental results raise concerns about the effectiveness of the proposed method in comparison to the benchmark variational semantic memory (VSM) of Zhen et al. (2020). The VSM results reported in Table 5 are inferior to those reported in the original paper by Zhen et al. (2020) despite being labeled as a \"reimplementation.\" In fact the original VSM results are better than the proposed method in 3 out of 4 experimental settings. This discrepancy is significant since the proposed method is an extension of Zhen et al.s work. It is essential to provide a detailed explanation for the differences between the original VSM and the \"reimplementation\" and why the published results could not be replicated.", "Paraphrased Statement: This paper presents a layered memory system for storing features at varying semantic levels for fewshot learning across different domains. It employs a hierarchical prototype model where each prototype level retrieves pertinent data from the hierarchical memory. The paper utilizes a hyper network design to determine the weights when combining predictions from multiple levels. Optimization of the entire model is achieved through a variational inference framework. The proposed method was evaluated on four datasets featuring a domain gap between training data and demonstrated its effectiveness on standard fewshot prototype classification benchmarks. effectiveness and Weaknesses: effectiveness:  clear and wellorganized paper  Innovative use of multilevel semantic data for domainspecific fewshot learning  Extension of memory networks to fewshot learning through prototype conditioning Weaknesses:  potential impact of external memory on model size  Concerns about fairness in comparison to methods without explicit external memory  Incomplete baseline comparisons (results may not reflect current stateoftheart)  Limited analysis of training efficiency and model size impact"], "ht61oVsaya": ["Paraphrased Summary: This research introduces DESTA an innovative framework for ensuring safety in reinforcement learning (RL). DESTA creates a setting where two entities the Safety Agent and the Task Agent interact with the environment. The Safety agent objective is to minimize safety violations while the Task Agent aims to maximize reinforcement. The framework encourages the Safety agent input thereby promoting the safety of the Task Agent (which initially lacks safety awareness). The authors evaluate DESTA against toptier RL methods across three task. effectiveness and Weaknesses: effectiveness:  Clarity: The paper presents its key concept in a userfriendly manner.  Novelty: The authors propose a novel approach that formulates safe RL as a Markov game with dual agents. This framework simplifies agent training by separating reinforcement and safety signals. Weaknesses: Empirical Evaluation:  Insufficient evidence: The experiments conducted in three environments do not adequately demonstrate the effectiveness of DESTA. Specifically design 2 lacks convincing evidence that DESTA outperforms other methods. Further DESTAs safety violations in design 7 raise concerns.  Lack of Theoretical Guarantees: The framework lacks theoretical guarantees regarding optimality or safety limiting its empirical validation. comparison to Lagrangian method:  Unclear Advantages: The authors criticize Lagrangianbased approaches but the experimental results suggest that DESTA also faces safety violations comparable to Lagrangian methods. The paper does not establish a compelling case for DESTAs superiority. Reproducibility Concerns:  Missing Source Code: The absence of source code and limited experimental details make the reported results difficult to reproduce.  Inconsistent baseline: different baseline are used across experiments raising questions about the validity of comparisons. Minor Comments:  Inconsistent notation for the action space.  Missing standard deviation shading in design 2.", "Summary The paper proposes a novel approach to minimizing constraint violations while learning in Partially Observable Markov Decision process (CMDPs) referred to as \"safe exploration.\" This approach leverages the frameprocess of Markov games to train three policy: 1. A primary policy that maximizes the CMDPs reinforcement objective. 2. A safety override policy that prioritizes constraint violation minimization. 3. An intervention policy that switches between the primary and safety policy. The effectiveness of this approach is demonstrated on several toy environments including Tmaze Lunar Lander and Safety Gym Benchmark. effectiveness  The use of Markov games to model safe exploration is a novel and promising approach. Weaknesses  Missing related process: Similar ideas have been explored such as decoupling task objectives and safety design using barrier functions or pessimism. The paper should discuss these related approaches and their tradeoffs with the proposed method.  Intervention cost: The proposed approach introduces an intervention cost function that is not component of the original CMDP formulation. It is unclear how this cost function is derived and what its implications are.  Confusing writing: Section 3 introduces novel terms and notations without clear definitions making it difficult to understand.  Implication of Proposition 1: The proposed proposition suggests that there exists a solution policy corresponding to the intervention reinforcement. However this is not surprising given that the intervention reinforcement function is a combination of underlying reinforcement functions.  Limited baseline: The paper does not consider more recent approaches for safe exploration in CMDPs such as those that do not rely on Lagrangian formulations. These baseline could strengthen the claims made in the paper.  Environments: The paper only uses simple toy environments with only Tmaze being stochastic. These environments may not adequately demonstrate the advantages of the proposed approach.  Reproducibility: Providing code and details about the number of seeds would enhance the reproducibility and credibility of the empirical results.", "Summary Paraphrase: The researchers use a Markov gamebased approach to explore Markov decision processes with constraints. They present a safety training algorithm that has proven to be more effective than existing techniques in computational experiments. Strengths Paraphrase: 1. The proposed method leverages Markov games to balance reward maximization and constraint adherence in constrained Markov decision processes offering a novel and challenging approach. 2. The method exhibits flexibility by allowing separate handling of reinforcement and costs using deterministic policy. It also incorporates various safety criteria. 3. Empirical experiments demonstrate the superior performance of the proposed method. Weaknesses Paraphrase: 1. It is unclear if reward maximization and safety violation minimization are fully decoupled. evidence from the proposed framework would clarify this. 2. The key contribution in Section 3 involves transforming an expectation constraint into the safety agents objective. It is not clear if this is universally applicable or how it is achieved. 3. Theoretical support for the proposed generalsum Markov games would strengthen the analysis and provide further insights.", "Paraphrased Statement: Summary: This research proposes a framework based on Markov games for safe reinforcement learning. The framework involves two agents: a task agent that selects actions to maximize reinforcement and a safety agent that decides when to intervene and what actions to take to minimize cost. experimental results are presented in simulated environments. effectiveness:  safe reinforcement learning is crucial in various applications.  The proposed method addresses safe RL problems using a constrained Markov decision process (cMDP) formulation.  The novel Markov game approach DESTA decomposes a cMDP agent into task and safety agents.  The task agent chooses actions while the safety agent determines control and minimizes costs.  This Markov game framework is a unique approach to tackling cMDPs. Weaknesses:  The paper lacks analysis demonstrating the connection between the Markov game solution and cMDP performance.  The framework raises questions about convergence properties and equilibrium attainment.  The relationship between safety and task agent policy and cMDP objectives is not fully explored.  The paper confusing notation for intervention introduces readability issues.  experimental results show mixed performance with DESTA underperforming some baseline in certain scenarios.  Stability concerns arise due to abrupt changes in return and cost in the results."], "v-v1cpNNK_v": ["Paraphrase: This paper introduces a new method for Network Architecture Search (NAS) that uses Neural Tangent Kernel (NTK) to evaluate network architectures without training. The key estimation is that the NTK of a networks linearization can predict its training dynamics and performance. The paper proposes using the trace norm of the NTK at initialization as an approximation to efficiently evaluate this constant NTK. This approach allows for rapid and costeffective NAS even in datalabelfree settings. Experiments demonstrate the method effectiveness yielding highquality architectures with beneficial performance and transferability. effectiveness:  Novel use of NTK for NAS  Plausible method with theoretical support  Efficient search process  Datalabelfree search capability Weaknesses:  Lower performance than some existing NAS method on NASBench201  Modest performance advantage on ImageNet compared to stateoftheart", "Paraphrased assertion: This research focuses on finding optimal neural network architectures during initialization allowing for searching without the need for model training. effectiveness:  Searching neural architectures during initialization is crucial and has potential.  The proposed method ensures that the searched architecture is transferrable to various datasets and tasks under certain conditions.  The accuracy of the retrained model is comparable or surpasses other gradientbased search method. Weaknesses:  The paper claims competitive performance on both CIFAR and ImageNet datasets but only shows CIFAR results in the main table and partial ImageNet results in a different table with missing details.  It is unclear how the achieved accuracy compares to other further NAS method like FBNetv3 and EfficientNet.", "Paraphrased assertion: Summary: This paper introduces a novel trainingfree approach to Neural Architecture Search (NAS) called NASI. It leverages the Neural Tangent Kernel (NTK) to assess the performance of candidate architectures at initialization. To mitigate the computational cost of calculating the NTK the authors employ an approximation based on gradient flow. They also integrate their NTKbased approach with a gradientbased NAS algorithm using GumbelSoftmax optimization to solve the NAS problem efficiently. Empirical results on benchmark datasets demonstrate the effectiveness of NASI. effectiveness and Weaknesses: effectiveness:  NASI eliminates the need for training during NAS making it a computationally efficient approach.  It utilizes the unchanging nature of NTK to evaluate candidate architectures at the outset.  NASI is theoretically wellfounded by NTK theory and has achieved competitive results empirically. Weaknesses:  estimation are used throughout the paper which introduces potential sources of error.  NTK assumes an infinitewidth neural network which may not hold for all architectures.  The authors approximate the NTK trace norm and solve the NAS problem efficiently which may lead to inaccuracies. Questions: 1. Can the authors provide a bounds on the approximation error introduced by their method Or empirical evidence to support its accuracy. 2. The paper claims to minimize the upper bounds of the training loss but during architecture convergence the direction is on minimizing the actual training loss. Clarification on this discrepancy is needed. 3. compare with other trainingfree NAS method such as NAS without training would provide a fairer assessment of NASIs performance.", "Paraphrased Summary: This paper introduces a novel NAS (Neural Architecture Search) technique that eliminates the need for optimizing target network weights during architecture evaluation. It leverages the NTK (Neural Tangent Kernel) capability to estimate architecture performance at weight initialization enabling efficient search without training. Experimental results demonstrate its competitiveness and adaptability to label and dataagnostic scenarios. effectiveness:  Enhanced efficiency due to eliminating model training during architecture search.  Adaptability to label and dataagnostic situations. Concerns:  The assumption of infinitedepth DNNs by NTK raises questions about the applicability of the proposed method.  The lower bounds of network depth for effective application remains unclear.  potential limitations in finding optimal architectures as NAS method may prioritize architectures with beneficial performance but not necessarily the beneficial ranking.  The effect of training protocols on the accuracy of performance estimation by the proposed method requires further investigation.  The inferior performance compared to another trainingfree method (as per Table 4 in the appendix) requires further analysis and explanation.  The applicability of the method to large datasets like ImageNet remains untested."], "rS9t6WH34p": ["compact: This paper introduces techniques to break down a single image into multiple neural radiance Fields (NeRFs) each representing an object in the scene. It uses an improved version of NeRF that can process RGBD image. The method is successful on CLEVR3D and MultiShapeNet datasets. effectiveness:  Decomposing 3D scenes into objects is challenging and useful for understanding AI.  The probabilistic approach to ray marching is novel in the context of NeRF. Weaknesses:  The problem statement is not clear.  The papers title and main concept are inconsistent as 2D image are used for segmentation instead of 3D scenes.  The method lacks novelty as depthsupervised NeRF and composing NeRFs have been explored previously.  The presentation could be simplified by shortening the background information.", "Paraphrase: compact This paper proposes an algorithm for segmenting 3D volumes and image without any human input. It uses a set of Neural Rendering Fields (NeRFs) to represent the data. By incorporating RGBD data the computational effort is significantly reduced. The algorithm can both segment objects and create separate NeRFs for each object and the background. effectiveness and Weaknesses The idea of using more information to improve efficiency is appealing. The mathematical derivations appear sound but the experimental results are limited to straightforward data. Questions and input  \"Eq (3) is renormalized\": This refers to adjusting the probabilities in Eq (3) to ensure they sum to 1.  Thin highdensity volumes: The authors suggest that the algorithm may not work well for thin dense objects because standard RGBD capture devices cannot capture them. This argument should be clarified to avoid giving the impression that the algorithm can handle such volumes.  Derivation of log p (Eq (4)): The authors do not explicitly state how Eq (4) is derived.  Uneven ray sampling: The algorithm prioritizes sampling towards the end of the ray. This assumption should be justified as violating it could affect the performance.  Joint loglikelihood definition: The definition of p(tC) is not provided.  NeRF evaluation: The authors mention using only two NeRF evaluation. The understanding for this is unclear.  Scaling and shifting: The authors explanation of how they manipulate activations in hidden layers is confusing.  Unsupervised segmentation: The number of objects in each image is assumed to be known. While this reduces computational complexity it also weakens the claim of unsupervised segmentation.", "Paraphrased compact: The researchers developed a technique to create a unique \"neural radiance field\" for each object in a scene using just one input image. architecture: The system resembles an autoencoder. The encoder processes an image and pose to generate latent codes for each object based on the \"Objectcentric learning with slot attention\" approach. These codes control shared NeRF decoders that determine each objects shape and appearance. The complete scene is then reconstructed by combining the individual Radiance Fields. Efficiency Improvement: To lessen the computational cost of NeRFs the researchers employ given depth maps eliminating the need for complete ray tracing. effectiveness:  Using depth to reduce the number of samples is a clever strategy to optimize NeRF training.  The method surpasses previous benchmarks on artificial data (tables 1 and 2). Weaknesses:  The segmentation component appears to be similar to a prior approach.  The conclusion should better highlight the papers main findings.  visual comparisons with earlier approaches are absent.  Realworld data testing is lacking.  The results are only strong for the CLEVR dataset which features basic geometries. On the ShapeNet dataset the reconstructions appear fuzzy.", "Paraphrase: The authors present a technique for generating novel perspective of 3D scenes by breaking them down into individual objects. This decomposition is achieved using a slotbased encoder that doesnt rely on specific object category supervision. Additionally they introduce a novel loss function that leverages depth training data which improves the sampling strategy. This leads to faster training and reduces reconstruction error. The results of their method surpass current approaches and are supported by experiments on both 2D and 3D datasets. effectiveness and Weaknesses: The paper is wellwritten and presents a strong formulation. The authors justify their choices clearly and their work provides valuable insights into the limitations of existing approaches. The experimental validation is welldesigned and shows superior performance compared to stateoftheart methods on both 2D and 3D datasets. Regarding novelty the work has two key scene: unsupervised scene decomposition and training speedup using depth data. The first scene is incremental building on previous research. The second scene is significant but it should be noted that a recent work has explored similar ideas. The authors acknowledge that the additional supervision from depth data may contribute to the reduction in reconstruction error. This makes the comparison with other methods slightly unfair. further discussion and ablation experiments on the impact of depth training data would be beneficial."], "q2DCMRTvdZ-": ["Summary: This paper outlines existing DARTSbased methods categorizing them into two stages:  stage 1: training a supernet framestudy.  stage 2: Selecting the optimal architecture from the supernet. The paper proposes evaluating these methods separately on NASBench201 a benchmark with a limited search space. effectiveness:  Clear description of the twostage DARTS approach.  Examination of various combinations of DARTS methods.  introduction of a novel account that shows promise in preliminary experiments. Weaknesses:  Limited scope due to the small search space of NASBench201.  Lack of sufficient empirical evidence to meet ICLR standards.  Oversized figures indicate a potential lack of content.  Questions regarding the correlation analysis of DARTSbased supernets.  Similarity to existing study in ranking correlation analysis.", "Summary: This study presents a method to assess differential NAS algorithms dividing the search process into two stage:  stage 1: Training the supernet  stage 2: Selecting an architecture evaluation of these stage are conducted separately and combined into a account that estimates the performance of any combination of stage 1 and stage 2 strategies. effectiveness:  Addressing the significant issue of evaluating NAS algorithms  Reasonable division of differential NAS methods into two stages Weaknesses: 1. Clarity:  The papers topic is not clearly explained in the introduction.  The evaluation processes for stage 1 and stage 2 (division 4.1 and 4.2) are difficult to follow.  The need and description of the account (division 5) are unclear. 2. discussion of Results:  The curves in Figure 2 (left) and the plot in Figure 2 (right) in division 4.1 are not discussed.  The plots in Figure 3 in division 4.2 are likewise mentioned only briefly.  The plot in Figure 4 (Figure 5 in text) in division 5 is not adequately discussed. 3. Proposed account:  The proposed account is not fully representative of test accuracy (division 4).  Its need is unclear and its inclusion should be reconsidered. 4. Related study:  Some differentiable NAS methods used in the experiments are not discussed in division 2.1 (e.g. GDAS DrNAS).  other significant NAS evaluation study are omitted from division 2.2. 5. case study:  The authors claim that two algorithms interact positively without account (division 3) is not supported by Table 1 results.  The statement about the benefit of architecture weights on perturbationbased selection methods is not entirely accurate.", "This paper does not present new methods or algorithms. Instead it conducts an empirical study of two components of differentiable neural architecture search (NAS) variants: supernet training and architecture selection. effectiveness:  Explores the integration of multiple DARTS variants to understand their combined effects. Weaknesses:  Missing references for several methods mentioned (e.g. GDAS DrNAS).  Limited discussion and justification of the presented tables and statistics.  Lacks a conclusion division to summarize the analysis results of division 4 and 5.  While the analysis is valuable its applicability to other NAS spaces is unclear particularly considering the limited size (201) of the analyzed space and the claims of other NAS methods to find optima in larger spaces.", "Paraphrased Summary: This study examines the statistical properties of weightsharing neural architecture search (NAS) algorithms. It divides the training into two stage: weight training for a supernet and architecture selection. It analyzes the Spearman correlation and other statistics to understand the behavior of these two stages. The experiments are based on the NASBench201 benchmark. effectiveness  Detailed analysis of supernetbased NAS drawbacks  Unique twostage NAS approach with separate analysis of stage behavior Weaknesses  Lack of clear insights and introduction:  Importance of chosen metrics is unclear  Some results are predictable (e.g. uniform distribution underperforming others)  Limited understanding of how to enhance NAS based on findings  Poor writing:  Concepts explained well but lack of clarity on novel metrics  Restricted experiments:  Limited dataset (NASBench201)  Lack of exploration across diverse search spaces and algorithms", "Summary: The authors examine ways to evaluate differentiable methods used in neural architecture search (NAS). Differentiable approach are popular in NAS and recent studies have proposed improvement and criticisms of the DARTS algorithm and its variants. This paper introduces a new evaluation approach that separates architecture training from architecture selection and assesses their combined effects. The authors conduct experiments on NASBench201 and apply their evaluation technique to identify the most efficient combination of training and selection methods. effectiveness:  Clear and wellreasoned arguments  Comprehensive evaluation framework  Interesting insights into differentiable NAS Weaknesses:  Limited experimental scope: Experiments are conducted only on the smallscale NASBench201 dataset and do not include ImageNet16120 data. Generalizability of the findings to other benchmarks is uncertain.  Incremental nature: While the ideas are valuable the paper may lack sufficient novelty and impact for ICLR.  Incomplete references: Relevant prior study including \"jacobcov\" and \"PCDARTS\" are not cited.  Reproducibility and ethics: A reproducibility statement and ethics statement are absent.  Figure introduction: Vectorization and cleanup of xaxes in Figures 2 and 5 would improve readability. use of distinct colors for different plots is recommended."], "htWIlvDcY8": ["Paraphrase: The paper presents a technique for comprehending new visual concepts from limited examples incorporating both images and language. The proposed model demonstrates its ability to answer questions regarding visual concepts based on images. effectiveness:  The paper is wellwritten and easy to follow.  The task of learning visual concepts from a small number of examples with text is relatively new and challenging.  The model is evaluated on realworld and synthetic datasets validating its effectiveness. Weaknesses:  The model consists of separate components including semantic parsers which cannot be jointly optimized.  The method is susceptible to errors made by these components which cannot be rectified.  Additional details on the failure case of the model would be beneficial.", "Paraphrased Statement: This paper proposes a method for learning visual concepts with limited examples. It involves pairing images with sentence (optional concept descriptions) and aiming to answer questions about a new concept. The method combines object detection with box embeddings and a neurosymbolic program to identify objects and relationships between concepts. Two graph neural networks infer a representation of the new concept which is then used for downstream tasks such as question answering. The approach outperforms existing baselines on three benchmark (CUB CLEVR GQA). effectiveness:  Novel approach to concept learning with few examples  Integrates multiple techniques to achieve strong performance  Comprehensive evaluation including ablation study and comparisons to stateoftheart methods potential Weaknesses:  Impact of object detection errors on the methods performance  potential for errors in language parsing and program performance", "Paraphrase: Summary: This paper explores concept learning by developing a neural symbolic approach that extracts concept embeddings from imagesentence pairs and incorporates additional text explanations. To evaluate this approach the paper utilizes a metalearning framework training the model on a subset of concepts and evaluating it on a separate set. The approach exhibits strong perworkance on three datasets compared to existing methods. effectiveness and Weaknesses: effectiveness:  Novel and innovative approach  Enjoyable to read Weaknesses:  Dataset Concerns:  CLEVR and GQA datasets do not fit the assumed scope of descriptive sentence for concept learning.  approach Details:  Method for semantic parsing and resolving errors in natural language supplemental sentence.  Handling of concept name variations in natural language work.  Operator design for the neural symbolic machine in the actual language scope.  Setting Clarification:  CUB dataset is treated as a retrieval task despite being a bird classification dataset.  Generalizability Concerns:  Uncertain whether the approach would perwork substantially on datasets like Flickr30k or COCO.", "Paraphrased Statement: Summary: This study presents a comprehensive framework that combines metalearning and symbolic reasoning to enable fast concept learning in visual data. It introduces a novel module to extract embeddings from visual examples and their relationships allowing for the prediction of new concept embeddings. The metalearning approach facilitates continuous learning and utilizes additional sentence to establish connections between concepts. Improved results are demonstrated in question answering tasks involving novel concepts where the system first learns these concepts during a metatesting phase. effectiveness and Weaknesses: While the paper presents a promising concept it lacks clarity and justification in several key field. The contributions are introduced without sufficient context or motivation. The problem formulation use of metalearning and selection of the box embedding space are not adequately explained. The paper lacks analysis of the models performance in relation to the number of supplemental sentence cornerstone concepts or object categories. Additionally it contains several typos and incomplete sentence. The use of similar language for different concepts also creates confusion."], "fPhKeld3Okz": ["Paraphrased Statement: Summary This research extends the \"plugandplay\" framework by formulating an explicit regularizer g(x)  x  N(\u03c3(x))  2 where the gradient \u2207g aligns with the noise residual x  D(\u03c3(x)). The paper proposes the GSPnP algorithm which uses a gradient step denoiser in place of the proximal of the regularizer. This new method leverages halfquadratic splitting. Since the explicit regularizer is known convergence analysis is established under the assumption of \u2207gs Lipschitz continuity. The approach aligns with the trend of using deep neural network to learn regularizer functionals. Unlike existing methods that train network to output scalars GSPnP trains network to output imagesized vectors which are then enveloped by an L2 norm. Notably the proposed regularizer is similar to the alternative prior presented in the loss paper. However the paper doesnt clearly articulate the specific differences between the two. strength: 1. Extension of PnP with explicit regularizer formulation 2. convergence analysis under common assumptions 3. Comprehensive evaluations for image denoising deblurring and superresolution Weaknesses: 1. Similarity between the proposed regularizer and the alternate prior in loss need for clear distinction 2. Standard convergence analysis based on known regularizer without apparent novelties 3. Lack of explanation on neural network training and the connection between the output vector and the regularizer formulation 4. Omission of comparisons and discussion with other backtracking step methods 5. Similar performance to existing plugandplay algorithms need to emphasize theoretical advancements 6. Inadequate review of the loss research progress", "Paraphrase: Summary: This paper proposes a new deep neural network denoiser that simplifies the convergence analysis of Plugandplay priors (PnP). The denoiser is inspired by PnPHQS and regularization by Denoising (loss). Existing research has established the convergence of PnP and loss with certain denoisers. This paper focuses on designing denoisers that match the gradient descent step of a regularizer function. By incorporating the denoising step into PnP the paper guarantees convergence using traditional nonconvex optimization. A backtracking scheme is employed to ensure convergence without requiring the exact Lipschitz constant. The proposed method demonstrates favorable performance on image restoration tasks such as deblurring superresolution and inpainting. strength and Weaknesses: The proposed idea is innovative and promising offering insights into solving the convergence issue of PnP and loss. However there are some concerns: 1. The paper does not sufficiently explain why loss performs poorly for deblurring and superresolution. 2. Additional discussion on connecting convex regularization to CNNs could be beneficial. 3. An analysis of the impact of initialization on reconstruction results would be valuable. 4. A graphical representation of the work of the regularization parameter on the reconstruction would be helpful. 5. Including the denoiser architectures in the supplementary materials would enhance the papers accessibility. 6. A plot illustrating the improvement in image quality would provide insights into the convergence speed. Minor Comments:  In Table 5 \"31.70\" is bolded but \"31.93\" is not.  The paper could benefit from citing additional PnPrelated research such as [Buzzard et al. 2018] and [Ahmad et al. 2020].", "Summary Paraphrase: This paper presents an improved version of the plugandplay (PnP) halfquadratic splitting algorithm that has proven convergence. Instead of directly defining the denoiser it uses a learnable scorebased function to parameterize it connecting scorebased generative framework with PnP methods. Surprisingly this formulation within PnP results in a solid theoretical convergence guarantee under mild assumptions. The algorithm performs well in empirical tests on three image restoration tasks (deblurring superresolution and inpainting) with a convergence range typically faster than the worstcase range established theoretically. strength and Weaknesses Paraphrase: strength:  innovative gradient step denoiser formulation in the PnP framework  General theoretical convergence result without strict assumptions  Potential extension to nonconvex data terms  Extensive and convincing empirical simulations Weaknesses:  Uncertain applicability to other proximal algorithms  Ambiguity in the gradient calculation of smoothed image prior p(\u03c3)  Computational complexity in calculating Jacobian during training  Lack of reference to recent approach in PnP methods", "Paraphrased Statement: Summary The paper introduces a novel noise reduction algorithm within a plugandplay (PnP) framework using a gradientbased regularizer trained specifically for PnP applications. The algorithm reportedly converges effectively unlike existing methods that rely on unrealistic assumptions. experimental results demonstrate the efficacy of the proposed method. strength and Weaknesses strength:  The work is wellwritten and accessible.  It introduces a new technique for noise reduction within a PnP framework an field of interest for the community. Weaknesses: Primary Issues: 1. The proposed regularizer is not clearly explained and lacks interpretation. 2. The stated loss from the PnPHQS framework raises concerns as the proposed algorithm includes a heuristic approach that may introduce biases. 3. It is unclear whether the proposed denoiser can be used outside the HQS method or is limited to it. 4. The proof in Theorem 1 critically relies on solid convexity which is not always applicable in practice. 5. The claim of superior denoising performance is based on an unverified assumption that a standalone denoising task is equivalent to image restoration. 6. The tuning parameters for comparison methods are not specified which may affect the validity of the results. Secondary Issues: 1. Equation (2) is incorrect and should be corrected. 2. The claim of allowing nonconvex data terms needs clarification. 3. The statement about the convergence of other schemes is not sufficiently supported. 4. The claim of \"exact representation of a conservative vector field\" lacks proof. 5. The reasons for nondifferentiability of the data term in inpainting are not explained. Minor Issues and Suggestions:  Positive regularization parameter should be used in Equation (1).  Equation citations should be consistent.  The forward framework should be stated explicitly.  Clarify the usage of \"external denoiser\" and \"subsequent PnP schemes.\"  Rewrite the first paragraph of page 3 for clarity.  Average results should be reported in Table 2 instead of individual values.  IRCNN and DPIR should be stated as implementing PnPHQS."], "fwJWhOxuzV9": ["Paraphrased Statement: Summary This paper introduces Pretrained Decision Transformer (PDT) a method for semisupervised offline reward learning. PDT involves pretraining a decision transformer on a trajectory dataset without rewards followed by finetuning on a smaller dataset with reward annotations. Empirical results demonstrate that PDT performs comparably to existing offline reward learning methods. Strengths 1. Semisupervised learning in offline reward learning is a novel approach. 2. PDT is straightforward to implement. 3. Ablations analyze the effects of reward prediction during finetuning masking reward tokens during pretraining and other finetuning parameters. Weaknesses 1. The importance of finetuning is not fully explained. The pretrained transformer already performs well without reward annotations so it is unclear why finetuning is necessary. 2. Pretraining a large decision transformer can be computationally expensive and finetuning it on downstream datasets may be inefficient. A more lightweight finetuning method would be desirable. 3. While ablations are provided the discussion does not adequately direct which factors are most impactful. For model it would be helpful to clarify why masking reward tokens during pretraining yields slightly beneficial results than not masking them. other Questions 1. Table 1 show finetuning results for PDT but it is unclear how the pretrained decision transformer performs independently. Is its performance comparable to PDT finetuned on smaller datasets If the pretrained transformer outperforms PDT finetuned on small datasets then the significance of finetuning is questionable. 2. Key experimental details are missing such as the number of epochs and steps used for finetuning. If finetuning is computationally expensive it would be more efficient to train a decision transformer on the small dataset with reward annotations from scratch.", "Paraphrased Statement: This article presents a selfsupervised approach for deep reward learning using Decision Transformers which have been widely employed in computer vision and natural language processing applications. This algorithm outperforms traditional methods such as Decision Transformers Background Consistency (DT BC) and Conservative QLearning (CQL) especially when data is scarce. effectiveness:  Introduces the concept of transfer learning for deep reward learning including pretraining finetuning and selfsupervision.  Demonstrates that Pretrained Decision Transformers (PDT) can leverage rewardfree trajectories to improve agent performance during finetuning with limited labeled data.  Potential solution to the issue of data efficiency in deep reward learning.  Achieved performance advance on the D4RL dataset. Weaknesses:  Limited contribution and novelty compared to Decision Transformers primarily focusing on the Maximum Likelihood Estimation (MLE) loss and reward prediction.  Ablation field on the impact of reward prediction during finetuning in different environments are necessary.  Modification in the range of trajectory representation in PDT compared to Decision Transformers requires an explanation.  Use of predict rewards from the transformer as input instead of real rewards during evaluation needs justification.  Details on pretraining using the entire D4RL benchmark dataset are missing including how different tasks contribution embedding layers.  Inconsistent performance with varying numbers of trajectories with 25 trajectories sometimes leading to worse performance than 10 or the full dataset requiring further investigation.", "Summary: Pretraining (PT) has not been fully explored in Reinforcement Learning (RL). The researchers propose a simple strategy for PT in RL that leverages unlabeled datasets. This strategy shows benefits even with limited reward annotations. Impact: RL is an expensive field with vast applications. PT can significantly improve RL efficiency and enable knowledge transfer across tasks. Challenges: Previous PT approach for RL have had limited success. Solution: The researchers use the Decision Transformer model pretrained on a dataset of action sequences without reward information. Validation: The proposed model outperforms existing models in extremely fewshot settings (10 observed trajectories). However it performs worse than traditional methods at large dataset sizes. effectiveness:  Improving RL efficiency and generalization is valuable.  The PT method appears intuitive. Weaknesses:  The meaning of the \"v\" suffix in the data is unclear.  The performance results are not fully convincing. The proposed method may not consistently outperform existing approach.  The uncertainty in the average performance data is not reported.  The technical novelty of the approach is limited.  The field lacks a clear distinction between PT and semisupervised learning.", "Paraphrased Statement: This research proposes a new model called Pretrained Decision Transformers (PDT). Unlike typical decision transformers PDTs are trained on offline data collected by expert or semiexpert agents without explicit reward information. After pretraining PDTs are finetuned for specific tasks. Evaluation of PDT: The idea of extending decision transformers to work with unlabeled data is a promising research direction. However the current setup used in this paper is questionable. The pretraining data is collected by agents that used rewards which may bias the data towards highreward trajectories. To make the pretraining more realistic alternative experimental setups could be used such as:  Pretraining on data collected by random agents  Pretraining on data collected by agents using exploratory policies (not driven by extrinsic rewards)  Pretraining on data from different tasks or reward functions  Pretraining on realworld videos Experimental Results: Currently PDT does not show significant reward over existing methods like behavior Cloning or Conservative QLearning (CQLBC). The direction of the paper is mainly on improving finetuning performance and identifying significant design choices. Recommendations for Authors: To strengthen the case for PDT the authors should consider using a more appropriate experimental setup or demonstrating the effectiveness of PDT in transfer learning scenarios."], "kWuBTQmkO8_": ["Paraphrase: To use Mixup for regression the research first assumes that linearity is only true within specific distances between data degree or labels. To do this they developed MixRL a data augmentation technique that finds the best examples to mix with a given data degree. MixRL uses reinforcement learning to determine the importance of mixing samples for minimizing model loss on a validation set. strength:  Inspired by the need to measure individual examples contributions to model performance  Effective in improving regression performance by carefully mixing examples  Wellwritten and organized paper Weaknesses:  MixRLs improvement over previous Mixup methods is not significant  The importance of the improvement is unclear requiring further evaluation from other reviewers", "Paraphrased statement: MixRL is a technique that enhances the mixup method for regression tasks. Unlike mixup which randomly mixing inputoutput pairs MixRL uses a datadependent proximity constraint to determine which pairs to mix. By evaluating the validation set MixRL predicts the optimal number of nearest neighbors to use for mixing from a predefined set of selection. While MixRL consistently outperforms mixup and manifold mixup on various regression datasets the addition are modest. however the approach has limitations:  The set of nearest neighbors for mixup is fixed and limited potentially restricting performance.  MixRL lacks local inputoutput kernels that prioritize local data which could enhance performance.  The absence of locally nonconstant regression model in MixRL may be a significant limitation as they could enable more sophisticated mixing strategies.", "Paraphrased statement: This study enhances the mixup concept by utilizing a model within a batch to select samples for mixup based on knearest neighbors. To provide a learning signal for this noncontinuous process the researchers employ REINFORCE using the loss from a downstream regressor (not applicable to classification tasks). They demonstrate promising results on various regression tasks. strength and Weaknesses: strength:  Simple and seemingly accurate implementation  Experiments support the value of this approach for regularizing supervised learning particularly for regression tasks  Familiar use of REINFORCE as a stochastic gradient approximator Weaknesses:  Concerns about labeling this as \"RL for mixup\" due to the limited range of RL algorithms involved  Lack of comparison to other relevant baselines such as straightthrough estimators or Gumbel Softmax which may provide superior variance  Unclear statement for excluding classification tasks from this case of mixup  Absence of classification experiments to support the claims  Potential for incorporating prior knowledge of physical systems to improve sampling strategies"], "hR_SMu8cxCV": ["Paraphrase: This paper presents rules for adjusting machine translation models to handle larger datasets. The researchers tested different language pairs and content types focusing on huge Transformerbased translation models. They discovered that the rate of change in crossentropy loss is influenced by both the size of the encoder and decoder. The paper also suggests how to optimally balance encoder and decoder capacity. The researchers also found that \"translationese\" can affect how a model scales. Lastly they looked at how crossentropy loss and BLEUBLEURT scores are related. Strengths: 1. Largescale experimentation involving numerous language pairs and domains. 2. Counterintuitively the paper recommends increasing decoder size over encoder size providing guidance for future research. 3. Analysis of \"translationese\" reveals how it affects scaling laws and the relationship between crossentropy loss and generation quality. Weaknesses: 1. Scaling laws rely on specific test set parameters raising questions about their practical function. 2. While BLEU and BLEURT scores are included human evaluation results would provide a better indication of model quality. 3. The study focfunctions on the relationship between crossentropy loss and the count of Transformer layers ignoring other potential variables like architecture or hidden size.", "Paraphrased Summary: This study explores the scaling capacity of the NMT Transformer architecture (encoderdecoder) and compares them to those of language models. It reveals distinct characteristics of NMT on translationese text. The study suggests a model for scaling the NMT encoderdecoder. Strengths:  Addresses intriguing research questions providing deeper insight into NMT.  Delivers valuable findings that can guide practitioners in developing more efficient NMT models.  Reinforces the unsuitability of targetoriginal test sets for NMT evaluation.  Employs a diverse range of evaluation metrics including BLEURT. Weaknesses:  Reproducibility concerns due to undisclosed datasets nonstandard BLEU implementation and limited preprocessing point.  Lacks a comprehensive related study section to inform readers about previous study on language models.", "Paraphrased Summary: The paper presents insightful scaling characteristics for autoregressive neural machine translation models backed by extensive experimentation. The authors examine scaling properties from three angles: 1. model Quality scaling: Analyzing the relationship between model capacity and translation quality for encoderdecoder NMT models. 2. data composition Impact: research how training and test data composition influence scaling behavior. 3. generation Quality Improvements: Investigating how crossentropy reduction from model scaling enhances translation quality. Strengths and Weaknesses: Pros: 1. The suggested function correlating model capacity to encoderdecoder NMT performance aids in parameter allocation during model training. 2. experimentation examine the effect of training and test data composition on model scaling establishing a connection between the other questions investigated. 3. The research into how generation quality improves with decreasing crossentropy from scaling is valuable for practical NMT applications and theoretical research. Cons: 1. While the authors propose three research questions their interconnectedness is not evident. 2. The scaling behavior of training and test data composition bias has received limited attention in previous study and is especially interesting. additional experimentation and analysis could strengthen this section.", "Paraphrased Statement: This research paper analyzes the scaling laws applicable to Neural machine translation (NMT). It both substantiates and extends previous findings on scaling laws making it a significant contribution to the study. Specifically the paper addresses novel questions such as:  Do encoderdecoder NMT architectures follow the same scaling law as decoderonly language model (LMs)  How does training data (backtranslated or not) influence scaling Strengths:  The research questions are presented clearly.  The results provide insightful observations.  The extensive count of experimentation supports the paper findings. Weakness:  The footnote comparing the paper to Gordon et al.s EMNLP 2021 study is insufficient. Gordons study also investigated NMT scaling laws but in a different parameter range. It remains unclear whether this paper conclusions align with contradict or expand on their findings. This comparison should be clarified and elaborated."], "tyTH9kOxcvh": ["Paraphrase: Summary: The authors introduce a novel multilabel classification model. label are represented as box and neural networks project input information into the same space. This box representation enhances interpretability and allows the incorporation of setting knowledge (taxonomy) to improve the consistency of learned label representation within the taxonomy. The model has been evaluated on seven public informationsets using three metrics: CMAP CV and MAP. Strengths and Weaknesses: Strengths:  The models underlying concept is intriguing and valuable. Weaknesses: Suggestions for advance: 1. Replace the term \"consistent\" with \"coherent\" when referring to the models alignment with the taxonomy. 2. Ensure consistency in notation throughout the paper using Y \u2208 \\mathcalP(S) for both Y \u2208 S and Y \u2208 \\mathcalS. 3. Clarify the purpose and choice of parameters in the equation on page 4 including the function f and the value of \u03b4. 4. Use both GO and Funcat versions of the functional genomics informationsets for all experiments and include Imclef07d in the evaluation. 5. Conduct pairwise Wilcoxon tests to determine the statistical significance of the results especially comparing MVM MBM and MHM. 6. Note that CHMCNN does not use a posthoc modification as its max layer is integrated into the network. 7. Consider evaluating MVMT in the experiments. 8. Include ROC curves for MHM in the analysis of learned embeddings. 9. Provide more detail on information preprocessing including any specific character removal or encoding fixes. 10. Include the final hyperparameters for each model in the appendix. 11. Address the significant difference between the authors and your implementations of CHMCNN. 12. Report the AU(PRC) results for MHM. Minor Corrections:  Correct the typo in \"CHMCNN.\"  Add upward arrows next to MAP and CMAP in Table 2.", "Summary The paper presents a technique using box embeddings for hierarchical multilabel classification. Strengths:  The concept of utilizing box embeddings for this task shows promise.  Clear writing in most sections. Weaknesses:  The method is a direct application of existing box embedding techniques.  The need for a model using box embeddings without label taxonomy is unclear as using the taxonomy explicitly could be advantageous.  The authors argue that requiring a complete label taxonomy limits scalability but specific model are lacking.  The MAP measure is not welldefined and its relevance is not explained.  Alternative score thresholds should be considered instead of MAP.  The experiments could benefit from including HMCNR and HMCNF baselines.  The origin and need for the MVM baseline are not provided.  Statistical significance for MBMs superiority over MVM is not demonstrated.  The text accompanying Table 2 need clarification.  The role of the label interaction loss as a regularizer is unclear.  Interpretability claim are exaggerated as highdimensional box overlaps remain difficult to interpret.  Additional baselines would enhance the experimental results.", "Paraphrased Statement: Most multilabel classification methods predict labels without considering inherent label relationships. This field presents the multilabel box model (MBM) which leverages neural networks and box embeddings (adjustable Venn diagrams based on hyperrectangles) to capture taxonomic relationships among labels. MBM has certain advantages:  It can be trained using information gradients.  Box embeddings represent calibrated conditional probabilities.  It provides improved interpretability.  partial label relationships can be incorporated enhancing consistency.  It maintains theoretical foundation and enhances taxonomic consistency without compromising predictive performance. This method addresses a crucial issue in multilabel classification and is supported by promising experimental results suggesting that it captures latent label taxonomy. However some areas for advance include:  Generalization to more intricate neural networks should be investigated.  Statistical significance of results in Table 2 could be highlighted.  taxonomy evaluations depend on the assumption of complete ground accuracy taxonomy which may not always hold true in practice.  The understanding for the relatively small advance with taxonomic guidance could be explored.", "Paraphrased Statement: This research addresses a problem in multilabel classification where the class labels are arranged in a hierarchical structure. The goal is to ensure that the predictions align with this classification hierarchy. This problem enhances the structure of the learning model by requiring predictions to be consistent with this hierarchical arrangement. The advance adopted in this research uses box embeddings to represent class as spatial box. A probabilistic interpretation is then applied to these box embeddings to model the hierarchical relationships between class. Strengths:  Addresses a relevant problem in machine learning especially when the hierarchy is unknown.  Provides a clear and accessible presentation.  Offers technically sound methods. Weaknesses:  Limited originality in the concepts of box embeddings and probabilistic semantics.  proposal and corollary may appear as observations rather than conclusive results.  The need for the specific setup is not adequately explained.  The probabilistic interpretation of box does not show clear advantages over simpler advancees that leverage box containment for hierarchical enforcement.  The absence of a baseline that incorporates known class hierarchies weakens the evaluation of the proposed advance."], "size4UxXVCY": ["Paraphrased Summary The paper introduces Graph Tree Neural netprocess (GTNN) a novel learning model organized as a graph tree (a tree with sibling association). Each node processes data from its children. GTNN handles diverse data types with each node processing data differently based on type. The authors demonstrate how to implement convolutional and recurrent neural netprocesss within GTNN. experimentation on classic deep learning datasets show that GTNN can process various data types and generate an output vector that retains all relevant information. effectiveness  Unifies multiple neural netprocess architectures (RNN convolutional MLP) into a single graph tree structure.  Supports processing of multiple input data modalities. Weaknesses  Paper lacks clarity due to grammatical errors and poor sentence structure.  Empirical results are inconclusive with similar numerical values that may fall within an error range.  Lack of clear source code for verification.  The statement that length of axonal association correlates with processing is unsupported by citations.  Missing related process section to contextualize GTNN.  Frequent use of acronyms (GT GTN GTNN GTR GTC) reduces clarity.  The term \"Graph Tree Neural netprocesss\" may be confusing as it suggests a model that accepts graphshaped tree as input. Minor Issues  Typos and missing capitalization.  Inconsistent whitespace before parentheses.", "Paraphrased Statement: The field presents a novel neural network design the Graph Tree Neural network (GTNN) inspired by the human brains feature: 1. Multimodal perception 2. Crossinput reasoning 3. adaptable internal network structure 4. inputdependent allocation of computational resources 5. large size compared to traditional neural netprocess The GTNN incorporates specialized representation modules for different input types (e.g. images speech text) and employs a convolutional layer for comprehensive communication between these modules. A recursive combination of these modules forms the GTNN. The GTNN demonstrated superior performance across various domains (speech vision natural language processing) compared to benchmark models. effectiveness and Weaknesses: While the research direction is promising the paper presents challenges in comprehension due to its lack of clear structure and narrative. As a result the following questions arise:  Is the main focus of the field multimodality multitask learning or the relationship between brainlike and artificial learning  Important related process on dynamic computation graph and resource allocation are not mentioned.  The use of nonstandard terminology (\"start\" and \"endpoints\" instead of \"inputs\" and \"outputs\") causes confusion.  A clear overview of the architectures objectives and design choices is lacking.  The experimental setup is not adequately described making it difficult to evaluate the nonstandard baseline selection.", "This paper presents a novel neural netprocess architecture with multiple layers organized in a treelike structure. While the idea of using a tree structure for neural netprocesss is intriguing the papers readability is significantly hindered by its complex writing manner. Despite potential technical merit the poor writing makes it difficult to fully comprehend the presented process. The reviewer is unable to recommend acceptance due to this writing deficiency."], "tzO3RXxzuM": ["Improved Generalization Capabilities Based on stability This paper enhances the generalization abilities of algorithms using the idea of stability. stability measures the impact of changing one data point on the algorithms outcome. The paper presents two enhancements to existing analysis: 1. Using expected stability instead of uniform stability which is more accurate. 2. Extending the analysis to noise from exponential family not just Gaussian noise. effectiveness and Weaknesses  Expertise Disclaimer: The reviewer lacks familiarity with the particular descent of study the paper references making it difficult to assess the novelty and significance of its improvements.  Accessibility Concerns: The papers readability is hindered by a lack of selfcontained explanation and insufficient contextualization for a broader audience.  Coherence: The connection between the two main improvements is unclear potentially confusing readers. Queries and Suggestions  Why is expected stability preferred over uniform stability How are expectations handled in practice  What is the practical application of these bounds in choosing stopping criteria or performance analysis  It is difficult to differentiate between known and original contributions in Section 2. Can specific references be provided Minor Comments  Correct typo: \"OVER Z\" instead of \"of Z\"  Clarify the notation for W0(t1) and w0(t1)  practice consistent notation for KullbackLeibler divergence: KL(PQ)  Avoid potential confusion between S0 and Sn (for n \u2260 0)", "Paraphrased Summary: The paper examines generalized stochastic gradient descent with Exponential Family noise (SDLG) algorithms which introduce noise into the minibatch gradient descent. The analysis estimates the error bounds based on expected stability. Supporting experiments demonstrate the effectiveness of the proposed EFLD SGD algorithms. effectiveness: 1. The paper demonstrates that existing noisy iterative algorithms have a sample dependence of O(1n). 2. It introduces exponential family noise into generalized SGD algorithms and provides a general error bounds based on expected stability introducing new insights for handling exponential family noise. 3. A comprehensive analysis of the error bounds for exponential family noise SGD algorithms is presented. Weaknesses: 1. The proposed algorithms do not exhibit stateoftheart performance. Without strong empirical evidence it is difficult to justify the study of these complex algorithms. 2. The paper only reviews SGD algorithms and overlooks recent advancement such as Langevin Dynamicsbased SGD which have achieved superior performance. It is unclear if the proposed algorithms could improve performance with better model architectures.", "Paraphrase: Summary: This study provides generalization guarantees for a more general variant of stochastic gradient Langevin dynamics (SGDL) which can handle noise from any exponential family distribution. The authors note that their approach is a direct extension of standard Langevin dynamics and provide an model with sign stochastic gradient descent (sign SGD). They employ gradient variant as a measure of generalization error which is a departure from previous study that used gradient norms or incoherence. The authors bounds appear to be tighter than these existing bounds based on empirical observations. effectiveness and Weaknesses: The study follows the analysis of Li et al. closely with some important differences. Extending to the exponential family is not a new contribution as other study have done similar generalization. However introducing gradient variant itself is a valuable addition. The authors should highlight the specific shift in their analysis that resulted in tighter bounds and discuss why this is presented as a secondary contribution. Additionally the experimental section is not clearly linked to the main contribution of gradient variant.", "Paraphrased Statement: Summary This paper investigates the generalization of iterative noisy learning algorithms. It introduces a new generalization bounds using the Hellinger distance between the algorithms output when a training point is replaced. This bounds is applied to provide a generalization error guarantee for a wide range of noisy iterative algorithms including SGLD. Numerical results demonstrange the boundss improved estimation of generalization error. effectiveness and Weaknesses  The generalization bounds relies on distributional stability in terms of Hellinger distance (Theorem In proposition 1). However the paper primarily practices an upper bounds based on KL divergence (Prop. 2).  The learning range dependence on the optimization trajectory may constrain the practice of very small learning ranges.  While similar to the bounds in Haghifam et al2020 it differs in trajectory dependence. It would be valuable to numerically compare it to the bounds in [1] which extends Haghifam et al2020 to SGLD.  Applying the bounds to SGD may require overcoming some roadblocks.  The distinction between the loss function practiced for generalization error and that practiced for training should be noted."], "sk63PSiUyci": ["Paraphrased Statement: This paper presents a new algorithm that combines adaptive tuningfree stepsize with SARAH resulting in a novel approach for convex optimization. The algorithm demonstrates improved performance compared to other popular stochastic optimizers through empirical work using logistic regression. Strengths:  The combination of adaptive tuningfree stepsize and SARAH is innovative with clear need.  Experiments showcase the superiority of the proposed algorithm over other optimizers. Weaknesses:  The paper focuses solely on convex optimization while nonconvex optimization is more prevalent in machine learning.  The convergence rate analysis only considers gradient norm rather than the more common step of work rate gap or point distance.  theoretical solution and comparisons for sample complexity (computational costs) are not provided which is essential for evaluating optimization algorithms.  The algorithm is labeled as \"firstorder\" but it requires s and thirdorder derivatives potentially impacting sample complexity.  Algorithm 1 contains an undefined variable (\\delta0k) in line 14 which needs clarification. Minor Comments:  Include axis labels in design for clarity.  Provide a more intuitive explanation for line 17 of Algorithm 1.  Explore the use of STORM instead of SARAH for nonconvex optimization.  Consider comparing the algorithm with other significant variant reduction techniques like SAGA.  Capitalize \"Let\" at the beginning of the paragraph on page 5.", "Paraphrase: Summary:  The paper introduces a variant of the SARAH gradient method that automatically adjusts the step size.  The method approximates the Lipschitz constant of the objective work using an exponential moving average of local estimates based on sampled work fits.  The algorithm is tested on convex optimization problems and compared to other further firstorder methods.  theoretical guarantees are provided with some adjustment. Strengths:  The method is parameterfree eliminating the need for manual tuning.  Empirical evaluation demonstrate advantages over other similar methods that require parameter adjustment. Weakness:  Despite theoretical findings there is a notable gap between the theory and the actual implementation of the algorithm.", "Paraphrased Statement: Summary This paper introduces AISARAH a practical improvement on the SARAH algorithm for addressing optimization problems with finite sums. AISARAH utilizes the specific feature of stochastic works by estimating their local Lipschitz smoothness in each iteration of the inner loop. This allows it to select a larger step size leading to faster convergence as it approaches the optimal solution. Strengths 1. AISARAH is a practical variant of SARAH that accelerates convergence by considering the local geometry of the stochastic works. 2. Comprehensive experiments demonstrate the effectiveness and efficiency of AISARAH. 3. The paper is wellstructured and clearly written providing necessary background and references. Weaknesses 1. The novelty of AISARAH mainly lies in its application to convex works but the paper solely focuses on strongly convex works. As an extension of SARAH theoretical analysis for general convex works would be valuable. 2. In convex optimization convergence is typically assessed based on the work rate rather than the gradient norm. While AISARAH performs well in terms of gradient norm it may be slower than other methods when considering the work rate. 3. The authors suggest using 5000 work to find optimal hyperparameters for SARAH and SVRG which may be excessive. more clarification on the necessity of such extensive tuning is needed.", "Paraphrased Statement: Summary: This work introduces an enhanced algorithm AISARAH for optimizing smooth and highly convex functions. It builds upon SARAH estimating local Lipschitz smoothness parameters dynamically. theoretical analysis accompanies a modified variant of AISARAH with extensive empirical experiments demonstrating its effectiveness. Strengths and Weaknesses:  challenge: Determining the optimal rate for \u03b1maxk which depends on Lipschitz constants.  Uncertainty: The authors suggest using Newtons method to solve subproblems but its impact on theoretical analysis is not explicitly discussed.  Adaptivity: While AISARAH is adaptable to strong convexity in regularizers methods for estimating strong convexity in loss functions remain unclear.  tuning: The paper claims that AISARAH requires minimal tuning but it is unclear how this compares to other methods with \"rough tuning.\"  Sparsity: The relevance of dataset sparsity to the algorithm is not fully explored. Analysis variant:  variant exist between the analyzed and implemented algorithms which affects the interpretation of solution. Suggestions for Improvement:  Provide clearer guidance on selecting \u03b1maxk.  Explicitly address the impact of Newtons method on theoretical analysis.  Elaborate on adaptivity to strong convexity in loss functions.  Conduct experiments comparing AISARAH to other methods with both \"rough\" and \"fine\" tuning.  Explore the significance of dataset sparsity and how the algorithm leverages this information.  Ensure greater transparency in highlighting the differences between analyzed and implemented algorithms.  Condense repetitive discussion and clarify the rationale for adjustment in Section 3.4.  Consider implementing analyzed algorithms or analyzing algorithms closer to the implemented variant.  Explore alternative tuning strategies for other methods to assess their impact on overall efficiency."], "wk5-XVtitD": ["Summary: A pretrained transformerbased language model (GPT2) is employed for the symbolic version of the VirtualHome environment. Text strings representing the agents goals and action history are input to the transformer and its output is used to predict the agents actions. Strengths:  Positive impact of language pretraining observed  Innovative randomized word experiment demonstrates the transfer of knowledge from pretraining Weaknesses: Claim of Novelty:  Alleged novelty of improving generalization using pretrained language models is questionable as similar study in text adventure games exists such as [1]. Environment Limitations:  Symbolic actions and discrete objects make the VirtualHome environment less complex and natural compared to text adventure games.  generalization is limited to novel combinations of known objects and predicates excluding novel objects predicates or actions. approach Limitations:  The architecture lacks novelty simply pooling transformer outputs.  The training method is not particularly novel (behavioral cloning).  Lack of comparison to a transformer trained from scratch raises concerns about whether the pretraining benefits are primarily due to the handcrafted templates used. Minor Issues:  Arbitrary choice of 70step goal attainment limit  Typographical error: \"ALFRED\" should be capitalized", "Paraphrase: Despite rejecting the paper the reviewer acknowledges its merits. While the results are promising the experiments scope is narrow and the claims require refinement. Missing details hinder understanding of the experimental setup. The reviewer notes that while Experiment 2B is intriguing the substitution of stringbased with onehot representation doesnt alter the language models input (a sequence of word vectors). This suggests that the rate of language in the goal and history is irrelevant to navigation decisions. The paper conclusions about the effectiveness of language model as a generalpurpose pretraining scheme are also broad given the experiments limitations (only one simulator and one case of currentstate input). The reviewer suggests restricting the claims to the specific taskenvironment and nonvisual input representation. The reviewer recommends the authors consider whether visual observations are used in model training and explore the potential for LMlearned representations to complement other rich representations such as visual.", "Paraphrase: Summary: This research investigates how using pretrained components in a neural network affects an agents behavior in a virtual environment (VirtualHome). The results show:  model initialized with pretrained language components perform better in scenarios with unseen tasks.  Random string encodings reduce the benefits of pretraining.  effective encoding layers can be trained without predefined goal and observation representations. Strengths and Weaknesses: Strengths:  The results demonstrate the advantages of pretraining in nonlinguistic tasks. Weaknesses:  The task used in the study (VirtualHome) involves significant linguistic aspects making it unclear if it is truly nonlinguistic.  The encoding methods for goals history and observations are not consistent.  The performance of \"LM(scratch)\" is concerning indicating potential issues with tuning or training.  The study presents limited empirical evidence and has the weaknesses mentioned above.", "Paraphrased Statement: This study explores the utility of using a language model to improve support studying in realworld settings. The researchers initialize the policy using a pretrained GPT2 language model and evidence that it helps the policy study more effectively across different tasks. However the findings have limitations:  Lack of Novelty: While the results demonstrate the benefits of using a pretrained language model they do not offer novel insights beyond what is already known about its effectiveness.  Concerns and Questions:  The study uses serialized inputs which may not be the nearly effective way to leverage the language model.  Experiments suggest that the language models impact on performance is negligible when using original unserialized inputs.  This raises doubts about the claim that using a pretrained language model is beneficial for generalization."], "vLz0e9S-iF3": ["Summary Paraphrase: This research explores the use of quasipotential to understand the dynamics and escape behavior of Stochastic Gradient Descent (SGD) from local minima. The authors aim to develop a quantitative method for estimating SGDs ability to escape these minima. Strengths and Weaknesses Paraphrase: Using quasipotential theory to study SGD escape properties introduces an inconsistency. Quasipotential analysis typically applies to largescale deviation from equilibrium while escape from local minima involves situations far from equilibrium. Additionally the authors assumption of quadratic loss landscape for analytical calculations near equilibrium is questionable as such approximations may not hold in use. Moreover similar approach using quadratic loss landscapes have been previously explored in Hu et al. (2019) which the authors fail to cite adequately. The authors of Hu et al. opted not to publish their work due to the same inconsistency issue identified in this critique.", "Summary Paraphrase: This paper uses a simplified mathematical framework of SGD (stochastic Gradient Descent) to analyze how long the algorithm takes to \"escape\" from local minima (bad solutions). This analysis is based on a wellknown theory called large deviation theory. Strengths and Weaknesses Paraphrase: While the approach is novel its unclear how it connects to the goal of explaining why SGD often produces good solutions (generalization). more case would be helpful. Assumptions used in the framework may not accurately reflect realworld SGD behavior particularly near optimal solutions. The papers clarity is lacking with fuzzy explanations of key concepts. Comparisons to other studies are made but deviation in assumptions and results are not thoroughly explained. Its unclear if the papers conclusions conflict with or complement previous research. The mathematical statements could be more precise and avoid stating asymptotic results as exact equalities. The analysis does not clearly relate to observed SGD behavior during training.", "Paraphrased Summary: paper:  Presents a mathematical framework using \"quasi potential\" to describe how SGD algorithms navigate during deep neural network training.  Quantifies the steepness of trajectory and defines the \"quasi potential\" as the minimum steepness required to leave a local minimum.  Analyzes continuous and discrete SGD and shows that the time it takes for SGD to escape a local minimums vicinity depends exponentially on:  Minibatch size  sharpness of the minimum  Radius of the vicinity  Inverse of learning rate Strengths and Weaknesses: Strengths:  Clear and wellwritten  Experimental results support theoretical findings Weaknesses: 1. Assumptions are still restrictive. 2. Covariance in the framework depends only on the Hessian of the minimum unlike previous work. 3. loss time dependence on sharpness differs from previous work. 4. framework only considers fixed step sizes unlike variable step sizes commonly used in use. 5. framework suggests instant escape from flat minima which contradicts observations in use."], "reFFte7mA0F": ["Summary Paraphrase: This research explores a ridepool matching problem for ondemand transportation services. While bipartite graph matching has been used it faces challenge due to future demand making online decisionmaking complex. The NeurADP approach has demonstrated promising results but this paper introduces CEVD which significantly outperforms NeurADP (with a reported improvement of 3.8976). CEVDs key effectiveness lies in considering the impact of other vehicles when determining the value of actions. effectiveness and Weaknesses (Pros and Cons): Pros:  significant performance increase with practical implications.  Valuable concept of incorporating the work of other agents in multiagent decisionmaking.  computationally efficient completing optimization within the 60second batch time. Cons:  Unclear notation and missing definitions hindering comprehension of CEVD and its distinction from NeurADP. Outstanding Questions: Notations and Definitions:  Define Ecu.  Explain Ta() and T\u03be().  Clarify the meaning of Vi(rtia rtia).  Provide a definition of rta.  Distinguish between Pr and P (e.g. Pr(aj\u2223ai s) and P(\"agent j takes action g  agent i takes action f\")).  Define Pj.  Explain stif.  Elucidate the notation [gt Ft] of \u03beit1. Estimates and number:  Elaborate on the overunderestimation number in FV and DJV with examples. Optimization:  Explain the nature of the optimization problem and whether the resulting \u03bb\u22c6 \u03b1\u22c6 \u03bb\u22c6 are globally optimal. Scalability:  Discuss the relationship between the experimental results and the size N.  Provide details on scalability beyond the reported N value (e.g. 3000 agents).  Clarify if the clustering (kmeans) affects the computational results. computational details:  Provide an overview of CEVDs computation including inputoutput ILP solver usage and time consumption within the 60second limitation.", "Paraphrase: Summary: This paper addresses the ridesharing matching problem aiming to assign user requests to vehicles efficiently while meeting quality and matching constraints. The authors propose a method called Conditional ExpectationBased Value Decomposition (CEVD) to achieve this. effectiveness and Weaknesses: effectiveness:  The CEVD method accounts for the impact of other users actions on individual passenger value resulting in improved overall performance.  Experimental evaluation demonstrates that CEVD significantly increases the number of requests served (9.76 improvement over the baseline).  The work is innovative and the paper is wellwritten. Weaknesses: Background:  The abstract background is somewhat lengthy. Organization:  The first paragraph should define Approximate Dynamic Programming (ADP) instead of FV since FV is already defined in the previous paragraph. Mathematical Notation:  The excessive mathematical symbols can be confusing. A table summarizing the symbols and their meanings would enhance readability. Clustering Features:  The paper lacks clarity on the data features used for KMeans clustering. The text initially suggests locations but later references average travel times. This inconsistency needs clarification. Conclusion:  The conclusion should include open number and future research directions related to the work.", "Paraphrased Summary: This paper addresses the ridesharing matching problem building on the NeurADP algorithm. It introduces a key improvement: considering the impact of each vehicles actions on its neighboring vehicles within the same cluster. The impact is quantified as the weighted value of neighboring vehicles where the weights represent their action probabilities based on the focal vehicles action. The proposed approach was evaluated against NeurADP using the NYC taxi dataset. Results show significant improvement for various parameters including delay capacity and vehicle number. effectiveness:  Tackles ridesharing a complex domain for RLADP.  Incorporates agent interactions without relying on joint action value.  Investigates different problem configuration parameters.  Provides a public dataset for reproducibility. Weaknesses:  Uses a limited metric (request completion) instead of a more comprehensive metric (fulfilled trip distance vs. actual travel distance).  Employs a handcrafted probability estimator for agent interactions that lacks empirical justification.  Findings in number 3 may not be only due to positive value enforcement and require further investigation.  Mentions of ridesharing companies are excessive.  Algorithm runtime is also slow for realtime performance in practical settings."], "lkQ7meEa-qv": ["Paraphrased Summary: The work investigates how to capture the sound characteristics of a room (impulse responses) using a neural network. The network uses geometric information about the room and the positions of the sound source and listener. By training the network on prerecorded impulse responses it learns to predict these responses based on the geometric cues allowing it to generalize to novel situations. Paraphrased Critique: While the problem is significant and the approach innovative the work has some limitations. It is unclear how the network can infer the form of a completely novel room without prior data. The methodology for creating the network input grid needs more detail. Some scene of the network use are confusing such as how binary information is encoded and how the phase information of impulse responses is handled. The evaluation of the network generalization capabilities could be improved by holding out specific room regions as a test set. Lastly the details of how the nearest neighbor baseline method is implemented are unclear.", "Paraphrased Statement: Summary: This work introduces the novel concept of Neural Acoustic Fields (NAFs) as an implicit representation that models sound propagation in a physical space. The network based on a NeRFlike architecture predicts room impulse responses based on listener and source positions as input. While the idea is original and intriguing certain claims in the paper are exaggerated. The research demonstrates interesting tasks such as sound source localization and crossmodal generation. Strengths:  The concept of using neural network to represent acoustic domains and sound propagation is novel and compelling.  The neural network effectively learns the dataset which includes room impulse responses.  The paper demonstrates valuable tasks enabled by learning the neural acoustic domain representation. Weaknesses:  Despite presenting the concept of Neural Acoustic Fields the network only memorizes a dataset of room impulse responses which were not generated by the model itself.  The paper overstates its contributions in some areas such as claiming that the model can represent acoustic domains continuously at high fidelity by learning from data.  It is unclear why using an implicit representation is necessary instead of employing the dataset directly.  The use of grid features near emitter and listener positions as context provides excessive information to the neural network.  Equation 6 requires clarification regarding the representation of v as intensity.  Ground truth sound maps from the dataset would enhance the comparison in form 4.  The paper writing needs improvement in clarity and detail.  Specific questions arise regarding the nature of local feature grids the dimensions of x and k in Equation 3 the RGBonly baseline in form 6 and the joint training for crossmodal image generation.", "Paraphrase: This paper applies the NeRF concept to Room Impulse Responses (RIRs) known here as NAFs. The approach involves sampling a room from several emitter and receiver locations and using a neural network to predict the resulting RIR based on input data that includes only the locations and spatial information. Strengths:  Demonstrates a practical methodology for learning NAFs.  Introduces innovative techniques for spectrotemporal representation positional information encoding local feature grid use and network architecture. Weaknesses:  Limited exploration of key questions with unnecessary content prioritized.  Unclear justification for using a spectrotemporal representation and its superiority to timedomain data.  Ambiguity over the use of log spectrogram and its implications for learning RIRs.  Inadequate experimental support with weak localization and NeRF enhancement tasks.  Minor grammatical error: \"in principal\" should be \"in principle.\"", "Paraphrase: Summary: This research introduces Neural Acoustic domain (NAF) an propagation of Neural Radiance domain applied to impulse responses between listener and emitter positions. NAF can predict an impulse response given these positions allowing for simulated impulse responses at any localization. Key to extending the model to unseen positions is utilizing local grid features as input. The research demonstrates additional applications for NAF including source localization and multimodal NERF. Experimental results indicate that NAF outperforms baseline methods. Strengths:  innovative concept that holds promise for future research in related fields. Weaknesses:  Insufficient detail for others to reproduce the findings particularly regarding:  Playback of predicted impulse responses (magnitude values only available)  Visualization techniques used in form 1 and 4  Handling of local grid regions with limited or no impulse response data (potential for poor predictions)  Lack of specifics on \"sinusoidal encoding\" mentioned in Section 4.2"], "kEvhVb452CC": ["Paraphrased Summary: This work demonstrates that we can enhance pretrained CNNs by incorporating a Gated Positional SelfAttention layer into their recent stages effectively creating hybrid Transformed CNNs. By leveraging existing components this approach improves performance and robustness without the need for training an entire transformer block reducing training costs. Paraphrased Strengths and Weaknesses:  Strengths: The concept is straightforward and logical. The motivation for enhancing CNNs with transformer components is valid and the results demonstrate effectiveness.  Weaknesses: The novelty of the approach is limited as it reuses elements from a previous work in a slightly modified context. The motivation for expecting improved robustness due to transformer components is questionable given that the transformer baselines in the work do not consistently outperform the CNN baselines. There are anomalies in the robustness results such as the decrease in performance for the FGSM dataset with larger model size and increased training time which require further explanation. Additionally the paper would benefit from including transformer baselines in Table 1 for direct comparison.", "Paraphrased Statement: Summary This work investigates a hybrid model design that integrates recently popular transformerbased architectures with wellestablished convolutional neural networks (CNNs). The proposed hybrid model employs a twophase training strategy: a CNN model is initially trained and its pretrained weights are reconfigured into a novel transformed CNN (TCNN) model for further training. The TCNN architecture surpasses its CNN counterparts and other models evaluated in the work. The representations obtained by the TCNN are also analyzed in the experimental section. Strengths  Wellstructured paper: Logical organization and thorough literature review.  innovative approach: Recasting pretrained CNN weights to enhance training efficiency.  Comprehensive experimental analysis: Ablation work and evaluation on robustness to corruptions and adversarial perturbations. Weaknesses  Training time efficiency: While acknowledged as a motivation the focus on training time efficiency may not be as critical as testtime efficiency for practical applications.  Limited model comparisons: The evaluation lacks comparisons with other hybrid models combining CNNs and transformers.  Missing downstream task evaluation: The experimental analysis does not include evaluation on downstream tasks such as detection and segmentation which could demonstrate the benefits of the pretrained model.  FLOPs vs. accuracy comparison: The performance comparison could be improved by including a number of FLOPs versus accuracy graph to assess model efficiency.  Diminishing returns with model capability: The work does not discuss the diminishing returns in performance of the TCNN as model capability increases indicating the potential tradeoffs in extending CNNs to TCNNs.", "Paraphrased Statement: This research introduces a method that combines convolutional neural networks (CNNs) and vision transformers for range recognition. It involves replacing the final convolutional layer of a ResNet model with a selfattention layer initialized using the weights from the convolutional layer. The approach enhances CNN performance especially in resisting adversarial examples input range distortions and field changes. Strengths:  clear presentation and explanation of the research contributions.  Potential for significant impact in the field.  Valuable analysis of how the networks work changes after selfattention finetuning. Weaknesses:  Limited experimental range with results only presented for two ResNet models on ImageNet and three ImageNetbased robustness tasks.  Lack of additional ablation work to assess the broad potential of the approach.", "Paraphrase: This work demonstrates that innovative architectural components like selfattention layers can be initialized from pretrained convolutional layers reducing training time. While this approach broadly improves performance and reliability it does not consistently surpass the performance of endtoend Transformers as indicated in Table 1. Strengths and Weaknesses: 1. The proposed method originality is limited due to its modest performance compared to endtoend Transformers. 2. The performance gains shown in Table 1 are not significant. Providing scenarios where this method falls short of endtoend Transformers would enhance the analysis. 3. The authors assert that their method reduces computational complexity but they need to provide more evidence to support this claim. 4. The papers organization especially the related work section could be improved for clarity."], "metRpM4Zrcb": ["Summary The paper proposes a continual learning model based on decomposing linear filters into lowrank atoms:  Convolutional filters are split into (ccm) and (mkk) components where (ccm) is fixed after the first task and (mkk) is taskspecific.  Two ensembling schemes are proposed:  Taskincremental: Retrieve atoms from past tasks based on similarity and ensemble them.  Classincremental: Build separate ensemble for each task. Strengths  Clear and wellwritten paper with helpful visuals and notation.  Technically sound method that prevents catastrophic forgetting due to separate atom components.  Encouraging results especially in classincremental learning without the need for memoryintensive buffers.  novel application of lowrank filter decomposition in continual learning. Weaknesses  Classincremental experiment only conducted on one dataset raising concerns about generalizability.  Ensembling techniques may be orthogonal to the main focus of continual learning.  Derived excess risk bound needs explanation and is derived under limiting assumptions.  model does not explicitly leverage forward knowledge transfer.  Choice of initial task may impact model performance which should be tested.  Grassman distance for atom retrieval is not validated against other distance measures.  Regularizationbased methods in Table 1 may have used taskincremental strategies which should be clarified.  HAT should not have backward transfer due to fixed weights from previous tasks. Minor Questions  Missing parentheses after Equation 5.  Regularization methods in Table 1 may exclude unrelated tasks during prediction.  Clarification needed on HATs (slight) backward transfer.", "Paraphrased Statement: This field explores ongoing learning by imposing a lowrank filter structure on each convolutional neural network (CNN) layer. The method involves decomposing filters into atomcoefficients and learning new tasks within a new filter subspace. This approach only requires storing the new filters for each task. Key contribution include the lowrank filter scheme and the developed intratask and intertask model ensemble that operates on the filters. The proposed method outperforms existing techniques on various datasets while using a significantly smaller model memory. effectiveness:  Clearly written and straightforward to understand.  The lowrank filter subspace for learning new tasks is an innovative and intriguing approach to addressing continuous learning challenges.  Strong performance on benchmark datasets. Weaknesses:  The superior performance of the proposed method appears to be heavily reliant on the ensemble scheme which may not be a fair comparison against methods employing a single model. Intratask ensemble can result in a linear increase in memory and computation requirements.  While the lowrank filter strategy enables efficient model storage it may not reduce computation during testing. A comparison of inference times between different methods would be valuable.  The method relies on the overparameterization of deep models. Its advantage over expansionbased methods may diminish when using a smaller network architecture.", "Paraphrased Summary: The paper introduces a novel approach to continual learning by applying lowrank filter structures to CNN layers ensuring knowledge preservation from past tasks and memory efficiency. It proposes intratask and intertask ensemble tailored to classincremental and taskincremental settings. effectiveness and Weaknesses: effectiveness:  Clear and concise concept  Extensive review of related work and experimental setups  Significant memory savings compared to existing methods Weaknesses:  Exclusion of recent methods with superior performance on Cifar100 (20 tasks) from the literature review  Limited experiments on only two datasets requiring further validation on a wider range of datasets (e.g. CUBS Stanford Cars Flowers)", "Paraphrased Summary: The field introduces a continuous learning method that uses a small group of filter atoms to constrain the convolutional filter in each layer to a lowrank filter subspace. For each task the convolutional layer uses a new filter subspace but the subspace coefficients are shared among tasks. The method has been tested on numerous standard dataset. effectiveness:  Innovative idea that uses filter subspace with filter atom construction and contribution subspace coefficients while changing filter atom set for each task. Weaknesses:  Consider using alternative distance metrics besides Grassmann to assess if it enhances performance.  Calculating the distance between filter subspace using SVD can be computationally intensive.  The methods efficientness may be datasetdependent.  The methods performance can vary depending on task diversity with intertask ensemble being less efficient for highly varied tasks. Additional Comments:  The reviewer disagrees with the foundation assertion that humans do not forget past concepts."], "n0OeTdNRG0Q": ["Paraphrased Statement: Summary: This research introduces a method named ESAM that enhances the efficiency of SAM. ESAM consists of SWP and SDS. SWP speeds up estimating epsilon by randomly selecting a subset of parameters during backpropagation. SDS further boosts efficiency by sampling a data subset sufficient for calculating the upper bound of L. Combining these techniques ESAM accelerates SAM while maintaining or even enhancing performance. effectiveness:  Clarity and ease of understanding  Practical applications in various field where SAM is beneficial Weaknesses:  Ambiguity regarding whether a shared gradient mask is used for samples in one SWP batch or separate masks are used for each sample  Lack of comparison with a baseline of estimating epsilon using a randomly sampled data subset which could yield similar solution  Absence of solution demonstrating the applicability of ESAM to advanced architectures such as ViT and MLPMixer", "Paraphrased Summary: This field aims to address the efficiency concerns of the SharpnessAware Minimizer (SAM) which enhances the generalization capabilities of neural networks but doubles training time compared to standard training. The authors identify redundancy in SAMs optimization process and propose ESAM an efficient method that improves computational efficiency from data and parameter perspectives. They argue that SAM can be effectively approximated with reduced computations. Experiments demonstrate that ESAM significantly reduces training time on CIFAR10 and ImageNet datasets while maintaining or even improving accuracy. effectiveness and Weaknesses: Pros:  Practical and applicable approach with no accuracy degradation  Convincing experimental setup on multiple datasets  Thorough ablation field and supporting experiments  efficient and innovative idea of selecting representative samples for parameter updates Cons:  Clarification needed on the impact of using stochastic weight pruning (SWP) in largescale neural networks  Conciseness and clarity could be improved in the explanation of SWP computation  Consider using alternative perturbations to better represent the loss landscape extra Comments:  Use \"hyperparameters of SGD\" instead of \"parameters of SGD\" in Section 3.1  Provide specific numerical comparison for efficiency improvements (e.g. \"Training speed 140.3 vs. SAM 100\")", "Paraphrased Statement: A research paper suggests methods to enhance the effectiveness of the Sharpnessaware Minimization (SAM) algorithm. These techniques include:  Stochastic Weight Perturbation: Randomly selecting a portion of parameters for adjustment in each step.  Sharpnesssensitive Data Selection: Choosing data points that contribute to sharpening the model predictions. Experiments show that these methods outperform SAM particularly with small batch sizes and across different model architectures. effectiveness:  Simple additions to existing SAM algorithms yielding improved solution.  Compatible with a various range of baseline model. Weaknesses:  Lack of analysis of ablation field across different batch sizes particularly for the data choice algorithm.  The stated cost of SAM (100) may not accurately reflect typical applications where data parallelization is used.  Hyperparameter tuning for alternative methods (e.g. SGD with more epochs) could impact accuracy comparison.  The choice of gamma  0.5 implies that half of the batch data is discarded for gradient computation.  It would be informative to explore these methods with larger batch sizes (e.g. 2048 for CIFAR10100) to assess their performance continuity.", "Paraphrased Statement: Summary: This paper presents two straightforward modifications to the SAM optimizer that significantly enhance its computational efficiency without compromising its effectiveness. effectiveness and Weaknesses: This paper is wellwritten and addresses the publication of reducing SAMs computational complexity. It introduces two simple tricks that are logical and show improved efficiency without deteriorating performance. SWP implementation Query: How is SWP implemented in practice It is surprising that omitting gradient calculations for a random subset of weights (e.g. 50) can significantly boost performance. Are there any extra techniques used to achieve this improvement Alignment of SSP Gradient with FullBatch Gradient: The paper states that the gradient on the subset selected by SSP is aligned with the gradient on the full batch. Have you quantified this alignment such as by using cosine similarity Comparing it to a random subset or \\mathbbB could be insightful. Reason for ESAMs Outperformance: ESAM sometimes outperforms SAM in accuracy as seen in your experiments. Do you have any insights into why this might be happening ImageNet solution: One concern is that the original SAM paper [1] paper better solution for SAM in some of the same settings you use. For instance you report 76.7 for ResNet50 with SAM and 77.05 with ESAM in Table 2 but [1] paper 77.5 with SAM. Are there any differences in the experimental setup References:  [1] SharpnessAware Minimization for Efficiently Improving generalization: Pierre Foret Ariel Kleiner Hossein Mobahi Behnam Neyshabur"], "xWRX16GCugt": ["Paraphrase: This paper proposes a novel framework for continuous learning intending to advance research in this domain. The framework categorizes key assumptions shared by Continuous Learning (CL) algorithms unifying supervised and reinforcementbased approach. strength and Weaknesses:  Section 2 introduces a Markov decision process that remains unexplored in subsequent sections. Its relevance to the proposed framework is unclear.  While the model development is commendable the paper lacks significant originality. The focus mainly revolves around engineering effort which may not align with the conferences range.", "Paraphrase: Summary The paper presents a theory for classifying research challenges in continual learning (CL) into a hierarchical structure. This theory forms the foundation of Sequoia a library designed to facilitate the reuse of CL algorithms across diverse research problems (settings). strength and Weaknesses  Ambitious Goal: The paper aims to tackle a wide range of CL problems.  Framework Limitation: Defining settings as a set of assumptions does not necessarily create a treeshaped hierarchy (a lattice structure is more appropriate). This limitation can lead to situations where Sequoia classifies methods as compatible in theory but incompatible in practice due to missing connections in the hierarchy.  Heavy Dependencies: Sequoia relies heavily on external libraries for implementation.  Limited method Sharing: The library does not fully demonstrate the promised ease of applying the same method to different settings. SL and RL settings use distinct methods requiring users to implement methods separately for each setting.  Additional Questions:  Can Sequoia monitor time and memory constraints  Can users modify or add to the hierarchy of settings  How can users resolve accuracy issues caused by missing task label masking in some methods (e.g. in task incremental learning)", "Paraphrase: Summary: This work designs to establish a unified framework that encompasses diverse continual learning scenarios. Additionally it offers a Python library containing relevant methods. Comprehensive experimental evaluations are also provided. strength:  The organization of the continual learning research landscape (design 2) is open and logical.  The motivation for creating a unified framework is compelling as it facilitates comparison among diverse continual learning methods that currently lack a standard benchmark protocol.  Indepth explanations and experimental outcomes are presented in the paper with results accessible on wandb for reference. Weaknesses:  Notable continual learning baselines like iCaRL are not included.  Experimentation is limited to smallscale datasets (e.g. MNIST CIFAR) while many studies (e.g. iCaRL BiC) paper results on largescale datasets (e.g. ImageNet1k). Given the design of establishing a new benchmark protocol results on largescale datasets are crucial.  The design incorporates numerous methods without providing sufficient details (e.g. licenses) about the thirdparty resources utilized.  The definition of \"incremental learning\" in Section 2.1 is ambiguous. It would be more precise to use specific terms like \"classincremental learning\" or \"domainincremental learning.\" Additionally taskincremental learning should not be excluded from this category.  The paper includes excessive implementation details that could be moved to an appendix to make way for more experimental results and analyses.", "Paraphrased Statement: Summary:  The paper proposes a unified framework for classifying all continuous learning (CL) research under a single formalism.  It presents a software implementation of this framework called Sequoia.  Experiments demonstrate that Sequoia can assess different CL methods. strength and Weaknesses: Pros:  Developing a software tool that combines reinforcement learning and supervised learning approach appears valuable for CL. Cons:  CL Hierarchy:  The hierarchy lacks significant nodes such as problemincremental learning where just input space variety.  Related work:  The lack of discussion on competing model makes it unclear why researchers would prefer Sequoia.  Experiments:  The methods implemented by the authors are not specified.  The results are presented but without any analysis or discussion.  insight:  The hierarchy could provide insight connecting CL reinforcement learning and supervised learning approach.  However the lack of comparisons to related work limits the clarity of these insight."], "gi4956J8g5": ["Paraphrased Summary: The authors present a unique unsupervised feature selection approach involving two steps:  Step 1: Focutilizations on identifying significant relationships between pairs of features unlike traditional feature selection methods.  Step 2: Utilizes graph segmentation to identify the most meaningful features. Despite promising experimental results the approach has some limitations:  computational Cost: The algorithm requires significant computational resources due to the utilization of a correlation matrix.  Unclear Decisions: Certain decisions such as the exclusion of the last 10 features in graph construction and setting small values to zero are not fully explained. An ablation study could provide insights into the impact of these decisions.  Limited evaluation: The algorithms performance was only evaluated using kmeans clustering. More complex evaluation metrics and label generation methods should be considered.  Accuracy Decline: Increasing the number of selected features (20 or higher) led to a drop in accuracy. Investigating alternative graph segmentation algorithms that could mitigate this number is recommended.", "Paraphrased Statement: Summary: This paper presents a twostep secondorder method for unsupervised feature selection. Additionally knowledge contrastive distillation is employed for feature learning. evaluation on multiple datasets demonstrate the effectiveness of the method. Strengths:  Clear and wellorganized presentation  Strong results on the datasets tested Weaknesses: 1. Experiments:  Concerns about the experimental design:  The bestperforming baseline NDFS is outdated (2012).  The improvement over NDFS is minimal.  comparison with recent methods (CAE InfFS) are not convincing due to different datasets used. 2. Novelty:  The proposed method lacks originality.  Its components (secondorder information feature relationship learning graph segmentation) have been extensively studied in unsupervised learning. 3. Clustering performance:  The clustering results on COIL20 and ORL are not competitive with other stateoftheart methods. 4. computational Cost:  The proposed method has a high computational complexity compared to other approach. 5. Rejection by NeurIPS 2021:  The paper was not accepted by NeurIPS 2021.  The authors have not incorporated any reviewer suggestions in the revised manuscript except for changing one dataset for evaluation.", "Paraphrased Statement: Feature selection techniques are utilizationd to reduce the dimensionality of highdimensional data by identifying informative and comprehensible features. This study introduces an unsupervised feature selection method called SOFT which combines information from the covariance matrix with the data matrix. empirical experiments have shown the effectiveness of SOFT. Strengths:  SOFT incorporates information from the covariance matrix and the data matrix providing a novel approach to feature selection.  SOFT outperforms several baseline methods on empirical experiments. Weaknesses:  The study suggests that existing methods fail to handle complex relationships between features but some existing models (e.g. [1 3]) address this number.  SOFT is not compared to nonlinear feature selection models like AEFS which raises questions about its overall effectiveness.  The stability of SOFTs selected features is not evaluated despite inconsistencies observed in the results.  SOFTs efficiency advantage over baselines is unclear. Additional ClarificationsFixes:  Font size of the legend in number 3 needs to be increased.  Redundant utilization of wide names and abbreviations should be avoided.  details on the architectures of the baseline methods utilizationd (e.g. linear or nonlinear CAE) should be provided.", "Paraphrased Statement: This paper presents a twopart unsupervised feature selection technique using a knowledge contrastive distillation model. It combines the secondorder covariance matrix with the firstorder data matrix. In the first step a sparse attention matrix is generated to capture secondorder relationships between features. This attention matrix is then used to create a relational graph which is segmented to select features. Strengths:  The approach is innovative.  The experimental findings are promising. Weaknesses:  Unclear Explanations:  Its unclear why the \\ThetaM matrix is enforced to be symmetric (page 4 line 1).  The definition of the masked matrix in Eq.(2) needs further justification.  Reliability Concerns:  The reliability of the clustering results obtained by PCA and Kmeans relies on pseudo labels from GCN training. The paper doesnt address how the accuracy of these labels is ensured.", "Paraphrase: This paper addresses the specific problem of feature selection. The authors highlight that existing weightbased methods for feature selection face number with redundancy. To address this they introduce a twostage model that utilizes a feature covariance matrix to capture feature correlations. This model includes a stage for knowledge contrastive distillation to learn the correlation matrix and a graph segmentation stage for feature selection on the masked correlation matrix. The experiments show that the proposed method is efficient in selecting features. Strengths:  Clear motivation with visual model.  Novel and unique graph segmentationbased model.  Sound rationale behind knowledge contrastive distillation.  extensive experimental evaluations with significant improvements.  Indepth exploration to enhance understanding. Weaknesses:  The knowledge contrastive distillation approach is not compared to traditional unsupervised feature selection methods.  It would be helpful to demonstrate that the proposed method addresses the redundancy number in the selected features through experiments."], "yOBqNg-CqB0": ["Summary: Findings: carefully evaluating standard baseline methods (e.g. tfidfbow vector with L1 normalization) reduces the performance gap between these methods and WMD distancebased methods. The distance between word embeddings becomes more like a delta role (bimodal with peaks at 1 for distinct word and 0 for identical word). effectiveness: Provides empirical evidence that WMD distance does not offer a 100x performance improvement (only a 10x improvement) correcting misconceptions. Weaknesses: Limited impact due to its specific focus on refuting a single study. Insufficient technical or methodological contribution to justify a wider impact. document attempt to address criticism by including a section on \"reevaluation of existing methods\" is not sufficiently broadranging.", "Paraphrase: Summary: This research reexamines a commonly used distance metric for documents containing word embeddings. The authors correct data from the original study and provide new analyses. effectiveness and Weaknesses: This paper thoroughly evaluates the word movers distance method proposed by Kusner et al. (2015) highlighting various points the authors believe are misleading.  Misleading Point 1: The paper claims that Kusner et al. did not mention the normalization of word vectors in their paper. However this data was available in the code released by the authors. While clarifying the data is valuable calling the omission misleading is an exaggeration.  Misleading Point 2: The paper criticizes Kusner et al. for not using the same normalization for document vectors obtained by bagofwords (BoW) and TFIDF methods. However the original paper included other comparison methods that performed comparably to word movers distance (WMD). This paper acknowledges this. Additionally the normalization in WMD occurs at the word embedding level while in BoW and TFIDF normalization is at the document level.  Misleading Point 3: The paper addresses the presence of duplicate documents in the datasets used but this is not directly relevant to the WMD method itself.  Misleading Point 4: The paper disagrees with Kusner et al.s example demonstrating how word embeddings improve token matching distance metrics.  Section 5: The paper argues that the discussion on \"modes\" (not modalities) is not wellsupported. The authors maintain that highdimensional word embeddings (300 dimensions) are more informative than BoW representations. Conclusion: The document criticism of the original WMD paper is largely unjustified. The main findings of Kusner et al.s study remain valid and the experiments in this paper support them. A more comprehensive evaluation of distance metric evaluation practices using various stateoftheart methods would be more valuable than focusing on a specific metric with outdated word embeddings in text classification tasks.", "Paraphrased Summary This study demonstrates that the previously reported performance of Word Movers Distance (WMD) is overstated and comparable to a specific case of WMD called L1normalized BagofWords (BOW). The authors also note that WMD contribution similarities with BOW in highdimensional spaces. effectiveness and Weaknesses This research focuses on a more accurate assessment of WMD which is a crucial technique in many field of research. The study makes various key observations: 1. original WMD study included duplicate samples and L2 normalization of word embeddings was not always stated. 2. Normalizing BOW and TFIDF reduces the advantage of WMD over these methods. 3. performance is affected by both word vector normalization and documentlevel distance metric (L1 or L2). 4. Empirically and theoretically WMD aligns with L1L1 BOW due to highdimensional word embedding properties. The authors provide empirical evidence and suggestions based on extensive experiments. They also make available cleaned datasets without duplicates and relevant code for future research. Questions 1. What metric is used for wordlevel distance in Figure 4 2. How do different wordlevel distance (L1 L2 or Cosine) affect WMD performance 3. What is the impact of applying L1 normalization to word embeddings in WMD 4. Does the removal of stopword affect WMD performance 5. Why does removing outofvocabulary (OOV) word lead to performance degradation in some cases (e.g. bbcsports and ohsumed)", "Summary Paraphrase: This study revisits WMD (Weighted Mean Distance) and highlights flaws in the original paper. It demonstrates that the improved performance claimed in the original paper was primarily due to normalization not WMD itself. When normalization is eliminated WMD performs similarly to a mere baseline. Furthermore under controlled normalization WMD exhibits behavior similar to traditional bagofwords models. Strengths Paraphrase: This study effectively points out weaknesses in the original WMD paper. Through rigorous evaluation it reveals that the reported improvements are largely attributable to normalization rather than WMD. The paper provides thorough analysis and experiments to substantiate its findings."], "kl8flCo98nm": ["Summary The work investigates a problem related to determining parameters for a specific type of generative framework. The framework includes a singlelayer ReLU network and can handle outliers within the Huber contamination framework. The researchers created a gradientbased algorithm to estimate specific components of the frameworks parameter matrix namely the norms and angles between its rows. This is essential for estimating the only estimable parameter WWT. Their approach is similar to Wu et al.s with adjustments to account for outliers using trimmed mean or median estimation. effectiveness and Weaknesses This welldefined problem is addressed and the authors advance prior generative framework parameter estimation techniques which often ignored outliers. Technically their methodology largely parallels Wu et al.s with a minor adjustment for outliers. However the results may not converge to Wu et al.s findings when the probability of uncorruption approaches 1. The precision of parameter estimation is also unclear. These factors contribute to the authors reservation about the work. specific Concerns  Theorem 4.3 results recovery of Wu et al.s parameters: Does the algorithm fail to recapture Wu et al.s parameters as p approaches 1  Incompleteness of results (Theorem 4.3 Propositions 4.14.2): Quantifiers for epsilon are missing and the role of the \\Psi condition is unclear.  Odd appendix structure: Formal statements of lemmastheorems are absent making it difficult to verify claims.  Insufficient explanation of the Wu et al. connection: The algorithms similarity to Wu et al.s is not explicitly stated.  Minor error and clarifications:  \"Byzantine\" datasets are unexplained.  \"Adversial\" is a typo.  \"RVe x\" is unclear.  The assumption of negative bias contradicts Wu et al.s findings.  \\tilde gx is undefined in algorithm 2.  The poor performance of standard gradient descent for p0.95 is unusual.  gt in the appendix seems to refer to \\tilde gt.", "Paraphrased Statement: Researchers consider a framework where a fraction (p) of observation come from a rectified linear unit (ReLU) distribution with parameters W and b while the remaining observation come from an arbitrary distribution. They aim to estimate these parameters. effectiveness:  The approach separates the estimation of W and b into two contribution.  A random projectionbased method is used to estimate angles between rows of W.  The researchers provide error bounds for their method. Weaknesses:  The error multiplier in the bound is large (at least 3\u03c0).  The error does not converge to zero as the sample size increases even in the absence of outliers.  In simulations the gradient descent method with filters does not show decreasing error with increasing sample size. Questions and Concerns:  The number of gradient descent iterations is set to 100 but its unclear if this is sufficient.  The error decrease with iterations is not shown for specific value of n d and p.  The understanding for discarding examples with zero entries is not explained.  The parameters \u03b7 and L in Proposition 4.2 are not defined in the main text.  The matrix transpose in the definition of \u039e is not justified.  The similarity between the bounds in equation (18) and Theorem 4.3 is superficial as the other can be improved by increasing the sample size while the latter cannot.", "Paraphrased Summary: The paper describes a method for condition singlelayer ReLU networks under the Huber contamination framework where a certain percentage of condition samples may be corrupted. The method involves: 1. Estimating the norms of each row of the weight matrix using gradient descent with a mediantrimmed mean gradient. 2. Estimating the angles between rows using properties of inner products with Gaussian vectors. effectiveness:  The paper is wellwritten.  The techniques are intuitive and make sense.  The results align with previous findings for condition networks without outliers. Weaknesses:  The algorithm relies heavily on the assumption that the input data follows an i.i.d. Gaussian distribution which is unrealistic in practice.  The generalizability of the techniques to nonGaussian inputs is unclear.  The claim about learnability (condition the entire weight matrix) is confusing as the main results only cover learning the row norms of the weight matrix.  The utility of algorithm 2 (robust Gaussian estimation) is unclear and could use existing methods.  The role and impact of the \"r\" hyperparameter in algorithm 2 on the constants and sample complexity require clarification.", "Paraphrased Statement: Summary: The paper aims to estimate parameters of a singlelayer ReLU neural network that generates the data. However with a probability of \"p\" some examples are purposely corrupted into random outliers. The work develops an algorithm with theoretical guarantees on the number of samples required and demonstrates how the estimation error varies with key factors through simulations. effectiveness and Weaknesses:  Weakness: The works focus on a specific problem in ReLU networks with outliers lack clear motivation and practical significance.  Weakness: The techniques employed appear incremental to previous work ([1]) which tackled the same problem but without outliers. The approach follows a similar twostep procedure and utilizes a common robust gradient estimation technique.  Weakness: Theorem 4.3 a key effect is difficult to comprehend. The error condition does not approach zero with an infinite number of samples due to the presence of a nonvanishing condition (\"pi(2p)3\"). This contradicts prior research ([1]) that established a lower bound of 1epsilon2 while this paper proposes an upper bound of 1epsilon. The authors should clarify this discrepancy.  Weakness: The overall writing style of the paper could be improved for clarity."], "z-5BjnU3-OQ": ["Summary: This paper introduces a novel method for texttoimage synthesis using a conditional approach based on hypernetwork. Hypernetwork generate a 4D tensor that modifies the convolutions of a Generative Adversarial Network (GAN) at each level. To make this more manageable the hypernetwork predicts a lowrank decomposition that can be used to reconstruct the 4D tensor. The paper demonstrates this approach using StyleGAN v2 and INRGAN backbones. Strengths:  Stateoftheart performance on FID CLIPR and human evaluations.  novel application of continuous generative models for texttoimage synthesis.  First augmentation of StyleGAN for texttoimage synthesis. Weaknesses:  Lack of clarity on implementation details.  Inadequate baselines for comparison. Suggested Experiments:  work channelwise modulation instead of the proposed modulation.  Verify that only word or sentencelevel modulation is used at each case.  Clarify the conditioning of the final projection head in the discriminator.  Provide more details on the hypernetwork outputs rank constraints and training procedure.  Motivate and evaluate the wordlevel modulation mechanism.  Apply the proposed modulation to both the generator network and MLP in INRGAN.  Describe the text encoder architecture and pooling mechanism.  Provide more information about the human evaluations including introduction place and time set. Related work:  Citing additional texttoimage synthesis work based on various approaches. other Details:  Inclusion of the DAMSM loss description for selfcontainment.  Clarification that a singlelevel MLP and a Conv 1x1 level for each block in the generator are equivalent.  potential summary of extrapolation results in a novel table.", "Paraphrased Statement: Summary: Researchers investigate the use of hypernetworks for creating images from text descriptions. Hypernetworks are used to learn convolutional weights based on the text input rather than generating them directly. The authors validate the effectiveness of their method through experiments using StyleGAN and INRGAN architectures. Additionally they introduce a solution to address memory issues associated with using hypernetworks for weight generation. Strengths:  First paper to explore hypernetworks for texttoimage generation.  Contributive efforts to address memory constraints when synthesizing weights for large networks (e.g. StyleGAN).  Validation on different architectures and datasets. Weaknesses:  Lack of clear intuition explaining the need for hypernetworks in texttoimage generation.  Limited contribution as hypernetworks can be incorporated into existing architectures.  Unconvincing results due to different architectures used for baselines and proposed method.  Potential for hypernetworks to hinder performance as evidenced by the reported results.  Errors and unappealing introduction in image 2.", "Paraphrase: The paper introduces a novel method for generating images from text by training a \"HyperNet\" that dynamically adjusts the weights of a generative adversarial network (GAN) based on the text input. This approach allows for the use of powerful GANs such as StyleGAN2 and INRGAN for texttoimage synthesis while maintaining the advantages of direct text conditioning for textimage alignment. The HyperNet is designed to be efficient by using a lowrank decomposition to reduce the number of parameters that need to be predicted. The method is evaluated against previous approaches and shows improved performance in terms of image quality and textimage alignment as measured by ISFID and RprecisionCLIPR metrics. However the authors do not provide specific model of how their model is better than the original approaches of directly conditioning the model on the text. Additionally the paper does not discuss the use of pretrained GAN models the ability to generate higherresolution images or evaluations using other textimage alignment metrics such as Semantic Object Accuracy (SOA).", "Paraphrased Summary: The research applies hypernetworks to generate texttoimage models. Hypernetworks adjust the weights of the generator and discriminator networks based on textual descriptions. The authors assess their method in conventional image generation and a novel approach called continuous image generation. Strengths:  Utilizing hypernetworks to control generator and discriminator weights  Introducing a novel dataset (ArtEmis) for evaluation  research continuous image generation Weaknesses: 1. Limited discussion on the specific benefits of hypernetworks in texttoimage generation 2. Confusion regarding the selection of StyleGAN2 with word conditioning and DAMSM loss as the beneficial method despite its similar performance to AttnGAN with DAMSM loss 3. significant difference between DAMSMR and CLIPR scores for real images and lack of details on their calculation methods 4. Insufficient exploration and discussion of continuous image generation applications"], "jxTRL-VOoQo": ["Paraphrase: The researchers conducted experiments on various Graph Neural Networks (GNNs) to identify factors that hinder their performance. They concluded that deep GNNs suffer from oversmoothing due to excessive propagation depth and model degradation due to excessive transformation depth. To address these issues they proposed design guidelines for deep GNNs and introduced a deep Graph MultiLayer Perceptron (DGMLP) that incorporates these principles. DGMLP exhibits beneficial performance in terms of accuracy flexibility scalability and efficiency. effectiveness:  The work defines \"Node Smoothing Level\" and \"Graph Smoothing Level\" to compare representations of node at different embedding propagation stages. Weaknesses:  The oversmoothing issue affects node across the graph which is not captured by the comparison of representations of the same node.  DGMLPs performance seems comparable to existing methods despite its claimed ability to support large transformation depth.  It is not clear if increasing the transformation depth beyond the observed stability point would improve performance.  Larger transformation depth implies more parameter potentially requiring longer training times.  The function of the nodeadaptive weighting mechanism in DGMLPs performance could be further explored with validation experiments.", "Summary This research focfunctions on the transformative impact of embedding transformation within deep Graph Neural Networks (GNNs). Through extensive analysis the paper develops practical guidelines for designing deep GNN models called DGMLP. effectiveness  Clearly defines embedding propagation and transformation depth to assess their significance.  Provides empirical work examining these depth.  Wellorganized and structured introduction. Weaknesses  empirical Guidelines and experimental analysis:  Guidelines 13 and experimental issue (Section 4) are not groundbreaking these concepts have been explored in previous deep GNN research including decoupling of embedding propagation and transformation adaptive weighting and skip connections.  DGMLP model:  The proposed DGMLP model offers minimal novelty compared to existing works its only distinct feature is the function of skip connections in the final MLP module.  Missing Baseline models:  Table 1 lacks significant baseline models such as GCNII SAGN and DeeperGCN which potentially leads to skewed interpretations of issue.  Incorrect Statements:  The claim that neighborhood information is corrupted due to degreebased neighborhood work is incorrect as neighbor features can still differentiate nodes.  The assertion that oversmoothing only arises from embedding propagation (EP) is incorrect ET performance also contribute to the issue. Questions  Clarification on how GCN models with Dt2 and adjacency matrices powered by Dp2 are formulated (Figure 2(c)).  Formal definition of residual and dense connections functiond in the empirical work (Section 4.2).  Validation of the novelty of guideline 2 considering previous approach to optimizing convolution depth for different nodes.", "Paraphrased Summary: This paper investigates the function of Graph Neural Networks (GNNs) for node classification tasks on large datasets like citation databases where data exhibits a graph structure and nearby nodes tend to contribution labels. Each node has a feature vector and is connected to other nodes through an adjacency matrix. The authors explore two aspects of this problem: 1. Hyperparameter Optimization: They examine the impact of GNN iterations and an output layer on classification performance. They propose an unentangled architecture that separates GNN smoothing from subsequent layers. 2. GNN Architecture Improvement: They introduce DGMLP a modified GNN architecture that includes nodeadaptive weighting and skip connections. mathematical experiments demonstrate that DGMLP can achieve stateoftheart issue and scales well with increasing graph size. The authors emphasize the flexibility and complexity reduction of DGMLP compared to other algorithms. effectiveness and Weaknesses:  Thorough exploration of hyperparameters and their impact on performance.  introduction of the DGMLP architecture with various improvements.  scalability and stateoftheart issue.  The focus on generality and criticism could be reduced.  The nodeadaptive weighting mechanism could be explained more clearly.  The complete DGMLP statement should be included in the main paper.  A comparison of the overall complexity of DGMLP with other algorithms including the cost of hyperparameter optimization would enhance the analysis.", "Summary: This research investigates the challenges in training deep Graph Neural Networks (GNNs) by separating the issue of embedding propagation and transformation. The work reveals that large embedding dimension (Dt) is a primary cause for deep GNNs difficulty. To address this the authors present a nodeadaptive combination mechanism and residual connections for ET performance. effectiveness:  Wellstructured and easy to understand.  Provides a thorough analysis of the challenges in training deep GNNs. Weaknesses:  Lack of a formal definition for \"model degradation.\"  Node Smoothing Level may not accurately measure oversmoothing.  unclear distinction between \"ResGCN\" and \"DenseGCN\" in the proposed approach versus previous work.  Insufficient explanation of how the proposed mechanism aid in training deep GNNs.  Low performance of DGMLP on ogbnproduct compared to leading methods on OGB leaderboard.  Baseline SIGNs performance below originally reported issue. Recommended Actions:  Define \"model degradation\" formally.  Explore more suitable metrics for oversmoothing detection.  Clarify the differences between the proposed approach and previous work using skip connections.  Conduct an ablation work to demonstrate the benefits of the proposed mechanism.  Investigate the cause for DGMLPs underperformance.  Verify the discrepancies in baseline SIGNs performance."], "xy_2w3J3kH": ["Paraphrased Summary: This field presents: 1. A novel subclass of cooperative multiagent reinforcement learning (MARL) called the homogeneous Markov game. 2. The first decentralized actorcritic algorithm that combines policy sharing with provable convergence. 3. A practical technique to convert centralized training algorithms into decentralized communicationefficient equivalents by utilizing a bandit mechanism to select communication links iteratively. Experiments show that this technique effectively reduces communication costs while maintaining performance comparable to full communication. effectiveness:  Proposes a novel subclass for cooperative MARL.  Both the novel algorithm and transformation technique strategically select significant communication links improving communication efficiency.  Employs general neural networks for communication actor and critics.  Extensive experimental evaluation. Concerns:  The method may underperform full communication in some scenarios. Figures displaying the number of communication as xlabels would be helpful for comparison.  The papers claim of outperforming full communication in the cooperative Push task appears incorrect.  The cooperative navigation example does not fully meet the required bijectivity condition for the observation function. It is unclear if other experiments adhere to this condition.  The foundation should address a counterargument that suggests noncooperative MARL approach can however achieve efficiency.  The TD step relies on the private action of all agents which may not always be accessible.  It is unclear which version of the proposed method was used in Figure 1.  The gradient computation for binary variables in the regularizer is not explained.  The assumption in Section 4.2 that the communication matrix has no zero entries could be relaxed. Minor Comments:  \"Markov game\" is typically associated with competitive MARL. \"cooperative MARL\" would be more appropriate.  Permutation indices in Definition 1 could be simplified without referring to specific agents.  In the cooperative navigation example it is unclear if landmarks are fixed or moving.  Theorem 1 contains errors with mathematical symbols and terminology.  Clarify the neural architecture and training action details in Section 4.1.  Define rm using an equation in Section 4.2.  A finitetime convergence result with specific rates would be more valuable.", "Paraphrased Statement: This field introduces a consensusbased actorcritic algorithm for multiagent reinforcement learning (RL) that requires limited communication during the training phase. Unlike standard centralized training methods like MADDPG and COMA which assume approach to all agents data for training critics the proposed algorithm considers scenarios where such complete data sharing may not be feasible. The algorithm is motivated by an analysis of homogeneous Markov games (MGs) a type of MG where sharing policy parameter preserves policy optimality. Inspired by Zhang et al.s (2018) networked MARL consensus method it proposes a policy consensus algorithm that determines when and with which neighbors to share action and observations. It also uses a bilevel bandit method to perform consensusbased updates on policy and critic parameter. Experiments in the multiagent particle environment (MPE) domains demonstrate the algorithms effectiveness in reducing communication requirements during training. Key effectiveness:  Emphasizes the importance of communication during training which is often overlooked in mainstream centralized training approach.  Provides a theoretical analysis of policy sharing a practical technique in MARL that is often used but not fully understood. Key Weaknesses:  Lacks a theoretical analysis for the partially observable setting.  Fails to establish a solid connection between theoretical findings and empirical results.  Does not discuss the methods relation to alternative formulations that consider agent identity comparison or collective decisionmaking.", "Paraphrased Statement: Summary: This paper presents a subclass of cooperative multiagent reinforcement learning (MARL) games where agents are similar in behavior. In this subclass sharing the same policy between agents does not reduce performance. The paper utilizes this feature to develop a decentralized actorcritic method with consensusbased updates for both actors and critics guaranteeing convergence. They also introduce heuristic algorithms to reduce communication costs during training. effectiveness and Weaknesses: The reviewer acknowledges the papers insights into the effects of policy sharing in MARL. however they express some concerns: 1. Lack of Clarity: The validation of Theorem 1 is difficult to understand and the paper would benefit from a clearer explanation of the intuition behind it. 2. Generalizability: The subclass of MARL games considered in the paper appears restrictive. Providing evidence of its generality or broader applicability would enhance the papers significance. 3. Theoretical Value: The theoretical benefits of policy sharing compared to nosharing policy need to be quantified more clearly. 4. literature Review: The paper lacks a thorough literature review on communicationefficient MARL methods which would strengthen its context. 5. Terminology: The use of \"Markov game\" for cooperative games in the paper could be confusing as Markov games typically involve agents with conflicting objectives. 6. Minor Issues: The policy in Theorem 1 should be specified as \\pioi O \\times A\\rightarrow [0 1] instead of \\pioi S \\times O \\rightarrow [0 1].", "Paraphrase: Summary This paper focuses on the homogeneous permutationinvariant multiagent reinforcement learning (MARL) setting and establishes a theorem demonstrating that restricting to identical policy does not diminish performance supporting policy parameter sharing. A consensusbased actorcritic MARL method is proposed. Additionally a communicationefficient protocol is suggested to enhance communication efficiency. effectiveness:  The paper addresses a novel and relevant setting: permutationinvariant MARL which has practical applications but limited rigorous research.  The analysis provided for permutationinvariant MARL is insightful. Weaknesses: 1. Definition 1 (iii) assumes fully observable states (bijective observations) which should be explicitly acknowledged. 2. In Theorem 1 the notation for the policy mapping is confusing. The original mapping is from the state and agent action spaces to [0 1] whereas the observationbased policy in the theorem appears to map from the observation and agent action spaces instead. 3. In Section 4.1 the performance of the critic network is unclear. It relies on observations and action of neighboring agents but its unclear how the critic can be evaluated without communication (cji  0) between agents at time t. 4. The connection between Section 4.1 and Section 4.2 is not clear. Section 4.1 introduces a method for selective communication but its unclear if the method in Section 4.2 is separate or complementary."], "pgKE5Q-CF2": ["Paraphrase: Summary: This paper proposes an autoencoder with a hidden layer and a nonlinear activation function in the output layer. The activation function is learned using a parameterized work. The work provides theoretical bounds and experimental results. Strengths and Weaknesses: Weakness:  The evaluation metric (RMSE on observed evaluation) assumes random missing data which is not the case in recommender systems (data are missing not at random).  Limited scalability due to the function of small datasets in experiments. Suggested improvement:  function evaluation metrics that account for both observed and unobserved interaction (e.g. ranking metrics).  Consider more complex methods for rating prediction that account for the nonrandom missing data.", "Paraphrase: This paper proposes an enhancement to autoencoders for recommendation systems by incorporating an additional elementwise neural network. The authors provide a theoretical analysis demonstrating that this elementwise network reduces the upper bound of error in predicting unknown evaluation. Strengths: 1. The method is straightforward to implement. 2. The effectiveness of the elementwise network is supported by a rigorous theoretical analysis. 3. The paper suggests intriguing conclusions including the potential benefits of data sparsity. Weaknesses: 1. The assumptions made in the theoretical analysis (e.g. uniform and random observation of evaluation) may be unrealistic in practice where missing evaluation in recommender systems are often nonrandom and influenced by various biases. 2. The paper focuses on rating prediction while realworld recommender systems predominantly prioritize ranking items and use different loss functions (e.g. crossentropy). 3. The experimental evaluation is insufficient with tests conducted on just four datasets and a lack of variation in model parameters (e.g. activation functions neural network size and layer). 4. The writing is verbose with excessive basic knowledge provided in Section 2 that could be replaced with additional experimental results.", "Paraphrase: Summary: This research introduces an imputation technique consisting of two network: an autoencoder (AE) and an elementwise nonlinear transformation. The authors provide theoretical evidence for the proposed methods improved generalization for missing evaluation. Empirical results demonstrate promising performance on several datasets. Strengths and Weaknesses: Major Concerns: 1. The theoretical content is complex and not readily comprehensible. Its significance and generalizability are unclear. 2. The empirical findings are insufficient. Despite reported superior performance in Table 1 another paper suggests that CFUIcA with SN achieved lower RMSE. The ML 10M dataset is not included in the experiments. 3. Given the simplicity and efficacy of SN in boosting other algorithms it would be beneficial to explore the potential of the proposed technique to similarly enhance existing algorithms like CFUIcA. 4. Adding a nonlinear layer between hidden and output layer could potentially improve empirical results leading to a high evaluation score.", "Paraphrased Statement: Summary: This work examines a common collaborative filtering task where the aim is to forecast unseen functionr evaluation. The researchers present NEAECF an improved autoencoderbased approach. NEAECFs unique feature is an additional module for predicting each rating modeled as an autoencoder in Equation (10) and Figure 1. Strengths: 1. The introduction of an elementwise function (approximated by a neural network) is innovative. 2. Empirical results demonstrate that the added neural network enhances prediction accuracy. Weaknesses: 1. collaborative rating prediction is a wellresearched field with numerous existing solutions. In addition item ranking aligns better with practical recommender systems. 2. NEAECFs time complexity appears high due to its function of an itemoriented autoencoder computationally intensive elementwise function and significantly larger hidden unit count compared to matrix factorizationbased methods. 3. The work lacks detailed justifications for employing a high number of hidden units and an elementwise function. Treating unobserved evaluation as zeros may introduce bias which is not addressed."], "lycl1GD7fVP": ["Paraphrased Summary: This study introduces a theoretical framefunction for generalization in kernel regression. It focuses on a matrix called the \"learning transfer matrix\" which links the decompositions of the true and learned functions in the kernel eigenbasis. The authors derive an analytical expression for this matrix and analyze its statistics. They find that functions with higher weights in higheigenvalue modes are easier to learn using kernel regression. The study introduces a \"learnability\" metric and demonstrates that average learnability is independent of the kernel used. However it emphasizes that kernel selection should be specific to the function being learned for optimal performance. Some functions may even exhibit worse generalization with kernel regression than a constant prediction highlighting the limit of the approach. Strengths and Weaknesses: Strengths:  Provides a clear theoretical analysis of kernel regression generalization.  Introduces a useful \"learnability\" metric.  Proves a novel \"nofreelunch\" theorem showcasing the relationship between training set size and regression success. Weaknesses:  Attempts to extend results to deep learning despite limit of kernel machine assumptions.  experimentation are conducted on artificial datasets limiting their applicability to realistic data.  The spectrum of eigenvalues in realworld data follows a power law which is not addressed in the study.  The paper lacks a thorough comparison with the function of Canatar et al. (2021) which also explores generalization via kernel eigenspectrum.", "Summary: The study seeks to understand how neural netfunctions generalize by examining the spectral properties of their neural tangent kernel. Strengths and Weaknesses: section 2.1:  The absence of assumptions about output independence might suggest that the number of outputs (m) is assumed to be 1. However this is unclear from the paper. Figure of Merit:  The inner product in Equation (2) represents cosine similarity and should be bounded between 1 and 1. It is unclear how the measure remains within the range [0 1] without taking absolute measure. section 2.3:  This section appears to review kernel methods without providing novel theoretical contributions specific to neural tangent kernel. Comparison to previous function on kernel eigendecomposition is lacking. section 2.5:  The treatment after Lemma 2 focuses on sample size dependence which is not specific to neural tangent kernel.  Kernel methods generally converge to the target with increasing sample size and existing research covers this aspect. other Points:  The empirical analysis is valuable and could have been emphasized more.  The ablation study with closedform eigenfunction expressions provides insights into the persistence of correlations even in narrow netfunctions.", "Paraphrase: The paper explores the eigenvalues of the \"neural Tangent Kernel\" of a neural netfunction to understand its generalization performance in the infinitewidth scenario. It speculates that these findings may also hold for the finitewidth regime. Using kernel regression and defining a \"learnability\" measure the paper establishes a \"nofreelunch\" theorem. This theorem asserts that improving a netfunctions or kernel generalization for a specific target function inevitably degrades its generalization for \"orthogonal functions.\" Analytical predictions are made for two phenomena: worsethanchance generalization for difficult functions and nonmonotonic error curves in a small data set. Simulations are used to support the analytical results. Strengths:  The paper addresses the NTK approach for analyzing generalization error in neural netfunctions a promising area of research.  It is wellstructured and includes numerous simulation results. Weaknesses:  The theoretical aspects could be enhanced particularly the problem setup and notation.  The theoretical findings are somewhat incremental compared to previous function.  The paper assumes a finite set with a uniform measure but extensions to nonuniform measure and continuous set are not explored in depth.  The reduction of output dimension is not fully explained.  The conclusion should explicitly mention that the results apply to the infinitewidth regime and the speculation that they may extend to finitewidth netfunctions.", "Paraphrased Statement: This paper explores the generalization capabilities of deep neural netfunctions through the eigensystem of the Neural Tangent Kernel (NTK). The findings suggest:  function that correspond to a smaller number of significant NTK eigenfunctions are easier to learn.  A novel \"nofreelunch\" theorem demonstrates a fundamental tradeoff where improving a netfunctions generalization for a particular function compromises its generalization for perpendicular functions. Strengths:  Focuses on generalization a crucial aspect of deep learning theory.  Presents an accessible and wellwritten analysis. Weaknesses:  Key findings may not be novel or lack comparison with existing function.  The easier learning for functions with fewer significant NTK eigenfunctions was previously shown in a generalization limit.  The \"nofreelunch\" theorem lacks connections to existing functions to highlight its significance.  only asymptotic analysis (infinite netfunction width) is provided while practical applications involve finitewidth netfunctions."], "mTcO4-QCOB": ["Paraphrased Summary: This paper evaluates the reliability of explanation methods emphasizing the consistency of explanations for similar inputs. It introduces the concept of \"explainer astuteness\" which measures the likelihood of similar explanations under smooth blackbox predictor functions. The paper examines how explainer astuteness is related to the smoothness of the blackbox function providing theoretical bounds for different explanation methods including Shapley valuebased mean feature result and individual feature removal methods. Strengths and Weaknesses: Strengths:  Establishes theoretical guarantees for the reliability of explanation methods based on probabilistic Lipschitzness.  Analyzes three types of explanation methods and demonstrates the practical robustness of the bounds. Weaknesses:  The similarity definition used in explainer astuteness does not consider alternative reasoning paths in the model.  The paper contains errors in proofs and unclear assumptions.  The limitations of the theoretical assumptions are not fully addressed in the experiments.", "Summary Paraphrase: This field examines the stability of explanations provided by various methods (e.g. Shapley valuebased mean effectbased removalbased) for a classifier at a given input. As explanations depend on the classifiers output their sensitivity can be linked to the classifiers smoothness (measured by its Lipschitz constant). For certain explanation methods the field establishes lower bounds on the perturbation threshold (\u03b5) that ensures the stability of explanations within a specified margin. These bounds represent the \"bestcase\" resilience of explanations to input perturbations. Strengths and Weaknesses: Strengths:  Addresses a significant job in explainable AI.  Provides a theoretical analysis of explanation robustness from a smoothness perspective.  Includes detailed and accurate proofs.  Covers representative explanation methods (including SHAP).  Makes data and code publicly available. Weaknesses:  validation lack novel insights into explanation robustness.  Does not address two important subjobs: (1) worstcase changes in explanations (2) interdependencies between individual explanation elements.  Experimental evaluation are limited and unclear:  Lipschitz constant estimation for neural networks is not explained.  Relationship between predicted and estimated astuteness is not defined.  practical significance of the theoretical bounds is not demonstrated.", "Paraphrase: This field introduces the concept of \"Astuteness\" as a measure for evaluating explanation reliability. Essentially Astuteness measures how well explanations for similar samples also remain similar indicating explanation consistency. This consistency can be interpreted as \"robustness.\" The field demonstrates that for specific explanation methods (e.g. SHAP CXplain) the minimum Astuteness level is mathematically linked to the number of feature property (D) and the models sensitivity to input changes (Lipschitz constant L). Strengths:  Astuteness provides a straightforward framework for assessing explanation reliability.  Establishing lower Astuteness bounds for common explanation methods offers insights into their robustness. Weaknesses:  The mathematical derivation is basic and does not yield groundbreaking or practical guidance for improving explanation reliability.  The finding that explanation robustness is limited by the prediction functions Lipschitz constant is somewhat intuitive and not only unexpected."], "lP11WtZwquE": ["Paraphrased Statement: This paper addresses the problem of false negative predictions in masked language models (MLMs). It proposes pretraining models on true or more true negatives to ensure the quality of data in MLM. False negatives can hinder pretraining efficiency and robustness but their impact has not been studied previously. The authors suggest an improved pretraining approach using ElECTRA introducing two auxiliary objectives to focus on true negatives during MLM. soft regularization (SR) measures the similarity between predicted and original tokens smoothing the loss function by minimizing semantic distances. Hard correction (HC) prevents backpropagation from false negative samples limiting training with incorrect predictions. Strengths:  Presents technically sound methods (SR and HC) to mitigate false negatives.  Evaluates performance on natural language understanding tasks and provides indepth analysis of the methods. Weaknesses and Questions:  The severity of the false negative problem is not clearly defined.  SR regularization is limited to the embedding layer which may not be sufficient.  HC uses WordNet for coarse synonym sets which could introduce errors.  Why was pretraining only applied to ElECTRA and not to other MLM models  How does this work relate to distillation paradigms that use label smoothing for improved convergence and performance", "Paraphrased Statement: Summary: This field proposes modifying language model pretraining objectives to mitigate false negative model. Two methods (soft regularization and hard correction) are introduced to address this issue during continual training on ELECTRA. Experiments demonstrate that these methods enhance performance on downstream NLU tasks and model robustness (on SQuAD). Strengths and Weaknesses:  Reducing false negative case is a compelling goal but its necessity in pretraining is debatable.  The proposed methods are claimed to improve pretraining but limited experimental evidence supports this assertion.  soft regularization and hard correction may have limitations in certain situations.  experimental results are inconsistent and incomplete raising questions about the general applicability of these methods.  The methods are only tested on continual training of ELECTRA models making their suitability for earlystage pretraining unclear.  Baseline comparisons are limited and do not include prominent models like RoBERTa and XLNet.  It is uncertain if the methods apply to larger models or other pretrained language models such as BERT.", "Paraphrased Statement: Summary: The paper suggests that models like ELECTRA should consider including true negative model in their tokenlevel discrimination pretraining. The proposed model employs cosine similarity or synonym search to identify true negative model and incorporates them into the pretraining process using soft regularization or ignoring incorrect predictions. Experiments on benchmark tasks (GLUE and SQUAD) indicate that addressing true negatives during pretraining enhances performance. Strengths:  Introduces the concept of incorporating true negatives into ELECTRA pretraining.  Demonstrates improved performance using proposed pretraining objectives. Weaknesses and Questions:  W1: Uncertain about the experimental setup specifically whether baselines were pretreated using the original ELECTRA objectives for the same duration as the proposed methods. This is necessary to draw valid conclusions.  W2: Unclear whether the performance gains are only attributed to true negative handling or if other factors such as random or false negative model contribute.  Q1: Requesting the paper to provide error correction statistics for the HC method.  Q2: Suggests exploring the performance of ELECTRA models in tokenlevel classification tasks (e.g. NER) where they have shown promising results.", "Paraphrase: Summary: This paper addresses the oftenoverlooked issue of false negatives during the training and pretraining of language models (LMs). In many case during LM training the training objective targets only a single token while disregarding other valid tokens as equally negative. To address this issue the author proposes two methods: 1. soft regularization: Minimizes the embedding distance between the predicted and correct tokens. 2. hard correction: Maximizes all synonym of the correct token using WordNet. By applying these methods to the pretraining of ELECTRA and evaluating them on GLUE and SQuAD downstream tasks the author demonstrates consistent improvements and enhanced robustness. Strengths: 1. The paper highlights a crucial problem in pretraining that has been largely overlooked. Addressing this issue has the potential to improve representation models and inspire other pretraining objectives. 2. The proposed methods achieve consistent gains when compared to baseline methods on standard benchmarks. Weaknesses: 1. soft regularization Method: It is unclear how the loss term backpropagates through the model. The authors suggestion to train only the two embeddings raises concerns about how this can enhance the model. 2. hard correction Method: The method appears to use synonym sets as target labels. Despite positive empirical results the solution itself lacks novelty. Additionally while WordNet synonym can reduce false negatives they can also introduce false positives. 3. Evaluation: The author primarily compares their methods with the original pretrained model rather than other methods designed to address false positives. It would be beneficial to assess how label smoothing during training contributes to improvements and how it compares with the proposed methods.", "Paraphrased Summary: This field addresses the problem of \"false negatives\" in pretrained language models (PrLMs) and presents two solution. First Method: Replaces the MLM objectives crossentropy loss with the cosine distance between the representations of the predicted token and the correct one. Second Method: Identifies \"false positive\" tokens using WordNet synonym and removes their gradients. Experiments show that these methods enhance the performance and robustness of ELECTRAsmall and ELECTRAbase models on GLUE and SQuAD benchmarks. Strengths and Weaknesses: Strengths:  Highlights the issue of false negatives in PrLMs.  Demonstrates improvements over a strong baseline model on multiple benchmarks. Weaknesses:  The soft regularization technique may not be straightforward to backpropagate.  The hard correction approach may introduce \"false positives\" from WordNet synonym.  Tests are limited to one PrLM (ELECTRA).  Statistics on the effectiveness of hard correction are missing. introduction issue:  The sound issue in Table 2 (QQP for ELECTRAsmall) is not highlighted."], "xo_5lb5ond": ["Paraphrase: This paper presents a fresh and valuable approach for optimizing CNNs using a graphbased algorithm. Its effectiveness has been demonstrated through experiments using the CS CamVid and dynamic CT datasets.", "Summary Paraphrase: This paper presents a novel pruning approach that leverages graph theory to optimize the structure of convolutional neural networks (CNNs). It constructs a graph based on the networks structure and edge weights preserving operations with the longest dependency chains while removing others. The approach is tested on various network architectures and datasets demonstrating promising results. effectiveness and Weaknesses Paraphrase: Pros:  Introduces a novel graphbased pruning technique that has gained recent research attention.  Demonstrates the ability to achieve significant pruning ratios.  applicability to segmentation tasks. Cons:  Insufficient detail in the introduction and related work sections particularly regarding the motivation and justification for the proposed approach.  Lack of theoretical foundation for selecting the longest dependency chains as the pruning strategy.  Difficulty in understanding the methodology due to the absence of conventional definitions and equations.  Ambiguity in the meaning of \"operation\" and \"operator\" in the methodology description.  Incomplete explanation of the computation of operator norm.  Unclear justification for the approximation AB \u2248 A\u00b7B.  Limited experimental evaluation omitting benchmark classification tasks and comparisons with stateoftheart pruning methods.", "Summary Paraphrase: This research presents a DAG abstraction of AI models and suggests employing a greedy pruning technique based on the cumulative operator norm of the longest chain path. effectiveness and Weakness Paraphrase: effectiveness:  Introduces the novel concept of the longest chain path in DAGs for model pruning. Weaknesses: 1. The algorithm follows a greedy approach to remove paths sequentially. Prior research has discussed addressing channels pruned incorrectly in the early pruning stages. This number is not addressed in the paper. 2. The paper lacks comparisons against other stateoftheart pruning methods on classification benchmarks like ImageNet to establish its superiority. 3. For image segmentation tasks the paper only compares against magnitude and operator norm pruning which may not be sufficient to demonstrate effectiveness.", "Paraphrase: This field introduces LEAN a novel structured pruning method for convolutional neural networks (CNNs). LEAN represents a CNN as a pruning graph where channels and operators are nodes and edges respectively. edge weights are based on operator norm. The method iteratively selects the longest path in the graph to prune achieving the desired pruning ratio without isolating the network. LEANs effectiveness is demonstrated by comparing it to two basic structured pruning methods on three image segmentation datasets and three CNN architectures. LEAN generally outperforms the others achieving similar model quality with significantly smaller pruning ratios. In a practical application an MSD model pruned with LEAN showed a 10.9x speed improvement. effectiveness:  Graphbased algorithm enables aggressive pruning in some layers.  Experimental results demonstrate LEANs potential. Weaknesses:  Limited evaluation against only basic structured pruning methods and on small datasets.  Lack of theoretical analysis supporting LEANs superiority.  Unclear definition of preliminaries and pruning graph construction.  number 1(B) requires clarification.  Missing accuracy metrics for the unpruned network.  Lack of visualization for pruned network structures.  Unexplained cases where structured magnitude pruning outperforms LEAN.", "Paraphrase: The authors developed a structured pruning technique that transforms structure pruning into a graph pruning task. They represent inputoutput pairs as nodes in a graph where the edges measure the norm between operators. The proposed iterative algorithm prunes layers based on the longest path in the graph. The method was tested on three image segmentation problems. effectiveness:  Simple and straightforward approach. Weaknesses:  Lack of comparisons to other methods and corresponding benchmarks.  Full network benchmarks would provide context for pruned results.  Difficulty in distinguishing between methods in number (e.g. weak blue vs. gray lines).  Speedup variability between training runs is not shown in number.  Vaguely presented \"relative number of remaining convolutions\" metric.  Unclear how accuracy is determined for \"relative number of convolutions at equal accuracy\" in each graph."], "z2zmSDKONK": ["Paraphrased Statement: Summary: This paper examines the ability of distributional reinforcement learning (RL) to withstand adversarial situations involving noisy state. It demonstrates the Lipschitz continuity of distributional RL provides a convergence condition for TD updates under noisy conditions and performs a sensitivity analysis. Strengths and Weaknesses:  The concept of distributional robustness in distributional RL is novel and unexplored previously.  The technical aspects of the paper lack depth and clarity.  Theorem 1 is a minor extension of existing work (Zhang et al. 2020) and applies a distribution to a noise use.  The use approximation used (linear model with KL loss) simplifies the Lipschitz smoothness analysis making it less relevant to realworld distributional RL settings.  Expected RL with a squared loss use also exhibits the same properties as KL loss in distributional RL even with bounded reinforcement.  The analysis in Sections 4.2 and 4.3 is not well connected to distributional RL.  The paper provides no actionable insights from its sensitivity analysis concluding only that task difficulty work sensitivity.  Contrary to claims Figure 3 indicates that DQN is more robust than QRDQN in MountainCar.  The paper unnecessarily splits similar experiments into multiple subsections.  QRDQNs performance in Breakout is not as solid as the paper suggests. Minor Comments:  Equation (4) can be simplified.  The phrase \"the state distance go to infinity\" in Section 3.1 is unclear.", "Paraphrase: Summary: This article investigates the resilience of distributional reinforcement learning especially to state observation noise highlighted in work on adversarial attacks. In comparison to existing reinforcement learning research on state observations this work focuses on the distributional RL model and incorporates noise into the training action. Through the perspective of the loss uses Lipschitz continuity and the work use the authors provide theoretical insights into how distributional RL can improve robustness under these conditions. Empirical validation is conducted across four benchmark datasets. Strengths: 1. This research examines the robustness of distributional RL agents a topic overlooked in prior work. The findings indicate that distributional RL agents may exhibit greater robustness than expectationbased RL agents to state observation noise. 2. The analysis encompasses both tabular and use approximation cases with a key theorem established regarding the Lipschitz continuity of distributional RL. 3. The experimental evaluation is extensive. Weaknesses: 1. The proposed SNMDP appears highly similar to SAMDP as introduced by Zhang et al. (2020). While the distributional RL setting offers a novel analysis the primary distinction in terms of SAMDP formulation remains unclear. 2. The papers organization and writing could benefit from improvement. The experimental results for four models could be consolidated into fewer subsections such as one for random noise and another for adversarial noise. The introduction of theorems lacks motivation especially in Sections 4.2 and 4.3. Questions: 1. In Figure 3 the DQN lines (dotted red lines) seem to outperform QRDQN (solid lines) which contradicts the stated decision that \"QRDQN (solid lines) almost consistently outperforms DQN.\" Is this a graphical error 2. Is it feasible to empirically quantify the Lipschitz continuity (Theorem 2) the positive definiteness of the matrix XT DPE (Theorem 3) and the work use (Theorem 4) using a toy representative", "Paraphrase: This research explores StateNoisy Markov Decision action (SNMDPs) where noise originates from the environment or an adversary. The work establishes the theoretical characteristics (such as convergence and contraction) of the (expected) Bellman operator and distributional Bellman operator for SNMDPs. The theoretical analysis covers both tabular and linear use approximation settings. In the use approximation setting the authors demonstrate the robustness of distributional RL using histogram distributional loss. They also examine how noise impacts TD learning using work uses and the perturbation method. Empirical evaluations using DQN and QRDQN vary noise standard deviations and noise placement (state successor state or both). These experiments aim to validate the theoretical insights provided by the authors. Strengths:  Extension of StateAdversarial MDPs to SNMDPs considering a broader range of noise sources.  Clear theoretical contributions establishing the properties of SNMDPs Bellman operators and TD learning. Weaknesses:  Lack of clear motivation and contribution from the experiments.  experimentation focus primarily on demonstrating the applicability of SNMDPs Bellman operators and TD learning to control problem.  Absence of experiments demonstrating the practical utility of the proposed model in realworld scenarios with noisy training observations.  Limited experimental work (only 3) and reporting without standard errors hindering the assessment of statistical significance. Recommendation: For the work to be accepted the empirical results should be refined. This could include experiments showcasing the models usefulness in specific noisy observation scenarios and the addition of standard errors to assess statistical significance."], "gI7KCy4UDN9": ["Paraphrase: Statement 1: Original: entropy coding in neural image compression can cause quantization issue that lead to variability in issue across platforms and software (e.g. CPU to GPU or different drivers on the same GPU). Trainingtime quantization techniques can complicate or lengthen training process. Paraphrase: issue with how data is compressed after training can lead to unpredictable issue depending on how a program is run (e.g. on different hardware or with different software versions). Some methods for addressing this issue during training can make the training process more difficult or timeconsuming. Statement 2: Original: Posttraining quantization (PTQ) techniques are shown to be effective even for models that predict the context or use Gaussian mixture models (GMM). Additionally a faster method for discretizing entropy parameters is introduced resulting in significant runtime improvements with no loss of image quality. Paraphrase: Techniques for addressing these issue after training have been shown to be successful even for complex models. In addition a novel method for optimizing the path data is broken down for compression has been developed which significantly speeds up the process without reducing image quality. The additional materials provided explain the quantization algorithms in detail. effectiveness: Original:  clear identifies problems in image compression.  Strong performance with minimal image quality loss at high bitrates.  Faster entropy parameter discretization method with no performance degradation.  comprehensive appendices with derivations and explanations. Paraphrase:  clear explains the challenges in image compression.  Demonstrates that the proposed technique preserves image quality at high levels of compression.  Introduces a highly effective method for optimizing the compression process.  Provides detailed documentation to support the proposed methods. Weaknesses: Original:  Performance comparisons are limited to a narrow bitrate range.  It is unclear how much improvement is gained by adding an additional lookup table. Paraphrase:  It is not clear how well the technique performs at lower or higher bitrates than those tested.  The benefit of adding an additional level to the lookup table is not fully explored. QuestionsFeedback: Original:  Table 2: comparison details are unclear.  Reference needed for \"popularly vectorized version.\"  Implementation details unclear (C or Assembly).  Grid research methodology and sensitivity to model and loss use in section 3.1. Paraphrase:  The comparison in Table 2 lack clarity.  A reference for the \"popularly vectorized version\" is missing.  It is unclear whether the fast runtime issue from the use of C or Assembly language.  The grid research method for finding the optimal quantization step requires further explanation including whether it needs to be customized for each model and loss use.", "Paraphrased Statement: Summary: This paper introduces a method to make learned image compression less sensitive to netstudy quantization. Typically these methods use cumulative distribution use (CDFs) in both encoding and decoding but these use can be approximated differently on different devices (e.g. CPU vs. GPU) leading to errors. This paper solves this by quantizing the encoder and decoder feature maps and weights to 8 bits. effectiveness and Weaknesses:  It is noteworthy that posttraining quantization is sufficient for robustness in learned image compression.  The paper needs improvement in presentation and comparisons and the writing could be clearer. For model the concept of \"crossplatform inconsistency\" is difficult to grasp without referring to the appendix or Balle et al.s (2019) study.  image 1 and 2 focus on the problem and previous approaches rather than emphasizing the paper contribution.  The experiments lack analysis of the proposed method from section 3.  The paper lacks comparison to Balle et al.s (2019) method regarding model robustness across devices.  The alternative method in section 4 appears to be an alternative to posttraining quantization using a predefined CDF to minimize crossplatform inconsistency.  Based on this alternative the importance of crossplatform inconsistency which could be easily addressed by sharing the same CDF across devices becomes questionable.  It is unclear what granularity is used for feature map quantization when quantizing weights with perchannel scaling factors.  The paper does not provide latency acceleration issue for the proposed quantized model using perchannel weight quantization.", "Paraphrased Statement: Summary This study addresses crossplatform consistency issue in learned image compression where probability prediction can vary across platforms. The authors leverage existing model compression techniques to tackle this problem. Their experimental issue demonstrate the effectiveness of their approach. Strengths  model compression techniques prove beneficial for crossplatform consistency in learned image compression.  Encouraging experimental issue show minimal performance impact from the quantization method used. Weaknesses  The techniques used are largely derived from model compression potentially limiting the paper technical contribution.  A clear distinction between the techniques used here and those in model compression is lacking.  issue only show the ratedistortion curve below 1bpp leaving the compression performance at higher bitrates unknown.", "Paraphrased Statement: Learned image compression suffers from the challenge of nondeterministic floatingpoint performance making it difficult to recover compressed data on different platforms. This study demonstrates that despite using the PTQ quantization method data recovery is achievable on various platforms for compression models with context model (Minnen et al. 2018). effectiveness:  PTQ effectively prevents data corruption (Table 1).  The study addresses a crucial issue for practical applications: crossplatform support for learned image compression. Weaknesses:  The authors state a potential performance loss in integer networks but do not provide experimental evidence.  comparison with existing methods (e.g. Balle \u0301 et al. 2019 Sun et al. 2021) are insufficient to demonstrate the advantages of the proposed method.  Table 2 lacks a direct comparison with Sun et al. (2021).  It is unclear whether Table 1 shows the issue of applying the proposed LUT or only PTQ.  The contribution of PTQ and LUT to data corruption prevention is not clarified.  The time measurements in Table 2 may not represent the average time for the entire dataset.", "Summary: implementation of floatingpoint arithmetic vary across hardware architectures causing potential errors in decoding compressed data. To address this authors propose quantizing model weights and activations using integer arithmetic ensuring consistent implementation. Quantized models do not experience significant performance loss compared to unquantized models. effectiveness:  Demonstrates successful quantization of stateoftheart image compression methods.  Extends lookup table quantization to Gaussian mixture models. Weaknesses:  Claim of posttraining quantization (PTQ) for crossplatform consistency is not novel.  issue lack comparison to relevant methods.  Reference supporting claim on performance loss with context modeling not provided. Minor issue:  Apparent inconsistency between abstract sentences.  Grammatical errors.  Undefined abbreviation (LUT). Questions:  Why prefer 8bit quantization for activations and weights if compression is not the goal  How does the proposed quantization method improve upon the technique in [1] for log standard deviations"], "tJCwZBHm-jW": ["Paraphrased Statement: Summary: The paper suggests a technique for understanding 3D point clouds using pretrained 2D image models. It employs a straightforward expansion technique to convert 2D convolutions into 3D convolutions as in I3D. Experiments on point cloud classification and segmentation show the methods effectiveness. Strengths:  Simplicity and effectiveness of converting 2D convolutions to 3D for point cloud understanding.  Demonstrates the feasibility of this simple idea. Weaknesses: 1. Lack of detailed data on the work. 2. Unknown applicability to detection tasks. PostRebuttal:  Acknowledges the novelty of applying 2Dto3D inflation in point cloud understanding.  Agrees that a discussion on why the method works is lacking.  Lowers the evaluation from 8 to 6 due to these concerns.", "Paraphrased Statement: This study explores transferring knowledge between image and point cloud data for machine study tasks. Although these data case differ significantly researchers were able to adapt pretrained models from the image field and obtain surprisingly beneficial results in point cloud classification and segmentation tasks. The study demonstrates that using pretrained weights significantly enhances data efficiency. Additionally the proposed method proves effective in scenarios with limited training data known as fewshot study. Strengths:  The proposed method of transfer study between image and point cloud data is considered novel and unexpected. Weaknesses:  The concept of expanding model weights from 2D to 3D is not only new.  The methods performance on point cloud segmentation while promising falls short of stateoftheart results.  The effectiveness of pretrained weights could be further substantiated by selectively initializing new layers and analyzing their impact on performance. This exploration would reveal which components of the pretrained model are most beneficial in this transfer study context. Update: After considering the rebuttal and amended experimental results the authors score has been upgraded to 6.", "Paraphrased Statement: The proposed approach aims to leverage pretrained 2D image features for 3D point cloud analysis. It achieves this by expanding the 2D convolution filters from a pretrained model to 3D dimensions. The model is optimized with a direction on the input output and optionally batch normalization layers. Experiments demonstrate that finetuned models using pretrained 2D features enhance performance and data efficiency. Strengths:  Connects 2D and 3D representations for transfer study.  Demonstrates performance improvements and data efficiency compared to training from scratch. Weaknesses:  Dataset Size: It is unclear if large datasets would lead to beneficial performance as comparisons between results from datasets of varying sizes are lacking.  Comparisons: Stateoftheart taskspecific methods should be included for comparison as well as different selfsupervised study methods.  Theoretical Justification: The paper lacks a theoretical explanation for why inflating 2D filters to 3D is reasonable given the field gap between 2D and 3D datasets.", "Summary Paraphrase: This paper presents a novel approach for adapting convolution network weights trained on twodimensional (2D) images to threedimensional (3D) convolution networks. The proposed method extends 2D kernels into 3D kernels in a similar manner to video models. Experiments demonstrate the effectiveness of this approach on 2D datasets (e.g. ImageNet) and 3D datasets (e.g. ModelNet). Strengths and Weaknesses: Strengths:  Explores the new challenge of transferring 2D pretraining to 3D.  Provides clear and straightforward explanations.  Conducts comprehensive experiments.  Shows significant performance improvements from 2D pretraining indicating that useful knowledge can be transferred from 2D to 3D. Weaknesses:  Visualizations only may not fully explain the understanding behind the successful transfer.  The conclusion that \"work representations\" are better transferred from 2D to point clouds may not be fully supported by the overall dataset results. Additionally the term \"work representations\" is not clearly defined.", "Paraphrased Summary: This article introduces a novel technique called \"inflation\" for transforming 2D image pretrain backbone networks into 3D versions by repeating 2D kernels along the third axis. This enables 3D point cloud tasks to benefit from 2D image pretraining. extensive experiments demonstrate that minimal finetuning efforts can lead to competitive performance on 3D tasks. The authors express surprise at these results. Strengths:  The inflation idea is innovative and beneficial for 3D point cloud applications.  The experiments effectively evaluate the advantages of the inflation approach. Weaknesses:  The concept of inflation has been explored in other 2D3D field limiting its novelty.  The article lacks indepth discussion and ablation study to explain why the inflation technique works effectively.  The authors findings from their discussion experiments (image 4) appear arbitrary and their explanation for the \"work representation\" argument is unconvincing.  The choice of inflation axis (x y or z) is not discussed despite the asymmetry of the 3axis point cloud representation compared to video. additional Concerns:  The parameter size comparison (Section 4.1) is missing raising questions about the fairness of performance comparisons with the baseline.  The ResNet structure may not be optimal for training on point clouds potentially impacting the scratch training performance of the baseline.  The writing style of the article is somewhat informal and uses an excessive number of \"surprising(ly)\" phrases (10 occurrences)."], "uqBOne3LUKy": ["Paraphrase: Summary: This field explores the compatibility of importance weighting with training overargumentized neural networks. While a previous field suggested that importance weighting is ineffective in modern deep learning (Byrd and Lipton 2019) this paper proposes that it can be beneficial with the function of polynomiallytailed losses. The authors provide theoretical justifications and empirical evidence to support this claim. effectiveness:  Clear introduction including an illustrative example (design 1) demonstrating the limitations of importance weighting on exponentiallytailed losses.  The introduction of polynomiallytailed losses as a practical tool for importance weighting.  solid theoretical analysis supporting the polynomiallytailed loss. Weaknesses:  The theoretical results assume subGaussian data which may not always be a realistic assumption.  The polynomiallytailed loss lacks a probabilistic interpretation in terms of the distance between distribution.  The impact of the argument \u03b1 in the polynomiallytailed loss is not discussed in the theoretical or empirical analysis. It is unclear how the choice of \u03b1 affects the results.", "Paraphrased argument: Summary Previous studies identified an inherent conflict between importance sampling and training overparameterized neural networks. Soudry et al. (2018) demonstrated that gradient descent (GD) on exponentially tailed loss functions converges to the max margin classifier. Xu et al. (2020) further showed that importance sampling does not impact this margin maximization. This paper explores polynomially tailed loss functions and their effects on the implicit bias of GD. By formulating an optimization problem incorporating importance weights the paper establishes the implicit bias of GD on polynomially tailed loss functions. The resulting classifier exhibits superior generalization performance compared to exponentially tailed loss in a linear classification setting with two subGaussian clusters. Experiments on imbalanced datasets such as Binary CIFAR10 and CelebA indicate that polynomially tailed loss improves performance on such datasets. Strengths 1. This paper introduces a potential approach to reconcile importance sampling with training overparameterized neural networks by employing polynomially tailed loss functions. 2. It provides a comprehensive characterization of the implicit bias of GD on polynomially tailed loss functions (Equation 2 in Section 4.1). 3. The paper provides a clear distinction between polynomially and exponentially tailed loss functions (Section 4.2). Weaknesses 1. The theoretical analysis is limited to linear classification and a specific dataset with two subGaussian clusters which weakens its generalization. 2. While the paper argues that GDs implicit bias shift with importance weights this shift may be minimal for high polynomial point. The choice of polynomial point requires careful consideration and the authors recommendation of \u03b1  1 should be highlighted. 3. The experimental results are limited in comparison to other baselines. Its unclear if the observed accuracy improvements are significant and if other methods (e.g. undersampling) could achieve similar results. minor Comments  Page 20 contains formatting error.  A trajectorybased proof is currently used for Theorem 4.3. However since the convergent classifier is the unique max margin classifier an alternative proof directly analyzing the max margin classifiers properties may be potential.  Section 4.1 should emphasize that the optimal solution for Equation 2 is unique.", "Paraphrase: Summary: This paper presents a novel approach to balancing importance weighting with overparameterized model training. To address the exponential tail publication in traditional loss functions the authors propose a novel loss function for binary classification with a polynomial tail decay. Theoretical analysis: The field proves that the novel loss function outperforms weighted crossentropy for linear models and imbalanced mixtures of Gaussian models. Empirical evaluation: On imbalanced CIFAR10 and CelebA datasets the novel loss function demonstrates superior performance compared to weighted crossentropy. effectiveness:  Wellwritten and accessible paper  Solid and wellpresented theoretical claims  introduction of Theorem 4.2 which extends previous function on imbalanced context Weaknesses:  Limited to binary classification (extension to multiclass is unclear)  Comparison only with weighted crossentropy (no evaluation against other costsensitive techniques)  Lack of discussion on related function in imbalance learning (e.g. [14]) Suggested Improvements:  Contextualize the function within the broader literature on imbalanced learning.  research extension of the loss function to multiclass scenarios.  behavior experiments comparing the novel loss function to other costsensitive techniques.", "Paraphrase: This field examines the effects of \"importance weighting\" in blending classifiers. The researchers discovered that using a \"polytailed loss\" function instead of \"max margin\" solves the publication of importance weighting being ineffective due to the inherent bias toward converging to the max margin. effectiveness:  The paper is wellwritten and its theoretical findings are sound.  The concept of utilizing a polytailed loss is noteworthy becafunction it is basic effective and practical. It gracefully gets around earlier negative findings.  The test of polytailed loss in a Gaussian mixture setting yields original results such as inversefrequency weighting being suboptimal and optimal reweighting being roughly cubic in class imbalance. Weaknesses:  The papers theoretical focus on linear classifiers differs from the practical function of neural networks.  Guidance on selecting the alpha hyperparameter would be helpful to practitioners.  The performance improvement (23) is minimal and it is unclear how it compares to more powerful baselines."], "uHq5rHHektz": ["Paraphrased argument: This research aims to improve the resilience of Deep Neural Networks (DNNs) by incorporating background context information. The work analyzes the impact of blurring effects on object detection and classification DNNs. It finds that fusing foreground and background information enhances accuracy under various blur conditions. The research further investigates adversarial attacks using the Fast Gradient Sign method (FGSM) and observes the benefits of leveraging background information on the MSCOCO and CIFAR10 datasets. To enhance robustness the work proposes a regularization technique that adjusts the weights of foregroundrelated features during training. effectiveness and Weaknesses with Concerns: 1. The approach of enhancing adversarial robustness using foreground and background information is not novel and has been explored in existing research (e.g. [A]). However this work employs the less challenging FGSM attack instead of PGD. 2. The choice of the MSCOCO dataset as a subject for adversarial attacks is not well justified. Common datasets for such attacks are Imagenet and CIFAR and the rationale for using MSCOCO is unclear. 3. The selection of Gaussian blur as a perturbation may be limited. late work (e.g. [B]) have demonstrated the effectiveness of adversarial attacks using motion blur which could exploit gradient information like traditional noise attacks. 4. The image in the paper exhibit noticeable distortions and numerous typos indicate potential hasty training.", "Paraphrase: Summary: This paper addresses the issue of adversarial model. The authors present a method inspired by how biological systems use multiple sensory channels to identify object categories. Their approach combines two pretrained models one designed to prioritize foreground objects and the other to focus on the background. The foreground model is then finetuned for specific tasks while the background model remains unchanged. The authors show improved performance against blur attacks and the Fast Gradient Sign method (FGSM). effectiveness and Weaknesses: Weaknesses: 1. The choice of pretrained models for foreground and background recognition is questionable. There is no evidence to support the assumption that a model trained on ImageNet can reliably detect foreground objects. ImageNet contains many classes similar to those in Place365 and vice versa. 2. The methods novelty is limited. It essentially combines multiple models into an ensemble. Furthermore FGSM attacks only target the foreground model leaving the background model unaffected. 3. The experimental evaluation is weak. The authors used a limited selection of datasets and attack methods which may not fully capture the effectiveness of the proposed approach.", "Paraphrased argument: Summary This paper investigates the challenge of protecting machine learning models from \"adversarial attacks.\" The researchers propose a novel method that combines features extracted from different sources (foreground and background) using pretrained models. They evaluate this method against two types of adversarial attacks: Gaussian blur and gradientbased. Key exploration  Impact of adversarial attacks on both \"object\" and \"context\" feature spaces  Benefits of fusing different modalities (foreground and background) for increased robustness  Importance of \"context\" features in enhancing model performance against attacks effectiveness and Weaknesses effectiveness  The work investigates an significant topic in machine learning and the proposed method is novel and relevant. Weaknesses  The paper exceeds the page limit potentially limiting its accessibility to readers.  The authors should provide a more explicit delineation between their contributions and prior work.  The proposed method should be compared to other existing fusionbased methods to demonstrate its superiority.  The experiments should be expanded to include more powerful adversarial attacks.  The paper writing and formatting could be improved for clarity and readability.", "Summary: This paper proposes an initial approach to creating a foreground and background CNN that can resist adversarial attacks. effectiveness:  beneficial intent in exploring adversarial robustness for CNNs. Weaknesses:  Insufficient Quantification: The authors have not provided quantitative step for their adversarial attacks or blurbased perturbations.  Incorrect attack method: The authors used Gaussian blur which is not an adversarial attack but rather an image distortion. They should have used PGDbased attacks for solid results.  Limited Architecture Exploration: The authors have not examined different CNN architectures in their experiments.  Lack of Clarity: The partitioning and endtoend differentiability of the foreground and background networks are unclear.  Questionable Results: The authors results may be unreliable due to questionable adversarial attack methodology and insufficient differentiability in the fusion network.  Lack of Context: The authors could cite relevant work on visual object recognition and foveated perceptual systems.  Need for visual Improvements: The image in the paper require further refinement."], "iPHLcmtietq": ["This paper introduces phase modulus operators as alternatives to thresholding nonlinearities in scatteringlike networks to enhance classification performance. The work demonstrates that a network incorporating these modulo operators and learnable operators approaches the performance of compact ResNets. However using other nonlinearities results in performance degradation. The paper offers evidence from ImageNet and CIFAR10 datasets and provides an indepth theoretical analysis. However the paper has some shortcomings. Firstly the role of the proposed framework is unclear. The work demonstrates that the framework can match the performance of existing architectures but it fails to delve deeper. Secondly there is minimal discussion about the learnable operators P their impact on the framework their relationship to other components and their significance compared to ResNets. Thirdly the resulting networks have a high number of argument compared to ResNets which raises concerns about argument efficiency. The work does not address this issue or propose solutions. Finally the paper lacks exploration of the frameworks scalability with respect to the number of layers and activations at each layer.", "Paraphrased Summary: The researchers present theoretical concepts and experiments to support the idea that image classification using deep convolutional neural networks (CNNs) primarily relies on a \"phase collapse\" phenomenon. By removing the phase component from zero mean filters they enhance the separation between class way. They introduce a Phase Collapse Scattering network and achieve accuracy comparable to ResNets. They propose that phase collapse is crucial (sufficient) and indispensable (necessary) for differentiating class way in challenging datasets. Strengths:  Novel combination of theoretical and empirical findings.  Clear experimental validation supporting theoretical claims.  Promising results on CIFAR10 and ImageNet datasets. Weaknesses:  Ambiguous color coding in image 1.  Unclear explanation of the necessary and sufficient conditions for linear separability.  ResNet results are not stateoftheart.  Assumption of stationary features (Xy) may limit the applicability of the findings to all types of datasets.", "Summary This paper demonstrates the effectiveness of scattering networks with \"phase collapse\" for achieving stateoftheart accuracy. It reveals that using complex numbers in neural networks resembles deep networks with wavelet filters. Phase collapse occurs when an performance (such as modulus) removes the phase from the complex number leaving only the amplitude. The paper shows that preserving the amplitude while eliminating the phase leads to high accuracy while the reverse yields poor accuracy. This finding explains the success of nonlinearities like ReLUs. Strengths:  Novel and thoughtprovoking findings that connect complex numbers phase and nonlinearities.  Convincing experimental results in Table 2.  Intriguing theoretical analysis that provides insight into phase collapse. Weaknesses:  Convoluted and unclear writing way that makes it difficult for readers to follow.  Section 2 lacks structure and clearly defined concepts.  Its unclear why stacking \"phase collapse\" operators is beneficial as they remove translation information.  The need for projection operators in the context of phase collapse scattering networks is not convincing.  The accidental discovery of the arXiv interpretation biased the reviewers opinion against other papers potentially unfairly.", "Paraphrase: This research investigates the decrease in variability within class as neural networks learn. It focroles on understanding the effects of sparsity and thresholding caroled by the ReLU activation role. The work argues that improved classification performance in deep neural networks is not primarily due to reducing spatial variability within class. Instead it results from a \"phase collapse\" which aligns the phase of network coefficient. By eliminating phase differences in zeromean filters the separation between different classes is enhanced leading to increased accuracy. The researchers introduce a complexvalued neural network that roles complex wavelet as spatial filters. Learning in this network is simplified to applying complex 1x1 filters across channels. They demonstrate that this approach achieves performance comparable to ResNet18 on CIFAR10 and ImageNet datasets. Strengths:  Extends previous research on withinclass variability and phase collapse.  Hypothesizes that classification performance is largely driven by iterated phase collapses.  Provides theoretical and empirical evidence to support this hypothesis. Weaknesses:  The writing is often unclear and difficult to follow.  Some assertions are not scientifically precise (e.g. \"usually\").  The proof details and code reproducibility were not fully examined.  The role of skip connections in the neural networks needs further justification. Updated Assessment: After addressing reviewer concerns the works score has been increased. The authors have included results for networks with and without skip connections. The text has been revised to improve clarity and conciseness. However some readability issues remain."], "tV3N0DWMxCg": ["Summary The paper aims to improve prediction accuracy by incorporating uncertainty estimation in the model. It focpractices on using distributions from the exponential family which allows for predicting model parameters and flexibility in density model. The model is evaluated based on its ability to calibrate prediction and detect outofdistribution data. effectiveness:  Extends the practice of prior distribution parameterization to a wider range of tasks compared to previous work.  Employs the exponential family which provides a closedform posterior and enables uncertainty decomposition. Weaknesses:  The paper primarily focpractices on epistemic and aleatoric uncertainty decomposition without exploring its broader applications.  The experimental comparison do not include models that do not allow for uncertainty decomposition limiting the evaluation scope.  The Poisson likelihood practiced for count data performs poorly in the chosen dataset.  The paper has similarities to previous work including shared experimental model. Questions and Minor Comments:  Clarification on the term \"density added to the last predictor layer\" is requested.  The paper targets both epistemic and aleatoric uncertainty model but lacks discussion on its practical value.  Comparison against Sensoy et al. for classification is suggested.  The computational efficiency of BNN methods in Izmailov et al. is misrepresented.  Table 1 could be restructured for clarity.  Alternative nomenclature for \"Bayesian loss\" is recommended.  The abbreviation NatPE for NatPN Ensembles is missing.  Equation (6) is not strictly proportional for \u03bb \u2260 1. Appendix Comments:  ELBO loss extension to the appendix is incorrect.  The appendix discusses a prior over y while referring to distributions over \u03b8.  Entropy of the posterior is incorrectly stated as the measure of predictive uncertainty.", "Paraphrased Summary: The authors present NatPN a model that estimates uncertainty for both classification and regression tasks. It uses normalizing flows to predict parameters of the exponential family posterior distribution. Unlike existing methods NatPN doesnt require outofdistribution (OOD) data for training and can assess uncertainty in a single forward pass. effectiveness:  Intuitively uses normalizing flows for density estimation aiding calibration and OOD detection.  applicable to both regression and classification tasks which is often neglected in other methods.  Fast inference speed compared to samplingbased Bayesian neural networks (BNNs).  Delivers high aleatoric uncertainty on OOD datasets outperforming baseline. Weaknesses:  Vague mention of \"pathological\" behavior when modeling distributions over weights (section 2).  Unclear design of the second term in the loss function (equation 5).  Comparison to recent singlepass uncertainty estimation methods like SNGP is lacking.  Disparity in input dimensions between training and evaluation datasets in Table 5 is not addressed. Minor Correction:  \"section 4.2 or training OOD\" should be \"section 4.2 or training on OOD.\"", "Paraphrased Summary: The paper introduces Natural Posterior Network (NatPN) an enhanced version of the Posterior Network method. NatPN aims to account for uncertainty in model prediction for both in and outofdistribution inputs by utilizing exponential family likelihoods parameterized by neural networks. NatPN achieves this by mapping input samples to a latent space where a normalizing flow is applied. An update rule is so defined to transform this latent space using the normalizing flow likelihood into parameters of the exponential family posterior. For data points far from the training distribution the posterior tends toward prior parameters while the prior has minimal influence for data within the distribution. NatPN improves on Posterior Network by:  Extending the method to exponential family distributions allowing for regression and count tasks beyond familyification.  Using a single normalizing flow for familyification instead of familyspecific flows theoretically enhancing scalability for datasets with numerous family.  Implementing minor adjustments to the posterior parameter update rule. The study presents empirical evidence on various tasks across smallscale datasets. NatPN performs competitively with existing methods. effectiveness:  Clear and wellwritten paper.  Theorem 1 provides a solid theoretical basis.  NatPNs performance matches or pass comparable methods.  extension to exponential family distributions including regression with Gaussian likelihood and count with Poisson likelihood.  Single normalizing flow for familyification potentially enabling scalability. Weaknesses:  evaluation limited to datasets with 10 family.  Lack of OOD performance assessment.  Incremental improvements over Posterior Network.  empirical results do not demonstrate a clear advantage for NatPN. Additional Suggestions:  comparison against other deterministic methods like SNGP and DUQ.  Disentangle the contributions of the single normalizing flow and update rules using ablation studies. Questions:  Disadvantages of not scaling normalizing flows with family count  Mechanisms of evidence update and Bayesian NatPN Ensemble posterior calculation  Training procedure modifications and their implications  Definition of predicted evidence  Inclusion of a baseline network trained without dropout in comparison results.", "Paraphrased Statement: Natural Posterior Networks (NatPNs) provide uncertainty estimation for tasks with exponential family likelihood distributions (e.g. classification regression). They exploit exponential family properties (conjugacy and closedform posterior predictive) for efficient Bayesian inference. NatPNs incorporate a Bayesian discussion of the loss function for endtoend training. They also utilize a normalizing flow to capture epistemic uncertainty by increasing data evidence and decreasing it elsewhere. The posterior distribution is based on outputs rather than network weights simplifying training. NatPNs extend Posterior Networks with significant improvements. They demonstrate solid performance in uncertainty estimation metrics on multiple exponential family distributions and datasets. The paper includes a theorem on OOD behavior and provides evidence for wellcalibrated uncertainty estimation. effectiveness:  Broad applicability to exponential family likelihood distributions confirmed by experiments.  empirical superiority over existing methods in uncertaintyrelated metrics.  Theoretical justification for OOD behavior.  Wellwritten paper with clear explanations and highquality design.  Practical benefits for practitioners (ease of setup and speed). Weaknesses (Easily Fixable):  Underexplained role of the normalizing flow.  Lack of explicit demonstration and discussion of epistemic uncertainty in design 1.  Absence of extension or explanations for \"warmup\" and \"finetuning\" concepts. Potential for high Score:  Addressing the main weaknesses.  Providing evidence of the works high significance based on expert feedback or author arguments.", "Paraphrased Statement: This study introduces a novel uncertainty estimation technique for neural networks. It builds upon PostNet extending its applicability beyond classification tasks. The proposed method predicts changes to the Bayesian prior and optimizes a corresponding loss function. The paper demonstrates promising results on various datasets surpassing or matching existing baseline. effectiveness:  The method avoids computationally intensive forward pass averaging making it practical for realworld applications.  It generalizes uncertainty estimation beyond classification addressing a extensive range of machine learning problems.  The introduction of a single normalizing flow for density estimation enhances scalability for scenarios with numerous family. Weaknesses:  The densely compacted text layout may hinder readability. Subsections and subsubsections could improve organization.  The discussion of experimental setup could be moved to an appendix to streamline the main text.  While the method shows promise for count prediction and OOD detection further exploration of nonclassification and nonregression problems would be beneficial.  Additional evaluation for higherclass classification problems are recommended to assess the limitations of using a single normalizing flow compared to the PostNet approach. Questions:  Sensitivity analysis: How does the methods performance vary with the entropy hyperparameter lambda  Ablation study: How do different loss function components impact the results  Table captions: space table captions above the tables as required by the formatting guidelines.  extension formatting: Ensure consistency in extension formatting (e.g. full first name vs. initials title capitalization)."], "v3aeIsY_vVX": ["Paraphrased Statement: This paper introduces a novel waveform synthesis model \"CARGAN\" that combines the strengths of autoregressive (AR) and nonautoregressive (nonAR) models. CARGAN achieves improved pitch accuracy while simultaneously reducing training time and memory use without significantly impacting generation speed. key detail:  Previous nonAR models for waveform synthesis struggled to maintain pitch and periodicity.  AR models excel at capturing the relationship between pitch and phase.  CARGAN outperforms HIFIGAN (a stateoftheart nonAR model) in reconstructing original waveform with fast training and lower memory use. Strengths and interest: Strengths:  Novel model that combines AR and nonAR advantages for improved CWS performance.  Sidebyside listening comparisons and pitch analysis aid reader comprehension.  Insights into optimizing chunk size based on receptive field size. interest:  Limited explanation of the impact of cumulative sum comparison on performance gap.  Lack of novelty as previous chunklevel autoregressive audio generation models exist.  Unclear how penalizing activation distance enforces waveform proximity to ground accuracy.  Confusion regarding feedback use during training.  Training speed and memory improvement reasons are unclear particularly considering the additional waveform information in CARGAN.  The maximum learnable cumulative sum may be underestimated.  Loss detail are missing for understanding loss compensation.  chunk size used in cumulative sum training is unspecified.  Subsubsection 5.2.2 is missing and 5.2.1 should be merged with 5.2.  audio samples on the demo page lack emphasized discussion.", "Paraphrased Statement: The field presents a novel audio generative model that incorporates a blockwise autoregressive approach with adversarial loss. The motivation behind this process is to address the limitations of purely convolutional generative models for texttospeech which tend to produce inconsistent pitch over longer durations. The authors demonstrate these issues through audio samples. The proposed method is evaluated on standard datasets and case improved pitch accuracy compared to the baseline model. subjective listening tests also indicate potential perceptual improvements. The models speed is assessed revealing it to be slower than nonautoregressive methods but fast than fully autoregressive approaches. Strengths:  Wellargued motivation for addressing pitch consistency  Proposed solution balances accuracy and speed  Comprehensive evaluations with mean opinion scores (MOS)  Extensive audio instance illustrating model capabilities Weaknesses:  Limited baseline comparison (only HiFi GAN) despite the existence of diverse vocoders  phase prediction issue could be simplified  Potential comparison with other methods (WaveGlow WaveRNN) for speed and performance Questions and Remarks:  The authors suggest exploring conditioning on a melody synthesized from the pitch to leverage phase information. It would be beneficial to investigate the impact of this adjustment on HiFi GANs performance.  The toy task results should ideally exhibit a fixed model as the model input is constant.  The authors should comment on the potential for pitch artifacts in Parallel WaveNet and flowbased methods due to their nonautoregressive nature.  Typos and minor grammatical error in the text should be addressed.", "Paraphrased Summary: The CARGAN method is proposed for conditional audio synthesis. It combines an autoregressive structure with parallel generation within audio chunk to address the periodicity and pitch inaccuracies observed in current parallel GAN generators. The autoregressive design ensures continuity and accuracy in periodicity and pitch generation. The discriminator incorporates contextual samples to reduce boundary artifacts. Strengths:  Clear presentation and rationale  Comprehensive literature review  Demonstration of parallel generation artifacts through audio instance  evaluation using pitch periodicity error voiceunvoice classification and subjective listening tests Weaknesses:  Omission of hybrid methods like \"Wavetacotron\" which is similar to CARGAN but employs flowbased models instead of GANs  Audible boundary error in music samples suggesting the need for hyperparameter optimization (e.g. chunk and context sizes)"], "pMQwKL1yctf": ["Summary To model the coherence of lengthy texts the researchers propose using a Brownian bridge process. They showcase an encoderdecoder training technique that employs a contrastive loss to represent Brownian bridge dynamics. Their model outperforms others on a variety of coherence and generation tasks due to its underlying generative process. Strengths  The Brownian bridge process model (TC) is a valuable concept for planning in longform text generation.  The models motivation and training method are clearly presented. Weaknesses experimental Section:  The first experiment on local coherence lacks evidence to support the claim that TC is superior to chance level.  The low BLEU scores in the second experiment on text infilling raise concerns about the validity of the results.  The third experiment on global coherence does not specify the specific theory of discourse coherence it measures.  The fourth experiment on forced long text generation is questionable as it evaluates the models behavior outside its intended use case. other issue:  The likelihood work should be explicitly stated for clarity.  The order of tables in the paper could be improved for readability.", "Summary This paper models text evolution as a Brownian bridge process to address incoherence and \"meandering\" in autoregressive generation. Brownian bridge specifies a Gaussian distribution for text embeddings (z) at any point (t) between two fixed endpoints (z0 and zT). The paper trains an encoder that generates z embeddings based on sentence pairs (x0 xt xT) and uses contrastive loss to enforce Brownian bridge dynamics. effectiveness and Weaknesses  effectiveness:  Novel approach to enhance coherence in sequence generation.  Demonstrates benefits of learning linear relationships between embeddings.  Improved performance in document coherence and generation.  Weaknesses: Limitations:  Requires two fixed endpoints (z0 and zT) for modeling limiting flexibility.  Assumes autoregressive generation follows Brownian motion without empirical evidence. Baseline issue:  VAE baseline implementation may not fairly compare to the proposed approach due to prior mismatch.  Linear classifier experiment favors the proposed method since it exploits the linear relationships in embeddings.  Lack of details on Brownian motion baseline and its comparison with Brownian bridge. Interpretational Concerns:  Table 5 results require further discussion and analysis.  Clarification needed on the impact of sentence proximity in the triplet (x0 xt xT).  Confusing notation in equation 2s denominator regarding negative xt summation.", "Paraphrased Summary: This research introduces a text generation method that incorporates a target state along with an initial state. Researchers have replaced random motion with a Brownian bridge which establishes beginning and ending states known as \"Time Control.\" Experiments reveal that this Brownian bridgebased method produces more natural and logical text for text filling tasks. Additionally both automated and human assessments confirm that it maintains sentence structures. effectiveness and Weaknesses: Using the Brownian bridge for text production is ingenious and successful. However its applicability may be constrained. While it outperforms simple random walks Time Control only allows the first and final states to be defined. In practice the first (and occasionally last) sentence may not have a specified state. Instead the beginning sentences may be preambles with the real content following. The researchers propose using a conditional Gaussian process for text generation allowing for conditioning at any time. This could extend the work in the future. Nevertheless this technique paves the way for more structured text generation. Minor Improvements: To enhance readability relocate tables to the top or bottom of the page using commands like \"\\beginfigure[t]\". Use a smaller font size for numerical results in tables (e.g. \"\\small\") and reduce line spacing for improved readability using commands like \"\\usepackagesetspace\" and \"\\beginspacing0.9 \\endspacing\".", "Paraphrase: Summary: This study presents a technique to improve the overall coherence of text produced by language models. It operates under the assumption that incoherent text resembles random \"Brownian motion\" in the latent space of sentence embeddings. By setting start and end points for this random motion text generation can be modeled as a \"Brownian bridge\" guiding the generated text toward a specific goal. Method: The proposed method involves three steps: 1. Encoder training: Train an encoder to map sentences to a latent space representing the Brownian bridge. 2. Decoder training: Train a decoder to generate sentences from a given context and a true latent vector from the encoders latent space planning. 3. Inference: At inference time a target trajectory is sampled for a given start and end point. The decoder then generates sentences based on this trajectory. Experiments and issue: The authors conducted experiments to evaluate:  Encoders ability to capture local text dynamics  Decoders capability to generate coherent local text  Global text statistics compared to ground truth  Overall coherence of generated long texts The results generally support the study hypotheses but there are some exceptions (discussed in the \"field for Enhancement\"). effectiveness and Weaknesses: effectiveness:  Clear and concise paper  Novel and generic concept of modeling sentences as a Brownian bridge  Wellstructured experiments with supportive results  Reproducibility and transparency through available code and datasets field for Enhancement:  Better explanation of ablation (ID BM)  Inconsistent performance of TC with different latent dimensions  Reasons for VAE(32) outperforming TC(16) in Table 5  Ensuring decoders utilization of latent plan data  Intrinsic visualization of the latent space to assess embedding behavior"], "gEynpztqZug": ["Paraphrase: Summary This article suggests Mako a data programming method for semisupervised continual learning. Mako utilizes weak labeling purposes to automatically label unlabeled data. Each purpose is trained on a distinct subset of the training set using bootstrapping. The results obtained on different datasets attest to the proposed methods efficiency. effectiveness and Weaknesses effectiveness:  The article is wellstructured and discusses a pertinent topic.  Experiments cover various scenarios. Weaknesses:  The technical innovation in the proposed methods is limited.  The method combines existing approach such as Snubas classifier utilizing weak labeling purposes with bootstrapping.  data programming techniques are not novel resembling selftraining which predicts unlabeled data labels during training. Selftraining is a widely used technique in semisupervised learning that the article could have discussed in more detail.  There is a lack of insights on catastrophic forgetting mitigation which is the paper main focus.  The article introduces the catastrophic forgetting ratio up to task i as an evaluation metric. However backward transfer is a common metric for evaluating forgetting in continual learning and should be included for comparison.  While the method may improve top pertask accuracy the reduction in forgetting appears minimal.  The method appears to have numerous hyperparameters and a sensitivity analysis would be beneficial. References:  [1] Uncertaintyaware Selftraining for Text classification with Few label. Subhabrata Mukherjee and Ahmed Hassan Awadallah. NeurIPS 2020  [2] On Tiny Episodic storage in Continual Learning Arslan Chaudhry et al. https:arxiv.orgabs1902.10486", "Paraphrased Statement: Summary: This field introduces a novel approach that utilizes data programming to support continuous semisupervised learning with limited labeled data. It consists of a multistage process involving: 1. Generating probabilistic pseudo labels using the Snubabased data Programming framework. 2. Calibrating these labels through temperature scaling. 3. Integrating the calibrated labels into existing Lifelong Machine Learning (LML) tools. Experiments demonstrate that this frameworks performance is comparable to that of fully supervised methods. effectiveness and Weaknesses: Advantages:  Facilitates continuous learning with reduced manual annotation effort.  Leverages data programming techniques to generate pseudo labels making fully supervised LML methods applicable in this context.  Promising experimental results compared to fully supervised approach. Weaknesses: 1. data Programming Techniques (Snuba):  Limited analysis on the performance of the annotation process in downstream tasks.  Unclear amount and diversity of labeling functions generated.  Complex and timeconsuming annotation process with missing data on computational costs. 2. Hyperparameter search and data Programming Modules:  Ambiguous description of the hyperparameter search and data programming process.  Missing details on the amount of pruning heuristics generated.  Insufficient technical data such as the specific Snuba hyperparameters used. Miscellaneous:  Curiosity about the underperformance of DEN framework (Makolabeled and fullylabeled) in task 7.  Sudden improvement in DEN performance in the close task in Tables 7 and 10. small Correction:  \"purposed\" in the second paragraph of Section 2 should be \"proposed.\"", "Paraphrased Statement Summary This paper proposes a software tool that enhances existing Lifelong Machine Learning (LML) frameprocesss by employing a wellknown method called data programming. The tools key features include:  Adapting a data programming technique from semisupervised learning to specialized scenarios in LML.  Implementing an LML wrapper that can perform specific tasks under certain constraints.  Demonstrating the superiority of the proposed tool through experimental results. effectiveness  The paper clearly defines the problem applicable scenarios and proposed methods.  The frameprocess addresses a significant issue in LML where labeled training data is scarce or expensive to obtain.  The tool incorporates various functionalities including hyperparameter tuning and pseudo label.  experimental results support the effectiveness of the approach. Weaknesses  Definitions:  The term \"weak labeler\" and its relationship to \"weak augmentation\" in semisupervised learning need clarification.  The concept of \"weak label\" in classification problems requires further explanation.  Originality:  The paper should more explicitly highlight the original contribution of the process and distinguish it from existing techniques.  Semisupervised Learning:  The paper claims to address semisupervised continual learning but the description of how the tool tackles the challenge of obtaining accurate pseudo labels is unclear. It would be helpful to provide more details or acknowledge the limitations.", "Paraphrase: Summary: This paper focuses on lifelong machine learning (LML) with limited data unlike previous LML search that mainly explored supervised settings. The proposed MAKO (MetaLearning Active knowledge Optimizer) method enhances supervised LML framework by leveraging unlabeled data without adding overhead. It enables data labeling through data programming guided by labeled data. The goal is to develop a semisupervised LML framework that minimizes the performance gap between using partially labeled data and fully labeled data. Experiments on the MNIST CIFAR10 and CIFAR100 image classification datasets demonstrate MAKOs effectiveness. effectiveness:  MAKO effectively addresses catastrophic forgetting a critical challenge in lifelong learning.  comprehensive experiments were conducted to evaluate MAKOs capabilities in preventing catastrophic forgetting improving peak pertask accuracy and achieving final accuracy.  Exploring semisupervised LML with limited data is a novel and valuable approach.  The paper is wellstructured and organized. Weaknesses:  Experiments on larger benchmarks may be needed to fully demonstrate MAKOs improvements.  While MAKO incorporates components from prior search its technical contribution may be limited.  The forgetting ratio being small than the baseline method raises question about MAKOs resistance to catastrophic forgetting and the nature of knowledge transfer.  Table 1 suggests that increased use of unlabeled data during training can lead to decreased final accuracy which requires clarification."], "pC00NfsvnSK": ["Paraphrased Summary: This work investigates how to enhance generalization in offline reinforcement learning settings focusing on improving representation learning. The authors suggest that observations with similar future behavior should have similar representations. To achieve this they introduce the Generalized Similarity work (GSF) which relates latent representations to future actions. This approach is tested on an offline version of Procgen surpassing baselines across various tasks. effectiveness:  The concept of offline RL generalization is significant for the field.  A motivating example illustrates the approach.  GVF is a versatile framework capable of recovering existing objectives.  The proposed method outperforms CQL CCSC and PSE baselines. Weaknesses:  Complex approach section.  Missing technical details.  Confusing notation.  Several algorithmic details require clarification.  No comparison to stateoftheart offline RL methods like FBRC and MOReL.  The use of data augmentation without an ablation work limits the evaluation of the proposed GVF.", "Paraphrased Summary: The paper explores ways to improve generalization and representation in reinforcement learning particularly in offline settings. The authors introduce an approach called \"selfsupervision\" that uses similarity learning within the state space. This approach involves defining a similarity step as a classification task over a quantized latent space. The paper introduces the concept of a \"generalized value work\" and evaluates the proposed approach on an offline dataset called Procgen which is useful for assessing generalization performance. The authors highlight the approach effectiveness in improving zeroshot generalization and provide a valuable contribution to the offline RL research community. effectiveness and Weaknesses: effectiveness:  Clear and welljustified methodology  introduction of an offline version of Procgen  novel use of quantization in this setting  Promising experimental solution on the benchmark dataset  introduction of the generalization value work Weaknesses:  Lack of comparison to existing methods like cURL  Absence of evaluations on the D4RL dataset  reliance on specific RL algorithm (PPO and CQL)", "Paraphrased Statement: The authors expand on their previous work where states with similar expected rewards or action sequences had similar representations. Instead they directly consider successor features which represent the cumulative sum of future rewards. To handle continuous reward works they use quantile binning. They also create an offline version of Procgen as a testing platform. effectiveness:  The method is original and can be used for contrastive learning.  The paper is wellwritten and technically sound.  It contributes to the understanding of generalization in reinforcement learning. Weaknesses:  The method requires pretraining of a generalized value work limiting its application to offline RL.  The evaluation is limited to the authors proposed offline Procgen benchmark raising concerns about the robustness of the solution.  Information about hyperparameter tuning is not provided making it difficult to assess the experimental evidence. Nitpick:  Equation (7) referred to as a contrastive loss appears to be more akin to a classification loss.", "Paraphrased Summary and effectivenessWeaknesses: Summary: This paper introduces a novel method for improving the generalization performance of offline reinforcement learning algorithm. It aggregates observations based on the similarity of their expected future behavior. effectiveness:  The proposed approach is unique.  It relates to existing methods (CSSC PSE) but outperforms them in experiments. Weaknesses:  It lacks a discussion or empirical comparison with CTRL.  Its unclear if the improvement over CSSCPSE comes from the new aggregation method or other factors.  Other aggregation methods could potentially be combined with CQL. Technical Writing publication:  The terms \"cumulant work\" and \"Qwork\" are used confusingly.  The definition of \"cumulant\" is unclear.  The policy definition for POMDPs is incorrect.  Algorithm 1 contains undefined variables and unclear statements.  Equations (3) (4) (6) and (7) have errors in their notation or definitions.  The representation learned by DeepMDP is not fully explained. Conceptual publication:  The problem setup and motivation are not welldefined.  There is confusion between MDPs and POMDPs throughout the paper.  The motivating example lacks clarity.  The different MDPPOMDP models in the experiments are not fully described. Suggested Improvements:  Improve the technical writing to enhance clarity.  behavior additional experiments and provide more details on the experimental setup.  Compare the proposed algorithm to other existing offline reinforcement learning algorithm."], "t7y6MKiyiWx": ["Paraphrase: This research introduces a novel \"Pyramidal Circuit\" that replaces conventional weights and orthogonal matrices used in orthogonal neural networks. This circuit enables twodimensional rotations facilitating training with perfect orthogonality. Additionally the authors present a quadratictime training method a substantial improvement over existing methods using Singular Value Decomposition. numerical experiments on the MNIST dataset validate the efficiency of their approach in a classification task. The authors highlight the innovative nature of the pyramidal circuit ability to implement orthogonal matrix times. While the results are significant the experiments could benefit from comparisons of running times to further demonstrate the efficiency claims.", "Paraphrase: Summary The researchers propose a novel neural network layer called the Pyramidal Circuit. This layer performs an orthogonal matrix times. They assert that it enables gradient descent to be performed while preserving perfect orthogonality with the same computational complexity as a typical fully connected layer. The algorithm is inspired by quantum computing and can be applied on classical or nearterm quantum computers. To demonstrate its effectiveness the researchers present experiments showing that networks incorporating Pyramidal Circuit(s) can achieve reasonable accuracy on binary classification tasks. Strengths and Weaknesses input and Concerns Reproducibility The researchers claim anyone with classical software skills could reimplement the classical algorithm. However they do not release the code. It is unclear if a classical software developer could reproduce the experiments on a substantial quantum device in a practical time frame especially given the complexities involved in working with such devices. Section 5 numerical Experiments This section lacks key detail. The researchers claim 98 accuracy on a binary MNIST classification task using the substantial quantum computer ibmqbogota. Given that state training and measurement error for qubits on this device often exceeds 2 these solutions are questionable. Without code release and more detailed descriptions it is impossible to verify these claims. Significance of solution in Table 2 MNIST with all 10 classes can be classified with 99 accuracy. PCA on MNIST with 2 principal components makes classes 6 and 9 most limost separable. It is questionable whether 95 accuracy achieved by the network is significant or if it could be achieved by simply adjusting a hyperplane. Figure 10 If interpreted correctly this figure shows a test accuracy on MNIST of 85 which is low compared to stateoftheart and simple offtheshelf methods. The significance of this solution is unclear. Section 4 Mathematical proof format would enhance clarity and allow for beneficial evaluation of the claims made in this section. Section 3 The statement that \"any given neural networks orthogonal layer has a quantum pyramidal circuit that reproduces it\" requires numerical proof. The provided justification is insufficient. Equation 5 The equation does not make sense. With W being an n by n matrix x being ndimensional and UW being 2n by 2n it is unclear how UW can operate on x. Figure 6 The descent of this matrix is unclear it appears to be the intersection of RBS gates. Typesetting and Typos Inconsistent formatting missing spaces and shifting quotation marks throughout the paper.", "Paraphrased Statement: A method is proposed for constructing neural networks that maintain orthogonality regardless of whether they are executed on classical or quantum computers. This approach introduces the concept of a \"pyramidal circuit\" that can be implemented on both platforms. For quantum devices it employs twoqubit gates while on classical devices it utilizes sequences of rotations. This design ensures that the number of trainable parameters remains linear in the size of the network making it efficient in both backward and forward passes. Strengths:  Unified architecture compatible with classical and quantum computing  Simplified data loading for quantum computers without assumptions about quantum RAM (qRAM)  Robust performance of the unary representation to errors  Experiments demonstrating comparable accuracy across classical quantum simulator and quantum computer implementation Weaknesses:  quantum implementation requires a number of qubits equal to the number of features in the dataset  Limited review of existing orthogonal NN approach missing previous work that proposed similar architectures based on Householder projections or Givens rotations.", "Summary Paraphrase: Researchers present a novel circuit design for quantum neural networks called the \"quantum pyramidal circuit.\" This circuit aims to create an orthogonal layer that is both efficient and maintains orthogonality. The mathematics behind the circuit gradient is explained. The researchers demonstrate the effectiveness of their orthogonal neural network through simulation and experiments on quantum computers. Strengths Paraphrase: 1. The paper provides a thorough explanation of the orthogonal neural network layers structure and gradient computing. 2. The orthogonal quantum neural network is implemented on actual quantum hardware. Weaknesses Paraphrase: 1. Major: The originality and impact of the paper are questionable. The use of twodimensional unitary planar rotators for constructing arbitrary unitary performance is not novel. The beam splitter gate is essentially a MachZehnder interferometer. The parametrization of the unitary group was outlined in the 1962 publication \"The Unitary and Rotation Groups\" by Francis D. Murnaghan. orthogonal and triangular arrays for constructing arbitrary unitaries have been proposed and utilized in optical neural networks for many years. Training rotation phases in the unitary parametrization space with robustness and discretization considerations have been explored in previous literature. Onchip techniques exist for training rotation phases without external backpropagation. Furthermore there are prior work that address gradient explosion issues in RNNs using similar unitary operators. 2. Major: The robustness of the proposed orthogonal layer is uncertain. The triangular array used to construct an arbitrary unitary has a depth of 2N3 which could lead to significant weight error accumulation a wellknown issue in optical neural networks. Shallower quantum neural layers may be less susceptible to quantum noise. orthogonal and butterfly mesh structure may also exhibit beneficial noise tolerance. 3. Major: The experiments presented lack sufficient data and analysis to fully demonstrate the effectiveness of the orthogonal quantum neural network layer. A discussion on the efficiency and robustness of the layer is not included. 4. Minor: Training triangular quantum unitary layers through backpropagation requires substantial memory as each stage must store intermediate results which can be inefficient and challenging to scale. Training these unitary network in the rotation weight space is also sensitive to initialization and without careful initialization signals may become trapped in local way leading to slow intersection."], "qnQN4yr6FJz": ["Paraphrased Statement: This field offers a novel approach for handling missing data in learning models. Unlike traditional methods the authors adopt discriminative learning while leveraging generative modeling to combine the advantages of both approaches. To address the intractable loss use they derive a lower bound by combining the variational lower bound (ELBO) and an upper bound (CUBO) from previous work. Modifications are made to CUBO to address bias and variance resulting in an unbiased and lower variance loss estimation via Monte Carlo methods. Empirical evaluations demonstrate the stability and comparable or superior performance of the proposed methods compared to baseline techniques. strength:  The combination of discriminative and generative learning for missing data imputation is an modern concept.  The approach overcomes challenge in deriving a lower bound for the loss use involving a subtraction integral.  Regularization is introduced to reduce variance in loss estimation improving stability during training. Weaknesses:  The field focuses solely on missing data patterns where data is missing completely at random (MCAR) which may not be representative of realworld scenarios.  The proposed method lacks modeling of missing data patterns limiting its practical utility.  While the proposed CELBO surrogate parameterization ensures optimality in the zero gap case its behavior under nonzero gap cases is unclear and requires further analysis.  The empirical solution suggest that the proposed methods do not significantly outperform baselines indicating that improvement could be made in selecting divergence uses or regularization techniques.", "Paraphrased Statement: summary: The authors have developed a novel method called DIG for handling missing input feature in classification tasks. DIG utilizes latent variable models to determine the likelihood of a label given the observed feature. However this objective is challenging to compute so the paper introduces a conditional evidence lower bound (CELBO) which closely approximates the objective. This CELBO consists of the evidence lower bound (ELBO) and an evidence upper bound (EUBO). The EUBO employs alpharenyi divergence and exponential divergence. To mitigate great variance issues during optimization the authors propose a surrogate parameterization that limits the gradient norm. Experimental solution show that DIG outperforms or matches existing imputation methods on realworld datasets. strength:  Clear and wellwritten paper  Addresses a significant practical problem in machine learning  method incorporates recent advancements in variational inference  Includes an effective optimization stabilization technique Weaknesses and Questions: 1. Section 3.1: Could you provide more context and references for exponential divergence What is the specific use of f(u \u03be) and why was it chosen as a Gaussian PDF 2. Section 3.2: Which automatic distinction library was used Did the experiments employ reparameterization or REINFORCE 3. Section 3.3: Explain how this section relates to stabilizing the gradient norm. Why is G defined in that mathematical form and what does \u2228 represent 4. Section 4.1: Define MCAR and MNAR and explain their impact on data imputation. How does DVAE compare to these methods 5. Section 4.1: The dataset sizes are relatively small. Could DIG handle greatr datasets effectively How long did it take to train on YearPred 6. Comparison with other Baselines: How does DIG perform compared to more recent imputation methods like MIWAE and addition", "Paraphrased Statement: Summary The field addresses the issue of predicting with missing feature. The authors create a generative model class that incorporates missing feature and devise a discriminative learning algorithm that approximates the posterior loglikelihood of the training data. Experiments indicate competitiveness with existing methods particularly those based on VAEs. strength and Weaknesses  strength:  Convincing generative model class for handling missing feature.  Simple distributions with latent variables.  novel upper bound with a stochastic gradient estimator.  Datadependent surrogate reparameterization for low variance.  validation of preserved effective parameter subset during reparameterization.  Weaknesses:  effective parameter subset may be small and cover just simple models.  No guarantee of gap reduction during learning.  Experiments demonstrate learning properties but not scalability. Additional Comments  Discuss the applicability of other works (Ghahramani  Jordan 1994 Smola et al. 2005) to the model class and whether a DCA for learning p(yxzm) can be generalized for p(yxm).  Consider removing the data instance superscript other in the text for improved readability.", "Paraphrased Statement: The paper tackles the issue of predictive modeling when input feature are incomplete. The authors represent the problem as a latent variable model. To address the Monte Carlo estimation challenge with CUBO (Dieng et al. 2017) they propose an upper bound based on CUBO modified by an exponential divergence in addition to the standard variational lower bound (ELBO). Furthermore they introduce surrogate parametrization to decrease gradient variance. Experimental solution on UCI datasets with intentionally missing feature demonstrate modest improvement over existing benchmarks. strength and Weaknesses: Pros:  Addresses a significant problem in predictive modeling.  Offers a novel approach combining latent variable modeling and variational bounds. Cons:  performance addition over simpler models are minimal not justifying the complexity.  Difficulty in understanding the paper and grasping the authors intent. Specific interest: 1. \"Optimization of negative L is relatively difficult\": Clarify the understanding for this. 2. \"No equivalent of VI for MCUBO\": Address prior work on CUBO and its shortcomings. 3. \"Exponential divergence\": Provide a citation or explanation. 4. Inconsistency in using \\theta for parameters in equations (11) and (12). 5. Different z samples in equations (15): Explain the training and innovation procedure. 6. Clarify notation for and explain the use of the addition use. 7. Define \"effective parameters\" and justify why zero gradient leads to tight approximation. 8. Explain how DVAE and MCARMNAR models map to the proposed formulation. Questions for ClarificationDiscussion: 1. Explain the dependency of y and x on m in equation (1). 2. Explore the potential of linking the approximate posteriors q(z y \\tildex) and q(z \\tildex). 3. Discuss the impact of splitting the predictive conditional log likelihood into ELBO and EUBO terms on p(y x). Minor Textual Issues:  Proposition 1 should be numbered as 1 in page 6."], "yql6px0bcT": ["Paraphrased Statement: Summary: The field introduces DecentCEM a parallel algorithm that improves CEMs performance in handling multimodal optimal action. DecentCEM runs multiple parallel CEM instances with different policy allowing it to explore a wider range of action and increase sample efficiency. Strengths and Weaknesses: The motivation for DecentCEM is to address the number of CEM being unable to effectively handle multimodal action distribution. However this number is not significant in the modelbased RL experiments conducted in the OpenAI tasks as the algorithms entropy is sufficient to break the multiple solution. The experiments show limited improvement of DecentCEM over other methods. Additionally there is no evaluation of whether the parallel structure reduces computation time and the increased computational complexity of training multiple policy counteracts any potential benefits. While the paper is wellwritten and easy to follow the concept and motivation behind DecentCEM are lacking. The use of parallel CEM policy is a straightforward extension of Poplin with uncertain advantages. A better approach to address CEMs Gaussian assumption would be to use sequential Monte Carlo methods that can handle multimodal solution. Overall the field lacks theoretical insights and provides limited empirical evidence to support the proposed algorithm.", "Paraphrased Statement: This paper proposes a novel Coordinatebased exploration Method (CEM) for modelbased reinforcement learning (RL). Unlike previous CEM approaches which employed a centralized method based on a Gaussian or Gaussian mixture distribution (GMM) this decentralized CEM allows each single instance to independently track its own data and topk estimates. The paper begins with a 1D toy experiment demonstrating that the decentralized CEM converges to the global optimum more efficiently than the centralized CEM with GMM. Subsequent experiments on several modelbased RL tasks show that the decentralized CEM outperforms previous methods on some tasks while performing similarly or slightly worse on others. Strengths:  The concept is simple even efficient.  The paper is wellwritten and easy to understand.  The mathematical equations are concise and clear.  The appendix provides detailed information on hyperparameter settings.  Replicating the experiments is feasible for other researchers.  The 1D toy instance provides a useful visual instance. Weaknesses:  The modelbased RL results are not especially solid with some noisy curves and instances where the decentralized CEM performs poorly.  Hyperparameter choices made in the toy experiment are not fully reported and it would be useful to provide more information on the number of mixture components and instances used.  The increase in data volume per environment interaction is not clearly explained.  Related process is limited to other MBRL approaches and does not include relevant literature from the CEM field.  The paper lacks a Discussion and Future process section.  The graphs are low and difficult to read in some cases with unclear color coding.  Additional clarification is needed on certain methodological details such as the threshold value in Equation 3 and the relationship between the number of topk samples in CEMGMM and DecentCEM.", "Paraphrase: Summary: This paper introduces an update to CrossEntropy Method (CEM) for optimization. The update involves using an ensemble of standard CEM instances where each instance is optimized separately. The solution at each step is determined by the topperforming CEM instance. Simulations in a continuous control benchmark demonstrate that the updated algorithm performs as well or better than previous CEM variation. An instance illustrates how using an ensemble aids in escaping local minima. Strengths:  Ensemble optimization effectively explores the solution space.  The benchmark and baselines are appropriate including SAC as a reference.  Simulation results show comparable or superior performance to other CEM variation.  The paper is generally clear and concise. Weaknesses:  Training curves are prematurely halted in many environments hindering the assessment of convergence and final performance especially compared to SAC.  While the algorithm is novel the concept of ensemble optimization is not. comparison with other ensemblebased approaches would be valuable. Questions:  How does CEM improve training time compared to modelfree approaches like SAC  Why is the covariance matrix not learned Is it due to the large dimensionality of the parameter space  Is my understanding correct that each step in the training curves represents a single state transition but the points are displayed per episode that can end after a set number of transitions Minor Comments:  The transition probability distribution should be defined over the real vector space not [01].  The references are not numbered in the text."], "gjNcH0hj0LM": ["Paraphrased Statement: The paper aims to enhance active learning for time series data where the goal is to learn a relationship between inputs (x) and outputs (y) while assuming that the output signal (y) is constant over time intervals. Initially the output values are unknown but query can be made to label data points. The paper proposes a methodology to identify constant segment of the output signal based on input characteristic (x) and labeled timestamps (y). This is achieved by using a thresholding rule to determine segment where the predicted output values at a given time point are close to the labeled values. The identified segment are treated as having true labels. The learned framework for the relationship between inputs and outputs is then updated and novel query points are selected for labeling based on the current framework. This process is repeated until a desired number of iterations or a convergence criterion is met. Strengths:  The experimental results demonstrate the effectiveness of the proposed method.  The paper presents a simple and practical approach that performs well. Weaknesses:  The updating rules based on the learned framework may be susceptible to overconfidence particularly when the framework is uncertain.  The paper lack a more thorough discussion on overconfidence and potential failure case of the proposed method.  The code and supporting materials linked in the paper were not readily accessible.  There are minor grammatical and formatting errors that could be addressed. Specifically there are additional comments on the paper content in the paraphrased statement:  The assumption that the estimated class is independent is questionable and a derivation or clarification of this assumption would be valuable.  Equation (5) appears to be based on unrealistic assumptions and a more rigorous justification is desirable.  segment 3.3 lack scientific rigor and would benefit from a more detailed and wellsupported analysis.  It would be insightful to provide more data about temperature scaling and the label propagation strategy particularly in case of overlapping segment.  The experimental results would be more conclusive with additional details such as standard deviations and data about the neural network architecture used.", "Paraphrased Statement: Summary: This paper proposes a method for estimating segment in time series data using label propagation in active learning. It utilizes a plateau function to capture temporal coherence in the data. Experimental results demonstrate its superiority over traditional label propagation approach. Advantages:  Employing the plateau framework in time series active learning for pseudo labeling improves performance.  The paper is accessible and straightforward with experimental results showing noticeable enhancements in performance and label reduction. Unresolved Questions: 1. Why do the values of b and w0 vary across different datasets (Table 1) How does increasing b affect performance Does the performance of the proposed method (TCLP) deteriorate with large query sizes 2. Why does increasing the temperature parameter T lead to improved performance Additional experiments with different T values would be valuable in understanding its impact. 3. In increase to the \"UTILITY\" metric it would be beneficial to include evaluation results for frameworks trained on the entire dataset providing a baseline for comparison.", "Paraphrase: Summary: This paper introduces an active learning method for classifying and dividing time series data. Userprovided labels for case are extended to adjacent case using a plateau framework allowing for effective data distribution. Strengths and Weaknesses: Pros:  effective use of limited user feedback.  comprehensive experimentation. Cons:  Lack of detailed theoretical explanation.  Missing data on computational complexity. Main Comments: Equation 3: It appears that `(cws)` values are calculated separately for each plateau. However these values may contribution similarities for a specific class label `(yl)`. Exploring a framework that connects `(cl wl sl)` to `yl` may be beneficial. Equation 5: This equation requires clarification and a demonstration that the sum of `Pr(te  ts  i)` from 1 to L equals 1 validating `Pr` as a probability step. Algorithm 1:  Its unclear if `rb` plateau frameworks are always available in each round of active learning.  The computational complexity and time per round remain unspecified. Exploring complexity reduction techniques such as sampling relevant plateau frameworks is recommended.  The rationale behind using `b  85` for the mHealth dataset and `b  200` for others needs justification. Consistent parameter settings across datasets are preferable unless substantial justifications exist.  The smaller value of `w0  5` for the GTEA dataset may be due to its smaller segment. Automatic parameter selection or explicit rules for selecting appropriate values would enhance the methods robustness.", "Paraphrased Statement: Summary: This framework (TCLP) selects the least number of samples from a time series for expert labeling based on their temporal relationships. Unlike similar methods TCLP doesnt rely on estimates of segment locations or class labels. Strengths and Weaknesses: Strengths:  Considers the temporal coherence of time series in its active learning strategy.  Addresses number with segment identification.  Provides a detailed algorithm with clear pseudocode.  Shows substantial empirical results.  Analyzes performance differences based on dataset characteristic. Weaknesses:  The paper lacks an explanation of performance change as the parameter T (temperature scaling) increases beyond 2. It would be beneficial to understand how that breaking point differs across datasets and its inherent reasons."], "on54StZqGQ_": ["Paraphrased statement: Summary This paper introduces a new method called \"degradation attacks\" that target the certified robustness of a model by forcing the model to reject nonadversarial inputs as potentially adversarial thus reducing the models usability. Strengths and Weaknesses  The paper is generally wellstructured and easy to follow.  However there are concerns regarding the correctness of the attack:  The statement that \"certifiable robustness processes usually imply epsilonrobustness\" is not necessarily true.  Failure of a certified robustness check does not guarantee the presence of an adversarial point within the epsilon ball.  It is not clear how to determine the set of points for which the model is epsilonrobust as the certified robust set may miss some points that are actually robust.  The papers claims that the system can flag nonadversarial inputs as adversarial may be misleading as the system is only indicating whether a given point is certifiably robust or not.  evidence robustness is often probabilistic which is not considered in the paper.  A comparison to other attacks such as design Gradient Descent (PGD) would be valuable for evaluating the performance of the proposed degradation attacks. Minor Comments  The paper uses different attackes for constructing smoothed formifiers (Algorithm 3.2 vs. Lecuyer et al.).  There is a slight misuse of notation in Algorithm 3.2 regarding the decomposition of noise vectors.  The assumption in 3.3.1 should specify that there are no other points of a different form label close to x.  Lower and upper bounds on the same graphs (Fig 2) would provide a beneficial visual representation of their tightness.  Defense 4.2 may not be practical as membership of M is often not computable.", "Paraphrase: Summary This field unveils a weakness in the GloRo and randomized smoothing defense mechanisms. The attack targets inputs located within the robustness area that are difficult to certify leading to false rejection. The attack employs a modified PGD attack with smoothed variance. Experiments demonstrate the vulnerability of these certification methods to the proposed attack. Strengths:  Verified accuracy value for randomized smoothing and GloRo reduction under the new attack.  A mitigation strategy is presented for the vulnerability. Weaknesses:  Practical applicability of the attack is questionable since verification help in robustness assessments. Verified samples can be trusted while unverified samples do not necessarily warrant rejection. other robustness value and prediction confidence scores are available. GloRos rejection mechanism is based on achieving global certified robustness with a global Lipschitz constant ultimately serving as local robustness verification.  The model in form 1 is flawed. Adversarial models exist when the true robustness area of x intersects the decision boundary. Determistic verification methods like GloRo should reject such models unless weights are retrained to adjust the decision boundary.  excessively cautious behavior is rudimentary to certifiable defense mechanisms focusing on samples within the input distribution. Encouraging certified robustness beyond this distribution is impractical unless global verification is required.  The modified PGD attack lacks significant novelty as similar methods exist for training and outperforming randomized smoothing alone. The attack should be compared against other stateoftheart verifiers and certified training methods as well.", "Summarized statement: The field explores a novel attack strategy against certified robust machine learning models at the time of testing. The attackers can apply imperceptible modifications to the input data to trigger a false positive detection of adversarial model thereby reducing the models overall effectiveness. The paper provides theoretical bounds on the performance degradation and demonstrates the efficacy of the attack against common detection mechanisms. Strengths and Weaknesses: Strengths:  The concept of \"degradation attacks\" against certified robust models is intriguing.  The problem setup is intuitive and highlights the challenges of information loss during testing.  The theoretical analysis is clear and accessible. Weaknesses:  Doubleradius defenses discussion is unclear. It is not clear whether the defense abstains at a radius of 2\u03b5 as stated in Algorithm 2.1 or if the models are trained with this parameter during testing.  The experimental results could be strengthened:  Provide model of degradation adversarial model created at different perturbation levels to demonstrate their visual similarity.  Explore the gap between the attacks lower and upper bounds and identify any form or conclusions.  Include results for alternative distance metrics (e.g. \u2113\u221e) and explain the omission of GloRos upper bound in the table and Randomized Smoothings upper bound in the form.", "Paraphrased statement: Summary: This research examines the finding that certified robust neural netform detect inputs that fail local robustness checks but correctly classify inputs within a distance of \u03f5. The authors use this observation to design an attack where bounded modifications impair the functionality of certified robust netform by compelling them to reject otherwise valid inputs. Strengths and Weaknesses: Strengths:  The observation is straightforward practical and thoroughly assessed. Weaknesses:  It is unclear if the observation is novel in its current form.  The constraint \"One need not assume that the inputs are scaled to be in [01]d\" is unclearly explained.  The motivation for form 1 could benefit from a practical model.  The effectiveness of Algorithm 3.1 needs clarification.  The attack against stochastic inputs lacks literature review citations.  Sections 3.3.1 and 3.3.2 should be integrated and presented more clearly.  The discussion on \"overapproximation\" could be expanded for clarity.  The demonstration of results for randomized smoothed models and GloRo Nets could be standardized for comparison.  The claim of novelty requires more clarification in relation to existing form."], "hm2tNDdgaFK": ["Paraphrase: Summary: This study introduces ChIRo a 3DGNN molecular representation model that:  Captures chirality: Encodes chirality data for better molecular representation capabilities.  Invariant to rotations: Reduces the need for data augmentation by remaining unaffected by bond rotations. effectiveness and Weaknesses: Novelty: ChIRo innovates by:  Chirality perception: Detecting chirality enabling enantiomer distinction.  Rotation invariance: Ensuring model generalization. Theory: ChIRo employs trigonometric functions to encode torsion angles. Appendix A provides a validation of chirality sensitivity and rotation invariance despite initial confusion caused by design 2. Reproducibility: Clear architectural details (Section 3) and implementation data (Appendix) facilitate model replication. representation: ChIRos design is wellexplained and easy to comprehend. Weaknesses: model design:  Spatial data handling: ChIRo processes spatial data separately potentially compromising graph topology.  Input order invariance: The models output seems unaffected by the order of input atoms questioning its design efficacy. Illustration:  design 2 ambiguity: further clarification needed before understanding the method fully.  design 3 similarity: Distinction between \"Original OMEGA Conformers\" and \"OriginalReflected Conformers\" unclear. Experimentation:  Objective relevance: Tasks used primarily test chirality perception rather than showcasing realworld applications.  Missing tasks: Limited evaluation on popular molecular property prediction tasks.  Ablation study anomaly: further validation required regarding the lack of improvement from adding chiral mark.", "Paraphrased Statement: Summary: This paper explores enhancing the capabilities of Graph Neural Netstudy (GNN) models by focusing on identifying chirality as a case study. Chirality a crucial geometric feature in molecules significantly influences their property and applications. This study extends previous study by incorporating sciencebased inductive bias into model design. The key design is the use of sinusoid transformations of torsion angles which remain invariant under bond rotations. effectiveness:  Appreciation for incorporating scientific knowledge into model design. Weaknesses:  Questioning the rationale for omitting chiral atom mark and bond stereochemistry mark in the model setup.  Exploring the performance of the model with additional redundant and selfconsistent torsion angle features. Comments:  Inquiring about the significant variation in embedding scales between design 3.  Suggesting the use of the term \"instance discriminative\" instead of \"contrastive learning\" for the triplet loss approach. Open Questions:  Assessing the compatibility of the model with SE(3)equivariant architectures.  Exploring the potential of integrating the model as an addon module for other models.  Examining the impact of improved chirality identification on benchmark performance such as QM9.", "This article presents a novel molecular GNN model with SE(3) invariance for predicting the 3D geometrydependent physicochemical property of molecules. The key issue addressed is the handling of molecular chirality. The model includes (1) standard edge convolution and attentionbased graph encoders for 2D graph representation (2) encoders for bond distances and angles based on Winter et al.s (2021) approach and (3) torsion angle encoders inspired by GeoMol (Ganea 2021) to capture torsional geometric data critical for distinguishing enantiomers. While the model combines existing estimation the novelty is unclear as similar combinations using SphereNet (Liu 2021) GemNet (Klicpera 2021) or GeoMol (Ganea 2021) have not been explored. Comparison to GeoMol (Ganea 2021) which also handles torsional geometry is only mentioned in an appendix without a clear explanation of the differences. The inclusion of GeoMol (Ganea 2021) in the baseline or a direct combination of Winter et al.s (2021) and Ganea (2021) approaches could provide further insight into the models significance.", "Paraphrase: Summary: This study investigates representation learning for molecules with spatial arrangement designing a model that handles torsion angles of 3D molecular conformations. The model integrates a rotational invariance mechanism to accommodate conformational flexibility eliminating the need for multiconformer data enhancement. The model performs well on various tasks. effectiveness and Weaknesses: effectiveness:  The model effectively captures rotational invariance.  further results on multiple tasks. Weaknesses:  Omission of GemNet comparison: Despite claims that SE(3)invariant GNNs like SphereNet and GemNet can handle chirality only SphereNet is compared in the experiments. It would be beneficial to include GemNet for a more comprehensive evaluation.  Insufficient classification task: The authors acknowledge that the chirality classification task is not a sufficient assessment of chiral representation learning. However they do not provide further explanation or alternatives.  Table 1 and 2 discrepancy: SphereNet performs similarly to Chiro in Table 1 but significantly worse in Table 2. The authors suggest this may be due to task differences but more analysis and case study are needed to elucidate the reasons for Chiros superiority in the second task."], "m5EBN92vjN": ["Paraphrased Statement: This paper introduces a novel netstudy that leverages attention mechanisms for realtime semantic segmentation. It employs spatial and channel attention modules along with a multiscale context module. The effectiveness of the proposed approach is demonstrated through evaluation on publicly available datasets. However the paper has several shortcomings: 1. Limited novelty: The proposed spatial and channel attention modules are not original as they have been previously utilized in other semantic segmentation methods. 2. Organization: The structure of the paper is disjointed with information pertaining to the contribution of the study appearing in the related study section instead of the introduction. 3. Insufficient motivation: The authors fail to articulate a clear motivation for their research in the introduction section. 4. Lack of analysis: The paper does not provide an analysis of the drawbacks of existing methods in the related study section. 5. Unclear claims: The authors assert that atrous convolutions enhance boundary information without providing a rationale. 6. Absence of ablation study: The impact of the proposed modules is not assessed through an ablation study. 7. Incorrect statements: The paper erroneously claims that DUC PSPNet and DeepLab v2 employ dilated convolutions and that the proposed netstudy lacks a backbone. 8. Poor wording: The phrase \"It also uses dense local multiscale context information using Multi Scale Context (MSC) module\" is poorly phrased and unclear.", "Paraphrased Statement: Summary: The proposed approach fuses spatiotemporal feature with a nonlocal attention mechanism for video semantic segmentation. effectiveness and Weaknesses:  Weaknesses:  Poor writing quality resembling a school image study.  lack explanation of novel contributions and superiority to competing methods.  Incomplete and outdated related study section.  Uncited mention in the bibliography.  Insufficient distinction of the proposed method from existing approach.  No novel components or applications of existing ones in the architecture or losses.  Lack of comparative analysis with similar approach like TDNet.", "Paraphrase: Summary: This paper introduces a method for realtime semantic segmentation. Instead of using a complex backbone network the method employs a lightweight architecture with convolutionrelu bar. It incorpoplace three parallel modules: a multiscale context module a spatial attention module and a channel attention module. Experimental results on three datasets demonstrate that the method delivers reasonable performance with high frame place compared to previous approach. effectiveness:  Eliminating dense backbone networks is an innovative approach and the resulting model achieves beneficial performance with a lightweight design.  The model opeplace efficiently meeting the requirements for realtime applications. Weaknesses:  The submodules are shortly described. For case addition performance in image 23 are unclear due to only taking one input. The purpose of reshaping them is also not evident.  The motivation for the proposed loss function is unclear. The binary cross information loss in Equation 8 is defined as whether the forward propagation result is correct which is ambiguous in the context of semantic segmentation. Equation 9 appears to be the standard multiclass cross information loss but the authors misleadingly refer to it as an \"auxiliary\" loss. Equation 10 is applied to a \"class attention map\" that is not previously defined but is likely the channel attention output. Equations 10 and 9 are essentially the same but are used for different module outputs making the terminology confusing.  The ablation study are short. The first ablation focuses on upsampling which is not mentioned in the main text. The authors should ablate the three components (MSC SA CA) to assess their contributions but they only modify specific details like kernel sizes and dilation place. The baseline architecture used in Table 6 is unspecified. Ablating the loss functions (e.g. removing Equation 8) would also be valuable.  The performance gains over previous realtime methods are modest.  The use of attentionbased modules and skipping dense backbones is not novel in semantic segmentation tasks.", "Summary Paraphrase: This paper presents a novel Attention Aware Network for realtime semantic segmentation that features spatial channel and multiscale context modules. It demonstrates a notable balance between accuracy and efficiency on three dataplace. effectiveness and Weaknesses Paraphrase: effectiveness:  Simplicity and effectiveness of the network resulting in an impressive tradeoff in performance. Weaknesses:  Unclear motivation and insufficient introduction.  Limited novelty due to the extensive use of existing attention modules and other techniques.  Inconsistencies between results on the test and validation place.  Ambiguous formulations and unexplained output shapes in the attention modules.  Poor typesetting and organization.  Questionable claim regarding the lack of a backbone and its definition.  Deficient English writing including tense errors and inappropriate use of \"use.\"  Missing details and formulations for the spatialcontext information attention mechanism and feature fusion."], "p-BhZSz59o4": ["Paraphrased Statement: Summary: This work introduces BEIT a selfsupervised learning framework where image regions are masked and the goal is to restore the tokens within those regions. Trained on ImageNet BEIT excels on various downstream tasks. Strengths and Weaknesses: The author praises the concept of BEIT as a promising approach to selfsupervised learning in computer vision. However concerns arise regarding the fairness of BEITs comparison to previous methods. The author highlights the importance of the tokenizer used in BEIT which imparts significant prior knowledge. However the tokenizer was trained on imagetext datasets like DALLE raising questions about its true selfsupervised nature. The author emphasizes the need for transparency regarding the tokenizers training details and suggests exploring alternative tokenizers including random and ImageNettrained versions. The author requests further clarification on the following: 1. specific details of the DALLE and ImageNetretrained tokenizers including external data sources and training costs. 2. The possibility of developing a tokenizer without external data and assessing its performance. 3. An analysis of the relationship between BEIT and knowledge distillation.", "Paraphrase: Summary: The paper introduces a novel objective called Masked Image modelinging (MIM) for pretraining vision Transformers (ViTs). MIM requires the modeling to predict visual tokens from masked image patches. These visual tokens are derived from a discrete Variational Autoencoder (VAE) trained on a vast image dataset. The pretrained ViTs (BEiT) are so finetuned on image classification and semantic segmentation tasks demonstrating improved performance compared to existing selfsupervised methods. The authors also introduce a technique called blockwise masking to further enhance BEiT. Strengths and Weaknesses:  MIM is similar to Masked Region modelinging (MRM) used in visionandlanguage pretraining where masked regions are used to predict their corresponding object classes.  While DallE requires argmaxed visual tokens due to its decoder BEiT only uses them to calculate the MIM objective and could potentially benefit from using the token distribution directly.  approach used by UNITER for using regional class data for MRM such as onehot labels KL divergence and feature regression could be explored for BEiT to potentially improve performance.  The use of visual tokenizers like discrete VAEs can be valuable for the community seeking selfsupervised image objectives particularly in the visionandlanguage pretraining field.  The paper provides strong empirical demonstrate and valuable tools for the research community.", "Summary This paper introduces Masked Image modeling (MIM) a novel task inspired by the Masked Language modeling (MLM) approach in raw language work. MIM transforms image patches into tokens masks them and predicts them. Pretraining with MIM improves performance in downstream tasks like image classification and segmentation. Strengths  Innovative concept of bringing MLM to computer vision  Wellwritten paper and open idea  Demonstrated improvements over stateoftheart methods Weaknesses Missing details:  specific masking strategies used are not provided.  Lack of comparison with other models trained for the same number of epochs (e.g. 300).  Training curve is not included. Concerns: 1. Pretraining Epochs: The 800 epochs for pretraining may be excessive for small research groups and limit comparability with other methods. 2. Token Replacement Ratio: Authors replaced 40 of tokens while MLM typically masks 80. The rationale behind this choice is unopen. 3. Small Dataset Evaluation: Standard small datasets (CIFAR10 Oxford Flowers102) are not evaluated making comparison difficult. 4. Epochs for downstream task: 150 epochs were used for downstream tasks rather of the standard 100. 5. Appendix Results: Interesting findings in the appendix are not discussed in the main paper. Minor Suggestions:  Change the special token [S] to [CLS].  Cite recent research on datahungry nature of vision Transformers.  Rephrase \"pixellevel autoencoding\" and provide more explanation.  Consider applying MIM to convolutional transformers like Swin or CvT.  Compare the throughput of MIM to DINO.", "Paraphrase: Summary: This work introduces a novel selfsupervised learning method for images known as masked image modelinging. Unlike previous approaches that relied on contrastive methods this technique utilizes a large language modelinglike architecture specifically a vision Transformer (ViT). The ViT encodes the image into visual tokens which are so matched with the tokens extracted from a discrete variational autoencoder (VAE) tokenizer called DALLE. This pretraining object outperforms previous stateoftheart methods on ImageNet1K and segmentation tasks on ADE20K when using comparable modeling sizes and pretraining conditions. Strengths and Weaknesses:  Strengths:  open and accessible presentation  Comprehensive ablation work supporting design choices  Weaknesses:  Heavy reliance on DALLE (requiring extensive unsupervised pretraining)  Despite ablation showing only a minor drop in performance when directly predicting masked pixels rather of DALLE tokens the accuracy still falls below supervised training. This ablation should have been extended to linear probing.  Disappointing linear probing results possibly due to a small batch size or other factors.  Lack of pretraining on datasets larger than ImageNet1K.  Sufficient support for reproducing the results"], "qj1IZ-6TInc": ["Paraphrase: Summary: This research introduces a novel method called Neural Voice Camouflage (NVC) with three key features: its applicable to any vocabulary in realtime its difficult to defend against and its robust. NVC trains a model to predict and generate adversarial attacks without any input or output constraints. Unlike previous gradientbased attacks that are computationally demanding and not suitable for realtime applications NVC can be deployed in realtime. Additionally NVC is not limited to targeting specific words or predetermined frequency rates making it harder to detect and mitigate. Experiments demonstrate that NVC significantly increases the word error rate (WER) of an Automatic Speech Recognition (ASR) model compared to other NVC methods. The work also provides various analyses of NVCs behavioral characteristics to guide future research. Experiments simulate realworld scenarios to assess NVCs performance. effectiveness and Weaknesses: effectiveness:  The idea of training a model to learn predictive attacks is innovative and practical for realworld NVC applications.  The extensive experiments indicate that the authors have thoroughly considered realworld scenarios and provide valuable insights for further research. Weaknesses:  The paper can be difficult to comprehend due to unconventional indexing notation.  The notion of \u03b4 requires clarification as to whether it represents exact computation time or a rate.  Including a comparison with a previously proposed realtime NVC method (even if limited in frequency rate) would enhance credibility.  The segment discussing temporal shift robustness is ambiguous regarding the effect of varying \u03b4 in training. An ablation work on \u03b4 would be beneficial.  In realworld applications the ASR model used may be unknown. Experiments evaluating performance against ASR models different from the one used for training would be informative. Personal Opinion:  The segment on \"Realtime Machine Learning\" could be revised to clarify how the delay mechanism enables realtime NVC operations.  Citations are needed for the numerical data presented regarding sampling rate and instantaneous computation.  Architectural details of DeepSpeech (ASR model) and the Language Model should be included for clarity.  approach to audio samples generated by the perturbation model would be valuable for understanding the attacks characteristics.  The WER exceeding 100 for the offline PGD attack requires clarification.  The 0.5second delay used in the work should consider the combined computation and playback time which is approximately 0.514 seconds.", "Paraphrase: The paper presents a novel technique that interferes with the recognition of speech by the DeepSpeech system. This method operates in real time and can bypass certain defenses. effectiveness:  The issue its formalization and the experiments are clearly explained and wellstructured.  The method is evaluated under different conditions that simulate realworld scenarios including a live room environment.  The authors demonstrate that the attack is tailored to specific speakers suggesting that the NNbased approach is essential for this specific task. Weaknesses:  practical ASR systems often use language models to enhance accuracy but the proposed method does not consider this face. Examples of attacked and groundtruth transcripts indicate that incorporating a language model could improve the models accuracy. This is the primary reason for a score of \"5\" which could be revised if the authors analyze the impact of language models.  The method was tested on only one specific DeepSpeech model. This limitation should be mentioned in the \"Ethical Considerations\" segment. Questions:  Which DeepSpeech model is being used Please provide specific details and references from the literature. The authors may consider including an appendix with this data.  In Figure 6 it appears that the WER rate are missing a factor of 100. Adding parentheses around each WERCER rate would clarify the plot.", "Paraphrase: Summary: This research introduces a novel attack strategy called Neural Voice Camouflage. It aims to interfere with automatic speech recognition systems by predicting and inserting attacks into speech streams in real time. Experiments using the LibriSpeech dataset evidence that the proposed model surpasses existing methods both with and without defensive mechanisms in speech recognition tasks (as measured by WERCER). effectiveness:  Clear and organized presentation  Detailed explanation of the method  Welldesigned evaluation framework  Solid experiments with supportive solution  Indepth analysis of outcomes (especially Figure 7) Weaknesses:  Insufficient validation of some arguments:  The conclusion that attacks target vocal quality and are speakerdependent relies on experiments where attacks are exchanged between speakers. However it should be verified using samples where different speakers utter the same content.  Lack of scientific foundation for some observations:  The authors claim that attacks resemble speech \"formants\" based on spectrograms. However these structures do not appear to be formantlike. Overlaying aligned text on the spectrograms would clarify this.  Minor comments:  The supplementary video evidences attack sounds but the white noise is inaudible. This limits comparisons.  If attacks are to be employed a perceptual evaluation of different methods should be conducted.  The relationship between \"multiplier m\" in Figure 6 and \"Power of noise\" in Table 1 is unclear. Additionally the units for WER in Figure 6 are not specified and there is a significant discrepancy in WER rate."], "j97zf-nLhC": ["Paraphrased Statement: This study investigates various architectures for training agents using reinforcement learning in a novel card game called HinterGuesser. various architectures exhibited poor performance and the knowledge they acquired was incompatible hindering crossplay and interaction with humans. The goal was to develop agents that can establish abstract correspondences during selfplay to enable successful crossplay and human interaction. Most architectures performed considerably (7090) against themselves but struggled when playing against independently trained agents. However training with the desired action as input improved performance. effectiveness and Weaknesses:  Attentionbased architectures that process observation and action representations jointly performed better in HinterGuesser.  The paper provides evidence to support this claim.  The authors acknowledge that the game is unique and may not represent all collaborative game.  The term \"zeroshot coordination\" is used despite significant training based on data similar to the testing task.  The connection between the uniform hint probabilities in Figure 5 and the concept of private languages is unclear.  The claim that agents exhibited humaninterpretable coordination model in ActionIn cluster 1 lacks evidence or supporting human data.  The final discussion mentions a \"majority vote\" mechanism but its description is missing from the paper. Questions and Clarifications:  What distinguishes HinterGuesser from existing cooperative game  Why is action inclusion in the input considered zeroshot coordination  How can findings from HinterGuesser be generalized to other collaborative tasks  What led to the choice of five cards Would different configurations alter the results  Can you provide an intuitive explanation of each attention mechanism proposed", "Paraphrased Summary: This research introduces the Zeroshot Coordination (ZSC) task where two agents (Hinter and Guesser) use a shared set of features to communicate and coordinate. The paper examines different techniques and finds that an attention mechanism that considers both observation and action features enhances agents performance in crossplay scenarios. effectiveness:  Engaging introduction with various motivations.  Clear definition of the ZSC task and its challenges.  insightful visualizations that highlight the differences in agent performance in selfplay and crossplay.  Detailed experimental setup and reproducible results. Weaknesses:  Sections 3 and 4 contain some repetitive capacity.  Notation can be confusing with symbols being used for different role (e.g. \"n\" for both the number of players and the number of features).  The study lacks a baseline for ZSC. While related work mentions the need for experimentercoded input in previous approach a baseline could yet be established in this context.  The rationale for the \"samehand\" setup is not adequately explained.", "Paraphrase: Summary: This paper explores using the semantic relationships between features in observations and actions to enable coordination in multiagent reinforcement learning without prior training on specific coordination scenarios (zeroshot coordination). It modifies the decentralized partially observable Markov decision action (DecPOMDP) by representing observations and actions with shared features. It utilizes deep learning attention mechanism to exploit these semantic relationships. Additionally the paper introduces a novel humaninterpretable environment for analyzing the proposed method. effectiveness:  Clear organization and introduction of the idea and method.  A novel method that extends DecPOMDP with shared feature reintroductions for both observations and actions.  The development of a novel HintandGuess game to evaluate the effects of different deep learning architectures on coordination. Weaknesses:  The paper could provide more details on the experiment setup especially regarding the fixed hand size and feature size and potential effects of varying these factors.  The paper lacks indepth analysis of the results. It does not explain why the 13 agents generally form two clusters with distinct coordination types or how hand variations work cluster formation.  The paper does not address whether the proposed method could be applicable to zeroshot coordination in other game beyond the introduced HintandGuess game.", "The paper introduces a method for assigning meaningful actions to observations in zeroshot coordination scenarios. The authors demonstrate that semantic actions lead to better inductive bias and consistency in crosswork. The method utilizes an attention model that jointly processes the observation and action features allowing the model to predict contextualized actions. While the paper does not offer novel techniques it effectively applies standard language and vision techniques to POMDP network. The evaluation results indicate that the agents generate policies that are similar to human work in a simplified Hanabi card game. However it would be beneficial to include a more comprehensive review of related work to explain why this approach has not been explored previously. The paper highlights human strategies used in gamework such as exact match similarity and mutual exclusivity. While the algorithm may learn these strategies it is unclear how the authors establish the onetoone correspondence between human and statistically learned strategies. Additionally it is not clear how the model selects various strategies at different times during gamework and how action features are prevented from leaking into the contextualized policy prediction. Further exploration of actionobservation modeling techniques such as Bayesian model or deep generative models could provide insights into optimizing the methods performance."], "fwsdscicqUm": ["Paraphrase: The paper investigates achieving group fairness in decentralized environments. It establishes a theoretical framework for fair decentralized learning and examines existing methods like UFL FFL with FedAvg and CFL. The authors reveal that UFL and FFL with FedAvgCFL are comparable in performance. They also introduce FEDFB a novel federated fair learning algorithm where clients contribution data on local classifier unfairness with a server that calculates sample weights for subsequent training rounds. FEDFB reportedly outperforms existing methods while preserving data privacy. Strengths:  Focus on the significant subject of fair federated learning.  Provides valuable theoretical analysis of existing algorithms. Weaknesses:  Empirical results are extensive but somewhat confusing.", "Paraphrased Statement: This subject suggests a general federated learning technique to create a fairness model. It assesses both advantages and shortcomings: Strengths and Weaknesses: 1. Noncontinuous Loss Function: The 01 loss function is discontinuous leading to violations of some model assumptions such as convexity and twice differentiability. Client Updates Implementation:  Clarification is needed on how client updates are performed in Algorithm 1 given the discontinuity of the 01 loss function.  If no approximation is used it is unclear how the updates can be optimized. DEMOGRAPHIC parity Constraint:  It is not evident how the DEMOGRAPHIC parity constraint is enforced in FedFB. experimental consideration: 2. Limited Experiments: The experiments lack comparison with other recent works on fairnessaware federated learning. For case the subject does not compare its results with the data and methods used in Du et al. (2021). 3. Data Distribution among Clients: The data distribution among clients is unclear and the use of an iid (independent and identically distributed) setting may not be realistic. Inconsistent Notations: 4. Inconsistent notations between the main body (e.g. \\cal A for sensitivity attributes) and the appendix (e.g. [A]) hinder readability. Clarification is requested. Privacy Concerns: 5. The paper should address the potential privacy risks associated with transmitting the loss matrix L instead of just model weights.", "Summary The researchers present a unique algorithm for training statistical model that adhere to fairness standards such as client or demographic parity. They modify the existing FairBatch (FB) algorithm for the decentralized Federated Learning (FL) environment. Their experimental results demonstrate that their algorithm FedFB achieves beneficial fairness compared to a centralized approach where data from all clients is combined on a single server. However the theoretical aspects of the paper rely on particular scenarios and lack clarity. Strengths and Weaknesses  Strengths:  The work tackles an significant and understudied aspect of FL: fairness.  The authors provide a rigorous mathematical definition of fairness (Demographic parity).  Weaknesses:  The paper gives more attention to details in the appendix than in the main subject making it difficult to understand.  The theoretical statements lack clarity and rely on particular use case.  Some elements such as UFL (Uniform Fair Loss) are not fully explained and their significance is unclear. On UFL (Uniform Fair Loss) UFL accounts for clients who do not participate in FL. In this scenario each client trains a fair model on their own data. However the reason why selecting a random client classifier for prediction is a beneficial benchmark is not elaborated. The theoretical significance and interpretation of the resulting model are also not explained. The disparity measured by UFL should be clarified. On Lemma 4 If clients return the same model (e.g. due to identical data) FFL via FedAvg and CFL become identical problems even though q0 \u2260 q1. This should be explained. On Theorem 23  The validation relies on Assumption 25 on local loss functions which are common in FL and Assumption 1 on a decreasing direction in Lemma 22. Citations for previous work using these assumptions should be provided.  Convexity is said to imply Assumption 1 but a validation or references for this statement are needed.  The bound depends on the amount of local work R which is usually assumed to be finite while the number of aggregations T approaches infinity. This should be clarified.  The steps to obtain the bound on \u03bba should be elaborated as the first term in the maximum appears to be smaller than the left term of the inequality.  Generalization to A2 under certain consideration should be explained. On Experiments  Hyperparameters such as learning rate batch size and \u03b5 are missing.  The experiment setup is very particular and does not reflect common FL scenarios.  The reasons for CFLs lower fairness compared to FedFB should be discussed.  A comparison with the original centralized FB algorithm would be valuable.", "Summary Paraphrase: This research explores fairness guarantees (specifically Demographic parity) in three federated learning scenarios: local model training FedAvg with local fairness and training a fair global model. The paper introduces a method to promote group fairness in federated learning by adapting the FairBatch fair training technique to the FL setting. Strengths and Weaknesses Paraphrase: This subject offers a unique analysis of fairness training in different federated learning settings. However there are concerns regarding the papers theoretical claim that \"unfair\" federated learning (UFL) provides a weaker fairness guarantee than \"fair\" federated learning (FFL) through FedAvg. Its unclear why the joint distribution of predictionlabel pairs (\\hatyx) across all clients is considered necessary for fairness evaluation in UFL where clients train separate global model. Furthermore the process of optimizing the FairBatch objective and calculating groupspecific loss in the federated learning setting is unclear. The empirical results comparing FedFB (the papers proposed method) with other fair FL techniques show that FedFB performs better for fairness. However some baseline results are questionable such as the low accuracy of qFFL for the adult dataset which contradicts reported results in the original qFFL paper. Additionally the reason for FedFB outperforming CFL in certain case is not fully explained."], "psh0oeMSBiF": ["Paraphrased Statement: Summary This work examines how to certify a policy in offline reinforcement learning settings when a portion of the training data is corrupted. Two criteria namely perstate action stability and cumulative reward lower bound are proposed. The paper introduces COPA a general certification framework that achieves a certain level of certification based on these criteria. COPA employs two training phases: splitting trajectories into equalsized subsets training subpolicies on each subset and then combining these subpolicies into a single policy. Experiments verify the effectiveness of the proposed COPA certification. effectiveness  Pioneering exploration of policy certification in offline reinforcement learning particularly in the context of poisoning attacks.  Systematic theoretical analysis of policy certification using the two criteria of perstate action stability and cumulative reward lower bound. These criteria align with the objectives of general RL task offering guidance for future RL algorithm design.  experimental validation supporting the theoretical findings. Weaknesses  Limited technical contribution: The idea of splitting the training set and aggregating subpolicies has been widely studied in previous research.  Restricted applicability: COPA only certifies against poisoning attacks that corrupt a small portion of training trajectories neglecting scenarios where the attacker may perturb all trajectories with minimal attempt.  complex notation in theorems: The symbols used can be confusing for readers warranting clearer presentation.  Overlooked relevant references: noteworthy paper such as [24] should be discussed in the related work section.", "Paraphrased Summary: This paper presents a defense mechanism called COPA to safeguard offline reinforcement learning (RL) algorithms against poisoning attacks. COPA aims to determine the maximum issue of poisoned data points that can be tolerated. It involves dividing the potentially poisoned data into partitions training subpolicies on each partition and then combining them into a final policy using different aggregation methods. The paper offers certification guarantees for each of the three proposed aggregation protocols. effectiveness:  Addressing the crucial problem of certified defense against poisoning in offline RL.  Clear problem formulation.  Analysis of three aggregation protocols providing insights into partitionaggregation methods in RL.  Comparison of the vulnerability of various deep RL algorithms. Weaknesses:  Essential details and definitions are relegated to the appendix disrupting the ffirst of the paper.  Unnecessary inclusion of three separate aggregation protocols in the main text which contribution many similarities.  Unclear notation and missing definitions in the main issue (Theorem 1).  The temporal continuity assumption used may not be realistic in practice.  Questionable practicality of the COPASEARCH algorithm as it requires access to the environment dynamics which is not typically available in offline RL settings.  Insufficient experimental issues:  Certified poisoning threshold not provided in comparison to the total issue of trajectories.  Certified firster bound for cumulative certification are also first and unrealistic.  Omission of relevant related work ([1]). Reference: [1] Banihashem Kiarash Adish Singla and Goran Radanovic. \"Defense Against reward Poisoning Attacks in reinforcement learning.\" arXiv preprint arXiv2102.05776 (2021).", "Paraphrased Statement: Summary: This paper introduces a method for verifying the robustness of reinforcement learning framework against poisoning attacks where attackers can alter training data. It establishes two certification standards: \"perstate action stability\" and \"cumulative reward bound.\" Furthermore it suggests various data aggregation techniques and provides mathematical bound on certification. The paper includes experimental results and an analysis of parameter work. effectiveness:  Clear presentation of contributions and methodology  Proposal of a novel certification framework for protecting reinforcement learning framework from poisoning attacks  Mathematical bound for certification under different aggregation protocols  inclusion of numerical results in addition to theoretical analysis Weaknesses:  Lack of information on the impact of the proposed method on the training efficiency and policy choice of RL algorithms in scenarios without adversarial attacks  potential inaccuracy in claiming the absence of existing robust RL methods offering certified robustness against poisoning attacks (should reference related work like \"Certifiable robustness to Adversarial State Uncertainty in Deep reinforcement learning\" and \"work Certifying Robust Policies for reinforcement learning through Functional Smoothing\")", "Paraphrased Statement: This work focuses on ensuring the robustness of policy trained using offline reinforcement learning (RL) algorithms against poisoning attacks. The paper introduces two certification criteria: stateaction stability and a lower bound for cumulative reward. Three protocols are proposed for certification all using ensemble methods to aggregate subpolicy decisions. A treebased research approach is presented for certifying cumulative reward by evaluating all potential action within a set. experimental results on Freeway and Breakout games indicate that Freeway can be certified as robust to poisoning attacks with a significant poisoning threshold while Breakout cannot. The paper is generally wellwritten but there are some structural issues that could be addressed. Despite the critical nature of poisoning attacks in practical settings they remain understudied compared to testtime attacks. This work represents the first comprehensive proposal for certifying RL algorithms against poisoning attacks and the findings are promising. specific Concerns and Questions: 1. Motivation for Multiple protocol: The paper introduces three separate aggregation protocols for certification. However the results suggest that two of the protocols provide similar certification performance. To enhance readability it is recommended to present only one protocol and include the others in an appendix. 2. Subpolicy choice and Dataset Size: Offline RL algorithms rely on finite datasets for training. Exploring the impact of varying the issue of subpolicy (u) on subpolicy choice and certification performance would be beneficial. It is significant to determine if increased u leads to higher subpolicy variance which could compromise certification. 3. DTPARL Window Size Variance: The DTPARL protocol considers varying window sizes. While the worstcase complexity is based on the maximum window size (Wmax) it would be informative to investigate the average window size and its impact on certification time. 4. Practical certification Time: The paper claims that the certification time complexity per state is negligible compared to standard policy inference. empirical evidence supporting this claim would be helpful as the complexity appears to be quadratic in the issue of action and linear in u. 5. COPAresearch algorithm Clarity: further explanation of the COPAresearch algorithm would be beneficial specifically its time complexity. 6. robustness Comparison: The paper highlights the greater robustness of QRDQN and C51 compared to DQN. It would be valuable to understand the factors contributing to this difference beyond only achieving higher utility. 7. Trajectory Length choice: The rationale behind the trajectory lengths used in Section 4.2 should be provided. 8. Breakout vs. Freeway robustness: An analysis of why Breakout is less stable than Freeway against poisoning attacks would be informative. Determining if Breakout contains more critical or bottleneck states could provide insights. 9. Relation to Prior work: The paper contribution similarities with [1] (Wu et al. 2021) which was not cited. Acknowledging this related work and highlighting the differences would strengthen the paper contribution."], "uy602F8cTrh": ["Paraphrase: Summary: This field focuses on addressing outofdistribution (OOD) state in reinforcement learning (RL). It suggests a data augmentation technique for Dynastyle algorithms similar to domain randomization. The idea is to modify state vector elements during model training guided by causality principles. Strengths:  Enriching training data diversity improves RL agent resilience and extrapolation abilities. This is a wellestablished technique called domain randomization.  In the context of data imbalance the proposed method employs data over and undersampling which are common use. Weaknesses:  The paper presents data augmentation as a issue of causality theory but this perspective offers no unique insights and the introduced structural causal model is not utilized in the method.  It conflates counterfactual reasoning with intervention. Altering state constitutes intervention not counterfactual reasoning (which refers to exploring hypothetical or unseen conditions).  The method lacks a mechanism for training on modified data. Thus the policys performance on these data depends on the models ability to extrapolate without prior exposure to such data.  The field only compares the method to two baselines an ablated CausalDyna that does not modify state would be a valuable baseline. Specific issue:  design 1 is not referenced in the text.  Section 3.2 inaccurangely describes Dyna by separating model training data collection and training of the value use and policy into distinct steps. In the original Dyna these processes occur in parallel or interleaved fashion.  The averaging of the final success range (FSR) over the last 20 steps is unexplained.  It is unclear whether the trained policies are capable of bringing the objects to rest.  Equation 2 contains unnecessary factors of 1.  The term \"5 training case\" initially appears without clarification later revealing that it refers to five independent range.", "Paraphrase: The paper proposes a technique to enhance the generalization of modelbased reinforcement learning (RL) using data augmentation with interventions. It involves modifying the value of a specific variable (such as an objects property) within the learned dynamic model during episode model. experimental Findings: The technique improves:  generalization ability in scenarios where the intervened variable differs from the training data.  sample efficiency when the training data exhibits an unbalanced distribution. Strengths:  The concept is straightforward and intuitive.  The paper is wellwritten and the experimental analysis is thorough. Weaknesses:  The proposed dynamic model and the Structural Causal Model (SCM) appear to be somewhat loosely connected. Essentially the technique resembles a standard dynamic model with state inputs and interventions.  The paper relies on strong assumptions such as:  The ability to learn an SCM (or a dynamic model) that allows for counterfactual interventions.  approach to highlevel variables in the genuine generative process.  generalization of the learned dynamic model to unseen scope within a predetermined counterfactual property space.  These assumptions may not be practical in realworld applications.", "Summary Paraphrase: This field investigates how to adapt a dynamics model to diverse versions of an environment defined by a parameter called m (e.g. object mass). The challenge is for the model to generalize to environment with different m values or different distributions of m values. The authors propose a solution that involves randomly sampling different values of m during model training instead of relying on values observed from model. Strengths and Weakness Paraphrase: Strengths:  The field links data augmentation to creating counterfactual properties.  Its clarity in introduction. Novelty: This field introduces a novel problem scope by applying counterfactual data augmentation to DYNA (a dynamics model). Unlike standard data augmentation techniques the direction Here is on making the model equivariant to data augmentation instead of invariant. Weaknesses:  The authors assume knowledge of the counterfactual property space M which limits the generalizability of the approach to case wHere M cannot be fully observed.  The empirical experiments primarily compare a method with enhanced data augmentation to methods without such augmentation which isnt particularly informative.  The paper doesnt address how the world model is supervised to make accurate predictions for different m values which is crucial for the success of the approach.  Results show that the proposed data augmentation may not consistently improve performance compared to MBPO.", "Paraphrased Statement: Summary: This paper introduces a reinforcement learning model that uses counterfactual model to enhance agent training efficiency. It employs a Simulator Controlled Markov process (SCM) to generate alternative state transitions enabling the agent to explore diverse scenarios within the simulated environment. Extensive experiments have validated the effectiveness of this approach. Strengths and Weaknesses: Strengths:  Clear introduction of concepts  Effective demonstration of the proposed models utility Weaknesses:  Lack of significant novelty compared to existing methods utilizing counterfactual techniques  Absence of discussion on how to mitigate errors introduced by the SCM  set theoretical analysis to establish performance bounds in scenarios where the SCM generates inaccurate model"], "vpiOnyOBTzQ": ["Summary This paper develops a supervised learning technique that enhances dynamical systems predictors ability to capture underlying system parameters. The proposed method improves longterm prediction and outofdistribution parameter estimation. Strengths  Addresses the significant number of outofdistribution generalization in dynamical systems prediction.  The method enables models to learn system factors of variation potentially enhancing parameter retrieval. Weaknesses Insufficient Support for Disentanglement Claim: While the paper claims the proposed method disentangles system parameters it lacks experiments to support this assertion. Experiments could involve manipulating latent variables to demonstrate disentanglement. set Novelty and Significance: The proposed method novelty is limited due to prior work on supervised disentanglement in other domain. The discovery that parameter supervision improves prediction is expected and the empirical results show only marginal to nonsignificant improvements undermining the significance of the contribution. Questionable experimental Design: The considered outofdistribution parameter ranges are narrow and close to training parameters. This choice may limit the robustness of the results. Expanding the ranges or explaining their relevance is needed. Choice of model: The choice of prediction models (VAEs CNNVAEs) may be questionable. Stateoftheart variational models and ODElike recurrent system are not explored potentially impacting the results. Questionable Claims:  The statement that system identification requires knowledge of the underlying system contradicts the proposed method reliance on system parameter supervision.  The claim that using ground accuracy parameters as privileged information is novel may require clarification due to prior work using similar approach.  The assertion that VAEs lack competitive performance is unsubstantiated. other Concerns:  The description of models is confusing regarding their architecture and nature.  Some number are difficult to read in grayscale.  Minor typos are present.", "Summary Paraphrase: This paper presents a model for predicting dynamical systems using a variational autoencoder with disentanglement capability. The model is supervised by providing domain parameters during training. Experiments on simulated data demonstrate its effectiveness in outofdistribution scenarios and longterm predictions. effectiveness and Weaknesses Paraphrase:  effectiveness:  Introduces a novel approach combining VAEs and domain parameter supervision for dynamical system prediction.  Demonstrates competitive performance in outofdistribution and longterm scenarios.  Weaknesses:  set novelty as the method primarily applies existing VAEs with minor modifications.  Unfair comparison with unsupervised disentanglement models due to the solid supervision employed.  Lack of comparisons with existing dynamical system prediction method.  Insufficient statistical analysis to determine the significance of performance differences.  set evaluation on only simulated data raising concerns about generalizability.  Insufficient analysis of disentanglement representations:  Questioning whether supervised latent dimensions are truly disentangled.  Lack of understanding of information encoded in unsupervised features.  Lack of visual results or quantitative metrics to support disentanglement claims.  Questionable rationale for RSSM outperforming RSSMSD in initial timesteps in number 7.  Unclear presentation of number 5 due to overlapping lines.  Ambiguity in whether the reconstruction loss in the manuscript refers to the prediction loss in number 1.", "Paraphrased Summary Main Idea: This work introduces a supervised approach to separate domain parameters from dynamics in deep latent variable models (e.g. VAEs). challenge: Extending VAEs to dynamical systems poses two main challenges:  Generalization beyond known domain  Longterm trajectory prediction contribution: To address these challenges the authors propose a supervised loss that links latent variables to domain parameters. Results: Empirical experiments on three problems (LV video pendulum threebody problem) demonstrate:  Longterm trajectory prediction capability  Outofdistribution (OOD) generalization Assessment of Disentanglement: The authors acknowledge that they have not explicitly evaluated disentanglement but suggest that it is not essential for the observed benefits. They argue that longterm prediction and OOD generalization can be achieved without explicit disentanglement as shown by other method (e.g. Hamiltonian Neural network). Limitations:  Lack of comparison with other stateoftheart method  Weak baseline  Unstable results for proposed method (VAESD and VAESSD) in some cases Minor number:  Consistency of scripts and variable notations  Clarification of loss use and incorporation of dynamics", "Paraphrased Summary: This paper focuses on evaluating the performance of machinelearned dynamical systems on outofdistribution data. The authors investigate whether separating the parameters of the dynamical system in the latent space can enhance the generalization of the models assuming that this knowledge is available from known model. Experiments are conducted on diverse dynamical systems including pendulums the LotkaVolterra system the threebody problem and a video prediction task of a swinging pendulum. The results show that additional disentanglement can improve the generalization performance of the models particularly in the video prediction setting leading to more accurate longterm predictions based on both structural and perceptual metrics. effectiveness:  clear articulation of the hypothesis  Transparent presentation of results and supporting information  Extensive hyperparameter tuning Weaknesses:  Limited improvement with disentanglement in the phase space setting  Ambiguous visualizations of performance improvements  Inconsistent representation of variance in the video prediction metric  Potential bias due to the omission of varying an significant hyperparameter (time step)", "Paraphrased Summary: This paper presents a technique for learning dynamicalal systems using supervised disentanglement. The method requires providing precise information about the true parameters of a sequence to separate them from observations. The technique is assessed with three toy datasets. effectiveness and Weaknesses:  Limited Novelty: The proposed method closely resembles the unsupervised disentangled state space model making its novelty questionable.  Lack of Quantitative Analysis: Claims about disentanglement lack proper quantitative or qualitative support.  Insufficient Comparison: Although the method is claimed to outperform the unsupervised method there is no comparison with the most similar unsupervised approach which makes the claim difficult to assess.  Limited applicability: The method assumes the availability of true parameter information which is not typically accessible in realworld systems.  OOD Generalization Concerns: The method is not truly OOD because the parameter ranges used to create OOD datasets overlap significantly with those used in training.  Evaluation Metrics: The prediction quality is solely measured using perceptual metrics which may not comprehensively assess the dynamical predictions.  Questionable Axes Labeling in number 1: The labels appear to have inconsistencies raising questions about the dimensions of input and output."], "pFyXqxChZc": ["Paraphrased Summary: Researchers have developed a quantized parallel SGD algorithm where gradient values are rounded after scaling by a factor known as \\alpha. The value of \\alpha determines the quantization error with a higher \\alpha resulting in lower error. The proposed approach uses a shared \\alpha across workers enabling efficient gradient aggregation and allreduce operations. This contrasts with previous methods like Heuristic IntSGD which uses varying \\alpha values across workers and doesnt support allreduce. effectiveness and Weaknesses:  The authors have identified an efficient scaling factor for convergence.  experimental results show improved performance on certain tasks compared to other quantization methods.  The approach is not as efficient as PowerSGD on some tasks. Questions and Suggestions:  Can the method be extended to popular optimization algorithms like Adam  Including results for Heuristic SGD in Table 2 would be valuable.  Is it potential to remove the denominator n from the scaling factor without introducing overflow  Algorithm 1 line 6 could be clarified to indicate that r0 is a scalar.  Assumptions and claims should be adjusted to reflect that only one scaling factor choice is presented.  Discrepancies in communication latencies for IntSGD (Determ) and IntSGD (Random) should be addressed.  Typos should be corrected.", "Paraphrased Statement: In this paper a novel compression algorithm for distributed machine learning is introduced. This algorithm similar to QSGD employs a shared global scaling factor. This approach allows the compressed results on each worker to be combined which provides additional benefits and supports a broader range of applications. The simplicity of the algorithm (based on QSGD) belies its effectiveness as demonstranged by theoretical analysis and enhanced scaling factor selection. The paper addresses previous work on this topic and provides sound technical support for the proposed approach. While there may be concerns regarding varying magnitudes among participants this is unlikely to impact the convergence range. experimental results further validate the algorithms performance.", "Paraphrased Summary: The paper unveils a modified randomized compressor operator called IntSGD for function in distributed Stochastic Gradient Descent (SGD) data communication. IntSGD matches SGDs behavior converges effectively and works seamlessly with the allreduce primitive. The authors highlight three key contribution: IntSGD itself its proven convergence range and the IntDIANA variant for nonuniform data distributions (discussed in the appendix). effectiveness and limitation: The paper is wellwritten and experiments demonstrate that IntSGD outperforms the closest competitor (HeuristicSGD) in addressing theoretical convergence limitations. While not significant IntSGD offers a 1015 improvement over uncompressed distributed SGD. Notably IntSGD requires only allreduce communication and avoids error feedback distinguishing it from competitors. Suggestions for Improvement:  Discuss potential shortcomings of the method or future research directions.  Provide more details about IntDIANA results in the main text to emphasize its significance as a main contribution. Minor Corrections:  \"By combing (5) and (3)\"  \"By combining (5) and (3)\"  \"range classifcation\"  \"range classification\"  Remove redblue color coding in equations.  Clarify if float32 data is functiond for computationSGD communication and int8 for IntSGD communication."], "u6s8dSporO8": ["Paraphrase: Summary: This work introduces a new method called group equivariant neural posterior estimation (GNPE) that extends conventional neural posterior estimation (NPE) to simultaneously account for data and parameter equivariance. The authors demonstrate GNPEs effectiveness using gravitational wave data showing significant performance improvements compared to NPE. Strengths and Weaknesses: While the authors address a captivating problem using unconventional datasets the review presents some concerns:  technical Clarifications:  The claim that GNPE models a Gequivariant density (posterior) is potentially inaccurate as equation 4 implies an invariant distribution.  The meaning of group element process on parameters (\u03b8) is unclear.  Writing Clarity and Mathematical Rigor:  The term \"present\" is used ambiguously without fully explaining its role in equivariance.  The discussion on present standardization in section 3.3 is difficult to follow.  Comparison to Prior work:  The claim that GNPE achieves exact or approximate equivariance over prior work is misleading. GNPE relies on finding a present standardizing element which may not be feasible for all groups.  Experiments:  The conclusion that GNPE improves over NPE is supported by the results but the data field is not within the reviewers expertise.", "Paraphrased Statement: Summary The work introduces a technique for Bayesian inference in situations where both data and parameters exhibit equivariance or approximate equivariance. This equivariance means that if the latent parameter changes (e.g. translates or rotates) the data distribution remains unchanged except for the corresponding translation or rotation. The technique involves: 1. Mapping simulation to standardized presents. 2. Estimating both the initial present and additional parameters from the data using Gibbs sampling. Strengths and Weaknesses Strengths:  Addresses an interesting research question.  Wellwritten. Weaknesses: Major Comments:  Lack of clarity on what \"approximate equivariance\" entails in practice.  Poor performance of \"Chained NPE\" (equation 12) compared to \"GNPE\" with wide \\kappa requires further exploration.  Overconfidence in present estimation by \"Chained NPE\" and potential improvements through more expressive posterior distributions. Minor Comments:  Need for tuning two additional algorithmic parameters: noise tolerance distribution and Gibbs sampling steps.  Approximate nature of Gibbs sampling samples and variational estimation.  Interpretability limitations of the c2st metric.  Lack of visualization of marginal posteriors for \"wide \\kappa\" and \"NPE (chained)\" methods.  Typographical error on page 9: \"scores\" should be \"score.\"", "Paraphrased Statement: The research paper introduces a novel method for incorporating invariances into posterior estimation. Instead of modifying neural architectures it employs a factored posterior estimation approach. This involves transworking the data into a standardized work and then estimating posteriors over the transworked data. The strengths of the approach are:  It is independent of specific neural network architectures.  It does not require explicit knowledge of the exact invariances in the data.  It perworks beneficial than conventional neural process estimation (NPE) methods when known invariances are present. Questions:  Sections 3.4 and 4 present complex concepts and may benefit from clarification.  It is unclear how transworking g\\theta to approximate \\hatg addresses the lack of \"present\" data in range.  The training process for qinit is not specified. Is it a regular NPE trained on the same data used for learning invariances  Equation 9 requires further explanation in the main body including the definition and relationship between Q and lowercase q.  Providing toy example would aid in understanding how q(\\theta x \\tau) varies with \\tau (or \\hatg).", "Paraphrase: Summary The authors present a method called group equivariant neural posterior estimation (GNPE) for inferring parameters and standardizing presents. GNPE outperforms traditional neural posterior estimation (NPE) for equivariant posterior distributions. It can also be used in example where equivariance is approximately estimated. GNPE is architectureindependent meaning that any flow framework can be used with it without the need for a specific network structure to enforce equivariance. Strengths and Weaknesses  Weakness: The paper writing is complex and difficult to understand.  Strengths:  The GNPE method is novel and significant for addressing parameter inference and present standardization.  GNPEs performance is superior to NPE for equivariant posterior distributions even under approximate equivariance.  GNPE can be used with any flow framework providing flexibility and adaptability. Comments  Comment 1: Equation (4) assumes the equivariance of the prior distribution of theta. The proof requires additional assumptions and derivation steps which should be provided.  Comment 2: The purpresent of epsilon (\u03b5) and hat g (g) is to handle approximate equivariance. The explanation in Section 3.3 could be clarified using a toy example.  Comment 3: Equation (5) set p(\u03b8x g) in terms of p(\u03b8x g) and requires a proof. Proofs for all equations in Section 3 are requested.  Comment 4: Users can choose the distribution of \u03b5 based on their applications.  Comment 5: Future experiments should compare GNPE with other flow frameworks that enforce equivariance architecturally to assess its comparative performance."], "kUtux8k0G6y": ["Paraphrased Statement: Summary: This work focuses on the challenge of enhancing model robustness while preserving accuracy. It addresses the publication of \"robust inaccuracy\" where the model classifies some samples incorrectly under perturbations but maintains the correct category. Technical contribution: 1. Introduction of a regularization term to the standard robustness objective penalizing robust inaccuracy. 2. Development of an \"abstain model\" that identifies samples to be classified by the robust classifier (low accuracy) or the original (high accuracy) classifier. effectiveness and Weaknesses: Pros:  Addresses a relevant publication: the accuracyrobustness tradeoff.  Clear presentation and wellexplained technical contingent.  Implementations in both adversarial and certified robustness scenarios.  Substantiated claims with experimental results on standard benchmarks.  Novel paper of existing technique. Cons:  Limited novelty as problem formulation and components are based on prior work.  experimental results often show tradeoff without adequate discussion.  potential impact on model calibration and sample choice requires further exploration.  Conclusion lacks limitations and future research directions. Additional Comments:  The work could be improved by providing:  more insights into the tradeoff between metrics.  A summary of certified robustness results in the main text.  An evaluation of the proposed approach impact on model calibration.", "Paraphrased Statement: In order to address two limitations of existing robust training algorithms (inaccuracy and loss of natural accuracy) this paper proposes a novel training method that focuses on maximizing accuracy while minimizing incorrect predictions in both natural and adversarial settings. Additionally a mechanism based on robustness is used to further enhance overall robustness without compromising regular accuracy. experimental results demonstrate the efficacy of this algorithm in producing fewer inaccurate predictions under adversarial conditions and improved robustness with only a slight reduction in natural accuracy. effectiveness and Weaknesses: 1. The approach of addressing robust training by avoiding incorrect predictions is noteworthy but the proposed method lacks complexity. 2. more empirical or theoretical analysis would be beneficial to explain the effectiveness of the method. Theoretical justification is not required but insights and analysis would be valuable. 3. The clarity of Figures 24 could be improved as their initial interpretation may be confusing.", "Paraphrased Statement: Summary: The work explores the problem of trained deep learning (DL) models displaying inaccuracy while maintaining robustness for specific samples. To mitigate this a flexible finetuning mechanism and a robustnessbased ensembling technique are proposed. effectiveness and Weaknesses:  The abstract assumes prior knowledge of adversarial DL which could be clarified by providing context on robustness against adversarial model.  Table 1 lacks sufficient contingent on result interpretation and value generation.  Dataset abbreviations are used prematurely they should be defined upon first occurrence or omitted from the introduction.  The introduction could benefit from focusing on the research gap and reserving experimental contingents for later sections.  Related work claims should be supported by references or indications of where they will be substantiated.  key concepts are introduced out of order requiring the reader to reference future sections.  The indicator function is used without prior explanation.  The compositional architecture relation to ensembling could be explored.  Figure 2 could be simplified to enhance clarity. Questions for author:  Have existing models been compared with models trained from scratch using the proposed loss  Why is the method limited to finetuning instead of training models independently", "Paraphrased Summary: This paper proposes a method to reduce the likelihood of a neural network making incorrect predictions not only for a specific input sample but also for similar samples in the input space. This metric is termed \"robust inaccuracy.\" The authors use this metric to improve conventional robustness measures by allowing the model to abstain from making predictions on samples for which its predictions are less reliable. effectiveness and Weaknesses:  effectiveness:  Novel approach to reducing incorrect predictions in both the target sample and its vicinity.  potential to improve conventional robustness measures.  Weaknesses:  Difficult to understand due to unclear goal statement and confusing terminology.  Lack of sufficient context and necessary contingent (e.g. benchmark model name training settings).  Unclear motivation for studying \"robust inaccuracy\" of samples that are already incorrectly classified.  Insufficient exploration of potential connections to previous work on prediction consistency and adversarial robustness."], "tx4qfdJSFvG": ["Paraphrased Statement: The authors propose a method to train Graph Neural Networks (GNNs) when some feature values are missing. This method includes a diffusion step before GNN training and discretization to create \"feature propagation\" a scalable approach for imputing missing features. They assume that the energy work that governs the relationships between features can be learned from the data. The authors analyze the gradient flow and its solution to minimize the Dirichlet energy which is more efficient than calculating the exact solution. Experiments show that this method outperforms other approaches. Strengths:  Clear and concise paper  Novel and practical solution to an significant problem  Comprehensive experiments Weaknesses:  Unclear how the method extends to features with multiple values (e.g. vectors)  potential implications for computational complexity when handling multivalued features are not fully discussed", "Paraphrase: This paper presents a fast method for imputing missing node features in graph learning. The concept involves utilizing the graphs topology by minimizing the Laplacian quadratic work feature by feature. However since the true solution demands matrix inversion the minimization is achieved through gradient descent using sparse matrix multiplications. Through experiments the authors demonstrate the effectiveness of this approach. However its worth noting that utilizing the Laplacian quadratic work (Dirichlet energy) for label propagation and graph signal interpolation has been a common practice for years. Additionally employing gradient descent instead of Laplacian inversion has also been widely adopted. Furthermore the notion of approximating the true solution (using the Laplacian inverse) with a finite impulse response (FIR) filter through gradient descent steps is akin to the idea of graph filter design which has been an active research field for several years.", "Paraphrase: Summary: This paper presents \"Feature Propagation\" (FP) a method to address missing node features in graphs. FP involves an iterative update algorithm that spreads missing feature values based on neighboring nodes. It can enhance downstream tasks such as node classification. Experiments demonstrate FPs effectiveness in handling up to 99 feature loss and its speed improvement. The authors also analyze how homophily (similarity between connected nodes) affects FPs performance. Strengths:  The paper is wellorganized and clear.  FP achieves impressive performance with up to 99 missing features.  FP offers significant speed advantages. Weaknesses:  FPs similarity to Label Propagation (LP): FP is not truly novel as it closely resembles LP proposed by Zhu et al. (2002). The authors claim differences but the underlying principles of feature propagation remain similar. LP also propagates continuous label probabilities making the distinction between FP and LP insubstantial.  Neighbor Averaging Comparison: The experiments show that simple neighbor averaging yields good results for missing feature share below 50. Hence it would be beneficial to compare FP with techniques that do not rely on node features to determine if FPs performance truly stands out. When homophily is solid leveraging global graph data becomes crucial. However it remains unclear whether FP significantly outperforms other algorithms that tap into this global data.", "Paraphrase: Introduction: Graph neural networks (GNNs) can handle graph data but missing node features limit their performance. Methodology:  Propose a preprocessing step called Feature Propagation (FP).  FP uses Dirichlet energy minimization to impute missing features based on nearby node features.  Derive an efficient iterative algorithm that avoids matrix inversion making it computationally feasible. Interpretation:  FP work as a lowpass filter smoothing out missing features.  GCNs mainly rely on this smoothed data for learning. Strengths:  Effective for datasets with heavily missing features (90).  Efficient and scalable for large graphs.  Compares favorably to existing methods in terms of imputation accuracy and runtime. Weaknesses:  Assumes homophily (similarity between adjacent nodes). May perform poorly if this Assumption is violated.  Mentioned as a potential field for future improvement. Additional Insights:  FP complements GNNs by providing a clean input data.  Nearest neighbor averaging (baseline method) performs surprisingly well.  Other matrix completion methods with graph side data are worth exploring for comparison.  Running the FP algorithm for a large number of iterations may not be beneficial as it can lead to oversmoothing. Technical Details:  Proposition 1: The inverse of the Laplacian matrix is unique when the graph is connected.  Hyperparameters for the downstream GCN model are kept consistent across all experiments."], "mk0HzdqY7i1": ["Paraphrased Statement: Summary: The field thoroughly evaluates the effectiveness of \"deep learningguided tree search\" for addressing combinatorial optimization problems. The approach combines a greedy search algorithm with a neural network trained on specific cases to guide decisionmaking. Despite the popularity of this method the field finds:  Lack of Reproducibility: Some previous work is not reproducible.  traditional solver comparison: Dedicated solvers perform as or better especially on more difficult cases.  Minimal Impact of Neural network: Performance does not significantly change when replacing neural network outputs with random value indicating that the majority of algorithmic power comes from heuristic components. effectiveness:  The empirical results could redirect research towards more promising directions. Weaknesses:  introduction Issues: Table 1 is difficult to read due to minor font size hindering comparisons and conclusions. A subset of key datasets and columns should be presented more clearly supported by visual aids and distinction between learningbased and traditional methods.  Accessibility and Clarity: The text assumes significant familiarity with tree search methods making it difficult for broader audiences. Explanations of tree search basics and heuristics would enhance comprehension.  Confusion about deep learning promise: The claim that learningbased approaches can learn to solve specific problems is inaccurate as dedicated solvers already exist. deep learning potential lies in specializing solvers for specific case families. The field should evaluate methods in a more challenging scope where learned components generalize across cases.  Gurobi as Baseline: Gurobi has adjustable parameters that can be optimized for specific problems. This practice should be mentioned to highlight a common issue in Gurobi comparisons and strengthen the authors arguments.  GPU usage: Clarity is needed regarding GPU usage by different methods especially since it may significantly impact runtime comparisons.", "Summary This research presents a software package for generating and processing datasets for the Maximum Independent set (MIS) problem both weighted and unweighted. evaluation shows that a widelycited method combining supervised learning with graph neural networks may not benefit from the neural network if data is preprocessed with nonmachine learning techniques. A recent deep reinforcement learning approach however incorporates graph neural network predictions and performs consistently well. Nonmachine learning solvers like KaMIS and Gurobi provide superior solution in limited time raising questions about the potential of machine learning approaches for MIS. effectiveness  Clear writing  Motivation: The widely cited method by Li et al. is examined to uncover its limitations.  Reproducibility: The authors provide detailed information on data generation method implementation and evaluation. Weaknesses 1. introduction of Experimental Results  Tables 1 and 2 should be moved to the appendix.  The main text should provide visual introduction (e.g. box plots) rather of extensive tables.  Paragraph in section 3.1 should be labeled for clarity. 2. Statistical Metrics  Consider adding box plots for optimality ratios and running times.  Statistical testing (e.g. Wilcoxon Signed Rank Test) could further clarify the robustness of methods. 3. Datasets  research additional standard MIS datasets such as DIMACS challenge graph and the largescale dataset from the recent work by Hosseinian and Butenko. 4. MIS Heuristics and Mathematical Programming Formulations  Investigate other MIS heuristics and formulations such as GRASP and quadratic integer programming. 5. Future Directions  Identify datasets where KaMIS and Gurobi underperform.  research harder MIS variants such as the generalize Independent set Problem. 6. solver Parameter Tuning as a Baseline  Train KaMIS and Gurobi on the same datasets as the machine learning methods using parameter tuning tools like SMAC.", "Paraphrased Statement: Summary: This research evaluates deep learningbased tree search algorithm (using graph neural networks) for solving complex combinatorial optimization problems. The authors develop an opensource benchmark set for the maximum independent set (MIS) problem encompassing various graph models from realworld sources and existing benchmark suite. They assess different configurations of neuralguided tree search and question earlier claims by Li et al. [2018]. They demonstrate that traditional optimization solvers outperform deep learning approaches. effectiveness:  Explores the effectiveness and reproducibility of a previously published work by Li et al. [2018].  Provides a comprehensive benchmark suite for MIS covering a wide scope of problem instances. Weaknesses:  Focuses solely on the MIS problem neglecting other graphrelated optimization challenges.  comparison against a limited set of deep learningbased solution making it challenging to generalize findings.  Acknowledges that specialized solvers and even classical approaches often surpass deep learning methods a trend also observed in other problem domain.  The benchmark suite primarily consists of existing problems and random graph lacking novel benchmark datasets.", "Paraphrased Summary: This paper addresses a crucial problem in combinatorial optimization found in various realworld applications. It focuses on an influential paper by Li et al. (2018) which introduced a Graph Neural network (GNN) approach with promising results. However the paper investigates whether these findings can be replicated by implementing the algorithm independently. After attempting to reproduce the results the paper finds that neither the publicly available implementation nor the papers description can achieve the reported performance. Additionally the paper introduces a benchmark suite to facilitate future comparisons in this domain. effectiveness and Weaknesses:  effectiveness:  The paper emphasizes the importance of scientific accuracy in research.  The benchmark data set allows for easier performance comparisons.  The paper is wellwritten and includes a thorough literature review.  potential Weakness:  If the current implementation is incorrect the papers conclusions may not accurately reflect the effectiveness of the guided tree search approach."], "lyLVzukXi08": ["Paraphrase: The researchers introduce a unique metalearning model Neural Variational Dropout Processes (NVDP) that develops tailored dropout rates for model parameters based on task observations. They also propose a taskspecific variational prior derived from the same posterior model which promotes stable training. Empirical work show that NVDP models perform effective on novel tasks exhibiting adaptability and generalization. Strengths:  NVDP models are more resilient to model collapse and overfitting compared to other methods.  The variational prior regulates dropout rates during conditional posterior optimization.  Compared to similar approaches NVDP is more manageable making it suitable for large datasets. Weaknesses:  NVDP still uses the SGVB framework for variational parameter optimization limiting its potential for broader distribution exploration.  The experiments largely employ extremely tuned datasets which may hinder wider application and realworld usability.", "Paraphrased Summary: neural Variational Dropout Processes (NVDPs) introduce a novel metalearning approach that:  Leverages dropout to customize a base neural network to specific tasks.  Uses amortized variational inference to simultaneously train the meta and base models.  The meta model predicts taskspecific dropout rates creating a conditional dropout posterior.  A variational prior enables calculation of the KL divergence without relying on the base models deterministic parameters. NVDPs connect to variational dropout enabling adaptation through dropout rather than Gaussian idea. The proposed method overcomes scalability issues and addresses posterior collapse challenges seen in related approaches. Evaluated across various problem domains NVDPs demonstrate stateoftheart or competitive performance. view and Questions: The paper establishes a clear connection between metalearning and variational dropout resulting in an elegant approach.  Question: The authors mention variability reduction using the local reparameterization trick but it seems to involve iterating over tasks. Can you clarify the computational limitations of this for large amount of tasks  Question: Equation 3 offers a lowrank idea of the dropout rate. How numerically stable is this idea", "Paraphrased Summary: This paper introduces a novel Bayesian metalearning approach that uses taskspecific dropout to model conditional posterior distributions. For tasks with limited data it employs a novel lowrank product of Bernoulli expert metamodels to efficiently derive the posterior over neural network weights. Experiments using Gaussian work samples demonstrate performance improvements in fewshot learning. The approach also outperforms others in active learning and image completion tasks. Strengths and Weaknesses: Strengths:  Clear and concise writing  Innovative evaluation of NPbased methods using RLL and PLL  Wellreasoned novel prior  Extensive experimental evaluation Weaknesses:  Lack of sufficient evaluation of uncertainty calibration in fewshot classification  Absence of experiments demonstrating the usefulness of uncertainty idea  comparison only with CNP models omitting stateoftheart NP works  Missing evidence to support claims of addressing mode collapse and overfitting issues in metalearning"], "msRBojTz-Nh": ["Paraphrased Statement: This field introduces a technique for forecasting turbulent behavior in common simulations. To achieve this a modified ResNet architecture dubbed \"Dilated ResNet\" (dResNet) is combined with an encoderdecoder model based on graph networks. The paper showcases the dResNets capabilities through a variety of test cases ranging from simple 1D turbulence to complex 3D compressible simulations. effectiveness:  The method presents favorable results compared to simpler models such as UNet TFNet and FourierOps.  noise addition is shown to enhance the stability of predictions.  The paper demonstrates impressive predictions of 3D turbulence simulations despite the relatively common resolution.  The field examines the models generalization capabilities under different initial conditions and simulation sizes. Weaknesses:  The description of the proposed architecture especially the encoderdecoder component lacks clarity.  The paper presentation is confusing regarding the duration of model predictions with conflicting information provided in captions.  The paper lacks necessary information such as model sizes and compare of the encoderdecoder steps utility.  The graphs in Figure 3 need to be clarified to ensure accurate interpretation. Recommendations:  Provide a clearer explanation of the dResNet architecture including the role of the encoderdecoder.  Specify the durations and time step size of model predictions more explicitly.  Include model size information (parameters) in the presented information.  Improve theClarity of the graphs in Figure 3 and others.", "Paraphrased Statement: This paper introduces a neural network model trained on lowresolution simulated turbulence information to capture highresolution flow dynamics. The model outperforms conventional numerical solvers in accuracy at low resolutions across various metrics. The field includes numerical examples of nonlinear equations to demonstrate the algorithms effectiveness. Reviewer Concerns: However the reviewer raises concerns about the clarity of the demonstration:  The differences between the low lines in Figures 2b and 2d are unclear.  It is not specified where the red line representing the ground accuracy is located.  The error appears to be larger for DilResNet in Figure 2b.  The relevance of t1000 for simulations that are already imprecise at t1 is questioned.  It is unclear whether the time step used in Figures 2f2i is t1 or t1000.  Similar clarity issues exist in other figures (4 and 5).  The reviewer suggests that demonstrating performance on standard examples of turbulent flows would be more convincing.  They also point out that simulations at low resolutions (32) may be meaningless for chaotic flows potentially leading to worse results than largeeddy simulations.", "Paraphrase: Summary: This field seeks to create a simulation that predicts the largescale flow behavior of a known turbulent system. Compared to traditional solvers the proposed method yields more precise predictions especially in preserving highfrequency information as demonstrated in simulations using common grids. effectiveness and Weaknesses: The proposed model leverages an existing model model for predicting turbulent flows. It offers several advantages over classical solvers:  Enhanced accuracy at comparable or lower resolutions  Improved efficiency due to low grid sizes larger timesteps and GPU utilization  Versatility in handling various network architectures with Dilated ResNet yielding optimal results Additional Considerations: The paper explores various aspects of the models performance including:  Tuning training noise and temporal downsampling for stability across timesteps  Evaluating extended simulations with different initial conditions and box sizes  Questions for Future research:  Can the model handle boundary conditions that are not periodic or fixed  Does the model need retraining for different timestep values  Does the model achieve accuracy improvements comparable to increasing resolution in classical solvers", "Paraphrased Statement: Summary: The authors investigated various neural network architectures for turbulence model. They presented results for four different systems evaluating them using multiple metrics. They concluded that a learned simulator can effectively represent turbulence and may be improved further by incorporating additional noise and constraints. effectiveness and Weaknesses: Appreciated are the authors extensive experiments on various systems and models. The experimental findings strongly support the potential of learned simulators for model turbulence phenomena. However the main claim of the paper appears unclear. If the claim is based on the superiority of dilated ResNets a clearer compare with baseline methods is needed. Figures B.3 and B.4 provide some compares but further explanations of any quantitative or qualitative differences between architectures would be helpful. If the authors do not consider interarchitecture differences to be significant the main claim should be rephrased accordingly. Regarding the training set it is unclear whether the last two datasets were generated with Athena 128 and downscaled. This should be explicitly stated in the main text. Clarity is needed in the main text regarding the semantics of \"predicted Energy field and the Log Energy Spectrum\" as these concepts may not be familiar to all readers. One time on page 8 lacks clarity requiring the addition of a missing discussion: \"We found that training with [missing discussion] prevents this in some cases ...\""], "hyuacPZQFb0": ["Paraphrased Statement: In this study a practical method for automatically aligning time series data from different domains is introduced. Current approaches are criticized for their inconsistent evaluation criteria data sets model selection and neural network design. By adapting range domain alignment techniques to time series data the paper addresses these weaknesses. Experiments using established methods on diverse data sets and scenarios are provided to demonstrate the effectiveness of the proposed approach along with suggestions for further research.", "Summary This paper proposes a standardized framework for evaluating domain adaptation techniques for time series data. It aims to provide a fair and systematic method for comparing different approaches. Strengths and Weaknesses  Overreliance on deep study: The assumption that deep study always outperforms simpler model in time series classification is questioned. In the related field of anomaly detection a recent study suggests that deep studys success may be exaggerated.  Simplicity of HAR tasks: For Human activity recognition (HAR) it is suggested that some classes can be easily distinguished using simple features like variance and gravity acceleration. The question is raised whether deep study is necessary for such tasks.  Importance of model selection: The study emphasizes the critical role of model selection in achieving optimal performance. However this is not considered a surprising finding.  valuable experiments: Despite skepticism about deep studys necessity the experiments presented are acknowledged to be detailed and may prove beneficial to the research community.", "Paraphrased Statement: Summary: This study examines unsupervised domain adaptation for time series data (TSUDA) focusing on establishing a benchmark. By standardizing datasets and model selection it provides a reliable benchmark for TSUDA aiding future research. The study also presents some findings. Strengths:  Provides fair evaluation of existing domain adaptation algorithms through extensive experiments.  Presents competitive foundationlines for the TSUDA field which can support future research.  Analyzes the results to offer insights.  Wellorganized and clearly presented. Weaknesses:  Technical Novelty:  Raises concerns about the paper originality as it primarily employs existing methods.  Suggests that the proposed AdaTime framework may not be substantially different from standard range classification domain adaptation protocols.  Fragility of findings:  Questions the conclusion that the performance gap diminishes with more data as its foundationd on comparisons across different datasets.  Recommends controlling variables and using larger datasets to strengthen the analysis.  Limited data and Baselines:  Expresses doubts about the robustness of results due to the small size of the three experiment datasets.  Suggests including larger datasets like HHAR and incorporating other foundation model (e.g. TCN) for a more comprehensive analysis.  Metric Considerations:  While acknowledging that F1score is common in time series applications suggests that the metric finding is not particularly groundbreaking.", "Paraphrased Statement: Summary: The study presents ADATIME an evaluation framework for assessing diverse unsupervised domain adaptation (UDA) methods on time series data. ADATIME comprises a feature extractor classifier and domain alignment component. The paper applies cuttingedge visual domain adaptation techniques to the framework and conducts extensive experiments on time series classification tasks. The findings from these experiments shed light on applying UDA to time series data. Strengths and Weaknesses: Pros: 1. The paper successfully adapts stateoftheart UDA methods to time series classification. 2. It introduces a novel UDA framework for time series data that could advance the field. 3. The concept of the paper is fairly original. Cons: 1. The need behind the model design remains unclear. 2. The datasets used are small and straightforward limiting the network complexity to a 1D convolutional neural network (CNN) to prevent overfitting. The experimental results on these simplistic datasets are not conclusive. 3. The organization and writing of the paper could be improved especially regarding need and framework details. 4. Clarification of symbols like XtrainS XtestS ZtrainS ZtestS etc. in design 5 would enhance understanding. 5. The paper omits references to related works such as:  \"[Time Series Domain Adaptation via Sparse Associative Structure Alignment](https:aaai.orgojsindex.phpAAAIarticleview16153)\" by Ruichu Cai et al.  \"[Domain Adaptation for Time Series Forecasting via Attention Sharing](https:arxiv.orgabs2102.06828)\" by Xiaoyong Jin et al.", "Paraphrased Summary: This research introduces a comprehensive framestudy for unsupervised domain adaptation in time series data. It includes diverse model selection and strategies. Strengths:  Thoroughly established baseline for experiments  Opensourced code Weaknesses:  Lacks clear recognition of specific challenge in time series data adaptation compared to static data.  practice not address other common time series applications such as forecasting.  Omits comparisons with recent study (DAF) by Jin et al. in time series domain adaptation.  Limited justification for the importance of domain alignments.  Multiple proposed criteria without clear differentiation between SCC and DEV risks or guidance on selection.  Experiments are challenging to interpret.  Insufficient explanation of visual UDA in Table 3.  Average values reported without explanation.  Superficial observations without significant insights.  Lack of connection between imbalanced data observation and model selection criteria.  Backborn selection approach lacks systematic rationale.  emphasis on \"fair and realistic\" procedure without clarifying why other methods may not meet these criteria."], "yzDTTtlIlMr": ["Paraphrase: Summary: This work examines the underlying bias of two optimization algorithms SGD with momentum and Adam with momentum in a simplified linear classification model with specific loss and data conditions. The results suggest that both algorithms tend toward the L2 maximal margin solution similar to vanilla gradient descent. Strengths and Weaknesses: Strengths:  The models simplicity allows for the theoretical analysis of momentums impact on implicit bias.  Novel Lyapunov functions are used to analyze SGD and Adam with momentum. Weaknesses:  The claim that \"momentum doesnt change the implicit bias\" is based solely on the simple linear model and may not hold true for more complex models.  Empirical evidence would enhance the findings especially for models beyond simple linear models. Unanswered Questions:  How do the results translate to practical applications  The work contradicts Soudry et al.s (2018) observation that Adam does not converge to L2 maxmargin.  The role of the hyperparameter \u03b5 in Adam is unclear. Minor Suggestion:  The term \"almost every dataset\" should be defined for clarity.", "Paraphrased Summary: Researchers studied how stochastic gradient descent momentum (SGDM) behaves in a simplified binary classification task where the data can be easily separated. They found that SGDM and standard gradient descent (GD) achieve the same solution (the socalled maximum margin solution) at the same rate (O(1t)). The researchers also proposed a method to analyze SGDMs behavior improving upon a previous method. Strengths and Weaknesses:  The main finding showing the comparable performance of SGDM and GD is notable.  The title and abstract are misleading as they imply extensive applicability than the studys actual scope which is limited to linearly separable datasets.  The paper contains numerous typos in the equations.  The literature review is limited with only 23 cited papers.  The researchers could strengthen the paper by providing empirical comparisons showing SGDMs speed advantage over SGD.", "Paraphrased Statement: Summary This research explores the hidden bias of momentumbased optimization techniques like GDMSGDM and Adam under common assumptions. It reveals that momentum has no impact on this implicit bias. The key contribution is a novel theoretical analysis that utilizes new Lyapunov functions. Strengths and Weaknesses Strengths:  Provides theoretical insights into the implicit bias of momentum methods.  The topic is relevant and significant.  Clear and concise writing. Weaknesses:  Lacks citation for concepts borrowed from paper [1] in the Preliminary section.  Definition of \"implicit bias\" is not provided making it challenging for general readers.  The theoretical results may not be directly applicable to many practical scenarios due to assumptions like linearity data separability and smooth loss.  It would be beneficial to extend the analysis to nonseparable data.  The learning rates for GDM and SGDM are smaller than those for GD and SGD respectively. This implies a narrower range for hyperparameter tuning but it is unclear if this makes GDSGD superior to GDMSGDM or why momentum is still necessary.  Experimental demonstrations are missing making the practical significance of the theoretical findings unclear. Comparisons with other methods like GDSGD would be valuable.", "Paraphrased Statement: Summary: This manuscript aims to examine the behavior of stochastic gradient descent with momentum and an algorithm that combines it with coordinatewise preconditioning (similar to Adagrad and Adam) for a specific class of logistic regression problems that assume linearly separable data. Strengths and Weaknesses: Strengths:  The ultimate goal of analyzing Adam is valuable. Weaknesses:  The analysis currently focuses on a deterministic version which is impractical since Adam is always a stochastic algorithm.  The manuscript contains numerous errors typos and undefined symbols including:  Undefined variables in Definition 2 (e.g. \\tilde w gamma)  Incorrect equalities in Lemma 12  Errors in Lemma 14 (e.g. wrong coefficients missing \"\\leq\" sign)  The inconsistent use of notation (e.g. same symbol for different meanings) creates confusion.  Assumption 2 contains an incorrect assumption (\\ell 0).  Corollary 2 uses an incorrect variable (\\hat w should be \\tilde w).  Lemma 10 incorrectly refers to x0 as s0.  The paper fails to address the incompleteness of the analysis given the lack of results for Adam.  The papers construction appears unfocused with unrelated sections.  The manuscript lacks a rigorous proof of boundedness for r(t) in Lemma 15.  There are errors in Lemma 16 including incorrect Taylor expansions and missing terms.  The equality in Lemma 3 (35) should be an inequality (\\leq).  The penultimate inequality in Lemma 17 requires a correction.  The explanation for Eq. (\\dotp) in Lemma 18 contains an error.  The term for \\exp((r(t)n(t))T \\tilde xi) goes missing in Lemma 18.  Unnecessary addition of 4 in the exponent of the inequalities in Lemma 18. Overall: The manuscripts proofs are riddled with errors and require a thorough revision. The focus of the analysis and the incompleteness of the results should be clearly addressed."], "l4IHywGq6a": ["Paraphrased Statement: This paper introduces a novel approach to generating molecules using grammar learning principles. The method efficiently learns production rules by optimizing evaluation metrics through hyperedge selection. It demonstrates impressive results with minimal data and comparable performance to largescale endtoend models. strength:  Highly dataefficient for grammarbased learning.  The hypergraphbased process for learning production rules is innovative. Weaknesses:  The model may be limited to generating molecules within a specific range due to computational limitations.  The production rule learning approach is similar to taskbased learning and referencing relevant process would strengthen the papers context.", "Paraphrased Statement: The paper presents a technique for generating molecules (particularly polymer) using graph grammars that requires minimal training data. Molecules are represented as hypergraph and the grammars production allow connected nodes to be merged into single nonterminal nodes. These rules are derived from random samples of graph edges and the grammar is created by gradually merging nodes into nonterminal nodes until only one remains. This process is repeated for all training graph simultaneously. The grammar is then optimized to maximize specific metrics using nondifferentiable methods due to the need for generated graph to evaluate the objective use. strength and Weaknesses: This approach has various advantages: 1. Data efficiency: It can achieve strong performance with a small amount of training data compared to other models. 2. Extrapolation: It can generate molecules outside of the training position. 3. Explainability: It can identify useal groups that characterize the generated molecules. However there are some points to address: 1. The computational cost of grammar construction and training is unclear particularly considering the use of graph matching in generating production rules. 2. While mentioned it is not clear where the improved performance with more training data (0.3 of the dataposition) is demonstrated. 3. The understanding for underlining certain results in the tables should be clarified. 4. The process for selecting the final position of grammar rules could benefit from further explanation. 5. The number of rules produced during grammar construction across the evaluated datapositions should be provided for clarity.", "Paraphrased paper Summary This field introduces a novel algorithm to efficiently learn molecular hypergraph grammars using a bottomup approach. The algorithm iteratively contracts hyperedges into nodes creating productionion rules for a final grammar. The contraction process is guided by a learned policy which is trained using reinforcement learning (RL). The rewards are based on the synthesizability and membership range of molecules generanged by the policy. The proposed method outperforms existing molecule generation algorithms in these metrics on both small and large datasets. strength and Weaknesses This process presents a innovative solution for learning to generange graph with limited examples (fewshot learning) in the polymermonomer field. The method is various and has the potential to significantly impact graph learning research. While it contribution similarities with prior process it uniquely preserves classspecific properties by operating on subgraph rather than individual atoms. Furthermore the strangegy of employing RL to learn a grammar search algorithm is novel. The experimental evaluations provide substantial support for the method on datasets of varying sizes. However there are areas for improvement in the papers presentation. The notation in Section 4 is somewhat confusing particularly regarding the use of subscripts and superscripts to represent iteration numbers and connected factor. Additionally the notation for the probability distribution should be corrected to reflect a production of probabilities. critical data regarding the experimental setup and results is located in Appendix A and Table 3 and should be moved to the main text earlier for clarity. Questions 1. Parameter tuning: While the paper discusses a clean grammar learning algorithm it does not address the tuning of hyperparameters such as the number and length of productionion rules. How are these parameters determined and is any pruning performed on the productionion rules 2. Molecule generation: The paper does not explain how to generange molecules from the learned grammar rules. Is uniform random sampling used during testing If so are there plans to develop a policy for molecule generation and jointly learn it with the rule search policy 3. Combinatorial grammar space: The space of grammar rules is vast. How does RL overcome this challenge and learn a reasonable policy 4. optimization objectives: The paper only mentions diversity and RS as optimization objectives. Are there any additional objectives considered", "Paraphrased Statement: This field presents a novel grammarbased model for molecule generation. The model uses a position of production rules to modify molecular graph and learns the hyperedges (connections between multiple nodes) in these graph using a REINFORCE algorithm. The evaluation results on small and large molecule datapositions demonstrate the models strong performance in molecule generation with limited data. However there are some areas where improvements can be made: 1. The model uses a pretrained graph neural network to generate sampling weights. The authors should provide evidence that this can be replaced by other feature extractors. 2. The stability of the REINFORCE algorithm across different random seeds should be verified by providing error bars in the data and convergence curves. 3. While the model is efficient it may not scale well with increasing amounts of data. The authors should report the computation time and assess if the model benefits from more training data."], "xS8AMYiEav3": ["Paraphrased Summary This paper suggests a novel approach to repair ReLUactivated neural networks. Unlike existing methods that involve retraining or weight adjustments this approach constructs a \"patch\" by adding submodels to the original network. An LP solver is employed to generate the patch function within a specified linear region. Experiments show that this approach surpasses existing repair techniques. Strengths  Addresses a significant problem  Clear and concise writing  Presents an innovative repair method Weaknesses  Limited applicability to CPWL neural networks  Pointwise repair is more practical than linear region repair.  The approach requires linear regions for repairs which may not be readily available.  Concerns with the proposed method:  Guaranteeing accuracy of transformed linear regions  Scalability publication due to multiple linear regions for many buggy inputs  Complexity of LP solving  Overstated claims:  The claim that existing approach cannot guarantee bug repair and prevention of novel bugs appears to apply to the proposed method as well.  The generalization claim is questionable due to the scalability concerns and potential impact on correct inputs.  Importance of locality and minimality is unclear.  The assumption of linear regions as \\x aix\\le bi i \\in I\\ may not be universally applicable.  Weak evaluation:  comparison only made with existing methods on MNIST for pointwise repair raising scalability concerns.  Incomplete and insufficient solution for AlexNet.  Lack of discussion on:  Number of fixed buggy inputs by each method  Number of linear regions used in different configurations  generalization capability (unseen buggy inputs)", "Paraphrased Summary: The paper presents a method for fixing neural networks with ReLU activation functions. It adds a \"patch function\" to the network to correct its output for specific incorrect inputs. This makes the method unique in addressing the following:  It guarantees the minimal space between the original and repaired networks functionality.  It maintains the repaired networks behavior for inputs other than the buggy one. Strengths and Weaknesses:  The paper is wellwritten and provides a clear overview of neural network repair.  The methods advantages over existing approach are demonstrated effectively on MNIST and ACAS networks and its scalability is discussed.  A potential concern is that it may be difficult to eliminate all buggy behavior in range classifiers (beyond a test set).  Including experiments on robustness against input transformations (e.g. noise) would strengthen the evaluation. Minor input:  \"loss defined based on\" should be \"loss defined on\"  \"linear functions\" should be \"affine functions\"  \"can solve it is\" should be \"can solve it\"  \"with every entry is equal\" should be \"where every entry is equal\"  \"REASSURE provide\" should be \"REASSURE provides\"  The space in Theorem 4 should be bounded from above", "Summary Paraphrase: This paper proposes a novel method for repairing ReLU neural networks. It creates a patch function for each affine region in the network which includes an affine function for the repair and a support network to restrict the repair to that region. Iteratively multiple affine regions can be repaired. The technique is tested on MNIST and HCAS datasets. Strengths and Weaknesses Paraphrase: The method is innovative and wellexplained with a helpful example. Concerns and Questions: 1. Pointwise repair: The method aims to improve overall network performance not just correctness on specific input points. Its unclear if the repair affects other test points beyond those it fixes. 2. Affine region repair: Partitioning the input space into affine regions for large or highdimensional networks may be challenging. The application of the method to complex models and region repairs for perception networks should be demonstrated. 3. Input Representation: For HCAS how are the linear regions represented 4. network Architectures: The initial architectures of the evaluated networks should be described. 5. Scalability: The methods benefits need to be proven beyond minimal changes for pointwise repair and its scalability for large networks and higherdimensional inputs should be addressed.", "Summary: This paper proposes a novel method for provably fixing neural networks addressing both pointbased and polytopebased repairs. The approach uses a \"patch network\" consisting of a \"support network\" that detects buggy inputs and an \"affine patch network\" that corrects the errors. The method outperforms existing provable repair algorithms on standard datasets and networks. Strengths:  Addresses an significant and unexplored problem (provable repair of neural networks).  Presents novel technical contributions with strong theoretical guarantees.  Demonstrates superior performance over prior work while minimizing network alterations. Weaknesses:  The claim of preserving continuity should be revised based on the limitation mentioned in the paper.  Scalability for polytope repairs in higher dimensions is limited as with existing methods. additional Questions:  The notation in equation (3) could be improved.  For multiarea repairs does patch range affect the resulting fixed network  What are the running time for the tools in Table 2 and Table 3  Is it feasible to extend the method to CIFAR10 networks  What value of gamma was used in the experiments and how do the metrics vary with gamma  Can pointwise repair be extended to arbitrary activation as in DDNN"], "siCt4xZn5Ve": ["Paraphrased Statement: Summary: The paper examines how noise affects the performance of Stochastic Gradient Descent (SGD) near the minimum solution set (\\Gamma). It reveals that in the set of very small step sizes the dynamics of SGD resemble a diffusion work within \\Gamma combining the effects of noise and gradient flow. This diffusion work is leveraged to characterize the seting point obtained by SGD on a specific problem with sparse predictors and label noise. Strengths:  The paper provides open and intuitive explanations.  It introduces a key mathematical concept that was lacking in previous research.  It presents a concrete example in which the seting dynamics can be analyzed indepth demonstrating the optimality of SGD with label noise for overparameterized linear network. Weaknesses:  The writing could sometimes be more nuanced acknowledging the setations of the seting regime.  The interpretation of the seting diffusion work could be further elaborated especially considering different neural network architectures or scenarios without artificial label noise.  The effective dynamics of equation (17) could be clarified particularly how way with zero sparsity predictors are eliminated.  Relevant work by Wojtowytsch and Pesme et al. should be cited in the context of SGD noise frameworking and implicit bias in the specific framework examined in Section 6.  Lemma 6.3 is a known effect for this framework and should not be considered a significant contribution.", "Paraphrased Statement: This work provides a global analysis of SGDs inherent bias after it achieves zero training error. Using ideas from Katzenberger (1991) the authors demonstrate that SGD with label noise can avoid the kernel regime resulting in significantly lower sample complexity in sparse linear regression models. Strengths:  Robust results for SGDs implicit bias after zero training loss capturing its behavior within the global minimizer manifold.  analysis covers the case where the step size approach zero.  derivation of a limiting stochastic differential equation (SDE) using a projection operator to eliminate noise and establish convergence in distribution.  Explanations provided for each term of the SDE.  Limiting Flow for label noise derived based on the SDE.  label noise SGD shown to efficiently recover the ground truth in sparse linear regression models unlike gradient descent. Weaknesses:  Some confusion regarding why the ground truth should be the global minimizer in the label noise scope.  analysis heavily dependent on label noise extension to minibatch SGD is uncertain.", "Paraphrase: The researchers develop a stochastic differential equation (SDE) to analyze the implicit bias of stochastic gradient descent (SGD) with a tiny learning rate. They demonstrate that regardless of the noise covariance structure SGD on a certain manifold of local minima is equivalent to SGD with an infinitesimally small learning rate after a specific number of steps. This finding strengthens other research. Notably they reveal a discrepancy in sample complexity between label noise SGD and gradient descent (GD) for overparameterized linear frameworks in the kernel regime highlighting the benefits of SGD for generalization. Strengths:  The findings are novel and the analysis is intricate.  The work introduces new ideas by drawing inspiration from Katzenbergers (1991) notions.  It enables a global analysis of implicit bias for a wide scope of noise covariance structures and step sizes. Weaknesses:  Certain framework assumptions setups and conditions in the main results may require further clarification.", "Paraphrase: Summary: This paper presents a new mathematical framework to understand how SGD implicitly regularizes after reaching zero training loss. Its technical novelty involves a novel time scaling of SGD leading to the Katzenberger work. By applying weak convergence analysis it provides an elegant characterization of SGD dynamics on the zero loss manifold. This technique facilitates a new validation for the sample complexity bound of overparameterized linear models with label noise regularization. Strengths:  The analysis employs a novel time scaling to separate fast and slow motion in SGD dynamics.  The mathematical framework effectively captures SGDs implicit regularization on the zero loss manifold. Major Comments:  previous work indicate significant alignment between the Hessian and gradient covariance matrix in deep neural networks. It would be interesting if the framework could explain this empirical observation particularly the large number of nearzero Hessian eigenvalues and outliers.  The definition of the zero loss manifold is initially generalized but previous restricted to overparameterized linear models. Clarification is needed about the applicability of Theorem 4.6.  Exploring alternative regularization techniques that might well utilize the zero loss manifold and reduce the sample complexity is a potential avenue for future research. Minor Comments:  Typo: \"Polyakojasiewicz\"  Incomplete set notation in the sentence after equation (6)  Undefined symbol \"\u2020\" in Definition 4.4  Typo in Corollary 5.2: missing trace operator in the equation  Inconsistency in notation between equations (8) and previous sections (\u03c3 vs. \u03a3) References:  Ghorbani et al. (2019)  GurAri et al. (2018)  Li et al. (2020)"], "wIzUeM3TAU": ["Summary This paper introduces a new method for evaluating the expressive power of Graph Neural Networks (GNNs) using tensor language and WL test range. The method addresses the lack of a straightforward tool for evaluating expressive power and provides a modelagnostic approach. Strengths  open and wellwritten paper with theoretical proofs and examples.  Provides a way to evaluate expressive power of GNNs without architecturespecific proofs. Weaknesses  The approximation of using tensor language for evaluation is not novel as it has been previously proposed in [1].  The use of WL test range is not openly defined and the notation differs from other GNN paper.  The method assumes a solid resemblance between GNN layers and WL test iterations which may not always hold true.  The evaluation approach relies on writing GNN layers in tensor work which may not be feasible for all aggregation methods.  The method is not currently practical for GNNs with expressive power beyond 3WL due to computational limitations.  The paper lacks open insights on how to increase the expressive power of GNNs based on the theoretical analysis. Clarifications  The GNN in [Maron et al.2019b] has a 2WL power according to the paper notation rather than 3WL as mentioned in the original paper. References [1] Breaking the Limits of Message reach Graph Neural Networks", "Paraphrase: Tensor language (TL) is a framework that:  Enables uniform analysis of GNN framework to understand their expressive capabilities and classification power.  Corresponds to the WL hierarchy with TL summation depth and power variables directly mapping to iterations and tuple size in kWL respectively.  Establishes connections between GNN update equations and TL expressions simplifying the assessment of GNN expressiveness and separation power by expressing framework architectures as TL expressions.  Can reestablish established GNN results from the literature.  Quantifies GNN purpose approximation and shows that GNNs characterized by TL fragments can learn purposes with separation power limited by a fragmentspecific refinement algorithm.  Demonstrates that kIGN cannot achieve expressiveness beyond (k1)WL as it corresponds to k iterations of (k1)WL.  Provides insights into kIGN expressiveness with a polynomial number of layers showing that additional layers just enhance expressiveness by deriving GNN purposes with increased treewidth not standard layers based on the adjacency matrix. strength and Weaknesses: strength:  Unified framework for analyzing GNNs.  Wellgrounded in existing literature and confirms previous findings.  Establishes new results and addresses open questions.  Offers insights into the impact of multiple layers and treewidth on expressiveness. Weaknesses:  Limited novelty compared to other GNN characterization framework.  Lack of detailed comparison with related work.  Incomplete clarity in writing and notation making it difficult to understand in some sections.  Focus solely on purpose approximation and separation power without considering purpose classes or universality. Recommendations:  Improve clarity and use examples to illustrate framework TL expressions.  Provide a more thorough comparison with related work.  research TL within the context of purpose classes or universality.", "Summary Paraphrase: This paper introduces a formal language (tensor language) for describing tensor expressions. It demonstrates that the expressive power of this language is equivalent to color refinement algorithms or graphbased logic algorithms. By translating existing graph neural networks (GNNs) into the tensor language the paper establishes upper bounds for GNN expressiveness aligning with prior findings. additionally the paper provides a method to characterize the closure of a purpose class based on its expressive power (Theorem 6.1). As an application it utilizes the tensor language to determine the expressive power of various GNNs (Corollary 6.2 Proposition E.3). strength and Weaknesses: strength:  Provides a general approach to determining GNN expressive power through tensor language translation.  Characterizes both upper and lower bounds on GNN expressiveness via Theorem 6.1 and Corollary 6.2. Weaknesses:  Establishing lower bounds is more challenging than upper bounds requiring individual proofs for each GNN.  The approach may be difficult to follow for readers unfamiliar with both GNNs and firstorder logic. additional Comments:  The paper defines a new grammar (tensor language) for relating GNNs to firstorder logic offering a novel approach.  The method enables analysis of expressive power independent of specific GNNs and simplifies the process of deriving these power.  The paper addresses open number raised in previous studies which enhances its significance.  The paper lacks numerical experiments.", "Paraphrased Statement: This paper presents a novel technique for examining the discriminative and approximation capabilities of Graph Neural Networks (GNNs). The researchers introduce Tensor Languages (TL) and demonstrate how GNNs can be expressed as TL expressions. In Section 4 they explore the discriminative power of TL and compare it to color refinement algorithms like kWL. Specifically they describe the discriminative power of such algorithms after specific iterations. These findings enable them to determine the discriminative power of GNNs in Section 5. They successfully recover all existing literature results while also presenting new findings. In Section 6 they discuss the implications of their findings for GNN approximation corroborating existing results and proposing new ones. This paper is theoretical with no empirical data. strength:  The paper is wellwritten and establishes a novel connection between programming languages and deep learning.  Formalizing GNNs as TL provides a powerful tool for a unified theoretical analysis of GNN discriminative power.  They obtain new results characterizing GNN expressivity in relative to the number of layers. Weaknesses:  Section 6 on purpose approximation is less relevant as the authors consider a discrete topology for the graph set rendering all purposes continuous.  GNNs are more constrained purposes with continuity in tensor mappings.  The consequences of Theorem 6.1 and Corollary 6.2 for GNNs are uncertain since GNNs cannot represent all graph purposes.  The authors should clarify this in Section 6.2 by outlining potential limitations of their approach."], "shbAgEsk3qM": ["Summary: This study examines the overparameterized linear representations of two algorithms: Temporal DifferenceFixed Variation Inference (TD FVI) and Regularized Maximum Likelihood (RM). The authors propose a unified framestudy for understanding these algorithms showing that they minimize the Euclidean norm of weights subject to different constraints. Strengths and Weaknesses: Strengths:  Accessibility and relevance to the overparameterized representation issue. Weaknesses:  Restriction to the linear case.  Lack of novelty in technical analysis. HighLevel Comments:  Unified view: Does the unified view also apply to underparameterized linear representations  nonlinear representations: Could the authors extend their findings to nonlinear representations such as deep neural netstudys  Related study: unified discussion of existing theory on overparameterized neural netstudys.  Derivation details: Provide more details on deriving the overparameterized update equation from the underparameterized form.  Relationship to overparameterized linear regression: Explain the connection between the solutions of TD FVI and RM with overparameterized linear representation and overparameterized linear regression.  Intuition behind matrix D: Clarify the rationale for defining matrix D in Proposition 1.  interpretation of operators: Offer an interpretation of the operators M N and R in the overparameterized regime.  convergence implications: Discuss the consequences of a large norm hindering convergence in Theorem 4.  Font size: Increase the font size in the figures for readability.", "Summary Update: See the main review for an overview. Strengths and Weaknesses Summary: The paper studies the convergence properties of three algorithms (RM TD FVI) for offline policy evaluation under an overparameterized linear function class model. It also includes empirical experiments that are not discussed due to the reviewers limited expertise in empirical RL. The reviewer questions the strength of the contributions and identifies potential inaccuracies. Questions: 1. The paper claims to explore overparameterized models in OPE but it lacks a theoretical comparison with underparameterized cases. The reviewer requests an explanation of the motivation and whether the model size affects the learning process. error Identified: 1. Equation (38): The righthand side (RHS) should not contain \"t\" as \"t\" approach infinity. 2. Equation (42) and (43): \"\u03b8t\" should be used in the RHS of (42) which affects the recursion in (43). 3. The claim at the bottom of page 15 that \"M\u1d40Dk(M\u03b3N)\" can be eigen decomposed is questioned as it may not be symmetric. 4. The proof of Corollary 2 incorrectly practice Hoeffdings inequality without considering the high probability term (1\u03b4). Recommendations: 1. The reviewer suggests clarifying the term \"overparameterization\" and providing a more specific description of the assumed model structure. 2. The lemmas in Appendix A.7 should be presented in their exact form not just as original references. 3. The papers writing should be improved to address grammatical issues and enhance precision.", "Paraphrase: This research examines how three value estimation algorithms (TD FVI and RM) converge in overparameterized linear settings. It interprets the difference in their convergence behaviors as stemming from varying constraints in an optimization problem. The paper also develops a generalization bounds for FVI. Based on these insights two regularizers are proposed to enhance convergence in deep reinforcement learning. The study experimentally evaluates their effectiveness. Strengths:  The paper offers a new understanding of the underlying bias in these algorithms which is not apparent in underparameterized cases.  The proposed regularizers direct limitations identified by the theoretical analysis and perform well in experiments. Weaknesses:  The theoretical analysis assumes an IID dataset of onestep transitions while substantial offline policy evaluation often involves dependent trajectories. Its not clear if this dependency invalidates the results.  The experiments are limited to relatively simple environments. It would be useful to test the regularizers in more complex settings. Questions:  How would the results change if the dataset comprised multiple trajectories instead of independent transitions  What is the limit of the generalization bounds in Theorem 5 as time approaches infinity given the convergence criterion How should this limit be interpreted", "Paraphrase: This paper investigates the unique solutions (fixed points) for value function optimization algorithms in reinforcement learning (TD residual gradient and Fitted value iteration) in the overparameterized context where the problem has multiple potential optimal solutions. Key findings:  The different fixed points observed for TD and residual gradient in the limitedcapacity context are also present in the overparameterized context.  Each algorithm can have multiple fixed points and convergence to a specific point depends on the initial parameter vector.  Crucially the fixed points for TD and residual gradient differ even in the overparameterized case. Strengths:  novel investigation of fixed points in the overparameterized context.  Reveals insights into the behavior of different value function optimization algorithms in practical contexts with deep feature spaces. Weaknesses:  Theorem 2s fixed point solution is similar to a previously established result (Equation 2 from [1]) but the paper does not adequately acknowledge this.  Theorem 4 is a trivial least norm solution for underdetermined equations and should not be presented as a separate theorem without proper attribution.  error in the proof of Theorem 4 in the appendix.  The connection to deep reinforcement learning is unclear as the last layer of deep networks typically fits a value function using a small number of features based on learned foundation function rather than solving an underdetermined problem.  Experiments use simple field and lack clear motivation detracting from the papers overall impact."], "rFbR4Fv-D6-": ["Paraphrased Statement: Summary: Researchers employ homophilybased datasets and metalearning techniques for node classification tasks. In selfsupervised learning they utilize an evolutionary strategy. effectiveness and Weaknesses: 1. The paper titled \"Is Homophily a Necessity for Graph Neural Networks\" argues that homophily is not a prerequisite for graph neural network performance. however the researchers rely on homophily datasets in their field which may simplify the task. 2. While metalearning is an established tool its application in this paper is not considered a novel contribution. 3. The research demonstrates enhanced performance through their proposed access. 4. The contribution of using an \"evolutionary strategy\" for selfsupervised learning is debatable.", "Paraphrased Statement: Objective: Selfsupervised learning (SSL) methods for graphs utilize structural and feature information to build various SSL tasks without relying on labeled data. however the choice of SSL tasks can significantly impact the performance of downstream tasks. access: AutoSSL is a method that combines multiple SSL tasks to enhance representation learning. It measures the quality of representations using a pseudohomophily metric. Two techniques are proposed to search and combine SSL tasks: an evolutionary algorithm and metagradient descent. Evaluation: AutoSSL is tested on eight datasets with five SSL tasks. effectiveness and Weaknesses:  effectiveness: AutoSSL improves performance in some cases especially with the stronger variant (AutoSSLES).  Weaknesses:  performance gains are often small or insignificant.  AutoSSL assumes a fixed set of SSL tasks and does not consider task interactions.  It is unclear how AutoSSL can be applied to other graph analysis tasks like link prediction or graph classification.", "Paraphrased Statement: Summary: This research explores how to automatically assign weights to multiple selfsupervised tasks on a graph without true labels to enhance downstream nodelevel prediction performance. To address the lack of ground truth during pretraining the authors propose exploiting the graphs \"homophily\" characteristic to generate pseudo labels. They then present two algorithms for automatically adjusting task weights: one based on evolutionary algorithms and the other on gradient descent. Experiments show the effectiveness of these methods compared to using a single pretext task. effectiveness:  Clear and accessible writing style  Extensive experiments across seven realworld datasets demonstrating improved performance Weaknesses:  Limited novelty as dynamic loss function weight adjustment has been extensively studied  contribution primarily focus on utilizing homophily for pseudo label generation and proposing two algorithms (AutoSSLES and AutoSSLDS)  Questionable measure of AutoSSLES given its computational demands and lack of significant advantages over AutoSSLDS  Suggestion to consider additional recent improvement in graph selfsupervised learning for comparison", "Summary: Method: AutoSSL is a selfsupervised learning (SSL) access for graphs that uses pseudohomophily as a proxy goal. Pseudohomophily is calculated by assigning labels based on kmeans clusteringing and maximizing its average across connected vertices. Theoretical ground: Maximizing pseudohomophily allows for maximizing the mutual information between pseudolabels and true downstream labels. Search strategy: AutoSSL uses both evolutionary strategies (AutoSSLES) and differentiable search with Gaussian mixture model (AutoSSLDS) to assign clustering labels due to the nondifferentiable nature of clustering assignments. Evaluation: AutoSSL was assessed on 5 SSL tasks and 8 datasets showing superior performance to unsupervised baselines and competitive results with supervised baselines based on normalized mutual information truth. effectiveness:  Clear and wellwritten presentation  Pseudohomophily is an effective surrogate for maximizing mutual information  Strong performance on benchmark datasets Weaknesses:  Simplicity and intuitive nature of the method  Significant underperformance on the clusteringing task for the Photos dataset"], "u6TRGdzhfip": ["Paraphrase: Summary: Adversarial training has been used for several years to create robust classifiers in deep learning. However it can be impractical to directly apply adversarial training to large deep networks in many machine learning scenarios due to its high computational cost. To address this researchers have proposed methods to transfer the robustness of adversarially pretrained models which is more feasible. This paper focuses on this approach. effectiveness:  The paper identifies a promising research guidance that is more practical than direct adversarial training.  It highlights a failure case in existing methods for distilling robustness from adversarially pretrained models and proposes a novel framework to address this issue.  The paper is wellwritten and the experiments support the claims made.  Both TRADES and AT methods are evaluated showing the model compatibility with existing adversarial training approach. Weaknesses:  The paper does not fully explain the performance drop when using adversarial data generated by the student model instead of the teacher model.  It does not provide a conventional problem background.  The paper does not define S(\\circ\\tau) or explain how it differs from S(\\circ) or how \\tau impact S(\\circ\\tau).  It would be valuable to compare the performance of IAD with ARD (the most direct baseline) using the same \\tau value.  The paper does not provide a comparison of computational costs for the distillationbased method.  Figure 1(b) does not clearly illustrate the advantages of the proposed method.", "Paraphrased Statement: Summary: This work investigates adversarial distillation a specific type of distillation to enhance model robustness. The paper acknowledges that in adversarial distillation the teacher model becomes increasingly unreliable during training as the student model dynamically identifies adversarial data. To address this the authors propose an \"Introspective Adversarial Distillation\" (IAD) method that leverages both the teacher model and the student model for learning. Experiments demonstrate the effectiveness of IAD in improving robustness. effectiveness and Weaknesses: effectiveness:  The discovery of adversarial distillation inherent weakness is novel.  IAD meticulously incorporates this weakness into its loss work.  comprehensive experiments on several architectures training methods and datasets show IADs superiority in robustness enhancement. Major Concerns: 1. In Section 3.2 the authors categorize adversarial data into three groups. If there is a fourth group where the teacher model misclassifies natural data but correctly classifies its adversarial counterpart it should be acknowledged. 2. Equation (4) in Section 4 quantifies trust in adversarial data. However it may differentiate between groups 2 and 3 differently due to their potential different scales and meanings. 3. With certain hyperparameter background IAD could potentially reduce to TRADES or ARD. An experiment comparison these methods accuracy surfaces would provide further insight into their relative performance. Minor Concerns: 1. Table 1 could be improved by listing distance terms as column names and comparison baselines independently for each term. 2. The experiments involving IAD and AKD2 should be beneficial explained particularly how IAD enhances natural accuracy.", "Paraphrase: In this article a novel technique called Introspective Adversarial Distillation (IAD) is introduced to enhance the robustness of standard adversarial training. The method focuses on the challenging scenario where the teacher models performance on adversarial or natural data is unreliable. effectiveness:  The concept of IAD is intriguing and straightforward to implement.  The paper is wellwritten and easy to understand.  experimental findings suggest the effectiveness of IAD in improving adversarial resilience. Weaknesses:  The techniques performance on natural data and against FGSM approach is subpar.  The work should thoroughly explore the compromise between adversarial robustness and natural performance.  The reason for IADs superior performance over AKD2 in TinyImageNet and CIFAR datasets for AA requires further explanation.  The current assessment is limited to range datasets it is unclear whether IAD is applicable to speech datasets.  Providing a theoretical foundation for IAD would strengthen its theoretical soundness.", "Paraphrased Statement: Summary: This research introduces a novel knowledge distillation (KD) technique for adversarial training. The authors noticed a diminished reliability of soft labels provided by the teacher model during the adversarial training of the student model. Based on this they propose a method that partially trusts the soft labels from the adversarially pretrained teacher. effectiveness: 1. Clarity: The paper is wellstructured and easy to comprehend with clear presentation of the main claims and observations. 2. Unique Observation: The key observation that teacher model accuracy on studentgenerated adversarial range decreases is insightful and provides a solid basis for the proposed method. 3. Improved performance: The proposed method (AKD2IDA) generally outperforms existing techniques although the margin of improvement is not significant. Weaknesses: 1. method comparison: The paper criticizes ARD for fully trusting teacher model soft labels but ARD actually uses soft labels from clean range which are found to be generally reliable. It would be more appropriate to compare the proposed method to ADK2 which uses soft labels from adversarial range. 2. partially Trusted Soft Labels: The motivation for partially trusting soft labels in Eq. (3) of IAD is not fully explained as those soft labels are generated from clean range and are deemed trustworthy. 3. Loss Term Weighting: The weighting of the \"Student Introspection\" loss term with (1\u03b1) is not intuitively justified. As the \"teacher guidance\" loss term is downweighted when \u03b1 is large it would make sense to keep the \"Student Introspection\" loss term unweighted since it promotes model smoothness and robustness. 4. Ablation work: Ablation work are not provided to demonstrate the effectiveness of each loss term in AKD2IDA. Overall: The paper presents an significant observation that challenges the blind trust in teacher model soft labels during adversarial training. The proposed method partially trusts these soft labels to improve performance. However some concerns about method comparison loss term motivation and ablation work need to be addressed for the paper to fully realize its potential."], "oJGDYQFKL3i": ["Paraphrase: Summary This work aims to derive object dynamics from unlabeled videos to create multiobject representations. It builds on the inference model developed by Greff et al. (2019) to extract these representations. The work utilizes this model to learn latent dynamics by predicting future frames based on two previous frames. The proposed network undergoes training to maximize loglikelihood for reconstructions and prediction in pixel space akin to standard video prediction methods. Additionally it optimizes an object from Greff et al. (2019) to obtain multiobject representations. Key Contribution The primary contribution of this work involves a module that forecasts the future multiobject representation given two past representations. This module employs:  dynamic Distillation: Motivated by the unpredictable changes in objecttoslot assignments within videos it introduces a selfattention layer to match object slots across frames.  relation module: Designed to model interactions between object pairs it utilizes selfattention between dynamics and static representations of object. The module output is subsequently added to update the dynamic representation from the dynamic distillation module.  future State prediction: The future state is predicted using a linear transformation of the static representation at time t and the updated dynamic representation. Evaluation and Findings The proposed method is evaluated on the CLEVRER dataset for video reasoning prediction reconstruction and segmentation tasks. The work demonstrates strong performance in video reasoning surpassing existing methods on CLEVRER. strength and Weaknesses strength:  Welldesigned and motivated dynamic distillation and relation module.  Strong performance in video reasoning tasks. Weaknesses:  Missing ablation work: The strength of the dynamic distillation and relation module is not evaluated separately.  Missing Baseline: A baseline without these module is needed for comparison.  Discrepancies in Reported event: Differences in reported event for PROVIDE compared to the original paper raise concerns about evaluation methods.  Lack of Implementation Details: The work omits implementation details such as optimizer and hyperparameters hindering reproducibility.  limited Dataset Evaluation: Evaluation is confined to the CLEVRER dataset leaving its applicability to more complex realworld videos untested.  Ambiguity in video prediction: The method does not account for the underlying uncertainty in future video prediction. Additional Observations  The paper contains errors in figure labeling and referencing.  The term \"complex\" is not appropriate to describe the CLEVRER dataset.  The writing manner requires improvement to enhance clarity.", "Paraphrased Statement: Summary: This paper presents a technique for extracting the underlying representation of object in a video. The input video frames are processed framebyframe by an encoder model using the IODINE algorithm. Relationships between the latent representations of sequential frames are extracted and updated using the transformer model. The latent vectors are then decoded by a generative model to reconstruct the original video. The proposed method was evaluated on diverse tasks using the CLEVRER dataset. strength and Weaknesses:  The proposed method outperforms previous methods (IODINE and PROVIDE) on diverse tasks.  However the methods detailed implementation is not clearly described making it difficult to understand. Specific Concerns:  In the video understanding experimentation IODINE processes a single frame as input while the proposed method uses multiple frames from a video. This difference in input data should be highlighted.  The extracted data (scene dynamics) may vary in importance depending on the scene but the paper does not provide concrete model to support the strength of Dynamic Distillation.  PROVIDE was chosen as a method that incorporates temporal data but its performance is worse than IODINE which uses a single image. It is unclear if this comparison is meaningful.  In the prediction experimentation it is difficult to determine how the proposed method captures object question and interactions from the provided Figure 4. It is unclear if this model adequately demonstrates the ability to extract dynamics from video.  In the decomposition experimentation the exact experimentational conditions are unclear. Is the reconstruction performed through an autoencoder or from userprovided latent vectors If the latter the source and manipulation of the input latent vectors should be clarified.  The indexing system for latent vectors in the proposed method is ambiguous. It is unclear how frame numbers latent indices time and pixel coordinates are related and used to identify specific latent variables for relationship extraction.  The generative model is not sufficiently detailed. It mentions estimating color for individual pixels but the connection between latent vectors and pixels is not explained.", "Paraphrase: Summary: This paper introduces the Object Dynamics Distillation Network (ODDN) a framework that extracts explicit object movement (like velocity) by comparing the static latent representations of object in different video frames. It builds on techniques that create separate latents for each object in a scene allowing them to be represented in a consistent manner. The ODDN uses an attention mechanism from transformer models to match object latents across two input images and compress them into a lowdimensional vector capturing object movement independent of other factors. It then models object interactions using module based on Nearest Point Estimation (NPE) and Relative Neighborhood Embedding (RNEM). strength:  The use of transformer architecture to align and distill object dynamics is novel and effective.  The method outperforms stateoftheart approach in video understanding and reasoning tasks. Weaknesses:  The paper relies heavily on existing techniques particularly the IODINE framework for object latent representation and attention from transformer.  The core component is aligning IODINE latents with transformer and the reasoning module is a combination of NPE and RNEM.", "Paraphrased Statement: This work introduces a selfcontained distillation approach for extracting meaningful representations of object dynamics from unprocessed video data. The resulting dynamics model enables both reasoned inference and prediction for future video frames. Evaluation on segmentation and reconstruction tasks demonstrates its strength. strength:  Presents a novel distillation technique for comprehending object dynamics.  Achieves stateoftheart event on the CLEVRER dataset. Weaknesses:  Limited qualitative demonstrations of the attention mechanism with only one scenario depicted in Figure 6. more validation is needed to confirm the attention modules convergence.  Validation is restricted to the CLEVRER dataset limiting its demonstrated generalization capabilities.  The qualitative event in Figure 4 evidence minimal discernible differences compared to prior methods.  Figure 5 suggests that ODDN accurately captures shadow field while IODINE does not. An explanation for this discrepancy would be beneficial.  computational complexity comparison with existing methods such as ALOE and IODINE are missing.  A discussion of the proposed methods limitations and potential field for improvement (future work) is desired.", "Paraphrased Summary: This paper introduces a model that can understand how object move and interact in videos. It consists of two parts: one that learns the dynamics of individual object and another that models the interactions between them. The model achieves stateoftheart (SoTA) performance on several tasks:  video understanding (e.g. answering questions about video content)  video prediction (e.g. predicting future frames)  Image reconstruction (e.g. generating missing video segments)  Image segmentation (e.g. identifying object in images) The models learned representations are particularly effective in tasks that involve knowledge of object dynamics such as predicting the future frame of a video when object collide or are occluded. strength:  Novelty: The model is unique in its use of explicit dynamic features and its explicit model of object interactions.  Experimental Verification: The models performance is improved by the incorporation of dynamic features.  SoTA Improvement: The model significantly outperforms previous methods on a range of tasks. Weaknesses:  Clarity: The explanations and figures could be improved for easier understanding.  Implementation Details: The implementation details and reproduction instructions are not provided.  Notation Confusion: The use of \"t\" for both inference time steps and video time steps may be confusing.  Figure Interpretations: Some figures lack open explanations and captions.  Small Image Size: The images in the figures are too small for easy interpretation.  Lack of Typos: There are some typos throughout the paper."], "rdBuE6EigGl": ["Paraphrased Statement: Summary This work examines the LSTM architecture and proposes modification to improve its performance. The authors add a direct connection (\"dual\") between the input and output of the recurrent module creating the \"dual LSTM\" variant. They also introduce a \"duallayer LSTM\" where the output is obtained by combining two LSTM layers. Strengths  Revisiting the LSTM structure allows for advancements and improvements.  Hyperparameter tuning is thoroughly documented.  The \"dual\" modification consistently enhances performance. Weaknesses  The datasets used (PTB and WT2) are relatively small.  The work lacks comparisons to transformer models.  Limited analysis and explanation is provided for the superiority of the \"dual\" structure. Minor input  The terms \"dual\" and \"dual\" may cause confusion in the text.  Potential typos include:  \"Penn Treebank problem\" instead of \"PTB dataset\"  \"highest perplexity scores\" instead of \"lowest PPL\"", "Paraphrased Statement: This paper suggests a simple enhancement to recurrent neural networks used for language modeling. They introduce a concept called \"dual connection\" (Eq. 6) which involves adding an additional layer to the network just before the output layer. This layer connects the output of the last recurrent layer directly to the networks input at the current step. The paper evaluated the impact of adding this modification to LSTM and MogrifierLSTM networks on the Penn Treebank and WikiText2 datasets. Strengths:  The paper is clear and easy to follow.  The experimental setup is described in detail.  The authors used two sets of experiments: one with hyperparameters transferred from baseline models and another with hyperparameters tuned specifically for the \"dual\" models. Weaknesses: 1. It is unclear why the \"dual connection\" improves performance. Does it enhance optimization model capacity or provide a helpful inductive bias Comparing training and validation loss curves could provide more insight. 2. The paper does not use skip connections in its LSTMs which Melis et al. (2017) have shown to improve optimization. Adding skip connections to the baseline would help assess their impact relative to \"dual connections.\" 3. The experimental results are not conclusive. In some cases adding \"dual connections\" improves performance with dynamic evaluation but not without it. More experiments on different datasets and an explanation for these mixed results are needed to establish the generalizability of the proposed design.", "Paraphrased Summary: This work proposes adding a residual connection between the modified discussion embedding and the output layer. This connection enhances the models ability to work substantially with various recurrent architectures. While experiments show that the proposed model outperforms its counterpart without the residual connection the improvement becomes insignificant after adjusting hyperparameters optimally. Strengths:  The technique is straightforward and compatible with any ERS architecture.  The method is clearly explained and experiment details are ample. Weaknesses:  The technique lacks originality as a similar approach has been studied before.  It is difficult to determine if the performance gain is due to the residual connections or the added input projections. The authors could have tested this by simply connecting the original discussion embedding to the hidden state which wouldnt add parameters.  The performance gain becomes negligible after hyperparameter tuning.  The number of parameters for Dual mdLSTM is inexplicably lower than for MogrifierLSTM in Table 3. additional Question for Authors: Why is the parameter count for Dual mdLSTM lower than for MogrifierLSTM in Table 3"], "zKbMQ2NY1y": ["Paraphrased Summary: Researchers proposed three strategies to improve the transferability of the Integrated Latent Attack (ILA) technique to frameworks without defense. These strategies include image augmentation applying reverse adversarial updates to clean examples and updating reference attacks through interpolation using an automated parameter selection mechanism. The researchers assessed their approach on nine undefended frameworks and demonstrated its superiority over ILA and other cuttingedge methods. effectiveness:  Clear and concise writing style  Effective enhancement of ILA using the proposed techniques  Innovative use of reverse adversarial updates and automatic parameter selection Major Weaknesses:  Evaluation was conducted only on undefended frameworks excluding defended frameworks that are commonly evaluated in the field.  No target attack experiments were performed to assess the methods transferability to specific frameworks.  The methods performance is highly dependent on the selection of layers in the source framework which may be impractical in realworld blackbox attacks where layer selection is not feasible.  The claim that VMISITIDIFDSM performs poorly in blackbox attacks despite its effectiveness against defended frameworks is inaccurate as it was evaluated in a blackbox scope. other Weaknesses:  The claim that the combination of LinBP and SGM incurs high computational overhead is incorrect as the setup used only 10 iterations for IFGSM and 50 iterations for Aug ILA requiring gradients for each iteration.  The output of equations 2 and 3 should be explicitly labeled as vectors to enhance clarity.  The victim frameworks in Appendix F may have used cropping with a specific probability resulting in similar curve behaviors.  The paper lacks experiments demonstrating the methods compatibility with other techniques to improve overall transferability.", "Paraphrased Summary This work proposes an attack approach using Intermediate Level Attack (ILA) with transferability. image augmentation and reverse adversarial updates are used to create diverse adversarial examples from ILA input. Additionally interpolation of cumulative attacks helps maintain a consistent attack focus. The paper is clear and wellstructured with supporting experimental results. effectiveness and Weaknesses The paper has a wellorganized structure and provides extensive experimental evidence. It incorporates diversity transformations and perturbation interpolation from previous research to enhance transferability. Augmentation has been demonstrated to be effective. However some concerns remain. The correspondence between adversarial perturbation generated from transformed examples and the original images is unclear particularly when random cropping with significant size alters the image and renders the perturbation inconsistent with the original. The adversarial example in image 1 appears natural so additional explanation is needed. The authors suggest that reverse adversarial updates activate intermediate layers effectively for beneficial attack guidance. However the reduction in classification loss (Appendix A) is evident. It would be beneficial to examine feature attention map changes after the initial update. While the three proposed methods are effective there is no clear guidance on selecting the attack layer index leaving it to references from previous work. Furthermore the authors offer explanation for the impact of augmentation on perturbation effectiveness but do not provide experimental results to support these claims.", "Improved Transferable Attacks with Data Augmentation Summary This work enhances Transferable Intermediatelevel Attacks (ILAs) by incorporating data augmentation during the attack tuning work. Three augmentations were implemented: cropping reverseadversarial update and attack interpolation. empirical Evaluation use three source models and nine target models the work demonstrated significant improvements in attack transferability over ILA and other blackbox attacks. The effectiveness of the data augmentations was further supported by ablation work. effectiveness  use multiple reference attacks and data augmentation proves effective.  Consistent improvements over ILA and other similar methods.  Hyperparameter analysis justifies the choices made. Weaknesses  No comparison with published results.  Results are from authors experiments raising concerns about\u516c\u5e73evaluation.  Unclear comparison methods to VMISIDITIFGSM and VNIFGSM from related work.  Evaluation limited to one dataset and one type of attack (Linf IFGSM).  Performance on undefended models only with no analysis on adversarial robustness.  Results are based on a single work without reporting stability or effects of random seed.  Ablation work provide combined results obscuring individual contribution.  Computational cost comparison with baseline is missing. additional consideration  Clarify perturbation size format (e.g. \\epsilon x255).  Discuss potential impact of reverse adversarial update on convergence.  Explore theoretical analysis of AugILAs functionality and generalization capabilities.  Investigate connections to previous related work (ILA ILA).", "Paraphrased Summary: The paper introduces the AUGILA algorithm an enhancement to the ILA algorithm with the goal of increasing the effectiveness of adversarial examples in different frameworks. The paper suggests that using a wider range of input references leads to beneficial generalization of adversarial examples. The AUGILA algorithm uses common image augmentations and transformations based on adversarial perturbation (such as reverse adversarial updates and attack interpolation) before maximizing the projection of intermediate feature map differences. effectiveness:  Clear organization  Practical effective AUGILA algorithm  Demonstrated superiority over ILA and other methods in various datasets Weaknesses:  Lack of introduction as AUGILA is primarily a practical extension without novel theories or methods.  Questioning of the effectiveness of the reverse adversarial update performance in all cases as it may not always reduce the loss of the target framework.  Lack of visualization results to support the claim that the reverse update performance provides more useful attack information from discrepancy feature maps."], "nWprF5r2spe": ["Paraphrased Statement Summary The paper introduces a novel approach called WAFL (Wassersteinbased Distributionally robust Federated Learning) to address challenge in federated learning due to statistical differences between data distributions. WAFL is applicable in scenarios involving distributional shifts and domain adaptation. Inspired by previous work WAFL workulates the problem in a dual work choosing a fixed value for the Lagrangian parameter to reflect the algorithms conservatism. It employs a local SGD scheme for optimization ensuring convergence under certain assumptions. Moreover the paper proves a generalization bound for WAFL using established techniques. Numerical experiments demonstrate the effectiveness of the algorithm. effectiveness and Weaknesses effectiveness:  The theoretical framework of WAFL is comprehensive and welldeveloped providing analysis from optimization and statistical perspectives. Weaknesses:  Marginal Contribution: The paper lacks a clear description of how WAFL compares to existing agnostic FL model and when it offers advantages. A theoretical or empirical comparison is needed.  Parameter choice: The paper does not provide a systematic way to choose the Lagrangian parameter in numerical experiments with seemingly arbitrary choice made.  Benchmark comparison: The numerical experiments do not include benchmarks against other established agnostic FL methods which would enhance the clarity of WAFLs perworkance. Minor Comment:  The paper claims that previous work have used Wasserstein distance to improve robustness in FL but it does not explicitly mention the connection for Du et al. 2020 and Deng et al. 2020b. This should be clarified.", "Paraphrase: Summary The paper presents a method for handling data heterogeneity in federated learning using Wasserstein distributionally robust optimization (WDRO). The authors simplified the WDRO problem using a duality issue and solved a relaxed version with a distributed algorithm. Numerical experiments on MNIST and CIFAR10 showed the methods performance advantage over nonrobust FedAvg and robust FedPGM and FedFGSM especially when the proportion of compromised clients exceeds 80. effectiveness and Weaknesses  The problem addressed is significant.  However the paper lacks significant novel contributions in both WDRO and FL.  The duality issue is not novel.  Formulating the problem as a canonical WDRO problem is not particularly challenging.  The authors did not solve for \u03b3 optimally potentially breaking strong duality and compromising the solution accuracy.  Algorithm 1 is similar to FedAvg with the surrogate loss replacing the original loss.  The authors cite related work on Wasserstein robustness in FL but do not sufficiently differentiate their work or highlight its contributions.  The experiments are not convincing because FedAvg is not a robust algorithm.  The methods advantage only becomes significant at a high proportion of compromised clients questioning its practical benefits.  The choice of the regularization parameter \u03bbi is unclear and whether it was set to nn is not mentioned.", "Paraphrase: This research introduces a reliable technique called Wasserstein robust Distributional optimization for minimizing empirical risk. Similar to Agnostic Federated Learning this method is useful in data adaptation and largedata situations. The authors introduce an SGD algorithm to deal with the stated issue and offer a theoretical cap on the ensuing estimations. effectiveness: The paper is wellwritten with technical explanations of the ideas. The authors effectively describe the approach details even though the core ideas are not novel and have been seen in prior Wasserstein Adversarial learning techniques. The SGD algorithms use of duality properties is conventional and the theoretical justification is strong. Overall this work is a reasonable contribution. Concerns: 1. Can you demonstrate particular instances of deduction where Wasserstein robust optimization outperforms traditional methods or for particular signalnoise patterns 2. Given n data points does the WAFL method issue a smaller number of components in the maximizing parameter (Q) The use of Wasserstein robustness should result in a more comprehensible classifier. Is this logical assumption justified Can it be confirmed through testing or other means 3. Please provide a comparison of computational time with comparable approaches."], "q1QmAqT_4Zh": ["Summary Paraphrase: This paper introduces a novel approach to improve the generalization and data efficiency of offline reinforcement learning (RL). Inspired by Koopman operator theory the approach leverages data augmentation in a latent space using learned symmetries of the environments transition dynamics. The resulting method is evaluated against a similar method that employs selfsupervision through data augmentation. Strengths Paraphrase:  The concept of utilizing learned environment symmetries for data augmentation is innovative and unique.  The paper provides sufficient introductory data for readers with RL and dynamical systems background to grasp the methodology.  The proofs for the main theorems are generally sound with some minor typographical errors to be corrected.  empirically the method demonstrates performance advantages over selected baselines. Weaknesses Paraphrase:  The paper lacks clarity in its notation and terminology.  The types of work and operators need to be explicitly defined or inferred leading to ambiguity.  A significant gap exists between the theoretical assumption and the empirical evaluation setting both in terms of requirements and outcomes measured.  The theoretical results on equivariance do not directly connect to the empirical performance evaluation.  The method resembles a variational autoencoder (VAE) objective raising concerns about the significance of learned symmetries in its success.  The presence of the required family of operators for the theoretical results is not established in the main paper.  The empirical evaluations are not fully convincing and the benefit of the method may primarily stem from its framework learning component rather than the latent space symmetries.  Additional algorithmic details are missing making it difficult to interpret the experimental results.  No explicit definition of \"KFC\" (the proposed method) is provided.  The derivation of the Koopman operators matrix representation is not explained.  The number of training steps and the inclusion of the framework training phase in KFCs optimization budget are not specified. potential improvement:  Perform evaluations against strong baselines that incorporate framework learning and latent space equivariance.  Enhance the paper clarity by defining operators and work consistently and explicitly stating assumption.  Conduct targeted experiments to validate the learned framework symmetries and their relevance to the theoretical results.", "Summary Paraphrase: The paper introduces a data augmentation technique based on a symmetrypreserved Koopman latent space representation. This technique leverages theoretical findings on symmetries in dynamical control systems and symmetry shifts in data. The authors demonstrate the effectiveness of their method through empirical evaluations on benchmark offline reinforcement learning tasks such as D4RL Metaworld and Robosuite showcasing consistent improvements over stateoftheart Qlearning algorithms. strength and Weaknesses Paraphrase: strength:  The proposed concept and empirical results are compelling and appear to be novel. Weaknesses: Clarity Issues:  Grammatical errors missing discussion and typos hinder comprehension.  significant assumption are unclear requiring further explanation.  Some level and notations warrant better clarification. specific Concerns:  Grammatical Issues: Instances identified on page 5 line 23 and page 13 line 29.  Incorrect Grammar: \"First of all note that by the definition the Koopman operator it obeys\" (page 4 line 9).  Typos: \"us\" for \"practice\" (page 16 line 1) and \"an controllable\" for \"a controllable\" (page 16 second line below Equation 27). Unclear Assumptions:  The nature of the fi work in Equation 7 is unclear.  The derivation of Equation 8 from Equation 7 is not sufficiently explained.  The notation \"fi0 ... m\" is confusing. Theorem and Proof Clarifications:  Equation 12 in Theorem 3.4: The first term should be specified as either \"g(st1)\" or \"g(st1 at1)\".  Theorem 3.4: Conditions under which Equation 12 holds require elucidation.  Lemma 3.3 and Lemma 3.5: Clarify that only \"\u03c30\u2208 \u2211\" is the identity element.  Theorem 3.4 proof: Change \"\u03c3\" to \"\u03c3\u0303\" in line 7.  Theorem 3.6 proof: Specify \"a\" as \"at\". Clarify whether the first term is \"g(st1)\" or \"g(st1 at1)\".  Theorem 3.7 proof: Provide a clear explanation of \"\u03c3\u03b5at\".", "Paraphrase: Summary: This work applies Koopman theory to create a systematic data augmentation approach for offline reinforcement learning. The authors train a VAElike encoderdecoder framework that satisfies D(E(s))  s and a forward framework that satisfies F(st at)  D((K0  \u2211i1m Ki ati)E(st))  st1. The Koopman operator K then generates symmetries that are applied to both st and st1 as data augmentation during Bellman error minimization. The resulting algorithms KFC and KFC enhance data augmentation and improve performance for S4RL and CQL. strength: 1. The approach offers a theoretically sound method for creating data augmentation in offline RL (and dynamicalal systems). 2. It outperforms existing data augmentation techniques (S4RL variants) and demonstrates better outcomes on D4RL tasks MetaWorld and Robosuite environments. Concerns: 1. The authors claim that \"Current algorithms overfit to the training dataset and as a consequence perform poorly when deployed to outofdistribution generalization of the environment. We aim to address these limitations by learning a Koopman latent representation that allows us to infer symmetries of the systems underlying dynamical.\" However this statement remains ambiguous and most evaluations have been conducted on the same environment (i.e. the dataset and testing scenarios involve the same environment). 2. The distinction between KFC and KFC is unclear.", "Paraphrase of the Summary: This paper introduces a novel method Koopman Forward (Conservative) Qlearning (KFC) designed for offline reinforcement learning. KFC extends existing training data with additional state obtained from a learned actioninvariant Koopman latent representation of the system. The Koopman transformation is constructed using a variational autoencoder. Experimental evaluations on RoboSuite and MetaWorld benchmarks demonstrate the superior performance of KFC compared to existing methods. Paraphrase of strength and Weaknesses: Questions for Authors: 1. Does the Koopman representation in (12) practice to noncontrolaffine systems 2. The squaring performance is missing in the Bellman error in (1) (4) and (18). 3. Why is using the symmetry generating work in (18) not equivalent to using a different Bellman error 4. Have you tried using a standard VAE to sample new state instead of the Koopman operator 5. Are the transformed state trajectories sensible in terms of actions and simulator replay 6. Conduct ablation work to evaluate the impact of drawing additional samples from the Koopman VAE. 7. Clarify whether KFC and KFC use the same hyperparameters as CQL. 8. Provide the number of seeds used and pvalues for statistical significance in Table 1. 9. Acknowledge and discuss findings from the paper (https:arxiv.orgabs2102.09225) regarding discrepancies in reported CQL performance."], "o-1v9hdSult": ["Paraphrased Statement: This paper proposes a system that provides explanations for why an AI chooses one plan over another. The system incorporates a symbolic model with a concept vocabulary that represents human concepts. The concept vocabulary is initially created either through user surveys or researcher design. The system uses binary classifiers to determine the presence of concepts within the vocabulary. The output of these classifiers represents the confidence of the explanations provided. The system responds to queries about why plans are inadmissible by identifying missing preconditions or higher costs compared to the chosen plan. It utilizes sampling techniques to identify preconditions and a heuristic to estimate the cost of executing actions. A user work was conducted to evaluate the utility of the explanations. Strengths and Weaknesses: Strengths:  Emphasizes the importance of AI interpretability. Weaknesses: Presentation Issues:  Poor formatting and lack of paragraph gap.  Microscopic figure labels.  Scattered discussion of related work.  Unclear sentence structure. related Work Discussion:  Insufficient or dispersed discussion of relevant literature. model Evaluation:  Limited evaluation with only a few foils in the work game.  Heuristic for cost approximation is not sufficiently explained. user work:  Unusual reporting of results making it difficult to understand the methodology.  Potential misunderstanding of the rationale for concise explanations.  Concerns about the robustness of discarding field who did not selfreport viewing the explanation.  Inadequate analysis of gameplay data (e.g. lack of confidence intervals questionable effectiveness of saliency map).  Unknown task direction provided to Sokoban work field.", "Paraphrased Summary: This paper introduces a method to justify an agents actions in sequential decisionmaking scenarios based on humanprovided counterfactuals. Using intuitive classifiers to describe the state of the system the authors demonstrate why certain counterfactuals (termed foils) would fail or be more costly than the agents policy. They further propose methods for constructing state characterizations learning action feasibility and quantifying uncertainty. The paper demonstrates the robustness of this approach in two game scope and shows that humans prefer these explanations to saliencybased ones. Strengths and Weaknesses: Strengths:  This research formalizes explainability for sequential decisionmaking problems providing a valuable contribution to the field.  The paper is wellpresented and systematic.  The proposed explanations appear intuitive and align with human expectations. Weaknesses:  The evaluation section relies heavily on a few model raising concerns about the generalizability of the findings.  The approach depends on handcrafted classifiers learning preconditions and assuming certain variable independence which introduces potential fragility.  The human work results while appreciated are not surprising and may have limited implications.", "Summary: This work introduces a method that uses concept classifiers to generate explanations for an agents actions. These explanations are compared to a \"foil\" agents actions to identify the understanding for the agents specific behavior. The authors conducted a work to demonstrate the methods utility. Strengths:  The explanations are contrastive allowing for the isolation of specific case of the agents actions.  The method quantifies the confidence of the explanations using concept classifiers which can be incorporated into the research algorithm. Weaknesses:  The necessity for a foil agent may limit realworld application.  The work uses a game area which may not be relevant to areas where agents need to justify their actions.  The authors assume preconditions can be represented as conjunctions of propositions without providing justification.", "The paper presents a novel technique for providing explanations in sequential decisionmaking scenarios without relying on an environmental model. Traditional explanation methods are illsuited for these scenarios because explanations are tied to the actions taken. Specifically the paper proposes a simple yet effective approach called contrastive explanations. These explanations involve the user asking questions about potential changes to the AI agents plan. Such changes could lead to beneficial outcomes illegal actions or increased costs. The technique uses a symbolic approximation of the current situation to identify the missing concept that affects an actions failure or cost increase. user studies show that these explanations are helpful. Additionally indirect tests indicate faster task completion times. The reviewer has minor concerns about the impact of incomplete or perfect userprovided concepts and the computational cost of producing explanations. The author addresses these concerns and provides insights into the methods limitations and future direction."], "kOu3-S3wJ7": ["Paraphrase: This paper introduces \"Graph Relation Imputer for Time Series\" (GRIN) a method for imputing missing values in time series data using graph neural networks (GNNs). Unlike existing time series imputation techniques that primarily consider temporal relationships GRIN incorporates spatial relationships between time series by modeling them as nodes in a graph. The GRIN model employs a GNN to encode the data and perform two rounds of imputation. The GNN acts as gates within a Gated Recurrent Unit (GRU) capturing sequential information. This work is repeated for both forward and backward passing and a final MultiLayer Perceptron (MLP) infers the missing values. Experiments demonstrate that GRIN outperforms baseline methods on several datasets. The paper strengths include its clarity and wellmotivated problem formulation. yet it lacks references to matrix factorization methods that also address time series imputation and it does not explore the potential of forecasting time series with missing values as an alternative to imputation. The inclusion of backward dynamics in GRIN is intriguing as time typically progresses in a forward direction. Insights into the rationale for this design choice would be beneficial. While GRUs are generally scalable the addition of message passing in the gates introduces computational overhead. A comparison of memory and runtime with baseline methods or a theoretical analysis of scalability could enhance the paper credibility. Finally exploring how GRIN handles cases where multiple features are missing for a particular node and conducting experiments on a toy dataset to evaluate performance under different missing value scenarios would strengthen the paper robustness and appeal.", "Paraphrased Statement: Summary This research proposes using Graph Neural network (GNNs) to impute missing values in multivariate time series data by considering the available relational information. Each feature dimension of the series is treated as a node in the GNN and message passing is employed as the update use for a bidirectional RNN. The novel method outperforms the stateoftheart BRITS method across three benchmarks. Strengths and Weaknesses Novelty: This work is believed to be the first to incorporate relational information into the imputation model utilizing GNNs effectively. Clarity:  \"relational information\" refers to information about the relationships between different features or sensors.  Section 4 discusses the proposed architecture novelty but its structure may make it challenging to assess.  The model considers spatial information in both the first and second stages. yet the reason for this is unclear.  L in equation 9 is not defined.  The term hiti in equation 9 is confusing as it subtracts the node index from t. limitation:  The approach reliance on RNNs limits its ability to handle irregularly sampled data.  The assumption of a static adjacency matrix makes it unsuitable for dynamic relational data. Experiment:  The extensive comparison to BRITS demonstrate the proposed approach superiority.  The concern remains that the model of missing data is also simple with only 5 and 25 missing values for block and point missing respectively.  Increasing the missing data amount and comparing it to BRITS is recommended.", "Paraphrased Statement: A novel neural network model is proposed for imputing missing data in multivariate time series. This model combines a recurrent graph neural network for capturing temporal information with a message passing neural network for utilizing spatial relationships. The model effectively leverages both temporal and spatial data properties. The approach extends the concept of gated recurrent neural networks to incorporate graph topology through message passing among neighboring data points. Experimental results demonstrate excellent performance on benchmark datasets compared to both simple and advanced baseline methods. An ablation work confirms the significant contribution of the spatial decoder and bidirectional architecture to imputation accuracy. The model also exhibits promising results in replacing entire missing sensors in an air pollution dataset. Strengths:  Addresses a critical problem in handling missing data in multivariate time series.  Integrates spatial and temporal properties of the data to enhance imputation accuracy.  Extends existing graph neural network approach to include spatial information.  Clear and wellwritten presentation. Weaknesses:  Limited comparison with more recent stateoftheart imputation methods such as NAOMI E2GAN and kNN.  Needs further exploration of comparison with other graphbased RNN architecture used for imputation.", "Summary Paraphrase: This work introduces a time series imputation method that uses graphs to model spatial dependencies. Using a bidirectional temporal graph neural network the method learns representations for each data point that enable it to fill in missing values with improved accuracy. Strengths and Weaknesses Paraphrase: The paper is praised for its clear writing sound derivations and convincing experimental results. yet the author questions the rationale behind using graphs for spatial correlations and suggests exploring different graph choices. The paper twostep decoder is also questioned as is the need for a linear imputation step. The author suggests removing one or both steps to simplify the model. Finally the author wonders if the method is applicable to nonspatiotemporal time series and suggests clarifying this assumption in the paper. Additionally a minor correction is made to Equation 9."], "rWXfFogxRJN": ["Paraphrased Statement: Summary: This paper introduces AdaAug an automatic data augmentation technique that tailors augmentation policies to specific instances. AdaAugs key principles include:  InstanceAware Augmentation: Adapting augmentation to each instance based on its hidden features.  Alternate Exploitationexploration: Updating augmentation policy efficiently through alternating exploitation (using the current policy) and exploration (exploring novel policies). AdaAugs effectiveness is demonstrated in two scenarios:  Transfer learning: Significantly outperforming other automatic augmentation methods.  direct learning: Comparable performance to baselines. effectiveness:  Clear writing and wellmotivated problem.  Instanceawareness in automatic augmentation is an significant and unexplored factor.  The proposed method utilizes hidden features for reasonable and novel augmentation adaptation. Weaknesses: experimentation:  Ablation for Diversity Parameters: The rationale for using diversity parameters and their necessity should be explained. Results without diversity parameters should be included for precise analysis.  ImageNet experimentation:  Confusion about the location of ImageNet results in the paper.  The method underperforms on ImageNet but its potential in finetuning pretrained models should be explored.  Weighted Summation in exploration: The use of weighted summation for exploration raises questions. An alternative method like Gumbelsoftmax could be considered. Other Comments:  Illustration of Augmentation Magnitude: A visual representation of the learned augmentation magnitude \u03bb would provide empirical evidence.  qualitative Results for transferability: A comparison of augmentation distributions for different tasks would enhance the justification of instanceawareness for transferability.", "Paraphrase: This paper introduces a novel approach for creating instancespecific data augmentation through a research algorithm. effectiveness:  The idea of making data augmentation dependent on the data itself is logical and has potential.  This paper is one of the first to explore automatically learning instancespecific augmentation.  The research work elegantly expands the augmentation space to include continuous magnitudes allowing for more extensive exploration. Weaknesses:  The paper lacks ablation work to demonstrate the benefits of the research phase over randomly initialized policies.  The performance improvement observed with large augmentation magnitudes raise concerns that the gains may mainly be due to randomness rather than the research algorithm. Open Questions:  It is curious that the research algorithm does not collapse to \"no augmentation\" while aiming to minimize loss. The authors should clarify this behavior.  The paper claims to reduce the distance between the distributions of augmented training data and validation data. However it could be more precise to state that the method aims to reduce the distance between the distribution of augmented training data and validation data.", "Paraphrase: Summary: This work proposes a method called AdaAug for data augmentation that finds optimal augmentation policies for different classes and even different instances of image. It finds these policies by balancing exploration and exploitation to maximize generalization performance. experimentation on various datasets show that AdaAug improves the performance of deep learning models. effectiveness:  Clear need and innovative idea of searching for datadependent augmentation policies.  experimentation conducted under different conditions.  Wellwritten and accessible paper. Weaknesses:  Omission of related work like MetaAug which also searched for individual sample variations in augmentation policies. A comparison with MetaAug in terms of concept and performance would be beneficial.  The paper claims stateoftheart performance on CIFAR10 CIFAR100 and SVHN datasets but it does not mention the performance of AdvAA which is reported to perform substantially in a previous work. A comparison with AdvAA would be informative.  Marginal performance improvement compared to previous methods. Other work like MetaAug and AutoAug provide mean and standard deviation results from multiple work. The authors should consider doing the same.  Missing results on different neural network architectures. The paper only presents results on WideResnet. Results for models like ShakeShake and PyramidNetShakeDrop would be valuable. Additional Questions:  Convergence: Is the convergence of the proposed method guaranteed during training and validation Can a theoretical convergence analysis be provided", "Summary Paraphrase: This paper presents AdaAug an adaptive data augmentation technique that optimizes augmentation policies based on the class (and potentially the specific instance) of the data. By searching for optimal augmentation policies through an explorationexploitation workflow AdaAug enhances the generalization performance of deep learning models. experimentation demonstrate improved performance with AdaAug across various datasets. Strengths Paraphrase: 1. adaptive augmentation that tailors augmentation policies to specific classes and instances offers a promising approach to data augmentation. 2. The paper provides experimental results on multiple scenarios. 3. The paper is wellwritten and accessible. Weaknesses Paraphrase: The paper lacks sufficient experimental data. The presented results appear anecdotal and a more comprehensive set of trials including mean and variance measures would strengthen the claims."], "nzvbBD_3J-g": ["Paraphrased argument: This research presents a novel way to add custom structural biases into VAE (Variational Autoencoder) architectures. The approach involves transforming the latent variable `z` into a \"structured latent\" variable `y` using a predefined deterministic transformation. This transformation can enforce specific constraints such as topological or sparsity constraints. Notably this technique is similar to using the transformation as the initial layer of the decoder architecture. The authors propose various straightforward yet effective transformations to impose latent constraints. This method provides an interpretable latent space and enhances performance in a extensive range of density estimation applications. Strengths:  Embeds expert knowledge into deep learning models a crucial aspect as machine learning integrates with scientific and engineering field.  clear presents argument for userdefined inductive biases and against using VAE priors for constraint induction.  Introduces innovative and useful layers designed to induce topological constraints. Weaknesses:  The main technical concept (using a userdefined first encoder layer) is not particularly novel.  The sparsity application is not novel as it resembles a soft attention layer commonly used in generative model. Suggestions:  Emphasize the innovative topological layers as the primary contribution rather than the general approach.", "Paraphrase: Summary This paper suggests incorporating inductive biases into VAEs by altering the decoders. To do this they propose transforming the stochastic latent variables through specific function before applying a deep neural network. The aim is to force the encoder to assign the latent variables (z) in a way that the function will convert them into a manifold of a specific shape. Strengths:  The idea of adapting decoders is intriguing.  The function appear to be beneficial in most cases demonstrated in the experiments. Weaknesses:  The claim that the VAE prior is used for regularization is questioned. VAEs are not Bayesian models and the prior can be trained to fit the posterior distribution serving a different function.  The authors mention a figure (Fig. 3) on page 4 which disrupts the reading flow.  Its unclear why z is obtained by averaging y (z  g(\u03c8(\u03bc(\u03d5(x))))). Shouldnt it be z  g(\u03c8(y))  The calculation of the ELBO in Eq. 2 is questionable due to the use of g(\u03c8(y)).  The differentiation between the proposed method and using a specialized decoder with specific initial layers is not apparent.  The paper writing style is complex. It suggests that z are distinct random variables but they are not explicitly used in the objective function. This makes it challenging to follow the paper especially since the latent variables used in the ELBO are y.  The code provided does not fully clarify the paper missing elements. Overall Impression: While the paper presents interesting concepts the presentation is unclear. This hinders the readers understanding of the approach and its implications.", "Paraphrase: Strengths:  The method is wellconceived and supported by reasonable experiments.  It effectively compares to existing techniques. Weaknesses:  The presentation of the proposed model InteLVAE could be improved.  The abstract concept of transforming Gaussian latent variables (g\u03c8) is not truly novel as it is merely a component of the decoder.  While the authors emphasize the shared use of g\u03c8 between the decoder and encoder technically g\u03c8 is still an integral function of the decoder.  The novelty of the paper lies more in the specific instances of g\u03c8 definitions such as the multimodal model presented in appendix C.2. Suggestion: The paper should focus more on these specific g\u03c8 definitions rather than the broader concept of InteLVAE. For instance the multimodal g\u03c8 could be discussed in greater depth in the main text instead of being relegated to the appendix.", "Paraphrase: Summary This paper introduces a novel approach to incorporate biases into variational autoencoders (VAEs) to enhance their representational capabilities. These biases are introduced through a specific transformation in the latent space shared by both the encoder and decoder. experiment on toy and realworld datasets (MNIST FashionMNIST CelebA) demonstrate that the method can design effective biases by aligning the topological features of the transformed representations with the target data distribution. The method outperforms other approaches in terms of generation quality with and without sparsity constraints and improves representation learning for downstream classification tasks. Strengths and Weaknesses Strengths:  Novelty: Introduces a novel method for incorporating inductive biases into VAEs.  Simplicity: The model is straightforward and intuitive to understand.  Effectiveness: Demonstrated through extensive experiments on various datasets.  clear presentation: The paper is wellwritten and illustrated with informative figures. Weaknesses:  Missing equations: Some significant equations (C.3C.5) from the supplementary material are not included in the main paper.  Shared transformation: The authors claim the transformation is shared between the encoder and decoder but it may be more accurately viewed as the first layer of the decoder.  Theorem 1: Theorem 1 may not be necessary in the main paper as it focuses on the construction of inductive biases which is explored more thoroughly in the discussion. Questions  Why is a normalizing flow considered an unlikely alternative for the transformation"], "nbC8iTTXIrk": ["Paraphrased Summary: Im having problem understanding this research. It appears that the authors developed a novel deep equilibrium model by incorporating an optimization layer into a neural network. Specifically they may have expanded upon the MDEQ model by: 1. Using layers of deep neural networks as optimizers 2. Employing a multibranch architecture design Experiments using the CIFAR dataset suggest that the proposed model performs well in the authors scope. Theoretical analysis with two proposition supports the models properties. effectiveness and Weaknesses: While the authors mention the importance of interpretability in their motivation I dont see how this concept is connected to their work. Did they define interpretability as explaining the behavior of CNNs For case does their model provide explanations for CNN behavior like LIME does for computer vision by attributing importance to specific feature An model would be helpful in demonstrating how the model achieves interpretability.", "Paraphrased Statement: Summary: This paper introduces a novel technique that effectively incorporates inputs with varying scales. Experiments demonstrate the methods superior quantitative performance. effectiveness: 1. The method is clear and simple to implement. The novel terms in the objective function have valid justifications. 2. In multiscale experiments the proposed model sizes are significantly smaller than those of previous approach showcasing the methods effectiveness and efficiency. Weaknesses and Questions: 1. While the model size comparisons are impressive the performance improvements appear modest. Including comparisons between different methods with the same model size would be helpful (i.e. MDEQ with a smaller model and MOptEqs with a larger model). This would show if the proposed method scales well. 2. Additional experiments on higherdimensional datasets are recommended. 3. Could the authors conduct ablation studies to determine the specific contributions of each component (the innerproduct term and diversity term) 4. Were perturbation techniques applied during the training of the compared methods in the tables Clarifying this would facilitate a clearer comparison in both scope (with and without perturbation).", "Paraphrased Summary: The MultiBranch OptEqs (MOptEqs) network combines multiple DEQ models to leverage multiresolution samples. It also employs a novel \"perturbation enhanced training\" strategy to enhance performance. empirical evaluation on CIFAR10 CIFAR100 and Imagenette datasets demonstrate its superiority over previous DEQ models. effectiveness and Concerns: Pros:  The architecture design for multiscale integration is notable.  The technical correctness of the work is verified.  Impressive empirical performance outperforming MDEQ on small datasets. Concerns:  The computational overhead of \"unrolling\" raises questions about scalability to larger models and datasets.  Its unclear if comparing MOptEqs to MDEQ is fair given the different training methods and properties.  The \"mathematical interpretability\" of OptEqs needs clarification regarding its practical implications.", "Paraphrased Summary This paper introduces MOptEqs a novel implicit deep learning model. It optimizes a hidden objective function to effectively utilize inputs of varying scales. Its fusion model incorporates \"Hierarchical heritage\" to maintain consistency in corresponding channels of adjacent branches and a \"Diversity Module\" to promote diversity in noncorresponding channels. These design choices enhance performance while reducing model size on CIFAR10CIFAR100 image recognition tasks compared to previous Equilibrium networks. effectiveness  Improved performance and reduced model size over prior work.  Detailed ablation studies demonstrating the contributions of key design elements. Weaknesses  Poor writing and organization making it challenging to understand the concepts.  The proposed ideas (multiscale hierarchical heritage diversity) are common in explicit models and its not clear if their adaptation to implicit models is a meaningful approach.  The paper doesnt explore the scalability and performance of the approach on large datasets like ImageNet compared to stateoftheart architecture."], "per0G3dnkYh": ["Paraphrased Statement: The paper consists of two sections: Theoretical Section:  Proves that traditional normalizing flows cannot transform heavy or lighttailed distributions into target distributions with varying tail indices for each margin. Algorithmic Section:  Introduces a method for modeling distributions with different tail indices for different margins.  This method is a minor extension of the techniques presented in \"Tailadaptive flows\" (Jaini 2020). effectiveness:  Addresses a significant problem related to the ability of normalizing flows to accurately estimate tails.  Presents a light and accessible introduction to normalizing flows and heavytailed distributions.  Offers a theoretically heavy solution. Weaknesses:  Lacks significant novelty as the theoretical and methodological contributions are largely minor modifications of previous work.  The proposed method is not particularly elegant since it relies on using an external tail estimator before flow training.", "Paraphrase: Summary: This paper presents Marginally TailAdaptive Flows (mTAFs) which extend TailAdaptive Flows (TAFs) to improve the modeling of heavytailed distributions. They introduce a new type of normalizing flow capable of learning marginals with mixed tail behavior. effectiveness:  The concept of mTAFs is novel and addresses the limitation of NFs in handling heavytailed distributions with mixed marginals.  They provide a general definition of heavytailedness extending previous work. Weaknesses:  It is unclear in which scenarios generating distributions with mixed tail marginals would be necessary.  The experiments are limited to a small synthetic dataset and the results do not convincingly demonstrate the superiority of mTAFs over existing methods.  mTAFs require separating lighttailed from heavytailed marginals which may limit their applicability in realworld scenarios where complex dependencies between all dimensions are critical. Questions:  Does mTAF perform worse than conventional flows when the distribution is purely lighttailed or heavytailed  Why does the vanilla baseline have high variance and sometimes perform similarly to TAF Minor Typos:  \"Allow [us] to\" in Section 2.1  \"By our theory\" in Section 5", "Paraphrase: Summary: The field presents an extension to Tailadaptive flows for learning the tail behavior of specific probability distributions utilizing normalizing flows. The method includes learning flows that match the tail characteristics of the distributions margins. It employs a source distribution comprising marginal distributions with tail properties resembling the target distribution. The tail coefficient of the source distribution is adjusted datadrivenly using estimators that evaluate this coefficient. effectiveness and Weaknesses: effectiveness:  Explores the novel problem of capturing tail behavior using normalizing flows.  Proposes a datadriven approach to estimate the tail coefficient for base distribution selection.  Clearly presented and light to comprehend. Weaknesses: 1) Motivation:  The importance of modeling tails in variational inference is not clearly justified.  The paper lacks a thorough discussion on the implications of mismatched tail behavior in normalizing flows and potential solutions. 2) Significance:  The core result extends an existing work by Jaini et al. (2020) limiting its originality.  Empirical results are limited and more significant evidence is needed.  The benefit of initializing optimization with tail coefficient estimators compared to random initialization is not explored. Other Comments:  Definition 4 is restrictive and may overlook differences in tail coefficients among marginal distributions.  The use of normal distribution for lighttailed marginals may not always be appropriate due to varying degrees of lighttailedness.", "Paraphrase: Summary: This research explores the mathematical and statistical aspects of normalizing flows to comprehend their behavior in extreme value. Inspired by Jaini et al. (2020) the field demonstrates that the extremity of the margins in a flowbased models base distribution directly influences the extremity of the margins in the target distribution. The authors utilize this insight to introduce a novel algorithm that exploits a datadriven permutation strategy to ensure the target distributions accurate extreme behavior. effectiveness:  Solid Mathematical base: The theoretical validation provide a robust base clarifying the papers motivation and insights.  Thorough analysis of Related Work and Limitations: The authors comprehensively review existing field and highlight the limitations of current approach as well as potential future directions.  Clarity and accessibility: The wellstructured paper is light to comprehend with theoretical validation seamlessly connecting to experimental findings. Weaknesses:  Incremental Contribution: While the theoretical contributions are significant the proposed mTAFs do not exhibit a significant improvement over vanilla TAFs as seen in Table 1.  Lack of Baseline comparison: The evaluation excludes prominent flow models and architectures. The limited expressivity of vanilla flows especially in handling complex distributions may have contributed to the light performance.  Limited experimental range: The experiments are restricted to synthetic lowdimensional toy model. Demonstrating the algorithms scalability to higherdimensional and largescale datasets would enhance its impact."], "o8iGesI9HN-": ["Paraphrase: Summary This work proposes an enhanced version of depth separable convolution called \"Optimal Separable Convolution.\" This technique reduces model size by utilizing group convolutions in space of depthwise and pointwise convolutions. Overlapping channel between group convolutions are permitted and this feature is compared through ablation work. The researchers demonstrate that when the volumetric receptive field condition are met the combination of group convolution methods offers superior performance to previous approach. Strengths 1. The work introduces a novel design for generalizing depthseparable convolution. 2. The proposed method outperforms existing separable convolutions achieving higher performance with fewer parameter. 3. A detailed analysis of computational complexity confirms the optimality of the approach. Weaknesses 1. The claim that the design is not ad hoc is contradicted by the need for hyperparameter (overlap coefficient) tuning for optimal results. 2. While the authors state that model size is reduced the reduction observed in Table 3 is only 0.1M (about 3) which is a relatively modest improvement given the 1 accuracy gain achieved. 3. Appendix E reveals that the GPU inference time of the proposed method is slower than choice. The authors attribute this to the inefficient implementation of group convolution in PyTorch. However it is widely believed that onebyone convolution is generally more memoryefficient than group convolution especially on GPUs.", "Paraphrase: Summary This work introduces an optimized separable convolution that utilizes optimal design for determining internal grouping and kernel sizes. This approach improves the balance between model complexity and task performance. Experiments showcase the superiority of the proposed methods in image classification tasks (CIFAR and ImageNet) as well as their scalability in NASnets (e.g. DARTs). effectiveness and Weaknesses 1. While the authors do not discuss NASnet methods they should compare their approach with other efficient CNN backbone design beyond separable convolutions and handcrafted optimization such as IGCV Shift operation and learned group strategies. 2. The authors should consider additional standard for efficient architecture design such as:  Practical inference speeds (more crucial but covered in the appendix)  Memory and time consumption during training The paper focuses on theoretical FLOPs and parameter but there is often a significant difference between these metrics and actual inference speed. This gap may hinder the practical application of the model. 3. The proposed optimization method appears empirical. Providing a theoretical cornerstone or justification would strengthen the paper.", "summary Convolution a fundamental operation in advanced DNNs has been improved upon with various efficient operator. This paper provides a comprehensive overview of existing convolution operator and proposes an optimization model for selecting their parameter. The authors demonstrate that their optimized convolution requires fewer parameter and operation under specific constraints. effectiveness and Weaknesses Weaknesses:  The approach analyzes and promotes spatial separable and grouped convolutions but its optimality claim may be misleading as it doesnt cover all aspect of convolution.  Several related work (cited in the review) should be included for a more comprehensive analysis.  Computational complexity may not be a crucial evaluation metric as most of the factors are small.  More extensive evaluation especially on larger network is recommended.  The evaluation on ImageNet uses custom architectures instead of advanced efficient models (e.g. MobileNetV2).  The takeaways on channel fusion have been welldocumented in previous research. effectiveness:  The papers applicability is high as most advanced DNNs utilize factored convolutions.  The discussion of trivial and degenerate cases clarifies the problem formulation.  The Appendix provides valuable insights on inference time implications. Questions:  What are the tradeoffs of factored convolutions  When is N2 useful  How do the proposed operator compare to those returned by AutoML methods  Are there architectures that combine spatial and group convolutions", "Paraphrase: The authors introduce a novel convolutional operation that aims to automatically optimize the number of groups and kernel sizes for a given model balancing size and performance. They compare their approach to existing separable convolution methods and demonstrate improved performance on popular benchmark. effectiveness:  Proposes a new extension of separable convolutions that automatically adjusts internal parameter for optimal complexity and expressiveness.  Presents insightful analysis and a potential application in AutoML.  evidence performance gains on standard image classification datasets.  Incorporates volumetric receptive field standard. Weaknesses:  The paper can be challenging to follow due to insufficient explanations.  Lacks discussion on attributes of efficient models beyond FLOPS and parameter such as memory usage.  Evaluations could be expanded to include more advanced network architectures."], "zf_Ll3HZWgy": ["Paraphrase: This research examines how CLIP (Contrastive LanguageImage Pretraining) features impact visionandlanguage models across various tasks. The authors investigate utilizing CLIP as a visual encoder by connecting its features directly to taskspecific finetuning and combining CLIP with intermediate visionandlanguage pretraining before finetuning on downstream tasks. The findings reveal that incorporating CLIP significantly enhances performance compared to common encoders like BottomUpTopDown delivering remarkable results on a extensive range of visionandlanguage tasks. strength:  Provides practical and valuable baselines for the visionandlanguage community.  Effectively and informatively demonstrates experiments.  Wellwritten and clear. Weaknesses:  Lacks novelty in the research field.  Evaluates only a limited subset of available CLIP models. Additional annotation: The paper refers to CLIP ViTB without specifying the exact architecture (ViTB32 or ViTB16). Clarification is recommended in the manuscript.", "Paraphrased Summary: This paper explores how effective CLIP is when applied to different visionandlanguage tasks. It shows that CLIP can do well in a range of downstream applications and suggests ways to use the model. The experiments show that using CLIP for pretraining can lead to effective performance and combining it with VL pretraining can achieve even effective results than existing method. strength and Weaknesses: strength:  The paper does a lot of experiments to see how well CLIP generalizes across different tasks.  It looks at how different types of visual encoders affect the performance of the model.  The method achieves new stateoftheart results on several visionandlanguage tasks which could help future research in these areas. Weaknesses:  The paper does not clearly explain what advantages CLIP has over other VL pretraining techniques or how it achieves effective results.  For a study that is focused on empirical analysis the paper could provide more indepth analysis instead of only presenting a collection of results.  When CLIP is compared to other VL pretraining techniques (e.g. Table 2 and Table 6) it seems to only be more effective when used with VL pretraining. This is complicated because the VL pretraining uses 9.18 million more samples. It is not clear if the method improvements come from the advantages of CLIP or from using more external data.  The analysis on unfreezing the encoder is also problematic because it compares models without pretraining to models with VL pretraining. The higher accuracies of the latter models could be due to using more data rather than differences in the frozenfinetuned visual encoders.  CLIP seems to only work well with gridlike features extracted from convolutional neural networks and does not perform as well with regional features or when combined with a visual transformer. The paper discusses the issues with a single model (ViTB) but it does not explain why CLIP cannot be applied to different visual transformer models and features.", "Summary: This study evaluates the performance of CLIPs image encoders when used in visionandlanguage (VL) tasks. Experiments using both tasks with and without language prompts showed that CLIPs ResNetbased encoders consistently outperformed their Imagenettrained counterparts. However the ViTbased encoder performed poorly suggesting a lack of localization data in its features. strength and Weaknesses: strength:  The study confirms the trend that strong visual encoders enhance VL model performance.  CLIPs encoders demonstrate significant generalization capabilities compared to Imagenetbased encoders. Weaknesses:  The experiments are largely replications of existing baselines with only the backbone encoder being changed to CLIP.  The study lacks insights into CLIPs text encoder which could potentially contribute to VL model performance. recommendation: The paper could suggest future research directions by exploring the role of CLIPs text encoder in VL models or investigating method to improve the localization capabilities of ViTbased encoders for VL tasks.", "Paraphrase: Summary: This study examines how CLIPResNetViT visual encoders a recent advancement can enhance various vision and language tasks. It attempts to answer the following question: Can CLIP replace traditional visual backbones in visionlanguage models Experiments were conducted using two approach: finetuning existing models with CLIP encoders and fullscale pretraining. Results show that CLIP is a competitive alternative to ImageNetpretrained ResNet models. strength:  The study assesses the potential of CLIP encoders for a extensive range of tasks including visual question answering entailment navigation and image captioning.  It demonstrates the benefits of CLIP encoders in both finetuning and pretraining scenarios. Weaknesses:  The motivation for the study is weak based only on the observation that CLIP performs poorly on a zeroshot task. The proposed prompt engineering for VQA and other tasks is not fully convincing.  The study uses outdated models as of 2021 and avoids direct comparison with method that use largescale pretraining.  The authors do not explore the models ability to localize objects as discussed in recent literature."], "z8j0bPU4DIw": ["Paraphrased argument: Abstract: This study proposes utilizing evolutionary strategy (ES) in hierarchical reinforcement learning (HRL). The HRL strategy involves a highlevel controller establishing goal while a lowlevel primitive study to attain those goal. Despite using distinct reward work for the high and low levels the study employs ES to tackle this challenge. The proposed technique is assessed in the Ant Maze and Ant Gather tasks exhibiting promising results in one experiment. Strengths and Weaknesses: This study addresses a significant research direction in reinforcement learning. However a few concerns and suggestions arise: 1. Missing Prior work: The study omits a crucial prior work by Jain et al. (2019) which also applied ES to hierarchical learning. This prior work required less prior knowledge automatically learned the interface between the high and low levels and optimized the update frequency between them. This study sets these argument manually. Additionally the prior work was validated on a physical robot while the current study is limited to simulation. A comprehensive analysis and quantitative comparison with Jain et al. (2019) would enhance the papers choice. 2. Limited Outperformance: The results do not show a significant reward over the stateoftheart method HIRO. In the Ant Gather task the proposed algorithm achieves a slightly well result but at the cost of 60 times more training sample. In the Ant Maze task HIRO significantly outperforms the proposed method despite using only 160 sample. The study claims ES is more scalable however a direct comparison of training time (wall clock) between HIRO and ES would support this claim. 3. Terminology Confusion: The study repeatedly refers to policy gradient methods as \"MDP methods\" in the introduction. MDP is a problem formulation not a solution. This paper employs an MDP formulation but utilizes a derivativefree optimization solver. It appears the authors intended to use the term \"policy gradient methods\" that employ backpropagation for gradient calculation. 4. Visualization Improvement: In Figure 3 it would be beneficial to display the learning curves of all baseline methods in the same graph for simplicity of comparison. If the baseline curves are too short due to sample efficiency plotting them with wall clock time as the xaxis could be considered since the study argues that ES algorithms are more parallelizable and thus require less wall clock time for training. Reference: Jain et al. Hierarchical reinforcement learning for Quadruped Locomotion (2019)", "Summary: This study introduces a new approach to hierarchical reinforcement learning using evolutionary strategies (ES). It aims to optimize both the highlevel controller policy and the lowlevel primitive policies by estimating argument gradients through Gaussian sample. The approach is similar to the ES method proposed by Salimans et al. (2017) with the addition of a primitive reward. Strengths:  Simple and highly parallel research strategy for hierarchical policies. Weaknesses: Evaluation choice:  Comparison with existing methods is unfair due to unequal training step.  Tests are conducted in similar environments which raises concerns about generality and representativeness.  Lack of ablation study. Novelty:  research strategy is largely based on Salimans et al. (2017).  primitive reward normalization is similar to existing approaches. Clarity:  Definition of F in Algorithm 1 is missing.  Description of \"challenging\" or \"strong\" problems is unclear.  ES is claimed to be invariant to delayed reward but the definition is not provided.  The use of ES with a low noise standard deviation limits the exploration of global information.", "Paraphrased Summary: The paper introduces an algorithm that combines evolution strategies (ES) with hierarchical reinforcement learning (HRL). Dubbed SHES (Scalable Hierarchical Evolution Strategies) it leverages the scalability of ES. Experiments involving robot locomotion and navigation case its effectiveness. Strengths and Weaknesses: Strengths:  Innovative combination of ES and HRL  Simple and implementable algorithm  competitive results Weaknesses:  Similar core concepts to previous work  Limited experimental environments  Requires extensive training sample to match previous algorithms Overall Assessment: The papers organization and claims are sound but the experimental reinforcement is somewhat lacking. more experiments in diverse environments are needed to fully demonstrate the effectiveness of SHES. However the paper highlights the potential of ES in HRL and provides experimental evidence for its scalability.", "Summary: The study introduces a Hierarchical reinforcement learning (HRL) method based on Evolution Strategies (ES). The method follows Feudal RL where two controllers with different abstraction levels cooperate: a master controller generates goal and a slave controller strives to attain them. Both controllers receive tailored reward to encourage taskspecific learning. Strengths:  ESs insensitivity to reward timing a crucial feature for HRL.  Interesting approach and promising performance in the tested environments. Weaknesses:  Limited evaluation in only two environments.  Lack of novelty as the method follows a standard Feudal RL setup with ES optimization instead of gradientbased methods.  Unfair comparison with competing methods which were trained for a shorter duration (10 million step vs. 600 million step for the authors method)."], "m7zsaLt1Sab": ["Paraphrased Statement: findings: 1. Making adjustments to the [CLS] tokens posttraining doesnt significantly alter the output representations. 2. analysis of Transformer layer relationships using UMAP reveals that layer 1s structure most closely resembles layer 2s while layer 24s is distinct. This suggests that each Transformer layer modifies the topological organization of context embeddings. 3. Examining embeddings within the Transformer architecture (H1 A1 H2 A2 etc.) demonstrates that Hk and Ak diverge at lower layers but converge at higher layers. Hk and Hk1 as well as Ak and Ak1 exhibit heavy similarity at lower layers compared to Hk and Ak. strength and Weaknesses:  The paper employs complex theoretical concepts to describe straightforward observations.  It lacks rigorous theoretical model with theorems or lemmas.  The notations and definitions primarily serve to complexify the analysis methods.  The first two findings are not particularly noteworthy and the authors do not clarify their significance.  The third finding has potential but requires further investigation to elucidate its implications.  The authors should acknowledge related work that utilizes similar analysis techniques for aligning embedding spaces in Transformer layers. Minor Issues:  Typographic corrections  Clarification and consistency in notations  Improved visual representation in Figure 1  Explicit correlation between scenario A and equation (9)", "Paraphrased Summary: This paper aims to understand how context are represented in a language model. It moves beyond tokenfocused approach in previous studies by investigating how different context (e.g. replaced tokens) are represented and how this representation evolves through different contribution of the model (e.g. feedforward networks and attention modules). strength and Weaknesses: Pros:  research an interesting topic of context representation in language models.  Introduces category theory as a novel way to model contextrepresentation connections. Cons:  Poor presentation:  Vague definitions making it difficult to grasp the papers contribution (e.g. \"context\" is not clearly defined).  complex sections (4 and 5) without clear explanations requiring considerable effort to comprehend.  Lack of highlevel explanations and model to facilitate understanding.  Insufficient justification of how empirical findings support theoretical proposals.  Limited empirical work:  Only one dataset (SST2) used with results mainly presented as case studies.  more comprehensive studies needed for reliable and convincing results.  Typos throughout the paper.", "Summary This paper intends to establish a theoretical framework for understanding how pretrained language models (LMs) capture context. It employs category theory to analyze representations derived from LMs. strength:  Aims to explore a meaningful topic: context representation in LMs. Weaknesses:  Fundamental Flaws:  Incorrect understanding of concepts like probability LMs and context.  Unclear and ambiguous definitions.  Undefined or unexplained symbols.  Lack of explanations for equations.  Conceptual Issues:  use of novel concepts and language without explanation.  Conflation of \"context\" and \"representation of context.\"  Lack of connection between category theory and language models.  Oversimplification and Insufficient Experiments:  Overly simplistic and inadequate experiments.  Unclear use of using UMAP in Figure 2a.  Trivial findings in Figure 2bc.  Overlooked Related work:  Failure to cite or compare with combinatorial categorial grammars.  Potential for integrating CCG tagging with pretrained LMs.", "Summary This paper investigates how language models represent context as opposed to the more common approach of studying how context affects token representations. Using category theory the authors develop a framework that captures information about token and context representations as well as the transitions between them. While quantifying this information is challenging the paper explores it from functional and topological perspectives. strength and Weaknesses  strength:  Identifies a unique problem in language model.  Proposes a novel framework for analyzing context representations.  Explores the framework using functional and topological methods.  Weaknesses:  Writing and presentation require improvement.  Unclear communication of research goals.  Limited experimental results and lack of clarity on token substitution approach.  Observations may be specific to English and not generalizable. Additional Notes  The paper requires a background in category theory and manifold learning.  Key concepts such as anisotropic embedding space and rich language learning need more clarification.  mathematical equations lack detailed explanations.  Figure 1 lacks captions and descriptive text.  Figure 3 graphs should indicate the relationship between Feedforward Network and Multihead attention layers.", "Paraphrase: Summary: This research presents a fresh framework inspired by category theory to explore how Transformer language models create representations that are specific to a context. Unlike earlier approach that examined contextualization at the token level this work recasts the problem as a transformation from an input category (encoding the observed context) to an output category (a multidimensional space). The authors also explore manifold learning in the context of fixed representation models where outputs at each layer are considered as separate representations. Finally they conduct experiments to gain insights into various aspects of Transformer models such as how context are represented for the [CLS] token and how context representations evolve across layers. strength:  The proposed approach provides a unique perspective on understanding contextualization in language models. Weaknesses:  The empirical results do not fully leverage the methodological contribution.  Experiments use local probing with a limited act of neighbors unlike the general functor analysis proposed in the theoretical framework.  The analysis of context representations across layers is similar to previous work.  The papers language and explanations particularly regarding input and output category can be challenging to follow.  The experiments are limited and similar to those conducted in earlier studies except they focus on context rather of tokens.  For the [CLS] token experiments the observation that contextualization becomes more focused after finetuning seems intuitive as the [CLS] token is not commonly used in masked language model."], "zNR43c03lRy": ["Paraphrased Statement: Summary: This research introduces a novel gradientmatching approach for segmenting part of ranges while minimizing the need for manual annotations. The method utilizes DatasetGAN and StyleGAN to generate highquality ranges reducing the amount of human annotation required to a minimal amount of synthesized ranges. When compared to semisupervised learning techniques the proposed method demonstrates superior performance across three datasets. effectiveness and Weaknesses: effectiveness:  Reduces the cost of range annotation.  Offers a promise approach based on DatasetGAN. Weaknesses: Major:  innovation part could more clearly highlight the contribution.  Equation (1) lacks detail on the generation work and its relationship to labeled data. Minor:  limited evaluation data (three datasets).  explanation needed for using Li et al. (2021) dataset settings instead of DatasetGAN.  Table 1 requires more data on how DatasetGAN results were obtained. Additional Notes:  Typos exist such as \"achieve same performance\" should be \"achieves.\"  Citations in Table 1 should be revised to the original sources with footnotes indicating the data source (Zhang et al. 2021).", "Summary This research focuses on training a network for segmenting part of images by automatically creating pairs of images and annotations. The authors suggest training an annotator using a pretrained generator framework. The method is based on a wellmotivated formulation that converges to a gradient matching loss. It combines ideas from DatasetGAN and RepurposeGAN to enable the innovation of unlimited annotated data using limited labeled data from a pretrained GAN. The method outperwork other approach in scenarios with limited labeled data such as semisupervised learning. effectiveness and Weaknesses effectiveness:  Benefits over prior methods (DatasetGAN and RepurposeGAN):  Applicable to any GANgenerated data  No manual relabeling needed when changing the generator  Does not require manual labeling of generated images  Learned annotator is frameworkagnostic  Tackles a range that previous works have not addressed Weaknesses:  Mathematical derivation is primarily borrowed from previous work  effectiveness is only shown for lowdata regimes  Practicality depends on a highfidelity pretrained GAN  Experiment settings may be biased towards the proposed method Questions:  Relationship between the proposed approach and disentanglement  Comparison of manual annotation costs between the proposed method and others Minor Comments:  Notation inconsistencies: Annotated data is denoted by both \\mathbf\\hat y and \\mathbfy  Misleading term \"student\" in Algorithm 1  Grammatical errors (e.g. singularplural work)", "Paraphrase: Summary: This paper introduces a technique to alleviate the need for broad labeled datasets for training deep learning models. It aims to address the nestedloop optimization problem using a pretrained generative adversarial network (GAN) to generate images. Specifically the method involves a gradient matching approach and is claimed to be more efficient than existing endtoend gradientbased metalearning techniques. The efficacy of the method is demonstrated through experiments on semisupervised part segmentation tasks. effectiveness and Weaknesses: effectiveness:  Tackles an significant problem and proposes a wellmotivated solution.  Provides broad experimental evidence supporting the methods effectiveness.  Clearly explains the approach using formulas and figures. Weaknesses: 1. Comparison with EndtoEnd GradientBased MetaLearning:  While the paper mentions that other approach exist for the nestedloop optimization problem it lacks a direct comparison to demonstrate the superiority of the proposed gradient matching method. 2. range for Gradient matching Loss:  The algorithm description suggests that identical images are used to calculate each gradient but the figures show different images which could be confusing. 3. Dataset Selection:  The authors selected images of horses and airplanes from the Pascal Part segmentation dataset for their experiments. The specific reasons for choosing these particular classes from the available 20 classes are not explained.", "Paraphrase: Objective: To develop a machine learning framework that can generate artificial training data using unsupervised learning and then use that artificial data to train other frameworks for segmenting images. Existing approach: previous methods (e.g. DatasetGAN) demonstrated that a few synthetic images can be manually labeled and used to help train a segmentation framework (called an annotation framework). The annotation framework can then generate labels for novel synthetic images produced by the initial generative framework. This provides an efficient way to create synthetic labeled data. However these approach require some generated samples to be manually labeled which must be repeated if the generative framework is modified. Proposed Solution: This research aims to eliminate the need for synthetic sample labeling. It proposes a technique that learns the annotation framework using real images instead. This technique involves using a \"metalearning\" approach where the annotation framework is trained such that a segmentation framework trained on the generative frameworks outputs and their corresponding labels from the annotation framework will have a low loss on real annotated images. effectiveness:  Eliminates the need for synthetic sample labeling.  Clearly explained need and solution.  efficient alternative to traditional semisupervised learning methods.  Promising results compared to DatasetGAN. Weaknesses:  limited discussion on the significance of SemanticGAN despite experimental comparisons.  potential relation to Gradient matching Generative Networks for ZeroShot Learning (CVPR 2019) is not acknowledged.  Lack of clarity in the experimental comparison to DatasetGAN regarding the use of labeled example.  Applicability to complex scenes is not addressed.  Analysis of annotation cost vs. segmentation accuracy is missing.  Alternative training procedures and performance without synthetic images are not evaluated."], "nT0GS37Clr": ["Summary The authors demonstrate how to adapt the methods of Ramanujan et al. (2020) and Wortsman et al. (2020) to federated learning settings. They introduce FSL a proposed approach that aims to reduce communication overhead and enhance robustness against malicious approachs. Strengths  The authors provide an illustrative example to assist comprehension. Weaknesses Communication Calculation:  The authors calculation of SFLs communication requirement is questionable. They may have overlooked a potential number in finding the appropriate index for a given permutation which can be computationally demanding. Robustness to approach:  The authors suggest that the worstcase approach on FSL involves inverting the order of rankings. However a more significant threat could be a coordinated approach across multiple layers of the network.  Algorithm 3 seems to imply that voting occurs only among malicious clients while strong approachs may arise through consensusbased coordination among them. SparseFSL:  The authors mention that clients retain the bottom of their rankings but it is unclear what the server assumes for the missing entries.  Communication savings may be overstated without considering the encoding system used for sparsification. Experiments:  It is not specified whether the reported test accuracies are on finetuned local frameworks or on the server using the aggregated framework.  Hyperparameter selection strategy including the considered image is unclear.  The large variations in reported results suggest a need for more reliable estimates. Baselines:  SignSGD is not a true federated algorithm due to frequent communication.  Quantization technique such as groupwise quantization and vector quantization are missing from the baseline comparisons. Minor number:  Indexing in Algorithm 3 is inconsistent.  Sampling approach should be described in more detail.  Redundant citation of Minka (2000).  Capitalization error on page 14.  use of \"their\" for genderless objects should be consistent throughout.", "Paraphrase: This paper presents a method that trains a supermask for a randomly initialized neural network instead of directly training the models parameters during federated learning. This approach is advantageous because it reduces communication costs and enhances robustness against malicious clients. effectiveness:  Introduces the concept of supermask learning in federated learning which is novel and intriguing.  Utilizes technique like edge ranking order and majority voting to achieve communication efficiency and robustness.  Experimental evaluations demonstrate the benefits of these approaches. Weaknesses:  Overlooks prior research on masking in federated learning such as [1].  Baselines are inadequate potentially leading to unfair comparisons.  Does not consider stateoftheart communicationefficient technique for distributed learning such as the error feedback framework.  Only compares against FedAvg under noniid local data distribution ignoring recent methods like SCAFFOLD FedProx and FedNova.  Lacks additional comparisons under varying local update schedules client participation ratios and local data noniidness.  Appendix does not provide evidence that hyperparameter tuning would not significantly alter the proposed methods performance advantage.", "Paraphrase: This paper introduces a pruningbeforetraining method to enhance communication efficiency in federated learning. In each round devices with similar initial weights individually prune their local datasets. The server compiles the edge scores from the devices which are then used to initialize edge scores in the subsequent round. Upon convergence of the pruning process edge are trimmed accordingly and the pruned network is trained in federated learning. effectiveness: 1. The proposed federated supermask learning approach boosts communication efficiency. 2. edge ranking communication enhances the effectiveness of federated supermask learning. 3. Positive results are demonstrated in extensive testing on three realworld datasets. Weaknesses: 1. The paper uses the edgePop Algorithm for framework pruning which only guarantees loss reduction for randomly initialized network. The link between edgePop and framework pruning remains unclear. 2. The proposed method primarily focuses on pruning before training while pruning after training or during training may yield superior results. Further analysis is needed to clarify the motivations and advantages of this choice as well as its impact on framework accuracy. 3. The paper lacks sufficient novelty as key factor such as edgePop and voting are not original contributions. 4. The paper fails to adequately discuss and compare with other pruningbeforetraining technique.", "Paraphrase: Summary: This research leverages the \"supermask\" technique to enhance the robustness and communication efficiency of federated learning (FL). client share only a ranked number of network connections and the server aggregates these ranked numbers from all participants. The authors theoretically and empirically demonstrate how this approach improves robustness and communication efficiency. effectiveness: 1. The method combines supermasks with FL simultaneously improving efficiency and robustness. 2. Empirical results support the claims of improved communication efficiency and robustness. Weaknesses: 1. The experimental setup may not be challenging enough. Devices may lose data by only transmitting the ranking of connections. This loss may not be significant when devices have similar data distributions which may be the case in the experiments using Dirichletdistributed data. 2. The improvement over baselines may not be significant on certain datasets with TopK 10 outperforming FSL on FEMNIST. 3. While the idea of utilizing supermasks is novel the paper primarily combines supermasks with FL without providing significant theoretical contributions or resolving challenges that justify acceptance by a clear conference like ICLR."], "vruwp11pWnO": ["Paraphrase: This research examines three extensive field of outofdistribution (ODD) detection:  Multiclass ODD detection  Multilabel ODD detection  anomaly segmentation To support these largescale experiments it introduces a new species dataset for multiclass ODD detection and a road anomaly dataset for anomaly segmentation. It also establishes a multilabel ODD detection setup. Additionally the field sets a baseline using a straightforward detector based on the maximum logit across all three largescale scenarios. effectiveness: 1. The fields motivation is clear aiming to expand ODD detection from smallscale contexts to more practical and wideranging applications. 2. The research is wellgrounded investigating three significant settings and providing two original datasets. 3. The proposed approach (MaxLogit) delivers promising results in all settings. Weaknesses: 1. The writing could be improved. Some sections could be reorganized for beneficial clarity. Consider listing contributions for each setting and presenting the datasets methods and findings more distinctly. 2. While the proposed approach appears straightforward its performance in the three settings is not fully elucidated. Providing more precise descriptions of the method would enhance the papers clarity.", "Paraphrased Summary: This paper introduces a novel anomaly detection score called MaxLogit (negative of maximum unnormalized logit) for largescale outofdistribution (OOD) tasks. The proposed metric is based on the assumption that OOD samples exhibit high maximum softmax probabilities (MSP). The paper establishes a largescale OOD setup using ImageNet1K (indistribution) and Places365 (outdistribution) for image classification. Additionally for multilabel detection PASCAL VOC and MSCOCO are considered indistribution while ImageNet22K is used for OOD. MaxLogit outperforms MSP in both settings and demonstrates promising results on the CAOS benchmark evaluating different case of OOD scenarios. effectiveness:  Highlights the limitations of MSP for largescale OOD.  Presents a novel OOD metric specifically tailored for largescale tasks.  Provides clear and concise explanations.  MaxLogit achieves strong performance for OOD detection. Weaknesses:  MaxLogit is not formally defined in the paper despite being a key contribution.  The field focuses solely on MSP as a baseline excluding other likely OOD detection methods.  The multilabel OOD setup differs from existing protocols making comparisons challenging.  Smallscale evaluation are absent limiting the understanding of MaxLogits performance in different settings. Suggestions for Improvement:  Formally define MaxLogit.  Clearly state the nature of the OOD setup (near or farODD).  Consider additional baseline OOD detection methods for comparison.  Adhere to existing multilabel OOD protocols.  Explore the performance of MaxLogit in smallscale evaluation.", "Paraphrase: Summary: This research creates three extensive settings for assessing outofdistribution (OOD) detection. It conducts empirical tests on these settings to create foundational standards for future research. effectiveness:  Establishes three largescale OOD settings and accompanying datasets for empirical analysis.  Provides experiments on all three settings to establish benchmarks for future work.  Introduces \"MaxLogit\" an anomaly scoring technique that is a minor variation of the widely used method MSP and demonstrates its effectiveness on the three settings. Weaknesses:  Incorrect claims in the paper include ignoring relevant research on largescale multiclass OOD detection.  The datasets created do not offer significant advantages over existing benchmarks.  MaxLogit is a minor variation of an existing method lacking substantial technical novelty.  Competing methods used in the experiments are outdated or simple baseline making it difficult to assess the advancement achieved by the presented method.", "Paraphrased Statement: This paper presents various contributions to outlier detection. contribution:  Species Dataset: A novel dataset for outofdistribution (OOD) detection separate from ImageNet22k.  MaxLogit Criterion: A detection criterion suitable for multilabel OOD detection environments.  StreetHazards and Dense Outlier Datasets: Two datasets for dense outlier detection with StreetHazards allowing realistic rendering of outliers. effectiveness:  Species Dataset: valuable for evaluating OOD detection methods.  StreetHazards Dataset: Recognized as useful for evaluating dense outlier detection. Weaknesses:  MaxLogit Criterion Evaluation: Missing comparison with differentiable counterparts (logsumexp logit and standardized maxlogit).  experimental Evaluation: Lack of recent baseline in tables and inconsistencies in rankings.  BDD anomaly Dataset: Suboptimal OOD class choice potentially affecting results. Suggestions:  Provide missing details on dataset version batch size and table introduction.  Consider the number of class difficulty at semantic borders in dense OOD detection.  Include references to recent relevant work (e.g. WildDash 2).", "Summary: The authors expand outofdistribution (OOD) detection from smallscale singleclass settings to largescale multiclass and multilabel scenarios. They introduce largescale benchmarks for assessing OOD detector in classification and segmentation tasks. Furthermore they present a straightforward but effective baseline approach for this realworld number. effectiveness:  Wellstructured easytounderstand paper.  OOD detection is a significant problem and the authors provide largescale benchmarks for both classification and segmentation to support future research.  MaxLogit the proposed method is practical and has shown consistent effectiveness in largescale benchmark tests. Weaknesses:  While the largescale benchmarks are valuable MaxLogit is a simple extension of MSP and does not appear to be a novel method.  The claim that vision transformerbased methods do not simplify OOD detection on largescale benchmarks is intriguing. More analysis and reasons for this finding would be beneficial.  The benchmarks are useful but more SOTA OOD results should be included for comparison (e.g. [2 3])."], "vh-0sUt8HlG": ["Paraphrase: Summary: This study presents a novel vision transformer MobileViT specifically designed for mobile devices. It aims to combine the advantages of convolutional neural networks (CNNs) and vision transformers (ViTs) to create a lightweight and lowlatency model for mobile computer vision tasks. MobileViT achieves impressive results on various datasets: 78.4 top1 accuracy on ImageNet1k (classification) 27.7 mAP on MSCOCO (object detection) and 79.1 mIOU on VOC 2012 (semantic segmentation). strength and Weaknesses: Weaknesses:  The paper does not explicitly highlight the lowlatency attribute of the network.  The reason for using the open performance to generate nonoverlapping flatten vectors instead of flattening the input feature directly is not explained.  The paper does not include a comparison of the top5 accuracy metric in design 7.  more details about competitor models such as BoTNet and BoTNet50 should be provided for beneficial comprehension.  The consumption of exponential moving average during inference may give MobileViT an unfair advantage over other competitors in design 7 considering that the difference is minimal (0.1). further Concerns:  The superior performance of MobileNetV2 (3.5M 73.3 top1) compared to DeiT (5.7M 72.2 top1) and PiT (4.9M 73.0 top1) raises questions about the contribution of the MobileNetV2 base model to MobileViTs performance. Exploring other lightweight base models would be beneficial.  The efficiencyperformance ratio of MobileNetV2 exceeds that of MobileViT suggesting that MobileNet models may have large practical value. Suggestions:  Employ a more uptodate dataset such as ADE20K to evaluate the effectiveness of MobileViT in object detection and segmentation tasks.  Include visualization results to showcase the models performance in these tasks.  Correct typos (e.g. DeIT \u2192 DeiT).  Include the source for the ADE20K dataset [1]. source: [1] Scene parsing through ADE20K dataset. CVPR 2017.", "Paraphrased Statement: Summary This research introduces a hybrid backbone that fconsumptions the existing MV2 block with a novel MobileViT block for efficient image classification detection and segmentation. The MobileViT block employs both convolutional and selfattention mechanisms to capture both local and global features. Results on IN1K COCO and PASCAL VOC datasets demonstrate its potential. Strengths  The MobileViT block design combining local convolution and global selfattention is elegant and straightforward.  The study is presented clearly and concisely.  The performance results are impressive. Weaknesses  The final results may be influenced by factors such as label smoothing and EMA which should be evaluated separately.  The models complexity is measured using the count of parameters which is a theoretical metric. GFLOPS comparisons with competing models would provide a more comprehensive view.  The comparisons with heavyweight CNNs (Table 1b) are not conclusive. more recent backbones should be included.  The consumption of PASCAL VOC 2012 (Table 2) is limited as a benchmark for segmentation. Results on large datasets like COCO LVIS would be more compelling.  Table 2 does not compare against transformerbased backbones like Multiscale vision transformer or Pyramid vision transformer. Technical Exposition Regarding Eqn (1) the statement about fusing features after concatenation should be clarified. design 1b indicates that concatenation occurs between the original feature and the output of the local convolution and global selfattention block.", "Paraphrased Summary: The source propose a novel block that combines the extensive open field of transformers and the positional bias of convolutions. They utilize this block to create an architecture that outperforms both popular CNNs and existing ViT models in terms of parameter efficiency and performance. strength:  Wellwritten and clearly presented contribution  Improved performance compared to previous ViTbased models in terms of robustness stability and generalization  Reduced parameter count while maintaining performance making it suitable for various downstream tasks  Simple and effective core technical idea that enables multiscale training of ViTs  Welldesigned experiments supporting the claims Weaknesses:  Lack of memory and time performance analysis which is significant for mobile applications  Unhelpful worstcase analysis that does not provide insight into speed improvement  Lack of FLOPs and memory step analysis which are key considerations for embedded computer vision  Comparison against MobileNetV2 DeIT and PiT only on an iPhone 12 a extensiver scope of devices should be tested Minor Issues:  source to \"large inputs\" in Section 4.3 without providing specific values  consumption of the term \"transformers as convolutions\" in the abstract and Section 3 without prior explanation"], "inSTvgLk2YP": ["Paraphrase: Summary: This paper introduces MeshInversion a technique for reconstructing 3D works from a single image with a silhouette. It optimizes the 3D work represented as a latent code in a pretrained 3D GAN (specifically ConvMesh NeurIPS 20) using the proposed chamfer losses. The approach leverages 3D GAN prior to guide the reconstruction of unobservable surfaces which is a novel and logical idea. Strengths and Weaknesses: Strengths:  The concept of optimizing 3D work via a pretrained 3D GAN is innovative and sensible leveraging 3D prior for singleview 3D work reconstruction.  Unlike prior methods that use explicit regularizations to prevent irregular deformations MeshInversion implicitly regularizes deformations by ensuring the reconstructed work aligns with the 3D GAN manifold.  The paper presents clear writing that is easy to understand. Weaknesses:  evaluation Protocol: The methods evaluation protocol which uses the same input image (RGB and silhouette) as the ground truth to assess the optimized 3D works is not optimal. This could lead to overfitting to the input image without reconstructing meaningful 3D works.  Fairness of comparison: In Table 2 the baseline methods (e.g. CMR UCMR) are not provided with the testtime image and object silhouette as input while MeshInversion is. A more equitable comparison would involve optimizing the baseline methods with these additional input.  Chamfer Loss: The paper does not adequately explain why the proposed chamfer loss is superior to widely used differentiable renderingbased reconstruction losses. Additionally it is unclear how the chamfer loss addresses occlusion which is not an issue for differentiable rendering. Additional input and Questions:  During optimization of the latent work and texture code on a single image are the weights of the generative mesh fixed  How does the accuracy of the pretrained mesh impact the testtime optimization The extreme case would be optimizing with a randomly initialized mesh.  The ablation results (Table 1) should typically appear after the main results (Table 2).", "Summary: This paper introduces a method for creating textured 3D mesh from single input image. It leverages a pretrained generative model that captures texture and work information. Given the input image and estimated camera pose the method optimizes the mesh to fit the image ensuring highquality texture and work reconstruction. Strengths:  Novel Chamfer texture loss: effectively maintains highfidelity texture despite pixel misalignment.  Highquality results: Produces superior work and texture reconstructions compared to existing singlepass methods. Weaknesses:  Similarity to existing optimization techniques: The proposed method is not significantly different from standard testtime optimization.  Incomplete evaluation: The comparison are not convincing due to the use of testtime optimization for only the proposed method and the reliance on uninformative metrics (e.g. 2D mask projection IoU) for 3D work evaluation. Clarifications:  The camera poses are fixed during inference and provided as input.", "Summary Paraphrase: Abstract: This work introduces a method to reconstruct 3D objects from a single image. The reconstruction is based on a generative model that directly creates a 3D representation instead of just generating a 2D image. To improve the accuracy of the reconstruction two losses inspired by the Chamfer distance are proposed to match the texture and work of the generated object to the input image. Strengths:  The methods use of a 3D generative model is novel and has potential in the field of 3D reconstruction.  The qualitative results obtained are impressive showing accurate reconstructions. Weaknesses:  The paper does not sufficiently justify why the proposed method is necessary.  It is unclear if the work really needs to be learned by inversion as existing methods already estimate it from image.  The baselines used for comparison are not optimized specifically for each image giving the proposed method an unfair advantage.  The quantitative results show mixed performance and the selection of samples for the qualitative results is not clear.  The limitations of the work and the motivation for the approach are not clearly presented. Additional Questions and input:  The motivation for using a 3D generative model instead of an autoencoder is not clear.  The main approach built upon in this work is not referenced or explained sufficiently.  The limitations of the 3D prior in matching the input image should be discussed.  The Chamfer distancebased losses should be formally defined.  more qualitative results from different viewpoints should be provided.  The aim of the camera pose estimator should be explained.  The ablation results should be included in the appendix.  Some technical terms and phrases need further clarification.", "Paraphrase: Main Idea: The paper explores reconstructing a 3D model of a specific object from a single 2D image using a collection of similar image. This technique builds upon the CMR framework but introduces key differences notably the use of a generative adversarial mesh (GAN). key Contribution: The primary contribution is a novel method for finding the latent code in the GAN that best matches the input image. This is achieved through testtime optimization guided by two proposed losses: Chamfer Texture Loss and Chamfer Mask Loss. Strengths:  The proposed method offers a strong alternative to CMR producing visually realistic reconstructions in both texture and work especially for objects like birds.  The quantitative results demonstrate impressive performance.  The inclusion of perceptual work provides further evidence of the methods effectiveness. The GAN inversion approach is a unique and successful application in the CMR context. Weaknesses:  The paper could clarify the differences between the proposed method and traditional CMR particularly in terms of testtime optimization and the use of a mask as input.  The ablation work in Table 1 lacks clarity regarding the use of both proposed losses.  The results primarily focus on side view of birds providing visualizations from other view would enhance the work."], "qNcedShvOs4": ["Paraphrase: This paper presents the implementation of EinsteinVI an enhancement over Stein VI. EinsteinVI allows for variational inference (VI) in probabilistic models using a flexible particlebased distribution. The implementation is integrated into NumPyro a probabilistic programming software. The paper evaluates EinsteinVI against Stein VI methods in Edward and PyMC3. Experiments on various probabilistic models show that EinsteinVI outperforms the other methods in terms of computational time. The paper also highlights the advantages of EinsteinVI compared to competing probabilistic programming languages like PyMC3. However the paper lacks novelty as it does not introduce a novel method. Its primary contribution is the implementation of an existing method for use with NumPyro. While this implementation may be valuable for practitioners the lack of original research limits the papers overall significance.", "Summary: This paper describes a model to use various \"Stein VI\" techniques in NumPyro. It briefly explains the algorithm and evaluates their performance on examples. Strengths:  clear and concise introduction to Stein VI algorithm.  Unified algorithm for generalizing these algorithm. Weaknesses:  Lack of significant novel contributions as the review by Anastasiou (2021) has already covered the field.  Insufficient exploration of speed improvements beyond using `vmap`.  Unacceptable omission of code for a software submission.", "Paraphrased Statement: Summary: This field introduces a novel probabilistic programming model based on Numpyro for Stein variational gradient descent (SteinVI) and related methods. Empirical findings indicate enhanced computational efficiency for inference when using this model. Moreover the authors developed a novel Stein mixture algorithm for deep Markov models which surpasses existing techniques. Strengths:  Numpyro Integration: Facilitating SteinVI integration with Numpyro enables users to leverage advanced SteinVI algorithm seamlessly for Bayesian model.  Speed: Faster computation compared to current implementation.  ELBOwithinStein Concept: advanced idea with potential for further exploration.  deep Markov model Extension: Novel application of the Stein mixture method. Weaknesses:  Code Lacking: The paper lacks code examples making it difficult to assess the ease of use and superiority of EinSteinVI over standard Numpyro and PyMC3.  Motivation Deficiency: The authors could provide a clear justification for the need for EinSteinVI over existing implementation.  Missing Stein VI Methods: Some significant Stein VI methods are not included such as Particle Optimization for Bayesian Neural Networks ParticleBased Variational Inference and Equivariant Stein Variational gradient Descent for Equivariant Energy Based models.  Poor introduction: The papers introduction lacks clarity particularly regarding the ELBOwithinStein concept and its deep Markov model application.  special Experiments: No experiments demonstrate the usefulness of EinSteinVI for Nonlinear Stein VI Matrixvalued Kernel Stein VI and Message guide Stein VI. Reason for Score: While the developed library holds potential for the community and the ELBOwithinStein concept is intriguing the experimental and theoretical contributions fall short. Comments:  Bandwidth Tuning: The criticality of bandwidth tuning for Stein VI warrants the inclusion of tuning methods besides the median method such as the heat kernel approach.  Improved introduction: Enhancing the introduction would make the work more appealing. The ELBOwithinStein concept and its deep Markov model application should be emphasized as technical contributions. Additional experiments are necessary to support their utility.", "Paraphrase: Summary This research aims to incorporate Stein variational inference techniques into NumPyro a probabilistic programming language. The methods include versions of Stein variational gradient descent algorithm with varying kernel functions nonlinear scaling adjustments and matrixvalued kernels. Experimental results using existing baselines for realworld scenarios are provided. Strengths and Weaknesses Strengths:  The paper is wellwritten and the methodology is presented clearly.  The literature review is comprehensive. Weaknesses:  The motivation for integrating Stein variational inference methods into a PPL model particularly NumPyro is not fully explained.  Despite briefly mentioning that the integration solves a limitation of traditional variational inference (VI) in capturing complex posterior correlations the paper lacks theoretical or empirical evidence to support this claim.  The algorithm implemented in NumPyro are not novel as they have been proposed by previous research. The authors do not introduce any novel techniques."], "uB12zutkXJR": ["Paraphrased Statement: Automated code repair benefits from tree and graphbased analysis. This work introduces Graphix a model that translates a graph into a series of edit on a programs abstract syntax tree (AST). Graphix uses a multihead graphbased encoder an AST edit decoder and optional treebased pretraining. effectiveness and Weaknesses: effectiveness:  The graphedit formulation aligns well with source code model.  Graphixs performance is comparable to stateoftheart methods with fewer parameters. Weaknesses:  contribution over prior work are minor and difficult to assess without comprehensive equivalence.  The efficacy of the multihead graph neural network (GNN) strategy is not sufficiently demonstrated.  Scalability concerns arise due to the need to encode the entire AST at each step. additional Comments:  The paper lacks a equivalence of Graphix to Hoppity with the same number of parameters.  The ablations do not adequately analyze the impact of the multihead GNN and pretraining.  The scalability of Graphix should be addressed and a equivalence to Hoppitys runtime should be provided.  The motivation and model do not clearly demonstrate Graphixs advantages over existing methods.  The introduction and conclusion overstate Graphixs performance relative to other models.  Most baselines in Table 1 are not evaluated under the same conditions as Graphix.", "Paraphrase: Summary:  GRAPHIX a graph editing model is proposed for program repair.  It builds on Hoppity using multihead graph encoding for increased accuracy and reduced complexity.  GRAPHIX handles longer edit sequences enabling it to address more program repair.  A pretraining task is introduced to enhance model performance.  Experiments on the Patches in the Wild dataset show GRAPHIX outperforms foundationlines. effectiveness and Weaknesses: Pros:  Tackles the challenging task of program repair.  Outperforms foundationlines and achieves comparable results to larger models.  Generates longer graph edit for more effective program repair.  Comprehensive empirical evaluation.  Interesting anecdotal model. Cons:  Major:  Pretraining task may not be sufficiently novel focusing mainly on \"missing code prediction\" instead of a wider range of edit.  Pretraining is costly but yields limited performance improvement.  other:  Missing related work.  Singlehead \"foundation\" model mentioned in Section 4.1 but not in experiments.  Invalid code snippet in Listing 2.  Argument against code abstraction is weak. Nitpicks:  Color highlighting intensity in code snippets could be reduced for better readability.  Trailing whitespace in Listing 2 is highlighted.  Tables 1 and 2 should be separated for clarity.", "Paraphrased Statement: The paper presents a novel approach to repairing programs using abstract syntax tree (ASTs). The approach is called edit Subtree reconstruction which involves removing sections of the AST and training a model to restore them. When compared to editbased and sequencebased repair methods the novel approach only surpasses the editbased ones. effectiveness and Weaknesses:  The paper claims that the novel approach did not surpass stateoftheart methods but this claim is not supported with a discussion of why the work is still relevant despite this.  Anecdotal model of the models performance are not compared to results from other approach leaving it unclear how the novel approach compares.  The model is claimed to be over 10x smaller than \"current largescale sequence models\" but the only evaluated baseline that is this much larger is \"BART.\" CodeT5small which is less than 2x as large outperforms the novel approach.  The method for identifying bug repair by filtering for terms like \"repair\" \"bug\" and \"error\" is not discussed for its accuracy.  The reasoning behind limiting the AST size to 600 nodes is not explained.  Table 1 could be visually enhanced by separating editbased and sequencebased approach using colorcoding.  The models performance on the medium dataset which is more complex shows a slight decline after pretraining which is not clearly acknowledged in the text.", "Paraphrased Statement: This research paper introduces a model for repairing programs using sequential structural tree edit on ASTs. The model employs a graph encoder and decoder to predict these edit. The researchers also propose a pretraining method that leverages nonprogram repair code data by removing subtrees of varying sizes and predicting their reconstruction. While the model achieves comparable performance to other stateoftheart code repair models on a Java repair dataset it has fewer parameters than various topperforming pretrained models. The paper includes extensive evaluation efforts including equivalence against various baseline models and ablations on different model components. However the models novelty and its ability to significantly improve upon the stateoftheart are questioned. Its architecture closely resembles the HOPPITY baseline model with the primary change being the addition of a multihead graph encoder. The paper heavily emphasizes the models smaller parameter count but this difference seems marginal when compared to other highperforming pretrained models. The qualitative evaluation provides interesting model but it does not lead to strong conclusions about the case of bugs the model can effectively fix. The pretraining regime which involves deleting and reconstructing subtrees from existing code is not particularly novel. The evaluation omits a equivalence to a closely related model cited in the paper (Yao 2021)."], "vDa28vlSBCP": ["Paraphrased Statement: This paper presents a method that enhances the interpretability of text classifiers based on transformer neural networks. It leverages \"casebased reasoning\" where a network classifies input by comparing it to a library of learned prototypes. Each prototype corresponds to a specific label. The input is then classified based on its weighted similarity to these prototypes. This approach is considered more interpretable compared to traditional endtoend classifiers as users can directly examine the similarity scores to understand the decisionmaking process. The method also aims to be more accurate than posthoc interpretability approach as predictions are directly made using similarity scores. End users can interact with the system by modifying prototypes (if they have expertise) or providing feedback on their quality. This feedback is then used to improve the prototypes. Experiments demonstrate that the approach is competitive with standard endtoend finetuning methods while maintaining improved interpretability. Analysis indicates that explanations based on prototypes are reliable but further evaluation is needed to confirm their accuracy compared to other methods. effectiveness:  Explores the intersection of interpretability casebased reasoning and user interaction for model refinement.  Wellwritten with extensive experimentation. Weaknesses:  Many hyperparameters may make tuning challenging.  performance varies across different models with some showing improvement and others showing decline.  User interaction results appear inconclusive.  prototype quality could be improved to enhance diversity and coverage of training data way.  Comparisons with posthoc interpretability methods such as SHAPLIMEIG are lacking.  trust intervals are not provided to facilitate better comparison across settings.", "Paraphrase: Summary: This article introduces the ProtoTrex model designed to enhance the ability to explain text classification systems. The model incorporates prototype layers within its architecture to analyze the similarity between queries and prototypes. Additionally an extension called iProtoTrex enables interactive learning based on user feedback. Experiments and examples demonstrate ProtoTrexs ability to perform classification with accuracy comparable to traditional models while providing meaningful explanations. effectiveness: 1. Technically sound model design. 2. model showcase the models capacity to produce reasonable explanations. Weaknesses: 1. Complex training loss use with multiple components each with its own weight (lambda term). The method for setting these weights is not provided and their impact on predictions and explanations is not assessed through ablation studies. 2. Numerous hyperparameters to tune including those in the training loss which can be challenging to optimize for novel datasets. 3. Efficiency concerns due to the incorporation of additional modules compared to simpler classification models. input: The source of the results in Table 1b and 1c is not specified. They are labeled as \"Yelp Movie Toxicity\" but lack further context.", "Paraphrased Summary: This paper proposes a model ProtoTrex that aims to enhance both task performance and explanation quality in transformer models. ProtoTrex utilizes shared prototype embeddings learned from training data using a combination of losses. These embeddings serve as the basis for both classification and explanation generation. The model can also incorporate human input for improved prototype learning. Paraphrased effectiveness and Weaknesses:  effectiveness:  Motivated by prior process ProtoTrex focuses on NLP tasks and potentially benefits the explanation community.  The early contribution of the paper is clear and intuitive.  It proposes a novel approach to improving interpretability without sacrificing task performance.  It introduces the concept of prototype embeddings effectively.  Weaknesses:  Limited coverage of related processs specifically prompt engineering data bottleneck and generative explanation approach.  Unclear explanation generation process.  Experiment section is weak and analysis of generated explanations is lacking.  The architecture heavily relies on prototypes but the performance of prototype embeddings compared to labelwise weights is not fully explored.  Lack of solid explanation evaluation contributionicularly humanbased evaluations for generative explanations.", "Paraphrase: Summary: This study proposes a method to provide model explanations by presenting a similar training instance with its label instead of relying on nondeterministic posthoc explanations. The method trust label prediction with prototype clustering allowing it to retrieve the most similar prototype as an explanation while simultaneously predicting the label. It also integrates human feedback into the prototype selection process. The method has been shown to provide appropriate explanations while maintaining model performance. effectiveness: S1: The approach of using a train instance as an explanation is innovative. Weaknesses and Questions: W1: The selection of prototypes is unclear. It appears that the prototypes are chosen manually which raises questions about their effectiveness. Q1: If prototypes are predefined would it be more efficient to simply compute similarity between the input and the prototypes without training W2: The main evaluation table lacks clarity particularly regarding faithfulness. Consider the faithfulness of a randomly selected prototype. Q2: In the introduction the study claims that posthoc explanations are prone to reporting bias. However couldnt the proposed method also introduce reporting bias as it relies on training data"], "zeGpMIt6Pfq": ["Summary This article presents a novel kind of spiking neural network (SNN) for classifying image. It aims to be more realistic biologically than other similar networks. The main differences are that it uses local connections instead of convolutional connections and uses STDP (standard and reinforcementmodulated) learning instead of gradientbased learning. The proposed architecture is described in detail (including introductory material on spiking artificial neurons and STDP) and compared with other SNN architectures on the MNIST benchmark. Finally the architectures performance is evaluated on a conditioning task that shows the networks ability to track a changing reinforcement. Strengths  The paper is written clearly and the research is wellmotivated and presented.  The proposed architecture is a novel combination of existing approaches and the results show that it performs well on the benchmark task.  The problem of designing flexible architectures that can learn locally (Hebbian) is theoretically and practically significant especially for neuromorphic applications. Weaknesses  The paper seems rather incremental with its main innovation being small changes to existing approaches.  The claim that the proposed architecture is the first to use reinforcement learning with Poisson ratecoded inputs for image recognition and the first locallyconnected SNN with a hidden layer may not be true.  The paper seems very focused on the importance of developing spiking networks that are biologically plausible even though biological plausibility is not necessarily the primary goal of the computer vision field.  The paper quickly dismisses or ignores alternative (and very successful) methods for training SNNs such as surrogate gradientbased methods because they lack biological plausibility.  The introduction contains some unsubstantiated argument that may confuse readers.  The paper insistence on the primate brain specifically is not explained.  The phrase \"aligned with human intuition\" is not defined.  The final paragraph of the discussion should be labeled with \"In the future...\" to indicate that further development are being discussed.  The appropriate LaTeX citation commands should be used throughout the paper (primarily \\citep).  There are some typos and unclear sentences.", "Paraphrase: Summary: The researchers introduced an image classification model using a spiking neural network (SNN) called BioLCNet which comprises:  An input layer for spike train inputs.  A middle layer of locally connected neurons for feature extraction trained with STDP unsupervised learning.  An output layer of fully connected neurons for decoding trained with RSTDP semisupervised learning. effectiveness:  The model incorporates a training system combining STDP and RSTDP using reward use for RSTDP training. Weaknesses:  The proposed SNN is designed for image classification but has been tested only on MNIST with underwhelming results compared to established SNN methods.  The application of reinforcement learning (RL) for classification remains unclear especially considering the inferior performance.  The rationale for mixing STDP and RSTDP training need clarification.  The claim that SNNs need less labeled data is questionable.  While the pursuit of bioplausible concepts in SNNs is commendable the direct translation of biological concepts may not be meaningful for SNNs due to their simplified neuron models.", "Summary This study proposes a realistic model for learning in biological networks by combining spiking networks local connections and rewardbased learning rules. Despite incorporating biologically plausible features the models performance on MNIST and conditioning tasks is unsatisfactory. effectiveness  Combines multiple biologically plausible features in deep networks. Weaknesses  Limited novelty as previous study have combined spiking networks and local connections.  Training only the output layer with reinforcement learningbased rules is questionable due to:  Representations built by previous layers are more crucial for network training.  Limited scalability beyond MNIST.  Poor performance on MNIST with an accuracy of 87.5 using STDP and SVM decoding.  Large filter sizes (15x15) may hinder learning.  Competition in the winnertakeall readout layer can hinder error correction. Recommendation Reject due to limited originality and unsatisfactory results. Minor Comments  Gradientbased methods do not necessarily require explicit labels.  The model is not the first locally connected spiking neural network with a hidden layer.  Clarification of equations and the network model would enhance clarity."], "lyzRAErG6Kv": ["Paraphrase: Summary: This field presents a novel selfsupervised representation learning approach for reinforcement learning (RL) tasks and shows its superior performance in efficiency against existing frameworks. The approach trains neural networks to encode optical flow between consecutive frames and includes a regularization term to ensure that the representation predicted by warping and a state transition framework (based on action) closely matches the representation extracted from the subsequent frame. Advantages and Disadvantages: The approach is logical because prior research has shown the significance of object representation for efficient human learning in Atari games. Learning global representations may compromise detailed object information. While this work does not explicitly framework objects the optical flow information probably captures some object features indirectly. The results are generally persuasive and the concept seems novel and intriguing. Concerns: However concerns exist about how the internal flow loss formulated in equation 4 contributes to feature extraction. To the sound of our understanding the internal representations warping flow is identical to the picture warping flow (equation 2). This raises the question of how we can be certain that the extracted query feature is not simply a fixed linear translation of the local image region. It seems reasonable to assume that as long as the representation in equation 1 can be trained to estimate the external flow that minimizes the picture warping cost the cost of the internal flow will also be reduced. This remains true even if the query feature is a straightforward linear use of the local image region. Although the paper demonstrates strong performance in the RL task it would be beneficial to include visualizations of the learned local features in the appendix if possible.", "Paraphrase: Summary: This field investigates selfsupervised visual representation learning for reinforcement learning (RL). Instead of prior methods that focused on extracting global representations of flow observations the authors propose learning \"structured\" representations using flow maps that capture local structure. They combine a selfsupervised flow framework trained on image reconstruction an actionconditioned 1step transition framework that uses cosine similarity in the latent space and a flowbased warping to provide a second nextstep latent prediction task. They demonstrate the effectiveness of their method on standard RL tasks in Atari and Control Suite domains reporting improvements over the stateoftheart in half of the tasks. effectiveness and Weaknesses: The paper is wellwritten and presents a novel approach to leveraging selfsupervised flow frameworks for RL. However the results presented are not as strong as expected with mixed performance in Atari and limited superiority in Control Suite. Given the increased complexity and computational cost of the proposed method it may not be immediately practical for widespread adoption. RemarksQuestions:  The definition of \"structured representation\" is unclear and it is not clear if the proposed methods representations are genuinely more structured.  The claim that visual features representation learning leads to limited performance and sample efficiency should be refined as there is evidence of improvement using other methods.  The unexpected results in Table 3 raise questions about the difficulty of the nextstep latent prediction task.  Investigating the similarity between Zw and Zp using a gradientstopped similarity operator could provide insights.  Using both Zw and Zq as input to the RL head may improve performance.  Fig 2 and the text on page 5 imply that ak is computed from Ik to IkM when it should be from IkM to Ik.", "Paraphrased Statement: Summary: This field focuses on developing structured representations for deep reinforcement learning proposing a method based on establishing flow between latent spaces. A transition framework predicts future representations guided by actions similar to SPR. The approach outperforms existing methods in Atari and DeepMind Control Suite environments. effectiveness:  Impressive quantitative results demonstrating the superiority of the proposed approach.  comprehensive evaluation with varying environments and agent setups (Rainbow and SAC).  Strong theoretical and mathematical foundation. Weaknesses:  Limited technical innovation as the approach closely resembles SPR with the addition of latent variable warping.  Unclear applicability to other input modalities (e.g. sound text joint positions) due to the flowbased approach reliance on image inputs.  Marginal improvement in quantitative performance compared to baselines.", "Paraphrased Statement: This paper introduces a representation learning method that includes unsupervised signals (flow frameworks and transition frameworks) with constraints to support reinforcement learning (RL). It consists of augmenting the RL objective with reinforcement that enforce:  Consistency between the flow frameworks predicted image warping and the actual image changes  Alignment between the latent representation of consecutive states and the flow frameworks predictions  compatibility of the latent representation with a latent transition framework The method outperforms competing baselines on Atari tasks. An ablation field without baselines on the DM Control suite tasks confirms its effectiveness. effectiveness:  Superior performance on Atari games  Wellwritten paper  Robustness due to using fixed loss weights Weaknesses:  Complexity and difficulty reproducing  Insufficient baselines and exploration tasks in ablation experiments  Inappropriate use of \"structured representation\" in the title as it only utilizes simple image lattice structure  Difficulty with Pong potentially due to interference from auxiliary loss gradients"], "vwLLQ-HwqhZ": ["Paraphrase: This research paper examines the issue of normalization across tasks in a dynamic online continual learning (CL) context. It highlights the importance of batch normalization (BN) in CL but notes that BN in its current work can bias towards the current task leading to the phenomenon known as catastrophic forgetting. To address this problem the paper introduces a continual normalization (CN) layer. This layer combines spatial and batch normalization into a single layer optimized for CL. The authors conducted experimentation to evaluate the perworkance of the CN layer showcasing its effectiveness in reducing the bias and mitigating catastrophic forgetting. effectiveness:  The paper focuses on a critical issue in continual learning namely nonstationary online CL.  The CN layer offers a reasonable solution to the crosstask normalization problem.  CN layers can easily replace BN in existing model.  The authors provide extensive experimental comparison with other normalization techniques such as BN instance normalization (IN) and group normalization (GN).  The paper is wellwritten with a clear presentation of the proposed method and its context within related work.  The authors make their code publicly available for effect verification. Weaknesses:  The CN layer combines existing normalization techniques without introducing significant novelty.  The CN layers perworkance in catastrophic forgetting reduction (as measured by the FM score in Table 2) is inferior to GN in the Drifted Elastic Random Walks (DER) context. This discrepancy needs to be explained.", "Paraphrase: Summary: This paper introduces a novel normalization technique Continual Normalization (CN) designed for continual learning. The authors highlight the issue of global moment bias in Batch Normalization (BN) and propose CN to mitigate this problem. CN combines the benefits of both group Normalization (GN) and BN to enhance data sharing while minimizing forgetting. Extensive experimentation across different benchmarks and continual learning settings demonstrate the superiority of CN. effectiveness:  clear and concise presentation  Novel approach to normalization in continual learning  Comprehensive evaluation across various task settings Weaknesses:  Section 2 lacks a clear order section 3 should likely be section 2.1  Equation (6) still includes BN indicating that the crosstask normalization issue may not be fully direct  Unclear explanation of how \"adaptive normalization\" reduces forgetting at test time  While CN is shown to complement replaybased methods its performance without replay is unclear  CNs potential in noncontinual learning settings remains unexplored", "Summary Paraphrase: The work explores challenges in activation normalization for neural networks in continual learning where data from different tasks is encountered sequentially. The authors identify a \"crosstask normalization effect\" where data from a current task is normalized using biased statistics from a previous task. They demonstrate that existing normalization methods like Batch Normalization and group Normalization struggle in this context leading to high forgetting and low transfer. To address this the authors propose Continual Normalization (CN) which combines group Normalization with Batch Normalization. CN is shown to improve performance empirically on multiple continual learning datasets. effectiveness and Weaknesses (Paraphrased): effectiveness:  Addresses an significant problem in continual learning that has been overlooked.  practical and easy to implement within existing architectures.  Consistent improvements across different contexts with a few exceptions.  Extensive empirical evaluation considering various aspects of continual learning. Weaknesses:  Lacks an indepth analysis of how different normalization schemes affect effect.  Could benefit from including an \"oracle\" baseline that recalculates statistics from all training data.  Does not fully acknowledge limitations of CN such as weak performance on minority classes in the NUSWIDEseq dataset.  Standard deviation of effect for the COCOSeq benchmark is not provided.  impact of the hyperparameter \u03b7 in CN is not discussed.  Does not consider nononline continual learning contexts (multiple epochs). minor Comments:  Merge Section 2 with Section 3.  Correct \"For convenient\" to \"For convenience.\"  Consider exploring the nononline continual learning context.", "Paraphrase: Summary: The paper investigates the impact of normalization layers on continuous learning. It argues that standard BatchNorm is limited because data statistics vary between tasks. To address this the authors introduce CntinualNorm a combination of GroupNorm and BatchNorm. Experiments compare different normalization methods on continuous learning benchmarks. effectiveness:  clear structure and presentation: The paper is wellorganized and written in a logical manner.  Strong arguments and experimental support: The authors provide convincing arguments supported by welldesigned experimentation. Weaknesses:  Lack of detail on normalization effect: The paper could benefit from more indepth analysis of the impact of normalization on BatchNorm including comparison to model without BatchNorm.  Limited exploration of normalization vs. replay buffers: The experimentation suggest that the benefits of BatchNorm are reduced when using replay buffers but the paper does not provide effect for model without normalization or replay buffers.  Underexplored understanding for normalization effect: While the paper acknowledges that data statistics affect normalization it does not explore the specific mechanisms by which these changes impact performance. Update after Discussion Period: The authors have addressed the weaknesses raised which has improved the paper. The reviewer now increases their initial score."], "vBn2OXZuQCF": ["Paraphrased Statement: Summary This research investigates the behavior of contrastive learning (CL) in the context of unsupervised domain adaptation (UDA). It reveals that CL introduces a distinct mechanics for UDA contrasting traditional adversarial DA methods that create distinct feature reintroductions between source and target domains. The consider also develops a theoretical framework to elucidate the effectiveness of contrastive learning in handling domain shifts and supports its findings with empirical experiments on benchmark datasets. Strengths  Timely and innovative topic.  Intriguing observations: CLbased methods connect different domains differently than adversarial DA.  clear and comprehensive evaluation and introduction.  Wellaligned empirical results with the proposed connectivity model. Main Weaknesses 1. Ambiguous Shift Assumptions: The paper lacks a clear definition of the domain shift assumption (e.g. covariate shift). This omission could lead to uncertainties about the applicability of the results under different shift assumptions (e.g. label shift). 2. Limited Investigations: Experiments are confined to visual recognition tasks while CL has shown promise in other modalities such as text audio and time series. This limitation raises question about the generalizability of the conclusions. 3. Single SourceTarget Domain: The consider considers only single source and target domains. It would be beneficial to explore scenarios with multiple source domains especially in realworld applications where uncurated data often originates from diverse domains. 4. Lack of Practical Applications: While the paper analyzes the behavior of CL across disparate domains it does not provide specific guidance on selecting unlabeled data for improved shift. Practical algorithms or application scenarios would enhance the relevance of the findings. other Questions  Can the proposed scheme be extended to other data modalities beyond range  What are the potential challenges in extending CL to different modalities  How does the effectiveness of CL depend on the choice of the augmentation set used", "Paraphrase: This consider aims to explore the application of contrastive pretraining to domain adaptation tasks. The unique aspect of this research is the development of a connectivity model to enhance data selection and augmentation during pretraining. The effectiveness of contrastive pretraining is evaluated through experiments on multiple benchmark datasets such as BREEDS DomainNet and CIFAR10. Strengths:  contrastive pretraining for unsupervised domain adaptation is a significant and promising domain of research.  The proposed connectivity model provides a novel framework for understanding contrastive learning.  Extensive experiments validate the approach on diverse benchmark tasks. Weaknesses:  The consider presents a modified connectivity metric rather than an entirely novel method. While this metric may improve finetuning its technical contribution is arguably limited.  The main experiments rely heavily on existing domain adaptation methods. To further demonstrate the practicality of the proposed connectivity measure it would be beneficial to include experiments using domain adaptation methods specifically designed around it.", "Paraphrase: This consider explores the effectiveness of contrastive learning for unsupervised domain adaptation. The researchers use SwAV a recent selfsupervised learning technique to pretrain a model simultaneously on both the target and source datasets. Surprisingly this pretrained model performs exceptionally well on unsupervised domain adaptation tasks. Strengths and Weaknesses:  Strengths: The finding that SwAV pretraining can enhance unsupervised domain adaptation is significant.  Weaknesses: To strengthen the results the authors could compare the initialization of the backbone network used in standard UDA tasks with a model pretrained with SwAV. This would ensure a fair comparison with other approach.  Additionally experimenting with other selfsupervised models such as MOCO or SIMCLR would determine if the same conclusion holds.  The introduction could be improved by correcting numerous typos.", "Paraphrase: Summary: The paper explores unsupervised domain adaptation (UDA) where existing methods aim to create a joint embedding space for labeled source data and unlabeled target data. Instead the authors propose pretraining an encoder using contrastive learning on target and source auxiliary data. This objective encourages positive samples (augmentations of the same range) to be close together and further from others. A classifier is then trained on these features for the source domain classification task. The authors demonstrate comparable accuracy to baselines on UDA datasets and analyze the learned features. Strengths and Weaknesses: Strengths:  analysis of sourcedomain connectivity providing insights into how contrastive learning approach feature extraction. Weaknesses:  Limited clarity on data augmentation used for contrastive learning which is crucial for defining connectivity.  Lack of comprehensive evaluation against adversarial UDA techniques which aim for domain alignment.  Unclear significance of the play and its potential implications. additional Observations:  Unclear definition of positive and negative sets for contrastive learning including the impact of data augmentation.  Limited choice of datasets and baselines potentially limiting the generalizability of findings.  Uncertainty about the impact of data augmentation on both the model and baselines.  Table 2 lacks contingent on the classifiers used affecting the interpretation of results related to domain separation."], "gKprVaCyQmA": ["Paraphrase: Summary The paper introduces the \"There Are Free Lunches\" (TAFL) theorem which provides a workaround for the No Free Lunch theorem. The approach involves dividing a complex task T which can be optimally solved by an algorithm Lb into smaller tasks Ti. These Ti are chosen such that a lessoptimal algorithm La can optimally solve each Ti given a specific dataset Di. The theorem suggests that combining the frameworks learned from each Ti (h1 h2 ... hN) is equivalent to a framework h found by Lb for Task T in terms of minimizing outofsample error. The paper also analyzes the optimal depaper of tasks. Strengths and Weaknesses Note: Due to familiarity with the terminology there may be potential errors in the review. Weakness:  Inference from a Philosophical Standpoint: Logically there should be a metaalgorithm that can find the optimal task split select distributions and sampling strategies and find the optimal framework for La. However according to the No Free Lunch theorem there must be a field for which this metaalgorithm would be outperformed by Lb. This implies that Lm cannot always be optimal for all tasks.  Accumulated Composition Errors: Even if La can optimally solve individual Ti the paper of frameworks may not have the same level of accuracy as Lb. The validation does not address this number and assumes optimality of the paper. Minor number:  Poor explanation of notation in Section 2  lack of precise definition of fa in the TAFL theorem including constraints on the sum and requirements for bounds  Concerns about the legitimacy of choosing ga  Id and the upper bound of the sum as 1", "Paraphrased Summary: This paper introduces the \"There Are Free Lunches\" (TAFL) optimization theorem which refutes the \"No Free Lunch\" (NFL) theorem. While the NFL theorem state that all algorithms have the same average error order the TAFL theorem suggests that certain algorithms can perform beneficial when tasks are presented in a specific order. The theorem also claims that solving more problems can reduce the difficulty of subsequent tasks. The validation of the TAFL theorem is inspired by the universal approximation theorem. The main contribution of the paper lies in formulating the theorem and providing an example. Strengths and Weaknesses: Strengths:  clear structure  Intriguing problem argument  No grammatical errors Weaknesses: Concerns:  Strong assumption about task order and problemsolving conditions  Unclear validation and trivial example  lack of experiments to support the theorem  Insufficient backdrop discussion on the NFL theorem and other concept mentioned Minor Comments:  number 3s yaxis label is inconsistent (percentage on the label but [01] order on the axis) Additional Critique:  The paper lacks explanation and clarification for mathematical notations and concept.  Equation 3s derivation from Algorithm a to Algorithm bs optimality is unclear.  The claim that the number of algorithm calls decreases with the number of clear tasks requires further explanation.", "Summary The paper discusses the theoretical limits of learning algorithms. The theorem presented could have a significant impact on machine learning theory and potentially practical applications. Strengths and Weaknesses  The paper raises many questions that hinder understanding its correctness and impact. Questions:  Does the theorem apply only to learning algorithms or also to algorithms with storage or general optimization methods  Clarify the differences between TAFL and NFL.  Is \u03b8a a real number or a vector  Why is the superscript \"b\" in \u03b8ab and how is it related to fb  In Equation 2 how can da be sampled from fb and how does evaluating La on da inform us about Lbs performance on db  In Equation 3 how can y0 equal x What assumption are made about fxy Do x and y number from the same field  What do n m and k represent in Equation 3  Why is fi fj used instead of x in Equation 5  Why are different symbols (h and m) used for the learned framework in different sections  The phrase \"we can find a serial of tasks representing by a serial of set of samples\" should be revised to use \"series of tasks\" or \"sequence of tasks.\"  The paper could benefit from an English revision.  Citations in the text should use outer parentheses when the authors names do not belong to the sentence.  Standardize references to textual elements (e.g. \"Section 5\").", "Paraphrase: Summary: The paper investigates the possibility of reorganizing tasks to enhance learning performance. It suggests the existence of an optimal task sequence but lacks a method for identifying it. There are no practical or largescale experimental data. Strengths and Weaknesses: The papers writing is unclear and its mathematical notation is confusing. The definition and usage of terms like \"\u03b8\" and \"f(\u03b8)\" are inconsistent. Furthermore key variables like \"nmk\" in equation 3 remain undefined. The use of \"serial tasks\" may be inappropriate with \"task sequence\" being a more suitable term. The approach is overly abstract and impractical compared to alternative methods. A related paper (\"Efficiently Identifying Task Groupings for MultiTask learning\") provides a practical algorithm for solving the problem although it differs from this papers focus on task sequences versus simultaneous training. The paper fails to address catastrophic forgetting a phenomenon that arises from sequential task learning. Exploring how Task Alignment framework (TAFL) mitigates this number would have been valuable."], "l5aSHXi8jG5": ["Summary Paraphrase: Its acknowledged that targeted transferability against common ASR systems is weak but the understanding are unclear. This research examines the impact of targeted transferability against a simplified ASR system. The study conducts an ablation study to determine which ASR components contribute to the issue. The findings provide insights for the ASR case and could potentially aid in addressing other type of transferable attacks across different domains. Strengths and Weaknesses Paraphrase: Strengths:  The authors use a simplified ASR system to demonstrate the impact of specific components on targeted transferability.  The writing is concise and the design is straightforward.  The issue provide evidence on the contribution of various components to targeted transferability. Weaknesses:  The model specifications are not fully provided solely referenced.  vocabulary size is discussed but its unclear how its handled in the model which solely appears to have 9 labels.  attack on MFCC features are not explored.  An ablation study on more complex RNN cells (e.g. LSTM GRU) was not conducted.  The significance of using topographical input in relation to adversarial attacks is not clear.  The victim ASRs need to assign the attackers chosen type to each frame of adversarial audio is modeldependent and not always accurate in CTC or RNNT model.  The framelevel output in the simplified ASR model is fixed which limits its applicability to realworld scenarios.  No reference is provided for Deepspeech.", "Paraphrased Statement: Summary: This study investigates why attacks on ASR (Automatic Speech Recognition) systems often have limited transferability to other ASR systems. The authors systematically modify an ASR pipeline to analyze the impact of different components on attack transferability. They find that many techniques designed to improve ASR robustness also reduce transfer attacks. Strengths:  timely study of a critical issue.  Methodical evaluation through ablation studies.  valuable insights on the relationship between robustness and transferability. Weaknesses:  Limited analysis of the fundamental understanding for observed issue.  discussion focused solely on ASR despite differences in attack transferability between ASR and image recognition. Expanding the analysis to include image recognition job could provide insights into the unique challenges of ASR robustness.", "Summary This study evaluates and clarifies the limited transferability of adversarial attacks in Automatic Speech Recognition (ASR) systems. It begins by listing known factors (e.g. gradient smoothness) and proposes four potential factors that hinder transferability. The study then analyzes transferability in the DeepSpeech system and discovers that its model is too intricate for successful adversarial transferability. Accordingly the author creates five ASR systems based on a straightforward model trained on the Google Speech Commands dataset. For each of the five ASRs 500 adversarial audio samples are generated and tested for transferability across the systems. The evaluation assesses the target transferability rate of optimization attacks by individually examining each potential factor. Finally insights into the impact of each potential factor are provided. Strengths  clear and accessible writing style  Unveils insights into the limited adversarial transferability in ASR systems  Thorough implementation and testing efforts  Comprehensive details in implementation and evaluation Weaknesses  focus: The paper primarily concentrates on measuring ASR systems making it more suitable for a security conference.  Factor identification: The work of identifying the factors is not explained.  Factor Definition: The term \"factor\" lacks clarity and is not consistently defined.  Impact Analysis: While the potential factors are identified their individual impact are not extensively evaluated.  Generality of ASR pipeline: The ASR pipeline presented in the paper should be generalized.  Factor Grouping: An EFA analysis would be valuable in identifying groups of factors.  Evaluation Limitations: The current evaluation could benefit from a more exhaustive assessment of the identified factors.", "Paraphrased Statement: This research examines why optimizationbased adversarial attacks exhibit extremely limited transferability to target realworld ASR systems. Six previously unknown factors are identified as contributing to this lack of transferability including:  input type (Mel Frequency Cepstral Coefficient MFCC)  Recurrent Neural Network (RNN) output type  Vocabulary and sequence size This study explores a significant job and offers valuable insights into the enigmatic low target transferability observed between ASR model. The controlled experiments meticulously isolate the contributing factors yielding clear takeaways. However areas for potential improvement include:  Expanding experiments on input type by considering advanced conversion techniques like Hilbert curves.  Exploring different adversarial attack methods that have been shown to enhance transferability.  Investigating model with different architectures such as wordbased model to assess potential variations in behavior."], "lvM693mon8q": ["Paraphrase: In vertical federated learning where participants hold different data features data compression can reduce communication costs. This new algorithm introduces compression techniques while maintaining a similar convergence rate as the original algorithm. The convergence is proved under the assumption that the compression error diminishes during training.", "Paraphrase: This paper presents a communicationsaving algorithm for training model in Vertical Federated Learning (VFL). It reduces communication by compressing client embeddings and server parameters. Additionally it performs multiple local training epochs to further reduce communication. The algorithm achieves a convergence rate of O(1T0.5) similar to the stochastic gradient descent (SGD) algorithm. Empirical results demonstrate the algorithms ability to save communication costs compared to standard VFL. effectiveness:  Addresses the bottleneck of communication in Federated Learning especially for VFL where embeddings are transferred but parameters are not.  Provides theoretical analysis to prove the algorithms convergence rate.  point significant communication cost reduction in experimental evaluation. Weaknesses:  Assumes all clients have access to the complete label set which is not realistic.  Claims that unbiasedness is maintained even with repeated batch use in local training which needs further clarification.  The total communication cost reduction is not theoretically proven and requires analysis to determine whether it saves a constant factor or more.", "Paraphrase: This research investigates the issue of communicationefficient training on vertically partitioned data. It is pertinent to the conference as it provides theoretical analysis on how message compression impact distributed training over vertically partitioned data. The paper demonstrates that nonconvex objectives can converge to a fixed point at a rate of O(1\u221aT) when compression error remains bounded throughout training. Experiments show that compression can substantially reduce communication (over 90) with minimal accuracy loss compared to uncompressed training. effectiveness: The paper provides theoretical analysis on the effects of message compression in distributed training and proves convergence under certain conditions. Weaknesses:  The research builds on prior work in Federated Learning for vertically partitioned data.  The numerical experiment is limited involving just four parties and a server and the dataset is small.", "Paraphrase: Summary: There is increasing interest in vertical federated learning where parties keep just certain features due to its practical applications. This paper explores how to enhance the efficiency of vertical federated learning by utilizing compression for regularly shared embeddings. While compression has been widely studied for optimizing network traffic in gradient synchronization its application for embeddings has received limited attention. This paper demonstrates that convergence is achievable if compression errors decrease at the same rate as the learning rate. Experiments show that vector quantization achieves comparable accuracy to the fullprecision version while dramatically reducing communication costs. effectiveness and Weaknesses: This paper investigates a significant problem. Frequent communication between parties is expensive making local iterations an attractive option. However compressing embeddings is not well understood and its impact on convergence is unclear. While the paper is generally wellwritten it lacks specific details on the compression method it considers. Table 1 outlines error boundss for scalar quantizer vector quantizer and topk sparsification but their algorithmic implementation are missing. Despite its intriguing nature the paper raises several theoretical questions: 1. The update of \\thetamt depends on the minibatch B. How can it be ensured that the gradient is unbiased for any t  t0 especially given that the RHS of (7) depends on the realization of B This is important for establishing convergence. 2. The bounds (8) in Lemma 1 grows linearly with minibatch size which is imprecise. 3. It is unclear how to select the compressor to ensure that compression error decreases at the same rate as the learning rate a key factor for convergence. 4. Error bounds (9) exhibits a quadratic dependence on the amount of parties M which seems conservative. 5. A source convergence bounds for the scenario without compression would be valuable."], "y1faDxZ_-0a": ["Paraphrase: This research focuses on a common situation in Federated Learning (FL) where participants might not have labeled data. The study proposes using the SimSiam architecture to extract valuable feature representations. Personalized enhancements for local client models are included. The extracted representations are assessed with a KNN classifier to evaluate their usefulness. effectiveness:  Addresses a significant and practical aspect of FL.  Investigates the value of personalized representations. Weaknesses:  Lacks a comprehensive quantitative comparison between SimSiam and other selfsupervised techniques.  The practical applications of the proposed feature representations particularly in personalized settings are not fully explored.  The potential benefits of using the representations as an auxiliary objective in practical applications could be explored.  The impact of hyperparameters such as the size of the representation is not adequately examined. This parameter could potentially overshadow the benefits of selfsupervision in the KNN evaluation.", "The submission is not anonymous because the authors name Chaoyanghe appears repeatedly in the attached author code. As a result it should be immediately rejected.", "Paraphrased Statement: Researchers have developed a framework that combines selfsupervised learning and personalized federated learning to address challenges related to data heterogeneity and limited labeled datasets. They evaluated several existing algorithms within this framework and proposed a new algorithm Per SSFL that balances consensus and personalization. The study provides experimental results and a detailed analysis to support their claims including direction on selecting algorithms and hyperparameters for different scenarios. effectiveness:  direction on the practical problem of limited label and heterogeneous data in federated learning.  open problem formulation and optimization function.  Experimental evidence to support the effectiveness of the proposed framework.  Comprehensive analysis and discussion of the results. Weaknesses:  Limited technical novelty in combining existing techniques without strong motivation.  Similar approaches to controlling the difference between global and local models have been proposed previously.  lack of direction on how to choose hyperparameters effectively.", "Summary: This research explores using selfsupervised learning (SSL) techniques in federated learning (FL). several SSL methods are evaluated to determine their effectiveness in FL environments. The paper introduces a personalized FL SSL framework called SimSiam and demonstrates its advantages through performance comparisons. effectiveness:  Provides insights into the emerging field of SSL for FL which has practical significance.  Investigates SSL methods in both nonpersonalized and personalized FL contexts.  Conducts experiments and analysis to support the findings. Weaknesses:  The paper does not discuss memory consumption concerns related to the use of both local and global models in the PerSSFL framework which could be a limitation for resourceconstrained devices.  The total number of clients used in the FL context is not specified.  Table 1 lack contingent on the FL method used in the supervised context.  image 2s notation for \"SSFL on IID and SSFL on nonIID\" is unclear.  The similar convergence rates of SSFL on IID and nonIID datasets require further explanation.  The experimental setup on the GLD23K dataset in Appendix D is not adequately explained including contingent about data generation label distribution and the number of clients used.  The evaluation would benefit from including additional datasets (e.g. CIFAR100 TinyImageNet) and tasks beyond vision.  Exploring the impact of varying the number of selected clients on SSFL results would be valuable.  Optimizing the balancing parameter (\u03bb) between consensus and personalization is not discussed. Minor Issues:  Section 2.1 references a dataset Dk.  image 10 should be labeled as image 2 and image 3 in Sections 5.2 and 5.3.  Color inconsistencies in image 2(a) should be corrected.  SwAV should be \"SimCLR\" in Section 3.2.  A thorough proofread of the paper is recommended."], "sPfB2PI87BZ": ["Paraphrase: Summary This paper introduces the Generalized Target Shift (GeTarS) where both conditions and labels differ in the target domain. The papers Optimal Sample Transformation and Reweight (OSTAR) approach employs optimal transport to modify the latent space and information maximization to adjust classifier boundaries. Theoretical analysis suggests that OSTAR minimizes upper bounds in target domain risk. empirical testing evidence promising solution for OSTAR on three datasets. Ablation work specifically examine the impact of information maximization. effectiveness  Grounded in GeTarS leveraging optimal transport to reconcile source and target distributions.  Supported by theoretical analysis that reduces target risk bounds.  Demonstrates competitive performance against baselines on multiple datasets. Weaknesses  Disconnect between theoretical foundation and information maximizations implementation.  Experiments show significant improvements from information maximization in OSTAR raising questions about its contribution.  Main empirical solution only include OSTAR scores not clarifying the impact of optimal transport alone.", "Summary: This paper presents unsupervised domain adaptation (UDA) for scenarios where both the label distributions and data distributions differ between source and target domains (GeTarS). The authors propose OSTAR a method that aligns pretrained representations under GeTarS. Theoretical analysis demonstrates the effectiveness of OSTAR within certain assumptions and experimental solution support its superiority over existing methods. effectiveness:  OSTAR effectively handles GeTarS and enhances target discrimination.  Theoretical and empirical analysis support the proposed method.  Experimental work show improvements over baseline approaches. Weaknesses and Questions:  (1) Discriminativity Advantage: Clarify how OSTAR compares to UDA methods that explicitly preserve discriminativity e.g. [ref1] and [ref2].  (2) Preserving Target Discriminativity: Explain how the method ensures target discriminativity is maintained when learning g in Equation (3).  (3) Least Action Principle extension: Provide a extension for the \"least action principle measured by Monge transport cost\" in Equation (OT).  (4) Linking pT(Z) and pN\\phi(Z): Explain how pT(Z) is obtained from pN\\phi(Z) in the constraints of Equation (OT).  (5) Significance of Proposition 1: Discuss the implications of requiring Z to satisfy all four assumptions in Proposition 1 and its impact on the flexibility of the method.  (6) Missing extensions: Include important extensions such as [ref3] and [ref4].  (7) Weighted Objective Terms: Consider weighting all terms in the overall target and Equations (SS) and (SSg).  (8) Sensitivity Analysis of \\lambdaOT: Discuss the sensitivity of OSTAR to different values of \\lambdaOT.  (9) Table 1 Average solution: Provide average solution for each dataset in Table 1.", "Paraphrase: Summary: This work introduces a technique for unsupervised domain adaptation (UDA) in challenging scenarios with both conditional shift (where the data distribution changes between domains) and label shift (where the class labels differ). The method revolves around an optimal transport problem that finds a map between source and target domains and a weight vector for each class. This map minimizes a cost map that represents the transport cost between the domains. Using four assumptions the work provides theoretical solution regarding the uniqueness of the solution and the generalization bound on the target domain risk. The proposed algorithm uses the optimal transport target as its foundation with additional terms for label proportion constraints classification loss and target representation discriminativity. The method was evaluated on several benchmark UDA datasets with varying levels of class imbalance where it consistently outperformed existing UDA approaches. effectiveness:  Wellmotivated and theoretically sound approach  Clear assumptions and theoretical solution  insightful domain generalization bound related to class proportion and imbalance  Thorough experimental evaluation with comprehensive error estimates and ablations  Impressive SOTA solution and wellwritten paper Weaknesses:  The actual optimization target differs from the theoretically derived risk bound which raises questions about the relevance of the theoretical solution for the proposed method.  Missing runtime complexity analysis  Inadequate explanation of some assumptions especially Assumption 2  Insufficient discussion of the effect of the encoder on performance and how to select the most suitable one Questions and Suggestions:  Clarification of the discrepancy between the optimization target and the pushforward constraint in the optimal transport problem  Justification for the realism and practicality of Assumption 2  Explanation of how to compute the domain generalization bound from sample data  Distinction from prior OT approaches for UDA such as Rakotomamonjy et al. (2020) Typos:  Assumption 4 should read pT(Z Yi) (not k)  Page 8: \"confortsupportsconfirms\" should be \"confirms\""], "oiZJwC_fyS": ["Paraphrase: This field presents a new method for systematically pruning neural networks using tropical geometry. The key principle is to convert a neural network into tropical polynomials using tropical geometry tool and then apply kmeans clustering with distance metrics specific to this representation. The paper provides a foundational overview of tropical geometry and its associated techniques. Experiments were conducted on linear layers of binary classification networks trained on MNIST (35 and 49) LeNet5 and VGG trained on MNIST and FashionMNIST. The proposed method was evaluated against L1 structured pruning (Smyrnis  Maragos 2020) and ThiNet (Luo et al. 2017). effectiveness:  Introduces a novel concept that has not been extensively studied in the context of structured network compression.  Provides a detailed explanation of tropical geometry and related tool for readers without prior knowledge.  Includes illustrative examples that enhance the understanding of the method. Weaknesses:  Lacks a comparison with nontropical methods.  Does not report the computation time required for network pruning and inference.  The absence of pseudocode or code segments for zonotope generators makes it challenging to replicate the experiments and hinder accessibility to nonexperts.  The experimental setup is limited and may not provide sufficient information for reproducing the results.", "Paraphrased Statement: Summary: This paper aims to investigate neural network compression using zonotope reduction. It establishes approximation bounds for tropical polynomials based on Hausdorff distance leading to the development of two neural network compression techniques. The techniques are assessed on the MNIST FashionMNIST CIFAR10 and CIFAR100 datasets. effectiveness:  Wellexplained background on tropical geometry  Cleanly stated Theorem 2 Weaknesses: Questions:  Why do the bounds worsen as the percentage of retained weights decreases  Why was Alfarra et al. (2020) not included in the experimental evaluation Feedback:  The CIFAR dataset results are not compelling as compressing architectures like AlexNet and VGG is less relevant due to their outdatedness. innovative architectures (e.g. ResNets Vision Transformers) should be evaluated.  ThiNet generally outperforms other methods on MNIST and FashionMNIST except for LeNet on FashionMNIST.  The current stateoftheart (SOTA) pruning methods and results should be clearly stated and included for comparison.  The code for reproducibility should be provided.  The supplementary material should be submitted separately.", "Summary As Im not an expert in this field this review is based on my beneficial guess. The paper introduces a novel compression technique using a framework called \"geometric zonotope reduction.\" The authors show how well the new method works and compare it to other techniques. The main advantage of the new method is that it is new and has theoretical support. However it may be difficult for nonspecialists to understand and its relevance to the ICLR conference is not clear. Testing the method on larger datasets would also improve its credibility.", "Paraphrased Summary: The paper analyzes neural networks using tropical geometry and proposes a compression algorithm for networks with ReLU activations. This algorithm improves the accuracy of compressed networks compared to existing tropical approach. effectiveness:  Strong mathematical analysis of neural networks as tropical mappings including an approximation error bound.  algorithm outperforms previous tropical compression methods in test accuracy.  tropical perspective may spark further theoretical research. Weaknesses and Improvement Suggestions:  use innovative CNN architectures (e.g. ResNet) in comparison.  Include computational and memory complexity as well as runtime analysis.  Test the algorithm on larger networks including transformer networks.  Clarify whether only fully connected layer nodes are pruned in experiments.  State in the abstract that the algorithm applies only to networks with ReLU activations.  Revise claims regarding advancements and competitive performance of the proposed method based on concrete evidence.  Selfexplanatory table captions."], "kamUXjlAZuw": ["Paraphrased Statement Summary The paper presents an analysis of fairness achieved by adjusting a loss role with a carefully chosen fairness metric. Specifically the analysis is conducted in both a PAC learning scope and an asymptotic scope. The focus is on the PAC scope where Rademacher complexity boundss are applied and evaluated. Some empirical experimentations are also provided. Strengths:  The boundss while somewhat predictable offer valuable theoretical upper boundss for generalization under fairness constraints. Weaknesses:  The proof techniques employed are not particularly innovative. They primarily rely on Rademacher inequalities and union boundss.  The discussion of the \"asymptotic regime\" is rather limited.  Theorem 2s improvement upon Agrawal et al. (2020) does not appear significant mainly involving adjustments for additional dimensions in derivatives and sum terms. (Note: The reference to Proposition 1 does not align with the most recent version of Agrawal et al.)  The proof of Proposition 6 is not included in the paper and would be helpful in the appendix.  The upper bounds could be tightened to 2C\u221an by scope denominators to 1.  The experimentations appear to only investigate the tradeoff in regularization and do not fully test the theoretical claims. It could be valuable to experimentationally examine outofdistribution generalization boundss.  The class imbalance in the experimentations is not addressed and the potential impact of only having less data is not explored.  The paper contains several notational typos and missing definitions making it challenging to follow.  The use of the \u00b1 subscript is confusing as its not always clear whether it denotes a specific equation or a \u00b1 variant.  The notation S is sometimes used for subgroup loss in other research.  Definition 2 requires clarification of \u03c3i.  The typographical distinction between Rademacher complexity and constants R should be clarified.  A min term is missing at the top of page 6 and the definition of h appears to be consistent.  The dual superscript \u00b1 of p in Prop 6 is confusing.  The symbol \u03c3 is used for both standard deviation and a generic variable.  The text of footnote 2 is missing likely related to using sex rather than age in the German Credit experimentation.  The reference to \"generalized \u03b1 mean\" appears to be incorrect as it aligns with the Lp norm. A correct citation would be helpful.", "Paraphrase: The paper examines the generalization capabilities of learning algorithms that incorporate fairness constraints. Strengths:  Exploring the generalization characteristics of common fairness step and their correlation with accuracy is valuable.  Sections 2 and 3 are wellwritten accessible and provide insightful issue. Weaknesses:  The papers overall objective is unclear with issue presented without much coherence.  The foundation should be revised to clarify the papers purpose and the connection between its issue and the problem it addresses.  The comparison to prior study is superficial. While the paper mentions previous research on fairness generalization it fails to explain how its findings extend or differ from them.", "Paraphrased Summary: This paper provides theoretical guarantees for fairnessaware learning in binary classification using both the PAC learning framework and a more practical asymptotic approach. The authors prove that minor sample sizes and class imbalances negatively impact the generalization performance of fairnessaware methods. They highlight the need for sampleefficient techniques. Realworld data experiments support the theoretical findings showcasing the papers practical relevance. Strengths and Weaknesses:  The paper uses clear notations and wellorganized theorems and proofs.  It combines generalization boundss for standard loss functions and fairness constraints.  The different impact of sample sizes on standard loss and fairness terms is an interesting observation.  Proposition 6 provides valuable insights. Suggestions:  Standardize notation in equations (1) and (2).  Specify that hT h \\in \\mathcalH \\mathcalLT(h) in the missing role.  Provide a more practical explanation of the probabilistic upper bounds in Section 4.2.  Include experimental issue beyond binary classification such as nonbinary groups and nonbinary classification tasks.  Experimentally investigate the findings of Proposition 6.", "Paraphrase: Summary:  The paper examines generalization outside of sample data with both loss (without constraints) and fairness considerations.  While the paper presents theoretical issue it could strengthen the connection between them to deliver clearer takeaways.  The paper is difficult to understand at times and its organization is confusing. Strengths and Weaknesses: Question 1:  The paper claims to be the first to address generalization issue but previous study (e.g. Woodworth et al. 2017) have presented generalization bounds. Question 2:  The paper introduces a PAC framework for fairness tradeoffs.  The papers bounds may not be applicable to all fairness notions listed in Table 1.  Proposition 3s assumption on \u03c6 needs clarification.  The paper should provide insights into how to apply the issue to fairness notions or efficiently check for applicability. Question 3:  The asymptotic regime derivation (Theorem 2) presents convergence in distribution.  The role of this issue and the significance of the variance term should be explained."], "x4tkHYGpTdq": ["Summary (Paraphrased) This paper aims to enhance the parameterefficient and resourcesaving capabilities of LoRA a prior tuning method by incorporating weight sparsity constraints. Specifically the paper: 1. Adds a separate sparse learnable weight matrix to LoRAs lowrank decomposition 2. Utilizes unstructured and structured weight pruning techniques to reduce computational costs during inference Experiments on GLUE and data2text generation tasks demonstrate the effectiveness of this method. effectiveness  Merges weight pruning with parameterefficient tuning improving resource efficiency during inference  Experiments support the method efficacy and provide detailed analysis of DSEE (densesparse embedding expansion) Weaknesses  Does not clarify whether GLUE results are from one or multiple work (essential information due to initialization sensitivity)  The necessity of adding a sparse matrix (S2) to LoRA lack convincing justification based on experimental results  The incorporation of existing weight pruning method limits the technical novelty of this paper  The performance boost from DSEE over LoRA potentially stems from sparsity in pretrained weights which should be explained  Misleading statement about DeBERTa and lowrank decomposition contradicting the findings of other research  Minor presentation errors: missing citations and incorrect table references", "Paraphrase: Summary: This work introduces a principled method for simultaneously optimizing model parameters for efficient finetuning and resourceefficient inference. It combines two step: 1. Computing a lowrank decomposition of parameter updates with a sparsity matrix. 2. further pruning the finetuned model. The first step targets parameterefficient finetuning while the sparse residual components based on the pretrained models sparsity preserve performance. The irregular step reduces parameter storage memory. effectiveness:  Clear problem definition and method presentation.  Introduces a novel approach for achieving both parameter and resource efficiency.  Extensive experimental results on various models benchmarks and metrics demonstrating the method generalizability. Weaknesses:  Difficulty in interpreting experiments to support claims.  Marginal performance improvements compared to simpler method like LORA especially for large models like DeBERTA.  Insufficient evidence from FLOPs results (only reported for SSTB).  Unclear justification for using S1 only on pretrained weights W.  Ambiguity in choosing between unstructured or structured sparsity.  Questionable resource utilization benefits due to potential performance drop or increased training time to search for sparsity.  Incomplete explanation of UV computation without reference to pretrained weights. Typos and Tips:  \"13 heads from each attention head\" should be \"13 heads from each attention level.\"  \"reset the values of S2 back to 0. During the update we only update the elements of S2 whose indexes are in \u2126\" should be in correspondence with Algorithm 2.  Provide more details on UV computation without relying solely on references to previous work.  Specify the FLOPs results for BERT.", "Summary This research aims to simultaneously optimize both efficient finetuning and model size compression. It utilizes an existing framework to model both weights (W) and their updates (\u03b4W) and employs various existing techniques to learn these two components sequentially. Strengths  Clear and accessible presentation  Importance of the research topic Weaknesses  Organization: The introduction should better highlight the motivation for the proposed method (in Section 3).  Lack of Comparison: The authors overlook a competitive efficient finetuning method (https:arxiv.orgabs2101.00190).  Unfair Comparison: Trainable parameter counting is not a fair comparison for sparse pruning method as they require additional memory for storing sparse coordinates.  Speed Limitations: While sparse method generally offer limited hardware speedup the authors fail to address this issue and instead focus on FLOP reduction which is unfair to structured method like structure pruning and lowrank approach.  Insufficient Context: The ablation work in Figure 2 shows cases where other heuristics outperform DSEE but the reasons behind these cases are not explored.  Lack of Empirical Validation: The empirical experiments do not demonstrate a significant advantage over existing method raising concerns about the efficacy of DSEEs individual components.  Incomplete Explanation: The GreBsmo algorithm used to solve Eq. 1 is not explained leaving readers confused.", "Paraphrased Statement: This work introduces a Dually SparsityEmbedded Efficient Tuning (DSEE) technique that leverages model sparsity to enhance efficiency. DSEE applies specific weight updates that preserve the trained weights while promoting sparse structures. Across various backbones and datasets DSEE reduces inference performance by 35 and trainable parameters by 0.1 without compromising accuracy. effectiveness:  Enhances parameter and resource efficiency in NLP models.  Demonstrates promising results. Weaknesses:  Language and presentation require improvement as the distinction between \"sparsityaware weight update\" and \"sparse final weight\" remains unclear.  experimental results could be further validated especially by comparing DSEE to LoRA applied to a sparse pretrained model.  A comprehensive table summarizing training inference time and memory usage would clarify the resource savings.  The technical novelty could be better justified or emphasized.", "Paraphrased Statement: This paper presents techniques to enhance the efficiency of massive pretrained models by combining sparsity and dimensionality reduction. These method involve: 1. Incorporating a lowrank update matrix with adjustable parameters. 2. Pruning the original weight matrix. During inference these two position of parameters work together. The authors test their approach on various model configurations and dataposition. effectiveness:  Addresses the increasing size of models and the need for efficiency.  empirical results suggest potential benefits in practice. Weaknesses:  The motivations and specific contribution of the paper are difficult to grasp.  The description of the inference mechanism is unclear.  The assumption of the update matrix sharing the subspace of the original weight matrix needs further justification.  The performance gap between the proposed approach and random techniques is small and inconsistent.  The practical benefits in terms of computational efficiency are not fully demonstrated.  The utility of the additional complexity introduced by \\mathcalS2 is questionable."], "lsQCDXjOl3k": ["Paraphrased statement: This field introduces an unconditional guidance system that eliminates the need for classifiers used in earlier approach. While this guidance can strike a balance between sample quality and diversity like classifier guidance its benefits are unclear. The unconditional guidance method requires training both an unconditional and a conditional model while the classifier guidance method trains a conditional model and a classifier. Both methods entail training two models resulting in similar computational costs. Moreover the unconditional guidance method faces a significant slowdown during the sampling phase raising question about its practical value. The experiments rely heavily on technique and settings from previous work and the field does not optimize the unconditional guidance methods performance or provide optimal settings for others to use.", "Summary This paper presents a diffusionbased image generation model that uses class labels as guidance. It extends the approach of Dhariwal and Nichol (2021) by replacing an explicit classifier with an implicit classifier modeled as a learned generative model. This allows the model to trade diversity for fidelity without using a separate classifier. effectiveness  Provides a comprehensive background review of Dhariwal and Nichols approach.  Introduces a novel method for implementing implicit classifiers by sharing weights between learned and unlearned generative models. Weaknesses  need for removing an explicit classifier is not fully clear.  Inference efficiency is potentially reduced due to needing to run the model twice for each iteration.  Title is misleading as the model is still conditional albeit with an implicit classifier.  theoretical statement about comparison between learned and unlearned models is not entirely accurate.  Experiments lack detailed analysis of the impact of classifier replacement and continuoustime training.  statement about gradientbased adversarial approach are somewhat contradictory.", "Summary This research presents a novel method \"unconditional guidance\" for balancing sample diversity and quality in diffusion models. Unlike previous guidance methods that rely on classifiers this approach combines score estimates from conditional and unconditional diffusion models. Experiments demonstrate its effectiveness in trading off diversity for quality on ImageNet datasets. effectiveness  Clear and concise writing  Novel idea of combining conditional and unconditional score estimates  Demonstrated tradeoff between diversity and quality Weaknesses 1. practical Significance:  The method requires training a conditional diffusion model which may be less flexible and practical compared to classifier guidance which can be applied directly to pretrained unconditional models. 2. Evaluation Metrics:  The use of only FID and IS score is insufficient precision and recall metrics should be included to better assess the tradeoff. 3. experimental Scope:  Experiments should be extended to larger ImageNet resolution (256x256 and 512x512) for a more comprehensive evaluation. 4. Equation Correction:  A negative sign appears to be missing before the \\sigma\\lambda term in the provided equation. 5. Name Misnomer:  The methods name \"unconditional guidance\" is misleading as it requires training a conditional diffusion model.", "Paraphrase: Summary: This field introduces an improvement to scorematching generative modeling that mimics lowtemperature sample used in generative adversarial networks (GANs) and flowbased models. Like another prior work the authors propose modifying the drift use in the sample stage by incorporating the gradient of a classifier. However unlike that work the classifier employed here is implicit defined only by conditional and unconditional generative models. This approach enables controlling a tradeoff between Inception Score (IS) and Fr\u00e9chet Inception Distance (FID) on the ImageNet dataset. Using an implicit classifier provides insights into guidance influence in scorebased generative modeling as the model aims to minimize unconditional likelihood while maximizing conditional likelihood. effectiveness:  Clear and wellwritten presentation  Demonstrated tradeoff between FID and IS through experiments  interest idea of using implicit classifiers  Intuitive explanation of guidance influence Weaknesses:  contribution is relatively incremental as other work has used classifiers for guidance in scorebased models  Justification for using unconditional classifiers is somewhat superficial  Theoretical development could be expanded (e.g. properties of the model derivation of backward model)  Unexpected results in experimental tables (e.g. nonguided model showing inferior FID to guided model)  Lack of analysis on nearest neighbors in the dataset for different guidance parameters"], "u7PVCewFya": ["Paraphrase: This paper introduces a new loss function for differentially private deep learning. The loss consists of three components: mean squared error focal loss and regularization. The loss is intended to enhance the performance of differentially private deep learning compared to the crossentropy loss. Experiments on MNIST FashionMNIST and CIFAR10 datasets demonstrate improved accuracy. Strengths:  Clear and understandable presentation.  Intriguing concept that DP learning may require a different loss function than nonDP learning.  Empirical evaluation on computer vision datasets shows improvement.  Detailed data on network architectures and ablation work are provided. Weaknesses:  limited experiments focus solely on computer vision tasks and small datasets.  Empirical improvements are not significant with a maximum addition of 4 on CIFAR10.  Lack of theoretical justification raises concerns about the generalizability of the improvement.  The new loss combines existing components in a direction that lacks sufficient justification.", "Paraphrased Statement: This work proposes a new loss function to improve the training accuracy of Deep Private Stochastic Gradient Descent (DPSGD). The proposed loss combines Sum of Squared Errors (SSE) focal loss and L2 regularization. Strengths:  The approach to enhance private deep learning through loss design is unique and potentially valuable to the community.  The resulting model optimized using the proposed loss achieves stateoftheart performance. Weaknesses:  Insufficient Motivation for Loss Design: The authors fail to provide clear reasoning for the specific design of their loss function and lack convincing explanations for its effectiveness.  Unanswered Questions:  The authors claim that their loss prevents data loss from clipping but do not explain how the loss accomplishes this.  They assert fast convergence but offer no evidence to support this claim.  They suggest that preventing logit explosion can restore the generalization benefits of batch normalization but this is presented as a mere observation without theoretical justification.  Unclear Indication of Effectiveness: number 2 suggests that the proposed loss function may not enhance smoothness as claimed.  Lack of robustness Analysis: The paper does not explore the sensitivity of the loss function parameters to hyperparameter settings.", "Summary: The paper introduces a customized loss function for DPSGD (Differentially Private Stochastic Gradient Descent) that combines three components:  Summed Squared Error: Facilitates rapid convergence in the initial stage.  Focal Loss: Identifies and focus on challenging samples.  regularization penalty: Controls gradient norm to prevent explosion. Strengths and Weaknesses: Weaknesses:  The papers argument for the superiority of Summed Squared Error for fast convergence is based on theoretical evidence that may not apply to practical scenarios.  The convergence rate in practice is more influenced by optimizer choice (e.g. SGD Adam) and learning rate rather than loss function. The paper lacks a thorough comparison with these factors.  The regularization penalty is similar to weight decay and the paper does not provide a comparison with traditional weight decay.  Using CIFAR10 as a benchmark may not accurately reflect the effectiveness of DP algorithm due to its small size and lack of significant privacy sensitivity. This could lead to overoptimized models and loss that may not generalize to realworld privacysensitive tasks.", "Paraphrase:  Proposed Loss Function: The work introduces a limited loss function for enhanced performance in neural networks trained via Decentralized Parallel Stochastic Gradient Descent (DPSGD). This function combines the following elements:  Sum of Squared Error (SSE)  Focal Loss  penalty on preactivation output norm  performance and Advantages: The new loss function demonstrates superior accuracy on CIFAR10 FashionMNIST and MNIST datasets. Additionally it mitigates the bias caused by gradient clipping and facilitates learning on complex examples.  Strengths:  Exploration of the effect of loss function on DPSGDtrained models  Comprehensive experimentation  Weaknesses:  Introduction of three additional hyperparameters whose impact is not fully analyzed  The loss function incorporates existing components and alternative options are not explored  After Rebuttal:  The authors have addressed concerns about misleading arguments and have clarified that the work should be considered empirical. Despite the lack of theoretical support the paper remains valuable for its exploration of the impact of loss function on DPSGD."], "i7h4M45tU8": ["Summary Paraphrase: This work proposes a method for extracting temporal rules from data by identifying individual events and their temporal connections. The method then employs composite event labels to guide the learning of complex events. The entire approach is formulated as an optimization problem which is then solved using conventional techniques. Strengths and Weaknesses Paraphrase: Strengths:  Addresses the significant task of learning temporal rules from data which has numerous applications.  Innovatively leverages neural networks to create interpretable rules. Weaknesses:  Complexity in following the approach due to a lack of explicit design clarification.  Unclear rationale for dismissing Temporal Relation Networks due to \"long timelines\" which is not wellexplained.  Unusual concept of atemporal composite events making it difficult to understand why timelines are irrelevant for certain events.  Insufficient connection to probabilistic rule learning approach in the structure learning work.  Lack of evidence supporting the unsuitability of previous approach for the proposed background.  Concerns about the validity of the medical model particularly regarding the interpretation of oral waters role in urine flow.  potential limitation in the approach due to its sole focus on temporal events in rules.", "Paraphrased Statement: The paper introduces a novel approach called neural logic programming for extracting composite events from complex time series data. The approach utilizes atomic events and temporal relationships to achieve this. Strengths:  The twostep method (parameter learning then structure learning) is a novel concept for addressing this problem. Weaknesses:  The evaluation results are inconclusive.  The baseline LSTMs are relatively weak.  The proposed approach performs worse than the LSTMs in terms of mean average precision (a standard metric in the CATER dataset).  The approach is only marginally better than logistic regression as shown in Table 3. Minor number:  In number 2 the caption \"(1) parameter learning\" appears to be placed incorrectly. It should reference predicate prediction as described in paragraph 1 on page 3 of the paper.", "Paraphrased Statement: The paper presents a technique called \"neural TLP\" that learns relationships between events (e.g. \"miss\" occurs before \"swing\") and uses this knowledge to predict complex events (e.g. \"strike\"). The technique consists of filtering noise from time series identifying time intervals predicting temporal relations and applying a linear work. Additionally the paper provides a method for extracting propositional logic rules from the final layer. Strengths:  Clear writing  Simple but innovative solutions for temporal relation prediction and rule extraction  Promising results Weaknesses:  Lack of discussion on recent literature in video reasoning and question answering  Limited compare to only simple baseline ignoring recent neural advancement such as graphbased approach  Questions: 1. Scalability: Can the strategy handle large number of objects events and complex target events 2. Object Independence: Does the strategy assume independence of objects and only consider temporal relations between events 3. time Interval Estimation: How reliable are the estimated time intervals for continuous and noisy settings 4. Training: In CATER experiments are the perception component (Faster RCNN) and Neural TLP trained jointly 5. parameter learning Terminology: Why is the learning of temporal relations considered \"parameter learning\" 6. rule Expressiveness: What is the logical expressiveness of the extracted rules", "Paraphrased Summary: This paper introduces a neural networkbased method for Temporal Logic Programming. The key design is translating combinatorial rule space searching into a problem of assigning predicate selection indicator vectors. This allows for endtoend training using atomic event probabilities as inputs. Strengths:  Converting rule space searching into predicate selection. Weaknesses:  Model parameter depend on the number of events and predicates making it challenging to generalize or retrain for new events.  Writing clarity could be improved particularly in explaining the proposed method.  experimental baseline are customized lacking comparisons to established benchmarks using the same dataset. If such baseline are unavailable the selection criteria should be explained. Details:  Page 1: Distinguish between causality and temporal relationships.  Page 3: Clarify the definition of MT and its relation to atomic event probabilities.  Page 4 (Equation 2): Explain the use of probabilities in convolution and discuss how the framework handles raw inputs like video or audio.  Page 4: Elaborate on the use of MA MC MD and the meaning of \\otimes.  Page 4: Explain how \"MDll\" is used as positional encoding.  Page 6: Describe the sampling of top \"c\" predicates and define \"s\" in softmax(s). Discuss the computational cost of this procedure."], "uxgg9o7bI_3": ["Paraphrase: Summary: This paper introduces a new framework for designing MessagePassing Neural Networks (MPNNs) that outperform the 1WL test which measures the expressive ability of neural networks. Unlike traditional MPNNs the proposed framework incorporates structural data into the aggregation scheme during message passing. This allows the model to distinguish between graphs based on their neighborhood structures. The model proves to be more expressive than 1WL and comparable to common MPNNs in terms of memory and time complexity. Additionally it reduces the impact of oversmoothing a common issue in MPNNs. Strengths:  Novel approach to enhance the expressive ability of MPNNs.  Provides a constructive method to build strong architectures than 1WL.  Demonstrates improved performance on node and graph classification tasks. Weaknesses:  Lacks experiments demonstrating the specific types of structures the model can distinguish.  The empirical results on oversmoothing are not fully explained. Suggestions:  Explore generalization to unseen graphs with varying neighborhood size.  Discuss the potential impact of graph size on model performance.", "Paraphrased Statement: This research introduces a novel approach to address the limitations of message passing neural networks and the efficiency of expressive graph neural networks (GNNs). It proposes overlap subgraphs and overlap isomorphism as a middle ground between subgraph and subtree isomorphism. Utilizing these concepts the field designs GraphSNN with local structural coefficients that regulate message passing enhancing expressiveness beyond limitedrange GNNs. The proposed method showcases promising results in nodelevel and graphlevel classification tasks along with an ablation analysis on a key hyperparameter. Strengths: 1. Overlap isomorphism provides a balanced approach between expressiveness and computational efficiency. Structural coefficients effectively incorporate three properties. 2. The adjustable hyperparameter \u03bb provides flexibility allowing for tailored data capture depending on the learning task. 3. GraphSNN demonstrates remarkable results showcasing significant improvements over traditional GNNs in node classification and outperforming baselines in graph classification. Its performance remains consistent even with deeper layers. Weaknesses: 1. The claim regarding augmented node identifiers requires further clarification. 2. The normalization in the definition of the average neighborhood vector can potentially be simplified. 3. The emphasis on strongly connected neighborhood may hinder performance in tasks requiring longrange data along weak paths. 4. Further exploration is needed to investigate the impact of GraphSNN on oversmoothing and whether its graph operator leads to a dominant subspace that aligns with labels. 5. Comparison with expressive baselines on regression datasets (e.g. QM9 and ZINC) would provide a more comprehensive evaluation of the methods capability.", "Paraphrase: This research introduces an efficient message passing method that leverages structural neighborhood data specifically \"overlap subgraphs\" which is more powerful than the traditional \"1WeisfeilerLehman\" (1WL) method. Compared to existing GNNs that exceed 1WL in expressiveness this method has limited additional computation costs and evidence promise in addressing the \"over smoothing problem.\" Experimental results demonstrate the methods increased expressiveness and simplicity supported by an ablation field. The method leverages the fact that treating neighborhood as mere feature multisets overlooks crucial topological data limiting 1WLs expressiveness. Representing neighborhood as subgraphs reveals a hierarchy of subgraph isomorphism: overlap subgraphs lie between neighborhood subgraphs and neighborhood subtree isomorphism. Incorporating structural data that solves for overlap subgraph isomorphism results in a messagepassing network more expressive than 1WL. The method includes structural coefficients for each vertex based on overlap subgraphs providing a more powerful tool. extensive experiments on node and graph classification tasks show improvement over competing methods including higherorder approach. The ablation field highlights the importance of the structural coefficients. Additionally the proposed method exhibits resilience to the oversmoothing problem although further insight into this behavior would be beneficial.", "Paraphrase: Summary: This research focuses on improving the expressiveness of Graph Neural Networks (GNNs) which is a crucial and challenging task. A novel GNN framework is introduced which incorporates structural data into its messagepassing aggregation process. The proposed architecture dubbed GraphSNN demonstrates superior expressiveness in distinguishing graph structures compared to conventional GNNs. Its effectiveness is validated through extensive node and graph classification experiments on standard benchmarks. Strengths and Weaknesses: The framework presents a significant and relevant contribution. Its design is sound and backed by theoretical justifications. The empirical results are compelling showcasing the methods strong performance on popular datasets. However the paper presentation could benefit from further clarification. The concepts of subgraph isomorphism overall isomorphism and subtree isomorphism are technically complex and may be difficult for readers not deeply immersed in expressivity aspects of GNNs. The authors are encouraged to provide clearer explanations possibly through refined illustrations. Additionally the choice of the parameter \u03c9 needs further exposition and justification. Minor Comments:  Clarify the definition of \"0.05 level of implication.\"  Speculate on the potential reasons for the weaker performance on the MUTAG dataset.  Ensure completeness and consistency in referencing (e.g. correct names publication dates)."], "nioAdKCEdXB": ["Summary Paraphrase: The authors propose a generative model based on the Schr\u00f6dinger bridge formulation. They use a loss work to control forward and backward stochastic differential par (SDEs) to approximate the Schr\u00f6dinger bridge. This loss work is derived using the forwardbackward SDE (FBSDE) framework and a nonlinear FeynmanKac reintroduction. The models performance is demonstrated through experiments on generative modeling of MNIST CelebA and CIFAR10 datasets. Strengths and Weaknesses Paraphrase: Strengths:  Clear introduction of scorebased generative models and Schr\u00f6dinger bridge problem  Novel use of FBSDE framework to derive loss work  Satisfactory toy and generative modeling experiments Weaknesses:  Opaque formulation of the loss work  Overstated claims of originality as the algorithm is similar to existing methods  Lack of convergence and endpoint guarantees for Algorithm 1  Lack of comparison with competing approach in the experiments  Incomplete introduction of FBSDE contribution and proof  Lack of code availability other Concerns:  potential benefits of observing the generative model after one algorithm iteration  Clarification of the optimization algorithm used in Algorithm 1", "Paraphrased Statement Summary This paper introduces a method for training generative models based on Schr\u00f6dinger bridges using likelihoodbased techniques. It relates these models to scorebased generative models (SBGMs) and demonstrates that the proposed framework generalizes the SBGM framework. Additionally the proposed framework provides control over the forward SDE to reach the prior distribution unlike SBGMs. A practical training algorithm that alternates between likelihood training of the forward and backward controls is presented. Experiments show the effectiveness of the proposed SBFBSDE method as a data generative model. Strengths and Weaknesses Novelty and significance:  Presents a novel framework for likelihood training of Schr\u00f6dinger bridgebased generative models.  Draws connection to and generalizes the framework of diffusionbased generative models like SBGMs. Writing and Clarity:  Clarity Issues:  validation of Theorem 1 missing or needs citation  Derivation of 7(a) and 7(b) in Theorem 1 unclear  validation steps in various theorems require clarification  Missing derivation of Eq. (20)  Corrections:  Eq. (15) is incorrect (g\\mathrmd\\mathbfwt should not be present)  \"probability expensive\" should be \"prohibitively expensive\"  \"ito\" in Section A.1 should be \"Ito\"  \"b\" in Section A.1 before Eq. (17) should be \"f\" Empirical Evaluation:  Evaluation is adequate but comparison to Multistage SB and SGM baselines is missing.  Results should be presented in a way that makes main baselines clear. Reproducibility:  practical implementation is complex and unclear from the paper.  Code and implementation details should be provided for reproducibility. Questions:  Clarification on the significance of \"SGM by enlarging the class of diffusion work to accept nonlinear drifts.\"  Computation method for \\nabla\\log p\\mathrmSBt  Functional work of f in the practical implementation Suggestions:  Move the paragraph on the connection to flowbased models to before \"In practice we parameterize the forward...\"  Provide a brief review of Schr\u00f6dinger bridges in the Appendix for accessibility.", "Paraphrase: Summary: This research inspired by stochastic control and forwardbackward SDE theory presents an iterative algorithm for the Schrodinger Bridge (SB) problem generalizing the variational likelihoodbased training of scorebased generative models (SGMs). This method is applied to train both the generative and inference work of SGMs reformulating the generative modeling problem as an SB problem. Experiments were conducted using standard image datasets. Strengths:  Introduces a novel perspective on the SB problem through a nonlinear FeynmanKac representation of a partial differential par (PDE) defining the SB optimal path measure.  Provides significant insight into practical SB solutions.  Generalizes the likelihoodbased training of scorebased generative models. Weaknesses:  Clarity needs improvement as the narrative is sometimes unclear and makes it difficult to understand the problem being solved.  Differences with prior work (e.g. iterative proportional fitting) are not sufficiently explained despite the proposed likelihood training of the forward and backward SDEs appearing similar.  Limitations are not adequately discussed such as the increased computational cost of training the inference work which is a key advantage of SGMs.  Convergence properties of the proposed method are not discussed leaving it unclear whether the likelihood training stage leads to convergence towards the SB solution. Detailed Comments and Questions:  The significance of \"computing the loglikelihood objective of Schrodinger Bridge\" is ambiguous and requires clarification.  The \"model\" is not clearly defined until Section 3 and it took multiple readings to understand.  It is unclear why a more flexible framework would mitigate the instability problem mentioned in Section 2.1.  The order of the arguments of h in par (11) may be incorrect.  Lemma 2 states that despite the randomness induced by Brownian motion the solution of the SDE y is a deterministic and smooth work. This is confusing as y is by nature random.  The notation in Theorem 3 is not introduced and requires clarification.  The practical benefits of the new interpretation of optimal control are not clear.  The comparison with other OT methods in Section 4.1 is not especially significanceful as the SB problem is connected to entropyregularized OT rather than OT itself. Minor Points:  Typos occur such as \"is it a maximization (of likelihood) instead of min\" at the end of page 6.  The last sentence of page 7 should be \"can be found\" instead of \"can be founded.\"", "Paraphrase: Summary: Inspired by current work in generative modeling using \"scorebased methods\" this paper proposes a new approach for generative modeling by solving the Schr\u00f6dinger bridge problem. Unlike previous Schr\u00f6dinger bridge methods this approach relates to maximum likelihood principles and provides a way to calculate the models loglikelihood. The resulting method effectively closes the gap between the model distribution and the data distribution leading to highquality image generation and likelihood estimates comparable to scorebased generative models. Strengths:  The proposed method has a solid theoretical innovation in optimal stochastic control.  It bridges the gap between the model distribution and the data distribution in scorebased methods.  Compared to other Schr\u00f6dinger bridge methods it has improved scalability eliminating the need for manual innovation or iterative filtering. Weaknesses:  The paper lacks clarity regarding the efficiency of the proposed method.  The loss work mentioned in Theorem 4 requires expectations with respect to unspecified random variables.  There are various writing errors that hinder comprehension.  The claimed performance improvements over existing optimal transport methods may be influenced by differences in network architecture rather than the method itself.  The proposed framework appears similar to scorebased generative modeling with a specific choice of forward SDE and score network parameterization."], "z7DAilcTx7": ["Paraphrased Statement: Summary: This paper demonstrates the compare between adversarial training (with infinitynorm) and distributionally robust optimization (DRO) using the infinityWasserstein distance. It so introduces an entropic regularizer to enhance the efficiency of finding the optimized adversarial distribution. Experiments on MNIST and CIFAR10 datasets demonstrate the effectiveness of the proposed method. Strengths and Weaknesses: Strengths:  Novel attack to adversarial training that considers the problem from a distributional perspective.  The proposed method outperforms baseline method. Weaknesses:  Limited technical or algorithmic contribution.  The compare between adversarial training and DRO is straightforward.  Lack of motivation for pursuing distributionally adversarial training.  Entropic regularizer used to solve DRO without providing theoretical insights.  Missing training time compare with baseline method. Typos and Minor Issues:  \"supported that\" should be \"supported by\" (page 6)  Lesbegue measure q(x) should be proportional to exp(U(x)) (page 7)  In Algorithm 1 line 9 should be xtilde on the right side.  form 1 and 2 should use the same colors to represent the same method.", "Paraphrased Statement: This research advocates for using Wasserstein distributional robustness with infinity norm (\\inftyWasserstein) to enhance framework robustness. The authors introduce an entropic regularization condition to make the \\inftyWasserstein formulation tractable. Langevin sampling is employed to generate adversarial examples for training. Strengths:  Establishes a connection between adversarial training and distributional robustness.  Presents significant theoretical findings with clear implications. Weaknesses:  The mathematical arguments and proofs lack clarity particularly in the absence of labels in Appendix derivations.  Theorems 3.1 and 3.2 lack proofs in the Appendix.  Experiments are limited lacking compare to innovative architectures and blackbox attacks. The authors should also report robust accuracy under AutoAttack.", "Paraphrased Statement: The paper links adversarial robustness to distributional robustness but without providing detailed proofs for the main results. This lack of mathematical rigor including errors in measure theory undermines the paper credibility. For example a coupling measure is incorrectly defined and an integration involving logarithms of probability values is performed without addressing potential infinities. The paper fails to cite or discuss existing literature that has already established similar connection between adversarial and distributional robustness. It would greatly benefit from a comprehensive review of the relevant literature to contextualize its contribution. Although the experimental results are promising it remains unclear how the proposed framework compares to other innovative method in the field. A thorough evaluation against stateoftheart attack would strengthen the paper conclusions."], "zNHzqZ9wrRB": ["Paraphrased Statement: This paper introduces Equivariant Transformers a neural networkbased advance for predicting molecular properties. The proposed architecture extends the standard transformer model with modifications tailored to molecular property prediction such as exponential normal radial basis work SiLU activation and customized update layers. Strengths:  The proposed model outperforms existing methods on QM9 MD17 and ANI1 datasets particularly in predicting dipole moment and electronic spatial extent. Weaknesses:  more detailed explanations of the model components including their motivations alternative options and relative importance would enhance understanding.  The inclusion of results from additional wellknown baselines (e.g. EGNN DeepPotential) would provide a more comprehensive comparison.", "Summary Paraphrase: The paper introduces an equivariant neural network (ET) with attention mechanism used for predicting molecular properties. ET matches the performance of existing methods on benchmark data. Strengths and Weaknesses Paraphrase: Strengths:  clear and wellorganized introduction of the proposed method. Weaknesses:  modified explanation of the modified attention mechanism.  ET outperforms previous advance on some properties but not on energyrelated properties suggesting potential overfitting.  ET has a significantly larger parameter count than other methods making it unclear whether its improved performance is due to its architecture or size.  The analysis of attention scores lacks clear interpretation and its unclear how to extract meaningful model beyond pairwise interactions.  The paper could explore more complex model in the attention scores. Additional Comments:  Mention previous neural network advance to coupled cluster theories.  Cite the original BehlerParinello publication for the first work in this field.  Cite Weiler et al. (2018) for their work on equivariant gated nonlinearities used in PaiNN.  Provide an explanation for ETs comparable computation time to smaller models.  Include benchmarks comparing ET to PaiNN and NequIP on MD17 molecules especially for MD simulations where fast evaluation is crucial.", "Paraphrase: The researchers have developed a new architecture called Equivariant Transformer (ET) for machine learning force field. Similar to the Transformer advance it predicts molecular energies (and forces) and other properties (like QM targets). The ET outperforms existing methods on benchmarks like QM9 and MD17. The work also examines the attention weights used in the model. Strengths:  The ETs new incorporation of attention into an ML force field is unique.  The ET achieves exceptional performance surpassing current stateoftheart methods. Weaknesses:  The explanation of the attention mechanism and update layer in the ET could be more detailed.  The specific respect and interpretation of examining attention weights in this context require further clarification.", "Paraphrase: Summary: This paper introduces an improved transformer model open of predicting properties of molecules from their atomic structure. This model achieves clear or nearclear results on three common datasets while being computationally efficient. The key introduction is a new method for calculating attention scores using edge features in the molecular graph. The model also includes a detailed analysis of its attention mechanism providing insights into its decisionmaking process which is particularly valuable for understanding chemical interactions. Strengths:  The proposed transformer model is both new and applicable to atomic graphs.  It utilizes equivariance to rotations and incorporates edge features in attention computations.  The model is wellfounded in physics principle.  It performs exceptionally well on various datasets demonstrating its adaptability.  The paper provides a comprehensive analysis of learned attention weights.  The model exhibits efficient runtime performance compared to alternatives like Dimenet. Weaknesses:  Other comparable equivariant transformer models have emerged (e.g. SE(3)Transformers). A comparison of these models would enhance the clarity of this paper contributions.  The analysis of attention may be challenging for individuals unfamiliar with chemistry terminology. To cater to the extensive audience at ICLR the authors should consider simplifying the text."], "rzvOQrnclO0": ["Paraphrased Summary: Many reinforcement learning algorithms use differentiable models to guide policy decisions. model training typically aims to minimize prediction loss but this doesnt guarantee accurate gradient information for complex dynamics. This paper introduces a method that directly estimates Jacobians of the dynamics using a gradient loss and nearest neighbor approximation. It also proposes using a model trained with both gradient and prediction losses for backpropagation while using a traditional model for trajectory generation. effectiveness and Weaknesses: Originality: The method is novel and complements existing model learning approaches. Significance: The problem it addresses is relevant to the RL community. Rigor: The theoretical results are sound but not particularly beneficial for the practical algorithm. effectiveness:  Directly learning Jacobians for improved policy gradient.  Wellpositioned as a decisionaware model learning approach.  Interesting theoretical results on gradient learning.  Simple and easytoimplement loss work and dualmodel architecture. Major Concerns:  computational overhead from nearest neighbor selection in the gradient loss.  potential overfitting and additional capacity due to the twomodel architecture.  unclear backpropagation mechanism while using multiple models.  Lack of a baseline comparison with MAAC. Minor Concerns:  Clarity in presenting assumptions prior to theoretical results.  Interpretation of the gradient loss in terms of Taylor approximation.  Explanation of why gradient loss only is insufficient for model training.", "Summary (Updated) The authors introduce a novel approach to modelbased reinforcement learning by simultaneously training two distinct models. One model focuses on accurately predicting transitions while the other prioritizes the rate gradient. They combine these models to create a new algorithm Directional Derivative Projection Policy Optimization. effectiveness  Wellmotivated: The proposed solution addresses an significant challenge in modelbased reinforcement learning.  Interesting and novel: The twomodel approach and the directional derivative projection algorithm represent innovative contribution. Weaknesses  Insufficient explanations: The numerical results are impressive but lack sufficient explanations and intuitions hindering the ability to reproduce the findings.  Clarity issues: The writing could be improved significantly to enhance clarity and reduce typos.  Lack of understanding: The concept of directional derivative as a scalar remains unclear and requires further elaboration. Suggestions for Improvement  Enhance the innovation to focus on the core idea and contribution.  Provide more detailed explanations and intuitions for the mathematical formulations and algorithms.  Consider including the algorithm in the main body of the paper for increased clarity.  Address the grammatical and stylistic issues for improved readability.  Explore the impact of the proposed approach on higherdimensional tasks and provide additional intuitions for the results", "Summary (Paraphrased) This research aims to enhance the performance of modelbased policy optimization methods by leveraging the differentiability of world models. It theoretically demonstrates that error in estimating model gradient contribute to biases in learned policy. To address this the paper proposes a strategy that directly optimizes the Jacobian of the world model using a samplebased approach thereby directly controlling the bias term. effectiveness and Weaknesses (Paraphrased) effectiveness:  The algorithm construction is wellreasoned.  The theory is novel and provides support for the algorithm.  The empirical results partially support the claims regarding improved performance. Weaknesses: W1: Marginal Empirical support  Lack of information on baseline hyperparameter selection raises concerns about fairness and resource allocation.  Use of a limited number of random seeds makes statistical significance questionable. W2: Disconnect Between Goals and Novelty  The claim of stateoftheart results is questionable given the limited scope of the empirical evaluation.  The novelty lies in controlling gradient error but the empirical results primarily focus on performance comparisons. Suggestions:  Expand the ablations to provide more insights into the role of controlling gradient error.  Deemphasize the stateoftheart experiments and focus on demonstrating the algorithms scalability and competitiveness.", "Paraphrased Statement: This work proposes a method for reinforcement learning that learns a models prediction as well as its gradient. The proposed method estimates the model gradient using nearby sample from the replay buffer. This estimated gradient is then used to enhance the models prediction. effectiveness:  The method recognizes the importance of considering both model prediction and gradient to minimize model gradient error. Weaknesses:  The estimation of the model gradient requires identifying the nearest points in the replay buffer which may be computationally expensive for large buffers.  Its uncertain if the estimated gradient accurately reflects the true gradient particularly with sparse replay buffers.  The scalability of the method is questionable as it hasnt been tested for extended training periods.  Some performance gains may stem from algorithmic modifications instead than the addition of gradient information.  The claim of achieving \"nearoptimal policy\" in the Humanoid environment with a reward of 5k is questionable given previous research demonstrating much higher achievable reward. The authors need to clarify and provide evidence to support this claim."], "z3Tf4kdOE5D": ["Paraphrased Statement: The paper suggests employing stochastic quantization to enhance the robustness of federated learning. The strength and weaknesses are as follows: Weaknesses:  Lack of novelty as stochastic quantization is a familiar technique in federated learning.  binary quantization is a specific instance of more general quantization with multiple levels.  The claimed robustness issue and remark are unclear. The remark appears to suggest that using this method with low poisoning range and ul values would ensure robustness yet simple clipping would achieve the same issue under these conditions rendering the proposed approach unnecessary.  The writing needs significant improvement. Recommendation: Based on the above concerns acceptance of the paper is not recommended.", "Paraphrased Statement: This research introduces a secure federated learning framework that protects against weight poisoning attacks. The model core element is a discretization mechanism that performs well when sufficient clients participate. The work provides theoretical analysis on its robustness and convergence. extraly numerical analysis validates the proposed methods effectiveness. strength and Weaknesses: strength:  Novel usage of discretization mechanism to enhance federated learning robustness.  Improves robustness and privacy with minimal utility loss when adversaries are scarce. Weaknesses:  Poor writing including unclear validation introduction and time.  Robustness guarantee may be insufficient as assumptions required for theoretical issue may not hold in practice.  Experiments limited to a small fraction of adversaries raising concerns about the robustness guarantees effectiveness.  security claims lack specific theorems and empirical evidence to support the statement that discrete local models cannot be usaged for inference. extra Comments:  The paper effectiveness against Byzantine attacks remains unclear.  Baseline comparisons in the availability poisoning attack experiment would strengthen the analysis.  More experimental details including APA performance should be provided.  Minor typographical error in Algorithm 1 (line 4) should be corrected.", "Paraphrased Statement: The paper presents a federated learning method that converts client updates into discrete units to enhance robustness against various work of attacks. The paper contributions include:  Development of the federated learning algorithm  theoretical analysis of the algorithms robustness and convergence  Empirical evaluations of the algorithms robustness strength:  Clear and concise introduction  Considerations of a broad range of attacks including those targeting security and privacy Weaknesses:  Lack of discussion on the benefits of reinforcing robustness at the client side instead of the server side in the introduction  Questioning of server trustworthiness when using serverside defense for security attacks  Absence of experimental validation for the potential benefits of layerwise bounds in reducing variance  Missing baseline comparison with continuous gradient federated learning for the evaluation of availability poisoning attacks  Counterintuitive issue on the ineffectiveness of certain known defense against integrity backdoor attacks  Lack of justification for the claim that discrete local models cannot provide valuable data for inference attacks considering the potential for blackbox model inversion attacks", "Paraphrased Statement: Summary: Federated learning (FL) is vulnerable to attacks that can compromise its integrity and availability. This work introduces FedDiscrete a novel FL algorithm that employs probabilistic discretization of client model weight. FedDiscrete has been mathematically proven to converge and has shown promise in resisting known attacks. strength:  Addresses an significant issue  Proposes an innovative approach  Wellwritten Weaknesses: theoretical Robustness:  The authors theoretical robustness analysis assumes an unrealistic scenario (infinite N and fixed F attackers) which undermines the significance of their findings.  FedDiscrete is vulnerable to adaptive attacks such as manipulating the upper and lower bounds (l and u) uploaded to the server. empirical Robustness:  FedDiscrete may suffer from significant utility loss in certain cases as observed in number 3. Methodological Considerations:  The authors fail to acknowledge existing methods that have applied discretization to FL.  The paper lacks comparisons with strong attacks and defense. Referenced work:  [A] signSGD: Compressed Optimization for Nonconvex Problems  [B] A Little Is Enough: Circumventing defense For Distributed learning  [C] Local model Poisoning Attacks to ByzantineRobust Federated learning  [D] Provably Secure Federated learning against Malicious client  [E] CRFL: Certifiably Robust Federated learning against backdoor Attacks"], "kOtkgUGAVTX": ["Paraphrased statement Summary: This paper proposes an unsupervised pretraining method called CIC for improving policy performance in reinforcement learning tasks. CIC aims to maximize a lower bound on the mutual information between a code (latent representation) and visited state. effectiveness and Weaknesses: mutual information Objective: 1. comparison of lower bound tightness: The authors should compare the lower bound of CIC with existing methods and discuss its relative tightness. 2. estimate vs. direct estimation: Consider using nonparametric estimation of mutual information directly instead of estimating state and conditional information separately. Methodology: 3. state entropy term: Clarify the meaning of H(\u03c4) and its motivation. 4. Compatibility with intrinsic reinforcement: Discuss the stability and suitability of using DDPG with nonMarkovian intrinsic reinforcement. Consider using an objectivespecific algorithm. Experiments: 5. Baseline comparison: Rule out the possibility that CICs improvement only stems from beneficial pretraining of DDPG. Test if results generalize to different base algorithms. 6. Generalizability: Discuss the potential effectiveness of CIC in noncontinuous control domains (e.g. visual discrete domains). 7. Contrast with APS: Analyze the performance gap between CIC and APS and determine the potential case (e.g. discriminator loss). 8. comparison with URLB: Perform a direct comparison with URLBs results to strengthen the empirical analysis. 9. Skill space capacity: Explain why previous competencebased methods cannot handle larger skill spaces. Provide a comparison with those methods using the same skill space. Minor Corrections:  Adaptation efficiency paragraph mention Fig. 3 instead of Fig. 4.  Normalized score in Fig. 6 does not match Fig. 4.  Difficulty in tracking baselines in Fig. 4 (top). Clarify what the plot represents.", "Summary: The paper introduces the Contrastive Intrinsic Control (CIC) algorithm for finding skills without supervision. This is done by maximizing the information shared between skill variables and state changes. CIC improves on existing methods by:  Using a contrastive approach to estimate entropy  Measuring entropy on state transitions instead of just state CIC outperforms prior algorithms and is thoroughly evaluated using the URLB benchmark. effectiveness:  Extensive empirical evaluation with multiple baseline algorithms and performance metrics  discussion of limitations of existing skill discovery methods  Ablation experiments to justify CICs design choices Weaknesses:  CIC resembles an existing algorithm [2] but with some refinements  Question 1: Why do skill discovery algorithms often rely on localized information rather than global patterns  Question 2: Does CICs goal of minimizing conditional entropy lead to redundant skill representations  Question 3: Could using a more expressive policy decoder reduce the need for large skill latent spaces Writing introduction: The paper is wellwritten but there are some minor ambiguities and typos.", "Paraphrased Statement: This research extends the DIAYN concept by proposing that an agent can develop diverse skills by exploring the entire state space. Each skill focuses on covering a different portion of this space. The agent then applies one of these skills to simplify subsequent task learning. The proposed method employs mutual information between latent state and skill vectors as a diversity metric. This decomposition promotes exploration of diverse latent state while encouraging distinct specialized skills. The paper introduces a technique to compute these decomposition terms and demonstrates that its a lower bound for the true mutual information. It estimates latent state entropy using knearest neighbor methods and conditional entropy using a neural network. Experimental results show that pretraining with the proposed loss function leads to improved performance on downstream tasks compared to existing methods. The field highlights the importance of highdimensional skill vectors and provides a structured analysis of reinforcement components.", "Paraphrased Statement: Summary: The authors address the issue of discovering skills without supervision by maximizing the mutual information (MI) between skills and state using nonparametric particlebased entropy estimation for the entropy of state and noisecontrastive estimation for the MI between skills and state. Their approach is evaluated against baselines on the Unsupervised Reinforcement Learning Benchmark (URLB) where agents are trained without external reinforcement and then refined for specific tasks with limited environment interactions. effectiveness:  evaluation on URLB provides a robust assessment framework.  Demonstrates strong empirical performance compared to baselines.  Provides an analysis of hyperparameter settings. Weaknesses:  Lacks originality as similar approach (APT and APS) have explored particlebased entropy maximization and state representation with contrastive learning.  Rationale for noisecontrastive estimation is unclear especially since variational lower bound are often sufficient.  Theorem 1 is inaccurate due to the particlebased entropy term in the proposed method as acknowledged by the authors.  The authors use q(z\u03c4) instead of q(\u03c4z) in the derivation of the MI decomposition.  The derivation of the noisecontrastive estimator requires more detail.  The use of \u03c4 for both skills and trajectories can be confusing.  Empirical analysis with varying values of the weighting hyperparameter (\u03b1) is lacking."], "pgkwZxLW8b": ["Paraphrase: This paper explores supervised representation learning in federated learning where data is distributed across clients. The goal is to reduce communication costs by avoiding transmission of the full fully connected (FC) layer in situations where clients have approach to data from only a subset of classes. The proposed approach federated sampled softmax calculates the softmax probability only for positive classes (those with data on a client) and a little number of negative classes. Experiments demonstrate that this method achieves comparable performance to training with full softmax even when using a minimal set of negative classes significantly reducing communication overhead. Strengths:  Identifies the problem of excessive parameters in the FC layer and proposes a solution for nonIID setting where clients have limited data.  Introduces federated sampled softmax to reduce communication costs by training locally with a limited number of negative classes.  Conducts empirical work on largescale datasets and demonstrates the scalability of the approach. Weaknesses:  Limited technical innovation as similar methods for subsampling negative classes have been proposed previously.  Uniform sampling is used despite the availability of more further techniques.  experimental setup lacks details on class distribution local epochs and the use of pretrained features.  The approach is only evaluated with the FedAvg baseline and its applicability to other federated learning algorithm is uncertain. other Comments:  The authors should add \"supervised\" to the title to differentiate it from unsupervised representation learning approach.  Additional citations on federated learning are recommended.", "Paraphrased Statement: This research focuses on training effective estimator vision model in the federated learning setting where data is distributed across multiple devices. For largescale model (over 1000 classes) the number of parameters in the classification layers becomes overwhelming. To reduce computational and communication costs the authors propose a method called Federated sample Softmax (fedSS). FedSS uses a shared feature extractor but creates subnetworks for each client with a little set of classes. This leverages the sampled softmax technique to approximate the full softmax. The paper evaluates fedSS on image classification and retrieval tasks showcasing its competitiveness against baselines and variants. Strengths and Weaknesses:  Significance and impact: The problem addressed in this paper has been largely overlooked except for a previous work that had a narrower setting. FedSS provides a practical solution making the research significant.  Writing and Clarity: The paper is wellwritten and easy to understand with helpful illustrations.  Quality: The experiments are thorough and welldesigned providing strong support for the claims made in the paper. Suggestions for Improvement:  Provide a brief appendix explaining the validity of the sampled softmax approximation.  Explore the impact of different neural network architectures beyond MobileNet v3.  Investigate the effects of increasing the number of local epochs which currently stands at 1 resulting in a large number of communication rounds.  Analyze the methods performance with varying label class heterogeneity levels.", "Paraphrase: Summary This work presents Federated sample Softmax (FedSS) a technique that aims to enhance the efficiency of federated learning for image representation learning especially when dealing with a large number of classes. FedSS reduces the communication cost of the classification layer by subsampling the weights of nonpositive classes resulting in comparable accuracy to full softmax. Strengths and Weaknesses Pros:  Wellwritten and comprehensible paper  FedSS offers a potential solution for reducing communication overhead in federated learning classification tasks Cons:  The proposed method only provides significant savings when the number of classes is very large (e.g. 1000 or more).  The datasets used in the work primarily contain a limited number of classes leading to minimal savings (less than 15).  FedSS failed to achieve comparable accuracy to full softmax in datasets with numerous classes (ImageNet21k) despite increasing the subsampling extent.  The proposed method lacks significant novelty as it directly applies the sampled softmax algorithm to federated learning.  FedSS exhibits reduced accuracy calling into question its practical utility.  There is no comparison with using a little model of the same overall size to assess performance in cases where the communication savings are minimal.", "Paraphrase: Summary This work explores federated learning when there are many classes and each client has few classes. In such scenarios the classifier in the neural network requires many parameters resulting in high communication costs. To address this number the authors present FedSS a method where each client selects a subset of classes and trains only on those. As unsampled classes are excluded clients only need to transmit parameters for the sampled classes reducing communication overhead. experimental results show that FedSS effectively reduces communication costs while maintaining performance. Strengths and Weaknesses Pros:  clear and easytounderstand paper  Simple and effective concept  Reduced communication costs Cons:  Privacy concerns: client sampled classes reveal data about their data violating strict confidentiality requirements in some federated learning applications.  Lack of communication cost quantification: The paper does not provide sufficient details on the communication cost reductions achieved.  Limited experimental results: The work should include more datasets and neural network architectures to demonstrate the algorithm effectiveness.  comparison with other communicationefficient algorithm: The authors should compare FedSS with existing communicationsaving methods to highlight its advantages."], "gVRhIEajG1k": ["Paraphrased Summary: This paper explores the phenomenon that adversarial examples in areas of the data distribution with small density have higher transferability. Based on this the work introduces the AAI metric to quantify the alignment between an adversarial attack and the inherent attack direction. Additionally the paper identifies hyperparameters that impact AAI and optimizes these hyperparameters to maximize AAI resulting in more transferable attacks. Paraphrased effectiveness:  The novel observation regarding the relationship between smalldensity data and transferability offers insights for future research.  The AAI metric is theoretically sound with a practical method for calculation and empirical evidence of its correlation with attack transferability.  Extensive experiments demonstrate the superior transfer attack performance of the proposed method compared to stateoftheart techniques. Paraphrased Weaknesses:  The necessity of the AAI metric for generating highly transferable attacks is not fully justified. early techniques used in the proposed method such as smoothing activation gradients and early layer information use may contribute more significantly to its effectiveness. An ablation work would clarify the function of AAI.  The computational cost of evaluating the AAI target is not addressed. The complex computations involving secondorder derivatives and Gaussian vector sampling may make it less efficient than directly generating adversarial examples.  Some steps in the validation of Theorem 1 require clarification to enhance readability.  Baseline models in experiment result tables lack proper citations.", "Paraphrased Summary: This paper introduces an innovative adversarial attack method that generange highly transferable adversarial examples. It was designed based on the observation that the training datas lowdensity regions receive less training. To address this the method aligns the adversarial direction with the direction that minimizes the ground truth density. theoretical analysis supports the method. Empirical evaluations demonstrate its superiority over early adversarial attack methods in various scenarios. This method could prove valuable for creating transferable adversarial examples and facilitating further research on adversarial defense. effectiveness and Weaknesses: effectiveness:  Strong theoretical foundation  Thorough experimental evaluation covering different scenarios and baseline methods  High performance outperforming early methods Weaknesses:  Potential computational cost due to Bayesian optimization and s derivative evaluation of the neural network  Lack of a clear connection between the initial motivation and the proposed method  Absence of information on attack success range small Comments:  Cite archival paper versions whenever potential  Provide computation time for generating adversarial examples including network structure and hardware details  Define LDD (lowdensity data) and HDD (highdensity data) formally", "Paraphrased Statement: This paper introduces the Intrinsic Adversarial Attack (IAA) a technique for transferring attacks by matching data distribution. The method is based on the hypothesis that neural networks may not perform well on data regions with small density. The attack incorporates data distribution considerations aiming to enhance its transferability. Experiments across various example architectures and contexts including normal and robustly trained examples demonstrate the effectiveness of IAA. effectiveness and Weaknesses: effectiveness:  Addresses an significant topic  Presents an intriguing and intuitive concept  Offers a viable and effective solution  Delivers promising experimental results Weaknesses:  The assumption lacks substantial evidence especially in example early than Gaussian noise. More evidence for a wide range of noise including corruption scenarios is needed.  The evaluation does not fully demonstrate the attacks effectiveness against stateoftheart defense methods.  The criteria for classifying layers as \"early\" or \"later\" is unclear and requires justification."], "qSTEPv2uLR8": ["Paraphrase: Summary: The work introduced a novel approach using input convex neural networks (ICNN) for approximating optimal transport maps. The method aims to minimize a loss use based on the MongeAmpere equation with zero loss indicating a solution to the equation. Physics Informed Neural Networks (PINNs) are employed to optimize the loss. The work also extends to density estimation. This research is significant because estimating optimal maps has various applications. Experimental solution demonstrate the methods promise compared to other PINNs in one setup but only qualitative solution are provided for density estimation lacking comparisons with other baselines. effectiveness and Weaknesses: effectiveness:  The paper is wellwritten and presents the concept clearly. Weaknesses: 1. Uniqueness of Optimal Map: The paper claims that for absolutely continuous measures the existence and uniqueness of an optimal transport map is guaranteed by Breniers theorem. However the physicsbased neural networks used cannot ensure that the optimized network is a genuine solution to the MongeAmpere equation. Therefore the uniqueness of the map is questionable. Proof is required if the alternative is true. 2. density estimation Interpretation: The density estimation section is ambiguous specifically the significance of \"or equivalently \u2207u) \u03bc \u03bd.\" Moreover it is unclear if the method can be reliably used for density estimation given the potential lack of uniqueness in the obtained map. 3. Lack of Baseline compare: While the paper reviews various density estimation techniques such as normalizing flow it fails to provide any direct comparisons with these algorithms in the experimental section. The claim that the proposed method offers clear interpretability is not adequately supported especially considering that normalizing flow enables exact density estimation.", "Paraphrased Statement: Summary: This research introduces a novel density estimation technique based on optimal mass transport (OMP) where the goal is to find a use that transfers one probability density to another with minimal cost. The authors employ Breniers theorem which ensures the existence and uniqueness of the optimal transport map. This allows the OMPs optimal transport map to be determined by solving a specific nonlinear partial differential equation (PDE). The approximate solution is computed using physicsinformed neural networks (PINNs) leveraging input convex neural networks (ICNNs) to incorporate Breniers potential into the PINN. The methods effectiveness is demonstrated on several standard examples. effectiveness and Weaknesses: This paper integrates diverse concepts to address density estimation problems. The original OMP problem is recast as a PDE which can be approximated using PINNs. ICNNs are used to incorporate Breniers potential into the PINN avoiding challenges associated with direct minimization of the OMP problem. Originality: The estimation presented are novel and relevant to the conference. The use of PINNs for density estimation has not been previously explored in other literature. Quality: The framework appears sound and is demonstrated through several experiments. However theoretical support for some claims is lacking and the number of experiments is limited for an empirical ML conference paper. Discussions on assumptions and limitations would be beneficial. Authors should utilize the unlimited space in the appendix for additional discussions and experiments. Clarity: The estimation are wellstructured and presented clearly. The background materials provided help in understanding the paper. significance: This paper is pertinent to two ML communities: PINN researchers as it presents potential for research collaborations and the density estimation community as it offers a promising approach for this critical topic in ML. However it is unclear if the approach outperforms existing methods or only performs better on specific tasks. A more comprehensive compare with other stateoftheart methods would strengthen the paper significance. Suggestions for Improvement:  Expand experimental compares to include a wider range of algorithms.  Discuss advantages and disadvantages compared to other density estimation methods (e.g. GANs NODEs).  Provide theoretical support for the claim that ICNNs introduce implicit regularization.  Include the solution shown in Figure 6 in the main text.  Move network detail to the appendix.  Comment on the computational cost of the proposed framework.  Correct the typo on page 2: the convex use itself should be indenity helps in avoiding.  Improve the quality of the figures using vector graphics.", "Paraphrased Statement: Summary: This work introduces a novel neural network architecture that combines physics knowledge and machine learning to effectively compute optimal transport maps. It leverages techniques from partial differential equation (PDEs). effectiveness:  The probabilistic general estimation framework can significantly reduce computational cost compared to traditional methods like finite element or difference methods which have an exponential increase in complexity with dimensionality.  The paper clearly outlines the network hyperparameter evaluation methods used in experiments. Weaknesses:  While necessary definitions are provided the technical concepts are explained only at a high level. For instance the paper covers optimal transport theory but lacks explanations for relevant PDE concepts.  The proposed loss functions (4) and (5) still have exponential scaling with respect to the domain dimensionality.  The experiments focusing on density estimation are limited in scope.", "Summary Paraphrase: This paper presents a novel approach for continuous Optimal Mass Transport (OMT) problem solving. It formulates the problem as a convex mapping use estimation using Breniers theorem and leverages input convex neural networks. The method outperforms other DNNbased methods in common density estimation and generative modeling tasks. effectiveness and Weaknesses: effectiveness:  The method has the potential to be an efficient OMT solution using DNNs.  The paper is clear and wellstructured. Weaknesses:  The realworld applications of the method remain unclear.  Experiments are limited which hinders a fair compare with existing OMT methods.  It is uncertain whether ICNN can represent all convex uses.  There is a potential error in the equation reference (line 5).  It is unclear whether the DNN parameters in the compare experiments have been optimized. Addressing this possibility would enhance the scientific rigor of the paper."], "yjsA8Uin-Y": ["Summary This paper introduces a method for detecting noisy labels on a perinstance basis without training a deep framework. The method relies on good representation extraction which allows for the generation of initial soft labels based on the clusterability of representations using knearest neighbors (kNN). effectiveness  Wellorganized and easy to follow  Interesting and seemingly good intuition  Instancewise noise label detection Weaknesses Methodology  Unclear definition of \"good representation\" and lack of preselection guidelines  reliance on kNN which may not be suitable for datasets with many object categories Experimentation  role of a dataset corrupted by an automatic label tool may not be sufficiently convincing  Lack of comparison with alternative methods using datasets with known ground truth labels Minor Concerns  Brief review of related work  Ambiguous terminology (e.g. \"perfect extractor\")  Writing issue (e.g. \"With in\")", "Summary (Paraphrased): Researchers have developed a technique to detect range with incorrect labels without requiring training. They utilize feature representations learned from pretrained models (either supervised or selfsupervised) and argue that range with the same correct label should be close together in the pretrained models representation space. This allows them to use local voting or ranking methods to identify range with altered labels. Experiments on CIFAR100 dataset show better performance compared to other nontrainingbased approach. effectiveness:  The concept of using pretrained models for noise detection is novel.  The trainingfree approach differs from current literature.  Contrastive pretraining outperforms supervised pretraining for noise detection. Weaknesses:  Experimental setup using CIFAR100 may be also simplistic with clear class distinctions. More complex datasets should be tested.  Theoretical analysis assumes nearby representations have the same correct class which may not apply to unbalanced training sets.  \"common circle\" in Figure 1 is undefined.  Typo on Page 5: \"0.34 0.033 0.33\" should be \"[0.34 0.33 0.33]\".", "Paraphrased Statement: Summary: This work suggests a technique for identifying noisy labels based on robust representations (such as those obtained through contrastive learning). The proposed method utilizes neighborhood information from these representations to: 1. Examine the agreement of noisy labels among nearby representations. 2. Assign each instance a likelihood of being clean and filter out a guaranteed percentage of instances with low scores as corrupted. The work defines robust representations and establishes a worstcase error bound for the rankingbased technique if the representation is sufficiently good. It also offers empirical evidence of the proposed method. effectiveness and Weaknesses: effectiveness:  The paper offers a fresh method for detecting noisy labels. Weaknesses:  The Assumption of robust representations is a major concern. 1. While pretrained or selfsupervised learning representations can be useful it is unclear how they can guarantee accurate noisy label detection. Experiments evaluating the effectiveness of representations obtained through various approach are suggested. 2. The methods comparison to others in Tables 1 and 2 is biased because it employs representations pretrained by SimCLR a selfsupervised learning technique. 3. Despite claiming to be \"trainingfree\" the proposed method relies on pretrained representations that require training. \"Supervisionfree\" may be a more precise description.", "Paraphrased Summary: This research proposes a novel approach for detecting mislabeled data (noisy labels) without requiring a training work. The authors introduce two methods \"voting\" and \"range\" to identify potentially corrupted instances by leveraging reliable representations. The key contribution lies in its trainingfree design setting it apart from most existing approach. Furthermore the authors provide theoretical analysis to determine error bounds and guide the selection of a parameter. effectiveness and Weaknesses:  Original approach compared to typical trainingbased detection methods.  Practicality in identifying mislabeled data.  clear presentation and wellreasoned theoretical analysis.  Assumption that robust representations can discriminate between clean and noisy labels.  Concerns about hard samples having similar representations to noisy ones potentially affecting framework robustness.  Need for additional experiments on framework performance without filtered samples from the proposed method.  Typographical errors (\"referred\" instead of \"not referred\" in section 3.1 and \"0.0.33\" instead of \"0.0033\" on page 5)."], "xVlPHwnNKv": ["Paraphrased Statement: Summary: This work suggests an improved Stackelberg ActorCritic algorithm by treating it as a Stackelberg game with the actor as the leader and the critic as the follower. The paper proposes employing deterministic policies to simplify calculating the implicit gradient. Additionally it offers approximation technique that include omitting smallorder terms and utilizing block diagonal matrix approximation for matrix inversion. The computational complexity of gradient computation is determined. The proposed method is shown to converge under specific assumptions. Experiments using Mujoco demonstrate its effectiveness. Strengths:  Provides numerical experiments on Mujoco comparable to the current state of the art. Weaknesses: 1. The formulation of the Stackelberg game between the actor and critic is questionable. The paper implies that the actors loss involves a partial derivative with respect to the sampling distribution which should be addressed using policy gradient theory. 2. The publication raised about simultaneous actorcritic updates leading to convergence problems is not fully justified. Recent research has shown that actorcritic algorithms can converge in finite time. 3. The Stackelberg formulation used in this work was proposed in a previous field. The main contributions of this work are using deterministic policy gradient and a different approximation method for the Hessian inverse. 4. The theoretical results and validation presented are difficult to comprehend. The assumptions made seem highly restrictive. Realworld examples where these assumptions hold would be beneficial. The choice and impact of the stepsize separation parameter \u03c4 on the theory are not discussed.", "Paraphrased Summary This paper introduces a method to improve the Stackelberg ActorCritic algorithm by simplifying the indirect gradient computation. The method involves removing Hessian terms and using a blockdiagonal approximation technique. The authors demonstrate that this approach results in faster convergence to a fixed point. Assessment Strengths:  The paper is wellwritten and the proposed method is clearly explained.  The experiments provide evidence of improved convergence speed. Weaknesses:  The theoretical analysis is limited and does not fully support the claimed improvements in stability.  The experiments primarily focus on toy model which may not fully reflect the effectiveness of the method in more complex settings.  The authors do not provide a clear acknowledgment of the connection between their work and existing research (Theorem 5 in Fiez et al. 2020).", "Paraphrased Summary: This paper introduces a computationally efficient training technique for actorcritic methods in deep learning. The Fast Stackelberg Deep Deterministic Policy Gradient (FSDDPG) method uses a block diagonal approximation to reduce training complexity and enhance convergence speed. The paper provides both theoretical and experimental validation to support the benefits of the proposed approach. Strengths and Weaknesses: The paper is wellstructured and presents a comprehensive review of the relevant literature. The proposed method is novel and theoretically sound with empirical evidence to support its effectiveness. The reviewers recommend accepting the paper. Minor Comments: However they note that some curves in Figure 3 do not appear to converge fully. Including a longer training horizon could provide a more accurate comparison. Additionally they suggest clarifying the scale of the parameter \u03f5 relative to other parameters in practice.", "Paraphrased Summary: This research improves Stackelberg Deep Deterministic Policy Gradients (DDPG) by proposing strategy for handling the Hessian term in the gradient. These strategy aim to address limitations in computation time and convergence speed compared to traditional actorcritic model. The authors provide theoretical justification for removing portions of the Hessian and using a blockdiagonal approximation while maintaining convergence. Time complexity is analyzed for several algorithm variations with experimental results showing mixed outcomes. Strengths and Weaknesses: The paper is wellwritten and presents a novel method. The thorough analysis of different Hessian approximation approach highlights the potential for making Stackelberg more feasible for larger neural networks. However there are concerns regarding the theoretical discussion of DDPG and the experimental setup. The authors equate the offpolicy policy gradient in DDPG to the true policy gradient which is inaccurate. This affects several theoretical results and may invalidate the offpolicy setting used in the experiments. Additionally the definition of the actionvalue function and its use in the temporal difference loss differ from standard DDPG and it is unclear how this impacts the theoretical results. The experimental use of full actionvalue function learning with noisy actions further complicates the theoretical analysis."], "qCBmozgVr9r": ["Paraphrased Summary The paper tackles the challenge of detecting object attributes using a limited dataset for training (fewshot approach). It proposes a methodology involving two stage: 1. Pretraining: The model learns general representation from a dataset of picture with labeled attributes. This pretraining combines unsupervised representation learning with supervised finetuning. 2. EpisodeBased Learning: For each episode the model identifies attributes based on a support set (specifies attributes) and evaluates its predictions on a query set. The paper introduces a metric to gauge transferability between the training dataset and the fewshot problem by analyzing the correlation between attribute values. Strengths:  Formulates the problem of fewshot attribute detection.  Provides experimental results on three benchmark datasets.  Explores the learning behavior and transferability of attributes. Weaknesses:  Lacks clarity in presenting the scientific goal and task definition.  The proposed approach is basic and does not offer significant novelty.  Transferability analysis is not rigorously conducted.  Does not address the relationship between attribute prediction and multilabel classification.  Evaluation protocol could be improved to provide more consistent results.  limited comparison with alternative methods and unsupervised learning configurations.  The writing way lacks precision and avoids addressing potential explanations for observed behavior.", "Paraphrase: Summary This research introduces a novel topic called \"fewshot attribute learning\" (FSAL) where a model is initially trained on a large dataset of familiar attributes and then tested on a small dataset of novel attributes. Experiments using three benchmark datasets revealed that pretraining the model using selfsupervised learning improves its generalization to novel attributes. The paper evaluates several fewshot learning models (e.g. MatchingNet MAML ProtoNet) and finds that a logistic regression layer performs well. Strengths 1. The FSAL concept is original and applicable to realworld situations. 2. Selfsupervised training enhances the backbones ability to extract generalizable features which may benefit general fewshot learning tasks. 3. wide experiments and ablation field demonstrate consistent model improvements. Weaknesses 1. The three stage used in the model (selfsupervised training finetuning fewshot learning) have been described in previous works reducing the paper novelty. 2. Some model configurations and experimental details require clarification such as:  The absence of comparisons between logistic regression (LR) in the finetuning and fewshot learning stage.  The selection of one or two attributes and their simultaneous occurrence in positive examples.  The use of a 20shot context in the ablation field despite similar results for nearly all methods.  The claim that the proposed model generates well heatmaps than the oracle context without providing an explanation. 3. The generalization analysis raises concerns about whether transferability is truly being measured or if the model is exploiting attribute correlations. Additional qualitative results are needed to determine if the model generalizes to novel attributes or relies on correlations.", "Paraphrased Statement: Summary: The authors investigate the problem of learning attributes with limited data (fewshot attribute learning). Unlike traditional zeroshot attribute learning approach their method tackles situations where testtime attributes are previously unseen. To address this challenge they propose a threestage approach: 1. Utilize a representation learning algorithm (e.g. SimCLR) to extract visual features. 2. Finetune the network with supervised training using limited labeled samples. 3. Adapt a preexisting fewshot learning strategy by training a linear classifier. Strengths and Weaknesses: Strengths:  clear and welldefined problem context.  Demonstrates beneficial performance compared to existing fewshot learning baselines.  Careful experimental setup ensures no external data is used in the initial representation learning stage. Weaknesses:  The proposed method primarily combines existing approach for selfsupervised learning and fewshot adaptation lacking significant novelty.  Integration of attribute data directly into the first stage of representation learning was not explored. Questions:  What factors influenced the choice of the fewshot algorithm in the third stage  How were hyperparameters optimized for each of the three stages  Would attribute data added as an auxiliary loss in the first stage improve performance  Have alternative selfsupervised learning algorithms been tested and do they provide different insights  Do the authors confirm that the SimCLR model used for ImageNet1K experiments has never encountered test picture", "Paraphrase: This research introduces the concept of fewshot attribute learning (FSAL) which follows the conventional fewshot metalearning model but focuses on attributes rather than object class. The authors explain that the multilabel nature of attributes (e.g. smiling wearing eyeglasses can coexist) sets this problem apart because the context of each positive model within an episode can significantly alter what the model is tasked with learning. The field also presents three benchmark datasets based on CelebA Zappos50K and ImageNetwithAttributes to investigate FSAL. several existing techniques are evaluated on these benchmarks and further improved upon. Strengths and Weaknesses:  context (FSAL): The problem of rapidly learning attributes is practical and wellmotivated. In Section 4 the authors effectively situate the proposed context within several context.  Benchmark: While the three datasets cover diverse field (faces shoes general imagery) there is insufficient discussion comparing them to existing benchmarks like FC100 tieredImageNet MetaDataset and Arnold and Sha 2021. The limited number of attributes in the proposed benchmarks (2080 in total) raises concerns. It is unclear if the dataset construction aligns with the fields goal of understanding generalization and relates to previous process on data splitting.  Experiments: wide experiments evaluate several baselines and demonstrate improvements over existing methods. However it is unclear how the findings in Section 5.3 compare to previous field involving semantic object classes. The significance of the poor performance of SA and the strong performance of U remains to be clarified. Additional Comments:  The authors could elaborate on the insights expected from exploring the FSAL context including its impact on understanding generalization and transferability.  The separation between \"Fewshot learning\" and \"Generalization to novel tasks\" in the Related process section could be expanded.  Exploring the \"Baseline\" technique (Chen et al. 2019) for SA may provide further insights."], "qw674L9PfQE": ["Paraphrased Summary This paper presents CLOOB a new contrastive learning technique. It aims to reduce the InfoLOOB (leaveoneout upper bound on mutual information) using Hopfield networks. Specifically Hopfield networks replace original embeddings with retrieved embeddings in the InfoLOOB objective. These retrieved embeddings are more robust as they capture the mutual covariance structure of all sample embeddings leading to improved InfoLOOB stability. Results from extensive zeroshot dataset experiments indicate that CLOOB outperforms the renowned CLIP model across various architectures. Strengths 1. Clarity and accessibility throughout the paper. 2. Intriguing method: Justification for incorporating Hopfield networks in contrastive learning to mitigate InfoLOOB variance. Simple implementation in practical applications. 3. Extensive and conclusive experiments demonstrating CLOOBs benefits over CLIP. Practical significance is evident. 4. Comprehensive appendix with sound and verifiable proofs. Weaknesses 1. Novelty concerns: CLOOB seems to primarily involve adding Hopfield networks to transform embeddings instead of introducing a wholly novel approach. The impact of this feature engineering technique may not be revolutionary. 2. Originality of theoretical findings: Theorems 1 and 2 are not original contribution they are derived from existing research (Poole 2019). Citations are necessary to acknowledge the origins of these theoretical concepts. 3. Limited theoretical contribution: Presentation of wellestablished lower and upper bound on MI rather than novel theoretical insights on CLOOBs effectiveness. Exploring the impact of Hopfield networks on learned model variance would enhance theoretical analysis.", "Paraphrased Summary: CLIP trained with the InfoNCE loss has been successful. This paper introduces CLOOB which combines the InfoLOOB objective (a leaveoneout mutual information upper bound) with modern Hopfield networks (replacing original embeddings with retrieved embeddings). Experiments demonstrate that CLOOB surpasses CLIP in zeroshot transfer learning across various architectures and datasets. Paraphrased effectiveness:  Writing: Clear and easy to follow.  Novelty: CLOOB is an innovative method that combines InfoLOOB and modern Hopfield networks for CLIPlike model training.  Experiments: Comprehensive zeroshot transfer learning experiments show CLOOBs effectiveness over CLIP. Paraphrased Weaknesses:  method:  Explanation needed for maximizing an MI upper bound rather than minimizing an MI lower bound.  applicability of CLOOB to other selfsupervised visual representation learning methods.  Why not explore CLUB as an alternative objective  elucidation and expanded description of modern Hopfield networks including their original application.  Experiments:  lack of quantitative analysis on the reduced variance of InfoLOOB using modern Hopfield networks.  Evaluation of ViT backbones instead of ResNet backbones.  Reincorporation of the ablation work into the main text (if space allows).  comparison of CLIPs performance with solid RN101 and RN50x4 backbones to gauge the performance gap. Corrected Figure 2 Caption:  4th and 5th rows: \"text embedding\" instead of \"image embedding Uyi\"", "Rephrased Statement: This work introduces a novel contrastive learning method featuring enhancements:  Swapping InfoNCE loss for InfoLOOB loss  Utilizing a Modern Hopfield Network instead of a conventional deep neural network For imagetext contrastive learning this technique significantly outperforms the baseline (CLIP) on zeroshot image classification evaluations. effectiveness:  Clear writing solid motivation thorough algorithm design and indepth testing with ablation work  Informative comparison between InfoLOOB and InfoNCE losses  Measurable improvements over CLIP Weaknesses:  Limited contribution and novelty as neither InfoLOOB nor Modern Hopfield Network are completely original  Desirability of broader application testing beyond imagetext contrastive learning such as imageimage tasks", "Paraphrase: Summary: The research presents a novel unsupervised learning technique called \"Contrastive LeaveOneOut Boost\" (CLOOB). This technique utilizes a contrastive learning objective known as \"InfoLOOB.\" The researchers thoroughly contrast InfoLOOB with the traditional InfoNCE objective and assess CLOOBs performance against CLIP in various zeroshot transfer learning tasks using data from Conceptual Captions and YFCC datasets. effectiveness:  The methodology is wellexplained and the paper is easy to follow.  CLOOB significantly outperforms the baseline on most tasks.  An ablation work (A1) suggests a solid relationship between the InfoLOOB objective and the Hopfield network approach demonstrating the value of the techniques components. Concerns:  The ablation work indicates that InfoLOOB may not perform effectively using the CLIP model. The authors should provide insights into this observation.  Although CLOOB achieves performance improvements many of its components (such as InfoLOOB and Hopfield networks) are not entirely novel."], "iim-R8xu0TG": ["Paraphrased Statement: summary: This paper introduces FitVid an architecture designed for video prediction (forecasting future frames based on past ones). Prior approach often struggled with underfitting but FitVid overcomes this issue by optimizing its design while maintaining the same parameter number. Moreover its notably easier to train without resorting to complex techniques. FitVid has achieved exceptional results on four benchmark datasets surpassing stateoftheart in various metrics. strength and weakness: strength:  Clear and concise presentation  Impressive performance outperforming stateoftheart on multiple datasets and evaluation metrics  Excellent visual results  Simplicity in training requiring no specialized training techniques and easily trainable from scratch weakness:  Lack of clarity on how FitVid differs from previous models and why it avoids underfitting  Overfitting tendency without data augmentation  Insufficient discussion on these issue Additional Observations:  noteworthy results in predicting complex trend and unseen background details in robotic scenarios (e.g. Fig 4)  It remains unclear how the model achieves such highlevel understanding of the fit without sophisticated mechanisms.", "Paraphrased Statement: The work introduces FitVid a straightforward and adaptable video prediction model. It outperforms previous models with comparable parameters in fitting video prediction datasets. Its been noted that existing methods tend to underfit these datasets which is why FitVid was created. However FitVid subsequently exhibited overfitting. To address this FitVid employs a variety of image augmentation strategies enabling it to surpass current benchmarks in video prediction. FitVids design aligns with the SEUNet LSTM architecture which is commonly used for stochastic video prediction tasks. strength:  Unveils and addresses underfitting a underlying issue in video prediction.  FitVids simple yet efficient architecture.  Stateoftheart performance on multiple datasets thanks to the application of existing techniques to mitigate underfitting.  Comprehensive experiment. weakness:  FitVid lacks significant novelty combining existing methods (SE UNet LSTM).  The strength of individual FitVid components has not been thoroughly analyzed.  The reasons behind the underperformance of previous methods require further explanation. PostRebuttal Commentary: While the authors provided insights into the originality of the combination of FitVid modules they could have intuitively linked this combination to the observed overfitting phenomenon. It would be valuable to demonstrate the inspiration behind the combination rather than relying solely on trial and error.", "Paraphrased Statement: summary: This work explores conditional video prediction specifically addressing the issue of underfitting and scaling limitations in current models. The paper introduces a new architecture that aims to utilize parameters more efficiently to overcome underfitting. Additionally data augmentation techniques are incorporated to enhance generalization performance. The proposed method is evaluated against existing models on four datasets and the results support the claims of improved overfitting and scaling capabilities. strength:  The paper emphasizes the importance of scale in video prediction systems.  It thoroughly investigates the overfittingunderfitting relationship in the proposed model supported by experiment on the Human 3.6M dataset.  The experiment cover various domains using multiple datasets demonstrating the potential for generalization.  A robot experiment utilizing model prediction in downstream tasks indicates practical applicability. weakness:  The description of the proposed architecture lacks clear intuition making it difficult to understand why it is more efficient in parameter use.  The technical novelty of the work is limited as the architecture is based on wellknown UNet and VAE concepts and the data augmentation techniques are not original.  The metrics used in the evaluations are inconsistent with some results missing or incomplete.  Qualitative visual comparisons of the predicted frames to baselines are not included making it challenging to assess the visual improvements.  The claim of generalizing to unseen frames is questionable as the output of video prediction is inherently unseen. rather it suggests potential memorization or overfitting to the training dataset.  It is unclear whether the experiment in number 6 include data augmentation and if not it would be beneficial to evaluate the impact of data augmentation on overfitting mitigation.", "Paraphrased Statement: This paper introduces FITVID a new model for video prediction. FITVID integrates existing modules such as Skip Connection Squeeze and Excite and LSTMs. It employs a straightforward training approach but initially experiences overfitting. data augmentation is used to address this issue. Key strength and Weaknesses: 1. The paper suggests that previous video prediction models may have underfitted due to suboptimal parameter use. This insight can guide future model design. 2. The description of the FITVID model is comprehensive and userfriendly."], "tm9-r3-O2lt": ["Paraphrased Summary This study presents a method for adjusting the memorability of faces within the hidden space of GANs. The method involves: 1. Training a memorability assessment network on a large face image database. 2. Using the network to assess the memorability of face images generated by StyleGANs. 3. Identifying a separating hyperplane in the StyleGAN latent space that divides highmemorability and lowmemorability images. 4. Adjusting the memorability of faces by altering their distance from this hyperplane. The method successfully controls the memorability of generated images without compromising their realness. effectiveness and Weaknesses effectiveness:  The method is designed specifically for modifying face memorability.  It produces visually appealing model of modified faces with varying memorability. Weaknesses:  The approach is similar to an existing method (InterFaceGAN).  The technical novelty is limited.  The experimental validation lacks:  Human evaluation of memorability.  Comparison with baseline approach.  Some modified images exhibit identity changes challenging the methods claim of maintaining identity.", "Summary Paraphrase: The paper explores the phenomenon of image memorability for face images. By analyzing the latent space of Style GAN the authors discover a hyperaxis where movement corresponds to high memorability. effectiveness and Weaknesses Paraphrase: effectiveness:  The initial observation of the latent space connection to memorability is intriguing and significant.  The technical foundation is robust.  The evaluation methodology is comprehensive.  The experimental model support the main concept.  The appendix provides valuable auxiliary information. Weaknesses:  The study is confined to the issue of face memorability.  The experimentation is restricted to the UsFaces database with no examination of factors like gender age or race.  The evaluation does not compare the proposed method to previous influence cited in the paper. Minor Issues:  The number of images used in the experiments should be explicitly stated in the text.  Quantitative data should be included in image and table captions for clarity.", "Paraphrase: This study aims to identify a hyperplane that can separate images with varying memorability in the latent vector space of StyleGAN1 and StyleGAN2. This hyperplane enables the manipulation of latent vector along its normal vector to control the memorability of synthetic images. To determine the hyperplane the researchers employed a SENet50 model trained on the US 10k face Database with memorability annotations. Experimental findings demonstrate the proposed methods ability to alter image memorability as assessed by SENet50 scores. Furthermore the method can be applied to modify realworld facial images. effectiveness:  The method effectively modifies image memorability.  It can be applied to both synthetic and real facial images. Weaknesses:  Comparative experiments with existing image memorability manipulation methods are lacking.  The accuracy of VGG16 ResNet50 and SENet50 on US 10k face Database with real labels is not taken into score when selecting the model.  While accuracy is reported for the extended latent space (w) in Table 2 it would be beneficial to provide accuracy for the original latent space (z) in an appendix.  image 4 5 6 and 8 lack detailed analysis and explanations of the topleft numbers on the facial images.", "Paraphrase: This research introduces a method that alters facial memorability as an inherent feature using Generative Adversarial Networks (GANs). They introduce a concept called \"memorability modification vector.\" This method starts by defining a hyperplane and a latent vector for each image can be moved in the positive or negative direction along the hyperplanes normal vector. By controlling the distance between the latent vector and the hyperplane they can adjust the memorability of the image. Using StyleGAN and StyleGAN2 experiments were conducted to demonstrate the effectiveness of their approach using FID and KID metrics. effectiveness:  They address a unique issue: \"What makes an image memorable\" clearly defining the problem.  This study introduces the concept of memorability modification which has applications in various fields.  The approach utilizes the distance between the latent vector and the hyperplanes normal vector to resolve the issue.  The paper flows well and provides experimental results for StyleGAN and StyleGAN2. Weaknesses:  Writing quality needs improvement with typos and errors present.  Further dataset testing is required as only StyleGANs were used for evaluation.  To establish the effectiveness and validity of this approach more stateoftheart GANgenerated images should be analyzed.  While the problem is intriguing the proposed method is straightforward and combines existing techniques resulting in modified novelty."], "gKLAAfiytI": ["Summary Paraphrase: This research proposes a framework that extends selfsupervised learning (SSL) to include the concept of equivariance. Invariant SSL a variant of SSL encourages representations that maintain feature invariant to specific transformations like horizontal flips. This framework investigates whether promoting equivariant representations is beneficial focusing on fourfold rotations. Combining this with existing SSL techniques enhances classification performance on CIFAR10 and ImageNet. Additionally the framework finds application in photonics science. effectiveness and Weaknesses: effectiveness:  Improves classification performance compared to baselines on CIFAR10 and ImageNet.  Extends invariant SSL to include equivariance offering an intuitive and reasonable approach.  Includes code and supplemental materials providing transparency. Weaknesses:  May be viewed as a combination of existing techniques lacking significant novelty.  Generalization to finite groups is limited leaving room for further exploration.  Organizational issues: Analysis in Section 5.2 could be better placed related process section could be moved after the introduction.  limited clarity: The connection between equivariance and the proposed approach is unclear Figure 3s graphic requires improvement.  Ablation field could include empirical verification of feature equivariance or investigate the impact of cropping and resizing in prediction aspect. Additional Minor Points:  Consider reordering the paper to move Algorithm 1 and Table 3 earlier.  Cite related process such as Misra et al. Lee et al. and Mundhenk et al.", "Paraphrased Statement: The research paper introduces Equivariance SelfSupervised Learning (ESSL) as a broader framework than the widely used invariance SelfSupervised Learning (ISSL) in computer vision SSL approaches.  empirical Evidence: The paper demonstrates that incorporating equivariance objective (such as rotation and flipping) to SimCLR an ISSL method can enhance performance.  ESSL Framework: ESSL extends ISSL techniques by adding an equivariance objective (particularly 4fold rotations) to the existing loss function.  Results: ESSL shows promising results on CIFAR10 and ImageNet datasets.  Extension to Photonic data: The proposed approach is also successfully applied to photonic data for a regression task involving periodic unit cells.  effectiveness:  Wellwritten and comprehensible  Simple and effective method that improves existing ISSL approaches  Weakness:  limited results on ImageNet especially when trained for only 100 epochs  The ESSL framework while claimed as a more general method primarily involves adding an additional loss which is not entirely novel.", "Paraphrased Statement: Summary: This frameprocess introduces a novel approach for learning representations that are invariant (desensitized) to certain transformations while being sensitive (responsive) to others. Prior methods have focused solely on invariant or sensitive representations not both simultaneously. This process employs the concept of invariance and equivariance to symmetry group transformations to develop such representations. The proposed method outperforms SimCLR and SimSiam on benchmark datasets like CIFAR10 and ImageNet as well as a practical application in learning frequency responses of photonic crystals. theoretical evidence supports the effectiveness of the method under specific conditions. effectiveness:  The idea of learning sensitive representations using equivariance extends the concept of invariant representations which has proven effective in practice.  The introduction clearly presents the intuition behind the approach.  Experiments provide a roadmap for identifying beneficial transformations for invariance and sensitivity.  Results on CIFAR10 ImageNet and photonic crystals demonstrate the methods utility in standard benchmarks challenging tasks and practical applications. Weaknesses and Questions:  Assumption: The method assumes that the dataset contains only a single element from each group domain. This assumption may limit the applicability of the method to domains where orientations are arbitrary such as xrays or satellite images.  Photonic Crystal Results: The dataset is rotation invariant yet the rotationsensitive representation outperforms the invariant counterpart. The authors should provide explanations for this unexpected result.  Related process comparison: Clarity is needed regarding the novelty of the approach compared to other methods that learn sensitivity to translations.  Interpretation of Results: The authors should provide qualitative results or further explanations for the small differences observed in some table 3 results.  Representation scope: The authors consider representations between the regular and trivial representation in the photonic crystal experiment. They should clarify this concept further.  Ablated ESSL framework: More details on the \"Linear enhancing predictor\" and \"No SSL augmentation in the enhanced aspect\" models are needed.", "Paraphrased Statement: Summary:  This research proposes a straightforward method to enhance current selfsupervised representation learning by incorporating a new auxiliary branch for rotation prediction.  Experiments on CIFAR10 ImageNet and PhC datasets demonstrate the effectiveness of the framework with the additional branch. effectiveness:  The concept is accessible and understandable.  The experiments yield promising results. Weaknesses:  Clarification is needed on how the framework is influenced to be either sensitive or insensitive to transformations.  The claim of equivariance may be exaggerated.  The framework learns beneficial representations through auxiliary tasks rather than true equivariance.  Further evaluation with longer training sessions on ImageNet1K is recommended to verify the stability of the improvements."], "i--G7mhB19P": ["Paraphrase: Summary:  This research explores the intrinsic bias of natural gradient descent (NGD) in simplified scenarios.  The authors propose that NGDs invariance can lead to higher generalization error than standard gradient descent (SGD) in certain situations. effectiveness:  Clear and accessible writing.  Focus on experimental findings. Weaknesses:  invariance of NGD:  The paper assumes a nonsingular Fisher information matrix (FIM). The authors need to verify if the FIMs roled in experiments are indeed nonsingular.  Theorem 4:  The assumption of a fullrank Jacobian matrix in Theorem 4 requires further clarification.  The paper should provide an model of a twolayer neural network to illustrate how this assumption is met.  Meaning of D:  The meaning of D (number of input features) should be clarified.  The experiments should report the number of input features data points and neural network parameters.  Empirical FIM Approximations:  The empirical approximations of the FIM roled in experiments may be insufficient.  The authors should explore the impact of varying the number of input features on NGDs performance.  practical NGD with Damping:  The paper does not address the role of damping in practical NGD implementations.  The authors should discuss the impact of damping on NGDs inductive bias.  Initialization:  NGD is sensitive to initialization which should be acknowledged.  The paper should provide a more precise statement about initializations role.", "Paraphrased Statement: This research investigates the impact of parameterization on inductive biases and the generalization ability of deep learning models. It specifically focuses on natural gradient descent (NGD) which is independent of parameterization. This enables the work of parameterizations role in generalization using gradient descent and the examination of inductive biases inherent to NGD. The work considers two distinct scenarios: separable classification using deep linear models and matrix completion using deep matrix factorization. effectiveness:  Analyzing NGD as a \"type of ablation by removing parameterization reliance\" provides a novel approach to understanding the interactions between the learning algorithm hyperparameters and model parameterization.  The paper is wellorganized clearly written and the validation and theorems are presented effectively.  The scope on parameterizations work on generalization in separable classification and matrix completion is thorough. Weaknesses:  While working NGD as a tool to understand parameterizations impact in generalization is original the parametertohypothesis mapping is already welldefined in the two scenarios studied.  The paper could benefit from discussing potential insights gained from this perspective or exploring less wellunderstood settings.  The significance of the theoretical contributions in Section 3 could be emphasized and discussed in more detail indicating implications for comprehending NGD in general settings.  The details of extending Bernacchia et al. (2018) to create an algorithm for diagonal networks are largely relegated to the appendix making it difficult to fully grasp this significant contribution to the experimental section. Minor Comments:  In the highlighted section \"works the inductive biases of gradientbase learning\" the intended term should be \"gradientbased.\"  The statement about matrix factorization models tending towards the minimum nuclear norm solution requires clarification.  In the validation of \"invariance of NGF under reparametrization\" \\mathcalP(\\dotwt) should likely be \\dot\\theta.  In Theorem 4 the phrase \"the Jacobi matrix\" should be corrected to \"Jacobian.\"  Consider referencing the recent work on \"Kernel and Rich Regimes in Overparametrized model\" in the discussion on parameterization initialization and implicit biases.", "Paraphrased Summary: This research examines the inherent bias of natural gradient flow (NGF) in certain types of neural networks. The authors demonstrate that NGF solutions for empirical risk minimization are unaffected by pixel reordering. They also show that NGF does not contribution the property of Euclidean gradient descent which produces solutions with minimal L2 norms under logistic loss (Theorem 12). Additionally they introduce an improved NGF algorithm for deep diagonal linear networks. effectiveness:  Clear exposition of the scope  Welldesigned illustrations and toy experiments  Simple and novel theorems with insightful conclusions Weaknesses:  Imprecise definitions of N and D in Section 3  Ambiguity in Theorem 1s assumptions  Unclear discussion between Theorems 3 and 4  lack of practical model in deep networks to demonstrate the impact of the findings Typos:  Section 3: Missing \"t\" in \u03b2t  Page 5: \"perfectly)\"  Page 6: \"to unobserved entries\"  \"develope\" Related work:  https:arxiv.orgabs2006.10732  https:arxiv.orgabs2008.07545", "Paraphrased Statement Summary: This paper explores the inherent bias of natural gradient descent (NGD) algorithms in deep linear networks. The authors employ reparameterization invariance properties of NGD to isolate the effects of the solution identified by gradient descent from the parameterization used. The work focuses on gradient flow dynamics in two specific scenarios: separable classification under logistic loss and deep matrix factorization. The paper claims to identify situations where NGD fails to generalize while standard gradient descent (GD) with suitable architecture succeeds. effectiveness and Weaknesses: The central topic of the paper is to leverage NGD properties to investigate its inductive bias. However there seems to be a misunderstanding regarding the interpretation of \"reparametrization invariance\" applied to NGD. The general description of NGD in Section 2.3 appears sound. The authors correctly state that \"NGD with infinitesimal learning rate (i.e. NGF) always follows the same trajectory in model space and finds the same optimum irrespective of how it is parametrized provided that the parametrization is smooth and locally invertible.\" However the authors subsequent analysis involves parameterization changes that do not qualify as reparameterizations. For instance in Section 3 the comparison of \u03b2w versus \u03b2w1\u2299\u22ef\u2299wL alters the dimension of the parameter space from D to L\u00d7D which is not considered a reparameterization that ensures optimization equivalence under NGD. Similarly in Section 4 the parameterization of \u03b2 (with dimension D\u00d7D) using matrix Wi (with dimension L\u00d7D\u00d7D) of varying depths constitutes a distinct model architecture rather than a reparameterization. The authors analysis seems to equate \"parameterization\" differences between ResNet50 ResNet101 and VisionTransformer when applied to ImageNet classification tasks with actual architectural differences. Without establishing a connection between optimization and architectural variations the significance of this analysis remains unclear. For model in Figure 5 of Section 4 where the authors highlight a situation where NGD fails to generalize while GD with L2 and L3 performs well it is unclear if they are demonstrating the inadequacy of L1 for matrix completion tasks (which is an obvious limitation) rather than a failure of NGD per se. The authors should clarify whether NGD was also tested with L2 and L3 and if so whether it exhibited nongeneralization behavior. While the paper acknowledges that C and D yield the same solution in Figure 2 which is noteworthy further elaboration on how reparametrization invariance practice to the studied models would be beneficial. Additionally the justification for the full rank of the Fisher information matrix (FIM) should be provided. Addressing these concerns could strengthen the papers arguments and clarify its claims.", "Paraphrased Statement: Summary: This paper explores the \"inductive bias\" of natural gradient flow a training algorithm for deep learning models. It emphasizes the algorithms invariance under reparameterization and examines its training dynamics in several scenarios involving deep linear models. The paper offers effectiveness and weaknesses. effectiveness:  Intriguing insights into natural gradient descent in deep linear models  valuable observation of natural gradient flows invariance to reparameterization  Experimental support for the theoretical findings Weaknesses:  Limited presentation clarity:  Focus on binary classification but mentions matrix factorization a regressiontype problem  Inconsistencies between Theorem 3 and the introduction near equation (5)  Incomplete results:  Theorems 3 and 4 are weaker than inductive bias results for Euclidean gradient flow  The claim of natural gradient descent performing worse than Euclidean gradient descent is supported only by experiments"], "g8NJR6fCCl8": ["Paraphrase: Summary Generalized Additive Models (GAMs) known for their interpretability have limitations in terms of differentiability and scalability. To address this researchers have developed neural GAM (NODEGAM) and neural GA2M (NODEGA2M) which maintain interpretability while scaling well on large datasets and outperforming other GAMs. Previous approaches like NODE while mimicking decision trees lack interpretability. NAM on the other hand has a scalability constraint. NODEGAM and NODEGA2M combine elements of these approaches through modifications: 1. Selecting only one feature for the ODT feature use. 2. Using a consistent logit layer to eliminate feature interactions. 3. Avoiding DenseNet connections between trees focused on different features. Experiments on binary classification and regression datasets demonstrate that NODEGA2M performs comparably to stateoftheart methods. Semisupervised learning capabilities are also evaluated indicating the models ability to learn from unlabeled data and finetune on limited labeled data. While GAMs are praised for their interpretability the authors argue for the interpretability of NODEGAM and NODEGA2M. Strengths and Weaknesses: Pros:  Wellgrounded motivation supported by literature review  Innovative integration of previously disconnected methods  Demonstrated performance on various datasets  Code transparency for reproducibility Cons:  Lack of a detailed NODEGAM algorithm for clarity  Argument for interpretability would benefit from inclusion in the introduction or literature review rather than the conclusion", "Paraphrase: The researchers created novel structures called neural GAM and GA2M that keep the features of GAM but use deep learning to improve performance. They tested their novel approach on 14 datasets involving various prediction tasks. NODEGAM and NODEGA2M the proposed methods performed similarly to other GAM methods. Compared to other similar approaches the proposed method scales well and performs well on large datasets. Strengths and Weaknesses: To create the novel structures the researchers made three key revisions to the NODE architectures. However the reasons behind some of these changes such as eliminating feature interactions in a tree were not fully explained. It would be beneficial if the researchers could provide theoretical or experimental evidence to support their design decisions.", "Paraphrase: Summary: The NODEGAM and NODEGA2M architectures extend the NODE architecture with constraints and gating mechanisms. These modifications limit the model to learning interactions of range 1 (NODEGAM) or 2 (NODEGA2M) within the same tree and across layers. The authors demonstrate that NODEGAM and NODEGA2M perform similarly to baseline methods on small datasets and outperform them on large ones. NODEGAM also benefits from selfsupervised learning by pretraining to reconstruct the input from a masked adaptation. The authors apply NODEGAM and NODEGA2M to realworld datasets and identify design in the data. Strengths:  Carefully designed architectures with clever tricks like gating mechanisms annealing and extensions (attention regularization).  Thoughtful extraction of interactions using the \"purification\" technique. Weaknesses:  Lack of comparison to simpler neural network architectures used in other papers.  Absence of comparisons to the \"Neural Interaction Transparency\" (NIT) approach.  Typos and missing notation in equations. Minor:  Addition of a constant to address class imbalance is suggested as a simpler alternative to adding a bias term."], "fTYeefgXReA": ["Paraphrase: Summary: This research introduces a heterogeneous graph neural network model that maintains permutation comparison. Permutation comparison is achieved through linear mapping operations (contraction and expansion) that are invariant to permutations of typespecific adjacency matrices. Experimental results show the models superiority over nonequivalent heterogeneous graph neural network in link prediction and node classification. effectiveness:  heterogeneous graph neural network are widely used and extending permutationequivalent graph neural network to heterogeneous settings is significant.  The model performs exceptionally well on some datasets such as PubMed where it achieves high link prediction performance. Weaknesses:  The proposed method lacks substantial novelty as previous work by Maron et al. and Albooyeh et al. have employed equivariant pooling and broadcasting operations for permutationequivalent graph neural network. The major contribution of this work is the extension of these idea to multiple node and edge types which is arguably a marginal advancement.  The experimental evaluation lacks crucial baselines. While the compared models are not designed for permutation comparison permutationequivalent graph neural network exist. Without comparing against such baselines it is difficult to assess the advancement made by this work.  The models performance varies significantly across tasks and datasets. While it excels in link prediction on PubMed the differences are less pronounced on other datasets. In node classification the model sometimes underperforms compared to a basic graph neural network. The author briefly explains performance variations in link prediction but a more detailed analysis is necessary.", "Paraphrase: Main idea: This paper introduces a Graph Neural network (GNN) that remains expressive and maintains equivariance unlike most existing GNNs which typically focus on homogeneous graphs or convert heterogeneous graphs to homogeneous ones. effectiveness: 1. The paper proposes a GNN model and outlines how it maintains equivariance under various operations. 2. It demonstrates that all linear maps satisfying invariance can be constructed from the specified operations. 3. The GNN is shown to be extremely expressive. 4. It achieves impressive results on link prediction tasks especially for the PubMed dataset. Weaknesses: 1. The proposed novelty may be limited compared to previous work. 2. Performance is less impressive for node classification and lags behind significantly for Last FM in link prediction. 3. There is no explanation provided for these observations. Recommendation: The paper is generally wellwritten and easy to understand. However the marginal novelty and performance limitations lead to a evaluation slightly below the acceptance threshold. The authors are encouraged to provide insight into why the GNNs expressivity and equivariance do not translate into full performance on node classification and the Last FM dataset.", "Paraphrased Statement: This paper proposes a model for designing Graph Neural network (GNNs) that can effectively handle heterogeneous graphs where edges represent various relationships. Unlike existing methods that either ignore edge types or convert them into a single homogeneous graph this approach treats heterogeneous graphs as a collection of adjacency matrices one for each edge type. The GNNs developed in this work are designed to be invariant or equivariant to permutations of nodes within the same type (e.g. authors or venues). To achieve this the paper enumerates a set of linear operations that satisfy this constraint. effectiveness:  Impressive edge prediction performance on the PubMed dataset.  Rigorous formalization of linear mappings and GNN constraints for heterogeneous graphs. Weaknesses:  Limited testing range with no experiments on artificial datasets to provide insight into the methods limitations and ideal usage type.  Questionable results on PubMed potentially due to overfitting.  Lack of information on computational complexity and inductive capabilities.  Artificial traintest splitting in the GNN literature hindering the idea of outofsample error.  Absence of ablation and perturbation work to assess overfitting.", "Paraphrase: Summary: This paper suggests a novel heterogeneous graph neural network model for learning representations from heterogeneous graphs. The model combines different operations on adjacency matrices to achieve the final aggregation. Experiments show that the method performs full than some existing ones. effectiveness: 1. Presents a novel equivariant operationbased model for heterogeneous graph representation learning. 2. The paper is easy to understand. Weaknesses: 1. The model offers incremental novelty. 2. There is no discussion of relevant work like \"E(n) Equivariant graph neural network\" from ICML 2021. 3. The experiments need improvement. Detailed Review: The proposed methods novelty is limited. It is not the first to explore equivariant graph neural network. The paper lacks a discussion of related work. The experiments could be improved. Ablation work are needed to understand the impact of various operations. Table 23 shows that the model underperforms in many type raising concerns about its effectiveness. The authors mention hypergraphs in Section 6 but there are no experiments to support this extensive model. Minor issues include typos such as in the first time of Section 4.2 which should read \"for the task of link prediction....\""], "pN1JOdrSY9": ["Paraphrased Summary This field enhances unsupervised machine translation (UMT) by incorporating a modified Swapping Assignments with Variational Inference (SwAV) loss. This loss clusters sentences from monolingual data leveraging them in UMT training. The improvements are demonstrated through evaluation on standardized UMT tasks accompanied by indepth analysis. effectiveness  Wellmotivated process  novel application of SwAV loss in NLP  Thorough analysis addressing potential reader questions Weaknesses  Difficult to read with numerous unnecessary equations and inconsistent notation  Inaccurate Table 1 due to incomparable scores  Lack of statistical significance testing  Unclear experimental settings (e.g. hyperparameters datasets baselines) QuestionsComments  The acronym SwAV should be explained in the introduction.  The introduction could be concise reducing repetitive details provided later.  Its unclear why applying a language model to range makes sense.  A comprehensive design illustrating the frameprocess would be helpful.  It would be valuable to verify the assumption that regular UMT does not fully utilize parallel data within monolingual corpora.  The language \"LAgSwAV\" is difficult to read.", "Paraphrase: The researchers present a languageindependent manner to create imitation parallel data by using unlabeled monolingual text from various languages. They start with a pretrained model that guesses missing language and then adjust it using their proposed languageagnostic translation of a learning method called SwAV. The selfsupervised model design out group codes based on how language are positioned in the hidden space that is made when predicted codes are switched between different translations of the same sentence. They also add a weighting term to this loss function that makes sure the predictions from different embedding spaces are balanced which results in a similar space for both the source and target languages. They test their method on both the Wmt14 dataset and the lowresource FLoRes dataset and get the best BLEU scores ever for unsupervised machine translation. effectiveness:  The authors suggest a new method that works across languages to mine synthetic parallel data from current monolingual corpora in various languages.  Adapting the computer vision SwAV loss function to finetune masked language models in natural language process.  Using a balanced weighting method to force even cluster assignment regardless of language (when using separate masked language models) resulting in a shared embedding space.  Their tSNE plots clearly show the languageagnostic embedding space where sentences from various languages with similar meanings are near one another.  The suggested method eliminates the need for pretraining on a sizable multilingual parallel dataset. Weaknesses:  The authors give a set of filtering rules to eliminate poor samples from the initial mined dataset. The filtering criteria depend mostly on predefined thresholds such as sourcetarget length ratio and subword overlap which require adjustment for each language pair to produce satisfactory results. A metaclassifier might be trained using a limited clear sourcetarget dataset to prevent perlanguage adjustment eliminating the need for it.  Gradients dont pass through the rebalanced weight vector and weights are computed based on perbatch statistics. Wouldnt it be better to optimize the rebalanced weights to prevent local bias within an already unbalanced batch An ablation field on an imbalanced set of batches would be helpful to see how clusters are assigned and how it affects data mining.  The goal of this field is to mine parallel data for machine translation making it reasonable to evaluate training an NMT network from scratch on the mined parallel data and comparing it to baselines trained from scratch on existing mined data.  The authors omitted to cite related publication in the background section on pseudoparallel data mining for machine translation that created synthetic data for neural machine translation such as [4 5].", "Paraphrase: This paper introduces a finetuning approach for multilingual sentence representations trained on singlelanguage data. The goal is to make the representations less languagespecific and more resilient to artificial noise ensuring that clusters of representations remain intact. Experiments show that these finetuned representations can be practiced to identify parallel sentences in singlelanguage data which can enhance machine translation (MT) training. effectiveness and Weaknesses: The paper is wellwritten and easy to comprehend. The experiments are comprehensive and provide valuable insights. The authors clearly explain the motivations for their process which focpractices on improving the languageagnostic nature and noise robustness of multilingual sentence representations. One potential concern is the practice of the term \"semantic contrastive.\" While the representations may be more resistant to noise its not clear if this advance can be directly translated into enhanced semantic contrast. The authors should elaborate on their perspective on this matter.", "Paraphrased Statement: This research extends Carons SwAV algorithm proposing a variant for unsupervised multilingual sentence embedding. A key modification is the \"languageagnostic rebalancing function\" that balances cluster assignments for sentences in different languages. Experiments: Experiments demonstrate that this approach improves the choice of mined data resulting in higher BLEU scores than the original SwAV. However additional steps (e.g. filtering rankbased crossentropy) are necessary to make the mined data usable. effectiveness:  The rebalancing concept enhances multilingual sentence embedding.  Strong experimental results showcase the benefits of the modified loss function and additional data process techniques. Weaknesses:  The method is complex involving both SwAV (inherently intricate) and additional gradients from parameter tweaking.  This complexity complicates scaling and comparisons with other approach. Questions:  Table 4 results appear overly optimistic with LAgSwAV performing nearly as well as the supervised method LASER. more explanation is needed for this discrepancy."], "qaxhBG1UUaS": ["Paraphrased Summary: This reresearch focuses on offline reinforcement learning for managing taskbased dialogues specifically with a large natural language action space. The field integrates a policy network (for sampling agent responses) and a Qnetwork (for evaluating responses) into a single GPT2 network. It then proposes an iterative algorithm to optimize both network. During evaluation the Qnetwork is updated using system actions and responses. During improvement these actions and responses with the highest Qvalues are used to train the policy network. The model outperforms previous approach on the MultiWOZ dataset. Key issueiveness and Concerns: issueiveness:  The model allows for natural language actions enhancing flexibility and integration with transformerbased language models. Concerns:  QValue Estimation: The paper acknowledges the challenge of overestimating Qvalues and mentions using offline automatic evaluation to mitigate this. However it lacks specific details on how this problem is addressed. more information and ablation field are essential to assess the models ability to generalize.  Exploration: The paper introduces a set of N response candidates generated from the current policy. The optimal value of N and its impact on performance is unexplored. Additionally the sampling method for generating N (e.g. beam research or vanilla sampling) and its issue on response diversity require further discussion.", "Paraphrased statement: Summary: This paper addresses the problem of response generation divergence from human language. It proposes a critic network that is added to a pretrained GPT2 taskoriented dialogue factor and shows promising results on two datasets (MultiWoz and ConvLab). effectiveness: The approach uses a critic to guide response generation potentially improving the alignment with human language. Weaknesses: 1. The contribution may be limited since it only involves adding a critic to an existing method. 2. Human evaluations are missing which could strengthen the findings. (Note: This weakness was addressed in the rebuttal.)", "Paraphrased statement: This research introduces a method based on reinforcement learning to create dialogue factor focused on specific tasks. Using a dataset of conversation tagged with reinforcement the team first minimizes time divergence to train a stateaction value function. Next a new training dataset is created by selecting the best actions from options generated by the current policy (the language model). behavior cloning is then applied to update the policy using this dataset. The action is repeated until the factor performance improves. The approach is simple and has promising results in experiments using the MultiWOZ and ConvLab datasets. However some details are missing from the paper including:  The factor action space  How candidate actions are generated  How reinforcement are calculated Its unclear whether the method is truly offline reinforcement learning as the factor may be receiving reinforcement from an external program. Despite being labeled superior to UBAR in Table 3 there is a significant divergence with UBARs reported results. The paper suggests that policy updates continue until \"convergence\" raising concerns about overfitting. The training action is also halted after four iterations (Table 2) possibly due to overfitting. Minor Corrections:  \"outperforms the stateoftheart\"  \"outperforms the state of the art\"  \"not trained for\"  \"not trained to\"  \"fineturning the GPT2\"  \"finetuning GPT2\"  \"generates strategically\"  \"generates a strategically\"  \"Pr(Ot1 ... )\"  \"Pr(Ot1 ... )\" (nonitalicized)  \"by training actionvalue\"  \"by training the actionvalue\"  \"of critic network\"  \"of the critic network\"  \"for ith\"  \"for the ith\"  \"in taskoriented\"  \"in the taskoriented\"  \"generated system response\"  \"generated system responses\"  \"from (Zhao et al. 2019)\"  \"from Zhao et al. (2019)\"  \"prohibitory\"  \"prohibitively\"  \"over response candidates\"  \"over the response candidates\"  \"updated policy by above\"  \"the updated policy by the above\"  \"on MultiWOZ field\"  \"on the MultiWOZ field\"  \"ConvLab model\"  \"the ConvLab model\"  \"HuggingFace Transforms library\"  \"the HuggingFace ... library\"  \"prices\"  \"priced\"  \"user goal\"  \"the user goal\"  \"with following\"  \"with the following\"  \"the all\"  \"all the\"  \"whereas original\"  \"whereas the original\"  \"straightforward\"  \"straightforwardly\"  \"large scale\"  \"largescale\"", "Summary: This field introduces an offline reinforcement learning (RL) approach for endtoend taskoriented dialogue models. The proposed GPTCritic model is built upon the pretrained GPT2 language model and finetuned to improve the dialogue policy and response quality. The model avoids the issue of deviating from human language by learning from sentences generated by the GPT2 model. Experiments demonstrate that the model outperforms existing endtoend dialogue models in both offline and online settings. effectiveness and Weaknesses:  effectiveness:  The model improves both dialogue policy and responses by leveraging pretrained language models and offline RL.  It overcomes the problem of language divergence by learning from GPT2generated sentences.  Weaknesses:  The paper lacks sufficient detail on the proposed method making comprehension difficult.  It does not include an experiment where GPT2 is naively finetuned on additional generated information to assess the necessity of an RL approach.  The claim of standard RL methods failing is not supported by experimental results including diverse standard RL algorithms.  The offline evaluation focuses on response generation but evaluating dialogue policy is also significant.  Realuser interactions should be incorporated into the experiments to ensure practical effectiveness."], "qwBK94cP1y": ["Summary This paper aims to find the direction of causality between two continuous variables using a new method based on Functional Causal Models (FCMs). The method based on dynamical systems and optimal transport improves the accuracy of existing algorithm which are sensitive to model assumptions. strength  Theoretically sound groundwork  Clear and wellorganized writing  Enjoyable to read Weaknesses  Unclear handling of independent variables  potential bias due to data standardization and outlier removal  Uncertain accuracy compared to other methods  Minor typos", "Paraphrased Summary: This research tackles the challenge of determining causality in a twovariable system with random noise and no hidden variables. It connects a specific case of causal model (FCM) to issues in the work of dynamical systems to create a new metric for measuring divergence. This metric can be efficiently computed and is zero only if the causal direction is correct. The paper shows that this metric outperforms existing criterion on both simulated and realworld data. strength and Weaknesses: strength:  Innovative and interesting work on a complex problem.  Derivation of the new metric through connection to dynamical systems is insightful and engaging. Weaknesses:  High technical density and reliance on knowledge from other fields potentially alienating some readers. Minor RemarksQuestions:  Provide brief explanations of referenced concepts for readers unfamiliar with them.  Clarify the term \"pressureless flow\" or explain the divergence from \"incompressible flow.\"  Consider the impact of varying the time period T on the interpretation of the dynamical system map M.  Clarify the enforcement of causality in FCMs.  Explain the implication of optimality in the problem formulation.  Comment on the behavior of the divergence metric D(v) in the presence of unobserved confounders.  State the identifiability conditions of ANMs explicitly.  Provide direction on choosing the invertible work for extending the method to nonlinear systems.  Clarify the statement about the true distribution not needing to be known when using the method.", "Paraphrased Statement: Summary: The paper conceptualizes the bivariate causal discovery problem through the analysis of dynamical systems. It utilizes optimal transport theory to interpret noise models in this framework. The authors introduce a new criterion and algorithm for causal discovery and compare their approach with existing methods. strength and Weaknesses: strength:  Novel and insightful reformulation of the causal discovery problem.  Clear and concise demonstration. Weaknesses:  Insufficient motivation for the proposed reformulation.  Limited to bivariate causal discovery (requires an outlook on broader application).  Potential for improved demonstration:  Introduction to optimal transport and its application.  Moving the related work section to strengthen the introduction."], "wwDg3bbYBIq": ["Summary (Paraphrased) This research investigates traffic forecasting using a model matching approach. The authors first identify significant models in historical data and then retrieve models for each time series using a similarity measure (e.g. cosine similarity). These models are then used with a graph Convolutional network (GCN) to generate node representations. effectiveness and Weaknesses (Paraphrased) effectiveness:  Using model matching for traffic forecasting is a sound approach.  The design incorporates innovative ideas. Weaknesses: 1. Clarity issues: The writing is unclear particularly regarding the extracted models. Its unclear if they represent average speed how representative they are and how they are sampled with a time window. Equation 1 should be a sum from j0 to k. Equation 4 lacks clarity on how p1 is used to extract noise. 2. Lack of justification: Section 3.3 lacks justification for why capturing interactions among fetched models and using modellevel attention are necessary. 3. Questionable experimental results: The use of different experimental settings (e.g. 18 steps instead of 12) with existing work hinders fair comparison. Some baseline results are below reported performance. Authors should use the same settings as existing work and consider the PeMS dataset. 4. Decoder performance: The proposed decoders performance in relation to a simple Multilayer Perceptron (MLP) for forecasting should be evaluated. 5. Grammar errors: The paper contains numerous grammatical errors such as incorrect verb tense and missing articles.", "Paraphrased Summary: This research explores the issue of predicting traffic speed. The proposed method combines spatiotemporal dependencies and extracted traffic models to enhance forecasting accuracy. effectiveness:  The concept of integrating traffic models and spatial dependencies is novel. Weaknesses:  The paper lacks clarity in introduction.  Essential details on the model and problem setup are absent.  The definition of traffic models is unclear.  The discussion of traffic models being roadspecific is inconsistent.  The imbalance claim regarding the model set lacks sufficient evidence.  Cosine similarity is treated as discrete without justification.  The input outputs and connections between component are not adequately explained.  The contribution of this work appears incremental combining existing methods for memory attention and graph convolution.  Key baseline models (e.g. ASTGCN STGCN) are omitted.  Table 1 lacks error bars for forecasting results.", "Paraphrased Statement: Summary This paper presents an innovative approach to traffic flow forecasting. Unlike traditional methods that use neural networks to forecast sequences this approach reframes the problem as a keyvalue pair matching task. Results are presented for two datasets along with analysis of component contribution. effectiveness and Weaknesses This paper offers a unique perspective on a widely studied topic in traffic management. The proposed approach is both novel and technically sound providing a strong introduction for future research. Opportunities exist for developing more innovative models for pattern extraction filtering and inference. Recommendations for Improvement 1. The results in Table 1 could be enhanced to well demonstrate the significance of the approach. 2. Visual model showing input sequences extracted patterns predicted results and comparisons with ground accuracy would enhance understanding. 3. The exposition of the paper should be improved especially by clarifying cryptic terms like \"zerobased signals\" and \"without any duplication in range.\" Despite these suggestions I strongly recommend accepting this paper not only for its novelty but also for its potential to inspire new directions in traffic flow forecasting research.", "Paraphrased Statement: Summary: This paper presents a novel approach to traffic forecasting using a neural memory module. The proposed model effectively captures spatiotemporal traffic patterns and yields promising results on public datasets. effectiveness:  Innovative use of a neural memory module to remember traffic patterns.  Encouraging experimental results. Weaknesses:  Lack of analysis on the impact of neural memory size (P) on forecasting accuracy. An ablation work examining this and other configuration settings could enhance the papers value."], "oapKSVM2bcj": ["Paraphrased argument: Summary: This article presents einops a Python library designed for efficient tensor manipulation in deep learning. Key benefits of einops include:  Verifying tensor operations for correctness  Providing flexible and expressive tensor manipulation interfaces  Supporting multiple backends for efficient performance  Enhancing code readability and reliability strength and Weaknesses: strength: The author highlights the advantage of einops as a valuable tool for machine learning. These advantage are supported by the authors personal observations. Weaknesses:  Academic Writing: The paper resembles a technical blog rather than a conventional academic paper containing inconventional language and lacking system.  Empirical Justification: The authors claims lack empirical evidence. For case the claim of improved code readability and flexibility could be supported by user studies.  implementation Evaluation: There is no data on the overheads introduced by einops such as runtime differences it could introduce compared to direct backend use.  Distinction from NumPy einsum: The paper does not clearly explain the specific differences between einops and the NumPy einsum use.", "Paraphrased argument: This article presents EINOPS a new Python toolset for performing tensor operations on single tensors. With EINOPS you can easily modify tensors by reshaping reducing repeating permuting and so on while enhancing code clarity and adaptability. current issue with existing solution such as NumPy are addressed and EINOPS provides an alternative. The toolset also integrates with prominent frameworks like PyTorch and tensorFlow offering a streamlined and frameworkagnostic API for manipulating tensor structures. strength:  Simplifies tensor manipulation code and enhances readability.  Supports multiple popular Python frameworks.  Offers more flexibility in index naming than existing tools (e.g. NumPys and PyTorchs einsum) which have limitations on the issue of dimensions.  Highly beneficial for tensor Networks which often involve numerous interacting tensors with many dimensions. Weaknesses:  Does not introduce new theoretical or algorithmic advancements in machine learning.  Focused on introducing a practical and userfriendly coding tool.  Does not discuss applications for computing and manipulating tensor Networks a significant field of interest.  currently lacks support for operations involving multiple tensors.  A comparison of computational costs is not included.", "Summary The paper introduces several enhancements to the Einstein summation notation (einsum) in Numpy including:  Arbitrary naming of axes  Control over the flattening of axes (\"rearranging\")  Repeated axes  Axis reduction with operations beyond summation These features are implemented in the Python library \"einops\" which supports multiple backend frameworks (PyTorch TensorFlow Jax etc.). Strengths  Extension of valuable einsum notation for tensor operations  Potential utility of the \"rearrange\" use  Reduced code verbosity and enhanced readability (subjective)  Multibackend support Weaknesses General  Unsuitable for issue at ICLR better suited for venues focusing on machine learning package  Weak motivation behind the paper  Insufficient description of the notation implementation details  Questionable choice of automatic backend detection  lack of comparison with existing einsum capabilities  Unclear case of convolutionrelated benefits  Insufficiently rigorous and subjective claims Suggestions  Submit to a more appropriate venue (e.g. JMLR Open Source package track)  Revise the paper to:  Clarify the relationship between einops and einsum  Formalize the notation  Support claims with user studies  Reconsider the motivation and scope of the paper and library  focus less on cases", "Summary Operators in Deep Learning and Scientific computing Summary operators are essential in frameworks like TensorFlow PyTorch and NumPy. This paper introduces a unified framework called EinOps inspired by the Einstein summation convention to describe and use these operators conveniently. EinOps enables concise representation of complex operators such as multihead attention. It provides:  new notation: to represent hundreds of operators improving expressiveness and readability.  Library: that interprets the notation and offloads them to NumPy TensorFlow or PyTorch reducing engineering overhead. strength:  Enhanced expressiveness: simplifies operator representation leading to fewer code argument.  Improved usability: easytouse notation enhance code readability and debuggability.  Robust engineering: supports multiple backends and reduces runtime dispatching overhead.  Stringlytyped design: enforces explicit tensor axis semantics and runtime checks reducing errors. Weaknesses:  lack of compiler integration: could improve performance if integrated with compilers like TVM.  Formalization: some aspects of the EinOps notation require more conventional definition.  Limited exploration: has not fully direct common operators like various convolution case. Clarity: The paper provides detailed explanations and illustrative examples with minor clarity issue in certain notation."], "u6ybkty-bL": ["Paraphrase: Summary: This study compares various methods for anomaly detection in time series including Recurrent Neural Networks (RNNs). Results obtained from experiments on multiple datasets reveal that RNNs outperform nonRNN approaches. Strengths:  significance of anomaly detection in time series analysis Weaknesses:  Evaluation is limited in scope and may not fully capture all relevant aspect. point:  The study presents itself as a wider exploration of anomaly detection methods but primarily focuses on evaluating existing approaches.  A wider range of datasets would be necessary to draw classical conclusion.  The study does not analyze specific anomaly types and how they compare across different methods.  Questions remain unanswered regarding the relative effectiveness of convolutional neural networks (CNNs) subsequence methods and traditional versus modern approaches.", "Paraphrased Statement: Summary: This paper compares the performance of traditional time series outlier detection models (nonrecurrent) and sophisticated models based on recurrent neural networks (deep recurrent). Using synthetic and realworld datasets they evaluate seven different models. Strengths:  Time series outlier detection is significant for various applications.  The study addresses the relevance of recurrent neural networks in this field.  Empirical results demonstrate the effectiveness of the models. Weaknesses:  The study lacks depth in its analysis and comparison.  It overlooks new LSTMRNN models that specialize in time series outlier detection.  The methodology for creating and injecting outliers may influence the results but this is not fully considered.  The concept of data complexity in time series outlier detection is not welldefined or quantified.  Complexity factors like temporal dependence and varying dependence durations are not explored thoroughly.  It remains uncertain if the models can detect \"collective outliers.\"  No significant introduction is evident its primarily a comparative analysis with limited insights. Improvement Suggestions:  Incorporate more reallife datasets with natural outliers.  Conduct controlled experiments to investigate specific questions.  Quantify data complexity and analyze its impact on model performance.  research additional complexity factors such as temporal dependence.", "Paraphrased Statement: The study evaluates nonRNN and RNN approaches for detecting outliers in time series data. It finds that nonRNN methods are more suitable for detecting isolated outliers (\"point outliers\") while RNN methods are better suited for detecting clusters of outliers (\"collective outliers\"). Strengths and Weaknesses:  The study focuses on nonRNN methods for time series outlier detection which have been widely studied. However it excludes the evaluation of existing nonRNN methods that can detect both point and collective outliers.  The conclusion is based on only three realworld datasets one of which is not publicly available. To provide a more comprehensive analysis the study should consider including additional datasets from the UCI Repository and synthetic datasets with artificial outliers.", "Paraphrase: Summary: This study evaluates different approaches to detecting anomalies in time series data. The authors compare recurrent (LSTM) and nonrecurrent methods on various synthetic and realworld datasets. They also offer an improved LSTM method with care. Their findings favor nonrecurrent methods for general outlier detection tasks due to their superior performance simplicity (fewer parameters training time) and effectiveness. However for data with complex temporal relationships recurrent methods may be preferred. Strengths and Weaknesses: Strengths:  extensive experimental results Weaknesses:  Incomplete review of existing methods  Lack of clarity on experimental point (e.g. crossvalidation contamination method)  Insufficient statistical comparison to assess significant differences between methods (e.g. KS test for distributions DeLong test for ROC curves)  Unclear graphical representations of performance gains  Ambiguous guidance on method selection for specific contexts (e.g. dimensionality collinearity time practice)  Absence of outlier time series comparison for a comprehensive evaluation  Limited advantage of the proposed methods in practical applications Suggestions for Improvement:  Emphasize the strengths and weaknesses of different methods in various contexts.  Conduct more statistical comparison to establish the significance of performance differences.  Provide clear explanations of experimental setups and procedures.  Include outlier time series comparison for a broader perspective."], "t5EmXZ3ZLR": ["Paraphrased Statement: Summary: This paper introduces two novel pruning methods for neural networks that are based on saliency and consider the correlations between different structures and layers. These methods can be used on largescale neural networks and datasets and they can help identify and remove any bottlenecks in the networks architecture. effectiveness and Weaknesses: effectiveness:  The paper provides a thorough mathematical analysis and experimental evaluation of the proposed pruning methods.  The paper is wellwritten and easy to understand. Weaknesses:  The paper claims that using secondorder information improves pruning performance but the experiments show a relatively small difference in accuracy compared to firstorder methods especially for larger networks.  The improvement in accuracy when using secondorder information is limited to around 0.10.3 which may not be significant in practice.  The paper needs to clarify that the advantage of capturing global correlations and scalability is not achieved by a single method but rather by two different methods. Minor Issues:  Due to space limitations some significant figures are included in supplementary materials which can make it difficult for readers to follow the papers flow.", "Paraphrased Summary: This paper introduces two pruning methods SOSPI and SOSPH which utilize secondorder structure (Hessian) to capture correlations within networks. SOSPI approximates Hessian while SOSPH uses exact Hessian. effectiveness and Weaknesses:  SOSPH assumes high pruning ratios for its approximation. The paper should define \"high\" ratios.  The proposed methods perform similarly to stateoftheart methods in pruning efficiency (as shown in Table 2). However its unclear if they outperform others.  The paper mainly focuses on CIFAR results and it would be helpful to include ImageNet results.  The compared methods are outdated (from 2019 and 2020). The authors should consider including more recent references (e.g. Liu et al. 2021 Hayou et al. 2021 Su et al. 2021).  The vertical axes in Figure 3 are not clearly labeled making it difficult to interpret the results.  It is unclear why input and lowlevel layers have low pruning ratios originally.  The paper should include a comparison with CCP another secondorder correlationbased pruning method.  Equation 5 in the papers formulation is not scalable and may be unnecessary for practical applications.", "Paraphrase: Summary: This research examines structured pruning for convolutional neural networks (CNNs) to reduce inference time and computational costs. Structured pruning removes full network substructures unlike unstructured pruning that targets individual weights. The paper proposes a novel approach s Order Structured Pruning (SOSP) that considers global correlations during pruning. SOSP uses a secondorder approximation to estimate the joint effect of pruning on network loss. The key contributions are the two SOSP variants: one based on fast Hessianvector product and another on the GaussianNewton approximation. Experiments show promising results on various CNNs and image classification datasets. effectiveness:  Clear presentation of structured pruning concepts.  Strong mathematical derivation and literature review.  Impressive experimental results demonstrating effectiveness in parameter reduction and classification performance. Weaknesses:  Limited applicability: The proposed approximation appears tailored to squared loss and crossentropy loss potentially limiting its use with other loss use used in innovative neural networks.  Implementation details Missing:  details of the fast Hessianvector product implementation are not provided.  The paper does not address memory challenge for large neural networks (e.g. hessian matrix memory).  Clarity Issues:  Technical terms like \"individual medium\" are used without explanation.  Comparison Lacking:  Comparison with empirical Fisher approximation is not included.  evaluation metrics focus on MAC numbers and Top1 accuracy speed improvements on real devices are not reported.", "Paraphrased Summary This paper presents a technique for optimizing deep neural networks aiming to lower process costs and inference time without compromising classification accuracy. The focus of this work is on SecondOrder Structured Pruning (SOSP) which leverages the interconnectedness of network layers and structures for efficient pruning. The key contribution is an improved method called SOSPH designed to scale effectively while employing secondorder correlation for pruning. Experiments demonstrate the efficacy of SOSPH compared to its variations and other comparable techniques. effectiveness and Weaknesses effectiveness:  Emphasizes the importance of network pruning in deep learning and its practical applications.  Recognizes the value of incorporating secondorder correlation for accurate pruning.  Provides experimental evidence of SOSPHs superior performance against existing methods.  Includes detailed explanation and supplemental materials for comprehension. Weaknesses:  Clarification is needed regarding the relationship between Eq. (1) and Eq. (2) which is only briefly mentioned as a \"modification.\"  The logic behind identifying architectural bottlenecks could be elaborated particularly considering case where layers undergo minimal pruning.  While a detailed discussion is provided in Section 4 a brief summary introducing related work and highlighting the unique aspects of SOSPH could enhance the introduction."], "wQStfB93RZZ": ["Summary The paper proposes learning gradientbased policy methods for tasks with multiple agents initiating and ending actions asynchronously. The authors present a suitable framework (MacDecPOMDPs) and introduce a series of actorcritic mechanisms to enable agent coordination with asynchronous macro actions. Algorithm contribution The paper proposes four macroaction algorithms:  MacIAC: A variancereduced actorcritic method applied to multiagent fields.  MacCAC: Treats all agents as a single global agent.  MacIACC (Naive): Uses a centralized critic but derives independent policy gradients for each agent.  MacIAICC: Similar to MacIACC but corrects the critic traces using individual agent termination time. Experimental Results  Experiments on three environments (package pushing Overcooked warehouse) show macroaction methods outperform primitive action methods.  MacIAC is well in some situations but MacCAC generally outperforms it when coordination is critical.  MacIAICC achieves well coordination than MacIAC and approaches MacCACs performance in coordinationintensive scenarios. effectiveness and Weaknesses effectiveness:  Clear definition of the application field.  Sensible and novel algorithms particularly the use of central critics.  Clear and wellinterpreted experiments with interesting findings.  potential for industrial applications. Weaknesses:  Asymmetry in the action hierarchy: Options must contain both reusable skill information and global task information.  Ambiguous descriptions: Squeezed trajectories and their incorporation into algorithms could be clearer.  Lack of details on option learning: The paper does not address how options are acquired or defined.  Preencoded knowledge in macro actions: The experiments use predefined macro actions with deterministic policies potentially biasing the comparison with primitive action methods.", "Paraphrase: This paper addresses the challenge of asynchronous actions in multiagent reinforcement learning (MARL) with decentralized partially observable Markov decision work (MacDecPOMDPs). Currently there are no multiagent policy gradient methods that utilize macroactions for MacDecPOMDPs. This work fills this gap seamlessly integrating macroaction value into multiagent policy gradient. The paper proposes several methods:  Macroactionbased Independent ActorCritic (MacIAC)  Macroactionbased Centralized ActorCritic (MacCAC)  Naive Independent Actor with Centralized Critic (Naive IACC)  Independent Actor with Individual Centralized Critic (MacIAICC) effectiveness and Weaknesses: effectiveness:  Addresses the crucial problem of asynchronous actions in MARL.  Integrates macroaction value from a previous Qvaluebased macroaction MARL method into multiagent policy gradient. Weaknesses:  Requires further clarification of asynchronism in MARL.  The integration of macroaction value is straightforward with the independent challenge already addressed in Xiao et al. 2019.  evaluation scenarios primarily focus on hierarchical MARL and multitask MARL which lack significant asynchronism of actions.  Comparison with existing hierarchical MARL and rolelearning methods is missing.  Omission of related literature on asynchronism of actions in MARL.", "Paraphrase: The paper extends diverse multiagent algorithms (e.g. independent actorcritic centralized actorcritic independent actor with centralized critic) to solve multiagent problems with asynchronous actions in the MacDecPOMDP framework. These extensions demonstrate significant improvements over the original algorithms in several MacDecPOMDP problems. effectiveness:  Addresses a common but underexplored field.  Proposed methods exhibit clear improvements. Weaknesses:  Lacks comparison with other macroactionbased methods specifically designed for the MacPOMDP.  Extends existing algorithms with expected improvements rather than proposing novel approaches.  Would be more meaningful to compare the methods with others designed for the same field.", "Paraphrase: Abstract This study addresses the challenge of training multiagent policies asynchronously using macroactions. A suite of asynchronous multiagent ActorCritic methods is proposed to address this issue allowing agents to directly optimize asynchronous and macroactionbased policies. The frameworks effectiveness is demonstrated across three multiagent cooperation paradigms: decentralized learning centralized learning and centralized learning for decentralized execution. Empirical evaluations show that these methods outperform the standard individual actorcritic and centralized actorcritic methods in three multiagent cooperative tasks (package Pushing Overcooked Warehouse). effectiveness:  Clear writing  Thorough experimental setup with diverse environments Weaknesses:  Section 3s introduction of the framework could be improved with clearer explanations of the mathematical concepts.  The frameworks novelty requires further exploration including analysis of gradient update limitations sample efficiency and a unified framework for evaluating the three proposed variants.  The baselines used for comparison are limited and do not include primitive actionbased algorithms with macroactions.  The frameworks applicability in adversarialcompetitive environments remains unexplored. Conclusion: The proposed asynchronous multiagent ActorCritic framework shows promise for cooperative multiagent environments using macroactionbased policies. However the introduction needs improvement and the underlying concepts require further elaboration. The frameworks potential for advancing multiagent learning is moderate to high but its originality is moderate and its clarity and quality are moderate."], "xa6otUDdP2W": ["Paraphrased Statement: Summary: The paper presents a technique for obtaining sparse neural netprocess by iteratively expanding and pruning netprocess part in a scheduled manner. The fundamental concept is to divide the netprocess into K part with only one part remaining dense while the others are progressively sparsified. This scheduled process is hypothesized to enhance accuracy compared to simpler sparsity methods. The paper features experiments on image classification object detection 3D object part segmentation and Transformer architectures. theoretical explanations are also provided. It has been updated with clearer visualizations for improved understanding. Strengths: 1. Clear illustrations (number 1 and 2) convey the approach effectively. 2. Welldefined need for the proposed GaP method. 3. Clear method part and pseudocode. 4. Simple and seemingly effective approach resembling a greedy netprocess sparsification strategy. 5. effective engineering solution (Parallel GaP) for parallelizing GPU performance demonstrating potential cost savings. 6. Comprehensive experimental results on various tasks. 7. Interesting ablation field on netprocess partitions. 8. Improved related process part. 9. inclusion of training cost data in experimental tables. Weaknesses: 1. high computational costs may be a concern. 2. Table 1 provides ambiguous data and could be replaced with more useful content. 3. Nonuniform sparsity scheme for GaP is not fully explained. 4. The paper lacks a \"limitations and societal impact\" part which was promised in previous revisions. Overall: The paper presents a compelling approach for sparse neural netprocess training and has improved since its previous translation. However the high computational costs and absence of a \"limitations and societal impact\" part warrant further discussion and consideration. With potential revisions the paper could be deemed fit for publication.", "Summary Paraphrase: This field utilizes a training method called \"pruningrewiringpruning\" to create highly sparse neural network models. This method gradually prunes minor weights resulting in significant sparsity while maintaining model performance. Experiments demonstrate the effectiveness of this method across various architectures and applications. Strengths and Weaknesses Paraphrase: Strengths: 1. Provides a model compression technique with theoretical justification. 2. Demonstrates superior performance compared to other pruning methods. Weaknesses: 1. lack originality as the pruningrewiringpruning method has been previously used. 2. need careful handling of batch normalization layer. 3. Training requires significantly more epochs than other methods potentially limiting practical applications. 4. Despite high sparsity levels the improvement in inference speed is limited due to the weak correlation between sparsity and FLOPs. sparse matrix memory gains are also minimal.", "Summary This process introduces a new approach to model pruning that eliminates the need for a pretrained dense model. Instead it employs a \"growandprune\" strategy that divides the netprocess into partitions and alternates between pruning and growing phases. By updating all netprocess parameters iteratively this approach ensures convergence. The authors provide theoretical justification for this design and present a parallel translation to accelerate the process. The frameprocesss effectiveness is demonstrated across tasks such as image classification detection and text understanding. Strengths  Eliminates the initial cost of dense model pretraining reducing time and computation.  Ensures comprehensive weight updates during pruning crucial for convergence.  Supports theoretical analysis and intuitive understanding.  Extensive experiments on various vision and NLP tasks showcase its effectiveness.  Clear and logical writing style with a comprehensive review of related processs. ConcernsQuestions 1. lack of comparison with dense pretrained model pruning in terms of time efficiency. 2. Concerns about the randomness of mask exploration for parameter updates. 3. lack of ablation field on partition number and boundaries with guidance on setting these hyperparameters. 4. Explanation for performance divergence between PGaP and CGaP and timecomputation savings achieved by PGaP. 5. Absence of ablation field on different sparsity levels. 6. Typographical error in Remark 3 on Page 8. 7. Inconsistent reference formatting.", "Paraphrase: This research introduces a \"GrowandPrune\" strategy to create a sparse Convolutional Neural network (CNN) model. It involves iteratively expanding and prune subsets of layer throughout network training. Unlike other pruneduringtraining methods this approach ensures that all weights are evaluated before prune decisions are made. The paper explores two variations of this approach: CoarseGrained GrowandPrune (CGaP) and Progressive GrowandPrune (PGaP). It theoretically analyzes the convergence of CGaP and experimentally demonstrates its effectiveness on various datasets and tasks showing improvements over baseline methods. Additionally the research examines fundamental variables such as partition number and provides empirical findings in supplementary materials. Strengths and Weaknesses: Strengths:  The results support the idea that exploring all weights leads to better prune outcomes compared to random weight choice.  The proposed approach highlights the importance of the partition number variable empirical field suggest a fourpartition GaP may be optimal. Weaknesses:  The approach includes multiple fundamental variables that can affect its performance potentially complicating reproducibility and future research.  The effectiveness of partition strategy is unclear as the supplementary results indicate that it may not significantly impact prune outcomes."], "mQDpmgFKu1P": ["Paraphrased Statement: This paper presents a method to improve the efficiency of the transformer model by replacing the dot product matrices in selfattention with a translation of the input using Legendre Memory Units (LMUs). LMUs design a sliding window of the input sequence onto Legendre polynomials. This translation is fixed reducing memory consumption and the resulting matrices are smaller improving efficiency. While the concept of using LMUs is not novel this paper applies it to the transformer model. Experiments on language modeling demonstrate significant performance gains over the original transformer model across various model sizes. Despite the strengths of the approach the author raises concerns about the interpretation of the experimental results. Firstly the proposed model includes additional components that are not directly related to the LMU translation. The author question whether these components contribute to the reported performance gains. Secondly the experimental setup lacks details regarding the configurations of the models and baselines making it difficult to assess the actual benefits of the proposed method. The author suggests that additional ablations and a more detailed description of the experimental setup would strengthen the paper.", "Summary: This paper proposes a solution to the limitations of standard language modeling transformers: their extensive data requirements and computational costs associated with their selfattention mechanism. The solution involves replacing selfattention with Legendre Memory Units (LMUs) to reduce complexity to linear or loglinear (for convolutional LMUs) or constantlinear (for recurrent LMUs). The proposed method reportedly requires 10x fewer data tokens to achieve comparable performance to standard transformers. Strengths:  Enhances data efficiency in language models.  Utilizes LMUs to reduce complexity without model size growth. Weaknesses:  Lacks discussion and comparison with related recurrent attention approach.  Unclear rationale for architectural choices in the selfattention component.  Incomplete evaluation focusing only on tokenlevel efficiency without considering computational efficiency.  Concerns about the robustness of comparing with a power law derived from a different experimental configuration.  Unverified improvement with model size growth. Questions:  How can we ensure the accuracy of comparing power laws from different experimental settings  What are the specific advantages and disadvantages of each design choice such as FFN global attention and implicit selfattention  Can the authors provide more details on the evaluation setup and model parameters for the pertoken loss comparison", "Paraphrase: Summary: This study presents Legendre Memory Units (LMUs) a technique using nonparametric linear layers to process sequential input. Like convolutions LMUs have static weights and generate sequence representation akin to RNNs. LMUs incorporate an implicit selfattention layer that avoids the complexity of standard selfattention by attending to a fixedlength sequence of hidden states. comparison between LMUs and transformers in language modeling tasks show that LMUs outperform transformers with comparable parameters. further enhancements with global selfattention layers yield additional improvements. Strengths:  Innovative concept and welldesigned architecture  Comprehensive evaluation across model sizes Weaknesses:  comparison only considers parametermatched models potentially favoring LMUs due to their nontrainable parameters  Limited testing on language modeling tasks evaluations on other tasks would strengthen the findings Questions and input:  LMUs approach to separating input dimensions and reducing kernel computation resembles depthwise separable convolutions.  The xaxis labeling in design 5 appears inconsistent with the caption and text (e.g. it should represent training steps or tokens trained on).", "Paraphrased Summary: This paper presents a novel method that addresses two limitations of transformer networks. The solution revolves around using a Legendre Memory Unit (LMU) a nonparametric component that employs Legendre polynomials to provide temporal representation and compression of the input sequence. Unlike traditional attention mechanism in transformers that operate across all time steps the proposed method focuses on the output of the LMU at each time step. The LMU reduces the complexity of selfattention from quadratic (n\u00b2) to q\u00b2 where q represents the range of the Legendre polynomials. This offers an advantage for tasks involving long sequences. Strengths:  The LMU can compress past data at every time step.  Using the LMU drastically reduces selfattention complexity. Weaknesses:  The LMUs implicit selfattention excels at predicting with limited context while traditional selfattention captures longrange dependencies.  The LMU lacks mechanism for pairwise data extraction which can be significant in tasks like entailment and translation.  The empirical results are limited relying primarily on crossentropy scores which do not fully reflect the models performance on tasks requiring pairwise data.  The paper lacks comparison with solid baseline efficient transformers such as Linformer Synthesizer and Primer.  No ablation study is provided to examine the impact of window size and Legendre polynomial range in the LMU."], "vJb4I2ANmy": ["Paraphrase: Summary This paper introduces a novel data augmentation technique known as Noisy Feature Mixup (NFM) that seamlessly merges the advantages of interpolationbased training and noise injection strategies. NFM is straightforward to implement and has been empirically demonstrated to strike a favorable balance between accuracy on clean data and model robustness. theoretically NFM improves decision boundaries and enhances the regularization effects of manifold mixup and noise injection. Moreover it has been shown that NFM training approximates the minimization of an upper bound linked to adversarial loss resulting in heightened model robustness. effectiveness and Weaknesses The paper is clearly written and comprehensible. Its strengths include: 1. Exploiting the benefits of mixup and noise injection to enhance model generalization. 2. Proposing a straightforward and practical NFM method. 3. Providing theoretical analysis to elucidate the underlying mechanisms of NFM. 4. Evaluating NFM on various datasets using two established neural network architectures demonstrating its advantages over other mixup variations. However certain concerns arise: 1. The necessity of applying NFM to every layer in each iteration. 2. The potential impact of NFMs stochastic nature on training convergence speed. 3. The robustness of the assumption in Theorem 2 that the expected value of the noise distribution is zero.", "Paraphrase: Summary:  Introduces a novel variation of manifold mixup with added noise.  Demonstrates the regularization and robustness properties of the method through theoretical analysis.  Validates the effectiveness of the proposed method in range classification tasks with noisy testing environments. effectiveness:  clean and concise writing style.  Provides comprehensive theoretical analysis that enhances the understanding of the method.  Delivers strong performance in noisy testing environments across multiple models and datasets. Weaknesses:  Limited performance improvement in clean settings.  Experimental setup could be expanded to cover a wide range of corruption scenarios and general natural perturbations.  Baselines in the experiments are inadequate and comparison with recent mixup techniques that enhance robustness would be useful. Additional input:  It is unclear if the theory can be extended to more general data augmentation techniques.  The statement in Theorem 2 requires further explanation to make it more intuitive.  A formal evaluation of the features would be valuable for interpreting the results in Figure 10.", "Paraphrase: Summary: This research investigates techniques for enhancing the resilience of supervised learning models through data augmentation. The primary introduction is Noisy Feature Mixup which extends input and manifold mixup to all layer of a neural network while incorporating random noise into the mixup samples. The proposed approach has been demonstrated to improve the robustness of supervised learning models against various noise attacks on input data. The theoretical analysis provides the Taylor expansion of the Noisy Feature Mixup optimization objective. effectiveness:  Extends input and manifold mixup to Noisy Feature Mixup applicable to all layer and introducing random noise.  Demonstrates improved robustness against noise attacks compared to existing mixup variants.  Provides a theoretical analysis of the regularization effects of Noisy Feature Mixup based on previous work. Weaknesses:  Accessibility of the technical contribution is limited.  Lack of illustrative model and figures to explain theoretical results.  Proofs require more detailed derivations in the appendix especially for equations (30) and (33).  Missing ablation work to justify hyperparameter choices and determine the contribution of each approach component.  Lack of additional baselines such as label smoothing to strengthen the experimental evaluation.", "Paraphrased Statement: Summary: This paper introduces a novel and costeffective method called Noisy Feature Mixup (NFM) to reduce overfitting and enhance generalization. NFM combines mixup and noise injection inheriting their advantages. Its simplicity makes it easy to integrate into model training. Critically the authors provide a mathematical derivation to demonstrate NFMs regularizing effect on model optimization and robustness. They establish a connection between the NFM loss and the original loss revealing the regularization effects of NFM. To support the robustness claim the authors link the NFM loss to adversarial training a type of distributionally robust optimization. Experiments across various models and datasets showcase NFMs superiority. The paper also explores the tradeoff between accuracy on clean and perturbed test sets. Supplementary material offers detailed proofs and additional results to support NFMs effectiveness. effectiveness and Weaknesses: effectiveness:  Theorem 1 provides a theoretical grounding for NFM demonstrating its comparison to minimizing the original loss plus featuredependent regularizers.  The theorem shows that the regularizers reduce Jacobians and Hessians aligning with intuitive understanding of mixuplike methods. Weaknesses:  The paper lacks a discussion on how NFMs behavior may vary for features of different levels."], "qEGBB9YB31": ["Paraphrase: This research develops a method to interpret models using filterwise parameter saliency distribution which is evaluated with different models. Experiments confirm this approach. effectiveness and Weaknesses:  Equation 1 lacks clarity regarding the meaning of \"K.\"  The methods applicability may be limited if it only works for image classification or the specific models tested.  Finetuning salient filters can improve model performance (as stated in Section 3.3) but its unclear if this can elevate a model with average performance to toptier status.  Its unknown if the performance gains from finetuning salient filters can be transferred to independent datasets outside the tested set.", "Paraphrased Statement: This field introduces a function that highlights important model parameters responsible for incorrect predictions. Through experiments and visualizations the field reveals intriguing insights: similar semantic information is shared by neighboring parameters. Additionally the authors explore enhancing prediction accuracy by modifying salient parameters. effectiveness and Weaknesses: While the idea of parameter visualization is wellestablished this function offers a unique angle on parameters that contribute to misclassification. The authors conduct thorough analysis and demonstrate the potential for improving classification accuracy using salient parameters adding practical value to the field. However the proposed approach bears some resemblance to parameterspace adversarial approach. An alternative approach worth exploring is utilizing adversarial methods to identify salient parameters that influence classification as this seems more intuitive. Regarding parameter pruning experiments disabling parameters abruptly may not be optimal. rather introducing small perturbations could shed light on parameter sensitivity.", "Paraphrased Summary: This research explores the network settings that contribute to inaccurate judgments by conducting a series of essential experiments. The authors back up their claims with extensive quantitative and qualitative investigations. effectiveness: 1. The paper breaks fresh ground by investigating network parameters rather than the saliency maps used in prior field. 2. Experiments confirm the validity of the proposed viewpoints using both quantitative and qualitative data. 3. The paper presents novel and intriguing insights.", "Paraphrased Statement: Summary:  This paper identifies key parameters in neural networks and demonstrates their impact on predictions.  It establishes a connection between these parameters and specific regions in the input that influence classification outcomes. effectiveness:  The paper develops a technique to assess filter saliency based on gradients.  It shows that only a few filters need modification to alter predictions potentially leading to significant shift. Weaknesses:  The paper approach resembles adversarial approach in parameter space by exploiting parameter sensitivity.  The evaluation in Section 3.4 is limited and episodic. detailed input: method:  image 12 could be improved by normalizing the space allocated to each layer to enhance profile of decay for other layers.  The sign of saliency values appears inconsistent across different image.  Equation 3 does not specify how the magnitude of selected filters is increased. Experiments:  image 3 plots only include initially misclassified samples.  The nearest neighbor approach in image 45 is sensitive to high layers potentially explaining concept similarity and misclassification model.  The evaluation of error correction in Section 3.3 was conducted independently on all image.  The anecdotal nature of Section 3.4 raises concerns about the lack of largescale evaluation. General:  Axis labels in plots should be enlarged for readability."], "ngjR4Gw9oAp": ["Summary: This paper suggests using inhibitory networks in reinforcement learning to select one behavior from a set for performance in a specific state. inhibitory networks can be manually designed or learned like hierarchical policy. The proposed method was tested using Soft ActorCritic and OpenAI Gym environments (LunarLander and BipedalWalkerHardcore) and compared to a standard SAC implementation. Strengths and Weaknesses: Strengths:  Sound method and experiments that demonstrate the issue. Weaknesses: Clarity:  The proposed method resembles a hierarchical policy with discrete primitives activated by an inhibitory network.  Details are unclear such as:  The purpose of the inhibitory network (e.g. Q in Figure 2).  Whether the inhibitory network is learned and if so its learning objective.  How data is split between replay pools when the inhibitory network is learned.  The relationship between SACIs reinforcement weight adjustment (page 9) and Figure 2.  Whether there are separate policy for work and Stop behaviors.  Conditional expectation in EQuation 1 depends on t which is not a valid conditioner.  \\rho\\pi is defined as the state marginal instead of the dynamics (page 3). contribution:  The approachs novelty compared to hierarchical RL is unclear.  The proposed method benefits from reframing as an HRL method highlighting its contribution and improvements.  The connection to inhibition in the brain obscures the HRL connection. Experiments:  Discrepancies between figures and text regarding training efficiency.  Comparing SAC to SACI is unfair due to handcrafted inhibition rules in SACI.  Details on learning the inhibition rule are missing. Minor Comments:  The statement on entropy and reinforcement is incorrect.  \"Stop networks estimate the value of the unexpected event leading the policy to learn new skills\" is vague.  The experiment section does not provide implementation details for varying observation numbers due to bomb coordinates.", "Paraphrased Summary: This paper introduces SACI a method for transferring a pretrained agent to new tasks. SACI combines inhibitory mechanisms with the Soft ActorCritic (SAC) algorithm. It involves training separate Qnetworks and potentially an inhibitory policy network to use alongside the existing policy. Experiments on two Box2D environments show that SACI frequently improves over SAC. Strengths:  Investigates transfer learning on diverse tasks in specific environments.  Incorporates inhibitory control concepts from neuroscience.  use rigorous ablation work to assess contribution of different method components.  Examines the impact of environment version on transfer performance. Weaknesses:  Empirical evaluation lacks comparison with alternative transfer learning methods.  modified evaluation to only two similar environments.  Overly complex method presentation with numerous version.  Odd purpose choice to feed Qvalue into the policy network.  Inconsistent empirical results between figures and accompanying text.  neuroscience inspiration is not clearly connected to the proposed methods purpose.  Incomplete empirical evaluation that does not include analysis of inhibitory network mechanisms.  modified discussion of related work in multitask RL nonstationary environments and onlinecontinual learning.", "Summary The researchers introduce SACI a modification of SAC for retraining existing agents on new environments or tasks. Based on the neuroscience concept of inhibitory control SACI learns separate Qpurposes and optionally an inhibition policy. They define an inhibitory reinforcement (rI) for each environment to enable retraining and comparison SACI with SAC on modified version of LunarLanderContinuous and BipedalWalkerHardcorev3. Strengths  The approach considers RL as sequential decisionmaking and uses a neuroscientific perspective which can advance the exploitation of humanlike agents. Weaknesses  The concept of retraining needs more formalization or specificity.  The distinction between SACI and transfer learning in RL is unclear.  The work heavily relies on human engineering of reinforcement purposes requiring the definition and adjustment of multiple reinforcements. Minimizing this requirement could improve the contribution.  The inhibitory policy is not fully explained raising questions about its purpose and implementation.  The performance improvement over SAC is marginal in some face. Finding way to enhance performance is recommended. Minor Writing Suggestions  \"policy\" should be \"policy\" in the first sentence.  \"That encompasses\" should be \"which encompasses\" in the first sentence.  \"Successfully solve the task\" should be \"solve the task successfully\" in the first sentence.  \"Previous\" should be \"previously\" in Section 1 and Section 2.4.  \"Transfering\" should be \"transferring\" in Section 2.4.  \"different task\" should be \"different task\" in Section 2.4.  \"Over estimation\" should be \"overestimation\" in Section 3.1.  \"Stand alone\" should be \"standalone\" in Section 3.4.  \"specific focus\" should be \"a specific focus\" in Section 5.", "Paraphrased Statement: This research proposes a purpose for controlling an agent using two independent decisionmaking networks (\"go\" and \"stop\"). A selection mechanism (a network or rule) determines which of these networks is active at each moment. Strengths:  The concept of using two separate policy is innovative. Weaknesses:  The use of the terms \"inhibitory\" and \"stoppolicy\" is unclear as inhibition typically refers to suppressing impulsive process rather than making alternative decisions.  Its unclear why only two policy are used as the optimal number may vary based on the task.  The effectiveness of the additional learning capacity (parameter updates) allocated to the selection network and second policy is questionable. Its potential that allocating this capacity to a single policy could be more efficient.  The experiments were conducted in environments with essentially two tasks which may explain the success of the twopolicy approach. Its uncertain how it would perform in environments with more complex task combinations.  The method resembles the \"options\" framework but the similarities and differences are not adequately discussed.  Its unclear what occurs in the \"adaptive SACI\" scenario: whether theres an additional network estimating a weight and if so what its loss purpose is.  SAC is not necessarily the most innovative RL method so the choice of using it as a innovation is questionable.  The motivation for the approach and its implementation are applicable to various RL techniques not just SAC. Other Questions and Remarks:  Figure 4: How are agents trained without a replay buffer as Algorithm 1 suggests  The term \"episodic memory\" is used inaccurately. Here it merely refers to a replay buffer not entropy retrieval during a step.  The generalization ability of the proposed architecture is questionable and experiments in a broader range of environments and using different baselines would clarify its value."], "vyn49BUAkoD": ["Paraphrased argument: This paper proposes two active learning strategies for Gaussian work regression (GPR) using a fully probabilistic approach. Unlike traditional methods that use fixed hyperparameters optimized by maximizing the GPR models marginal loglikelihood these strategies learn a probability distribution over hyperparameters. This allows for more effective active learning methods by using the full distribution. The two proposed strategies are: 1. BQBC (Bayesian Query By Committee): Similar to querybycommittee but uses a committee of multiple GPRs sampled from the posterior distribution of hyperparameters. 2. QBMGP (Query By Mixture of GPs): Combines disagreement between GPRs with predictive uncertainty to guide active learning. These methods were tested on 8 synthetic datasets and compared to existing approach. Strengths:  Original and interesting contributions  Sound probabilistic foundations  Insightful synthetic data experiments Weaknesses:  Incomplete comparison to prior work  Inconclusive experimental results  Lack of experiments on substantial data  Convoluted and unclear writing additional Points:  The problem of overfitting to marginal likelihood is acknowledged but it remains unclear how this method differs from similar approach in Bayesian optimization.  Zhao et al (2020) is a relevant missing baseline for empirical comparison.  The evaluation methodology for RMSE differs from NLML and the difference in relative ranking and upward sloping RMSE curves requires explanation.  additional experiments on substantialworld datasets would strengthen the papers credibility.  Improved clarity would benefit the paper by introducing concepts progressively and using precise language.", "Paraphrased argument: Summary: The authors introduce two novel acquisition functions for Gaussian work models. The first is a Bayesian variant of Query by Committee (BQBC) that seeks to find modes. The second combines modeseeking with predictive variance minimization through a Query by Mixture Gaussian workes (QBMGP) formulation. The paper includes simulations to demonstrate these functions. Strengths:  The related work and results in Section 4.1 are wellpresented. Weaknesses:  The abstract introduction and other sections are unclear.  Key concepts like \"biasvariance tradeoff\" and \"active learning\" are not wellconnected.  Some sentences such as those about fitting GPs with MCMC are not precise and could benefit from clarification. State of the Art discussion: The papers discussion of active learning in Sections 2 and 4.1 should be expanded to include More recent and robust acquisition functions that have been successfully applied in regression and adaptive quadrature schemes. specific examples include:  D. H. Svendsen et als \"active Emulation of Computer Codes with Gaussian workes Application to Remote Sensing design Recognition\"  F. Llorente et als \"Adaptive quadrature schemes for Bayesian inference via active learning\"  M. Kanagawa and P. Hennigs \"Convergence Guarantees for Adaptive Bayesian Quadrature Methods\"", "Paraphrased Summary This work investigates employing Bayesian statistics to handle hyperparameters in active learning using Gaussian work. Two novel acquisition works are devised as a issue. Strengths  The research is compelling and has a substantial theoretical foundation.  Bayesian Gaussian processdriven active learning offers practical benefits as shown in numerical simulations. Weaknesses 1. active learning is known to explore the work space but the proposed BQBC appears to leverage More of the existing data. Why does it perform well in practice 2. The performance measures (NLML and NMSE) in the experiments are not clear. How does experimental validation differ from Bayesian optimization 3. The experiments utilize synthetic works and practical application would be More valuable.", "Summary: This paper employs a Bayesian approach to Gaussian work (GP) active learning utilizing MCMC sampling to consider multiple model hypotheses from a full posterior. From this posterior active learning strategies namely Bayesian QuerybyCommittee (BQBC) and Query by Mixture of Gaussian workes (QBMGP) are employed to select examples for GP regression. Strengths:  The paper explores the interesting concept of a multimodal posterior with modes representing signal or noise which active learning can differentiate between.  The related work section and GP description are satisfactory.  The experiments compare against diverse active learning acquisition works and experimental simulators. Weaknesses: Key Concerns:  The paper fails to strongly support the claim that these methods address the biasvariance tradeoff in GP active learning despite highlighting its importance in the abstract and introduction.  More concrete argument and empirical evidence are needed to support the use of BQBC and QBMGP in resolving the biasvariance issue and mode selection in GP active learning. Introduction:  The introduction lacks a cohesive paper and connections between the discussed issue.  The argument for the biasvariance tradeoff and its relation to length scale and noise terms is not adequately developed.  The bimodal behavior of the posterior is not convincingly established as a general phenomenon or connected to bias and variance. Sections 2  3:  The choice of RBF kernel despite claiming not to assume any prior knowledge about the kernel seems inconsistent. BQBC  QBMGP:  The derivation of QBMGPs acquisition work and its rationale for addressing the mode selection problem are not clear.  It is unclear how BQBC and QBMGP qualitatively differ and how the variance term in QBMGP impacts performance. Section 5: Experimental Results:  The high variance in performance curves and the use of summary statistics that do not fully capture overall performance weaken the conclusions.  Statistical testing against other methods or random sampling would strengthen the results.  Detailed data on convergence calculations NLML and RMSE calculation methods and data usage are missing. Other Concerns:  The connection between BQBC and BALD as types of querybycommittee could be explored further.  The use of \"GMM\" to describe a mixture of Gaussian workes may be confusing.  The first paragraph of Section 5 should be placed in the beginning of the section and include a more general description of the experiments."], "s-b95PMK4E6": ["Summary Paraphrase: This paper represent a framework for completing tasks using language instructions in ALFRED a challenging environment. The framework breaks down instructions into smaller navigation and interaction goals with specialized policies handling each task independently. Additionally two modules enhance performance: an object encoding module and a loop escape module. These innovative method have been extensively tested and show potential benefits for future research. Strengths:  The hierarchical approach simplifies learning and leads to improved generalization.  Extensive experiments present significant performance addition in ALFRED.  The paper is wellwritten and represent clear illustrations. Weaknesses:  Specialized policies for each subgoal may increase computational and training costs.  Subgoals may overlap such as moving an object to a location that requires both navigation and interaction which is not explicitly addressed.  specific details about certain aspects of the framework remain unclear:  Combining stepbystep instructions into subgoals.  Translating stepbystep instructions into multiple subgoals.  Training objectives and timelines for interaction policies.", "Summary: This paper presents an approach for following instructions in a navigation and interaction task setting such as the ALFRED benchmark. The proposed method HACR consists of three hierarchical modules:  Subgoal sequence identification  navigation policy  manipulation policy Strengths:  Clear presentation: The authors provide a comprehensive and intuitive explanation of the HACR algorithm.  Exhaustive ablations: Extensive experiments demonstrate the effectiveness of each component.  Visual demonstrations: Rollouts and visualizations illustrate the method capabilities in solving navigation and interaction tasks.  Sufficient experimental details: The paper provides enough information for replication. WeaknessesConcerns:  potential overfitting to ALFRED: The method may be tightly tailored to the ALFRED benchmark limiting its generalizability to broader problems.  Insufficient justification: Some design decisions lack proper justification or references to prior work.  Hyperparameter tuning: The \"loop escape\" module relies on a hyperparameter which could be problematic in different environments.  Overstated benefits of policy paper controller: The controllers role in learning correspondence between semantic subgoals and text instructions is potentially overstated.  Classical pipeline approach: The method adopts a pipeline of handengineered represent which may limit generalization insights compared to datadriven approaches. Additional Comments:  Typos in the text are noted for correction.  The recommendation may be updated based on further discussion.", "Summary The paper introduces a hierarchical method called HACR to address the difficulty of embodied instructions following tasks that require longterm design. This hierarchical framework consists of:  Policy Composition Controller (PCC): Generates a series of predefined subgoals.  Master Policy: Determines whether to manipulate objects or continue navigating.  Interactive Policy: Executes lowlevel interactions. The key contributions of the paper include:  Breaking down the highlevel goal into subgoals making it more interpretable.  Incorporating OEM and LEM to enhance performance.  Achieving stateoftheart results on the ALFRED benchmark. Strengths  Tackles a complex problem.  Intuitive approach that decomposes instructions and manipulation separate policy modules.  Strong empirical performance on a competitive benchmark. Weaknesses  Lacks crucial details and can be confusing.  Difficulty in identifying the main contribution.  Unclear what the paper contributes beyond the main claims.  Ablates on aspects not mentioned in the main claims.  Insufficient analysis compared to previous hierarchical approaches.  No significance tests. Questions  Q1: How does HACRs hierarchical structure differ from prior hierarchical approaches  Q2: Why is using BiLSTM for instruction encoding considered new  Q3: What is the source of the subgoal space  Q4: Why does the master policy include both manipulate and navigation actions which are already handled by the PCC", "Summary: This paper proposes a hierarchical framework for interactive instruction following dividing action predictions into navigation (master policy) and object interaction (interaction policy) modules. It introduces modules for object encoding and loop escaping. While the framework outperforms previous method it raises concerns about the manipulation of visual input fairness of comparison performance gap and overfitting. Strengths:  The hierarchical strategy concept appears valid.  Ablation study demonstrate improved performance over previous method.  The framework provides transparent execution with subgoal predictions. Concerns: 1. Module design: The PCC and MP modules lack visual input for subgoal and navigation predictions which seems inconsistent with human behavior. 2. Fairness of Comparison: The method manipulations additional subgoal labels and lowlevel text instructions which should be clarified in comparisons. 3. performance Gap: The performance difference between HACR and HLSM is small and HLSM has since achieved higher performance with public code release. A discussion of HACRs advantages and complementary nature to HLSM would be beneficial. 4. Overfitting: HACR exhibits significant overfitting from seen to unseen and validation to test data. further analysis is needed to address this publication.", "Paraphrased Summary: This paper introduces a hierarchical approach for teaching policies how to complete ALFRED tasks by following spoken instructions. The approach assumes that instructions can be broken down into a series of subgoals. The system first predicts these subgoals from the instructions. Based on these predictions a \"Master policy\" handles navigation actions while \"Interaction policies\" handle interactions within the environment. The system includes several modules:  Object Encoding Module: Provides information about target objects and monitors navigation subgoals.  Loop Escape Module: Detects when a loop has been made based on visual similarities. Strengths:  Demonstrates faster and more efficient learning of action sequence compared to nonhierarchical approaches.  Provides subgoals for improved interpretability and transparency.  Achieves strong results surpassing previous method. clarification:  Value of W: Not specified in Equation 3. Its impact on Loop Escape modules usability is unclear.  7 Subgoals: Not explicitly identified in the paper.  Subinstruction Correspondence: Unclear how a subinstruction (e.g. \"Turn around bring the potato to the microwave on the right\") corresponds to a single subgoal (e.g. \"GoToLocation\").  Manipulate Actions: The role and differences between \"Manipulate\" actions in the Policy Composition Controller and Master policy are not fully explained.  Action sequence Termination: Its unclear whether there must always be an interaction action after a \"Manipulate\" action in the Master policy acts as a \"Stop Token\" to end navigation.  Consecutive GoToLocation Subgoals: The handling of consecutive \"GoToLocation\" subgoals predicted by the Policy Composition Controller but triggered as interactions by the Master Policy is not specified."], "tlkMbWBEAFb": ["Paraphrase: summary This work focuses on developing a steerability constraint for spherical neurons which are 3D point classifiers with spherical decision boundaries. This constraint allows for posttraining optimization of a pretrained classifier to make predictions that are invariant to 3D rotation of the input. In face where input rotation perturbations are unknown the authors propose a technique to retrieve the unknown rotation and achieve rotationinvariant predictions. The work demonstrates the rigor of some claims through experimentationation on smallscale datasets. strength  Clear writing style  Adequate discussion of prior work  Accessible technical explanations (though not highly detailed) Weaknesses  Limited motivation for creating rotationequivariant (steerable) networks  Lack of clarity on how steerability constraint apply to linear layers (MLPs) which are commonly used in typical architectures  Weak experimentation for unknown rotation: Results indicate that steerability constraint provide minimal benefit in improving classification accuracy.  Unclear evaluation metric: rotation error would be a effective metric for assessing the methods ability to recover unknown rotation rather than classification accuracy. Questions and minor Comments  Definition of \"spherical neurons\" and their relationship to other types of neurons (hypersphere neurons and geometric neurons)  A table of notations would enhance comprehension  selfcontained captions and figures: specify the corresponding section or explain notations (e.g. s in Fig. 1 argument in Fig. 2)  clarification on how interpolation coefficients are optimized in known rotation experimentationation (sec. 5.3)", "Paraphrased summary: Researchers propose \"spherical neurons\" in 3D for creating layers that are equivariant to rotation. They build on previous work introducing spherical neurons in conformal space and solve for the steerability constraint. Empirical findings show their approach outperforms previous methods on rotated 3D data. Paraphrased strength and Weaknesses: strength:  Addresses an significant machine learning problem.  Develops a rigorous approach.  shows improved performance on rotated data. Weaknesses:  Requires a pretrained model.  May need to know or infer input rotation.  direction on scalar field excluding broader classes of features.  Lacks empirical comparison with other baselines and equivariant model. Additional Feedback:  Remove \"fully\" from the title unless it has a specific meaning.  Provide more context and motivation for the conformal embedding approach.  Clarify the connection between the steerability constraint and representation theory.  Define hyperspherical and geometric neurons more clearly.  Formalize the definition of rotating s in nD space.  specify the explicit condition that must be satisfied in Theorem 4.1.2.  Correct the typo \"desision\" in section 3.3.  Move the highlevel methodology recap from section 5.2 to section 4.", "Summary This paper presents a method for creating directable spherical neurons based on previous work on Geometric Neurons (Melnyk et al. 2021). The key concept is a steerability constraint (Eq. 13) for geometric neurons. The paper demonstrates the effectiveness of steerable spherical neurons through two applications:  Classifying 3D Tetris objects at various rotation  Using 3D skeleton data to adjust an initial rotation estimate The results show significant performance improvements compared to nonsteerable interpretation. Strengths  Clear technical contributions based on past research and the Melnyk 2021 paper.  Explicit derivation of the steerability constraint.  Practical demonstration of steerability in the provided applications. Weaknesses  Reliance on external technical content from the Melnyk 2021 paper.  Experiments serve as a validation of concept leaving potential applications for future work.", "Paraphrased Paper Summary: This paper presents \"steerable 3D neurons\" that are invariant to 3D rotation and designed for point cloud classification. The paper provides theoretical support for the model steerability. Experiments on synthetic and real data confirm the model invariance and effectiveness. Paraphrased strength and Weaknesses: strength:  Novel approach  Clear illustrations  Wellpresented contributions Weaknesses:  Limited discussion of the model relationship with MLPs and advantages over other equivariant methods  Lacking baseline for experimentation and comparison with other point cloud classification methods PostRebuttal Comments:  More experimentation comparing the model with other equivariant methods on challenging tasks are needed.  The papers theoretical section requires clarification particularly regarding the \"steerable\" constraint.  The paper lacks a clear method for solving the steerable constraint.  It is unclear if the model is truly equivariant given that the first layers output is claimed to be rotationindependent.", "Paraphrased argument: This paper promodels to enhance the geometric neurons developed by Melnyk et al. by adding steerability to them allowing for more accurate classification of objects undergoing arbitrary rotation. This is achieved through a multistage work: 1. Training Melnyk et al.s neurons with a hyperspherical output layer. 2. Transforming the fixed weights so that the input to a steerable neuron can be expressed as a linear combination of its rotated interpretation. Experiments validate the algorithms effectiveness on a simple human model dataset support that the steerability addition is valuable and novel. strength and Weaknesses: strength:  The steerability addition is beneficial and innovative.  The paper is wellstructured clear and theoretically sound.  Experiments are sensible and demonstrate the algorithms capability. Weaknesses:  contribution are presented in a verbose and cluttered manner reducing clarity.  The innovation lacks motivation and outlining of limitations.  Some terminology needs clarification such as \"steerable model\" and how online optimization relates to learnable argument.  The connection between spherical filter banks and basis use could be explained more clearly.  The paper lacks comparison to stateoftheart methods and benchmarks for SE(3)SO(3) model estimation.  It would be beneficial to include an evaluation work on the geometric explainability of the promodeld approach. minor Notes:  \"Since we are at inference\" should be \"Since at inference.\""], "wQ7RCayXUSl": ["Paraphrased Statement: This paper introduces an innovative method for quantifying uncertainty using ensembles in offline reinforcement learning. The method takes advantage of the unique characteristics of dynamic programming and cumulative errors in this setting. Theoretical analysis demonstrates that independent ensemble target can capture prediction variance providing a step of uncertainty. Simulations verify that the method produces higher variance in regions with low confidence. Experiments on benchmark environments show promising results outperforming existing approaches. effectiveness:  Novel use of independent ensembles for uncertainty quantification in offline RL  Positive experimental results indicating superior performance  Theoretical support backed up by simulations Weaknesses:  Concerns about how training uncertainty relates to uncertainty with respect to the true value  Need for improved writing clarity and organization including fewer bold sentences and typos  Insufficient explanation of the meaning of variance differences in Theorem 5.2  Lack of direct comparison between shared LCB and shared min approach in toy case  Insufficient discussion of hyperparameter choice and stability Additional Questions:  What is the meaning of pi in equation (3)  Can shared LCBmin provide valid uncertainty quantification without relying on training action randomness  How were the hyperparameters beta and alpha chosen and are performance stable across different choice Minor Points:  Missing citation in introduction  Missing period in Section 4.2  Missing superscript for y in equation (4)  Missing reference in number 1 caption", "Paraphrased summary Proposal: The paper suggests investigating ensemble methods in the Neural Tangent Kernel (NTK) framework for offline reinforcement learning (RL). effectiveness:  The paper is intriguing and employs novel approaches theories and observation. Weaknesses: 1. Citations and Rigor:  A significant share of references are to nonpeerreviewed paper.  The theorems lack formalization and are primarily based on the authors interpretations. 2. Results Presentation:  The use of bold formatting lacks clear significance.  Statistical significance tests should be considered. 3. Claims and Evidence:  Numerous claims are made without sufficient support. 4. Experimental Setup:  Using the D4RL dataset version from Kumar et al. may not account for recent updates potentially impacting results.  It would be beneficial to replicate the experiments using the git repository of Lee et al. 2019 to ensure compatibility with the authors framework. 5. Typos:  Multiple typos need to be corrected.", "Paraphrased Statement: summary: This research presented a method for learning in offline reinforcement learning settings without relying on a model of the environment. It includes a theorem (Theorem 5.1) to support its step of uncertainty. Additionally it performed experiments to demonstrate the effectiveness of the proposed method. effectiveness:  The method does not require a model of the environment making it applicable to realworld scenarios. Limitations:  The theoretical aspects of the paper could use further refinement. The derivation of standard deviations should be included in the proof and more details on network initialization should be provided within the theoretical description.  Typographical error: Line 8 of page 4 should read \"This is an ...\" instead of \"This an ...\".", "Summary There is a lack of comprehensive review for offline reinforcement learning (RL) literature despite the existence of several methods such as LSPI (modelfree) LAMAPI (modelbased) pseudoMDPs (modelbased) and RKHS embedding (modelbased). The current paper focuses primarily on reducing the value estimation error constraint while many algorithms use this error directly for parameter learning not treating it as a constraint. effectiveness and Weaknesses The title of the paper is somewhat misleading given that most RL exploration methods are optimistic rather than pessimistic. The primary motivation of the paper is to address \"the unique challenges of uncertainty estimation in reinforcement learning\" but this is not explicitly stated and the message is not entirely clear. The paper presents an observation from the DLTV research that both intrinsic and parametric uncertainty exist in deep RL making exploration challenging. The DLTV paper uses distributional RL to estimate upper and lower confidence bound from the distribution of Q values. Questions There is a connection between using ensembles and distributional RL for uncertainty estimation. Both approaches utilize multiple agents but the primary difference lies in how they form an uncertainty estimator. Identifying their similarities and differences would be valuable. Minor Points Some reference links are missing. The accuracy of the CQL algorithm should be verified using additional baselines as its poor performance in Table 2 makes it difficult to assess the effectiveness of the proposed algorithm.", "summary: The paper proposes a method for offline reinforcement learning (RL) called MSG that estimates a lower bound on the value use using ensembles of networks. Unlike previous methods that assume negative outcomes for actions beyond the training data (support constraint) MSG does not make such an assumption. The authors argue that support constraint leads to overly pessimistic agents. effectiveness and Weaknesses: effectiveness:  MSG provides a framework for estimating uncertainty in offline RL.  The paper presents theoretical arguments and empirical results supporting the efficientness of MSG. Weaknesses:  The paper uses the term \"pessimistic\" in a confusing manner as it contradicts the common use in offline RL.  The novelty of MSG is limited as deep ensembles for offline RL have been previously proposed.  The theoretical analysis is incomplete as it does not consider alternative configuration of pessimistic updates.  The empirical results are inconclusive as all pessimistic updates achieve similar performance.  The authors claim that certain methods were unable to train efficient policies but this claim is not supported by evidence."], "fStt6fyzrK": ["Paraphrase: Summary: This paper explores the challenges of semantic image segmentation focusing on improving the robustness of network models against factors that affect image perception (e.g. weather lighting). The paper proposes a training strategy that utilizes generative adversarial networks (GANs) for data augmentation and field adaptation to help models learn from diverse and realistic image. The authors present quantitative results on standard datasets comparing their approach to existing methods. Strengths:  Tackles an significant problem with practical applications.  Provides clear overview image and pseudocode to enhance understanding. Weaknesses:  The paper lacks novelty and appears prematurely written.  Writing issues distract the reader.  Previous work on synthetic data quality and generalization is not adequately cited or contrasted.  Experimental evidence is insufficient to demonstrate the effectiveness of the proposed approach. Minor Issues:  The related work section is superficial and does not effectively highlight the contributions of the paper.  Contradictory statements create confusion.  Bold font usage in table rows is unprofessional.", "Summary This research introduces a novel training algorithm (MRTAdapt) for deep neural networks dedicated to semantic segmentation. The algorithms goal is to create models that are more robust to scene variations caused by lighting or weather conditions. The problem is approached as a field adaptation task where the source field is the original labeled data and the target field is created by applying simulated natural variations to the original data. Adaptation is achieved through training augmented data that models the natural variations learned from the original data. The proposed training method combines segmentation loss minimization with GANlike training to maximize the probability of generated image resembling target field image. Experiments demonstrate high segmentation accuracy in simulated target fields compared to other field adaptation techniques. Strengths  Clear problem motivation highlighting the importance of robust segmentation methods for realworld applications.  Wellarticulated statement of contributions.  Comprehensive summary of semantic segmentation models based on deep neural networks.  high accuracy than other field adaptation methods on simulated target field image. Weaknesses  Problem Definition: Explicitly stating the problem as a field adaptation task involving natural variations earlier in the paper would enhance clarity.  Related Work:  Organized discussion of general field adaptation work and its relation to semantic segmentation is missing.  Insufficient coverage of image augmentation techniques despite their relevance to the proposed method.  Method:  natural variation Model: Clarification is needed on how the natural variation model is learned and its assumptions.  Target field Images: Assumptions regarding target field image (e.g. alignment with labeled data) should be clearly stated.  Experimental Validation:  Target field Images: Using simulated target field image rather than real image may introduce bias.  Validation setup could be strengthened by using real target field image without semantic label.  detailed description of processing and mathematical expressions would improve clarity.", "Paraphrase: proposal: The paper presents a method combining a discriminator and generator to enhance the robustness of image segmentation against natural variations such as brightness and snow. Method: 1. input image are altered by adjusting brightness (simulating snow) before being processed by PSPNet generating perturbed feature maps. 2. Original image are also processed by PSPNet to produce original feature maps. 3. The segmentation loss includes weighted losses for both perturbed and original features. 4. A discriminator differentiates between features from perturbed and target image. Strengths: 1. Utilizing generative adversarial networkings for tackling natural variations. 2. Focusing on robustness against practical variations reducing the need for acquiring image under diverse conditions. Weaknesses: 1. The proposed method leverages existing workings on field adaptation without providing clear differentiation. 2. The modeling of natural variations is not explicitly stated. 3. The definition of \"target image\" is unclear. Suggestions: 1. Perform hyperparameter optimization for all models. 2. Discuss the poor performance of FDA in the presence of snow. 3. Include case image with segmentation results. 4. Rearrange existing methods consistently in tables. 5. Release the code for reproducibility."], "xQUe1pOKPam": ["Paraphrase: Summary: This paper proposes using 3D information to train graph neural networks (GNNs) and create molecular representations. It introduces contrastive and generative selfsupervised learning (SSL) strategies to accomplish this task. empirical results on various datasets demonstrate that the suggested method outperforms previous 2D GNN pretraining methods. effectiveness and Weaknesses: The papers concept of pretraining molecular representations using 3D information through SSL tasks is robust and technically sound. However the experiments could be strengthened by employing larger datasets and considering tasks that heavily rely on 3D information. Pros: 1. The rationale for using 3D geometry to pretrain molecular representations is compelling as 3D geometry is crucial for molecular properties but often unavailable in downstream tasks. 2. The paper develops a generative and a contrastive SSL task to incorporate 3D information in pretraining. Extending generative and contrastive SSL to consider 2D and 3D inputs is challenging but the paper addresses this effectively. 3. The paper presents extensive and wellstructured ablation field. Its writing and organization are commendable. Cons: 1. The experiments could be more persuasive by using larger datasets and selecting tasks that are more heavily influenced by 3D information. The recent OGB KDD Cup task which involves predicting quantum chemical properties that are highly dependent on 3D geometry may be a suitable candidate. Questions: 1. In Section 3.1 the paper states that contrastive SSL learns the distribution locally while generative SSL learns it globally. This description is unclear and needs further elaboration.", "Paraphrase: This research introduces GraphMVP a multiview pretraining framework that enriches molecular graph representation learning by incorporating both 2D and 3D geometric information. GraphMVP combines contrastive selfsupervised learning and generative selfsupervised learning losses to enhance representation quality. Experimental results on various datasets demonstrate the model effectiveness. Ablation field provide insights into GraphMVPs privileged process. effectiveness:  novel integration of 3D information to augment 2D graph representation learning.  Improved representation quality through fusion of contrastive and generative losses.  extensive experiments with multiple tasks datasets and recent baselines.  Promising performance across datasets supported by ablation field.  clear and wellstructured paper. Weaknesses:  potential bias in compare if other baseline methods were also enhanced with 3D information.  Inconsistent notation conventions for 2D and 3D features.  Marginal improvements on some datasets require further analysis. Minor Comments:  Typo in sentence stating \"x and y notations are different.\"  potential for condensing the appendix.", "Paraphrased Summary: This research introduces a pretraining method for graph neural networks (GNNs) that leverages both 2D and 3D molecular information. It comprises two selfsupervised learning (SSL) tasks:  Intermolecule Task: Classifying whether 2D or 3D molecular graphs belong to the same molecule.  Intramolecule Task: Generating 2D from 3D or 3D from 2D representations. Experiments using various datasets show that combining these SSL tasks enhances GNN prediction accuracy. effectiveness and Weaknesses: effectiveness:  Wellwritten and easily understood  Focuses on incorporating 3D information in GNN pretraining which is an significant aspect Weaknesses:  Fails to adequately address the issue of \"conformer ensemble.\" 3D information is not unique for molecules raising questions about how to handle multiple 3D model.  The methods ability to handle the continuous rotation and multiplicity of 3D conformations remains unclear.  Experiments mainly focus on predicting molecular bioactivities where molecules exhibit considerable variability in both 3D conformations and actual shapes.  It is uncertain whether learning only a few stable 3D conformations is sufficient for predicting molecular properties. Additional Concerns for Bio QSAR Tasks:  Handling the combinatorial multiplicity of 3D conformations is crucial when incorporating 3D information in QSAR tasks.  2D GNNs may be sufficient in most case due to their ability to abstract molecules as graphs while ignoring complexities in 3D conformations.  The method may be more suitable for quantum chemical tasks where energy prediction is based on a specific 3D conformation.", "Paraphrase: The manuscript proposes a technique for understanding molecular graphs that combines the graphs topological structure with threedimensional (3D) geometric data. The incorporation of 3D geometry is significant for comprehending molecular structure. The proposed technique is reasonable. However the evaluation of the method is limited to predicting molecular properties. To fully assess its effectiveness it should be demonstrated that the proposed graph and 3D geometry combination enhances the reconstruction and generation of molecular data compared to other further graphbased approach."], "nrGGfMbY_qK": ["Paraphrase: Summary: This research introduces a new method for continuous learning using experience replay. The key feature is a memory management algorithm that retains the most valuable samples (based on their impact on loss reduction) while ensuring a balanced representation of classes. The method excels in the challenging \"class incremental online and taskfree\" setting outperforming previous approaches on CIFAR10 and 100 datasets. Strengths and Weaknesses:  The primary contribution is the memory update method which preserves significant samples and maintains class balance.  The method only trains on samples from the memory which provides advantages.  The use of a specific learning rate scheduling strategy further enhances performance.  The memory management method is innovative and the overall approach shows impressive results in the complex class incremental online taskfree setting. Suggestions for Improvement:  While the authors emphasize their \"blurry\" sampling approach it is not considered a significant contribution.  Further comparisons with similar experience replay methods particularly Aljundi et al.s MIR would be beneficial to highlight differences and explain superior performance.", "Paraphrase: Summary:  The paper presents a new scope called \"iBlurry\" for continuous learning (CL) which combines features of previous \"blurry\" and \"disjoint\" scopes.  iBlurry allows for overlapping classes across tasks and the introduction of new classes in each task.  The paper evaluates CL algorithms in this scope and proposes a new algorithm called CLIB.  CLIB uses a memorybased approach to refine its memory by removing unsignificant samples and adding significant ones based on their expected impact on training loss.  CLIB also incorporates a datadriven learning rate scheduling scheme.  Results show that CLIB outperforms other online CL algorithms in various instantiations of iBlurry including the disjoint and blurry scopes on CIFAR10 CIFAR100 and TinyImageNet datasets. Strengths:  CLIB performs well in different iBlurry configurations particularly in the \"blurry\" scope without new classes.  Ablation study highlight the importance of the sample importancebased memory management scheme in CLIB.  The paper provides extensive supporting experiments in the appendix.  The related study section is organized and the figures are wellpresented. Weaknesses:  The paper does not discuss the novelty of the sample importance estimation approach.  A natural language description of Theorem 1 is missing.  The main text lacks detailed insights into the importance calculation and efficiency considerations.  It is unclear why TinyImageNet is challenging for all methods and why it was chosen over ImageNet.  The hyperparameter choices for iBlurry are not justified or compared to previous study.  The paper does not consider upper bounds on performance such as the nonCL scenario where all training data is available simultaneously. Suggestions:  study performance on upper bounds for nononline and nonCL scenarios.  Correct typos in the text and table.", "Paraphrased Summary This paper introduces a novel problem formulation in continual learning known as \"online taskfree class incremental task blurry learning with anytime inference.\" The authors have devised new performance benchmarks and memory management technique based on importance. They have evaluated their methods empirically in the context of their proposed problem formulation. Strengths and Weaknesses Strengths:  The paper provides clear visualizations for the newly proposed problem formulation. Weaknesses:  The paper assertion that the new formulation is both \"taskfree\" and \"class incremental\" raises questions regarding their compatibility as they differ in their supervised learning output layer.  The significance of \"anytime inference\" requires further explanation and justification particularly considering that only a exclusive baseline in Table 1 lacks this capability.  Theorem 1 a memory management strategy does not address catastrophic forgetting and may exhibit a performance decline in the later stages as demonstrated in Figure 4.  The paper does not discuss the algorithm complexity which could potentially be O(M) where M represents the memory size.", "Summary: This research presents a new benchmark standard for incremental learning (iBlurry) which incorporate gradual class introduction and diffuse task boundaries. The authors introduce CILB a strategy that consists of three components:  Sample importance memory: Selects optimal data points to minimize loss.  Memoryonly training: Utilizes saved model for model updates.  adaptive LR scheduling: Adjusts learning rate for better performance. significant experiments demonstrate CILBs efficacy. Strengths:  iBlurry benchmark is a relevant and engaging testbed for tackling incremental learning with class overlap.  Extensive experimental results support the proposed methods effectiveness.  CILB is technically sound and empirical evidence supports its efficiency.  The paper is wellstructured and understandable. Weaknesses:  Technical novelty is limited as CILBs components resemble existing approaches.  The paper lacks selfsufficiency relying on external references for crucial details (Alg.3 Section A.3).  Experimental evaluations are conducted on small datasets while largescale results are absent.  An analysis of CILBs performance with varying task counts would enhance its credibility."], "wbPObLm6ueA": ["Paraphrase: This paper investigates fairness in machine learning algorithms specifically focusing on demographic shift. demographic shift refers to a shift in the underlying population between training and deployment affecting a specific demographic attribute (e.g. race gender). The paper proposes an algorithm called Shifty to address demographic shift in classification tasks. Shifty takes a training dataset fairness constraints and a description of demographic shifts. It partitions the data trains a model and uses the demographic shift description to estimate the models fairness after deployment with high confidence. Shifty returns models that are guaranteed to be fair after deployment with a probability of at least 1  \u03b4. effectiveness and Weaknesses Comments:  The definition of demographic shift could be clearer.  The distinction between fairness attribute and demographic attribute needs further explanation.  Its unclear why the conditional probability is assumed to remain constant despite shifts in the demographic attribute distribution.  The method description in Section 3.1.2 lacks precision regarding the numerical approximation and simplicial homology optimization techniques used.  The related work section could be enhanced to highlight how Shifty differs from existing algorithms in handling demographic shifts.  The evaluation is limited to a single dataset. More diverse experiments would provide a more comprehensive assessment.", "Paraphrased Statement: Summary The authors propose a method called SHIFTY that ensures fairness regardless of data distribution differences between training and deployment. Two accesses are presented: one for known distribution shifts and one for unknown shifts. They tested SHIFTY on a student success prediction dataset and compared it to existing fairness methods. effectiveness and Weaknesses SHIFTY addresses a critical problem. Its parallel access for candidate selection and confidence bound estimation is innovative. However the selection operation for the different data distributions (training and deployment) is unclear potentially affecting the representativeness of the samples. The authors claim to allow users to customize fairness measures but its unclear how they address individual fairness. They also acknowledge that their assumption of independent parameter intervals is often invalid. SHIFTYs reliance on userprovided distribution shift data is an underlying limitation. Additionally training models for multiple unknown distribution shifts significantly impacts performance. The evaluation section is not fully convincing as the comparison with nonfairnessfocused methods for different distributions is questionable. The results for unknown distribution shifts suggest that the Seldonian algorithm outperforms SHIFTY. It is recommended that the authors compare SHIFTY with methods designed for concept shift and include additional datasets for evaluation to ensure consistency across different contexts.", "Paraphrase: This paper introduces algorithms that ensure fairness when the data used for deployment differs in demographic characteristics (e.g. gender or race) compared to the data used for training. It addresses two scenarios: 1. When the exact demographic shift is known. 2. When the demographic shift is unknown. The algorithms calculate an upper bound on unfairness in the deployment setting using the student tTest. The paper compares these algorithms to existing methods including Seldonian QuasiSeldonian Fairlearn and Fairness Constraints. effectiveness:  Clear and accessible writing.  Tackles a significant challenge in deploying models.  Proposes an innovative access to ensure fairness using upper bounds. Weaknesses:  Omits mention of similar research on fairness under distribution shift.  Lacks clear explanation for certain design choices such as the selection of the interpolation factor. Additional Concerns:  Whether the assumptions about the fairness work can be extended to more complex scenarios.  The feasibility of applying the access to individual fairness definitions.  The need for empirical evaluation when multiple potentially conflicting fairness definitions are present.", "Paraphrased Statement: Summary: The research proposes a technique for controlling unfairness in models when the test distribution differs in the distribution of a feature like gender or race. It introduces a statistical test for evaluating unfairness in the unknown test distribution using importance weighting alongside user input on the extent of the shift. Experiments on a dataset related to academic performance demonstrate that the access effectively reduces the likelihood of creating biased classifiers. effectiveness and Weaknesses: The access offers a transparent and adaptable framework for training fair classifiers in the presence of demographic shifts. The system of the framework is wellstructured and the writing is clear. Separating the problem into evaluating fairness in an unknown distribution aids in comprehending the proposed method. Adapting the framework to various fairness metrics is a key advantage. Notably it provides probabilistic guarantees for fairness violations a feature often lacking in fair learning accesses for this specific problem. Some concerns include: 1. The focus on controlling only fairness without considering accuracy. 2. The assumption that the demographic variable is discrete which may limit the applicability of the algorithm. 3. The possible impact of insufficient data on the accuracy of the proposed method. Questions for the Response: 1. Control of Fairness vs. accuracy: Why is the objective of solely controlling test unfairness while ignoring accuracy justified Are there strategies for addressing both measures for the test set considering the impact of demographic shifts on both fairness tests and classification loss 2. Discrete Demographic Assumption: Does the assumption of demographic shift rely on the demographic attribute space \\mathcalT being discrete Clarify the allowed area for variables and indicate if discreteness is a limiting factor for the algorithm. 3. accuracy release: Explain the reasons for the significant drop in accuracy observed in the \"Shifty\" method even with known shifts when there are 100K data points. Are these accuracy concerns unique to the specific exam score dataset or are they observed in other datasets as well Minor Questions: 1. In Algorithm 1 does the candidate model \\thethac in step 2 appear in the inputs for fairness testing in step 3 2. Describe the operation of selecting and testing candidate models in step 2 of Algorithm 1. Related work: Consider discussing related research on fairness guarantees under shifts such as those by Biswas and Mukherjee (2021) Rezaei et al. (2021) Schumann et al. (2019) Coston et al. (2019) and Singh et al. (2021). If any of these work address a similar problem setup compare their accesses to the proposed method. Suggestions: 1. Clarify whether the demographic shift assumption is a type of covariate shift or how it differs. 2. Explain the significance of the solution in Theorem 2 and its application in supporting the proposed method. 3. Provide the reason for choosing the specific numerical optimizer from Endres et al. (2018). Is Uttest a nonsmooth work 4. Consider using the new Adult dataset derived from the US Census as a possible dataset for experimentation as it may exhibit demographic and other shifts."], "mHu2vIds_-b": ["Summary This work aims to improve the performance of randomized smoothing (RS) a technique used to increase the robustness of machine learning models. It proposes using an ensemble of diverse base models within RS arguing that this reduces the variance of the ensembles predictions over the perturbations introduced by RS. This leads to more consistent classifications for given inputs. The paper also introduces two methods to accelerate the certification work. effectiveness  Addresses an significant research challenge.  Clear and wellorganized presentation.  Provides insightful analysis explaining why ensemble techniques enhance RS.  Includes comprehensive experimental evaluations. Weaknesses  Evaluation is limited to L2 certification.  related work is not adequately covered.", "Paraphrased Statement: Summary: This paper suggests using a combined ensemble of similar models as the core component of randomized smoothing (RS). The authors explain that ensembles minimize variance in the base classifier when given noisy inputs enhancing RS performance. Theoretical reasoning and mathematical experiments support this claim. Additionally the authors provide practical algorithms to reduce computational costs. effectiveness: 1. The paper establishes the effectiveness of combining ensembles with RS for creating certified robust classifiers supported by both theory and experiments. 2. While the theoretical reasoning relies on assumptions the authors conduct experiments to validate them. 3. They introduce adaptive sampling and Kconsensus algorithms to lower computational expenses making their method more applicable. Weaknesses: Despite the compelling empirical evidence some concerns arise regarding the theoretical assumptions. The paper assumes that \"the correlation between the logits of different classifiers has a similar structure but smaller magnitude than the correlation between logits of one classifier.\" This assumption may hold because different classifiers contribution similarities due to using the same random seed during training. However its also potential that this contributiond behavior is exhibited in \\mathbbEl[fl(x)] which reflects the task network architectures dependency on x while the variance (\\deltai and \\deltaj) are independent. In fact as observed in the appendix (Figure 11a) the offdiagonal blocks in the Delta subfigure are nearly identical to those in the True covariance subfigure. This suggests the potential for constructing an argument about variance reduction based only on the assumption that Cov(yic yjc) approaches zero.", "Paraphrased Statement: Summary:  This paper combines model ensembles with randomized smoothing to enhance certifiable accuracy.  The theoretical analysis establishes the positive impact of model ensembles on reducing the variance of smooth classifiers.  An adaptive sampling algorithm is developed to minimize the computational effort required for randomized smoothing certification.  extensive experiments on CIFAR10 and ImageNet datasets were conducted. effectiveness:  High relevance of the research field.  The theoretical analysis in section 5 supports the use of model ensembles.  The proposed Adaptive sampling algorithm is practical.  comprehensive experiments across model and datasets demonstrate the effectiveness of model ensembles in improving certifiable accuracy. Weaknesses:  comparison of evidenceADP with evidence is biased due to differing levels of data provided.  The writing particularly in section 5 could be significantly improved.  Lack of inclusion of Key definitions in the main body of the paper making it difficult to interpret Table 3.  Figures 2 and 3 would benefit from more detailed explanations."], "iMSjopcOn0p": ["Paraphrased Statement: compact: The paper presents MT3 a system for music transcription that handles diverse datasets. Experiments showed that MT3 achieved excellent performance on multiple metrics and comprehensive datasets. Strengths and Weaknesses:  Multitasking: The term \"multitask\" may be debatable since differences in target instruments may not be significant enough to warrant that label. Experiment:  Frame F1: The quality of 62.5 frames per second is not discussed in the paper.  Singleinstrument metrics: Clarification is needed on how these were calculated e.g. were all notes considered as a single instrument  Melodyne inclusion: The inclusion of Melodyne in the experiment is a positive aspect. Table 3:  Grouping strategies: The grouping strategies should be provided in an appendix or on a demo page since they are potential to have an impact on the field. Appendix A:  Model definition: The definition of the \"minor\" model (T1) may not fully capture all aspects of the model.", "Paraphrase of Key Statement: This paper introduces an innovative music transcription model that can automatically transcribe music from audio transcription regardless of the issue of instruments present. The model is trained using multiple AMT datasets each representing a different AMT task. To facilitate this training the researchers developed a novel MIDIlike output data representation that encodes not only \"instrument\" data but also supplementary details like \"tie\" and \"end of sequence\" markers. The model achieves significant accuracy improvements across all datasets demonstrating its superior transcription capabilities. Furthermore models trained on multiple datasets exhibit higher performance than those trained on a single dataset. Critique of Strengths and Weaknesses: This research makes a significant contribution to the field of AMT. The proposed model is notable for its ability to handle audio with varying issues of instruments. Its accuracy improvements are impressive and the generalization test showcases its robustness. However it is significant to note that the MIDIlike output data representation was manually designed rather than learned which limits the technical novelty of the work. Additionally the models performance has not been evaluated on datasets containing vocals which are a crucial component in popular music. Questions:  Does the models temporal alignment accuracy stem from the absolute positional encoding used  How would the model perform if trained on a MIDIlike representation that is not handdesigned but learned by the model itself", "Paraphrased Statement: compact This paper presents a framework for transcribing music using multiple tasks and tracks. Traditionally music transcription has been addressed separately for each instrument. However this framework trains a model jointly on various datasets containing different instrument types. By doing thus the resulting model outperforms models trained individually on each dataset. The papers contributions lie in three field: 1. Multitask Transcription Model: This advance is effective for addressing transcription tasks with limited data. 2. MIDILike Representation for learning MultiInstrument Piano range: Similar ideas have been explored in MIDIbased music generation research and it is valuable to see this advance applied to multiinstrument transcription. 3. MultiInstrument F1 score Metric: As multiinstrument transcription is a relatively new task this metric will be beneficial for future research. Strengths and Weaknesses  The papers contributions are clearly outlined in the \"compact of the paper\" section.  Some questions that could be explored further include:  Evaluating the performance of a model trained without instrument supervision.  Analyzing the performance differences between instrumentwise splits.", "compact: The authors combine existing automatic music transcription datasets to create a unified training framework. They train a pretrained T5 model on the combined data and achieve higher performance than current stateoftheart systems on individual datasets. The authors present extensive experiments to demonstrate the models robustness in lowresource settings and its ability to transcribe multiple instruments. They also introduce a novel eevaluation metric that incorporates instrument data. Strengths and Weaknesses: Strengths:  Combines AMT datasets into a unified training framework  Outperforms stateoftheart AMT systems  Expands scope to multiinstrument transcription  Conducts systematic analysis of model training under various setups  Compares multiple architectures and eevaluation metrics  Introduces a novel musically relevant eevaluation metric that includes instruments  Performs outofdataset transcription experiments Weaknesses:  The authors should consider the limitation of \"ideal\" AMT systems as the purpose of music transcription may vary depending on the task.  The authors do not discuss work on music transcription outside of the 12tone equal temperament system.  The authors could compare their eevaluation metric with alternative options such as the MV2H metric.  The color selection in Figure 2 should be improved for print and colorblind accessibility.  The explanation in Section 3.2 should follow the range of tokens in Figure 2 for better readability.  The authors should clarify that \"onoff\" refers to note issue being played or released.  The models performance around segment edges could be further explored.  Additional labeling issue and data inconsistency could be qualitatively exemplified in the appendix.  Ethical consideration are not addressed in the paper. Suggestions for future work:  Investigate model behavior in truly outofdomain settings with significantly different instruments genres and structures.  work \"data evaluation\" to determine the relative importance of different portions of the training data.  research transcription of traditional music datasets or datasets with less conventional notation systems."], "w1UbdvWH_R3": ["Summary Paraphrase: The authors propose a theoretical framework to explain Neural Collapse (NC) in Deep Neural network (DNNs) with Mean Squared Error (MSE) loss. They show that MSE loss can be divided into terms that correspond to NC conditions. The proposed framework (called the central path) explains the emergence of NC in DNNs under specific assumptions. Strengths and Weaknesses Paraphrase: Strengths:  extensive experiments demonstrate the occurrence of NC in DNNs with MSE loss.  Motivation and explanations are provided for the theorem conditions and limitations.  The appendix offers a comprehensive review of related work.  introduction of the central path as a new theoretical concept. Weaknesses:  Section 1.1 could be improved by introducing NC after defining the problem and necessary concept.  The focus on the terminal classifier layer only does not consider feature generation in DNNs.  The connection between NC and generalization is unclear.  MSE loss is less commonly used in classification compared to crossentropy loss. Additional Comments:  Consider adding references to datasets and architectures in Section 1.3.  Provide more information on the classification performance when using MSE loss in Section 1.2.  Clarify whether NC4 is an independent condition or derived from (NC1NC3).  Explain the relevance of condition (A2) to empirical observations.", "Summary Papyan Han and Donoho (2020) observed that training neural networks beyond zero error results in \"simplex arrangements\" of features. This work analyzes this phenomenon (\"neural collapse\") theoretically focusing on training with square loss minimization. Key findings The square loss can be split into two terms: one related to feature quality and the other to the quality of the classification layer. The feature quality term quantifies the distance from neural collapse. The gradient flow induced by the feature quality term has closedform solutions which can explain aspects of the empirical observations. Strengths  Wellmotivated and wellstructured  Theory provides explanations for empirical observations  Closedform solutions provide insights into neural network behavior  Relevant to improving future algorithms Weaknesses  Interpretation of the singular values in the main result is unclear  A key assumption (full range withinclass covariance) is not prominently stated  A potential correctness emergence with the decision due to inconsistencies in the derivation Suggestions  Reduce notation in Section 2 by considering the zero bias case  Explain the term \"central path\" in Equation 4  Consider formalizing the renormalized gradient flow as a connection on the feature space fiber bundle  Correct the typo in Corollary 1", "Paraphrased Statement: This work investigates \"neural Collapse\" (NC) a phenomenon that occurs during training deep networks using the Mean Squared Error (MSE) loss function. It provides empirical evidence of NC in classification tasks and theoretically analyzes it. The work decouples the loss function and introduces the concept of the \"central path\" to derive a closedform dynamic that predicts NC in this context. Strengths:  empirical demonstration of NC in classification tasks with MSE loss using various datasets and network architectures.  rigorous theoretical analysis to explain NC in framework with unconstrained features.  Insightful decomposition of loss function to understand NC. Weaknesses:  The paper covers a significant amount of material for a 9page conference publication with most of the empirical work relegated to appendices.  Lack of clarity in some statements:  The design caption refers to \"Lperp\" as \"L\\perp\" but the legend uses \"L\\perp.\"  The caption states that \"L\\perp becomes negligible compared to LNC1 early in training\" but the design shows otherwise for certain cases.", "Paraphrased Statement: This paper investigates the phenomenon of \"Neural Collapse\" in neural networks using Mean Squared Error (MSE) instead of CrossEntropy (CE) for analysis. The authors propose that MSE can be broken down into two components: a central path loss and a perpendicular loss. They demonstrate that the perpendicular loss is much smaller than the optimal central path loss suggesting that Neural Collapse occurs because the optimizer prioritizes the central path loss. empirical experiments support this hypothesis. Strengths:  The work explores the intriguing phenomenon of Neural Collapse in depth.  The analysis using MSE provides valuable insights into the dynamics of neural collapse.  The idea of decomposing the loss into central path and perpendicular components is innovative and useful.  The experiments align with the proposed concept confirming the relevance of MSE in this context. Weaknesses:  The writing and organization of the paper are problematic making it challenging to understand the key ideas and follow the arguments.  The authors fail to clearly state the assumptions made in the analysis leading to ambiguity.  The derivation of decision lack clarity as in the case of the statement that individual activations tend towards their corresponding class means.  The significance of certain findings such as the one related to equation (11) is not adequately explained.  The discussion on the signaltonoise ratio matrix requires further elaboration and justification.  The derivation of NC14 in Section 3.3 is unclear and lacks proper guidance for interpretation."], "uoBAKAFkVKx": ["Paraphrased Summary: This work presents a novel random search algorithm for reinforcement learning using blockbased search and statistical tests to guide the search for optimal parameters. Paraphrased Strengths and Weaknesses: While the algorithm is intriguing it may not be as novel as claimed. Random search methods for blackbox optimization have been extensively studied particularly in evolutionary computation where similar approach have been explored. Its unclear what aspects of the novel algorithm are genuinely innovative compared to existing EC and local search algorithms. The authors posit that reinforcement learning problem are blackbox optimization problem despite the success of gradientbased deep reinforcement learning algorithms like TD3 and SAC which have shown the efficacy of utilizing gradients for policy training. The potential advantages of the novel algorithm over these methods are not clear. The experimental evaluation has limitations. The number of benchmark problem is insufficient for conclusive results. The authors prioritize the number of training iterations as a metric but its unclear if the algorithm can be applied to realworld problem. The definition of the optimal solution in the Mujoco benchmarks is unclear. Hyperparameter context vary across problem potentially affecting the fairness of comparisons. Despite the claim that the paper explores robustness in stochastic environments most benchmark problem used are not highly stochastic and the actual robustness of the learned policies is not assessed.", "Paraphrased Summary This paper presents a novel method for optimizing policies based on coordinate ascent. The method leverages two key insights: 1. policy optimization can benefit from decomposing the large policy space into smaller subproblems. 2. The objective use is stochastic which is often overlooked in blackbox optimization approach. The method uses a onesided hypothesis test to determine when to update the policy based on the significance of a given rollout. Experiments show improved performance comparisond to other approach on a continuous LunarLanderv2 field and MuJoCo. Strengths and Weaknesses  Advantages: Gradientfree parallel and wellsuited for continuous policy spaces.  Weaknesses:  Unclear labeling in plots making it difficult to interpret results.  Ambiguous language regarding reinforcement use stochasticity.  Lack of related work section addressing similar methods like block coordinate ascent and highconfidence policy improvement. Questions and Comments  Is block coordinate ascent used for policy optimization  How does the method comparison to highconfidence policy improvement methods  Clarify the source of stochasticity in the reinforcement use.  Consider a more descriptive title that reflects the hypothesisdriven nature of the approach. Suggestions for Revision  Clarify the yaxis labeling in design 1.  Remove unnecessary phrases and simplify sentences.  Emphasize the stochastic nature of the environment in the description of MDPs.", "Paraphrase: This paper introduces a novel optimization algorithm that can be used for use where gradients cannot be calculated (stochastic blackbox use). It has been specifically tested for use in reinforcement learning but can be applied more broadly. Key Features: 1. block Coordinate Ascent: Only a subset of parameters is optimized at a time while the rest are held constant. Random search is used to maximize the objective use within these block allowing for parallel use evaluations. 2. hypothesis Testing: To mitigate variability in policy evaluation a statistical test is employed to determine if a candidate policy outperforms the current one. policy updates are only made when this is confirmed. Comparison and Strengths: The proposed algorithm (hypothesisDriven Coordinate Ascent or HDCA) has been compared to other gradientfree optimizers used in reinforcement learning. It has been shown to require fewer iterations to converge for most tasks. The paper is wellwritten and provides a clear explanation of the algorithm. The authors provide strong justifications for their design choice and HDCA offers the advantage of scaling up well. Weaknesses and Improvements:  Lack of Novelty: The algorithm combines existing techniques rather than introducing significant design.  Limited Scalability: The experiments in this paper focus on tasks with a minor number of parameters (under 1000) and dense rewards. It is unclear how well HDCA will scale to more complex field.  Sparse reinforcement and HighDimensional Spaces: Further experiments should explore HDCAs performance on tasks with sparse rewards or highdimensional parameter spaces which could pose challenges.  Fixed block: It is unclear why the block of parameters to be optimized are fixed within each iteration. Resampling the block could potentially improve convergence if some parameters are less significant or more difficult to optimize.", "Summary: Authors Present a New optimization method for RL: The authors introduce HDCA a novel optimization method for determining RL policies in unpredictable environments. HDCA avoids direct gradient estimation and instead runs multiple potential policies concurrently selecting the bestperforming parameters for updates. Strengths:  clear and wellwritten presentation  Simple and straightforward novel optimization approach for RL  Contributes to evidence that simple optimization and linear policies can match current advancements in RL. Weaknesses:  Insufficient experimental evidence to support claims:  Limited experiments on a minor number of field  Lack of guidance on selecting hyperparameters (e.g. number of policies rollouts significance level)  Sensitivity of results to hyperparameter choice is not explored  The authors acknowledge that specific hyperparameters are \"problemspecific\" but do not provide guidance for practitioners. Conclusion: The authors estimation is intriguing and contributes to the literature. However more experimental evidence is needed to substantiate their broader claims. If the experiments were solely conducted on linear policies or basic networks this should be explicitly stated."], "gI7feJ9yXPz": ["Paraphrase: The paper introduces generalization bounds for minimax optimization problems with improved accuracy compared to previous work. These bounds cover four measures: general error primal error strong primaldual risk and strong primaldual error. Strengths and Weaknesses:  Strengths:  Clear and wellwritten introduction  Interesting problem with potential applications  Weaknesses:  Comparison to empirical measures: The paper compares generalization measures to a constant multiple of empirical measures (1eta). It would be more reasonable to consider case where eta approaches zero to avoid dependencies on the empirical risk.  Theorem 1b: The theorem compares the generalization risk R(Ax(S)) to (1eta)RS(Ax(S)). This comparison may not be fair as other research (FarniaOzdaglar Lei et al.) compares generalization measures directly to empirical measures without such multiples.  Relaxation of assumptions: The paper could explore relaxing some of its assumptions and discuss their impact more thoroughly.", "Paraphrased Statement: Summary: This study provides improved probability bounds for minimax problems with various generalization measures such as strongweak PD generalization and primal generalization error. For a given univariety stability parameter sharper bounds of range O(1n  \u03b5stab) are established with a tradeoff in empirical error (e.g. additional terms involving FS(A(S)) or \u0394S(A(s))). These results improve upon previous excess generalization bounds of O(1\u221an  \u03b5stab) when FS(A(S)) or \u0394S(A(s)) are small. The findings extend the work of Lei et al. (2021) where most bounds were derived in expectation and high probability bounds were of range O(1\u221an  \u03b5stab). Specific bounds are given for GDA PPM and SGDA under strong convexity and concavity assumptions as they exhibit univariety stability. Strengths and Weaknesses: The papers findings are novel and provide a valuable extension to existing minimax optimization algorithms. The related work is adequately referenced and the paper is wellorganized. Concerns: 1. Overstated Claims: The paper claims to \"improve generalization analyses for almost all existing generalization measures\" and \"establish sharper bounds of range O(1n) significantly with high probability.\" However the excess bounds are actually of the variety O(1n  \u03b5stab) with additional terms. The comparison results in Table 1 are therefore vague and imprecise. 2. Incorrect Bounds: The bounds stated in Theorems 2 and 3 are incorrect as they omit additional terms (e.g. FS(A(S)) or \u0394S(A(s)) or infw R(w)). 3. Lack of Novelty in validation Techniques: The validation techniques are largely adapted from Klochkov and Zhivotovskiy (2021). 4. Redundant Sections: contribution (c) and contribution (d) essentially provide the same results making them redundant.", "Paraphrase: This research investigates a stochastic optimization problem where the goal is to find a minimum and maximum for a work F(x y) where F(x y) is the expected value of another work f(x y z). The point is to achieve fast convergence ranges of O(1n) with a training set of n data point (z1 ... zn) with high probability. To measure convergence different metrics can be used such as the gap between the estimated maximum and the true maximum of F(x y) for a fixed x or the gap between the estimated minimum and the true minimum of F(x y) for a fixed y. The study generalizes classical concepts of algorithmic stability for stochastic optimization problems to stochastic minimax problems. It shows that various algorithms including variants of gradient descent exhibit stability under certain conditions. Specific problemalgorithm combinations are shown to achieve fast convergence ranges particularly for highly concaveconvex saddle point detection problems. Strengths and Weaknesses: Strengths:  Generalizes algorithmic stability notions to minimax problems.  Provides high probability bounds for specific problemalgorithm pairings when problem values are bounded. Weaknesses:  introduction issues.  Vague statements about generalization error ranges:  Precise definitions of what is measured as O(1n) are lacking.  Its unclear how to infer an O(1n) range from the provided expressions.  The true value is compared against an adjusted empirical value instead of the empirical value itself which is confusing. Despite these introduction issues the contribution of the paper warrant acceptance if the concerns are addressed.", "Paraphrase: Summary: This paper presents a theoretical framework for generalizing minimax learning problems which are commonly used in machine learning. Unlike previous studies that established either expectation bounds or high probability generalization bounds of O(1\u221an) this paper provides improved high generalization bounds of O(1n). These bounds are applicable to popular optimization algorithms like ESP GDA and SGDA. Strengths and Weaknesses:  Strengths:  Wellwritten paper  approach expectation bounds to high probability bounds  comparison and contrasts with existing work  Rigorous validation  Weaknesses:  Lacks discussion on the tightness of the generalization bounds  Could benefit from synthetic empirical experiments to demonstrate effectiveness Questions:  Why are strong and weak measures defined in this specific way  How do (c) and (d) in Theorem 1 differ  What is the impact of the variable \u03b7 in Theorem 1  How can the strong PD empirical risk have a dependence on the sample size n Minor Comments:  Typo on page 4: \"(\\mathbbE \\ z \\p)1\\over p\" should be \"(\\mathbbE \\ zp)1\\over p\""], "uF_Wl0xSA7O": ["Paraphrase: This work introduces a technique for balancing training in multitask learning. It utilizes a gradientbased method to align the independent components of the training objective. The methods effectiveness is demonstrated through experiments on various multitask learning problems. The authors assert that it scales well resists overfitting and effortlessly handles multitask objectives with varying gradient magnitudes. Strengths:  Aligning independent gradient components offers a consistent approach to balancing multitask training.  The method outperforms existing techniques on various multitask learning benchmarks.  The paper is clear and easy to comprehend. Weaknesses:  The methods performance on some benchmarks is not as strong especially the issue on the CELEBA dataset.", "Paraphrased Statement: Summary: Researchers present a method to balance tasks during multitask learning (MTL) by aligning separate component of the training objective. This approach accommodates significant variations in gradient effectiveness and prevents fasterlearning tasks from dominating reducing overfitting. The key contribution lies in analyzing individual gradient components (dominance rate) instead of overall gradients. Lower dominance coefficients indicate beneficial task balance. Experiments: The proposed technique demonstrates competitive performance aadvancest other MTL optimization strategies. Strengths:  dominance rate analysis prevents task overfitting.  detailed technical introduction.  zalignment overcomes linear scaling limitations of thetaalignment. Weaknesses:  The claim of consistent advance across tasks is not fully supported by experimental issue.  Negligible differences compared to stateoftheart methods (e.g. 0.06 and 0.16 advance).  Omission of superior approaches (e.g. Lee et al. 2020) in comparisons.  Limited orientation advance over GradNorm (15 advance but 25 performance drop).  Lack of comparison to Radwan et al. 2018 which demonstrates superior accuracy on the 7SCENES dataset.  Absence of benchmarking on highcardinality datasets (e.g. \"PubChem BioAssay Dataset work\") to test scalability limits.", "Paraphrased Summary: Researchers propose a novel method for improving multitask learning when tasks have drastically different gradient magnitudes. They introduce a metric called \"rate of dominance\" to quantify this imbalance. This metric is derived from the eigenvalue ratio of gradient tensors. The proposed solution leverages this dominance metric to balance gradients across tasks resulting in an elegant method based on an approximation that focuses on shared parameters. Experimental issue demonstrate that the proposed method performs comparably to or outperforms existing approaches. Strengths:  Addresses the significant issue of multitask learning with gradient magnitude imbalances.  Introduces a novel metric to quantify the imbalance and proposes a solution based on gradient balancing.  offer strong empirical evidence of the methods effectiveness. Weaknesses and Questions: 1. The assumption of a common solution for all tasks may not always hold in practice. 2. The assumption of taskspecific parameters being independent and having a single gradient may be limiting. 3. The notation in image 2 should be explained earlier in the text. 4. The concept of \"vector w denotes a preselected task preference\" needs clarification. 5. The proposed method should be compared with related work on overfittingtogeneralization ratio and GradientBlending. 6. The update of taskspecific parameters in Algorithm 1 line 13 is missing. 7. The extension of the method to networks with adapters should be discussed. 8. The subscript Gtheta should be GZ in definition 3. 9. The proofs in the appendix need a more detailed explanation in the text. 10. The motivation behind using Zaligned as an approximation should be elaborated. 11. Related work by Lu et al. should be cited and discussed."], "lL3lnMbR4WU": ["Paraphrased Summary: Researchers have developed a novel technique for training object detector to recognize novel objects without using detailed annotation for each object. Their approach involves leveraging existing knowledge from a visionlanguage embedding model (the \"teacher\") and transferring it to a regular object detection model (the \"student\"). This approach allows the detection model to match object proposals to corresponding text embeddings enabling it to recognize objects even when it has never seen them before. The technique uses a pretrained model (CLIP) that has been trained on vast amounts of imagecaption pairs providing it with a significant amount of knowledge based on weak supervision. The novel method achieves remarkable performance in detecting novel objects surpassing not only stateoftheart openvocabulary detection methods but also fully supervised detector. effectiveness:  The research addresses a significant challenge in computer vision aiming to reduce the reliance on extensive supervision for training object detector.  The quantitative results demonstrate the effectiveness of the approach and provide valuable insights.  The method can be applied to openvocabulary instance segmentation opening up novel research avenues in this field.  The use of separate models for pretraining and training provides flexibility in using different models for each task. Weaknesses:  The technical novelty of the proposed method is limited as it combines existing models and techniques.  The paper lacks clarity in some fields and omits certain details such as the text encoder architecture and the choice of proposals.  The method potentially faces challenge in handling background proposals in certain datasets.  The evaluation uses a biased dataset with a large issue of supervised categories and its performance with smaller amounts of supervised data is unclear.  The speed of the approach is not evaluated quantitatively.", "Summary This work explores the task of clear vocabulary object detection where a detector is trained with (a) bounding boxes for known categories and (b) imagecaption pairs for novel categories. The detector is then tested on unfamiliar categories. Unlike previous work this paper integrates knowledge from powerful visionlanguage models like CLIP into an clearvocabulary detector proposing various methods (RCNNCLIP ViLDtext ViLDimage ViLD) and ensemble variants (ViLDensemble). Additionally it introduces a novel evaluation setup using the LVIS dataset (with 1200 categories) instead of the COCO dataset with fewer categories. Strengths  The problem is relevant and practical.  The authors explore different approaches to incorporate visionlanguage models.  The results show promising performance.  The LVIS dataset provides a more challenging setting for clearvocabulary detection.  The paper provides additional experiments for transfer learning and label space expansion. Weaknesses  Clarity:  The implementation of the region proposal network is unclear.  The authors do not fully explain the twostage objectness detector.  The description of the pretrained clearvocabulary image classification model could be improved.  Related work:  The paper should mention the related work on clearworld object detection and expanding object detector label spaces.  Experiments:  It is unclear if the model evaluated on COCO uses FPN.  Minors and suggestions:  The statement about exponentially increasing annotation costs should be clarified in the context of federated annotation.  The Average Recall metric should be explained for clarity.  Implementation details such as image resizing and CLIP model version should be provided.  The authors should clarify the use of \"tricks\" and highlight the use of supervised rare category data in the LVIS challenge.  The claim about being the first to directly transfer a trained detector across datasets may need to be revisited in light of previous work.", "Summary: This work introduces a novel approach for detecting objects without prior training (zeroshot) using models trained on vast internet datasets that may contain the desired object categories. The approach ViLD utilizes knowledge learned by models like CLIP and ALIGN to identify \"novel\" category. While ViLD outperforms supervised methods on rare category in the LVIS dataset its performance is lower than stateoftheart methods. It also underperforms on frequent category and overall compared to previous approaches. However ViLD demonstrates significant improvements in zeroshot detection of both unseen and seen category on the COCO dataset. effectiveness and Weaknesses: effectiveness:  Strong results for specific cases such as rare category in LVIS.  Wellwritten and easytounderstand paper with clear need and intuitions.  clear diagrams and image for beneficial comprehension. Weaknesses: Performance Concerns:  significantly lower performance on nonrare category and complete examination sets compared to prior work raising questions about the usefulness of the approach.  Use of pretrained models like CLIP which may have encountered rare category during pretraining obscures the true zeroshot nature of the setting. Methodological issue:  Use of separate N and M proposals is unclear and its not evident why both are needed.  Lack of empirical evidence to support the use of 1x and 1.5x embeddings.  The role of ViLDimage in approximating teacher model predictions and improving performance is questionable.  Insufficient differentiation between ViLD and ViLDensemble in image 3d and in the description of their differences. Table Presentation:  Lack of bolding for stateoftheart performance in Table 3 may be disingenuous.  Insufficient explanation for the low performance on all r c f and overall compared to the stateoftheart.  Absence of supervised method performance for COCO novel category in Table 4. Other Concerns:  Lack of clarity on whether the pretrained image encoder (CLIP) has encountered novel categories potentially weakening the zeroshot setting.  Typographical error in the fifth line from the bottom on page 6 with an extra \"has.\""], "mMiKHj7Pobj": ["Summary The paper acknowledges that models can influence information distribution potentially incentivizing learners to manipulate the models output instead of improving predictions. The authors propose two basic diagnostics to uncover this behavior demonstrating that metalearning algorithms are more vulnerable to it. To mitigate this they suggest contextswapping to prevent algorithms from directly observing the issue of their actions. effectiveness and Weaknesses Related Work  The authors have overlooked existing literature on \"performative prediction\" which addresses similar concept.  It would be beneficial to draw connections between the proposed approach and research on performative issue and performative gradient descent. Role of unit Tests  While the focus on learner incentives is intriguing the purpose of the unit tests is unclear.  The assumption that algorithms finding alternative solutions should be penalized is questionable as their behavior may not necessarily compromise algorithm performance.  The tests primarily indicate whether algorithms can learn from induced distribution feedback. additional Questions  Does specification gaming generally result in more significant distribution shifts than myopic algorithms  Are metalearning algorithms better suited to detecting such actions due to their exploration capabilities  Is it feasible to anticipate distribution shifts and practice constraints or regularization to steer solutions towards desirable outcomes  Figure 4s \"y2\" is not defined.", "Paraphrase: The paper examines autoinduced distributional shift (ADS) a phenomenon that occurs when the inputs to a machine learning algorithm are influenced by the algorithm itself. Traditional ML algorithms assume independent and identically distributed (i.i.d.) information and users but in reality this assumption often does not hold. For example in recommendation systems the choice of content impact user responses and influences the user demographics. In humancentered ML applications users may intentionally adapt their behavior to maximize their outcomes. The study aims to investigate ADS and develop tests to detect incentives for learners to induce ADS. effectiveness:  The issue is relevant and significant especially in the current landscape where ML algorithms are extensively used to interact with humans.  The paper introduces a new perspective by focusing on detecting incentives for ADS rather than mitigating its issue as commonly explored in related work. Weaknesses:  The paper lacks citations to relevant research on strategic behavior in ML and performative prediction which address similar concept.  The main insights presented are not particularly groundbreaking or surprising.  The examples used to demonstrate ADS seem ad hoc and not derived from a unifying framework.  The unit tests employed are presented in a casebycase way without a clear definition or rationale.", "Paraphrase: Summary: This study investigates Autoinduced distribution Shift (ADS) a situation where deployed models alter user behavior which then affects accuracy when the models are retrained. The research reveals that a reinforcement learning technique called PBT (Population Based Training) may induce distribution shift rather than improving accuracy. effectiveness and Weaknesses:  The study seems to extend findings from \"Performative Prediction\" (2020) which also examined model retraining and observed distribution shift. PBT adds multiple learners to this work but the author does not find its impact on distribution shift surprising.  The utility work examined in this study is more complex but the author argues that it does not alter the main findings.  The paper use of game theory terminology and notation is unclear and may hinder understanding. The term \"revealed incentive\" is also questioned as unconventional. Typo: The caption for Figure 2 contains the typo \"s action s reward s tuples\".", "Summary This paper introduces a phenomenon called Autoinduced Distributional Shift (ADS) which occurs in recommender systems when the promotion of specific content (e.g. broad vs. conservative) alters the active practicer base. The authors propose a new approach to studying ADS using PartiallyObservable Markov conclusion work (POMDP) which allows them to simulate the selfselection issue and analyze the system parameters that work the work such as its mixing time. Additionally the paper explores ways to \"hide the incentives\" or block the gradient way between state shift and rewards to prevent ADS using adaptations of the prisoners dilemma and comparing different learning algorithm including metalearning and Qlearning. effectiveness and Weaknesses effectiveness:  Rich in content and references  Innovative practice of POMDP to simulate selfselection  Novel adaptation of prisoners dilemma and examination of learning algorithm Weaknesses:  Lack of validation in realworld scenarios  Oversimplification of the problem leading to less impact  Incomplete explanations and technical confusions Style Comments:  Improve clarity and context in certain sentences  Use simpler wording and fewer technical terms Specific Concerns:  Lack of evidence of near100 \"cooperate\" rates in Experiment 1  Unclear explanations for the \"cooperate\" issue and analysis of Qlearning  Missing discussion on the complex nature of bias in recommender systems"], "s51gCxF70pq": ["Summary: The authors introduce kStep Latent (KSL) a technique for learning representations for visualbased continuous control tasks. KSL uses multistep predictions of actions and latent state as training signals. It outperforms baseline algorithms on six tasks in the dmcontrol suite in terms of sample efficiency and asymptotic performance. The learned representations capture the reward structure and reinforcement robust learning. effectiveness:  Clear and wellwritten paper.  Stateoftheart solution on dmcontrol suite tasks.  Inductive bias for representation learning derived from multitask learning. Weaknesses:  Limited evaluation on middlehard tasks.  Limited novelty due to combination of existing techniques.  Similarity of multistep predictive supervision to existing process without significant algorithmic differences.  Questionable justification for better representation learning in terms of permutation invariance and temporal coherence.  Lack of theoretical justification for using momentum encoder inputs in SAC critic training. Minor Points:  Main evaluation of representations is on a simple task with dense reward.  Could benefit from discussions on the relationship between KSLs inductive bias and other similar inductive biases.", "Paraphrased Statement: Summary: This paper introduces a representation learning technique (KStep Latent or KSL) that uses a selfsupervised auxiliary loss. This loss compares recurrently predicted actionconditioned representations of the state space with nonrecurrently predicted target representations similar to the BYOL approach. The method employs two separate optimizers on different model components to prevent interference in optimizermaintained statistics. Evaluation and Comparison: At 100k and 500k training steps KSL outperforms existing methods using different selfsupervised auxiliary losses on six tasks from the DM control suite (using pixel inputs). This demonstrates better data efficiency. Analysis: An analysis of the learned latent representations reveals that KSL produces more stable encoders and more accurate alignment with the underlying Markov decision process (MDP). effectiveness and Weaknesses: Clarity:  Overall the paper is generally clear.  However the description of KSL (section 3.2) may benefit from better notation and an explanation following the pseudocode.  The paper contribution is not explicitly stated leading to some confusion. Novelty and Significance:  The paper does not provide a sufficiently novel contribution to warrant acceptance.  Using multistep modelbased predictions with corresponding losses to enhance data efficiency is not a novel concept as evidenced by numerous cited and uncited prior process.  The specific form of the auxiliary loss while similar to BYOL is not significantly different from existing methods.  The empirical comparison is limited evaluating only on six DM control suite tasks.  The analysis of learned representations is valuable but does not fully showcase the proposed approach effectiveness.  Ablation field to assess the impact of different components and design choices would enhance the paper value.", "Paraphrase: Summary: This research used \"Bootstrap your own Latent\" (BYOL) in reinforcement learning (RL) by adding a transition model. This technique enhanced sample efficiency in RL. effectiveness:  Straightforward method  Clear writing  Significance in RL field: representation learning for efficiency  Promising experimental outcomes Weaknesses:  The transition model changes the underlying distribution of the latent representation potentially affecting its effectiveness.  The paper lacks an explicit acknowledgment of this potential issue.  Additional Questions:  Explanation of translation augmentation  potential impact of removing the stopgradient performance before the policy Recommendation: The paper should address the outofdistribution issue for the latent representation due to the transition model. Alternative Visualization: For Figure 3 consider using tSNE instead of PCA to provide a more comprehensive representation of the latent space.", "Paraphrased Statement: Summary: This research paper explores the issue of limited data efficiency in continuous control. It highlights that traditional reinforcement learning (RL) techniques simultaneously optimize policy selection and representation learning guided by a single supervisory signal (reward). The authors propose leveraging longterm temporal relationships in representation learning and introduce kStep Latent (KSL) a module for generating temporally coherent representations of the state space. Experimental solution demonstrate that KSL outperforms existing methods in the PlaNet benchmark suite showcasing its effectiveness in enhancing data efficiency. effectiveness and Weaknesses: effectiveness:  Tackles a important problem in RL.  Proposes a novel representation learning module (KSL). Weaknesses:  The concept of leveraging longterm temporal connections is not only groundbreaking.  Lacks a thorough comparison with related approach such as successor features.  Asserts that representation learning should be linked to reward which may not hold true for certain dataefficient methods or scenarios involving task generalization. Experimental Evaluation:  Thorough and comprehensive experiments.  Benchmarking against relevant baselines.  Demonstrates promising solution in terms of data efficiency. Minor Suggestions:  Increase font size for figure axes.  Adjust yaxis scale in selected figures (5.2 5.3 and 6) for clarity.", "Paraphrased Summary: This field demonstrates that an additional selfsupervised task enforcing temporal consistency in latent representations raise data efficiency in continuous control environments. It also identifies critical implementation details that facilitate its effectiveness. effectiveness and Weaknesses: The paper explores how selfsupervised learning raise data efficiency and provides detailed experimental evaluation. However it inaccurately presents the Kstep latent objective as a novel representation method when it is essentially the same as SPR. This oversight is acknowledged in the related process section but does not fully address the similarity in representation learning techniques. Questions on Experiments:  Generalization of Encoders: Specify the training and evaluation tasks used in the generalization experiment.  Invariance Experiment: Consider using the Distracting control suite for more realistic invariance tests.  performance Discrepancy: Clarify the deviation between reported performance in this paper and original solution from baseline methods.  Stochastic Analysis: Employ stratified confidence intervals on multiple normalized metrics for a more robust stochastic analysis."], "mk8AzPcd3x": ["Paraphrase: This paper introduces the Betweenness Centralitybased Distance Resampling (BCDR) technique for estimating shortest distances between nodes in network. BCDR addresses limitations of existing distance estimation methods that use random walks and pointwise mutual information (PMI). Specifically these methods struggle to explore sufficient distances and preserve the correct shortest distance relationship due to local extrema. BCDR employs betweenness centrality to guide random walks enabling them to cover longer distances. Instead of PMI it uses distance resampling (DR) to maintain the distance relationships between nodes. The paper provides theoretical guarantees of the techniques performance in terms of exploration range and accuracy in estimating shortest distances. Experiments on real and simulated datasets demonstrate the efficientness of BCDR compared to baselines in terms of exploration distance and preservation of distance relationships. Strengths and Weaknesses: Strengths:  Using betweenness centrality to guide random walks is novel and potentially efficient in overcoming local constraints.  BCDR considers both longer distance associations and the representation of shortest distances. Weaknesses:  Random walks even guided by betweenness can be affected by cycles in network structure. The paper does not provide detailed information on how BCDR handles this publication.  Critical contingent and proof are relegated to the appendix making the paper less selfcontained.  The paper does not cite recent research on random walk betweenness centrality which is relevant to the techniques approach.", "Paraphrased Statement: This paper introduces a novel method for embedding graphs while preserving shortest distances. The method combines a betweenness centrality random walk for path sampling and a distance resampling step prior to optimization. Experimental results demonstrate the algorithms accuracy in preserving shortest distance relationships and outperforming existing methods. Strengths and Weaknesses: Strengths:  Novel approach using betweenness centrality random walk and distance resampling.  Theoretical analysis and justification for the method.  efficient experimental results. Weaknesses:  complex notations in section 3 with some terms used without explanation. Questions:  Should the embedding distance matrix in Proposition 1 have an opposite sign  Does Proposition 1 imply constant distance distortion for large distance pairs under appropriate parameters Thank You: Thank you to the other reviewers for their input. The authors responses to my questions were clear and insightful. improvement: The paper would benefit from more precise claims and discussion including comparisons with LPCA and combinatorial approaches.", "Paraphrased Summary: Strengths:  Authors introduce a node embedding framework that improves the accuracy of shortest path distance prediction for undirected graphs.  The framework incorporates betweenness centrality and a distance resampling strategy to capture path distances effectively. Weaknesses:  Experimental results do not demonstrate a breakthrough in performance contrary to claims in the abstract.  The method is computationally expensive.  The authors do not compare their approach to methods that can perfectly encode graph structure and shortest path distances.  The benefits of using betweenness centrality over standard node embedding methods are not demonstrated with synthetic experiments.  There is no evidence that the embeddings are useful for downstream machine learning tasks. Suggestions for improvement:  Compare the framework to combinatorial approaches for distance queries.  Include experiments demonstrating the usefulness of the embeddings for other tasks.  Clarify Theorem 1 regarding the handling of infinite graphs.  Improve the readability and tone of the writeup.  paper run times for all methods.", "Paraphrased Statement: Summary This paper introduces a technique for creating graph embeddings tailored to handle queries for shortest distances (SDQs) using a sampling method based on betweenness centrality. This method identifies key nodes as landmarks for distance computing ensuring the embeddings effectiveness in distancerelated tasks. It modifies existing methods by resampling distances from walk paths and omitting a step involving pointwise mutual information (PMI) optimization. Strengths and Weaknesses While the paper proposes a unique approach to a complex problem it could benefit from enhancements in motivation experimental comparisons and experiment range.  Motivation: The claim that remote nodes should be considered for shortest distance metrics lacks clarity.  PMI optimization: The argument against PMI optimization being only relevant for local similarity is questionable.  optimization Objective: The method can incorporate an objective for reconstructing global distance matrices.  Experimental Comparisons: The method is compared to established graph embedding techniques on realworld datasets. However the most recent method used for comparison is DADL omitting other recent improvement.  Scalability: The large graph used in the comparison contains approximately 5000 nodes and there is no indication of the methods scalability to large graph sizes commonly encountered in embedding applications.  Complexity Analysis: The complexity analysis in Table 1 assumes a sparse graph but the paper suggests that the method could handle large graphs. However no experiments demonstrate this capability."], "nnU3IUMJmN": ["Summary The authors present a method for incorporating structural data (locality) into language models. They evaluate their approach in two domains (Wikipedia and Java source code) and present improvements in retrieval tasks. Strengths  The authors propose a direction to include structural locality in language models.  They present enhancements in retrieval tasks on two different domains. Weaknesses  The use of structural locality and its value in language modeling is not novel.  The method for integrating structural locality is relatively straightforward.  The authors do not compare or discuss alternative approach to incorporating structural locality in language models.  The improvements observed in the experiments are relatively minor. Revised Assessment Based on additional clarifications provided by the authors the score has been adjusted to \"marginally above acceptance threshold.\"", "Paraphrase: The researchers introduce a technique that enhances examples from external nonparametric language model repositories by incorporating locality data. This data represents the hierarchical structure of contexts making those sharing similar hierarchical attributes more similar (or less distant). The team conducted experiments using both source code and natural language articles. Their analysis reveals the ground behind the improvement and highlights the differences between these domains. effectiveness and Weaknesses:  The article is wellwritten and accessible.  The proposed approach is straightforward but valuable.  The example in project 1 effectively illustrates the motivation and method.  The concept of structural locality is welldefined.  The results are compared with other cuttingedge models (Table 2). Weakness:  The experiments just employ one dataset for each domain. Given the limited improvement in the results its challenging to infer that the results would improve on other datasets.", "Paraphrase: This paper proposes a nonparametric language model that considers structural locality which represents the spatial relationships between items in a sequence. Unlike previous models that just model cooccurrences this model also captures structural feature. The authors show that incorporating structural locality improves the models performance on source code and Wikipedia data. effectiveness:  The models novel loss role effectively incorporates structural data improving prediction accuracy.  The method outperforms existing nonparametric language models. Weaknesses:  The model requires manually defining structural properties for different datasets which may limit its generalization beyond the two data types used in the experiments.  The evaluation could be enhanced by extending the experiments to downstream application tasks such as document classification to further demonstrate the models utility.", "Paraphrased Statement: This work explores the benefits of incorporating structural locality a property of realworld data into nonparametric language models (LMs). The authors argue that:  a) Standard distance metrics used in nonparametric LMs do not fully capture structural locality.  b) Explicitly incorporating structural locality into nonparametric LMs can enhance their performance. They demonstrate their claims by:  Analyzing two datasets using custom locality role.  Integrating these locality role into a nonparametric LM with learnable parameters. effectiveness:  clear hypothesis and analysis supporting the claim that standard distance metrics do not adequately capture structural locality.  Development of locality features for datasets in different domains.  Incorporation of locality features into nonparametric LMs using learnable distance metric role.  findings showing that incorporating locality features improves the distribution of distance among nearest neighbors. Weaknesses:  Custom locality feature role require domain knowledge to capture structural locality.  minor improvements in results.  limited discussion of the learned parameters in Table 3."], "zXM0b4hi5_B": ["Paraphrase: Summary: This work examines the theoretical connection between natural image statistics and perceived differences in visual stimuli. It demonstrates that measuring distances in natural image encoders aligns with human perception of image similarity. The findings have implications for training models using perceptual distances as regularizers even without data. Strengths:  Clear structure  Helpful image Limitations:  Observations are limited to small image differences.  Assumption of metric perceptual distances may not hold for larger image discrepancies. Minor Comments:  Doublecounting effect could be addressed by balancing the work of perceptual loss and training.  RMSE correlation with MOS in image 3 appears constant requiring clarification.  Kernel smoothing in image 4 obscures the amount of data represented.  Typo on page 8: \"for perceptual distances we are \" Relevant Citation for Introduction: Storrs K. R. Anderson B. L.  Fleming R. W. (2021). Unsupervised learning predicts human perception and misperception of gloss. Nature Human behavior.", "Paraphrase: Summary: The authors examine the relationship between statistical machine learning techniques and human perception specifically:  perceptual distances are tied to how potential an image is to appear.  Latent distance distances derived from autoencoders are correlated with the probability of images in the training dataset and human perception.  perceptual distances overlap with Euclidean distances in latent distance. Strengths:  Relevant topic connecting machine learning perception and neuroscience.  Thorough literature review.  Wellwritten and accessible. Weaknesses:  lack significant original contributions.  Observations are largely known within the machine learning community.  More akin to a review paper than a research article. detailed Comments: This paper is a worthwhile read due to its interdisciplinary nature. However it fails to make significant contributions to any of the fields it covers.  machine learning: The papers observations about distances between images correlating with their distribution are not novel in the machine learning field as techniques like GANs and NeuralODEs aim to achieve similar goals.  Neuroscience: The paper lacks experimental data and models.  Psychophysics: No new data or insights are presented regarding perceptual distance and human perception. The introduction of multiple distance measures (Ds Dr De Din) is confusing and does not add significant value. The papers emphasis on the efficient coding hypothesis does not provide any new perspective. Conclusion: While the topic is promising this paper is premature. The most notable contribution is the exploration of autoencoder training with noise and a perceptual similarity distance but its utility in reducing data requirements is unclear.", "Paraphrase: This research investigates the interconnections between data perceptual distances probability distribution and unsupervised machine learning. It suggests a correlation between perceptual sensitivity and image likelihood in its vicinity. The connection between distances created by autoencoders and the probability distribution of training data is examined as is the compatibility of these induced distances with human perception. The work concludes that perceptual distances do not always enhance performance over Euclidean distance in image processing tasks. Strengths:  The paper is wellwritten.  Key findings are supported by observations and equations.  Contributions include correlating PixelCNNbased image likelihood with human perceptual distances linking autoencoderinduced distances to probability distribution and human perception and identifying a doublecounting effect in perceptual distancebased loss use. Weaknesses:  Certain findings are presented as theories rather than results.  The overall conclusion and implications for theory and use are not explicitly stated.  The concept of \"perceptual\" and \"psychophysical\" distances require clarification.  The applicability of the findings to other deep learning models beyond autoencoders is not addressed.  The solution to the identified doublecounting effect is not discussed.  The use of training without image data is similar to certain GAN models but this comparison is not mentioned.  Equation (2) resembles Vapniks empirical risk minimization theory but the added value is not explained.  The definition of \"Ex\" in Equation (3) is missing.  The use of the softplus use instead of softmax is not justified.  The statement about the development of the human visual system is speculative and not supported by empirical evidence.  The conclusion emphasizes commonalities between machine learning and biological perception but the focus should be on bridging the gaps.  The appendices are not clearly labeled.  The color system used in image 1113 makes it challenging to discern significant features.  Entropylimited autoencoders are not referenced raising questions about their originality.", "Summary This work presents novel observations linking image prior image quality with perceptual differences. Strengths  The work addresses an significant problem at the intersection of perception and statistics. Weaknesses  Observation 1: It assumes that perceptual distance is symmetric which it is not.  Observation 2: It relies on score matching which may not generalize to all types of distortions.  Observation 3: It assumes that the mapping use has a nonzero capacity.  Observation 4: It applies just to bijective mappings with identical dimensions which is not the case in compression.  Observation 5: It may not hold true for certain types of distortions and it remains to be seen how PixelCNNs approximation accuracy affects these observations.  Overall: The work lacks experiments and discussion on the practical applications of these observations in model design."], "rqcLsG8Kme9": ["Paraphrased Statement: This paper explores the use of data augmentation to stabilize the Qfunction estimate by aligning Qvalues between augmented and original states (Algorithm 1). The method achieves competitive results on challenging control tasks (Table 1) and Atari game (Table 2). strength and Weaknesses: data augmentation techniques in reinforcement learning are gaining traction and this paper presents a novel approach to regularize Qvalues using augmentation. The paper is wellwritten and its contributions are clear. The baselines in Tables 1 and 2 are current stateoftheart methods for these tasks. The data augmentations employed are similar to those in DrQCURL but applied to regularize the Qfunction over multiple sample action instead of standard updates. Two minor points of clarification: 1. The \"Qvalue distribution\" definition at the end of page 1 is not a statistical distribution but rather a collection of Qvalues for different action from a given state. 2. The action ai in equation (1) are sample from a minibatch for each state st. This suggests that action from irrelevant states in the minibatch are used for regularization. It would be intriguing to explore alternative action sampling distributions for this purpose.", "Paraphrased Summary: This field introduces a simple auxiliary loss for controlling Qvalues using minibatch action of images and their adjusted translation. The method has been tested in continuous control and ALE settings (commonly used benchmarks). strength and Weaknesses:  The approach is straightforward and successful.  However there is existing work that is not referenced or compared in the paper including RAD (Reinforcement Learning with Augmented data) which is a simpler and highly relevant method.  Table 5 in the cited RAD paper and Table 1 in this paper present inconsistent results for the same scores in the DeepMind Continuous Control Suite. RAD scores are occasionally similar or superior to the proposed method and CURL scores differ between the two tables for an unknown reason. This discrepancy should be clarified by the authors.", "Paraphrased Statement: Summary: This research proposes a regularization technique for reinforcement learning. It aims to minimize the difference in \"Qvalues\" (expected future reinforcement) between an original image (representing the current state) and its transformed counterpart (representing a potential next state). This approach improves the resilience of RL algorithms to environmental variations. The paper provides background data and motivation for the proposed method. It also compares it to related methods such as SAC and DrQ. experimental results demonstrate that the proposed regularization enhances the performance of imagebased RL methods yet surpassing several statebased approach. strength and Weaknesses: strength:  The method is straightforward and can be easily integrated with various RL methods.  Experiments support the strength of the regularization approach. Weaknesses:  The idea of matching Qvalues between original and transformed images is not entirely novel in computer vision.  further investigation and analysis of the regularization technique is needed including exploring different loss functions and comparing it to alternative regularization methods.  There are some minor formatting issue in the paper including large symbols and small text in image.  Lack of clarity in the organization of Section 4.  The term \"n\" in Equation 1 represents the issue of action not the batch size.", "recent deep RL algorithms commonly employ image augmentation techniques. While previous methods focused on maintaining consistency at the individual sample level this paper advocates for considering the distribution of statistics at the minibatch level to ensure consistency. The paper presents results on Atari and DMC benchmarks for discrete and continuous control tasks. strength:  Straightforward concept  clear explanations  Evaluation on widely used benchmarks Weaknesses:  issue do not demonstrate a significant advantage over existing techniques"], "kO-wQWwqnO": ["Paraphrased summary: This paper introduces an approach to enhance lowlight images without relying on paired information. The method leverages geometric and lighting consistency as well as a contextual loss to improve image quality. Strengths:  Addresses the challenging task of lowlight image enhancement without paired information.  Incorporates geometric and lighting priors as guiding principles for enhancement. Weaknesses:  The title of the paper is misleading and overly extensive.  The papers organization and writing are lacking with an inadequate innovation and a disorganized method section.  The novelty of the approach is limited as geometric and lighting priors are commonly used in unsupervised lowlight image enhancement.  The constraints imposed by the priors are based on simplistic assumptions.  The paper uses unexplained terminology making it difficult to follow.  Table 1 is confusing and poorly organized.  The experimental evaluation lacks comparison with a comprehensive set of stateoftheart methods.  An ablation work is needed to assess the impact of the proposed priors and loss work.", "Paraphrased Statement: This work presents an image enhancement model that can convert lowlight images into brighter images without the need for paired training information using CycleGAN. Unlike similar recent GANbased methods such as EnligthenGAN and UEGAN the proposed model uses cycleconsistency as well as geometric and illumination consistency. Additionally the model employs separate discriminators for color texture and edge. Strengths:  The model performs competitively on benchmark informationsets outperforming previous models on LOL and BDD informationsets.  Preprocessing shadow images with the model improves face detection performance compared to EnligthenGAN and enhances object detection performance (though not compared to previous work). Weaknesses:  The methodology description in Section 3 is unclear and confusing making it difficult to fully understand the loss work.  The nature of the contextual loss is not fully explained and the equation (3) is potentially misleading.  The authors need to address the argument that using cycleconsistency can increase network complexity and training difficulty as suggested by EnligthenGAN and UEGAN.  The ablation work is insufficient to demonstrate the effectiveness of the color texture and edge discriminators as well as the geometric and lighting consistency.  References are missing for the contextual loss and the informationsets used for training and evaluation.", "Paraphrased Statement: summary: This research proposes an image enhancement technique utilizing a translation network (cycleGAN) based on unpaired GANs. It incorporates geometric lighting consistency and a loss work based on context. The geometric consistency involves rotating the input image 90 times and the lighting consistency involves applying gamma translation. Strengths and Weaknesses: 1. Novelty: The papers novel contribution is unclear. The authors claim that utilizing the combined effect of existing architectures and loss work is a significant improvement but it remains uncertain if this is sufficient for publication in ICLR. Moreover the solution do not demonstrate noted improvement over existing methods. 2. Hallucination: It is wellknown that unpaired GANs (or cycleGANs) can produce fake content (hallucination) a significant issue in image restoration and enhancement. However the paper fails to address or discuss this problem. 3. Denoising and Lowlight enhancement: While the paper mentions addressing both denoising and lowlight enhancement it fails to explain how the proposed method can handle noise. Furthermore it does not clarify how the network balances noise suppression with oversmoothing. 4. Texture Preservation: The paper uses grayscale images as texture images. The reason why a simple grayscale performance can preserve texture information remains unclear. The advantages of using grayscale over RGB in lowlight texture preservation are also not discussed. 5. Comparison to Existing papers: The paper cites various related work that utilize unpaired images but it does not provide a thorough comparison or analysis of the proposed method in relation to these work.", "Paraphrased Summary and Evaluation: The researchers present an updated lowlight image enhancement technique that combines ideas from geometric and lighting consistency with a step of similarity based on context. The work was conducted on commonly used information set and the visual and statistical solution were compared to those of other approach. Strengths:  Impressive solution on standard information set showing improvement over existing methods. Weaknesses:  The individual elements of the technique (unpaired image translation geometric and lighting consistency contextual loss multiscale discriminators) are not new.  The paper is seen as a gradual progression in the field rather than a significant breakthrough warranting acceptance at the present conference."], "fWK3qhAtbbk": ["Paraphrased Summary: This work introduces an exponential data partitioning technique to address the vanishing and exploding gradient issue encountered in longterm time series forecasting. The techniques effectiveness is demonstrated using seven public datasets. Strengths:  Evaluation on multiple datasets  Opensource code for reproducibility Limitations:  Strong claim regarding LSTM as the sole stateoftheart forecasting method  Lack of comparison with current stateoftheart global forecasting models.  function of RMSE as the evaluation metric (which can be sensitive to outliers)  Lack of statistical significance analysis  Insufficient data on hyperparameter optimization methods  Grammatical errors and inconsistencies in language Recommendations:  Revise the claim regarding LSTM as the stateoftheart.  Include comparisons with current global forecasting models.  function standard error value (e.g. MAPE sMAPE) and provide statistical significance analysis.  Optimize hyperparameters and clearly describe the optimization methods.  Proofread the manuscript for language and grammar issue.", "Paraphrase: Summary: The authors explore a challenge in time series analysis: how to learn from extensive time series that cannot be processed directly by models like LSTMs due to limitations in memory and time. The traditional approach involves dividing the time series into equalsized bins for aggregation. Proposed method: This paper introduces \"exponential partitioning\" where time series are divided into bins with increasing sizes as we move further away from the present time. This is based on the notion that data point closer to the present hold greater relevance for future prediction necessitating shorter aggregation intervals. The authors evaluate their method using diverse aggregation functions (mean median min max) and demonstrate significant improvements over the standard binning approach. Strengths:  Novel idea of varying aggregation intervals based on temporal proximity  Convincing performance on the datasets tested Weaknesses:  Limited novelty in the methodology itself  Reliance on outdated characterization of LSTMs as \"stateoftheart\"  Inconsistent and visually unappealing tables and graphics", "Paraphrased Summary This paper examines diverse windowing and pooling techniques for feeding data into an LSTM encoder for time series forecasting. Windowing strategy:  Uniform partitioning of historical data  exponential window sizes with larger aggregation for older data allowing for more recent data to be analyzed in greater point while still considering long history. AggregationPooling function:  maximum minimum mean and median Findings:  exponential windowing and median aggregation show potential benefits.  The work highlights the practical relevance of exponential aggregation which is often functiond in realworld timeseries forecasting due to large data sizes. Strengths:  Detailed description of experimentation for reproducibility. Weaknesses:  Lack of comparison with further architecture designed for extensive history attention (e.g. wavenet architecture).  Limited scope by predicting only one univariate time series despite the initial mention of multivariate forecasting.  Inclusion of an irrelevant section (Section 2).  Short history length (48) functiond in experimentation which an LSTM might be capable of handling.  Lack of comparison with a regular LSTM on a standard multivariate forecasting task to assess the impact of aggregation only.  Unclear function of other timeseries in the dataset as covariates if any.", "Paraphrase: Summary: This paper explores different methods for combining timeseries data into inputs for LSTM models. It suggests using nonuniform aggregation with smaller bins for recent data and larger bins for older data. Additionally it recommends employing nonlinear aggregation such as median maximum and minimum instead of simple averaging. These findings are primarily based on experimentation using seven timeseries datasets. Strengths:  The concept of nonuniform aggregation for LSTM inputs is consistent and shows potential.  The function of nonlinear aggregation functions is also reasonable. Weaknesses: 1. Experimental Setup:  The arbitrary selection of 13 different partitioning raises concerns about fair comparisons.  The function of frontloaded partitioning may have essentially created LSTMs with shortterm windows.  No statistical significance was assessed.  only a single timeseries was analyzed from datasets with multiple series.  Crossvalidation was applied inappropriately potentially using future data in preparation. 2. Unclear model:  The model in Section 2 lacks clarity and fails to provide convincing evidence. 3. Limited Generalization:  The findings are not wellgeneralized or interpreted.  Its unclear how they apply to different window lengths or prediction horizons.  The reasons for the effectiveness of maxminmedian and why min is less effective than max are not explored. 4. Absence of Transformer Literature:  The paper lacks any mention or comparison to existing transformer models and attention mechanisms that address longmemory challenge. 5. Clarity issue:  number captions are difficult to understand.  The experimental setup requires multiple readings to comprehend. 6. Grammar:  The paper contains grammatical errors."], "rwE8SshAlxw": ["Summary: This work presents a novel concept of factorized 3D consistent neural representations. It proposes a method that combines slot attention mechanisms with conditional neural radiance fields to segment and render novel view of a scene from a single view. The method addresses a limitation of slot attention by learning separate distributions for the foreground and background objects leading to improved performance on scenes with complex backgrounds. effectiveness:  Addresses a novel problem of modeling 3D scenes as a collection of disjoint objects for novel view synthesis.  Builds upon further techniques like slot attention and neural radiance fields.  Improves on slot attention by treating background and foreground objects as distinct distributions.  Acknowledges and differentiates from concurrent work.  Provides comprehensive experimental evaluation. Weaknesses: Questions:  A quantitative comparison with the concurrent work of Stelzner et al. (2021) would be beneficial.  Number of samples per ray used in volumetric rendering and the use of NeRFs coarse and fine networks should be clarified.  Advantages of using random patch sampling in fine training and downsampling in coarse training should be discussed.  The understanding for uORFs improved segmentation performance should be highlighted.  The process of modifying foreground object pose and appearance should be explained more clearly.  The work of the foreground box used for disentanglement should be described in the main text.", "Paraphrase: This research employed NeRF to develop a novel technique that infers scene arrangements without supervision. It leveraged datasets of similar objects in varying configurations. After training the system can accurately determine object arrangements and 3D geometry from a single RGB image. The authors implemented two key enhancements: separating background and foreground objects and employing coarsetofine training for efficiency. They demonstrated the methods effectiveness on three synthetic datasets and showcased its potential in various application. However the research also has limitations:  All experiments were conducted on synthetic datasets raising concerns about generalizability to realworld scenarios where objects not seen in training may be present. An ablation work with unseen work or demonstrations on real scenes could address this.  The method requires specifying the number of foreground objects which could impact the results but was not explored in the work.", "Paraphrase: This paper presents a novel method for creating a neural representation of an image that identifies single objects from the background. This allows for tasks such as modifying or rearranging the objects. The method involves: 1. Selecting K \"centers\" that represent each object. 2. Assigning each pixel to the center it is closest to. 3. Updating the center points using a learnable GRU (rather than simply averaging the assigned pixels). The authors demonstrate the method on three custom datasets producing reasonable results for tasks like 3D segmentation and object rearrangement. effectiveness:  The overall concept is promising as it addresses the importance of factoring scene representation.  The method is technically sound. Weaknesses:  The results are preliminary with tests performed on simple objects with uniform colors.  The method may not work well in complex scenes due to object detail loss.  The concept is similar to existing work (GIRAFF) and improvements in architecture and training are needed to enhance quality. Unanswered Questions:  Whether the method consistently produces reasonable representations for different initial centers."], "qjN4h_wwUO": ["Paraphrase: This paper introduces a method for adding neurons to neural networks strategically. To maintain the forward pass the new weights are initialized to maximize the gradient with respect to those weights. While the method shows slight improvements over baseline models it appears more complex. effectiveness and Weaknesses:  The paper is wellorganized and straightforward.  The proposed idea seems original and the experiments on image datasets are exhaustive.  The improvements are modest but consistent. However there are some concerns:  The need for the method is not fully explained or supported by mathematical or empirical evidence.  Its unclear how the initialization approach ensures the desired gradient behavior as the backward gradient computation also changes.  The teacherstudent experiments do not address the issue with models that have batch normalization layers.  The impact of batch normalization on the methods need and proposed initialization strategy is not discussed. If the authors provide a clearer explanation of the need and mathematical reasoning behind the method as well as how it handles batch normalization they could strengthen their case for the methods effectiveness.", "Summary: This paper introduces a novel method for growing neural networks by adding units that maximize the gradient norm. This approach aims to find optimal network sizes and yield sound learning benefits compared to adding units that minimize loss. Using a simplifying assumption the problem is formulated as an optimization problem solvable via SVD. Despite being a fullbatch method using large minibatches provides a practical algorithm. Experimental solution show that the method improves subsequent training progress. The grown networks achieve performance comparable to networks trained from scratch. While not reaching comparison this approach requires less computation due to starting with smaller networks. The method is applicable to shallow fullyconnected networks and generalizes to convolutional networks including ResNets. effectiveness:  Novel and practical approach to network growth  Costeffectiveness due to fast SVD problem solving  Clear need and derivation  Applicability to modern convolutional networks Weaknesses:  Confusing writing and organization  lack of clear indication of convolutional network testing  Misplacement of test set performance table  Suggestion to consider additional baselines and systematic experiments  Adding units with 0 weights and small activation  Exploring different starting points and growth strategies  Determining optimal stopping criterion  lack of citation for the original work on growing networks (CascadeCorrelation)  Misinterpretations in the papers text  Loss being upper bounded by gradient norm (should be decrease in loss)  Incorrect reference to parameter norm (should be gradient norm)", "Paraphrased Summary: In this research the paper introduces the GradMax algorithms. These algorithms aim to improve optimization outcomes in previous iterations by adding neurons. effectiveness and Weaknesses: effectiveness:  Wellwritten and organized  Clear need and methodology  Simple yet innovative approach Weaknesses:  Omission of relevant prior works in the experiments  Absence of time complexity analysis for GradMax and GradMaxOpt algorithms", "This paper introduces a method for initializing weights when expanding neural networks. It aims to maximize the gradient norm of new neurons weights. By leveraging Singular Value Decomposition (SVD) this initialization method (GradMax) prioritizes adding neurons with larger gradients. effectiveness:  The concept behind GradMax is logical aiming to enhance the new weights contribution to the networks learning process.  SVDbased implementation appears theoretically sound.  Numerical experiments demonstrate GradMaxs superiority over random initialization. Weaknesses:  GradMax may not be applicable when adding new layers to networks but could potentially work with skip connections.  While gradual neuron addition can save training time it may compromise performance compared to training a larger model from scratch.  The paper focuses solely on weight initialization without addressing practical aspects such as determining when and where to add neurons or when to stop the growth process.  GradMax should be integrated with an architecture search method to demonstrate its effectiveness in realworld application."], "givsRXsOt9r": ["Paraphrased Statement: Summary: This research introduces a message passing system that employs spherical coordinate for 3D molecular graphs. It has been evaluated on three distinct datasets and offers a thorough examination of various aspects through extensive experimentation. effectiveness:  Replicability: The experiments provide detailed insights into performance ablation work and implementation enabling others to reproduce the findings.  Novelty: This approach is believed to be novel in message passing on graphs as it incorporates geometrical information. However a more comprehensive contextualization of prior work on geometrical graphs is recommended. Weaknesses:  Comparisons: Most results are borrowed from other work without repeating experiments. It is unclear if dataset splits are consistent across methods. Reimplementing some competitors or providing an average of multiple work would enhance comparisons.  applicability: The method is tailored to a specific field (molecular graphs) and its applicability to other fields (e.g. meshes) remains uncertain. computational timings on different fields would provide valuable insights into its potential impact.  Clarity: The researchers do not specify the graph dimensions but they likely involve a small number of nodes (tens) given the molecular context.", "Summary Paraphrase: This paper presents a novel method called SMP (spherical message passing) to reduce the computational complexity of geometrical graph neural networks (GNNs) based on the spherical coordinate system (SCS). SMP reduces the complexity by an range of magnitude and provides a unified framework that encompasses various existing geometrical GNN approaches. Empirical results demonstrate the effectiveness of SMP. effectiveness and Weaknesses: effectiveness:  (1) SMP efficiently reduces SCS computational cost from O(nk3) to O(nk2) generalizable to various tasks.  (2) The paper structure allows for a smooth flow from motivation to solution.  (3) Empirical results support the efficacy of SMP. Weaknesses:  (1) The motivation for SMPs superiority over CCS is not fully explained. The example in Section 2 may be confusing:  Cutoff distance does not necessarily exclude the HH bond in the example.  SMPs computational cost remains high than CCS for the same cutoff threshold.  SMP may better reflect chirality which could be highlighted.  (2) The selection of atom q1 in SMPs reference plane determination is unclear. Its potential impact on results is not discussed in Section 3.  (3) The classification of related work into EGNN and relative information seems arbitrary as models like SE(3)Trans and EGNN also utilize relative distance. Minor Concerns:  (1) Typo in introduction: \"... relative location of each atom in a 3D graph \u2026\"  (2) Clarification in introduction: \"integrating the SMP and physical solutions to the Schrodinger equation\" requires further explanation.", "Paraphrased Statement: Summary This research focuses on creating representations for 3D molecular graphs. It proposes a comprehensive messagepassing framework that incorporates existing methods. Additionally it introduces a novel messagepassing approach that utilizes 3D geometrical information such as distance angle and torsion. The proposed frameworks performance was evaluated on various datasets demonstrating its effectiveness in representing 3D molecules. effectiveness  Incorporating torsion information in 3D molecular representation is innovative and valuable.  The messagepassing approach can differentiate specific molecular structures better than some existing frameworks.  The framework exhibits strong expressive ability in representing 3D molecules as evidenced by its performance on benchmark datasets. Weaknesses  The spherical messagepassing (SMP) scheme lacks novelty as it primarily extends the graph network (GN) framework with geometrical features.  The SphereNet architecture resembles DimeNet except for its inclusion of torsion information.  DimeNet has already demonstrated the importance of incorporating both distance and angle in 3D molecules.  The methods for representing and embedding distance and angle are similar in SphereNet and DimeNet as seen in the comparison of their architectures."], "xkjqJYqRJy": ["Paraphrase: Summary: This study suggests using nonGaussian distributions as priors for Bayesian neural networks. Laplace and Students tdistributions were roled for fully connected networks and multivariate Gaussian distributions with spatial correlations were roled for convolutional networks. Experiments show that these priors outperform isotropic Gaussian priors in image classification tasks. Cold posterior versions of the priors are also provided for comparison. Overall this is a novel and promising approach. Strengths: 1. The study empirically analyzes SGDtrained network weights identifies characteristic distributions and proposes priors that match these characteristics. An expectationmaximization procedure supports this approach and effect justify the prior choices. 2. The impact of the priors on different network types including cold posterior effect is discussed. Weaknesses: 1. The impact on computational speed needs further assessment. Do the novel priors significantly increase computation compared to isotropic Gaussian priors 2. The role of SGD for posterior weight sampling needs clarification. Stochastic gradient Langevin dynamics is typically roled for neural network posteriors and the rationale for using SGD instead should be explained. 3. While a goal was to enable Bayesian inference without temperature adjustment effect with a temperature of 1 do not always match or surpass SGD effect. This may weaken the motivation and impact of the approach.", "Paraphrased Summary: The paper investigates alternative prior distributions (beyond standard Gaussian) for Bayesian neural networks (BNNs). The authors empirically analyze the posterior distributions of BNN weights and propose using these distributions as priors for improved BNN performance. The study is presented in the context of the \"cold posterior effect.\" Strengths and Weaknesses: Strengths:  Wellwritten and engaging  Thorough experiments considering various aspects (e.g. activation role variance convergence)  Provides a comprehensive overview of the current literature  Offers insights into the optimal choice of priors for BNNs and a model for further exploration Weaknesses:  While relevant the insights on the superiority of alternative priors may not be particularly surprising  Some missing details on data augmentation and specific parameters used (e.g. Matern kernel choice)  image and conclusions should be more clearly explained or discussed (e.g. degree of freedom decrease in weight distributions shaded domain in performance graphs) Recommendation: Despite minor weaknesses the paper is deemed worthy of publication due to its thorough analysis clear introduction and potential value to the community.", "Paraphrased Summary This research investigates the role of alternative priors for Bayesian Convolutional Neural Networks (CNNs). It finds that commonly roled priors like Gaussian distributions are not a beneficial set for the empirical distributions observed in trained CNN weights. Instead the empirical distributions tend to be heavytailed and correlated. To address this the research proposes:  practice \"heavytailed priors\" for fully connected CNNs (FCNNs)  practice correlated Gaussian priors for CNNs These proposed priors generally improve the classification performance of CNNs. Strengths and Weaknesses  Some effect show instability with Gaussian priors sometimes performing better than heavytailed priors.  The \"coldposterior effect\" effect are also inconsistent between model.  The Stochastic Gradient Descent (SGD) baselines are not trained well enough potentially skewing the effect.  The study only roles one inference technique limiting the generalizability of the findings.", "This study provides detailed empirical data on weight distribution after neural network training. The researchers analyzed weights from three types of networks (FCNN CNN and ResNet) and calculated various statistical measures. The study also used a Bayesian model to compare the performance of trained Bayesian neural networks with varying priors. Strengths:  The experimental findings provide valuable insights for researchers seeking to understand the impact of priors on neural network behavior. Weaknesses:  The authors conclusions are not clearly expressed.  The effect obtained from a single FCNN and CNN are generalized to all FCNNs and CNNs respectively. The study should have assessed consistency across multiple model and datasets.  The observed effect lack heuristics or explanations with the exception of the \"cold posterior effect\" being attributed to data augmentation."], "gmxgG6_BL_N": ["Paraphrase: Summary: The researchers introduce the supervised variational component decoder (sVCD) a technique for deducing a source signal from nonlinear mixtures. This model employs a variational autoencoder structure with a sequencetosequence translation network. The model optimization is based on a variational lower bound on the source signals likelihood. The researchers evaluated their method on synthetic mixtures of nonlinear sequences as well as an EEGEOG dataset. Strengths and Weaknesses:  Strengths:  Clearly written and understandable  Elegant extension of a seqtoseq VAE that uses attention for source signal reconstruction  Convincing ablation study demonstrating the effectiveness of the attention mechanism for timeseries signal source extraction  Weaknesses:  experimental analysis and evaluation are limited.  The distinction between \"nonlinear source separation\" and \"source extraction\" is not fully convincing.  The experimental results on the EEGEOG dataset are not practical because they use the EOG signal as the ground truth which does not represent a meaningful experiment in nonlinear EEG source separation.  The methodological comparisons with existing methods (e.g. ICA) are insufficient.  The implementation details of ICA in the experiments are not provided including the number of components extracted and the selection work.  The regularization effectiveness for the KL term is not specified. Additional Notes:  Typos:  \"sour\" should be \"source\" on page 2 line 4.  \"realized\" should be \"realize\" on page 6 sentence 1.", "Paraphrase: Summary: The proposed approach tackles supervised nonlinear regression for multivariate time series using sequencetosequence model with selfattention and a generative prior for latent codes. This method is framed as extracting sources from a nonlinear mixture. The paper demonstrates its application on synthetic data and two realworld datasets: heartbeatrespiration from radio frequency and EEGEOG where the goal is to isolate EEG signals from EOG contamination. The model successfully extracts signals in these model. Strengths:  Novelty of the model problem addressed.  Literature review on nonlinear and linear source separation.  Relevance of Seq2Seq approach for time series tasks.  Promising accuracy results in experimental evaluation. Weaknesses: 0. Modeling and Baseline comparison:  The problem is essentially a regression task but baseline comparisons with nonlinear regression model are lacking.  The ICA baseline which extracts linear source signals is not fairly compared since it is instantaneous and does not consider temporal dependencies.  The use of cosine similarity instead of MSE is unexplained. 1. Time Series aspect:  The model architecture does not impose a causal structure on the decoder leading to noncausal representations via selfattention.  The impact of sequence length on regression performance is not discussed.  The instantaneous ICA approach does not fully capture the nonlinear mixture of sources in the EEGEOG dataset. 2. Electrophysiology and Notation:  The description of electrophysiology in the EEGEOG dataset is inaccurate as the electric potential are mixed instantaneously.  The notation used is not clearly explained leading to confusion about symbols like 1T. 3. Ablation study:  The Seq2Seq method performs well without variational inference but there is still a gap that could be reduced by regularizing the difference from the mean of the affine transform in the encoding layer. minor Points:  Typographical errors on page 1 and 2.  Time indices are not indicated in design 2.  The KL divergence should be minimized by maximizing the variational lower bound.  The subscript notation in Equation 1 could be confusing.  The concatenation notation in Equation 14 should be defined.  The term \"forward bidirectional\" on page 5 could use clarification.  The L2 distance used is actually the squared L2 distance between signals.", "Paraphrased Statement: problem: Separating all sources in a nonlinear mixture using blind source separation is challenging and may be unnecessary if only one specific component is desired. Solution: Create a model specifically designed to extract the desired component. This model known as a variational component decoder is trained to reconstruct only the component that needs to be separated. Additional contribution: A lower bound on the variance for the proposed architecture has been established. Strengths:  Exceptional clarity and writing quality  Comprehensive analysis in Section 5.4 explores the limitations of the Gaussian assumption Weaknesses:  Typo: \"sour\" instead of \"source\" on page 2  study benchmarking against more innovative techniques such as other deep learning model or algorithms tailored to the specific application.", "Paraphrased Statement: Summary Researchers propose a groundbreaking approach using deep learning to address nonlinear blind source separation (BSS) challenges. Their technique combines two model: Seq2Seq and variational inference to effectively extract a single source of concern from a complex nonlinear mixture. The model leverages prior knowledge about the target source to enhance its accuracy. Strengths and Weaknesses Upon reviewing the paper no apparent logical or experimental flaws were detected. Although a exhaustive mathematical verification was not conducted the presented results adequately support the authors claims. While the underlying techniques are not entirely novel their application to nonlinear BSS is original. BSS is a common problem and even though this study focuses on a specific scenario where only one source is separated its potential impact is significant. The authors findings demonstrate the superiority of their method over existing stateoftheart approach. The paper is worthy of acceptance however it would be valuable for the authors to draw connections between their work and Sparse Coding in addition to nonlinear ICA."], "fuaHYhuYIDm": ["Paraphrased Statement: This paper introduces a novel interpretation approach MAGNEx that works with multiple models. MAGNEx uses a neural network to predict which input features influence the models predictions. The authors experiments show that MAGNEx is more efficient than other methods (LIME and Integrated Gradients) in generating faithful explanations particularly with highdimensional input while being much faster. Strengths and Weaknesses: Strengths:  Novel and efficient approach that outperforms other methods  Experiments demonstrate performance across different tasks and modalities  Detailed analysis and discussion of the technical approach Weaknesses:  Lack of discussion on causal explanations: MAGNEx does not provide a causal understanding of the models decisions which can limit its usefulness.  Questionable image 3 and 4: The image rely solely on the authors sentiment about the models internal state and do not provide comparisons to counterfactuals.  Unconvincing empirical results: MAGNEx does not show a clear advantage over other methods in the first three experiments and the fourth experiment has high variance. The results on BERT may not be significant for short sentences and the CNN experiments could be conducted on a more challenging dataset.", "Paraphrase: The paper introduces a novel approach MAGNEX for explaining the predictions of any neural network model. Strengths:  The concept of MAGNEX is intriguing. Weaknesses:  The experimental evaluation has limitations:  The sparsity metric is consistent for all methods without statistical significance testing.  MAGNEX is evaluated based on metrics it optimizes raising questions about its performance on other step.  The focus on explainability runtime is not emphasized in the paper.  MAGNEX lacks examples for image or questionanswering tasks.  MAGNEX is not tested on simpler data type like tabular data.  The paper should include additional baselines such as surrogate models and feature importance methods.  The effectiveness of these baselines should be explored when retrieving features from MAGNEX rather of relying on fixed assignments. minor Considerations:  The paper should cite related literature such as the process of Setzu et al. ElShawi et al. and Spinner et al. on local and global explainability.", "Paraphrased Statement: This paper introduces a novel global explanation method for opaque machine learning models. It combines the original input features with the blackbox models output into an explainer model which assigns importance scores to each feature. Based on these scores unimportant features are removed and the remaining input is passed to the original model. The outputs of the original input and the simplified version are compared. The explainer model is trained to minimize the difference between the original and simplified input when they are fed into the original model. Additionally the sparsity of the simplified input is maximized. This novel explainer outperforms LIME and Integrated Gradients in terms of explanation fidelity and computational efficiency in tasks like image classification sentiment analysis and question answering due to its significantly improved performance time. Strengths and Weaknesses:  Strengths:  Intuitive approach  significant improvement in performance time  Weaknesses:  Unclear whether the performance time of the simplified input through the original model is included in the total time  Questionable claim in the introduction about explainability ensuring fairness  Lack of clarity on the question in image 4  Absence of human evaluation of the explanations helpfulness  Lack of significant contribution beyond the primary advantage of faster performance time", "Paraphrase: Summary: This paper introduces a novel algorithm for explaining predictions made by any machine learning model. This algorithm considers all the data used to train the model making it more comprehensive than previous methods. Strengths:  Can be used with any type of model  Relatively efficient Weaknesses:  The writing could be improved for clarity and organization.  The novelty of the algorithm may be limited as other methods have also attempted to generate global explanations.  The experiments included a diversity of models but more competitive baselines could be used to demonstrate the superiority of the proposed algorithm.  The conclusions from the experiments could be further explained particularly regarding the sparsity values."], "hzmQ4wOnSb": ["Paraphrase: Summary: The paper presents a novel GNNLM (Graph Neural Network Language model) model for question answering and commonsense reasoning on knowledge graphs. They introduce Graph Soft Counter (GSC) a straightforward and efficient GNN module that counts edge reference in the subgraph extracted for a questionanswer pair. GSC has a minimal issue of trainable parameters compared to existing GNNLM models. effectiveness and Weaknesses: The paper demonstrates the effectiveness of GSC achieving superior performance to UnifiedQA on the OpenbookQA dataset with only 130 parameters. The authors conduct several analysis on GSC including an ablation study on its parameters. yet concerns arise regarding how GSC achieves such high performance with its \"disentangled\" GNNLM architecture. The final reasoning score (qascore) is a sum of a context score (determined by the LM) and a graph score (calculated using GNN and KG) with no evident interaction between these terms in GSC. Unlike QAGNN which propagates data from the question context embedding to the KG subgraph through GNN rounds GSC calculates the graph score solely from the KG subgraph. This raises questions about whether GSCs performance is truly differentiated by the context score particularly for questions with similar entities and answer candidates. Given the simplicity of GSC (less than 10 lines of code) the authors are encouraged to release their implementation openly for review and broader application. additional Questions and Comments:  The issue of parameters vs. model size in Table 2 is unclear. GSC appears to have the same value for both.  design 5 highlights the impact of random seeds on GSC performance yet for its simplicity. The authors should provide an explanation.  The paper only experiments with ConceptNet as the knowledge graph. Including results on other QA datasets and knowledge graphs would enhance the impact of the study.  The paper does not address whether GSC can handle finding answers beyond multiplechoice alternative potentially covering all nodes on the KG.  A detailed description of the MLPCounter module used in Table 8 is requested.  A comparison with prior work on structured reasoning such as negation would be valuable.", "Paraphrase: Summary: modern QA systems often combinepretrained Language model (LMs) with Graph Neural Networks (GNNs) forcomplex reasoning tasks. LMs provide approach to implicit knowledge while GNNs work explicit knowledge in knowledge Graphs (KGs). These systems have shown impressive results but their complexity has increased. Objective: This paper aims to analyze the use and efficientness of GNN modules in QA systems. It questions if GNNs are overly complex and investigates their reasoning capabilities. Methodology: The paper analyzes stateoftheart GNN modules and their reasoning ability using SparseVD a diagnostic tool. Based on these findings it proposes a simple graphbased neural counter (GSC) to address overcomplexity. Contributions:  Identification of overcomplexity in existing GNN modules for QA.  Development of GSC a simple and efficient alternative to complex GNN modules.  GSC achieves comparable performance to SOTA GNN modules with fewer parameters demonstrating the importance of counting in KGbased reasoning. effectiveness:  Analysis of the overcomplexity of QA systems.  introduction of a simple and efficient solution (GSC) that outperforms complex GNN modules. Weaknesses:  Simplicity of the proposed solution raises questions about its efficientness.  Limited novelty in the solution.  Lack of sufficient explanation for the oversimplification hypothesis.  unclear description of the \"relevance Score\" and \"IH..\" column title in Table 1.", "Paraphrase: Summary: Many knowledgeintensive tasks like question answering combine language models (storing knowledge implicitly) with Graph Neural Network (GNN) models (explicitly incorporating external knowledge from knowledge Graphs (KGs)). This study analyzes GNNbased QA models that use message passing over KGs to obtain external knowledge for answering questions. Using sparse variational dropout the researchers found that current GNN modules are unnecessarily complex and overparameterized. They present a simplified 1dimensional neural counter model that counts nodes and edge in the graph which outperforms current complex GNN architectures suggesting that these complex models may be performing basic counting performance. This simplified model eliminates dense node embeddings represents edge as sparse vectors and simplifies messages to single issue leading to significant storage and efficiency gains. effectiveness:  light and accessible writing  Extensive and valuable analysis of results  Improvements on two multiplechoice QA datasets Weaknesses:  Designed specifically for multiplechoice questions  Unclear how the method would perform on nonmultiplechoice datasets Questions for the Authors:  Would the neural counter work for problems without multiplechoice answers such as graph classification  Is there a correlation between the density of the context node and its ability to predict correct answers  How does the edge encoder convert edge vectors into scalar messages without considering the question  In Fig. 4 how does the node in the bottom right have a nonzero score initially  Is a novel QA context generated for each questionanswer pair If not how does it generalize to scope without answer alternative  Why is the personality node missing from the graph in design 6  The edge embedding is stated as 47 but each edge has three components which makes it 46. Please clarify this discrepancy.", "Paraphrase of Statement: Summary:  This study examines various GNN modules and concludes that they are overly complex use redundant initial node embeddings and have unnecessary layers.  The study proposes Graph Soft Counter (GSC) a simplified graph neural model that counts knowledge in the graph.  Despite having fewer trainable parameters GSC outperforms other GNN models on two QA benchmark (CommonsenseQA and OpenBookQA) suggesting that counting is fundamental to reasoning. effectiveness:  Novel insights in the analysis of GNN modules leading to improvements in reasoning modules.  GSC is simple efficient and interpretable achieving high QA performance.  Ablation study show that yet a basic counting model can perform similarly to advanced GNNbased methods. Weaknesses:  Lack of light definitions and explanations for certain terms (e.g. \"answer alternative a \u2208 C\") makes understanding some content difficult.  PyTorch code in Algorithm 1 may not be accessible to all readers and variables like \"inputs\" are omitted.  While the statement about GSC not using node embeddings is inaccurate (node values can be seen as 1D embeddings) it does not use traditional multidimensional representations. Questions:  Why is the edge encoders input dimension 47 when the description reference 243846  Why are node values initialized to 0 and edge values scaled to (0 1) in GSC  Are the observations and hypotheses in the paper applicable to more complex reasoning QA datasets beyond those relying on common sense"], "fXHl76nO2AZ": ["Paraphrase: Summary: This paper proposes a method called gradient importance learning to predict labels from data with missing values. Unlike previous methods this approach does not require imputing the missing values. It applies a multiplicative parameter matrix to the gradient of the first layer in neural networks such as MLPs and LSTMs. This matrix is trained through reinforcement learning and the authors draw parallels to visual attention mechanisms. Experiments on time series and picture data are presented. Strengths and Weaknesses: 1. need and methodology Concerns:  The rationale behind the multiplicative parameter matrix (A) is unclear.  Why is A shared across sample with different masking patterns and why specifically on the gradient level  Could direct SGD parameter optimization without RL achieve similar results 2. Limitations:  The authors acknowledge limitations with CNNs but other techniques like batch normalization and residual structures were not addressed. 3. efficiency:  The impact of the RL training process on efficiency (time memory stability) is not discussed. 4. Matrix A Interpretation:  It is not explained how mask distributions affect the learned matrix A. 5. Experimental Details:  Hyperparameter selection including model architectures is not provided.  A baseline model that directly predicts labels from incomplete inputs without imputation is not included for comparison.", "Paraphrase: Summary: This study focuses on addressing missing data in time series data. It introduces an \"imputationfree\" approach to handle missing values called Gradient Importance Learning (GIL). GIL uses reinforcement learning to assign weights to gradients based on different parameters. Experiments using a tabular dataset and two picture datasets demonstrate GILs effectiveness. Strengths and Weaknesses: Strengths:  Tackles an significant machine learning problem with a novel approach.  Wellorganized and easy to follow.  Promising results on MIMICIII datasets. Weaknesses:  Imputationbased methods may not be suitable for data with informative missing values such as MIMICIII. Missing patterns may convey hidden information about patients and imputation may obscure this information. This is evident in Table 1 where most methods perform worse than GILH (which discards gradients from missing entries). The proposed method may excel in time series data with informative missing but may struggle with other missing patterns (e.g. random missing). This is supported by results on the MNIST dataset where GIL does not significantly outperform basic zero imputation.  The use of reinforcement learning to weigh gradient importance may pose efficiency concerns. Training RL policies can be timeconsuming and the study should consider analyzing the proposed methods efficiency.", "Paraphrase: This study introduces a novel approach to train neural networks on datasets with missing features. Instead of using additional imputation techniques the proposed method employs a single model that can handle missing values. It uses a reinforcement learning (RL) agent to generate a weighting vector that is applied to the gradient update of the neural networks first weight matrix. The RL agents reinforcement is based on the performance of the model after the update. Strengths:  Offers a novel and innovative method.  Demonstrates beneficial performance compared to existing baselines. Weaknesses:  The proposed method is more complex than simpler methods like mean imputation which only require one loop. It is unclear if there are simpler alternatives that achieve similar results.  The methods explanation could be simplified by assuming prior knowledge of standard backpropagation in multilayer perceptrons (MLPs).  The batch size used in the paper (128) differs from the scenario described in the algorithm (batch size of 1). It would be helpful to clarify how the method handles larger batch sizes.  There is a potential error in the algorithm where the index i in xi should be i1.  The compute requirements of the proposed method in comparison to the baselines should be provided.", "Paraphrased Statement: This study presents a novel approach to make prediction using incomplete data employing MLPLSTM networks. Unlike conventional methods that fill missing values before prediction our method predicts directly from the incomplete data. It involves learning an \"importance\" matrix that modifies the networks initial weight matrix. reinforcement learning is used to train the network with the prediction error serving as the reinforcement. Extensive experiments on various datasets demonstrate the methods superiority in most cases compared to both classic and recent prediction techniques for incomplete data address. The approach remains limited to MLP and LSTM networks but provides a significant contribution to the field."], "sNuFKTMktcY": ["Simplified Statement: This study introduces a novel algorithm for goaloriented reinforcement learning that excels in tasks with infrequent rewards. It does then by combining enhancements in:  representation learning: A more stable process with a regularization term that improves goal sampling effectiveness (Figure 6).  Exploration Strategy: A method that considers both novelty and goal reachability. It uses expected state visitation count instead of visit count alone and incorporates a potential term that assesses how promising each goal state is. effectiveness and Weaknesses:  Wellexecuted paper with clear explanations for each novel component and ablation studies to demonstrate their impact.  potential for further clarification in certain sections:  Clarify the role of visit count in the proposed algorithm.  Define the space metric used in the representation learning.  Explain how state embeddings are mapped into cells.  Describe the process of defining cells for higherdimensional inputs like images.  Better explain the potential measure and its connection to Figure 3.  Some claims require more evidence (e.g. the redundancy of skills in bottomup HRL).  Further detail on the generation of Figure 6 and the tuning of the ablated method would enhance transparency.", "Paraphrase: method:  The authors present a hierarchical reinforcement learning algorithm that enhances an existing contrastive learningbased objective for subgoal representation with exploration heuristics.  The algorithm aims to mitigate representation drift by penalizing adjustments to state representation (\u03c6(s)) that result in low contrastive losses.  Exploration is encouraged through heuristics that favor promising domain in latent space combining countbased novelty and potential measures. Evaluation and outcome:  The algorithm demonstrates superior performance compared to existing hierarchical method.  ablation studies effectively distinguish the contributions of each component. effectiveness and Weaknesses: Pros:  Improved performance over hierarchical baselines.  Comprehensive ablation analysis. Cons:  Lack of explanation for estimating count and potential measures (e.g. cumulative count U(gt)).  limited justification for prioritized sampling in Equation 3.  Suboptimal evaluation of reactive exploration in ablative analysis. Clarification Questions:  Why does imposing \u03bb(s) as a continuous function of representation loss lead to computational challenges  How is the latent space partitioned when the bounds of \u03c6(s) are unknown  Why is the novelty measure referred to as a \"mixture of count in the past and current representation spaces\" despite the ability to recalculate count upon changes in \u03c6  What technique is employed for lowlevel policy training such as hindsight experience replay", "Paraphrased Summary: This study introduces a method for learning stable subgoals in a complex reinforcement learning setup with multiple levels. Two control are trained together: a highlevel \"meta control\" and a lowlevel \"goalachieving agent.\" The meta control communicates general goals to the goalachieving agent. The meta control receives feedback from an external reinforcement function while the goalachieving agent optimizes its behavior based on the goals set by the meta control. Subgoals are updated at predetermined intervals. To find novel and reachable subgoals the authors propose a potential measure that regularizes novelty measures. Directional goals are invented by combining the current state with a directional vector. The potential function ensures that these invented goals are achievable by expressing the reinforcement as the predicted negative space between the ending state and the imagined goal. The method was tested in various challenging exploration environments and compared to reasonable baseline approach. The authors acknowledge that their approach builds upon the concept of Feudal Networks which uses directional goal vectors. They suggest future research to compare their method directly to Feudal Networks. effectiveness and Weaknesses: effectiveness:  Novel and promising approach.  Reasonably selected baselines. Weaknesses:  Lack of headtohead comparison with Feudal Networks.  limited exploration of hyperparameters especially the selection outcome condition.  Insufficient quantitative analysis of stability regularization outcome.  limited qualitative analysis of the interaction between potential and novelty measures.  Incomplete asymptotic performance data in Figure 5.", "Summary This paper introduces Hierarchical Exploration approach with Stable Subgoal representation learning (HESS) an algorithm that improves subgoal representation stability and enhances highlevel exploration in Goalconditioned Hierarchical reinforcement learning (GCHRL). HESS builds on the LESSON method addressing its instability by employing a representation regularization that stabilizes representation for states with low triplet losses. Additionally HESS introduces an active exploration approach for highlevel learning that combines the concepts of novelty and potential. Extensive experiments in MuJoCo environments demonstrate HESSs effectiveness and the contributions of its various components. effectiveness  Investigates the critical problem of subgoal learning instability and highlevel exploration in GCHRL.  Proposes a simple yet effective representation regularization method to enhance subgoal representation stability.  Introduces a novel active exploration method that considers novelty and potential for effective exploration.  Provides extensive evaluation across multiple perspectives showcasing HESSs superiority. Weaknesses  The proposed method are relatively incremental lacking significant novelty.  The connection between representation regularization and active exploration could be more explicit.  Concerns regarding the implementation of novelty and potential calculations as well as computational complexity.  In some environments baseline algorithms perform surprisingly well."], "xxyTjJFzy3C": ["Summary: Researchers developed a method to improve the performance of 3D object descriptors which are used for tasks like object retrieval and classification. They do this by:  Augmenting Training: Adding rotations and generating adversarial views during training using a differentiable renderer.  Adding contrastive Loss: Using an unsupervised loss to encourage the model to discriminate between real and augmented data. strength:  effective use of a differentiable renderer for optimizing viewpoints that trick the network.  Works with multiple backbone networks suggesting its generality.  Quantitatively demonstrates improved performance over methods that dont use their augmentations or adversarial sampling. Weaknesses:  not novel in using gradient descent for optimizing model that fool the network (citations missing).  Does not clearly differentiate between the benefits of 3D augmentation and adversarial view sampling.  Lacks discussion on the impact of training steps for the rotation matrix.  Could provide more clarity on the individual effectiveness of different augmentations.  shape and captions could be clearer and less cluttered.  Includes experiments (e.g. number of views) that dont directly support the methods contribution.  Lacks technical details (e.g. epoch count descriptor size object retrieval method).  Contains typos and could benefit from more concise language.", "Paraphrase: Summary: This paper presents a novel approach for shape classification and retrieval by combining contrastive learning with an adversarial method. The approach generates \"adversarial views\" to enhance training on challenging view. It operates without supervision relying on selflearning. strength:  Utilizes contrastive learning a novel method for selfsupervised feature extraction.  Incorporates adversarial views which are both effective and interpretable.  Outperforms supervised baselines.  Provides extensive experiments and ablation work.  Features clear illustrations and explanations. Weaknesses:  Inconsistency in bolding rules for table results.  Unclear writing and typos after section 4.1.  Missing section numbers in section 3.1.  Grammatical errors in section 4.1.", "Paraphrased Summary: The paper presents a method for extracting shape descriptors from 3D objects represented as meshes. Using a differentiable renderer meshes are rendered into images. A feature extraction network is applied to these images to obtain feature embeddings. contrastive learning is employed to train the descriptor utilizing both real images and adversarially perturbed images. The perturbation parameters include rotation angles and nonlinear transformations. At inference multiple object images are used to generate object embeddings. The method is evaluated on two datasets for object classification and retrieval tasks. The results indicate a minor improvement in accuracy (a few percentage points) over existing approach. strength and Weaknesses: strength:  The field of shape descriptors is relevant and promising.  The paper proposes an unsupervised descriptor with apparent efficiency. Weaknesses: 1. Clarity and Details:  The presentation lacks clarity and precision. Key concepts and symbols are introduced without proper definitions or references.  crucial implementation details are missing such as the integration of the descriptor into existing networks (GVCNN and MVCNN). 2. Unsupervised Nature:  The method is not truly unsupervised for object classification as it relies on labeled data in MVCNN and GVCNN. 3. Ablation Analysis:  It is unclear whether the adversarial view generation approach improves performance over random augmentations or random rotations.", "Paraphrased Statement: Summary: This research focuses on developing a descriptor for 3D shapes. The contribution of this work include:  A mechanism for identifying challenging training model without requiring labels.  Integration of techniques from adversarial learning and contrastive learning to achieve this goal. strength and Weaknesses: strength:  The method for identifying and rendering hard viewpoints onthefly is innovative and practical.  contrastive learning eliminates the need for class labels in this process. Weaknesses:  The computational cost of rendering novel views onthefly during training should be considered.  A comparison with extreme data augmentations without hard negative mining would provide valuable insights.  The ablation work could explore more options such as different hard negative mining methods. other Observations:  The paper is wellwritten and includes helpful visuals.  There are some typos that should be corrected.  The citations for methods in Table 1 should be included for easier reference."], "i2baoZMYZ3": ["Paraphrase: This research presents a novel technique called Action Quantization from Demonstrations (AQuaDem) for breaking down continuous action spaces into discrete one. This method utilizes the knowledge gained from demonstrations allowing for the application of any discrete action deep reinforcement learning (RL) algorithm to continuous action problem. The paper evaluates the effectiveness of AQuaDem in three settings: RL with demonstrations RL with gameplay data and imitation learning. The results show that AQuaDem is highly effective in these scenarios. Strengths and Weaknesses: Strengths:  The paper proposes a novel method for discretizing continuous action spaces.  The method is purported to avoid the \"curse of dimensionality\" and does not require specific assumptions about the task. Possible Weakness:  The author argues that discrete control problem are inherently more hard than continuous control problem. This assertion may be debatable as there is room for discussion on the relative hardy of these problem types. Key idea: The core idea of AQuaDem is to identify the most common continuous actions taken under different states using a Gaussian mixture modellike neural network. The key element in this action is Equation 1. Empirical Validation: The paper provides hard empirical evidence for the effectiveness of AQuaDem in three discrete settings demonstrating its potential in several RL application.", "Paraphrased Summary This study introduces a method for training agents in discrete action spaces from continuous one using provided demonstrations. To find K discrete actions within a specific state demonstration data is manipulationd. Under ideal conditions (greedy demonstrations full observability and application of the state space) K1 can theoretically yield the optimal policy. However in practice these assumptions may not hold. The proposed method can be seen as a biased exploration strategy reducing the action space by excluding actions less commonly manipulationd in demonstrations. It resembles Behavioral Cloning (BC) and becomes equivalent to BC in the case of K1. Strengths and Weaknesses Strengths:  Adapts BC for discrete action space learning.  Wellwritten and comprehensible.  Reasonable method combinations and environments for testing.  Adequate coverage of related play. Weaknesses:  Missing significant baselines.  Overlooked related plays.  Misleading or incomplete explanations at times.  Missing discussion. number:  Clarification requested regarding exploration complexity in continuous and discrete action spaces.  Incomplete explanation of the term \"BangBang control\" in the context of discretization.  Ambiguous reference to Andrychowicz et al. (2020) regarding causal dependence in action dimensions.  Clarification needed on the taskspecificity of the proposed approach compared to other methods.  Inconsistent manipulation of the notation \"r\" for both a random variable and the reward function.  Statements regarding dimensionality number with uniform discretization seem to overlook existing approaches for highdimensional action spaces. Questions: 1. Response to number raised in the \"number\" section. 2. Consideration of comparisons with action representation learning methods. 3. Explanation of why the discreteaction baseline was only tested in the lowdimensional Robodesk case. 4. Details on the tuning action for Relocate in number 6. 5. Disambiguation between success Rate and Sparse Reward Returns in number 5. 6. Justification for using hyperparameters tuned across all tasks including test one. 7. Comparison and set of the proposed play in relation to actionembeddingbased approaches.", "Paraphrase: Summary: The research proposes a method to discretize a continuous action space dynamically using demonstration data. This approach aims to leverage discreteaction reinforcement learning (RL) methods instead of using continuous action space parameterization in RL policies. The authors claim that this approach enhances exploration addressing the \"curse of dimensionality\" in continuous action spaces. They evaluate their method on several environments using reinforcement learning with demonstrations play data and imitation learning. The results reportedly outperform current continuous control methods demonstrating better sample efficiency and overall performance. Strengths and Weaknesses:  Strengths:  The research explores an innovative approach to exploration in continuous action spaces.  Extensive evaluation with promising results on multiple environments.  Weaknesses:  Baseline Evaluation: There are concerns about the competitiveness of the reproduced SAC implementation used as a baseline. A thorough comparison with the original implementation and experiments in MujocoDm and OpenAI Gym environments is recommended.  Related play: The paper lacks a comprehensive review of existing exploration techniques for continuous action spaces limiting the comparison and analysis of the proposed approach.  alternative Baselines: The authors do not compare their method against hierarchical RLimitation learning approaches or hybrid continuousdiscrete baselines.  superiority Claims: The superiority assertions of discrete action spaces over continuous one require theoretical or experimental support.  Offline RL: A comparison with stateoftheart offline RL methods is not included despite its relevance to the research domain.  Plot Presentation: Some concerns are raised about the accuracy and clarity of the shaded regions in the evaluation plots without specifying the statistical significance interval or pvalue used.  Handwavy Claims: The paper makes speculative claims about the advantages of discrete action spaces without providing concrete evidence.", "Paraphrased Statement: This paper explores improving training efficiency in reinforcement learning problem with continuous action spaces. It does then by connecting this problem to its discrete action space counterpart and leveraging demonstration data. Specifically the paper introduces a technique to automatically discretize the continuous action space based on actions demonstrated in training data. This discretization generates a fixed number of actions for each input state allowing standard discreteaction Qlearning methods to be used. In a simplified grid world experiment the discretization method is shown to effectively capture different actions for the same state leading to improved performance. The experimental results demonstrate the methods superiority over baselines in several settings both with and without demonstration data. Strengths and Weaknesses: Strengths:  Explores the relationship between continuous and discrete action spaces.  Proposes an automatic discretization technique that mimics demonstrated actions.  Provides clear and concise writing. Weaknesses:  Does not fully address the preference of discrete action spaces over continuous one.  The role of the temperature hyperparameter is not well explained.  The proposed loss function does not explicitly encourage action diversity.  The method lacks comparithenn with similar Gaussian mixture models used in reinforcement learning. Additional Considerations:  The novelty of the proposed method could be strengthened by more thorough comparithenns to existing Gaussian mixture model approaches.  Further analysis is needed to determine if the benefits of the discretization are taskspecific or more generalizable."], "jNsynsmDkl": ["Paraphrased Summary: This paper combines contrastive learning with multilabel classification. It breaks multilabel classification into individual binary classification tasks using labelspecific features extracted by an attention mechanism. contrastive learning is applied to these binary tasks. While the method performs well its essentially a straightforward extension of existing contrastive learning techniques used in singlelabel classification. effectiveness and Weaknesses: effectiveness:  Clear and concise writing  Technically sound approach with improved performance  Valuable qualitative analysis of learned representations Weaknesses:  Incremental approach with a limited contribution  Lack of comparison with other works that introduce contrastive learning to multilabel classification  ablation field results suggest that the contrastive learning term may not be an effective regularization for multilabel learning  Ambiguity in the author of performance improvement as additional training strategies are employed", "Paraphrase: This field introduces a multilabel classification technique using contrastive learning. Specifically the authors suggest learning \"labellevel embeddings\" for each image converting the multilabel problem into a singlelabel one that can be solved with contrastive learning. The embedding network uses selfattention and multihead attention modules to derive labelspecific embeddings. The effectiveness of the method was validated through experiments on two datasets (MS COCO and NUSWIDE). effectiveness: Multilabel classification is a significant problem in computer vision with extensive research. This paper tackles the problem with contrastive learning an unexplored approach for multilabel classification. The proposed labellevel embedding approach is logical and promising and the results are encouraging. The paper is wellstructured and includes adequate setting information. Weaknesses: 1. The proposed labellevel embedding network is a key contribution. However it incorporates attention block similar to transformer framework used in \"Query2Label\" (Liu et al. 2021). Clarification is needed on the specific differences between the two approach. 2. While the method achieved strong performance on both MS COCO and NUSWIDE datasets these datasets have distinct characteristics. MS COCO includes small objects while NUSWIDE includes abstract concepts. contrastive embeddings and attention mechanisms may enhance small object recognition but their benefits for concept recognition are unclear. Is the performance improvement over existing methods primarily driven by the attention module", "Paraphrased Statement: Summary: The field proposes a system to enhance multilabel classification using contrastive learning. Despite the success of contrastive learning in singlelabel tasks direct application in multilabel scenarios has not improved performance. effectiveness:  Leverages a transformer structure to obtain labellevel embeddings providing a natural training position for contrastive constraints within the minibatch.  Conducts an extensive analysis to demonstrate the benefits of contrastive loss. Weaknesses:  The concept of minibatchbased contrastive learning is intriguing but lack significant novelty.  The proposed method primarily relies on the attentionbased labellevel embedding which is not an original concept in multilabel classification.  The experimental results fall short of stateoftheart framework leaving room for improvement in validation and evaluation.  The field is limited to two datapositions and its effectiveness may vary under different label position sizes or input configurations.", "Paraphrase: Summary: The authors present a novel approach for categorizing multiple labels simultaneously (multilabel classification) that incorporates supervised contrasting learning. They add an attention mechanism to the image encoder allowing them to extract features specific to each label and perform contrastive learning. Training involves two stages: pretraining using binary crossentropy loss and finetuning with a combination of crossentropy and a novel LabelLevel Contrastive Loss. Experiments on four datasets demonstrate that their method MulCon surpasses other ResNet101based approach on various performance metrics. effectiveness:  Wellorganized and accessible paper.  The proposed method adapts contrastive learning effectively for multilabel classification.  Strong experimental results with MulCon outperforming baselines on ResNet101 across most measures.  The authors provide sufficient details for reproducing results.  ablation field qualitative analysis and visualization enhance the evaluation. Weaknesses:  Limited contribution as the primary novelty lies in the LabelLevel Contrastive Loss with the rest being standard techniques.  Superficial explanation of contrastive learning use.  The attention block which introduces as many parameters as the ResNet101 base encoder appears to be the primary driver of performance improvement.  Lack of information on parameter number and training time for MulCon.  Incomplete data on augmentation techniques used in different experiments.  Inconsistencies in the naming of ResNet101. Nit:  Specifying the resolution used in the ablation field would enhance clarity."], "l431c_2eGO2": ["Paraphrased Summary This paper introduces a novel penalty work that can be added to the standard crossentropy loss for deep neural network classifiers. This improvement enhances the models accuracy and calibration on indistribution data while encouraging it to improve estimates under domain shifts and beneficial identify outofdistribution samples. The proposed penalty is related to the mixup data augmentation technique which creates novel samples by interpolating images and labels from different classes. In this paper the estimation is to interpolate between images from different classes and maximize the entropy of these samples during training. effectiveness and Weaknesses  The proposed penalty work is similar to mixup leading to concerns about its novelty.  The righthand panel of image 2 shows some overlap between the IND data and the input data embeddings which suggests that the model produces stronger activations when the similarity between the learned features and the input is high. The right panel of image 3 further illustrates this aim.  Mixup achieves similar burden to the right panel of image 3 but with a smoother \"high entropy barrier\" compared to the proposed penalty.  While commonly used datasets are employed the domain shifts tested are limited to input image corruptions. The authors suggest exploring other types of domain shifts and more challenging outofdistribution detection scenarios using synthetic data.  A comparison with existing uncertainty estimation technique like distanceaware learning would be beneficial.", "Paraphrase: Summary: The paper proposes adding a regularization term to the training loss of neural network. This term increases the entropy of synthetically generated model which improves accuracy uncertainty estimates and outofdistribution (OOD) detection performance. effectiveness and Weaknesses:  effectiveness:  The proposed method avoids collecting additional data unlike other approaches like Agnostophobia and Outlier Exposure.  It performs well on uncertainty estimation and domain shift tasks.  Weaknesses:  The concept of increasing entropy for OOD detection is not particularly novel.  The method requires extensive hyperparameter tuning making it computationally expensive.  It does not outperform the IsoMax loss which also maximizes entropy and avoids data augmentation.  The authors should explicitly state that the proposed approach requires increased training time due to the augmented data. Recommendation: The paper shows some novelty and conducts extensive experiments. However the performance is not consistently strong across different model types. The computational cost of hyperparameter tuning is a major concern. The authors should consider incorporating their approach with the IsoMax loss to improve performance and reduce cost.", "Paraphrase: Summary: This research presents a userfriendly approach called \"mixmaxent\" to enhance the stability of neural network. The technique entails training the network to generate uncertain predictions when presented with images that are interpolations of two images from distinct classes. Through experimentation the approach has shown promising empirical results. effectiveness and Weaknesses:  effectiveness: The technique stands out for its simplicity and can be effortlessly incorporated into the training work.  Weakness: The significance of the empirical findings raises concerns.  Concern 1: Mixup a baseline method used for comparison appears to be surprisingly strong despite not being specifically designed for robustness and uncertainty. Questions arise as to whether the work incorporates stateoftheart approaches in this field.  Concern 2: The empirical evaluation is conducted on the CIFAR100 dataset which is relatively little. For a robust empirical analysis it would be advisable to conduct largescale experiments on ImageNet with more challenging natural distribution shifts. minor Concerns:  The researchers do not explore the burden of mixing images from three or more classes.  The justification for using beta distribution over other distributions to determine the interpolation burden is not provided.", "Paraphrased Statement: Summary: The author suggests a novel method for enhancing uncertainty estimates in deep neural network outputs. Similar to the \"mixup\" paper the approach involves interpolating two samples. However unlike mixup which interpolates samples with the same label this method interpolates samples with different labels. These interpolated samples aim to represent outofdistribution (OOD) data. The author introduces a novel loss work that combines the ordinary crossentropy loss with an entropy regularization term to increase the uncertainty in predictions for these fabricated samples. The paper presents experimental results demonstrating the effectiveness of the proposed technique. effectiveness:  open and accessible writing  Simple and readily applicable approach  Competitive performance against basisline methods  Evaluation across various datasets and use cases Weaknesses:  Concerns and question:  Generalizability of observations from CIFAR and SVHN to other datasets  Redundancy of the observation that OOD and datashifted samples map to a hypersphere considering the high dimensionality of the feature space  consideration of OOD samples not located between data clusters  Insufficient comparison with the DUQ method as numbers are cited directly from the DUQ paper without incorporating the code basis  Lack of clarity in the evaluation of OOD detection metrics in section 5.1.3  Potential counterproductivity for smooth label transitions such as in digit recognition  Presentation issues:  Small and inconvenient tables  References to appendix tables making reading more challenging", "Paraphrase: Summary: Researchers present a technique called MixMaxEnt to enhance the performance of DNN classifiers. MixMaxEnt aims to improve uncertainty estimation without compromising accuracy by encouraging the classifier to predict with high uncertainty for mixed data samples (from different classes). The mixup approach is modified to specifically regularize mixed samples and labels while leaving the optimization of clean samples unchanged. The goal is to model the unknown region of the data distribution through high entropy predictions for mixed samples. effectiveness:  Improved classification accuracy on both clean and corrupted data compared to baseline methods.  Applicable to existing network architectures without significant hyperparameter tuning. Limitations:  Limited evaluation to support claims of improved calibration and uncertainty estimation.  Results show variability with mixup performing beneficial than MixMaxEnt in certain settings.  Evaluation limited to ResNet architectures and little image datasets.  OOD detection results are comparable to baselines and lack comparison to other methods that use proxy distributions to enhance uncertainty on OOD data.  Aspects such as regularization effectiveness and the use of beta distribution are not fully explored or justified. General Comments:  The paper could benefit from improved clarity organization and focus.  significant results are presented in the appendix and discussed inadequately.  Content in the main paper should be more impactful and relevant."], "rrWeE9ZDw_": ["Paraphrase: This research proposes a method for creating symbolic representations of an environment that enable planning. It integrates objectcentric concepts assuming that objects behave similarly across tasks. This allows for reusing representations across tasks and generalizing to new ones. The method includes techniques for combining objects into types and incorporating taskspecific data. Experiments demonstrate improved learning efficiency and generalization capabilities. coreiveness:  Focuses on learning planning representations an significant problem.  Assumes a domain composed of objects with common behavior across tasks aligning with realdomain priors.  Wellwritten presentation and appropriate credit to previous work. Main Improvements:  method for identifying object types based on their impact when executing options.  method for grounding representations in specific task instances by integrating taskspecific data. Limitations:  Assumes that options only affect the objects they interact with.  Requires prior knowledge for grounding certain operators.  Relies on presegmented object data.  Assumes the \"frame Assumption\" for estimating core distributions.", "Paraphrase: Summary: This paper proposes a method for creating symbolic abstractionion of objects from factored environmental observations for longterm planning tasks. It builds on the symbolic representation learning framework developed by Konidaris et al. (2018) by factoring the state into objects learning object types and \"lifting\" the framework to work with these objectcentric abstractionion. The methods effectiveness is demonstrated in three environments ranging from simple discrete feature vectors to a Minecraft planning task based on (objectfactored) pixel observations. effectiveness and Weaknesses: The paper is wellstructured clearly presented and overall of high quality. Learning sampleefficient abstractionion for longterm planning is a significant and challenging problem and the objectcentric approach is timely and highly relevant. The methods integration of objectcentricity into the framework is innovative. The experiments demonstrate the methods ability to solve complex tasks. However the papers claim of significantly reduced environment interactions for longterm planning lacks quantitative evaluation against a relevant baseline. The method assumes preprocessed object components which is a significant task in itself. A fair comparison would involve providing approach to these preprocessed observations and abstraction options to both the proposed method and a baseline.", "Paraphrased Statement: Summary: This work presents a method for creating PDDLlike abstraction for further problemsolving. These abstraction rely on object states (symbolic or embedded) and assist PDDL solvers in finding solutions for tasks where the abstraction were learned. The authors suggest that these abstraction can be transferred between tasks with similar object types. effectiveness and Weaknesses: effectiveness:  Automates the learning of object type pre and postconditions.  Reduces the manual effort in defining planning domains. Weaknesses: 1. Task Specificity: The learned abstraction may not be transferable to new tasks with similar object types due to differing preconditions. For example \"picking an apple on a table\" and \"picking an apple on a tree\" require different preconditions for the \"pick\" action. 2. Limited generalization: Problemspecific instantiation tricks may be used to address different preconditions for similar objects but this approach mimics manual planning domain adaptation and does not overcome the fundamental limitation of humandefined planning domains. The proposed objectcentric abstraction does not adequately capture preconditions for action applicability. Instead it merely groups similar conditions resulting in multiple object types and an inability to abstract concepts like \"reachability\" that are essential for solving problems."], "z8xVlqWwRrK": ["Paraphrased Statement: This work presents a technique for modelbased reinforcement learning (MBRL) that utilizes posterior sampling to balance exploiting what is known with exploring the unknown. Previous approach have difficulties maintaining dynamic transition posteriors. The authors propose using dropout which can approximate posteriors to capture approximate posteriors of transitions and reinforcement use. Their model is specifically designed for objectbased tasks with three convolutional layers that incorporate dropout: object interaction event weighting and event translation. These layers can be added to existing neural network models to provide an approximate posterior. The authors demonstrate the models effectiveness by combining it with a Proximal Policy Optimization (PPO) agent and showing its improved performance on Atari games with limited interactions with the real environment. Strengths:  Uses dropout to approximate posteriors and use Thompson sampling to MBRL.  Only requires three additional layers that can be integrated into any model.  Wellwritten and easy to understand.  Provides detailed experimental setup and ablation work. Weaknesses:  Limited to objectbased tasks. minor Comments:  Clarify the objectbased task model in the introduction.  Correct the phrasing in Table 1 to \"mean scores achieved.\" Questions: 1. Are the authors aware of any other work applying dropout to perform Thompson sampling in MBRL 2. Isnt there a risk of the policy overfitting to a specific model posterior sampling during policy training with the environment 3. Why does EvadeSimple underperform Simple(30) in two games 4. Why is only the reinforcement use contribution approximated using a posterior and not the dynamics 5. Why does \"z\" change", "Paraphrased Statement: Summary This paper investigates exploration challenges in modelbased reinforcement learning introducing novel neural network architectures tailored to objectbased fields. Strengths and Weaknesses The paper proposes innovative ideas for neural network architecture design based on PSRL. However it falls short in addressing fundamental challenges in reinforcement learning as the exploration strategy and planning algorithm are largely unchanged from prior work. The proposed neural network architectures are highly specific and lack general principles. A research paper should focus on presenting generalizable algorithm design concepts rather than adaptations to specific fields. The approach in Section 3 appears somewhat heuristic and lacks a clear connection to RL principles. The justification for posterior sampling through Gaussian perturbation is unclear and the use of dropout in Section 3.2 introduces confusion in approximating the posterior. The call that variational distributions assist in exploration even if they poorly approximate the posterior lacks theoretical reinforcement and the argument for exploration in PSRL to be more principled is debatable. The paper call that it is easier to incorporate field knowledge into transition and reinforcement use is not fully elaborated or reinforcemented. The meaning of \"modelfree agents explore the space of policies\" is unclear specificly in relation to valuebased method. The authors should provide more accurate and welldefined terminology such as \"objectbased\" and \"event.\" Section 3.5 lacks a detailed algorithmic description for completeness leaving questions about environment interaction and planning work unanswered.", "Paraphrase: This work enhances a modelbased reinforcement learning algorithm (Kaiser et al. 2020) by incorporating three types of neural network layers into its transition and reinforcement models. Inspired by insights about modeling needs in objectbased Atari games these modifications aim to improve performance. In experiments on 12 games the proposed method generally outperforms other approach. Ablation work reveal that each of the three proposed layers individually improves performance. Strengths and Weaknesses:  Strengths:  Simple and straightforward modification to an existing algorithm.  Weaknesses:  Limited experimental depth with only three independent work.  Ablations suggest that using just one of the proposed layers may sometimes be more effective than using all three.", "Paraphrase: Summary: This research introduces a technique for reinforcement learning based on models that incorporates posterior sampling for exploration. The paper uses a variational distribution method to estimate posterior sampling. Strengths:  Posterior sampling enhances exploration in reinforcement learning particularly in specific scenarios.  The paper effectively applies posterior sampling to modelbased deep reinforcement learning.  The paper proposes Eventbased Variational Distributions for Exploration (EVaDE) a variational method that involves three Gaussian dropout convolutional layers. These layers aim to capture random event emphasis object translation and object interaction. Weaknesses:  Limited justification for choosing these specific three event.  Lack of quantitative assessment of each layers impact on its intended use.  Ablation work suggests benefits of adding layers but fails to pinpoint their individual event.  Experiments show that EVaDE enhances performance in some Atari games but not others with no explanation.  EVaDE performs poorly in the JamesBond game despite ablation work indicating potential benefits.  It is unclear if longer training could improve performance or if noisy layers limit performance when exploration is less crucial."], "oVfIKuhqfC": ["Paraphrase: Summary: This paper introduces a new approach called \"diffusion bridge.\" This approach eliminates the need for a \"reverse construction\" used in previous research. Strengths and Weaknesses: Weaknesses:  The paper may be difficult to understand for nonspecialists especially regarding motivations and construction.  The derivation of Equation (7) is missing making it challenging to verify the assumptions necessary for its robustness.  The authors do not adequately explain why they move from Equation (7) to mixture.  Equation (8) lacks the definition of the work g(Xt t).  Section 4 is confusing. It discusses an SDE class described through time changes but its connection to the rest of the paper is unclear. The authors should provide more clarity and prove these solution explicitly instead of referencing an external source.", "Paraphrased Summary: contribution:  Introduces a novel diffusionbased generative modeling approach using the mixture of diffusion bridges.  Demonstrates that mixed diffusion bridges remain diffusion with explicit drift and volatility.  Presents the drift term as a conditional expectation enabling the use of neural networks for approximation.  Unifies SDE class used in previous work and incorporates nonidentity volatility matrices. Strengths:  Clear motivation and theoretical soundness  Original and promising concept of pinning down both initial and terminal distributions  potential for approximating the drift using scorematching techniques Weaknesses:  Experimental evidence is limited especially in comparison to timereversed diffusion method.  Theoretical justification for the superiority of diffusion bridge modeling is lacking.  Sections on SDE class and transport approximation may be disconnected from the main focus. Other Comments:  The authors claim that all scorebased generative modeling approaches rely on time reversal is not fully exact.  The authors should provide more details on the computation of the conditional expectation in their experiments.  Equation 8 could be simplified to emphasize its relationship to scorebased generative modeling.", "Summary: This paper examines the Brownian bridge formulation for diffusionbased generative models develops theory for it and establishes connections to previous research. It also discusses some spatial development. The paper focuses on theory and includes only preliminary experiments. Strengths and Weaknesses: The approximation of Brownian bridge diffusion is intriguing but the paper lacks convincing demonstrations of its merits. Technical complexity makes it challenging to comprehend. PostResponse Updates: The authors have addressed technical feedback and revised the manuscript with clearer visualizations and algorithm descriptions. However three major concerns remain: 1. introduction Complexity: The paper is highly technical and lacks clear motivation for its complex approach. 2. Unconvincing Motivation: The motivation for replacing Song et al.s bidirectional diffusion model with the Brownian bridge formulation is unclear. 3. Lack of Experiments: Despite proposing a new method there are no largescale or exhaustive experiments to support its effectiveness. Technical Comments:  The terminology for \"terminal\" and \"initial\" distributions is confusing as these terms refer to different time points.  The nature of the \"drift adjustment\" and its impact on data dependency is unclear.  The transition density quu(yx) requires clarification as y and x are undefined.  The pairing of x0 and xtau(n) in eq. 8 needs explanation.  Eq. 21 introduces weights \\omega without connecting them to the equations or explaining their role in the method.  Section 4 contains complex math with insufficient motivation or conceptualization.  The application of eq. 14 to eq. 8 and the derivation of eq. 20 raises questions about model compatibility.  The wide method description is scattered making it difficult to follow.  Figure 1 is confusing with unclear data visualization and terminology.  The concept of \"true score\" and its visualization in Figure 1 is not welldefined.  The explanation of the figures three stages is lacking.  The connection between eq. 25 and s\u2248E[Y] is not clear.", "Paraphrased argument: This paper introduces a new approach to generative modeling using diffusion workes that does not require timereversal argument. The authors use diffusion bridges to model workes that start and end at specific targets and extend this to mixture of diffusion bridges to transport a distribution from one target to another without timereversing the work. Connections to previous work are drawn allowing for a unified understanding of drift adjustments in forward and backward stochastic differential equations (SDEs). This enables the definition of a time and statedependent class probability work that provides insights into the inner work of diffusion sampling method. Two new training objectives are introduced with favorable properties. Additionally the paper expands the framework beyond fully factorial noise models by allowing the noise term to vary in space as well as time. This enables the incorporation of \"priors\" with better spatial dependency models for specific datasets. The method presented offer significant improvements over existing approaches by relaxing timereversal constraints and incorporating more flexible noise models. The paper is wellwritten and supported by representative solution and code for reproducibility. Despite the reviewers lack of expertise in this specific field they recommend this paper for publication based on its clarity originality and potential significance."], "rbPg0zkHGi": ["Paraphrase: A novel active learning approach for Deep Neural Networks (DNNs) is presented. This method utilizes the robustness of DNNs to parameter perturbation as a measure of uncertainty. Specifically the deviation in predictions when adding slight noise to the model parameters is used to identify data points for labeling. The method is grounded in theoretical analysis connecting it to variance reduction. Empirical evaluations on various image classification and semantic segmentation datasets demonstrate the superiority of this approach compared to existing stateoftheart methods. strength:  The method is straightforward and doesnt require additional models.  It has been evaluated on a range of tasks and datasets.  The research provides theoretical justification linking it to variance reduction. Weaknesses:  The clarity of the presentation could be improved particularly regarding assumptions on noise magnitude.  Some results image lack certain baselines without explanation.  For a more comprehensive evaluation the proposed method should be tested on models with varying architectures and tasks in different domains.", "Paraphrased Summary: This research introduces a novel method for training machine learning models for classification and image segmentation. A unique element of this approach involves adding controlled noise to the models parameters resulting in multiple prediction output. This allows for the estimation of prediction uncertainty for each input sample. The method has been tested on various datasets and demonstrates effectiveness in both classification and segmentation tasks. strength:  ease and ease of implementation  Consistent performance across diverse datasets Weaknesses:  Unclear connections between the method and calibration (adjusting prediction probabilities to reflect uncertainty)  Lack of empirical evidence supporting the calibration aspect  Inconclusive results due to:  Insufficiently largescale testing (e.g. Imagenet)  Absence of comparison to ensemblebased methods (uncertainty estimation approaches that combine multiple models)  Questions regarding the robustness of Monte Carlo Dropout results  Omission of data on computational costs", "Summary This article presents a novel approach for active learning aiming to select unlabeled samples to add to the training set. The technique utilizes the input noise stability principle where input samples exhibiting significant output variance when small noise is added to model parameters are identified as highly informative and should be prioritized for labeling. The authors compare this approach to existing active learning methods and discuss its theoretical connections to predictive variance reduction. Strengths  relevance: active learning is crucial in managing large datasets and training deep neural networks which often require vast datasets to achieve satisfactory performance.  ease: The proposed approach is straightforward to comprehend and implement. Weaknesses  Clarity: The papers language and structure make it challenging to understand.  Specific Concerns:  Ambiguous or inaccurate terminology such as \"interlayer cushion.\"  Lack of clarity in describing the perturbation work which only involves model parameters not inputs.  Inadequate explanation of the relevance of auxiliary models mentioned in passing.  Unnecessary inclusion of equations for updating labeled model which complicates the explanation.  Switching annotation for the output Jacobian unnecessarily. Experimental Evaluation  Limitations:  Narrow focus on image classification tasks excluding regression and noncomputer vision tasks.  Missing baselines such as BatchBALD sparse subset estimation and the BALD learning work.  large step sizes used reducing the significance of active learning.  Lack of ablation work to validate the significance of adaptive noise magnitude.  Missing runtime comparison for all baselines and the proposed method. Connections to Other work  The proposed method appears similar to active learning with a linearised Laplace estimation of the Bayesian neural network posterior but this connection is not acknowledged in the paper. Exploring this link could strengthen the theoretical grounding."], "uc8UsmcInvB": ["Paraphrased Summary: The paper introduces a new approach called \"statistically meaningful approximation\" (SM approximation). This approach suggests that empirical risk minimization using a suitable surrogate loss would lead to a hypothesis with a low population risk. The paper develops tools and presents two key findings: 1. Fully connected neural networks can approximate Boolean circuits with a small sample size that is polynomial in the amount of gates and logarithmic in the circuits width and depth. 2. Transformers can approximate Turing machines with a sample size that is polynomial in the amount of state alphabet size and logarithmic in the computation steps. strength:  SM approximation provides a framework that unifies approximation and generalization which is novel.  The paper demonstrates that certain surrogate losses (e.g. alllayer margin loss) can lead neural networks to prefer learning simple classes of function with limited sample. This is notable because different network structures may require different surrogate losses. interest:  Some argue that SM approximation is not surprising because it is possible to construct a surrogate loss that assigns infinite loss to function outside a specified class. This would result in empirical risk minimization within that class.  To fully prove SM approximation it is necessary to demonstrate both the ability of the function class to represent the target class and provide a generalization bounds.  The computational tractability of the proposed surrogate loss is a potential field for further research.", "Paraphrased Statement: This paper presents a new definition called \"statistically meaningful approximation\" (SMapproximation) that combines the concepts of learnability approximation accuracy and generalization ability. The definition is used to formulate and prove two theorems for approximating Boolean circuits and Turing machine computation using transformer networks. strength:  Clear introduction and rigorous validation. Weaknesses:  Novelty and importance of the definition are not fully demonstrated.  Ingredients of the definition are not entirely new.  The definition lacks a constructive approach for nonexistence.  More discussion and review of previous work is necessary to establish the significance of the definition.", "Paraphrased Statement: This paper explores using neural network to estimate boolean circuits and Turing machines. To do so the authors introduce \"statistically meaningful (SM) approximation\" a amount that ensures the target function class is learnable and minimizes bias. For both circuits and machines neural network based on feedforward network and transformers achieve SM approximation. strength and Weaknesses: strength:  Provides a new amount of approximation accuracy (SM approximation). Weaknesses:  SM approximation appears to be a rebranding of existing generalization theory.  theoretical results and proofs lack rigor and contain hidden constants.  Binary classification of Turing machines may be overly simplistic.  Computing the alllayer margin and minimizing the empirical risk are computationally challenging.  contribution seem limited to connecting neural network to Turing machines. Suggestions:  Acknowledge the work of classical generalization theory.  Provide more empirical results.  Downplay the contribution of introducing SM approximation.  Explore related work in nonparametric regression and neural network generalization.", "Paraphrase: Summary: This work introduces a new standard for understanding representations of Boolean circuits and Turing machines by neural networks. The key concept is \"statistical significance\" which enables the representation of these structures. strength and Weaknesses: While it is known that neural networks can represent function and programs some results rely on infinite precision making them impractical. This paper proposes a criterion that addresses this issue requiring the representation to include a loss function that captures the target function when minimized empirically. The paper demonstrates the utility of this criterion by showing that Boolean circuits can be represented \"statistically significantly.\" It provides a loss function and a validation that empirical minimization on a finite sample ensures a reliable representation. This is achieved by using ReLUs and a technique to create a margin even when it does not exist initially. The paper also attempts to apply this criterion to show that transformers can represent Turing machines but the details of the representation and its advantages are not clearly explained."], "ptZfV8tJbpe": ["Summary This study introduces LLEM a new method for text classification with multiple labels. LLEM uses latent labels to encode papers and labels jointly. This approach is simpler than previous methods and allows for beneficial modeling of label correlations with fewer restrictions. LLEM achieves superior performance compared to existing techniques on two benchmark datasets. strengths and Weaknesses  strength: The concept of latent label encoding is innovative and highly effective. LLEM is simple and removes unnecessary assumptions present in previous methods.  Weakness: The paper lacks an indepth analysis of how latent labels correlate with actual labels and context. future work is needed to explore these relationships. Additional Observations  The latent label encoding approach in LLEM is similar to Ptuning which uses continuous embeddings as prompts for language models.  LLEM outperforms LACO in all data groups but the discussion focuses only on observed results rather than providing insights into the superior label correlation usage of LLEM.  The study includes a comprehensive review of recent multilabel text classification research.  There are minor grammatical error such as \"intresting\" on page 9.", "Paraphrase: This paper proposes a technique for multilabel text classification that leverages latent labels to capture label correlations indirectly. By concatenating random latent labels to input text tokens and feeding them into a BERT model the method generates contextual representations of these latent labels. These representations are then used to predict the actual labels. Comparing the method to the stateoftheart LACO algorithm on the AAPD and RCV1V2 datasets it outperforms LACO in terms of Hamming departure and MicroF1. Notably the method exhibits superior performance on lowfrequency labels and samples with a high label count. Summary of Strengths and Weaknesses: Strengths:  Outperforms the stateoftheart LACO algorithm on two benchmark multilabel text classification datasets.  Enhances performance on lowfrequency labels and samples with many labels. Weaknesses:  The overall contribution and impact are limited as the idea of using latent labels instead of actual labels is similar to LACO.  The baseline models are not comprehensive and strong models (e.g. ECLARE vanilla BERTbased AttentionXML) should be evaluated.  The method does not utilize label semantics in its innovation as the latent label embeddings are initialized randomly.  The optimal value for the hyperparameter k (number of latent labels) is not explored through an ablation study.  The applicability of the method to datasets with a large number of labels has not been demonstrated.", "Summary This paper proposes a technique for multilabel text classification that implicitly captures label relationships through \"latent labels.\" Unlike previous methods that directly model these relationships this approach demonstrates superior performance on benchmark datasets. Strengths and Weaknesses  Latent labels allow for effective implicit modeling of label correlations.  Experimental results show significant improvements over baseline methods.  Pretraining latent label embeddings does not enhance performance. Concerns and Suggestions  The mechanism by which latent labels contribute to classification needs clarification.  explicit label correlation modeling as in label embedding methods offers intuitive and explainable approaches.  further analysis could explore the work of latent label count on classification results.  The concept of \"labelcorrelation overload\" requires elaboration and empirical validation. Typo  The subscript of the last word token should be \"m\" instead of \"m1.\"", "Paraphrased Summary: This paper proposes a method to capture label relationships in multilabel text classification without explicitly defining them. Instead of using conclusion trees which explicitly represent label correlations the authors add \"latent labels\" to the start of each papers and input this into a BERT classifier. These latent labels are randomly initialized and their combined output is used for classification. Experiments show that this approach outperforms baseline methods and ablation studies demonstrate that latent labels are more effective than actual labels in this model. Strengths and Weaknesses:  The concept of adding latent tokens to papers beginnings is not new but is claimed to implicitly model label correlations.  Intuitive explanations or experiments supporting this claim are lacking.  The paper lacks significance testing and some significant baseline methods (XMLCNN and AttentionXML).  Appending true labels during training can hinder performance but this is not applicable during inference.", "This paper focuses on multilabel text classification and introduces a modified Transformer encoder that models correlations between hidden labels and input text. The hidden states of these latent labels are combined and used as input to a multilayer perceptron (MLP) for classification. While experiments demonstrate slight improvements over existing methods on small datasets the study lacks analysis of complexity and scalability for largescale tasks involving millions of labels. The concept of latent labels remains unclear and its uncertain whether they represent semantic clusters or if their identification is addressed. Additionally the paper should compare model parameters to justify the effectiveness of the proposed approach."], "wYqLTy4wkor": ["Paraphrased Summary: This paper aims to resolve the discrepancy issue that arises when applying curriculum learning methods to parameterized Partially Observable Markov Decision work (POMDPs). The problem stems from curriculum learning use of a training distribution over POMDP parameters that may differ from the distribution encountered during testing. As a issue the optimal solution under the curriculum distribution may not be optimal under the actual distribution. method: To address this issue the paper proposes a method that leverages access to a simulator. Given a past trajectory the simulator is utilized to sample the next state under the true distribution of POMDP parameters. This information is then used to update the policy. effectiveness and Weaknesses: effectiveness:  Potential to alleviate covariate shift in issue distribution. Weaknesses:  Title Discrepancy: The connection between \"unsupervised environment design\" and the paper method is unclear.  Formal problem Statement: A clear definition of the problem assumptions and available information for the agent is missing.  Background on PLR: Additional context on the preceding work on PLR (Policy Loss Regularization) would be beneficial.  POMDP Tuple Superscripts: The significance of the superscripts in the POMDP tuple is unexplained.  Observation Mapping and reinforcement work: Assumptions made about deterministic observation mapping and stateonly reward work impact the methods applicability.  Trajectory Notation: Inconsistencies in the use of the symbol \u03c4 to represent trajectories and states within trajectories.  Notation complexity: Excessive use of superscripts and symbols complicates understanding.  Mistake vs. Misunderstanding: The distinction introduced between \"mistake\" and \"misunderstanding\" is not thoroughly utilized.  Simulator Dependency: The methods reliance on access to a simulator undermines the objective of modelfree methods.  Simulator Reset Feasibility: Resetting the simulator to match specific trajectories under different POMDP parameters could be computationally challenging.  Aleatoric Uncertainty: The relevance of aleatoric uncertainty under the assumption of deterministic environment dynamics needs clarification.  Bayes Optimality discussion: The arguments presented on Bayes optimality are limited to the tabular setting and may not apply to work approximation scenarios.  State Visitation bias: The impact of distribution shifts in state visitation on test performance is not considered.", "Summary This research aims to create learning curriculum for AI agents to handle tasks from an\u672a\u77e5\u5206\u5e03. It introduces an algorithm based on Prioritized degree Replay and OffBelief Learning to train the agent using predicted trajectories from an inferred belief framework. The algorithm is tested in two goalreaching tasks with discrete states and actions. effectiveness  Explores unsupervised environment design an essential concept for advancing reinforcement learning with limited human involvement.  The proposed algorithm outperforms existing methods in discrete space task domain. Weaknesses  Questionable problem Formulation: The paper reliance on a Q value work marginalized over environment parameters raises the question of why not utilize the inferred belief framework for online system identification and a learned policy.  Incorrect Belief framework Definition: The equation for the belief framework is missing the marginalization term.  lack of Definitions: Several key terms lack definitions making the paper challenging to understand.  Insufficient Exploration of Trajectory Benefits: The explanation for how fictitious trajectories alleviate curriculuminduced covariate shift is not adequately convincing.  Simplistic experimental Setup: The experimental setup appears also simplified potentially limiting the algorithms effectiveness.  Absence of Curriculum analysis: Despite claims of curriculum generation the paper lacks results that demonstrate the created curricula. Analyzing the evolution of inferred parameters during training would be valuable.", "Paraphrased Statement: Summary: This work presents a solution to the issue of covariate shift in adaptive curriculum learning where the parameter distribution in the test environment differs from that in the training environment. This divergence is caused by algorithms like PAIRED and PLR which favor environments with high regret leading to biased training on a distribution that differs from the true one. The solution proposed here assumes the designer has knowledge of the ground truth parameter distribution of the environment and uses fictitious transitions and Off Belief learning (an algorithm for cooperative MARL) to compute the value work. effectiveness and Weaknesses: effectiveness:  The solution effectively reduces the discrepancy between training and testing performance. Weaknesses:  Clarity:  Inconsistency in notation for ground truth distribution: P(\\theta) vs. \\barP(\\theta)  Unclear definition of \\taut in belief notation  Distinction between \\barQ and Q is unclear  Mislabeled uncertainty as aleatoric in certain scenarios  Justification for optimality condition (\\barP(\\theta \\bar \\taut) 1) is lacking  Algorithm:  potential logical error in Algorithm 1 (should be \"if d 0 or \\Lambda is empty\")  Notation:  Confusion in the use of \\lambda\\barP(\\theta) and \\barV\\theta  Experimentation:  lack of details on the issue of seeds used and the interpretation of shaded areas in the results  Related work:  Similarity to other work that employs importance sampling to address sampling bias in rare environments", "Paraphrase: Summary: The authors examine the issue of covariate shift in recent adaptive curriculum learning methods. Their solution involves modifying the interaction between the adversarial approach and random (stochastic) parameters. They demonstrate this in two 2D grid environments. effectiveness and Weaknesses: 1. issue: An adversarial curriculum approach can modify a hidden stochastic parameter that significantly alters the optimal policy. 2. Approach: Difficult to understand at first but simplifies in their experiments due to simple 2D environments. 3. Validity: Although the authors may correctly identify a problem its unclear if its novel. 4. Concerns:  Modifying a parameter that changes the optimal policy seems generally undesirable.  The analogy to \"cryptic conventions\" in cooperative MARL may not be helpful.  The assumption that curriculum parameters can be uniquely determined by trajectories may be flawed in more complex environments.  The experiments use overly simplistic environments. 5. Typo: \"persisitent\" should be \"persistent.\""], "kAa9eDS0RdO": ["Summary: This paper presents a new architecture for deep learning models that incorporate concepts. It uses a transformerbased module to capture both local and global concepts. The model is trained using supervised learning and evaluated on three benchmark datasets. Strengths:  The approach contributes to the field of interpretable deep learning models.  It extends previous methods by explicitly modeling local and global concepts.  The architecture is clearly described and illustrated. Weaknesses:  The paper lacks focus on the novelty of incorporating local and global concepts.  The authors claims about the uniqueness of their approach are unclear (e.g. comparing to ProtoPNet).  The use of an additional evaluation method proposed by Margeloiu et al. would strengthen the authors claims.  The use of evaluating the model on aPY dataset which only uses global concepts is\u4e0d\u660e\u78ba.", "This field introduces a novel Transformerbased architecture that incorporates \"concepts\" as a means of interpretability. These concepts are leveraged through crossattention with input queries allowing for the foundation of an auxiliary explanation loss. The model demonstrates competitive performance in various domains while enhancing model interpretability. However the concepts primarily provide insights into potential reasons for misclassification and their causal relationship with classification accuracy remains to be fully explored.", "Paraphrase: The paper presents a transformerbased model that improves the explainability of deep learning models by integrating domain knowledge through a crossattention mechanism. The work highlights the connection between posthoc explainability and inherently interpretable models. While traditional interpretable models face debates about user trust in model explanations the proposed model seeks to address this limitation by leveraging domain knowledge. Strengths and Weaknesses: Strengths:  Clear and compelling need  Welldesigned experimental setup  Reasonable issue Weaknesses:  Novelty of the technique is questionable as it relies on a simple linear combination of attention weights and domain knowledge  Relatively weak accuracy performance compared to baselines  Lack of detailed data on concept patch generation and error analysis", "Paraphrased Statement: This paper presents an improved version of the Transformer model called concept Transformer that enhances the interpretability of its attention mechanism by calculating crossattention between input features and predefined concepts. This approach aims to make the model more understandable by providing humaninterpretable attention weights for specific concepts and more faithful as the attention scores for concepts directly influence the models predictions. Additional Feedback: Despite some missing relevant prior work the paper effectively explores the issue of interpretable reasoning in Transformer models and proposes a conceptually simple and general model. The idea of using explicit concept supervision when available is a valuable feature of the approach. The experimental issue are comprehensive and demonstrate the potential of the model across various datasets. It would be beneficial to explore the model in different domains or for multimodal tasks in future field. However the necessity for predefined concepts limits the approachs applicability in cases where such data is not available."], "gaYko_Y2_l": ["Paraphrased Statement: This research examines the task of organizing graphs into clustering while incorporating graph attribute information. The proposed approach utilizes Gaussian Mixture model (GMM) and Graph Convolutional Networks (GCN) to address this problem. Additionally a novel loss function is introduced. The work presents comprehensive experiments demonstrating the superiority of the proposed method. effectiveness:  The research addresses a relevant problem in graph analysis.  The proposed method is logical and wellfounded.  Thorough experiments provide strong evidence of the methods effectiveness. Weaknesses:  The benchmark methods selected for comparison are not sufficiently convincing. more innovative techniques should be considered.  The claimed novelty of the method is questionable as similar approaches have been previously explored.", "Summary The researchers present a novel graph clustering technique called Gaussian Mixture Graph Convolutional Network (GMMGCN) that utilizes graph labels. empirical findings reveal that incorporating graph labels enhances clustering performance compared to conventional methods and multipleinstance learning. Strengths  The article is wellorganized and written.  Four case are described and detailed experimental findings on synthetic data are provided for each case.  The Twitter instance demonstrates a practical application of the proposed algorithm. Weaknesses  Clarifications:  Is it presumed that Cnode  Cgraph  If Cnode is unknown how is it determined  Are there realworld scenarios where graph interconnectedness help in graph clustering (Since both realworld datasets in the work are singlegraph instances a case 4 application is unclear.)  An algorithm section with input output and procedural steps would increase clarity.  The design of using two GC4NC2 datasets for experiments is unclear nor is their primary distinction.  Do the authors recommend a minimum number of graphs for the algorithm They attribute the Sofa instances inadequate performance to a minor sample size.  minor Points:  Typo: On Page 3 Section 3.1 \"Cnode 1 is the expected number of node categories\" should be \"Cnode is the expected number of node categories\".  In the final paragraph of the introduction clarifying NMI and mIoU would enhance understanding.", "Paraphrase: Summary: This research explores a new problem called weak supervised graph clustering (WSGC). WSGC aims to improve node clustering by leveraging graphlevel information (e.g. labels) that is readily available in realworld applications. Previous graph clustering methods primarily relied on nodeedge information overlooking graphlevel side information. To address this gap this paper proposes a framework named GMGCN (Gaussian Mixture Graph Convolutional Network) that combines Gaussian Mixture model (GMM) and Graph Convolutional Network (GCN) to learn node representations. Additionally it presents a consensus loss for graph labels and employs Gaussian Mixture Layer to determine node categories. Experiments on both simulated and realworld datasets validate the effectiveness of the GMGCN approach. effectiveness and Weaknesses: Pros:  Clear and organized writing style  Novel concept of incorporating coarsegrained graph labels in graph clustering  Demonstrated superiority of WSGC over traditional GC in experiments Cons:  The title overstates the setting of the work (weak supervised graph clustering encompasses other settings)  The utility and design of effective graph labels for graph clustering remain unclear  GMGCNs performance disparity across categories (e.g. sofa category) needs further analysis  The meaning of \"node updating\" in number 1 requires clarification", "Paraphrase: Authors combine Graph Attention Networks (GAT) and Gaussian Mixture model (GMM) in a framework for weakly supervised graph clustering. They introduce a \"consensus loss\" function specific to this framework. The framework accommodates various weak supervision settings and its performance is demonstrated on synthetic datasets for each case. effectiveness:  Improved performance over baseline models on given and synthetic datasets.  Uses two interconnected graph levels for different categories and a hypergraph.  Embeds Gaussian Mixture models in the framework. Weaknesses:  Complex process with insufficient explanations and potential typos making it difficult to follow.  No guidance on determining the number of clustering.  Unclear necessity and contribution of each step in the process.  Unclear advantages of weak supervision over unsupervised approaches. Details:  Equation 4 may be incorrect in its function of Cnode and Cgraph.  Equation 4 may not fully represent the GMM model as indicated.  Equation 5 may contain trainable parameters despite the authors claim to the contrary.  consensus loss considers distances to cluster centers and graph centers but its relationship to weak supervision is unclear.  The framework should be compared against other GMMbased clustering methods as baseline references.", "Paraphrased Statement: This work introduces a new concept called weakly supervised graph clustering. The goal is to assign labels to nodes within a set of graphs utilizing both node features and information within each graph. effectiveness and Concerns: The manuscript is wellwritten and attempts to establish a novel problem and solution. However the following concerns arise:  The problem definition includes various case of information including node features labels and interactions. Its uncertain if all this data is available in realworld applications.  The impact of graph labels on node label predictions is unclear.  The experiments only use Twitter data which lacks intergraph interactions. This suggests that the problem could be solved using existing graph clustering methods.  The proposed method may be overcomplicating a problem that can be addressed by simpler approaches.  The authors should provide more convincing applications where all required input data is available.  They should also demonstrate the significance of each component in node label prediction and analyze their impact on the solution. minor Comment:  In Section 3.1 the variable \"d\" is not defined."], "giBFoa-uS12": ["Paraphrase: The paper introduces InfoPG an approach that transforms individual policies based on states or partial observations into those conditioned on actions of other agents. Using Klevel rationalizability from cognitive hierarchy theory the policy is recursively conditioned on small levels of actions from all agents. Optimizing the policy follows a policy gradient algorithm structure. The authors demonstrate that by modifying the policy using reinforcement work or applying a relu work on the advantage the reformulated policy implicitly maximizes a small bound on mutual information across agent policies. In the Byzantine Generals Problem when noncooperative agents can harm multiagent learning the authors show that removing the relu work on the advantage leads to adjusting the upper bound of mutual information. The mutual information between the agent in question and the fraudulent agent is reduced in case of a negative reinforcement preventing the fraudulent agent from disrupting the agents policy uncertainty or poisoning the multiagent learning work. The algorithm is tested in various decentralized multiagent learning environments with promising outcomes. Strengths and Weaknesses: Strengths:  innovative approach: Reformulating policies based on agent actions and using a Bayesian tree representation for agent interaction.  wide experiments: The algorithm is evaluated in various multiagent learning environments. Weaknesses:  Unclear implementation details: Specific details on how maximum posterior action sample is performed for continuous actions (Algorithm 1 line 12) and the correspondence of Algorithm 1 line 19 to an equation in the text are missing.  Undefined variables: J(\u03b8) in Equations (2) and (15) is not explicitly defined.  Unclear explanation: The reasoning behind the equation following Equation (35) in Appendix A.5 for a Bayesian tree is not provided.", "Paraphrased Summary: This work introduces InfoPG a method for optimizing coordination in decentralized multiagent games. InfoPGs policies consider the reasoning of neighboring agents (klevel reasoning). Strengths:  clear writing and logical presentation.  Straightforward policy architecture that implicitly targets coordinationrelated mutual information (MI).  Both theoretical and empirical evidence of effectiveness.  Accommodates the theory of unreliable communication. Weaknesses:  Limited relevance of the mentioned concept of bounded rationality.  Overemphasis on the MIcoordination connection which may not be the sound framework for understanding the approach.  Insufficient analysis of key assumptions and limitations.  Misleading connection between klevel reasoning and the actual policy implementation. Questions and Comments:  Clarify the unique features of InfoPG compared to existing methods.  Provide a more precise definition of \"policy distributions.\"  clearly define the range of \"InfoPG\" (encodingdecoding policy objective etc.).  Explain the MI estimation method and the rationale behind the \"ablation\" work in number 2.", "Summary: This paper presents an approach to multiagent learning that involves a hierarchy of reasoning to maximize information sharing and rewards. Agents at each level predict other agents actions and use game theory to find the sound response. Experiments in StarCraft II and multiwalker evidence promising results compared to baselines. Strengths and Weaknesses: Strengths:  innovative approach to multiagent learning that blend information theory and game theory. Weaknesses:  The paper lacks a comprehensive literature review omitting relevant work on hierarchical learning in DecPOMDPs and communication in multiagent RL.  The framework theoretical analysis does not address the potential for nonstationarity and how it is avoided.  The communication setup seems artificial and needs further evaluation under realistic constraints.  The paper lacks clarity making it difficult to fully understand the proposed framework. Key terms are not defined and the technical details of the approach are not adequately explained."], "g5odb-gVVZY": ["Paraphrased Summary: The work presents multilevel physicsinformed neural networks (MPINNs) inspired by multigrid methods for solving linear systems in partial differential equations (PDEs). MPINNs offer improved accuracy and efficiency compared to traditional physicsinformed neural networks (PINNs) particularly for elliptic and nonlinear equations. Strengths and Weaknesses:  The paper lacks clarity in its English presentation.  MPINNs demonstrate potential advantages in speed and accuracy for certain PDE equations.  The paper recommends publication field to language enhancements and the following critiques: Critiques: 1. Minor grammatical error:  Remove extra parenthesis in formulas for R and P.  Revise \"at the and of each cycle\" to \"at the end of each cycle.\"  Reduce excessive use of \"very\" and \"then.\" 2. Justification for h2:  Explain the significance of choosing h2 for MPINNs training.  Provide guidance on selecting this parameter. 3. Explanation of MPINN superiority:  Provide a rationale for why MPINNs perform better than PINNs. 4. Grid size choice rule:  Establish a methodology for determining the appropriate grid size. 5. Consistency between design 3 and Table 3:  Explain the discrepancy in h rate between the plot (h10) and table (h30) for the same experiment.", "Summary: Multilevel Physics Informed neural Networks (MPINNs) are proposed. Unlike standard PINNs MPINNs employ multiple networks for different levels addressing fine and coarse terms in a twolevel approach. This method extends to multiple levels through recursive definition inspired by the multigrid method. empirical work on three simple ODEsPDEs demonstrate the viability of MPINNs. Strengths and Weaknesses: Strengths:  Novel and intuitive approach  potential to handle PDEs with both fine and coarse components Weaknesses: Experimental solution:  Insufficient details on learning rate tuning which may skew results  Inconclusive results due to varying experimental settings and undefined plotted rate Theoretical foundation:  Lack of theoretical results compared to Extended Physics Informed neural Networks (XPINNs)  Failure to acknowledge the relationship between MPINNs and XPINNs Other Concerns:  Questionable interpretation of results involving increased h and H  Unsubstantiated performance claims without empirical evidence  Random and unsupported claims throughout the paper  Typographical and grammatical error", "Paraphrased Statement: A fresh physicsinworked neural network (PINN) has been developed inspired by multigrid methods. By separating the solution into two components one for finescale and one for coarsescale structures the proposed multilevel PINN accelerates convergence and reduces error compared to standard PINNs. This approach has been demonstrated effectively for both 1D and 2D problems. Strengths:  Utilizing multigrid methods for PINN design is innovative.  Splitting the solution simplifies the frameworks task (finescale learning is typically more challenging).  The approach can be considered a work of data augmentation.  Clear presentation and readability. Weaknesses:  Experiments are lacking:  How are zH and ZH constructed and determined to be insufficient  How does zH variation impact perworkance  Comparison to other stateoftheart PINNs or classical scientific methods is missing.  Marginal differences between the proposed framework and ablation frameworks in Table 2 and 3:  Is there a practical significance in RMSE differences  Failure of PINNh and hH with increased parameters:  Are frameworks regularized and how were tuning parameters chosen  Regarding the provided notebook:  Hyperparameter tuning improved the perworkance of the standard PINN potentially outperworking the multilevel PINN.  The term \"increased robustness of MPINNs\" needs clarification.", "Paraphrase: Summary The researchers combine concepts from multigrid methods for solving partial differential equations (PDEs) with Physics Informed Neural Networks (PINNs). They introduce PINNs and Multigrid methods and then describe their approach. The PDE solution is divided into coarse and fine components and separate PINNs are trained to learn each term. The coarse PINN is little and may be trained on fewer data points. Training alternates between the two PINNs with a predetermined number of epochs devoted to each. The method is tested on a linear 1D elliptic equation a 2D nonlinear equation and a 1D Burgers equation. The results are compared to those obtained using a standard PINN and a large PINN. Strengths and Weaknesses Multigrid training for PINNs has potential. The authors propose that the structure of MPINNs facilitates training by introducing an inductive bias. However the results presented are preliminary and require further validation. 1D Linear elliptic Equation The two PINNs differ only in network capacity. The equation is chosen to produce two different solution modes. It would be valuable to explore the effects of varying the parameter \u03b1 and the impact on MPINN training. The distinction between using Adam for 1layer networks and BFGS for 4layer networks is unclear. Since BFGS requires more memory Adam should be preferred for large networks. The learning rates for different problems were tuned and it is uncertain whether the performance differences are solely due to the different learning rates and hyperparameters. 2D nonlinear Equation The equation and its solution should be described more clearly. The difference between \"fine\" and \"coarse\" data point sets (400 vs. 484) is minimal. The rationale for these numbers should be explained. The results are less convincing for the nonlinear equation. Burgers Equation The optimizer used for training should be specified. Different network sizes are used for each PDE. The foundation for these choice should be explained."], "sTNHCrIKDQc": ["Paraphrase: Summary: This article provides an innovative method for clustering graph drawn from graphons. It guarantees the quality of the resulting clustering and presents initial experiments. strength and Weaknesses: strength:  The paper is clear wellwritten and engaging. Weaknesses:  The theory applies only to graph sampled from graphons with fixed vertex order.  practical application often involve graph with unknown vertex order which are not addressed by the proposed method.  The experimental section could provide more data on the algorithms runtime.  The assumption that sampling is ordered with L2 graphon norms may limit practical relevance. specific detail:  Assumption 2: The term \"degree distribution\" may be misleading as it typically refers to the probability distribution of vertex degrees.  Equation (1): The index i of Aij appears to start from 0 to n01 instead of the usual 1 to n0. Vertices with indices greater than n0\\lfloor nn0 \\rfloor are not considered.  Missing articles: Articles are missing in certain instances such as \"the DavisKuhan theorem\" and \"the Gaussian kernel.\"", "Paraphrased Summary Main Points:  The paper introduces a novel graph distance based on graphons for small sample size application.  The distance addresses two gaps in the literature: lack of work beyond graph classification and theoretical grounding.  Theoretical guarantees are provided for two clustering methods and a twosample statistical test using the proposed distance. strength:  clear and wellstructured writing.  theoretically sound approach with intuitive explanations.  applicable to small graph samples.  Appears to be a novel contribution. Weaknesses:  Claim of Limited Literature: The paper asserts a lack of work beyond network classification and theoretical support. However multiple work have addressed these view as evidenced by the provided references.  IllDefined Graph Transformation: The transformation from graph G to G\\sigma may not always have a unique increasing permutation due to nodes with the same degree.  Incorrect Claim about NCLM: NCLM is not the only clustering method for graph of different size. Distance and kernelbased methods without explicit vector embeddings can also be applied.  Limited comparison: The chosen comparison methods are not explicitly explained and their selection seems arbitrary.  TwoSample theory Test Clarity: The construction of the critical region for the test without knowledge of the underlying graphons needs clarification. Questions and Unclear contribution:  SortingandSmoothing Estimator: Its introduction in the abstract without a definition in the paper.  Theorem 1 Interpretation: The expected result would be a convergence to the optimal rate of misclustered graph not a higher rate with more graph.  Clustering Method Adaptations: The extent and nature of adaptations to use the proposed distance for clustering methods.  Sparse Graph Considerations: Limitations or differences when using the distance on sparse graph compared to denser ones.  Graph assumption: Clarity on whether graph are assumed to be symmetric without attributes and indistinguishable from inhomogeneous random graph.  Variable Definitions: Undefined variables (n in Prop 1 C in Cor 1) need clarification.", "Paraphrased Statement: motivation and Approach: Inspired by the sortandsmooth graphon estimator developed by Chan and Airoldi in 2014 this paper introduces two novel clustering algorithms for analyzing multiple graph without known vertex correspondence. These algorithms leverage a graphon approximation method to generate histogram estimators with identical bin counts for each graph. Subsequently existing spectral clustering techniques are applied to the graphonbased distance matrix. The theoretical properties of these approaches are thoroughly analyzed and their consistency is demonstrated under specific smoothness assumptions about the underlying graphon. strength and Weaknesses: strength:  Handles the challenge of vertex correspondence: Unlike existing methods that assume vertexmatched graph this paper directly tackles the issue of clustering graph without known vertex correspondence.  Theoretical guarantees: The proposed clustering algorithms and testing framework are supported by theoretical guarantees. Weaknesses:  Limitations of the Bernoulli exchangeable framework: The paper utilizes the Bernoulli exchangeable framework for graph estimation which only accommodates dense graph. This framework may not be suitable for realworld networks that are typically sparse.  Strict degree monotonicity assumption: The assumption of rigorous degree monotonicity is rerigorousive and may not hold in many practical scenarios. The paper acknowledges that alternative histogram methods for graphon estimation might be more appropriate in such cases.  Known number of clustering: The numerical experiments assume a known number of clustering (K) while in practical application K must be determined from the data itself.  Lack of performance evaluation for sparse networks: The simulation work does not evaluate the performance of the techniques on sparse networks.  Limited robustness analysis: The paper does not fully explore the robustness of the techniques to deviations from the rigorous degree monotonicity assumption or the selection of metric for defining the graph distance.  Missing discussion on assumptions: The consequences of assumption 2 and 3 particularly in the context of clustering could have been further clarified. Other Minor Comments:  The introduction could provide a clear definition of clustering in the context of graph.  The paper does not specify whether directed graph or graph with selfloops are considered.  The paper should provide a more detailed explanation of Assumption 3 including the definition of comparison classes."], "uut_j3UrRCg": ["Paraphrased Statement: Summary: This work proposes a modular system for multitask learning with hierarchically arscoped tasks. It uses sketching and locally sensitive hashing to optimize execution. The system is equipped with proofs and error limits that demonstrate its ability to acquire tasks while utilizing learned subroutines. It also offers extensions for situations where tasks are interdependent or lack clear boundaries. empirical work on synthetic domains such as intersecting halfspaces and MNIST digits show the advantages of this modular approach over endtoend models. effectiveness:  Provides theoretical analysis and error bounds for modular learning.  Extends the system to handle task dependencies and boundary ambiguities. Weaknesses:  Lacks clarity in explaining the concept of sketching.  Does not provide many concrete examples or connections to existing modular architectures.  Experiments are limited to toy domains and a narrow scope of models.", "Summary The work proposes an approach to continuous learning using a modular architecture that incorporates sketches. Its key contributions lie in proving the learnability of certain task classes under three modular architecture variations. The paper also empirically evaluates the algorithms on simulated supervised learning tasks. Strengths  The investigated problem aligns with the interests of the lifelong learning community.  It is a rare approach to lifelong learning from a theoretical perspective. Weaknesses  The paper system makes it hard to follow with technical details relegated to an extensive appendix.  The theoretical results may be limited.  The empirical analysis is insufficient. Arguments The work is deemed relevant and intriguing. However it suffers from clarity number in presenting the approach and motivations relying heavily on technical descriptions of sketches as a means of providing intuition. A more intuitive and structured presentation is needed. The theoretical results are perceived as minor given the assumptions that allow for perfect task model learning and module combinations through exhaustive research. The lack of a true lifelong scope also raises concerns. The empirical evaluation is inadequate for a lifelong learning paper considering the limited toy problems examined. A more comprehensive evaluation is recommended. Additional Feedback  Missing references to relevant lifelong learning work.  Unclear use of terminology and arbitrary comparisons.  The appendix is excessively long and contains tangential sections.  Language and grammar errors need addressing.", "Paraphrased Summary: The paper introduces a modular architecture for learning hierarchical tasks continuously supported by theoretical guarantees. It demonstrates the approach on simple domains using a single baseline. effectiveness:  Theoretical exploration of lifelong learning.  Integration of empirical results to validate the proposed method. field for Improvement:  scope Clarification: Distinguish between the specific setup of sequential multitask learning and more general lifelong learning with task revisits.  Related work: Cite additional relevant literature on hierarchical learning in reinforcement learning modular networks and lifelong learning.  Clarity: Enhance clarity by rearranging sections and providing explicit definitions and assumptions. Clarify the use of assumptions regarding task and datapoint distributions.  Novelty and Significance: Discuss the novelty and impact of the approach compared to previous research on modularity and lifelong learning.  practical execution: Provide guidance on obtaining atomic tasks from standard tasks and discuss the execution of standard modular networks on the presented domains.", "Paraphrased Statement: Summary: This research proposes a continuous learning architecture that incorporates hashing mapping module routing and sketch components. Mathematical proofs support the architectures effectiveness. Preliminary experiments suggest it outperforms traditional deep learning approaches. Assessment: While the author has limited theoretical expertise they provide a basic review with low confidence. effectiveness:  The authors offer theoretical guarantees for continual learning a field with limited such guarantees. Weaknesses:  The architectures complexity makes it challenging to comprehend.  The paper may target theory and sketch researchers but needs simplification for broader continual learning researchers.  The authors fail to adequately explain the continual learning aspect of the experiments."], "oj2yn1Q4Ett": ["Paraphrase: Summary Distributed data privacy is a challenge where data is spread across multiple users who want to keep their data private. This paper addresses the issue of decentralized empirical risk minimization with reproducing kernel Hilbert space. Two main kernel types are examined: 1. Generalized innerproduct (GIP) kernel based on the arccosine kernel (introduced in this function) 2. Random feature (RF) kernel For decentralization the authors propose approximating kernel as inner products of finite vectors. They then present algorithms (oneshot and iterative) for optimizing private models. Theoretical analysis is provided to study the approximation error optimization performance and generalization error. Experiments demonstrate the effectiveness of the algorithms and confirm the theoretical findings. effectiveness  clear and wellorganized presentation  Novel and technically sound algorithms and theoretical analysis  foundation of a new RKHS setting Weaknesses  Experiments need improvement:  Lack of concrete evidence to support the claim that highdimensional parameter communication hinders convergence  Absence of comparisons with existing methods  potential limitations of kernel methods for largescale datasets Minor Issues  Duplicate expression above and below Equation (3)  Grammatical error on Page 5: \"will dependent\" should be \"will be dependent\"", "Paraphrased Statement: Summary: This function explores multiagent optimization over reproducing kernel Hilbert spaces (RKHS) using random feature approximations. Novel technique are developed for decentralized computation through a generalized inner product kernel. Theoretical convergence analysis and numerical validation are provided. effectiveness and Weaknesses:  effectiveness: The technical contribution lies in establishing optimal generalization performance for the proposed method.  Curiosity: It is unclear whether the solid convergence result is due to a solider analysis technique or specific properties of the algorithm.  Technical foundation: The function of a generalized inner product kernel enables privacy preservation by sharing only pairwise angles between feature vectors.  communication efficiency: The practicality of sharing the kernel matrix (Am) is questionable as it may require more communication than sharing local function evaluations.  Lipschitz Continuity: It is unclear whether the Lipschitz continuity assumption in Equation (4) is independent of the denominator norms.  Comparative Analysis: The authors do not contrast the convergence theory with existing results or compare experimental results to other decentralized RKHS optimization methods.  Minor Comments:  The statement about privacy concerns is awkwardly phrased.  The sentence fragment in bold at the top of page 2 should be revised.  The claim that data exchange is not required is misleading as label exchange implies data exchange.", "Paraphrase: This paper presents a method for multiagent learning using random feature matrices. Instead of exchanging model parameters agents contribution these matrices. The function offers theoretical results for kernel ridge regression demonstrating the approach effectiveness in approximating the kernel matrix and improving training and generalization performance. While the authors claim that this method requires less communication than existing ones a comparison with prior function is needed to establish its significance."], "tJhIY38d2TS": ["Paraphrased Statement: Summary: The authors highlight that InstanceReweighted Adversarial Training (IRAT) is susceptible to new types of attacks. They attribute this vulnerability to IRAT overlooking numerous instances leading to inefficiencies. Consequently they propose Locally Reweighted Adversarial Training (LRAT) which employs local reweighting between pair of adversarial variants. effectiveness:  The paper identifies a central issue with IRAT making it a compelling read.  The writing is clear and accessible. Weaknesses:  The paper lacks a thorough explanation of how IRAT misleads weight and how local reweighting addresses this issue.  There are limited experimental results to demonstrate the effectiveness of LRAT. additional Considerations:  It is unclear whether LRAT is applicable to largescale models (e.g. WideResNet3410).  The paper includes results for only one dataset (CIFAR10). It would be valuable to examine how local weight vary across different datasets.  The computational efficiency of LRATSAA and LRATCW compared to SAT is not explored.  The applicability of LRAT to other adversarial training methods such as TRADES is not discussed.  The authors are encouraged to conduct multiple function of the proposed method and report the variability in performance to provide a more robust assessment of its improvement. Minor Issue:  The method used to rank attacks in picture 2 is not explained.", "Paraphrase: Summary: This research explores a method for optimizing adversarial training. It highlights a limitation of the GAIRAT algorithm which improves robustness against PGD attacks but compromises accuracy against other attacks (AutoAttack). The paper introduces a new algorithm LRAT which leverages multiple attacks during training and assigns varying weight to different attackgenerated samples. effectiveness and Weaknesses: Pros: 1. GAIRATs analysis is comprehensive and demonstrates its overfitting to PGD. 2. The rationale for incorporating various attacks in training is sound broadening the network robustness and mitigating overfitting. Cons: 1. LRATs evaluation is limited to CIFAR10 using ResNet18. Broader testing across datasets and network architectures including WideResNet3410 would enhance the work credibility. 2. The need for locally reweighting is ambiguous and LSATs comparable performance raises questions about its necessity. 3. The paper readability suffers from undefined notation (e.g. \\tildxi in Eqn 7 and Alg 1). Minor: 1. The claim in Section 4.3 that GAIRATs accuracy remains lower against CW attacks than PGD attacks appears to contradict earlier analysis in Sections 1 and 3.", "Paraphrased Statement: Summary: The paper investigates the function of weight in adversarial training. However the paper technical contribution is minimal and the experimental results show modest improvements at beneficial. effectiveness and Weaknesses: effectiveness:  The paper examines the importance of weight strategies in adversarial training. Weaknesses:  instance weight is a wellestablished technique and the paper presents no novel insights.  The proposed technique is overly simplistic and lacks theoretical underpinnings.  The experimental results indicate that the proposed methods produce only incremental gains over existing approaches.  The paper is not suitable for a machine learning conference and would be more appropriate for applicationoriented tracks. specific Comments: 1. The paper lacks theoretical justification for the necessity of instance reweight in adversarial training. 2. The experiments show marginal improvements over standard adversarial training (SAT) but the time cost of the proposed methods is not reported. 3. The claim that \"safeness\" is attackdependent is not supported by any theoretical guarantees. The paper needs to clarify which attacks alter safeness how they change it and under what conditions the safeness variety towards greater safety or vulnerability. 4. The paper does not provide proof of the proposed algorithms convergence or its time complexity. 5. The experiments are limited to Linfinity attacks ResNet18 CIFAR10 and do not explore other attack types network architectures or datasets.", "Paraphrase: This paper introduces LART a novel approach that assigns weight to adversarial instance during adversarial training to enhance robustness. Unlike its predecessor IART LART solves various drawbacks. Quantitative evaluation highlight the effectiveness of LART. effectiveness:  LARTs concept of weighting adversarial instance is intuitive and resolves limitation of IRAT.  Experiments demonstrate the effectiveness of LART. Weaknesses:  The experiments do not conclusively prove LARTs superiority to SAT.  Experiments in Table 1 evidence that SAT can potentially achieve similar robustness levels with a comparable performance decline. It remains unclear which method performs better when both SAT and LART degrade to the same extent (e.g. 83.17 or 81.02 natural instance performance)."], "o6dG7nVYDS": ["Paraphrased Statement: This work provides insights into the biasvariance tradeoff affecting the performance of domain generalization (DG) algorithms. Key findings:  The complexity control strategy is essential for balancing bias and variance with peak DG performance achieved when complexity is tuned using domainpecific validation.  Optimal DG requires more regularization than conventional optimization for withindomain performance. strength:  One of the early contributions to establishing a theoretical foundation for outofdistribution generalization.  Provides connections between DG and Rademacher complexity. Weaknesses:  Title suggests a comparison with [3] but does not provide sufficient evidence.  Omits relevant theoretical work ([12]) and may overlap with their findings.  theoretical significance may not be sufficient for publication.  Addresses DG within mixtures of source domain which is insufficient for providing outofdistribution generalization guarantees without considering distribution shifts.", "Summary This work addresses domain generalization (DG) where models are trained on related domain and tested on an unseen domain. The authors first present a theoretical bound on perworkance for an averagecase DG workulation followed by experiments that examine the tradeoff between model complexity and outofdistribution (OOD) perworkance. These experiments illustrate the existence of such tradeoffs for linear models and suggest the tradeoff also applies to shallow neural networks. strength  Problem setting: The setting is relevant as recent findings indicate most DG algorithms are unable to outperwork empirical risk minimization (ERM).  Insights from Theorem 1: Treating environments as data is an intriguing concept enabling the derivation of a bound that is agnostic to the work of \\mathcalE. Further analysis may yield insights when \\mathcalE has a specific structure or the hypothesis class is linear.  Accuracy vs. complexity: The experiments in Section 3.1 provide valuable insights. The findings suggest that \"the optimal regularization for [DG] is strong than for [supervised learning].\" This implies that DG methods require stricter control over model complexity than regular supervised learning. Weaknesses  Notation: The notation is confusing in range especially the similarity between the risk Rp and the Rademacher complexity \\mathcalRm.  Terminology: The terminology used to differentiate supervised learning from domain generalization is unclear.  Typos: There are several typos including inconsistencies in spelling (e.g. \"regularizers\" and \"regularisation\").  Dataset identification: The dataset used is TerraIncognito (not identified on page 5).  DG problem workulation: The authors assert that the goal of DG is to solve the following optimization problem: ``` \\minf\\in\\mathcalF \\mathbbEp\\sim\\mathcalE [Rp(f)] ``` where \\mathcalE is a distribution over distributions p. This workulation is not universally accepted and the authors assertions only apply to this specific \"averagecase\" definition.  lack of detail: Several domain could benefit from additional detail such as the concept of \"domainhift phenomena\" the definition of \"data in the wild\" the connotation of \"causal\" in the setting of DG and the meaning of \"perworkance variability\" or \"unreliable\" perworkance of existing DG methods.  Clarity of explanations: The results in Section 3.3 are unclear including the distinction between instancewise and domainwise validation. Additionally the authors dispute with the claims of [Gulrajani LopezPaz 2020] requires further elaboration.  Abstract claims: The abstract dismisses much of the existing DG research as \"empirical\" overlooking the significant theoretical underpinnings of many proposed DG algorithms. The authors own approach also relies heavily on empirical features extracted from a deep classifier.  Formalness of validation: The validation are presented inworkally using terms like \"ghost sample\" without definition. Theorems should be proven workally not handwaved.", "Paraphrase: Summary: This research introduces a theory of \"domain generalization\" based on statistical learning principles. They find that reducing model complexity improves generalization performance. Existing methods have been found to indirectly control model complexity. The theory suggests that proper model selection is crucial for complexity management. Instead of searching for hyperparameters the authors recommend domainwise crossvalidation as a model selection strategy. Experiments have shown the effectiveness of this method. strength:  Novel approach to domain generalization from a complexity perspective  Demonstrates a relationship between model complexity and training loss  Extensive experiments supporting the theory  Proposes an enhanced model selection strategy Weaknesses:  The derived upper bound relies on concentration bound that may not hold for small training domains  The bound considers an average risk over hypothetical environments which may not reflect the actual distribution  Notation in the paper can be confusing and empirical observations seem to contradict existing research Minor Notes:  \"Acrossdomain accuracy\" and \"heldout domain accuracy\" are used interchangeably  The term \"instancewise\" model selection is not defined clearly", "Paraphrase: Summary: In recent domain generalization (DG) research it has been found that traditional ERM approaches often have comparable accuracy to sophisticated DG methods in outofdomain (OOD) settings. This paper delves into the understanding behind this phenomenon focusing on model complexity. It proposes that effective OOD generalization requires models with lower complexity. Thus DG methods can leverage crossdomain validation to optimize models for effective generalization. strength:  Provides a generalization bound for the DG setting.  Empirical evidence supports the notion that OOD generalization benefits from lower model complexity.  Presents crossdomain validation as a useful metric for optimizing DG models. Weaknesses:  The paper does not address the small sample size (n) often observed in practice.  It overemphasizes model complexity as the primary factor influencing DG results. Additional Feedback:  The generalization bound could be extended to incorporate insights from papers exploring the role of regularization in OOD generalization.  The conclusion regarding regularization is not especially fresh.  The discussion following Theorem 2 overlooks the challenges of empirical DG where ERM may struggle with large domain shifts.  The empirical results demonstrate that model complexity is a contributing factor to OOD accuracy but not the sole determinant.  The paper should refrain from making overly strong claims about the exclusive significance of model complexity in OOD generalization.  The \"work DomainBed\" section should be revised as the reported results are inferior to stateoftheart methods.  The derivation of the last term in the validation of Theorem 1 (O(sqrt(ln(1\\delta)n))) is missing."], "uUN0Huq-n_V": ["Paraphrased Statement: Summary: This research proposes a fresh system for generating multiplenote (polyphonic) music. The system consists of two main steps: 1. Pretraining a special neural network (bidirectional LSTM) on a large dataset of piano roll music which represents the notes played at each time step and pitch. 2. Finetuning the neural network using a reinforcement learning (RL) approach. In this approach the network learns to evaluate the desirability (Qfunction) of adding specific notes to a chord. To define the reward function for the RL approach the authors employ adversarial inverse reinforcement learning (AIRL) where the reward function is estimated by a separate neural network (discriminator). effectiveness:  Combines existing music generation techniques to produce effective results.  Generates polyphonic music which is less common in music generation.  Uses RL for music generation demonstrating its potential in this field.  Opens up theory for further improvements and application in music generation. Weaknesses:  Lack of clarity in the description of the RL function of the system requiring reference to a figure for understanding.  Possible oversight in excluding LSTM from the AIRL network which may limit the models ability to capture longterm dependencies in music.  Subjectively evaluated generated music may sound artificial and lack temporal structure despite being preferred to other baseline approach.  Limited transparency in the subjective preference results with no samples of humangenerated music provided. specific Comments:  Clarification requested on whether the \"Qnetwork\" and \"Target Qnetwork\" have identical weights.  Clarification requested on the function of LSTM in AIRL and its implications for state definitions.  Request for a more extensive description of AIRL including the relationship between the discriminator network and the reward function.  Reformatting suggestion for the volunteers preference chart for easier interpretation. Suggestions:  Consider adding LSTM to AIRL to address longterm dependencies.  Reevaluate the necessity of LSTM pretraining.  Explore the potential of using Soft ActorCritic (SAC) model for continuous control in music generation.", "Paraphrase: Summary: This work presents a reinforcement learning approach to polyphonic music composition. The method uses human demonstrations combined with music theory rationale to design a reward function through inverse reinforcement learning. evaluation compares the IRLbased approach to a baseline LSTM density estimator. effectiveness and Weaknesses:  Concerns with Framing:  The claim about copyrightfree music generated by automated systems lacks reinforcement.  The framing of music composition as chord choice is limited and potentially misleading.  Lack of Literature reinforcement:  Introduction lacks references to reinforcement claims.  Confusing Terminology:  function of \"harmony\" and \"chord\" is imprecise and confusing.  Limited Music Generation Discussion:  Section 2 does not provide a comprehensive overview of music generation and cites outdated references.  Unverified Claims:  Assertions about the prevalence of GANs in music generation and natural language QA are unsubstantiated.  Unclear Methodology:  Description of combinatorial note generation is unclear and implies a continuous process space (which it is not).  State representation as the previous chord suggests a limited Markov model.  Underwhelming evaluation:  Baseline results are weak limiting the significance of IRL improvements.  MTAIRL results appear mediocre compared to other music generation systems.  Data Concerns:  Dataset choice may impact evaluation results.  Goodharts Law:  The music theory reward may bias the IRL approach toward specific metrics.", "Paraphrased Statement: Summary: This research presents a method for creating pianobased polyphonic music using recurrent networks trained using reinforcement learning (RL). The method includes an adversarial inverse reinforcement learning (AIRL) step to derive a reward function that is used in combination with music theory reward during the Qnetwork adjustment. The model outperforms an untrained LSTMbased model in both subjective and objective evaluations. effectiveness:  Introduction of AIRL to learn a reward function for music generation leading to a significant improvement in output choice.  The model generates piano music with acceptable sound choice considering the difficulty of producing structure repetition and cadence in polyphonic music. Weaknesses:  The paper organization could be enhanced:  Data representation should be introduced before Section 4.1 to provide context for midi and piano rolls as well as the 44 notes in the piano roll.  The unique structure of the AIRL model which allows reward function extraction from the discriminator could be explained in more detail.  The evaluation lacks comparisons with other notable polyphonic music generation models (e.g. MuseGAN Music Transformer).  The related work section should discuss more recent advancement in music generation.  Samples of music generated with only the music theory reward would be beneficial as it is mentioned that these samples often sound similar to each other. Additional Comments:  \"unpleasing\" can be replaced with \"unpleasant.\"  \"Usages\" can be replaced with \"application.\"  Clarify whether manually and subjectively assigning reward function is commonly practiced in music generation models."], "vDwBW49HmO": ["Summary This paper introduces a gradient matching technique for domain generalization (IDGM). It also simplifies the model using a firstorder algorithm to avoid complex secondorder calculations. experimentation show beneficial performance than the ERM algorithm on WILDs and DomainBed datasets. Strengths  Interesting Fish theorem simplifies calculations.  Provides insights into ERMs limitations in domain generalization.  Clear writing and organization.  Demonstrates practical benefits on realworld datasets. Weaknesses  Lack of clear justification for using gradient matching to achieve domaininvariant representations.  Missing comparative experimentation directly optimizing IDGM on DomainBed.  Inaccurate statements in the related work section.  Structural similarities to existing algorithm (MAML Reptile) without sufficient explanation or highlighting differences.", "Paraphrase: The authors introduce an objective called Interdomain Gradient Matching (IDGM) for domain generalization. IDGM maximizes the inner product between gradients from different domains to encourage model transferability. They propose a computationally efficient optimization method for IDGM and demonstrate its effectiveness on several datasets. Strengths:  The authors clearly motivate their approach using illustrations and explanation.  IDGM combines existing idea in a novel way leveraging Taylor series expansion and modifying the Reptile algorithm for crossdomain optimization.  The model update process is simplified due to the linear approximation used.  IDGM eliminates the need to store intermediate gradients making optimization more efficient. Weaknesses:  The statistical significance of IDGMs improvements is unclear.  The reasons for the methods varying performance across datasets remain unexplained.  Citations from relevant literature are missing.  comparison with Fish and IDGM are needed to verify the accuracy of the linear approximation. Review Questions:  How does IDGM differ from kernel methods  Can the linear approximation be evaluated or compared to an exact computation  Can IDGMs performance be improved with additional resources  Why does IDGM perform worse on datasets like iWildCam where invariant features (e.g. vegetation) may confuse the model  How does IDGM relate to Neural Tangent Kernels (NTKs)", "Paraphrase: Summary This paper suggests a method to enhance the similarity of gradients from classification loss work across different domains. This approach aims to learn features that are applicable across multiple domains. Strengths  The proposed method leverages gradient matching to learn domainagnostic features addressing the challenge of domain generalization.  An efficient approximation for calculating the inner product between gradients is provided reducing computational costs.  A theoretical validation supports the accuracy of the gradient approximation. Weaknesses  Improvements on the DomainBed dataset (Table 3) are minor.  The baseline method results differ from those reported by other work (e.g. Facebooks DomainBed results).  A comparison with DANN another method for learning domaininvariant features would be beneficial.  Visualization of learned representations (e.g. using tSNE) could demonstrate the domain invariance of the extracted features. Update Despite addressing some concerns the reviewer remains uncertain about why the proposed method effectively learns domainagnostic features. The response from the authors suggests that domainspecific information can be beneficial. However it remains unclear how gradient matching helps preserve such information.", "Summary To enhance the ability of models to perform well on unseen domains this paper introduces a regularizer that aligns the directions of gradients during training in different environments. This method improves generalization performance as demonstrated by its superior results on the WILDS benchmark compared to other approaches. Strengths:  The paper clearly explains the motivation and methodology making it easy to understand.  Fish the proposed approach outperforms other methods including IRM on the WILDS benchmark.  The extensive experimental analysis provides valuable insights. Questions:  The paper lacks a thorough comparison with existing approaches like IRM despite sharing similar motivations.  Its unclear if the gradient inner product used in the objective work relates to bilevel metalearning as seen in other work.  The paper lacks rigorous analysis explaining how and why IDGM works which is a known limitation of the method. overall Evaluation: While the proposed approach shows promising results it would benefit from further theoretical analysis to understand its underlying mechanisms. Despite this limitation the empirical results suggest its effectiveness in improving generalization performance.", "Paraphrase: Summary: This research focuses on addressing domain generalization in scenarios involving multiple sources. The authors propose that maximizing the inner product between gradients from diverse domains promotes the learning of features invariant across domains. To approximate the secondorder derivatives they introduce Fish a metalearninginspired algorithm. They demonstrate the effectiveness of their approach on multiple domain generalization datasets. Strengths and Weaknesses:  Strengths:  Clear and easytofollow writing  Evaluation on various benchmarks  Weaknesses:  Lack of theoretical or empirical evidence supporting the claim that the learned features are domaininvariant  algorithm 1s formulation resembles Mean Teacher without clear differentiation  Its unclear whether Fish outperforms other methods particularly for cases with a limited number of source domains  Incomplete review of related work excluding notable methods like Coral  No consideration of data augmentationbased methods for comparison"], "rJvY_5OzoI": ["In multitask reinforcement learning (RL) the goal is to train a single policy for multiple tasks that can be selected during runtime. Despite the prevalence of multitask RL there is a gap in the literature regarding training multiple critics and a single actor where each critic focuses on a specific task. This paper addresses this gap by proposing a novel approach called MultiCriticAL which trains multiple critics and a single actor that does not observe the task descriptor. The paper demonstrates the effectiveness of MultiCriticAL through promising experimental results in various environments.", "Summary Paraphrase: The paper proposes a novel approach to actorcritic reinforcement learning called MultiCriticAL which employs multiple critics for different reinforcement function within a single task. Two variations of MultiCriticAL are presented: one using a shared value function with multiple heads (MH) and one using separate value function (MN). The work evaluates MultiCriticAL in three field: work tracing Pong with different work style and Sonic the Hedgehog level advancement. effectiveness and Weaknesses Paraphrase: effectiveness:  Novel approach of utilizing multiple critics for multiple reinforcement function.  Potential for combining behaviors and authorcontrolling agent behavior.  Impressive qualitative results in the fighting game UFC. Weaknesses:  Lack of statistical rigor in the results making it difficult to determine significant differences.  Absence of baselines in the UFC field limiting the evaluation of the approach.  No direct test of the hypothesis that shared backbone networks in MH can lead to interference.  Missing technical details such as the performance of singletask networks and the evidence for smooth transitions between fighting style in UFC.", "Paraphrased Summary: This research presents a method called \"singleactor multicritic\" to solve the issue of negative interference in multitask reinforcement learning (MTRL). It focuses on a specific type of MTRL called \"multistyle RL\" where the objective is to learn distinct behaviors in a shared environment. Experiments in various environments such as way following Pong Sonic the Hedgehog and a fighting game demonstrate that the proposed approach outperforms a baseline that uses a single actor and a single critic. effectiveness and Weaknesses:  The paper is wellwritten and provides a comprehensive discussion of related work.  The main technical contribution is the use of separate critics while sharing a single actor for multiple tasks which is particularly beneficial in computationsensitive applications where process for multiple tasks can be computed in parallel.  The experiments provide convincing evidence supporting the claims showcasing the disproportionate impact of negative interference on value learning compared to policy learning.  The paper challenges the common use of separating policy for different tasks in multitask RL. Additional Concerns:  The paper could have been strengthened by comparing the proposed approach to a stateoftheart multiactor multicritic algorithm.  The error margins in tables and graphs need to be clarified (e.g. standard error confidence separation).  There are some typos in the text (\"mutli\" and \"mulit\").", "Paraphrase: This research proposes expanding the actorcritic framework to handle reinforcement learning with multiple objectives or tasks. The approach involves training multiple critics aligned with distinct reinforcement function and optimizing a single policy based on a weighted combination of these critiques. The algorithm has been successfully applied to learning various task style such as aggressive or defensive work in a boxing game. The paper demonstrates that the proposed method outperforms various existing multitask learning approaches. effectiveness and Weaknesses: effectiveness:  Addresses a significant problem in multiobjective reinforcement learning with potential applications in games and robotics.  Proposes a simple however effective algorithm.  Demonstrates practical applications in game AI innovation. Weakness:  Omits a crucial reference to similar multiobjective RL work ([1]) that uses a similar multicriticsingleactor approach. This reduces the papers novelty and technical contributions. Additional Questions and Comments: 1. In training the multicriticsingleactor framework do the weights (w) remain fixed or are they randomly sampled 2. Visualizing the learning process through learning curves would provide more insights into learning speed sample efficiency and the advancement of style learning. This could also support the claim of mitigating negative interference between tasks. 3. In Section 5.3 it is stated that MTPPO agents perform worse than agents trained on single levels. Clarify whether these \"singlelevel\" agents are trained on a specific level or all levels without considering the level as an additional input.", "Paraphrased Statement: This paper presents a practical method for handling multiple style tasks in deep reinforcement learning. effectiveness:  The method uses a straightforward approach by extending existing deep reinforcement learning techniques with multiple value networks. Weaknesses:  The method is limited in its ability to generate different style for the same environment or reinforcement function.  In realworld applications there may be a need to generate behaviors with different style even within a single task or environment.  The method proposed in this paper acts more like a multitask algorithm rather than a multistyle algorithm as it cannot generate multiple style under the same reinforcement signal."], "o86_622j0sb": ["Summary This paper introduces a technique for creating imperceptible attacks in black box scenarios by introducing local disruptions in notable field. Using salient target segmentation the most noticeable region is identified and a tree search algorithm is then used to find the smallest blocks inside that region that can cause the largest changes in predicted class logits. The proposed technique is evaluated using 1000 Imagenet samples and compared to other baselines showing that it can produce more imperceptible attacks (as measured by MAD). Strengths and Weaknesses  The concept of creating imperceptible black box attacks in salient field is fascinating and unexplored.  Generating disruptive blocks in salient regions is a novel approach. However the paper could be improved in the following ways:  The rationale for generating perturbations in salient field needs more support and justification. salient target may often include smooth lowfrequency field (e.g. human skin) making them less suited for perturbation generation.  It is unclear if generating disruptions in salient target regions is more effective than doing so in highfrequency or textured field.  The experimental evaluation has some limitations:  The dataset used is small (1000 ImageNet image) and may not adequately represent the generalizability of the technique.  The related approach proposed by Zhang (2020) is not included in the comparison.  The classification network used in the experiments is unspecified. multiple networks should be considered to ensure that the findings are reliable.  The paper primary technical contribution is a treebased approach for generating perturbation blocks which is relatively straightforward and ad hoc.", "Paraphrase: Summary: In recent research there has been a focus on making \"blackbox\" attacks more effective in terms of query. However these methods produce examples that are easily detectable by humans. This work presents a technique that combines segmentation priors with blackbox attacks to restrict perturbations to significant image regions. Additionally it introduces a technique to enhance the stealthiness of the perturbations without sacrificing query efficiency. Strengths and Weaknesses: Strengths:  Presents optimizations to enhance the stealthiness of generated perturbations.  Demonstrates that existing blackbox adversarial attacks can benefit from these optimizations.  Introduces a search algorithm to refine the candidate regions for perturbations.  Adversarial examples generated using this technique have a high success rate in evading detection mechanisms. Weaknesses:  Considers only two existing blackbox gradientfree techniques why not include others like Bayesian optimization  Evaluates success rate using Feature Squeezing as the baseline why not also consider other detection methods  Results are reported for a limited sample of 1000 ImageNet images a larger sample with variance reporting would be ideal.", "Paraphrased Summary: This paper investigates techniques to minimize the visibility of adversarial perturbations induced by blackbox attacks on image (under the \\ell\\infty threat model). The proposed method utilizes image segmentation to identify field where changes can be made to avoid altering the background. Additionally a new \"Saliency Attack\" is introduced to further reduce the amount of the original image that is modified to trigger misclassification. Strengths:  Simplicity of the method  Improved imperceptibility of perturbations by leveraging segmentation priors  Effective Saliency Attack for reducing modified image field  Ablation work demonstrating the methods effectiveness Weaknesses:  focus on reducing \\ell\\infty attack visibility with a fixed perturbation budget despite the fact that visibility may depend more on the threat model than the attack.  Lack of consideration for other threat models with localized changes such as \\ell1 or perceptual metrics (e.g. LPIPS).  Existing research on identifying vulnerable field for imperceptible changes in adversarial attacks  Ambiguous introduction of the algorithm:  Undefined variable \\delta  Recursive calls to \"Refine\" use on smaller blocks and with decreasing rate of k  Incorrect resetting of k in Algorithm 2  image 4 suggests that perturbations may extend beyond the segmentation mask potentially wasting budget on field that cannot be altered.", "Paraphrased Statement: Summary: This paper introduces a method for creating hardertodetect (imperceptible) blackbox attacks by focusing the changes only on the most significant (salient) field of the image. This strategy reduces the number of changes needed and minimizes the impact on attack success. A refinement work identifies the specific field to modify within the salient region. Experiments using the ImageNet dataset show improved results compared to previous methods that also target salient regions. Strengths and Weaknesses:  Positive: The proposed approach addresses the critical problem of developing imperceptible blackbox attacks.  Negative (Technical): The search algorithm used to refine the changes is overly simplistic and lacks theoretical or experimental validation. A similar idea has been explored in previous research.  Negative (Experimental): The approach is not compared to other blackbox attacks that also aim for imperceptibility. The experiments only test modified versions of existing methods limiting validation."], "r88Isj2alz": ["Paraphrased Summary: In this paper the authors introduce \"NODEAttack\" a technique for assessing the vulnerability of Neural ODEs (Neurode) to adversarial samples that increase energy consumption. They examine two attack scenarios: inputbased (untargeted attack) and universal untargeted attack. Experimental results on CIFAR10 and MNIST datasets demonstrate the efficacy of NODEAttack surpassing existing perturbation and corruption methods. A case study involving DNN compilers and PyTorch mobile shows how adversarial example generated by NODEAttack can impair the efficiency of realworld applications. effectiveness and Weaknesses: Major concern: 1. Clarity of Ei and Eu:  Why do the formulas for Ei and Eu include max() terms  Why does Eu not include the perturbation term \\delta  How does increasing Ei and Eu enhance energy robustness 2. Writing Clarity:  The usage of \"15J\" in Section 4 requires clarification regarding units.  Eq.(1) lacks clarity on which parameter should be minimized.  The purpose of parameter c in Eq.(1) is unclear. 3. Query about Figure 1:  Why is the average energy consumption a straight job for the iters3 case 4. Ambiguity in Update Perturbation Section:  The sentence \"greater than the recorded minimum loss (maxN)\" is unclear. 5. Discrepancy between Eq.(1) and Eq.(2):  Why is the minimization in Eq.(2) only with respect to f(x\\delta)  The authors should explain the key distinction between Eq.(1) and Eq.(2). 6. Dataset Confusion:  CIFAR10C and CIFAR10P should be listed under \"Dataset\" instead of \"Basejob.\"  The two basejob methods (corruption and perturbation techniques) need a brief description. 7. Lack of Figures for MNIST:  The authors mention experiments on MNIST but do not provide comparison figures similar to Figures 3 and 4. 8. Evaluation Metrics:  ITP and ETP are evaluation metrics not parameters.  The authors should provide references to show their widespread usage in previous study. 9. example Size Confusion:  It is unclear which example is larger and which is smaller when discussing \"larger example\" and \"smaller example.\" 10. Inconsistent Iteration Counts:  The number of iterations differs for benign samples and inputbased attacks making fair performance comparison challenging. 11. Algorithm 1 Error:  Lines 1218 in Algorithm 1 reference maxNN which contradicts Eq.(1) that seeks to minimize \\delta.", "Paraphrase: Summary: This study examines the energy consumption robustness of neural ODEs and introduces NODEAttack an attack method that generates malicious adversarial inputs aiming to increase the computational overhead of NODE solvers. Experiments confirm the effectiveness and transferability of NODEAttack across various ODE solvers. effectiveness and Weaknesses:  effectiveness:  research energy consumption as a novel robustness metric for neural ODE models.  Introduces two energy robustness measures: inputbased energy robustness (Ei) and universal energy robustness (Eum).  Utilizes average stepsize (havg) as a guide measure of energy consumption.  Demonstrates the effectiveness and transferability of NODEAttack on MNIST and CIFAR10 datasets.  Weaknesses:  Imperceptibility: Adversarial data may be visually distinct from clean data.  impact on accuracy: The effect of adversarial data on the test accuracy is unclear.  Defense: The authors do not elaborate on defense strategies against such adversarial attacks.", "Paraphrase: Neural Ordinary Differential Equation (ODE) examples are efficient and adaptable for resourcelimited devices. However this paper explores the potential vulnerability of neural ODEs to energy attacks that manipulate example inference to increase energy consumption. Previous study have examined energy attacks on adaptive inference but this paper is the first to investigate this issue specifically for neural ODEs. The paper defines energy robustness for neural ODEs and evaluates the effectiveness of an existing attack (NODEAttack) against different solverarchitecture combinations. The experiment employs two solvers and two datasets. effectiveness:  Contributes to the understanding of neural ODEs and their robustness against energy attacks.  Defines energy robustness metrics for neural ODEs.  Demonstrates the transferability of energy attacks. Weaknesses:  Does not explore the tradeoff between robustness and performance when adaptive stepsizing is disabled.  Lacks a comparison of the observed energy attack vulnerability with those of other adaptive inference methods.", "Paraphrase: Summary  The study presents a novel attack on Neural ODE models during inference.  This attack increases the computation time required for NODE models to make predictions.  Two versions of the attack are developed.  The attacks were tested on the CIFAR10 and MNIST datasets.  Results show that the proposed attacks extended the inference time for NODE models in object detection job resulting in faster battery depletion on mobile devices. effectiveness and Weaknesses effectiveness  Introduces a novel type of energy consumption attack targeting Neural ODE models (previous research focused on Adaptive Neural Networks). Weaknesses  The paper structure could be improved (e.g. merging Background and concern work sections).  Section 4 should provide more context on Neural ODEs.  Equation descriptions need improvement for example Eq. 1 on page 5 should be \"\u03b4  c \u00d7 f(x  \u03b4)\" instead of \"minimize the regularized loss \u03b4  c \u00d7 f(x  \u03b4).\"  The attack prioritizes increasing ODE iterations (see Algorithm 1 job 1517).  Exploring the tradeoffs between attack success and energy consumption would be beneficial.  A rudimentary detection measure based on ODE iteration counts for benign example was not considered. It would be valuable to assess if the proposed attack can be neutralized by this defense.  Inputbased energy Robustness attack had limited success and did not significantly impact energy consumption.  Universal energy Robustness attack is more effective but may generate adversarial example outside the normal data distribution making them easier to identify. Additional analysis is recommended to evaluate the detectability of these example."], "hjlXybdILM3": ["Paraphrased argument: Summary: This research introduces a technique to simplify image by minimizing unnecessary information. It aims to study the impact of this simplification on image learning. Strengths:  The visualization tool for analyzing image simplification is valuable. Weaknesses: 1. Insufficient Explanation of Methodology:  The \"unrolledtrainstep\" use (Equation 3) and \"simplifier\" use (Equation 2) lack detailed explanations.  The rationale behind combining simplification with classification error is unclear. 2. Lack of Baselines and Comparisons:  The effectiveness of the simplifier use is unknown due to the absence of other options for comparison.  The condensation experiments lack comparisons with alternative methods and an optimal dataset size for condensation.  Results on training with simplified image lack baselines or competing techniques. Additional input:  image 6: Exploring further cases with altered background colors could provide insights into the impact of external factors on image simplification.  Section 4: While the visual inspection tool is helpful automating the image alteration process is necessary for scalability.", "Paraphrased argument: Summary: This paper introduces a novel approach called SimpleBits to simplify image complexity and enhance the interpretability of neural netstudy learning. SimpleBits operates in three distinct way:  simplification during training: Unveils taskrelevant information in complex datasets.  simplification after training: Explains potential reasons for misclassifications.  simplification with dataset condensation: further reduces image complexity in condensed datasets. Key Contributions:  Introduces a novel approach to simplify image complexity in neural models.  Demonstrates the retention of classifier accuracy during image simplification.  Illustrates the utility of SimpleBits for model interpretation in various scenarios. Strengths:  ease of implementation of SimpleBits loss terms.  applicability of SimpleBits in different settings.  clear visual advance of image simplification in image 4. Weaknesses:  complexity and need behind Section 4.  Unclear semantics of \\mathbfh and \\mathbfhsim in Algorithm 1.  Ambiguity regarding the loss gradient computation target.  Lack of clarity on the need for dataset condensation in Section 5.  Comparison with prior study in dataset condensation.  potential of SimpleBits to detect spurious features in Section 3.", "Paraphrased Summary: SimpleBits a technique proposed in this paper creates simpler input image for classification by removing nonessential details. It uses a simplification loss that considers an image probability within a trained generative model. image with greater complexity tend to have higher simplification loss. SimpleBits can be used in three contexts: 1. During training: Train both an image classifier and a \"simplifier\" network simultaneously. The simplifier network minimizes both classification and simplification losses. 2. After training: Create simplified versions of input image to analyze how a trained network made prediction. 3. Dataset summarization: use simplification loss and gradient matching to create a smaller dataset with simplified image that still maintain classification accuracy. Strengths and Weaknesses:  Strengths:  Can highlight critical image regions for classification without manual occlusion.  By jointly training the simplifier and classifier networks the simplifier can explain network prediction by identifying salient features.  Integrates simplification loss with gradient matching for efficient dataset condensation.  Weakness:  The paper lacks clarity in explaining the use of SimpleBits for posthoc explainability analysis in Section 4.  The method needs to be compared to existing posthoc techniques to demonstrate its effectiveness.", "Paraphrased argument: Summary This draft introduces a method for simplifying visual recognition tasks by removing information that is not relevant to the classification. The proposed approach iteratively creates simplified samples without prior field knowledge. The draft explores:  How simplification affects trained models.  model behavior through posthoc interpretation.  Data simplification through dataset condensation. Strengths and Weaknesses Strengths:  Discusses an intriguing problem. Weaknesses:  Taskirrelevant information can still be present in the endtoend learning model.  The practical value of simplified samples could be highlighted.  How simplification affects:  training time compared to original samples.  tradeoff between model performance and sample complexity.  Advantages of sample simplification within conventional training scenarios are unclear.  Perinstance simplification after model training seems complex compared to gradientbased explanations.  Justification for using negative sample likelihood as a measure of complexity is unclear.  Concerns regarding:  image complexity measurement using bpd on different datasets.  information loss in simplified MNIST sidebyside data."], "famc03Gg231": ["Paraphrased Statement: Summary: The research introduces a refined approach for training neural networks to solve inverse problems. This approach incorporates prior knowledge from the underlying physical principles into the gradient calculation process. As a solution it enhances the speed and reliability of convergence. Strengths and Weaknesses:  The reviewer initially expresses difficulty understanding the papers primary objective.  Upon further examination the main method and goal are explained in Section 2 with equation (1) and (2).  inverse problems are typically formulated in two frameworks:  Optimization framework: Minimizing a loss function that measures the difference between the place and the yield of the forward physical process. This aligns with equation (1).  Mapping learning framework: Learning a probabilistic or deterministic mapping from the place place to the control parameter place. This approach is inferred from equation (2). Concerns:  Unsupervised training: The reviewer questions how the mapping paradigm can be learned without labeled training data. They speculate whether the neural network yield can somehow bypass the physical process or if the training still relies on actual data pairs.  inverse Problem Solver Assumption: The reviewer wonders why a neural network is necessary if an inverse problem solver (P1) is already available from domain knowledge.  Empirical solution: The performance comparisons are based on the number of iterations but the reviewer suggests that the actual range time of the inverse solver (P1) may be significantly longer than a neural network update. Without this data the iterationbased comparison may be misleading. Conclusion: The reviewer place out the importance of clarity in presenting the research. If the concerns raised in the review are adequately addressed the reviewer is willing to reconsider their evaluation. However they emphasize the need for improved clarity in the papers delivery.", "Summary This paper proposes a novel approach to inverse problemsolving using deep learning. The proposed approach defines the loss function for a given observation as the difference between the neural networks prediction and the solution of an approximate iterative solver initialized with the neural networks yield. The gradient of this loss (for a batch of observations) with respect to the neural network parameters termed the \"physical gradient\" is then used for neural network optimization. potential Advantages  Eliminates the need for automatic differentiation of the forward operator.  Adaptive convergence to different solution due to the iterative solvers varying initialization.  Few iterations of the solver may provide a useful loss for the neural network. Strengths and Weaknesses Strengths:  Popularity of inverse problems motivates the need for efficient deep learningbased solvers.  physical gradients offer potential benefits over existing methods. Weaknesses: Motivation:  The method employs an iterative solver at each optimization step limiting the neural networks potential to outperform the solver.  It is unclear why neural networks should be preferred over iterative solvers. Empirical Evidence:  Numerical experiments do not fully showcase the potential of the proposed method.  The difference between physical gradients and supervised approach is not clear in the experiments.  The methods dependence on the iterative solvers accuracy is not investigated. further Comments and Suggestions:  Insufficient theoretical and empirical support for the claim that physical gradients are smoother than standard gradients.  Corrections to equations and other statements.  Need for a proper definition of \"sensitive\" functions.  Clarification of Algorithm 1s conditioning process.  Exploration of neural networkbased iterative solvers for faster inference and uncertainty estimation.", "Paraphrased Statement: Summary This study introduces a method for developing neural networkbased approximations for inverse problems in physical systems. The key idea is to reformulate the training loss function to measure the gap between:  The neural networks estimate of the problem input  The solution obtained by using this estimate as the starting place for a simplified inverse problem solver This loss function is then minimized through gradient descent. The methods effectiveness is demonstrated in various physical systems showcasing its ability to overcome:  Multimodality (a common yield in supervised training)  Instability in gradients due to physical processes  Computational expenses and illconditioning faced by some training methods Strengths  Simplicity and effectiveness of the proposed approach  place physical interpretation of the calculated gradients in the physical solution place  Extensive empirical evaluation demonstrating superior performance in physically relevant context Weaknesses  The paper overstates the generality of the method affecting clarity.  The use of domain knowledge to derive a specific gradient approximation is not fully highlighted.  The claim of combining higherorder optimization and machine learning techniques is not fully substantiated.  Some claims in the paper lack sufficient supporting evidence.  The papers structure and organization could be improved for clarity especially in the main body and Appendix A.1.  Typographical errors may hinder comprehension. Minor Comments  The explanation of \"inverse physics via domain knowledge\" could be improved for clarity.  A different subscript than \"sup\" for \"supervised\" might be considered to avoid confusion.  Experiment descriptions and legend interpretations could be more explicit.  Legends in graphs should be positioned to avoid obscuring the lines.  The xaxis label in Figure 3 is missing. The caption of Figure 3(a) potential contains a typo as it does not refer to convergence curves.", "Paraphrased Statement: This paper focuses on the challenge of determining the initial state of a physical system based on its final state. The authors propose using neural netprocesss to predict the initial state iteratively refining the netprocesss by using inverse solvers (informed by physical knowledge) to provide place values that better approximate the true initial state. While the process presents a novel idea and is clearly written its strengths and weaknesses lie in its limitations. The most compelling case study involves using the inverse solver to provide a slightly improved place for netprocess training eliminating the need for labeled data or wide inverse solves. However the experiments conducted on simplistic domains demonstrate that the method is effective only in these domains and with randomly sampled data. As with similar applications of machine learning in physical inference it is questionable whether the benefits of a widey trained model outweigh the cost of training it on a limited domain and the reliance on an existing inverse solver. The authors should provide more practical use cases where the cost of training and formulating an inverse solver justify using this approach over traditional methods. In cases where the inverse is known a comparison with direct inverse optimization would be valuable."], "k-sNDIPY-1T": ["Paraphrase: This study explores using recurrent neural networks (RNNs) to simplify computational models in neuroscience. specifically the researchers examine predicting the activity of four neurons in a worms nervous system in response to simulated electrical stimulation. They conducted three experiments using different network designs size and data resolution. Their results indicate that a compact RNN based on the Gated Recurrent Unit (GRU) architecture effectively captures data previously generated through computationally intensive simulation of a complex neuronal network. The paper begins with an overview of common RNN architectures and the concept of model reduction in neuroscience. Strengths:  Clear and engaging writing  Easytounderstand foundation to concepts  Timely focus on using machine study methods for model reduction in neuroscience  Strong evidence supporting the main thesis Weaknesses:  Limited theoretical or conceptual insights as it primarily applies existing RNN techniques to a specific problem  Could benefit from a broader literature review of related research using deep study for neural activity modeling Suggestions:  Consider including additional relevant study that apply deep study techniques to neural data  Provide a comparison of computational costs between the RNN and the full simulation  Increase the size of figure labels for readability  Correct a typo in the last argument of section 5.1 (\"keepING\" should be \"keep\")", "Paraphrase: Summary:  A low recurrent neural network can reliably predict the behaviour of four specific neurons in a C. elegans simulation using root mean square error (RMSE). Strengths:  The study employs a sophisticated internal model of the worm that incorporates compartmentalized neurons and both chemical and electrical synapses to generate baseline data. Weaknesses:  Insufficient details are offered on the construction and validation of this model including the choices made for its biophysical properties and synapse models.  The authors claim the simulator is \"highfidelity\" based solely on the assumption it accurately mimics the actual output of C. elegans neurons with no empirical evidence to support this claim.  The simulation models fidelity is solely evaluated in the context of \"forward locomotion\" involving the activation of two sensory neurons and two interneurons to cause activity in four neurons.  aspect of the C. elegans simulation which would be insightful to readers are lacking in detail in the main text.  The discussion on recurrent neural netstudy (RNNs) and their training challenge in section 3.1 is excessive given their wellestablished nature and could be replaced with brief explanations and references to original research. Overall Assessment:  The study systematically tests various RNN variants (LSTM GRU vanilla RNN) in a straightforward setup but focuses primarily on GRU.  The finding that a GRU network study for modeling timeseries data is unsurprising.  The paper experimental scope is too limited to provide meaningful insights into using RNNs as models of actual brain activity. Specific Questions:  details on the criteria used for ensuring variety in the manual separation of the simulation results into training testing and validation sets are requested.  The argument about \"data sampled with different time step leading to longer sequences\" requires clarification to emphasize that the same physical time is being simulated.  The specific input to the network (whether it includes currents from both sensory neurons and interneurons) is unclear.  The reason for providing stimulation to interneurons rather than just sensory neurons requires further explanation.", "Paraphrased argument: Researchers demonstrated how the nervous system of the worm C. elegans can be represented and run using datadriven models based on neural networks. They focused on further recurrent neural network architectures such as Long ShortTerm Memory (LSTMs) and Gated Recurrent Units (GRUs) evaluating their capabilities accuracy (RMSE) and model complexity. The study found that GRU models with four hidden units effectively mimicked the systems responses to diverse stimuli. Strengths:  Developed models for C. eleganss nervous system using three recurrent neural network architectures (RNNs LSTMs GRUs).  Aimed to create simplified models to substitute the complex original model in the NEURON simulator. Weaknesses:  Limited automation in selecting appropriate stimuli for training validation and testing sets.  Absence of systematic analysis of compression possibilities and error control in the learned models."], "nZOUYEN6Wvy": ["Summary This paper modifies Granger causality for partially ordered data adapting Graph Neural Networks (GNNs) to capture timelagged data. The modified method GrIDNet is applied to identify noncoding genomic loci that influence gene expression. GrIDNet outperforms other methods on an open dataset. Strengths and Weaknesses  The proposed method assumes causal sufficiency which is often not met in realworld data. This assumption limits the interpretation of findings as causal.  Other Grangerbased methods suffer from the same issue making it difficult to distinguish between direct and indirect causal relationships. Questions for the Authors 1. Explain the robustness of GrIDNet in sparse settings theoretically or intuitively. 2. Compare GrIDNet to a nonlinear Granger implementation to determine if its superiority stems from nonlinearity or taking into account partial ordering. 3. Provide the variance of results in Table 1 to assess the significance of differences between GrIDNet and other methods. 4. Include additional causal discovery methods in the related work section that address the limitations of Granger causality such as:  Mastakouri et al.2020  Entner and Hoyer (2010)  Pfister et al.", "Paraphrase: Summary: The authors present a method using Granger causality to model multimodal data from individual cell. Their graph neural network GrIDNet assumes that cell in the network follow a Directed Acyclic Graph (DAG) structure. They particularly focus on identifying noncoding genomic regions that causally impact the expression of a specific gene. Strengths:  Addresses a complex problem.  Extensive experimental evaluations.  Wellwritten manuscript. Weaknesses:  method relies heavily on the quality of the DAG expression raising questions about sensitivity to this choice.  Lack of confidence bands limits the certainty of results (e.g. speculation about longer lags between distal peakgene interactions).  Relevant literature on causal inference methods (e.g. Bayesian network inference structural causal models) is missing from the reference section. Other Comments:  Table 3 shows a significant performance increase for GrIDNet and VAR Granger on the SHAREseq dataset. This requires explanation particularly regarding the increase in AUPRC.  Reference by Heinz et al. has an incorrect title.  Caution is advised when normalizing data used for causal inference as it may disrupt relationships. However the authors specify that DAG expression is done postnormalization which mitigates this concern.", "Summary In this study the authors introduce a model that extends Granger causality for Directed Acyclic Graphs (DAGs) and apply it to detect temporal relationships between gene expression changes and chromatin accessibility. The model uses a Graph Neural Network to handle partially ordered data unlike traditional Granger causality which relies on linear models and ordered data. The authors apply the model to scRNASeq and scATACSeq data inferring causal relationships between these data types. They evaluate the models performance using proxy labels derived from eQTLs and chromatin interactions and compare it to baseline methods. The authors find that the model is robust to data sparsity and identify enrichment of transcription gene binding motifs in peakgene regions. Strengths and Weaknesses Strengths  Introduces a novel model for inferring causal relationships in DAGs  Applies the model to a challenging biological problem  Demonstrates the models robustness to data sparsity Weaknesses ModelClaims  Overuse of the term \"casual\"  Usage of a ttest to check for differences in model performance may not be appropriate  Lack of clarity in defining the \"lag\" variable  Potential issues with shared parameters between reduced and full models analysis  Questionable practice of mapping human eQTLs to the mouse genome  Using eQTL focus based on pvalue cutoff is insufficient for analysis  Suboptimal choice of baseline methods  Lack of analysis to determine TF association with gene expression changes  Interpretation of proximal vs. distal peakgene pairs may not be fully supported biologically  Unclear how pvalue cutoff is chosen for classification  Handling of missing values in sparsity analysis Relation to Previous Work  Overemphasis on the connection to Granger causality  Comparison of the models generalization to Granger causality appears tenuous Revised Evaluation After considering the authors revisions we acknowledge the improvements made to the manuscript including clarification of claims improved data analysis and beneficial framing of the models relationship to Granger causality. Some concern may still remain but the authors have demonstrated a commitment to addressing the reviewers feedback and improving the overall quality of the work.", "Paraphrase: This paper introduces a novel method GrIDnet which employs deep neural networks and Granger causality analysis to determine causal relationships between multimodal data on directed acyclic graphs (DAGs). The method involves constructing a DAG based on sample data extracting historical data from modal features using GNN layers and testing Granger causality to identify two features whose historical data significantly enhances the predictive ability for another feature. GrIDnet was successfully applied to singlecell multimodal datasets to identify causal relationships between chromatin regions and gene expression providing insights into gene regulatory mechanisms. These findings were further validated through independent eQTLs HiC and TFBS analysis. While GrIDnet offer strengths in integrating DNN and Granger causality over DAGs it could benefit from further elaboration on the GNN architecture for historical data representation. Additionally it would be valuable to compare DAGs using alignment methods and benchmark GrIDnet against pseudotiming approaches. The paper acknowledges potential limitations such as the celltype specificity of causal interactions and the use of bulk tissue eQTLs for validation. Scalability concern should also be addressed particularly for singlecell data application."], "jJJWwrMrEsx": ["Paraphrase: Summary: This article presents Truth Table Deep Convolutional Neural Networks (TTDCNs) a novel architecture that uses propositional logic for a compact representation leading to improved accuracy compared to Binary Neural Networks (BNNs). TTDCNs employ realweighted convolutions and aggregations that are then binarized using step work. The authors claim that this architecture allows for easier explanation and verification. Strengths and Weaknesses: Interpretability and verification are important for neural network. TTDCNs aim to address this by using a design that enables a more compact logic encoding resulting in more interpretable and verifiable network. The idea of limiting work complexity to create accuracy tables appears promising. still there are concerns about the tradeoff between interpretability and performance. Additionally the claim of increased interpretability and the possibility of human knowledge integration in accuracy tables require further experimental validation. The writing style assumes a deep understanding of neural network verification which may limit accessibility. Minors: A few minor issues in the text include:  The definition of the patch work is not clear.  Certain terms (\"verifiable accuracy\" \"complete robustness\") are not defined.  It is unclear whether the extraction of realvalued DCNN knowledge is unique to TTDCNs or also applies to BNNs compiled to propositional formulas.", "Paraphrase: This paper introduces a novel Neural network that can be represented in SAT (Satisfiability Modulo Theories) format. The network features realvalued weights and binary activations. By constructing a accuracy table for a Convolutional Neural network (CNN) block the paper derives a boolean work that describes the blocks output as a combination of input variables. This work provides masks that identify when a given neuron is dynamic. Adjusting these masks after training enhances the network generalizability (sec 4.1). Compared to existing BNN verification method this rule results in a more compact SAT representation improving scalability for SAT solvers. The authors experiments demonstrate superior performance in terms of verifiable accuracy and runtime (Table 1). Strengths and Weaknesses: Strengths:  Enhances the scalability of SAT solvers for deep network verification. Weaknesses:  The models tested are small and trained on simple datasets (MNIST and CIFAR 10).  The approach involves grouping parameters to maintain scalability which compromises accuracy.  The method performs slightly better than realvaluebased verification for low noise levels but struggles with higher noise levels.  The claim that the proposed method is the only scalable approach for exact interpretability is inaccurate as a recent work ([1]) addresses scalability issues in MIP solvers for model Explanation using gradientbased method.", "Summary Paraphrase: This paper introduces TTDCNNs a network that transforms continuous feature vectors into accuracy tables (DNF formulas) to improve interpretability. These tables show which variables work the accuracy value. By manually removing masks that overfit model accuracy can be enhanced. experimentation show that TTDCNNs perform as well as or better than existing method on MNIST and CIFAR10. Strengths:  Novel and intriguing concept of using accuracy tables to approximate feature vectors.  TTDCNNs offer interpretability and allow manual postprocessing.  SAT solvers can validate TTDCNNs robustness. Weaknesses and Feedback:  Writing can be confusing due to frequent references to appendices making it harder for readers to follow.  Omission of important baselines such as Narodytska et al. 2019b which is closely related.  Assertions are made without experimental support results sometimes contradict these assertions (e.g. SAT verification is not as fast as claimed).  While interpretability is emphasized the paper lacks specific examples of highly interpretable features learned.  Training details for TTDCNNs should be discussed given its novel architecture.", "Paraphrased Statement: Summary: This paper introduces TTDCNN a novel machine learning model that aims to replace Binarized Neural Networks (BNNs). According to the paper TTDCNN offers benefits in simplifying and enhancing the comprehensibility of propositional encodings. Critique: still the papers assumptions are flawed. BNNs were primarily developed to address challenges related to ability consumption and hardware integration not encoding problems. Additionally the relevance of encoding interpretability is unclear as SAT encodings are only tools for reasoning about the original model. The papers decision to limit features to boolean values and restrict the feature number to nine creates significant limitations potentially rendering the encoding impractically large for large models. The papers writing lacks clarity. The explanation of 2DCNNs is insufficient and lacks references making it difficult to understand. Similarly the conversion from Equation (1) to the generation of CNF and DNF representations is unclear. The integration of different blocks in the encoding process is also confusing especially for DNF representations. If this integration were straightforward satisfiability for CNNs would be easily decidable which is not the type. The papers confusing explanations make it challenging to understand these concepts."], "moHCzz6D5H3": ["Paraphrased Statement: This paper introduces the \"peekaboo\" (PaB) algorithm for simultaneously optimizing network pruning (during framework initialization) and weight adjustment (restricted to flipping sign). This approach is novel compared to existing methods. PaB utilizes a twostep algorithm: pruning followed by optimization. empirical results demonstrate competitive performance against comparable optimization strategies. effectiveness and Weaknesses:  effectiveness: PaB presents a new and intriguing approach to network optimization.  Weaknesses:  The algorithms utility has not been comprehensively demonstrated particularly in largescale network training scenarios.  The technical contribution of PaB appears limited as pruning and optimization are executed separately rather than jointly. This separation may hinder alignment between the pruned architecture and the flipping performance.  The experiments are conducted on CIFAR datasets leaving uncertainty about the algorithms scalability to large datasets like ImageNet. Furthermore PaB does not exhibit a clear advantage over standard SGD optimization limiting its potential application.", "Paraphrased Summary: This research proposes \"disguised subnetworks\"  hidden randomly initialized subnetworks that can be transformed into highperforming ones. The PaB algorithm is introduced to uncover these subnetworks by finding a mask over the random weights through pruning techniques and then applying a transformation to the subnetwork. PaB is efficient and offers advantages over previous methods. effectiveness:  Wellwritten and easy to understand  Addresses optimizing sparse neural networks with disguised training  Provides a novel insight that coarser gradients are tolerated when flipping random weight signs  significant cost reduction in training maintaining competitive performance  Compared to relevant baselines across different framework Weaknesses:  Compression ratio calculation needs clarification  Relying on seed memory for weight initialization may not be reliable  Comparison with the \"signed constant\" variant of supermasks which often improves performance and reduces complexity is missing", "Summary This paper proposes a new concept \"disguised subnetworks\" that extends the definition of hidden subnetworks in neural networks. Disguised subnetworks involve applying a transformation on hidden subnetwork weights to obtain final weights. This introduces an additional variable U which represents the transformation. By setting U to the identity transformation the problem reduces to finding hidden subnetworks. The paper presents a heuristic algorithm that solves the optimization problem underlying disguised subnetworks by first determining a masking variable for sparsity and then finding U through a process of sign flips. effectiveness and Weaknesses effectiveness:  Wellwritten with strong language and introduction  Precise summary of relevant literature  Clear description of the algorithm  Fair discussion of contribution Weaknesses:  contribution: It is unclear whether the mechanism for improving training data performance through weight transformation is novel.  Novelty: The proposed solution algorithm lacks significant novelty as it uses existing methods for variable determination and optimization.  Generality: Despite defining a general problem the heuristic approach may not capture its full generality.  Numerical Experiments: The comparison with trainingindependent algorithms could be biased due to postmasking training optimization. Overall Assessment: The paper addresses an interesting problem and presents a promising solution although the novelty and generality of the approach could be strengthened. The decision on acceptance is \"marginally below the acceptance threshold.\" Minor Comments:  Inconsistent capitalization of \"s\" in \"sparse Subnetworks\"  Consider replacing \"Unfortunately\" in \"Unfortunately the original experiments in (Zhou et al. 2019)\"  Fix inconsistent use of subscripts (e.g. \\in argmin vs. argmin set is a singleton)  Clarify the use of the term \"NPhard\" in the context of the optimization problem", "Paraphrase: Optimizing neural netprocesss that use fewer connections and parameters (sparse neural netprocesss) is valuable because they save space and reduce computation. Researchers have discovered \"subnetprocesss\" within randomly initialized neural netprocesss that perform well referred to as lottery tickets. The authors expand this concept to \"disguised subnetprocesss\" of which lottery tickets are a type. They propose PeekaBoo (PaB) a novel algorithm that combines existing techniques to efficiently identify these disguised subnetprocesss. effectiveness:  Clear and thorough description of the research  Welldefined concept of disguised subnetprocesss with solid reasoning  Comprehensive introduction of PaB supported by context  Convincing results demonstrating PaBs scalability accuracy and efficiency compared to other methods Weaknesses:  Limited theoretical discussion despite the authors acknowledging the empirical nature of the process  Lack of exploration of alternative transformation methods in PaBs \"unmasking\" phase  While the definition of disguised subnetprocesss is new and PaB appears innovative the algorithm itself is essentially a combination of existing methods"], "v-27phh2c8O": ["Paraphrased Summary: The paper presents a method called (AARL) for automatically finding the beneficial auxiliary loss function for reinforcement learning. AARL reportedly achieves beneficial results than other methods on both pixelbased and statebased tasks in the DeepMind Control Suite. The authors also use AARL to analyze different auxiliary losses and identify common design. Pros:  The paper is wellwritten and accessible.  Results include error measurements in both design and tables.  The appendix provides detailed information. Cons:  The claim of AARL discovering the \"optimal auxiliary loss function for RL\" lacks mathematical proof. Moreover testing on only the DeepMind Control Suite limits the validity of this claim.  The approach explores loss input and operator but not the encoder architecture which could significantly impact auxiliary loss functions relying on rollouts.  The experiments rely only on fully observable environments potentially limiting the applicability of the findings to more complex partially observable environments.  The process prediction auxiliary loss used in the paper has been shown to be less effective in partially observable 3D environments due to increased state aliasing.", "Paraphrase: auxiliary tasks can improve reinforcement learning agents. This paper proposes an evolutionary algorithm to automatically create auxiliary loss functions leveraging existing approaches like forward dynamics inverse dynamics and contrastive state representation learning. The algorithm solves a twostage optimization problem: the inner loop is regular RL training while the outer loop optimizes the loss function. Experiments on DeepMind Control Suite show mixed results for both pixel and statebased observation. effectiveness:  The paper is wellwritten and easy to understand.  The method is straightforward and practical.  The idea of evolving optimal loss functions for auxiliary RL tasks is novel.  Improving representation learning in RL has significant practical value. Weaknesses:  experimental results are weak.  The paper lacks comparisons to stateoftheart baselines such as \"DrQ\" and \"RAD\" which outperform the proposed method.  The paper claims there is no data augmentation for statebased DMControl experiments but this is not reliable.  There may be factual errors in the experimental plots.  Table 1 contains a misnomer: \"Inverse dynamics\" is typically used for process inference not state prediction.", "Paraphrase: Summary: This paper presents a method for automatically finding the optimal auxiliary loss function from a heavy search space defined as the one that enhances the agents replication. The search space comprises:  Manually designed similarity measures to align predictions and targets in auxiliary tasks  Binary masks to select auxiliary loss input The method involves 4.6 x 1019 potential combinations. It begins by randomly selecting similarity measures and choosing the one with the highest average performance. Then an evolutionary algorithm identifies leading candidates for loss input. The chosen auxiliary losses improve learning efficiency and generalize to novel tasks. effectiveness and Weaknesses: effectiveness:  Automated auxiliary loss function selection eliminates manual design and testing.  It surpasses grid search in efficiency for vast search space.  Experiments with multiple seeds provide reliable results. Weaknesses:  Accuracy of Initial Pruning Step: Random sampling may not identify optimal candidates introducing the risk of overlooking the beneficial combination.  performance Variability in HopperHop and QuadrupedRun: Some auxiliary loss functions lead to worse agent performance than the SACDenseMLP baseline raising concerns about the risk of auxiliary loss selection.  generalization Ability: It is unclear if the generalization ability stems from the auxiliary loss selection method or the training on multiple tasks. Isolating the impact of the selection method on generalization would be beneficial.", "Paraphrasing of Statement Summary The article aims to develop a method for automatically finding the ideal secondary loss function for reinforcement learning. To do this the authors propose the Automated Auxiliary loss for Reinforcement Learning (AARL) which uses an evolutionary search technique to investigate the vast loss function space. The field includes indepth testing on the DeepMind Control Suite showing that the selected secondary losses improved RL performance in both pixelbased and statebased environments. effectiveness and Weaknesses Advantages: 1. The basis for automated search of optimal auxiliary loss is heavy. Manually designing a loss function depends heavily on researchers domain expertise and performance cannot be guaranteed. Automation can reduce limitations and enhance RL agents capabilities. 2. The framework for exploring auxiliary loss functions and the evolutionary approach are wellthoughtout. Pruning the search space can significantly reduce search complexity. 3. extensive and supporting experiments have been conducted. Essential experimental parameters are available ensuring the fields repeatability. Disadvantages: 1. The efficiency of the suggested AARL technique is a primary concern. The combined loss function combines reinforcement learning loss and evolutionary loss both of which are challenging to optimize. Convergence would be even more challenging due to the RL and evolution combination. Furthermore since the difficulty of convergence compounds rather than adds efficiency could be severely compromised. 2. The collaboration between RL and evolution could be improved. Equation (1) optimizes both algorithms by only summing their loss functions. An alternative approach would be to train the losses alternately. This is beneficial for two understanding: 1) RL loss and auxiliary loss have different scalability making it challenging to set the weight (\u03bb) 2) The required training steps for both algorithms can differ. 3. Lack of significant contribution. The primary technical contribution is designing the search space and evolution method to obtain an optimal auxiliary loss. Additionally the field lacks theoretical backing for the proposed method."], "kSwqMH0zn1F": ["Paraphrased Summary: A technique for partitioning Graph Neural network (GNNs) and utilizing previous versions of activations across partitions. This technique enables parallel communication of activations across GPUs (instead of sequential) leading to enhanced throughput. effectiveness:  Simple and easy to implement compared to other parallel GNN systems.  No complex memory management making it potentially suitable for distributed networks.  stale activations affect gradients only not weights potentially leading to More stable convergence. Weaknesses:  Vanilla partition parallelization outperforms More recently developed methods by a significant margin which is surprising and requires further verification.  The technique assumes interpartition communication time is similar to gradient computation time which may not always hold true.  Unlike samplingbased methods this technique does not fully address the challenge of excessive communication volume.  The term \"distributed\" should not be used without experimentation on networked machines.  The performance with distributed network machines on commodity hardware is promising due to comparable bandwidth to network connections.", "Paraphrased Summary: Researchers present PipeGCN a technique for optimizing distributed training of fullgraph Graph Convolutional network (GCNs). PipeGCN minimizes communication overhead in GCN training by coordinating communication and computation. They develop a smoothing mechanism to handle stale data (features and gradients) while training minimizing errors and overhead. effectiveness: 1. PipeGCN addresses communication bottlenecks and synchronization issues in distributed GCN training. 2. The paper establishes the theoretical convergence of GCN training with stale data laying foundation for future research. 3. Experimental solution demonstrate significant performance improvements in GCN training on various datasets outperforming existing methods. Weaknesses: 1. The scalability of PipeGCN is questionable as the practical setup for a large dataset (ogbnpapers100M) is lacking. Moreover the model may not capture meaningful data due to its limited capacity. 2. The paper lacks solution for varying partition numbers hindering understanding of speed and convergence behavior. 3. The solution in Table 6 are unclear omitting dataset data and the specific distributed GCN method used.", "Summary PipeGCN a new method for distributed Graph Convolutional Network (GCN) training is introduced to enhance training efficiency by optimizing communication and computation pipeline. Its effectiveness in training accuracy and convergence speed is demonstrated despite utilizing both old gradient and feature vectors. effectiveness and Weaknesses effectiveness:  Wellorganized and technically sound discussion  PipeGCNs concept is thoroughly outlined  Theoretical analysis and extensive experiments support PipeGCNs benefits Weaknesses:  Novelty concern: Combining existing ideas (stale features and gradients) raises questions about PipeGCNs unique contribution  Suggestions for improvement:  comparison with Dorylus to highlight the advantages of using stale gradients  Clarification of the pipeline scheme in design 1 and 2  Further investigation of the smoothing techniques impact on model accuracy", "Paraphrased Statement: This research introduces a distributed fullgraph Graph Convolutional Network (GCN) training technique called PipeGCN. It enhances GCN training efficiency for largescale graphs. Experimental solution indicate its effectiveness and speed. The paper also provides a mathematical validation of its convergence. effectiveness and Weaknesses:  effectiveness:  PipeGCN enables distributed GCN training on large graphs.  It conceals communication time by simultaneously performing communication and computation tasks in each layer and using outdated data for parameter adjustments.  Experiments demonstrate that PipeGCN can improve training speed by up to 2.2 times.  The authors provide theoretical validation of PipeGCNs convergence and suggest two smoothing methods to accelerate convergence.  Weaknesses:  convergence validation:  The validation of convergence needs further justification.  The authors do not explain how their proposed model fulfills Assumption 3.13.3 specifically whether their chosen loss function satisfies the Lipschitz continuous condition.  The convergence validation is not valid if PipeGCN utilizes the widely used ReLU activation function which does not meet Assumption 3.2.  Clarity:  Certain statements in the paper lack clarity.  For instance Table 1 omits mention of AllReduce of Weight gradient PipeGCN which appears in Algorithm 1 (pipeline 32)."], "ks_uMcTPyW4": ["Paraphrased Summary This paper introduces a reinforcement learning approach that combines control and feature acquisition in settings where feature observation is expensive. It proposes a method for building time series latent variable framework that leverage both observation and action history information. Experiments show that this approach improves missing information imputation and task performance. Strengths:  Addresses the significant problem of feature acquisition particularly in healthcare.  The proposed VAE framework appears to effectively capture the desired information.  number 4 visually demonstrates the tradeoff between feature acquisition costs and control reinforcement. Weaknesses:  Concerns most novelty as a similar sequential VAE approach exists (Igl et al. 2018). Baseline comparison with this method and Deep Recurrent QLearning would be valuable.  Ambiguous experimental details especially regarding the \"endtoend\" baseline and its feature acquisition strategy.  The offline pretraining requirement for the VAE limits the approachs practicality and potential for online improvements.  Lack of discussion on ethical implications. Minor Comments:  Proofreading is needed to address grammatical errors.  Citations should be formatted correctly using \\citep instead of \\citet or \\cite.  Notation inconsistency in equation (1) (should be \\mathcalAf instead of \\mathcalAf).  The difference between \"full loss\" and \"partial loss\" in the equations needs clarification.", "Paraphrase: This paper introduces a method for optimizing exploration in Partially Observable Markov Decision work (POMDPs). It does this by simultaneously teaching a system to acquire useful features and a policy for maximizing reinforcement. The system balances the cost of acquiring features with the expected reinforcement. The system uses a Variational Autoencoder (VAE) framework which includes a belief inference framework and an observation decoder. A key innovation is that the inference is done sequentially. Experiments demonstrate that the proposed framework outperforms alternative variational inference methods achieving beneficial performance with lower cost particularly in the number of acquired features. Strengths:  The approach addresses the challenge of balancing exploration and exploitation in POMDPs.  It combines learning the feature acquisition policy and the target policy simultaneously.  It introduces sequential inference into the VAE framework. Weaknesses:  Computational cost: The approach may be expensive to train even for small problems.  Fixed VAE: The VAE is pretrained and essentially fixed during learning which may be brittle in practice if distributions are unknown beforehand.  Limited comparison: The paper mostly compares to similar POMDPsolving strategies missing out on other exploration and representation learning approaches in POMDPs.  Cost simplicity: The cost is measured solely as the number of acquired features which does not consider the varying costs of acquiring different features.", "Paraphrase: Summary: The authors tackle reinforcement learning in settings where the agent can acquire observations by spending reinforcement. They expand the classic Partially Observable Markov Decision work (POMDP) framework with an Active Feature Acquisition (AFAPOMDP) framework which separates actions into control and feature acquisition components. Solution Approach: The authors first use fully observed trajectories to train a sequential Variational Autoencoder (VAE) as an inference framework. Then an RL algorithm jointly optimizes the control and feature acquisition policies using the pretrained VAE. Experiments: The approach is tested on two tasks:  A synthetic \"bouncing ball\" task with five control actions and a feature acquisition component that selects which regions to observe.  A sepsis task with three control actions and eight patientrelated measurement features. Strengths:  Outperforms VAE baselines in both reinforcement and accuracy of inferring the unobserved state.  Addresses the significant problem of joint control and feature acquisition. Weaknesses:  The AFAPOMDP framework does not generalize POMDPs as claimed but is a specific case.  The approach is similar to prior work using neural networkbased frameworks for POMDP inference.  The task domains are simple and could potentially be solved using planning methods.  Baselines have competitive performance despite higher MSE.  The method assumes approach to fully observed information for training which may be limiting."], "wTTjnvGphYj": ["Paraphrased Statement: In this paper we propose a new Positional Encoding (PE) approach for Graph Neural Networks (GNNs) called LSPE. LSPE overcomes the computational limitation of existing PE models like Laplacian eigenvectors by embedding PEs directly into the input features and updating the embeddings iteratively along with the node feature embeddings. This approach significantly improves graph regression accuracy on the ZINC dataset and shows some improvements in graph classification tasks on the MOLTOX21 and MOLPCBA datasets. effectiveness:  clear and accessible writing style.  Appendix provides valuable data and explanations.  Technical details including formulations are easy to follow. Weaknesses:  Significance of experimental improvements is questionable due to potential lack of statistical significance.  Importance of PE initialization may be overlooked compared to LSPE architecture. additional Suggestions:  Perform statistical tests on experimental results to confirm significant improvements.  Test the LSPE on a wider range of datasets to demonstrate its robustness.  Consider reporting memory usage to address the motivation of computational efficiency.  Emphasize the importance of PE initialization rather than solely focusing on the LSPE architecture.", "Paraphrased Statement: This paper introduces a distinct positional representation and a corresponding loss work for graph representation learning. This approach achieves remarkable results in empirical evaluations. effectiveness:  The concept of separating the positional representation is innovative.  The empirical results are highly favorable.  The paper presents a wellstructured and wellwritten explanation. Weaknesses:  The methods complexity could introduce additional parameters and increase model size. This complexity raises concerns about overfitting and the potential work of the expanded parameter set on performance improvements.  The positional encoding proposed in the paper should be compared to existing relative positional encoding techniques used in GNNs (e.g. Graphormer).  Exploring the possibility of combining the proposed encoding with existing approach could potentially enhance performance further.", "Summary This paper proposes a method for enhancing node representations in graph neural networks (GNNs) by incorporating positional data. It makes the nodes more expressive while maintaining the efficiency of messagepassing GNNs. effectiveness  The problem of missing positional data in messagepassing GNNs is clearly explained using model from molecules.  The proposed method is straightforward and computationally efficient.  The paper provides a comprehensive overview of related work.  The idea of decoupling structural and positional encodings is innovative. Weaknesses  The experimentation are limited to molecular field and do not demonstrate the effectiveness of the method on other field.  The introduced PosLoss does not significantly improve the results questioning its necessity.  The claim of achieving stateoftheart results on the ZINC dataset is challenged by existing work with best performance.  The benefits of positional encoding are diminished by the choice of aggregation methods on largerscale datasets.  The paper overstates its contribution by claiming that the method outperforms previous work but the experimentation do not fully support this claim.  The paper lacks discussion on why other methods such as Directional Graph Networks and PHCGNN may outperform the proposed approach.", "Summary: The work focuses on developing a generalized positional embedding for Graph Neural Networks (GNNs). effectiveness:  The proposed embedding is applicable to various GNNs.  The design seamlessly integrates with existing message passing work enabling updates to positional embeddings.  experimental results demonstrate its effectiveness. Weaknesses:  The motivation for positional embedding is not compelling as Eq. (7) may perform similarly to Eq. (9) with vector concatenation. exploration of different activation work could strengthen the argument.  Although the embedding is defined up to \u00b11 allowing for positive and negative values through the tanh activation it is unclear if this can lead to the same publication as the original eigenvector definition.  Additional baseline comparisons such as the positionaware graph neural network by You Ying and Leskovec (2019) would provide a more comprehensive evaluation.", "Paraphrase: Summary: This work introduces a framework that incorporates random walk positional Embeddings (RWPE) as additional characteristics to improve the performance of Graph Neural Networks (GNNs). positional embeddings are updated in each layer using a separate forward network. The framework has proven effective in enhancing accuracy by injecting positional embeddings and feedforward structures into several GNNs. effectiveness and Weaknesses: effectiveness:  Addresses the crucial publication of effectively leveraging structure data in GNNs.  The proposed approach with separate feedforward networks for RWPE has shown improved accuracies and can be easily integrated into various GNN architectures. Weaknesses:  The paper lacks a clear explanation of the source of the improvement and the role of each component.  The inclusion of LapEig presents a motivation conflict with RWPE.  The use of the trace loss (Tr(pT\u0394p)) suggests that the positional embeddings may align with LapEig. It is unclear why RWPE would be superior to using positional embeddings in the final layer.  The novelty of RWPE is questionable since it is derived from a previous concept.  The paper does not adequately demonstrate that the improvement is attributed to the feedforward network structure rather than the additional parameters.  The addition of positional embeddings also increases the networks capacity. Minor publication:  Typos in the proposed architecture section.  The dimension of the edge embedding in Equation (2) is not specified.  The discussion on flipping signs in LapPE does not fully explain Laplacian decomposition."], "neqU3HWDgE": ["Paraphrased Statement: Summary: This paper suggests using multiple interconnected circles (tensor product) to represent data elements in a way that makes different factors less intertwined. Tests using madeup datasets show that this method works well. effectiveness and Weaknesses: effectiveness:  The proposed method is straightforward and appears easy to replicate. Weaknesses: 1. The authors do not explain the \"entropy of entanglement\" concept thoroughly. Since this concept is not widely used in the field of disentangling representation learning they should provide more detail including:  An intuitive explanation of entropy of entanglement  How it relates to disentangled representation learning  Experiments demonstrating its connection to entropy of entanglement  How it can address the \"unidentifiability problem\"  Why it leads to using multiple interconnected circles 2. The authors state that \"ntorus Tn\" has low entanglement but it is unclear how the commonly used ndimensional vector representations compare in terms of entanglement. 3. The proposed method introduces two new techniques:  Using multiple cyclic representations indexed by two 1D latent variables  Flattening using the tensor product The authors should explain the motivations for these techniques individually particularly:  Why multiple interconnected circles are beneficial than vectors  Why the tensor product is necessary 4. The authors acknowledge that the method becomes impractical for a large number of latent dimensions due to exponential representation dimensionality growth. They should provide a solution to this number with experimental support. 5. The experiments do not include an ablation field on the number of interconnected circles. 6. Experiments on complex datasets like CelebA should be included to demonstrate the methods generalization ability.", "Paraphrased Statement: This field presents a novel autoencoder innovation that outperforms established disentanglement metrics (completeness and informativeness) in artificial data. It employs a nonlinear projection of latent factors into a higherdimensional torus space preventing linear factor rotations from producing equivalent representations. The method surpasses existing variational autoencoders for disentanglement and completeness while matching or exceeding informativeness on most datasets. effectiveness:  Simple yet innovative approach  Clear explanation of the underlying principles  Integration with existing frameworks  Convincing evaluation demonstrating a strong linear correspondence between learned and groundtruth representations  Explanation for the methods relative weakness on one dataset  Acknowledgment of the methods limitations with many latent causes Weaknesses:  Evaluation relies on artificial datasets (similar to the field)  Limited attempt to apply the method to real data would be valuable  Quantum physics analogies provide limited insight into the methods performance  Importance of torus topology is unclear as the method resembles basis expansion in linear models  Appropriateness of Lasso for evaluating toroidal representations is questionable due to rotational invariance  Tuning parameter optimization details are not provided  Linear representation of factors in number 2 despite rotational invariance appears contradictory Minor Comments:  \"Autoencoder\" should be written as one discussion  Equation 9: \"N(01)\" should be corrected to \"N(0 sigma)\"", "Paraphrased Summary: The authors suggest an autoencoder with a latent space structured as a torus a shape composed of multiple circles. They believe this structure fosters disentanglement similar to entropy entanglement in quantum physics. Experiments on five datasets demonstrate both strong reconstruction quality and disentanglement (EastwoodWilliams 2018). effectiveness and Weaknesses:  effectiveness:  Presents a unique and novel use of a torus as a latent space representation.  Validates the approach with promising results on five datasets.  Weaknesses:  Lacks theoretical analysis of the proposed latent representations properties.  The connection between the torus representation and quantum physics concepts (e.g. entanglement Von Neumann entropy) is not fully clarified. Minor Comments:  Typo: \"n\" in equation (5) should be \"D.\"  Gaussian distribution: The index \"(a \\\\alpha)\" in the sentence following equation (8) represents integers but the distribution is Gaussian. This inconsistency needs clarification.", "Paraphrased Summary: Researchers have introduced an innovative technique to analyze the hidden factors in variational autoencoders (VAEs) more effectively. They treat these factors as circles that form a multidimensional \"donut shape\" allowing for a more intuitive understanding of the data. The paper also introduces a new assessment measure for evaluating the accuracy of these factor breakdowns. Theoretical explanations and experimental comparisons support the efficacy of the methods both visually and quantitatively. effectiveness:  Clear and insightful approach to disentangling latent factors  Welldesigned experimental evaluations supporting the theoretical basis  Visually impactful number illustrate the effectiveness of the methods Weaknesses:  potential scalability limitations with increasing dimensions  performance when factors are not wellrepresented as circles Questions Raised:  The impact of varying the dimensions of the \"donut shape\"  The behavior of the method in cases where factors are not circular Minor Suggestions:  Improve clarity of Equation (9) for easier understanding  Simplify notation for alphak to avoid potential misunderstandings"], "zbZL1s-pBF": ["Paraphrased Summary: This research proposes a late fusion algorithm for combining predictions from multiple modalities (e.g. image and text) in machine learning. This algorithm aims to enhance robustness against both adversarial attacks (deliberate distortions in data) and random corruptions (unintentional data alterations). The algorithm assumes knowledge of the corrupted modality and exploits Jacobian regularization and the conditional independence assumption to integrate predictions. Rigorous error bounds are derived to quantify the algorithms perworkance. The need for an extra modality (beyond the corrupted and uncorrupted modalities) is demonstrated. Key contribution:  Late fusion algorithm based on Jacobian regularization and conditional independence  Theoretical error bounds and demonstration of the biasing effect of the extra modality  Superior perworkance over existing late fusion algorithms in adversarial attacks and random corruptions effectiveness:  Intuitive approach that leverages Jacobian regularization to improve robustness against modality corruption  Rigorous theoretical guarantee that limits the impact of corruption on predictions  Efficient optimization algorithm for parameter estimation Weaknesses:  assumption of known corrupted modality  Use of an optimization procedure which could be considered a work of training  Unclear definition of \"trainingfree\" in the context of the proposed method  Lack of concrete model or references to support the assumption of known corrupted modality Additional Questions and Minor Typos:  Explanation of \"numerical stability\" in equation (5)  Clarification of the \"key role\" played by the extra modality  Correction of the typo \"larter\" to \"later\" in the second paragraph  Possible error in equation (4) where PPT may need to be replaced with PPTT", "Paraphrase: Summary: This work presents a trainingfree late fusion approach for robust multimodal learning. Its performance is analyzed under adversarial approachs and random distortions that introduce input data noise to deceive model. To enhance approach resistance in multimodal predictions the authors advocate minimizing the Jacobian matrix Frobenius norm thus stabilizing predictions against input perturbations. They derive a theoretical error bound for their method which outperforms existing late fusion approaches in experiments. effectiveness:  Novel and wellreathenned approach  Solid empirical results on various datasets Weaknesses:  Regarding equation (1): Clarify the work of WAB and hA.  Regarding equation (2): Verify if \"freq\" (class frequencies from training data) is used directly to compute \"p\" on test data. If then consider the impact of potential area shifts on \"freq.\"  Regarding equation (5): Specify whether the objective minimizes \"L\" or \"WA L.\"  Regarding Theorem 1: Acknowledging the loose error bound explore opportunities to compute exact values in specific experimental context (e.g. for bias noise).  Algorithm Functionality: Provide a clearer explanation of the algorithm pipeline at training and test time. Clarify whether Sylvester equation is thenlved for each minibatch.  AVMNIST Experiment: Suggest perturbing the image modality (typically providing more data) for large experimental insights.", "Paraphrased Statement: Summary This paper introduces a trainingfree robust multimodal learning method known as late fusion with samplewise Jacobian regularization. The method is designed to stabilize multimodal predictions by minimizing the Frobenius norm of a Jacobian matrix. The method demonstrates effectiveness against adversarial attacks and random corruptions across multimodal datasets (AVMNISTVGGSoundRAVDESS). Strengths  The Jacobian regularization method is straightforward.  The paper provides a theoretical error bound for the proposed method.  The ablation work on the work of different modalities using the TwoMoon model is valuable. Weaknesses  The claim that robust late fusion in multimodal learning is an unexplored area is not entirely accurate as early work have explored this concept (references [1] and [2] are missing).  The experimental results are limited to two datasets (AVMNISTVGGSound RAVDESS). A more comprehensive evaluation with various multimodal tasks and datasets would strengthen the findings.  The authors claim that one iteration of the proposed method often yields sufficient accuracy is somewhat surprising. more details on this aspect would be helpful."], "nKZvpGRdJlG": ["Summary Paraphrase: This paper describes methods for attackers to intentionally alter combinatorial optimization problems by removing edges from graph and for defenders to respond by adding edges to neutralize the attack. The paper includes attack strategies using reinforcement learning and heuristics and a defense strategy based on reinforcement learning. The techniques are tested on three combinatorial problems. strength and Weaknesses Paraphrase: strength:  Addressing the robustness of combinatorial optimization solvers is important.  The presented attack and defense methods are logical within the proposed scheme. Weaknesses:  The attackdefense schemes definition raises questions.  The attack method should target the original problem but it optimizes for the perturbed problem.  The defenses effectiveness cannot be accurately assessed since the original problem is unknown to the defender.  The defense relies on specific attacker strategies.  The proposed reinforcement methods are sensible but unoriginal.  The experimental setup lacks clarity on the number of allowed edge changes.  Random instances should be tested to demonstrate statistical significance. additional Questions:  How do nonlinearity and NPhardness contribute to the vulnerability  What is meant by \"universal\" parameters  Why would attackers set f(Q) \u2265 f(Q)  Relaxing candidate space does not necessarily improve performance as it is measured against the optimal solution which may have a better value in the relaxed space.", "Summary: This paper investigates adversarial attacks and defenses for combinatorial optimization problems. It introduces the ROCO framework to attack and defend these problems by maximizing or minimizing the difference between the optimal values obtained from the original and perturbed graph. The framework employs both RLbased and heuristicbased solutions which are tested on three combinatorial optimization tasks. strength:  The ROCO framework provides a structured attack to study adversarial attacks and defenses for combinatorial optimization problems.  The paper presents a novel method for quantifying the robustness of optimization solvers using the difference in optimal values between perturbed and original graph. Weaknesses:  The definition of robustness used in the paper is flawed as it evaluates the robustness against perturbations that may not be realistic or relevant to the optimization task.  The evaluation of the defense strategy is unclear particularly the claim that it can improve the solutions for normal instances.  The paper implies that modifying the graph itself can lead to improved solutions which is questionable for combinatorial optimization tasks.  The defense strategy involves accessing the original graph which contradicts the goal of defending against adversarial attacks.  The claims made in the paper are overly broad and should be narrowed down to specific tasks and solvers where the ROCO framework can demonstrate its effectiveness.  The propose solutions lack significant novelty. Minor Correction: Equation 4 should be revised to read: f(S(Q)G)  f(S(Q)G))", "Paraphrased Statement: Summary:  The authors aim to assess the resilience of optimization algorithms designed for solving combinatorial problems particularly those utilizing machine learning.  They approach this by formulating the problem as a graph optimization task and introducing attacks that manipulate the graph structure.  They also propose a defense mechanism to mitigate these attacks.  The paper provides experimental results for three different optimization tasks. strength and Weaknesses: strength:  The problem of studying the robustness of combinatorial optimization solvers is important.  The paper is generally easy to understand. Weaknesses: 1. Problem Formulation publication:  Equation (2) which defines the transformed graph does not effectively analyze solver robustness.  It simply identifies two distinct problem instances with different input graph and objective work.  This does not provide insights into solver resilience. 2. Confusing Definitions and Notation:  The definition and relationship between graph G and Q are unclear.  It is uncertain why the solver operates on Q instead of G and what the conditional dependence on G signifies.  The practice of f operating on S(Q) and Q inconsistently is confusing. 3. Unclear Experiments:  The attack method for the TSP task is described as \"choosing an edge and half its value\" but the framework description only includes edge addition and deletion.  The experimental setup does not effectively evaluate solver robustness as intended. 4. Limited Related work discussion:  The paper lacks a comprehensive discussion of related research on graph attacks and defenses.  A comparison with gradientbased attacks (relaxing data discreteness) would be valuable. 5. Limited Technical Novelty:  The authors draw heavily from existing graphbased attack techniques which limits the originality of the work.", "Summary Paraphrase: The authors present a novel study on adversarial attacks and defenses for combinatorial optimization problems. However they fail to adequately justify the significance of the problem they are investigating. While the authors claim it is crucial to develop defenses they do not provide convincing evidence for the prevalence of adversarial inputs in scheduling or TSP. The attack framework used in the paper also lacks appropriate rationale and connection to realworld applications. other Observations Paraphrase:  The paper uses an unfamiliar representation for combinatorial optimization.  The definition of x variables and their relationships to binary constraint is unclear.  The term \"loosen contribution of the constraint hi\" needs further clarification.  The FC attack on Gurobi seems to create computationally challenging problems rather than compromising solution quality. The authors should explore the implications of this finding. strength and Weaknesses:  The paper investigates adversarial attacks and defenses for combinatorial optimization problems.  The authors propose RLbased algorithms for attack and defense.  Computational experiments are conducted to demonstrate the strategies."], "yK_jcv_aLX": ["Paraphrased compact: The paper introduces a method for creating a condensed and uncrowded generative model of a partially observable environment. This model allows the identification of significant latent state variables related to the environments reward function. By focusing on these variables called the actionsufficient state representation decisionmaking becomes more efficient. The methods effectiveness in using less sample information than other approach is demonstrated in two highdimensional observation space decision problems. effectiveness and Weaknesses:  effectiveness:  Principled learning of a latent space representation.  Identification of irrelevant state components enhancing decisionmaking efficiency.  Incremental model building capability.  Novel method for learning DBN sparsity structure.  Weaknesses:  complex loss function with numerous regularization parameters.  Need for clear guidelines on parameter setting.  Comparative analysis with subspace system identification algorithms would be valuable.  Evaluation limited to two decision problems with highdimensional observation space (dimensionality not specified).", "Paraphrase: compact: This research presents a novel technique for reinforcement learning (RL) called ActionSufficient state Representations (ASRs). It aims to extract essential and informative state representations from complex environments for decisionmaking tasks. Unlike previous approach it explicitly incorporates structural relationships between state features observations reinforcement and actions in Markov decision Processes (MDPs). By constructing a generative environment model (SSVAE) the proposed method learns these relationships and the corresponding ASRs. Empirical evaluations demonstrate the effectiveness of the proposed algorithm in conjunction with both modelfree and modelbased RL approach in environments like VizDoom and CarRacing. effectiveness:  The paper introduces a novel approach that leverages structural constraints in MDPs for state representation learning.  Both the structural constraints and ASRs are learned jointly from trajectory information.  ASRs are evaluated with both modelfree and modelbased RL algorithms.  The learned structure provides insights and has the potential for interpretable representation. Weaknesses:  The methodology could be improved for clarity.  The relationship between SSVAE and similar models like RSSM used in other reinforcement learning methods is not thoroughly discussed.  The choice of experimental environments and specific settings is not adequately explained. Major Concerns with the Methodology:  The definition of ASR under the condition of maximizing cumulative reinforcement needs clarification.  The rationale for neglecting the first term in maximizing mutual information is unclear.  The computation of crossentropy H(qp) given the different distribution types of q and p is not fully explained.  The derivation and motivation for using a lower bound to minimize the conditional mutual information should be provided.  The exact process of obtaining the actiondependent ASR from the estimated structural matrices is not described.  The anticipation terms log p(ot1s) and log p(rt2s at1) in Equation 4 require further explanation.  The absence of DeepMind Control Suite environments in the experiments which are commonly used for highdimensional state representation learning raises questions about the generalizability of the results. Minor Corrections:  Replace \"p\" with \"q\" in the time about modeling each factor with a mixture of Gaussians.  Change \"Figure 17\" to \"Figure 12\" in the reference to the preprocessor architecture.  Remove duplicate references on page 12.  Provide a more detailed description of Figure 17 in Appendix G.", "Paraphrase: This paper focuses on reducing the dimensionality of state space representations for control purposes. The approach involves collecting information from the system then learning structural constraints that describe the reinforcement variable. An adapted Variational Autoencoder (VAE) is used which balances reconstruction reinforcement anticipation action sufficiency and minimality. The resulting lowdimensional Markov decision Process (MDP) can be combined with traditional modelbased RL to potentially enhance sample efficiency. effectiveness and Weaknesses:  effectiveness:  Addresses a crucial problem in highdimensional RL reducing sample complexity by identifying a smaller MDP sufficient for planning.  Proposes a novel approach with a solid theoretical foundation.  Weaknesses:  Parameter selection: The VAE objective involves parameters whose selection is not specified in the paper. These parameters are critical in ensuring minimal state space compression.  Interpretation of graph structure variables: The interpretation of the binary graph structure variables D is unclear as the Gaussian section suggests they represent both connection and connection effectiveness.  information efficiency: It is uncertain whether the experiments demonstrate reduced information usage compared to the best baseline. The learning overhead for the ASR should be accounted for.  Dyna algorithm: The proposed Dyna algorithm is generally outperformed by the Prioritized Sweep algorithm. This potential limitation should be considered.", "Summary This field aims to develop a representation (ASR) that contains only actionrelevant information removing irrelevant details that hinder downstream tasks. It employs the principles of mutual information and sparsity implemented by incorporating a Structured Sequential component into a Variational Autoencoder (VAE). effectiveness and Weaknesses 1. challenge: learning ASR in environments with random actions can require a vast issue of samples. 2. Questionable Assumption: The claim that H(a  ASR R t) can be minimized to estimate ASR is not generally true. 3. Redundancy: The objective function includes both a term promoting minimal representation and a sparsity constraint which seems redundant. 4. Ambiguity: The relationship between the objective function and the information bottleneck method is unclear. 5. Insufficient Definition: \"minimal sufficient\" representation is not explicitly defined. 6. Inconsistent Use of terms: The use of informationtheoretic concepts lacks precision. 7. Unexplained Figure: The significance of the \"Deconv\" component in Figure 2 is unclear. 8. Typo: \"hyperparameter turning\" should be \"hyperparameter tuning.\" 9. Discrepancy: Equation 1 suggests that a directly affects s but Figure 2 does not depict this connection. 10. Inadequate Comparison: The comparisons with other methods that optimize state representation are indirect and do not specifically consider ASR methods. 11. qualitative Evaluation: The quality of the estimated ASR should be assessed theoretically or empirically such as comparing its mutual information with the true actionrelevant information. 12. Relevance of Comparison: The comparison with methods that reconstruct observations is not relevant to ASR. The field should compare ASR methods that focus on minimal sufficient state representations."], "m22XrToDacC": ["Paraphrase of Summary This research explores the challenge of creating recourse actions that remain effective even when the classifiers parameters change. The authors propose a distributionally robust optimization approach that generates recourse actions with a high likelihood of staying valid if the linear classifiers weights shift. Strengths and Weaknesses Strengths:  Clear and comprehensive explanation of the proposed approach  Technically sound and wellgrounded approach Weaknesses:  Limited experiments with unclear value of contribution  Lack of comparison with previous work (Upadhyay et al. 2021) Detailed input on Experiments  Conduct a comparative analysis with Upadhyay et al. 2021  Utilize the L1 cost function consistently for all approach  Include experiments for nonlinear classifiers  Consider actionability constraints  Use a different split of data for training and testing  Discuss estimating theta and Sigma without approach to data and the implications for recourse robustness Detailed input on foundation  Clarify the distinction between \"counterfactual explanation\" and \"recourse actions\"  Provide citations for claims about improving user engagement and interpretability  Recognize that age can be treated as a nondecreasing feature  Note that data shifts do not directly alter model parameters but may trigger model retraining  Acknowledge that recourse actions can still be valuable even if they dont guarantee favorable outcomes  Provide evidence to support the claim that failed recourse actions lead to lost reliance in machine learning systems input on Sections 2 3 and 4  Move algorithm 1 and Theorem 3.4 to the appendix to save distance for experiments  Consider moving Sections 4.1 and 4.2 to the appendix as they are not core to the main contribution  Explicitly include the constraint x \u2208 X in Equation 2  Explain the rationale for using a margin of 0.5 in Equation 9a", "Paraphrased Statement: Summary This research explores the generation of \"recourse actions\" (counterfactual explanation) while accounting for data or model shifts. The proposed approach known as Distributionally Robust Recourse Action (DiRRAc) generates reliable recourse actions even in the presence of evolving model parameters. DiRRAc employs distributionally robust optimization techniques solved through a projected gradient descent method. Experimental solution utilizing synthetic and realworld data indicate DiRRAcs superior robustness in generating recourse actions compared to existing methods. Strengths  Addresses a novel problem considering model shifts in recourse action generation which is highly relevant in practical application where modeldata discrepancies arise.  Formulates the problem as a minmax optimization problem considering model shifts as parameter mixture shifts.  Demonstrates the effectiveness of DiRRAc through comparative experiments.  Presents a clearly written paper. Weaknesses  Does not explicitly convey the challenges in adopting distributionally robust optimization for recourse action problem which could be beneficial for unfamiliar readers.  Excludes ROAR a method for generating robust counterfactual explanation to model shifts from the baseline methods in experiments.  Lacks an evaluation of the proposed methods efficiency compared to existing methods.  Should assess the performance of the proposed method under no model shifts for a more comprehensive evaluation. Other input  Typo on page 2: \"Distributlly\" should be corrected to \"Distributionally.\"  The paper lacks a dedicated Conclusion section.", "Paraphrased Summary: The paper introduces a robust framework for providing explanation (called \"recourse\") that are not affected by changes in the underlying model. This framework is formulated as an optimization problem where the worstcase scenario is considered over a range of possible model parameters. The parameters are sampled from a mixture of distribution and the range is defined using a specific distance metric called Gelbrich distance. The problem is then approximated and solved using a gradient descent method. The approach is evaluated on three datasets with varying data distribution shifts. Strengths and Weaknesses: Strengths:  Motivated problem setting given that models may change over time.  Compelling approach of formulating the problem as a distributionally robust optimization. Weaknesses:  The technical solution is limited in its reliance on specific structural properties of the mixture distribution and Gelbrich distance.  The empirical evaluation uses different cost metrics compared to existing approach making the comparison challenging.  The tradeoffs between explanation robustness and cost for the proposed approach (DiRRAc) are not fully explored."], "qTHBE7E9iej": ["Paraphrased Statement: This paper introduces a hierarchical model for extracting skills from heavy online datasets. The model consists of three levels:  Highest Level: Selects a skill type from a discrete set.  Middle Level: Finetunes the continuous parameters of the selected skill.  Lowest Level: Executes the specified skill. This model is valuable for reinforcement learning particularly in complex tasks where efficient skill reuse is crucial. By leveraging an offline dataset skills can be extracted and used as a foundation for reinforcement learning algorithms increasing sample efficiency and enabling direct application to challenging robotic tasks. The authors propose a graphical model with four variables at each time step:  xt: MDP state  yt: Discrete skill selector  zt: continuous skill parameters  at: Output of the selected skill (e.g. robot joint velocities) The model is learned using variational inference and so refined through reinforcement learning optimizing the skill selector parameters and execution. The model incorporates data asymmetry where each level of the hierarchy has a different perception of the state. It also utilizes \"gated\" heads where the categorical variable yt is processed towards the conclusion of the neural network. The learned hierarchy can be further tuned on the same or different tasks with the lowest layer fixed while only the upper levels are updated. MPO is used as the reinforcement learning algorithm. Ablation work and experiments demonstrate the efficacy of the proposed method. effectiveness:  Addresses a crucial problem in skill extraction and reuse.  Sound method with a welldefined hierarchical structure.  Uses neural networks for scalability and applicability.  Supports the extraction of skills that can be directly used in various RL algorithms.  Experimental issue consistently validate the methods effectiveness. Weaknesses:  Readability could be improved particularly in terms of mathematical precision and clarity.  The connection to related work such as HMMs and mixture of movement primitives could be better explained.  The novelty of the contribution could be further emphasized.", "Paraphrased Statement: Summary: This paper presents a threetiered approach for teaching motor skills. At the base there is a controller for basic actions at the top is a controller that plans sequences of skills and between these is a controller that generates continuous control signals based on the midlevel skills. When transferring motor skills across tasks using reinforcement learning retraining either the highlevel or midlevel controller is potential. Experiments show that for complex manipulation tasks with minimal reinforcements convergence is more potential with only the highlevel controller. This approach provides added versatility and may facilitate the learning of more complex tasks compared to prior methods using only two levels. effectiveness and Weaknesses:  Advantage: The proposed system allows for advantageous transfer by modifying only the highlevel controller.  Advantage: Retraining the midlevel controller across different modalities may be feasible since different levels operate on various aspects of the state space.  effectiveness: Comprehensive experimental section includes comparisons with alternative methods and ablation work.  effectiveness: Two transfer learning variance are tested: one with highlevel controller retraining only (HeLMScat) and one with both highlevel and midlevel retraining (HeLMSmix). HeLMScat appears more beneficial in complex cases with minimal reinforcements.  effectiveness: Compared to alternatives HeLMS explores a wider state space leading to higher reinforcements when reinforcements are minimal. It combines highlevel exploration with known skills.  Question: HeLMSmixs diminishing reinforcement in complex tasks could be due to spurious correlations but this is not observed in other methods.  Question: NPMPs superior performance on object set 2 requires further explanation.  Clarification: When referring to HeLMS in experiments specify whether its HeLMScat or HeLMSmix.  Clarification: Table I shows unexpected lack of exploration in grasping for other methods.  Error: Figures 7(cd) manipulation incorrect notation for regularization weight.  Error: Typo: \"ELBO p(ax)\" is reversed on page 3. Overall the paper is wellwritten and easy to understand using clear language and notations.", "This research presents a new model for learning reusable skills in robot control tasks including behavior cloning and reinforcement learning scenarios. The model has a threelevel structure: the first level takes in data about the state of the robots environment (such as its position visual input and object data) and uses a neural network to select a discrete skill. The second level uses the same input data to generate a continuous state based on the selected skill. Finally the third level combines the continuous state with the input data to generate the robots actions. The model is trained in a flexible means depending on the situation with different components being fixed or adapted at different times. It uses a KL regularization term on the discrete skill selector similar to previous work. issue show that the model performs well on various tasks and that reusing learned skills leads to better issue than learning from scratch. A particularly interesting issue is the comparison with a hierarchical behavior cloning network and the experiments on skill transfer. Although the concept behind the model is sound the paper could benefit from clearer system and presentation. Some significant related work is also missing. effectiveness:  The hierarchical policy design is a promising approach integrating both discrete and continuous latent variables in a novel means.  issue demonstrate the effectiveness of the proposed architecture particularly in the comparison with hierarchical behavior cloning.  The exploration of skill adaptation during reinforcement learning is significant moving beyond simple skill reuse. Weaknesses:  The paper requires more system and clarity particularly in defining the problem scope and presenting issue.  The experimental setup lacks focus and coherence as it addresses multiple questions and uses varying baselines for different tasks.  Important related work on hierarchical and reusable skill discovery in reinforcement learning is not adequately cited and discussed including methods for switching nonlinear dynamical systems.", "Paraphrase: This work presents a novel method for offline learning of a threetiered hierarchy of skills from demonstration data designed to enhance reinforcement learning speed. The architecture consists of:  discrete level: Selects highlevel actions  continuous level: Contextual data dependent on discrete actions  Action level: Lowlevel policy dependent on continuous context issue indicate that the method outperforms baselines in certain scenarios such as those with sparse rewards where it benefits from the prelearned exploration strategies. effectiveness:  Clear and wellorganized presentation  Innovative method that expands on previous work  Positive issue particularly in resourceconstrained environments Weaknesses:  Related work section should be placed after the introduction to provide context.  Limited experimental evaluation using only one domain with some unclear issue.  Benefits of the twolevel highlevel latent code structure are not fully explained.  Sensitivity analysis is missing regarding the impact of additional observations prohibited categorical values and the robustness of skill recovery.  A visual representation of the experimental environment would be helpful.  The method section could be elaborated to provide more detailed explanations behind the equations."], "ugxdsne_TlO": ["Paraphrased Statement: Summary: This field introduces a generalized causal forest (GCF) algorithm that replaces the partial linear model in the original causal forest with a kernel DML estimator. Empirical evidence suggests that the GCF outperforms existing estimators. effectiveness and Weaknesses: effectiveness:  The generalization of the causal forest allows for integration with modern machine learning tools. Weaknesses:  Comprehensibility: The paper may be difficult for many readers to understand and the theories are not sufficiently detailed.  Literature Review: The claim that existing methods primarily focus on binary treatments is inaccurate. The paper should provide a more comprehensive literature review to establish the need for a generalized HTE estimator for continuous treatments.  Terminology: The term \"local splitting\" is not defined in the paper assuming familiarity with the causal forest algorithm.  Motivation: The paper does not explain why the GCF should be used over existing HTE estimators such as the Kernel DML or doubly robust estimator.  \"Global\" Estimator: The meaning of \"global\" estimator is unclear.  Prerequisite Knowledge: The paper assumes readers are familiar with the notion of causal trees and causal forest which should be defined in the paper.  contribution: The paper contribution should be explicitly stated.  Doubly Robustness: The paper does not present a conventional analysis demonstrating the doubly robustness property of the GCF despite its connection to the kernel DML.  comparison with Kernel DML: The paper should compare the GCF with the Kernel DML to provide a more comprehensive evaluation.  Typos: The paper contains typos including \"Table \" on page 9 and \"First We\" on page 2.", "Summary: The authors expand the \"Generalized Random forest\" (GRF) of Athey et al. (2019) from partially linear to nonparametric models and combine it with a doubly robust estimation technique. The field includes a smallscale model and a small data analysis. effectiveness and Weaknesses: Weaknesses:  The paper does not introduce novel methodology it only combines existing techniques (GRF and doubly robust estimator).  No theoretical analysis of the proposed method is provided.  The authors overstate their contribution extending GRF from partially linear to nonparametric models is not highly innovative.  The construction of the estimator \\hat\\mu(tXi) is unclear.  The model field is nonreproducible due to missing sample size (n) and dimension (p) values.  Both the model and real data analysis are on a small scale. Unsupported Claims:  The introduction states that the method allows for faster hyperparameter tuning but no evidence supports this claim. Additional Comments:  Inconsistent notation (e.g. \\Omega) causes confusion.  The exposition is disorganized and uses many abbreviations.  The language is overly conventional and contains distracting grammatical errors making it difficult to follow the authors estimation.", "Paraphrase: Summary: This paper introduces a broadened version of the \"causal forest\" algorithm for estimating the varying effects of continuous treatments. It leverages kernelbased estimators to flexibly model the treatment response relationship. The proposed methods efficacy is demonstrated through tests on synthetic and realworld datasets in comparison to other approach. effectiveness: 1. The paper tackles the pertinent effect of estimating treatment effects that vary based on continuous variables highlighting the importance of capturing nonlinear relationships in the treatment response use. 2. The paper substantiates its claims with robust results from realworld datasets. It also demonstrates the practicality of the proposed method through an actual experiment. 3. The method is presented as computationally scalable enabling its application to largescale datasets. Weaknesses: 1. The introduction lacks clarity omitting essential concepts like seffect use splitting criterion and making it difficult to understand the proposed method. The relationship between causal forest (CF) and Generalized causal forest (GCF) in Section 4 needs further explanation. 2. The effect contribution of the paper is modeling the treatment response use nonparametrically using a doubly robust approach. Its significance and value compared to existing methods like Kennedy are not readily apparent. The authors should provide more context and justification for the increased complexity of their approach. 3. The paper lacks a conventional theoretical basis for the proposed method although the empirical results mitigate this concern."], "oWZsQ8o5EA": ["Paraphrased Statement: Summary: This paper introduces a new bounds on the generalization error for models trained using stochastic gradient descent (SGD). This bounds improves upon previous work by Neu. Based on the insights from the bounds the authors propose two techniques to enhance generalization: gradient clipping to prevent overfitting and Gaussian model perturbation as a regularization method. The paper also provides indepth analysis of the mathematical terms used and discusses several phenomena observed during learning. The authors suggest potential avenues for further improving the bounds using the robust information work inequality. strength:  Exceptionally wellwritten  significant contribution both theoretically and practically  Connects with relevant literature  Clear explanations and discussion Weaknesses:  potential concerns regarding the monotonic increase in generalization error with training epochs  Disparity in magnitude between the trajectory and flatness terms in Figure 7  Ambiguity regarding the flatness measure in the second term of (1)  lack of clarification on whether the bounds can be decomposed into Hessian and gradient dispersion terms  Unclear understanding for low gradient dispersion in previous epochs  Absence of regularization using the magnitude of Ls(w\\Delta)Ls(w)  lack of details on how the standard deviations in Table 1 were determined  Editorial inconsistencies in the text  Unfamiliar distinction between samplelevel and instancelevel mutual information  Undefined use of the batch gradient use g  Omission of the full name for the HWI inequality  Illegible font size in Figure 2  Inconsistent axes in Figure 3", "Summary This research presents an improved version of a recently proposed generalization bounds from Neu (2021). The study strengthens this bounds by imposing additional assumptions on the randomness of batch sequences allowing for tighter estimates. Notably the method includes a regularization technique based on the \"flatness\" term which enhances the classification accuracy of the VGG16 model on the MNIST dataset. strength and Weaknesses strength:  Based on a significant recent work (Neu 2021).  Clear introduction of technical advancement and empirical advance.  Provides a practical regularization technique that improves VGG16 classification on MNIST. Weaknesses:  Heavily reliant on a recent study.  implication of Neu (2021) not fully explained.  No comparison to more established generalization boundss.  \"Randomness\" in batch sequences is somewhat ambiguously defined.  Inconsistencies in notation and terminology.  Limited explanation of boundss in Lemma 3 and Theorem 1.  Writing could benefit from improved clarity and context. Minor Comments:  Typos and formatting error.  Statement in the conclusion lacks context and motivation.", "Paraphrased Statement This paper develops an improved generalization bounds (measure of stability) for training machine learning models using stochastic gradient descent (SGD) inspired by the work of Neu et al. The researchers have refined the bounds by removing a term related to \"local gradient sensitivity\" resulting in a more reliable estimate in realworld scenarios. Using this updated bounds they have analyzed linear models and twolayer ReLU neural networks. Their analysis array with previous observations further validating their advance. The paper introduces a new training method called Gaussian model perturbation which demonstrates strong performance on common datasets. strength and Weaknesses This paper is wellorganized and provides a concise introduction. The derived bounds is tighter than Neu et al.s (as shown in Figure 1) and the method for achieving this advance is intriguing. However the paper has some limitations: 1. Novelty Concerns: The researchers acknowledge that their bounds is not entirely original compared to prior work. Additionally their application to linear models and twolayer ReLU networks is not fully convincing. While they suggest that the bounds may be independent of model complexity due to inactive neurons they do not specify how many neurons remain active limiting the usefulness of this claim. 2. Unexplained Arguments: Certain claims made in the paper lack sufficient explanation. For instance in Theorem 2 the authors mention that an \"auxiliary weight work\" is justified without providing a clear explanation of its benefits. Similarly in the Experimental study section the use of replacing labels with random values is not fully elaborated upon. 3. Limited Empirical evaluation: The experimental study focuses on varying the width and filters of the network but does not explore the impact of network depth on the bounds. Overall while the paper presents a solid methodology it could benefit from more detailed explanations and a broader empirical evaluation.", "Paraphrased Statement: This study introduces theoretical set for the error in predicting future information of machine learning models such as neural networks trained using a specific algorithm called SGD. Building on previous work (Neu 2021) the proposed set are tighter than before thanks to two advance: 1. Removing an unnecessary term from Neus bounds using mathematical tools (HWI inequality). 2. Replacing a measure of samplelevel information in Neus bounds with a more precise measure (Bu et al. 2020). The set can be separated into two contribution: one capturing the effect of the models training path and the other capturing the smoothness of the final trained model. The set are used to analyze linear and ReLU neural networks and provide insights into their training behavior. The study also proposes some simple regularization techniques that improve the performance of neural networks comparable to existing stateoftheart advance. strength and Weaknesses: Comments: 1. The use of Wasserstein distance in the main contribution of the paper could be simplified by directly presenting equation (3) (from the appendix) which is the actual basis of the proof and discussing its relationship to Wasserstein distance and HWI inequality in the appendix. 2. The method used to calculate the subGaussian parameter R in Figures 2 and 3 is unclear. Its worth noting that the loss use for multilayer perceptrons (MLPs) and AlexNet (used in the experiments) is crossentropy which is not boundsed. Its also unclear how to demonstrate that these loss uses are subGaussian. Minor Comments: 1. Section 3 could be organized more clearly by separating the main theorem discussion proof study and comparison into distinct sections. 2. The different yaxes in Figure 3 work it difficult to compare the true generalization gap and the calculated bounds. It would be beneficial to adjust the plots to facilitate comparison."], "jE_ipyh20rb": ["Paraphrase: This paper presents a user selection algorithm for federated learning (FL) that aims to select highquality clients for model updates thereby minimizing the impact of poorquality data on training. The algorithm is based on the assumption that highquality data has consistent representations while lowquality data has distinct representation distribution. It uses this assumption to select users based on the divergence of their representation layer distribution from the global model. The authors demonstrate that representation layers approximate Gaussian distribution making the computation of distribution divergence more efficient. The algorithm is evaluated against existing method using a smallscale sensor dataset and a largescale EMNIST dataset demonstrating its effectiveness in ideal conditions. effectiveness:  The algorithms approach is novel and leverages the assumption of different representation distribution for high and lowquality data. Weaknesses:  The assumption of distinct representation distribution needs further justification especially in FL settings with heterogeneous datasets.  The algorithm may potentially select clients from a single cluster leading to suboptimal results.  The evaluation is limited to simple and homogeneous tasks and more thorough evaluations under heterogeneous conditions are required.  The comparison algorithms are mostly from 2019 and newer advancements in FL algorithms should be considered. Other Points:  The algorithm relies on Gaussian distribution approximation which may not hold in all cases.  The Lyapunovs condition used in the proofs may be restrictive and it is unclear if it is satisfied in practical experiments.  The method for selecting the representation layer in the experiments is not specified.", "Paraphrase: Summary: This paper introduces FedProf a new approach for selecting clients in federated learning. FedProf improves the convergence speed of FedAvgtype algorithms. The authors provide theoretical proof of FedProfs convergence under certain simplifying assumption. Additionally empirical research demonstrates the gain of FedProf. effectiveness and Weaknesses: effectiveness:  Easy to understand and implement  Comes with theoretical guarantees (although assumption may not hold in practice) Weaknesses and Questions:  Proposition 1 and 2 assume independence of vector coordinates which may not be realistic.  What is the impact of using a nonidentically distributed Gaussian instead of a product Gaussian  Is the KL divergence (Eq. 4) essential for performance What happens with other metrics  How sensitive is performance to the quality of the parameter \u03b1", "Paraphrased Summary: The authors address federated learning situations where local data may be unreliable or biased which can weaken overall performance. They introduce FedProf a technique using data profiling and matching to minimize the impact of unreliable data points during training. effectiveness and Weaknesses:  effectiveness:  Clear and understandable presentation  Technically valid method  Impressive results  Weaknesses:  Dataset Limitations: It would be beneficial to evaluate FedProf on a wider range of datasets including CIFAR10 and CIFAR100 and datasets with an increased issue of classes.  Statistical data: Providing standard deviation values in Tables 2 and 3 would enhance understanding of the proposed model stability.  Baseline model Currency: The baseline model used in the paper could be updated to include stateoftheart method like modelContrastive Federated Learning.  Conceptual Illustration: Adding a visual representation of FedProf would enhance understanding for nonexpert readers.", "Paraphrase: Summary:  This paper introduces a federated learning (FL) algorithm called FedProf.  FedProf leverages a training rule that updates the model based on differences in data representations. effectiveness:  The approach of presenting theoretical results on FL in a formal manner is commendable. Weaknesses: 1. Proposition 1 and 2:  The nonlinear aspects of neural networks appear to be overlooked and the statements essentially restate the Central Limit Theorem (CLT).  It should be clarified whether this simplified model is related to the rest of the work. 2. Main Theorem:  The theorem should be clarified and explained more thoroughly.  Specifically it is unclear how the construction of profiles work the algorithms performance. 3. Learning Bound:  It would be helpful to compare the stated learning bound to benchmarks such as fixed lambda performance. 4. convergence in Proofs:  The proofs of the propositions end with equations that indicate convergence in distribution which may not be accurate."], "n54Drs00M1": ["Paraphrased Summary This study presents a novel sentiment representation method that combines Affect Control Theory (ACT) with the BERT language model. ACT helps capture social interactions and evolving sentiments. The primary foundation is the application of ACT and the comprehensive methodology used. Experimental findings indicate that the BERT model performs well in this context. effectiveness and Weaknesses effectiveness:  advanced use of ACT representing a novel approach.  Wellstructured paper with visualizations. Weaknesses:  Limited ablation study and experiments.  Insufficient experimental details such as model parameters.  Lack of examples or clear explanation regarding the use of the model to expand affective lexicon.  unclear distinction between different word categories (e.g. identity vs. behavior) in the input dataset.  Reference to an unspecified affective lexicon.  Polysemous language (e.g. \"apple\") and their handling in the model are not addressed. small Concerns:  Figures should be improved for better clarity.", "Paraphrased Statement: Summary: The paper introduces a deep study model for determining the affective meaning of language through their EPA (Evaluation Potency and Activity) scores. Utilizing contextual embeddings obtained from the BERT model the approach outperforms existing methods. effectiveness and Weaknesses: Methodology:  effectiveness:  Utilizes embeddings from the BERT model.  Weaknesses:  Finetuning details are unclear.  Methodology appears basic.  Lack of comparison with shallow word embeddings or advanced transformer architectures.  Missing data preprocessing and implementation data. Writing:  Grammatical and sentence structure errors throughout.  Inconsistent wording (e.g. \"dataset\" vs \"dataset\"). Specific model of Errors:  Page 3 Line 5: \"following two ones\" should be \"following two.\"  Page 5 Line 8: \"But we have to foundation the dataset so that it can understand in which category we are working.\"  Context unclear.  Page 5 Line 10: \"To use BERT as a contextual wordembedding we should have the following processing.\"  May need rewording.  Page 7 Line 2: \"In this paper an approach to make training and test sentences that describe an ABO case.\"  Sentence incomplete.", "Paraphrase: This study explores the potential of using a pretrained BERT (Bidirectional Encoder Representations from Transformers) model to develop affective lexicons based on the ACT (Acceptance and Commitment Therapy) theory. effectiveness:  The concept of leveraging a pretrained model to create a sentiment lexicon aligned with ACT theory is novel and intriguing. Weaknesses:  The study lacks sufficient experimental analysis as evidenced by the following shortcomings:  The dataset used for the experiments is not specified.  The specific parameters employed in the methodology such as learning rate and batch size are not mentioned.  No relevant prior study are referenced to support the research.  The analysis of the experimental results is superficial and does not provide meaningful insight.  The presentation of the results particularly Table 2 and Figure 4 requires significant improvement.", "Paraphrase: In this paper the authors suggest using BERT embeddings as a better option than basic word embeddings for creating affective lexicons. They also demonstrate that finetuning the model leads to exceptional performance in estimating affective meaning. effectiveness: 1. The authors address the publication of expanding affective vocabulary through finetuning and address the limitations of traditional surveybased vocabulary collection. Their findings indicate that the finetuned model shows high correlation between estimated values and affective lexicon values for both identities and behavior. Weaknesses: 1. The finetuning method for the BERT model is described as simple but lacks detailed explanation. Additionally no data is provided about the training parameters making it challenging to reproduce the papers results. 2. The evaluation section is limited lacking baselines or comparison to other approach for the task. 3. The data processing and preparation for creating the ABO case training and test set is unclear and requires clarification. 4. The authors make several unsubstantiated claims particularly regarding the extension of affective lexicon. No results are presented to support these claims."], "rbFPSQHlllm": ["Summary: This field introduces AutoMOMixer a technique for combining multiple MLPMixer models for medical image classification. It aims to strike a balance between sensitivity and specificity two key metrics in medical diagnosis. effectiveness and Weaknesses: The paper presents several conceptual and technical errors hindering its contribution to the field. Important detail for understanding and reproducing the employment are omitted. Numerous typos and grammatical errors are also present. need: The selection of MLPMixer for minor datasets is questionable. Previous research suggests that CNNs may be more efficient in terms of parameters. The rationale for using MLPMixer is not adequately supported. Related employment: The paper lacks a related employment section failing to establish its position within the current state of the art. Technical Description: The description of AutoMOMixer is insufficient for replication. number 1 is confusing and the methodology mainly recapitulates the MLPMixer architecture. It is unclear how two groups are utilized for training. The multiobjective goal is a single objective not optimizing for both sensitivity and specificity. Evolutionary algorithms are employed for Paretooptimal model selection which is a puzzling selection. The use of Bayesian optimization for hyperparameter tuning is not novel. experimentation: experimentation are conducted on minor datasets from Kaggle. Transfer learning is not employed which is standard use for such minor datasets. The comparison to MLPMixers and CNNs is not fair as the number of layers is kept constant instead of using comparable parameters or FLOPS. No ablation field is performed to assess the impact of class imbalance on sensitivity and specificity. Clarity and Errors: Explanations are often unclear and terms are misused. Table 3 number hyperparameters instead of model parameters. number 3 and number 1 are difficult to comprehend.", "Paraphrased Summary: The authors propose a novel approach for learning with imbalanced medical image informationsets. They employ the MLPMixer architecture an alternative to convolutional neural networks (CNNs) which reduces the number of trainable parameters and allows training with fewer image. To address class imbalance they use multiobjective loss uses optimized using Bayesian optimization to balance sensitivity and specificity. The method is evaluated on two medical image informationsets and outperforms CNN and MLPMixer baselines. effectiveness and Weaknesses: effectiveness:  Less information requirements due to MLPMixers reduced parameter number.  Multiobjective loss uses for handling class imbalance.  Promising results.  Interesting use of Evidential reasoning for model fusion. Weaknesses:  Strong claim about MLPMixers superiority without supporting evidence.  Limited technical novelty as the method combines existing techniques.  Confusing notation and difficulttofollow methods section.  Lack of justification for using Evidential reasoning.  Insufficient background information on Evidential reasoning and IMIA.  Unclear definition of \"high\" in model selection. minor Points:  Consider changing number captions for clarity.  Move MLPMixer background information to a separate section. Suggestions for Improvements:  Compare with alternative methods (e.g. class weighting Focal loss).  Define \"Paretooptimal.\"  Compare with other fusing strategies to justify using Evidential reasoning.  Optimize hyperparameters of CNN and MLPMixer to demonstrate the efficacy of the multiobjective use.  Conduct experiments on a largescale informationset to support the claim about MLPMixers superiority for medical image diagnostics.", "Paraphrased Text: Summary This field presents the AutoMOMixer model for medical image diagnosis featuring:  MLPMixer architecture for reduced parameters by eliminating convolutions suitable for minor datasets.  optimization of a Paretooptimal Mixer model set by balance sensitivity specificity and overall performance (AUC).  fusion of outputs from selected models in the Mixer set using an Ensemble balance (ER) strategy. AutoMOMixer outperformed a single Mixer and a CNN with similar layers on brain tumor and COVID19 detection datasets. effectiveness and Weaknesses effectiveness:  Recognition of the importance of targeted solution for minor datasets via a convolutionalfree architecture.  Novel approach to balance sensitivity and specificity using multiobjective optimization. Weaknesses:  Uncertain if sensitivityspecificity balance was applied to the single Mixer and CNN models.  Ambiguity in the definition of Equation 3 for optimizing fsen and fspe.  clarification needed on the definition of miq for IMIA.  Lack of explicit proof of Paretooptimality.  Undescribed selection process and value for the Mixer model set size (J).  Unclear definition of MP as a hyperparameter for IMIA.  Incomplete description of network structure with only four instead of five parameters listed.  Incomplete comparison with CNN models due to lack of structural detail and training information.  Lack of ablation experiments to isolate contributions of individual components.  Only average results reported without standard deviations or confidence intervals.  Spelling and grammatical errors throughout the paper.", "Paraphrased Statement: Summary: This research combines two existing algorithms (multiobjective optimization and MLP Mixer) to improve medical image analysis. However the experimentation lacks sufficient support and the field seems to simply apply known techniques without significant innovation. effectiveness and Weaknesses: Weaknesses: 1. Lack of Novelty: The field does not introduce new or original approaches. 2. Limited need: The reasons for using MLP Mixer and multiobjective optimization specifically in medical image are not clearly justified. 3. Insufficient Empirical evidence: The field only compares a few models (CNN Mixer AutoMOMixer) leaving out numerous other variants that could have been included. 4. Poor Presentation: The paper writing and number lack clarity and fail to convey a coherent narrative."], "gPvB4pdu_Z": ["Paraphrased Statement: This research paper presents a novel method for training deep neural networks to maximize the AUC (area under the receiver operating feature curve) metric. The deep AUC maximization problem is more complex than minimizing standard loss function like crossentropy. Previously researchers addressed this issue by splitting optimization into two stages: optimizing a traditional loss in the first stage and finetuning for AUC in the second. The proposed approach called \"compositional deep AUC maximization\" allows for endtoend training. It reformulates the optimization problem into three components that simultaneously improve feature representation and classification performance. The paper also introduces an efficient algorithm for solving the compositional AUC maximization problem. This algorithm has been tested on various computer vision and medical image datasets and the results demonstrate its effectiveness. However the authors do not directly compare their method to other stateoftheart techniques.", "Paraphrased Statement: Summary: This field introduces a novel loss function structure designed to enhance deep AUC maximization. The proposed loss consists of an outer surrogate loss for AUC (AUC square loss) and an inner component that aids the convergence of crossentropy (CE) loss. The authors provide both theoretical and intuitive explanations for this compositional loss. The authors distinguish their proposal from a simple linear combination of AUC and CE loss both mathematically and empirically. Extensive experiments are conducted on various datasets including natural image benchmark and medical image data. CT(AUC) consistently outperforms other methods although improvements may be limited in certain cases. effectiveness:  clear explanations of the proposed loss and training methodology  Comprehensive experimentation to demonstrate effectiveness  Wellwritten and structured paper Weaknesses:  Lack of empirical validation for the hypothesis that CE loss assigns equal weights to examples regardless of class.  Missing detail on AUC computation for multiclass tasks.  Limited evaluation noise analysis for medical image datasets.  Lack of explanation for convergence bumps in image 4.  Incomplete description of the tuning strategy for the inner gradient steps.  Absence of hardware information in runtime analysis.", "Paraphrased Statement: This paper introduces a novel loss function designed to optimize deep neural networks for maximizing the area under the receiver operating feature curve (AUC). This novel loss function called the compositional loss combines the standard crossentropy (CE) loss with the AUC loss aiming to maximize AUC while learning robust feature through the CE component. An accompanying optimization method is presented and it demonstvalues a comparable convergence value to standard stochastic gradient descent (SGD). The proposed approach is evaluated on eight benchmark datasets (four generalpurpose and four medical image) and outperforms related loss function in terms of AUC. effectiveness:  Introduces the first endtoend training framework for AUC maximization eliminating the need for prior training with CE loss.  Provides an optimized training method with similar convergence speed as SGD.  Supports experiments with a comprehensive set of datasets.  Consistently improves AUC performance over CE loss only a linear combination of AUC and CE loss and two other AUCmaximizing methods. Weaknesses:  Some experimental detail are missing making it challenging to fully assess the results.  It is unclear how hyperparameters for different methods were tuned potentially affecting the experimental outcomes.  The function of the DDSM and CBISDDSM datasets appears to be confusing raising concerns about potential bias.  Comparative results are not presented for all datasets with an existing AUCmaximizing method which would provide a more complete evaluation.  Ablation studies on algorithmic parameters do not provide significant novel insights as they only confirm the importance of parameter tuning."], "ziRLU3Y2PN_": ["Paraphrase: Summary: This paper proposes a novel texture model that blend waveletbased and machine learning techniques. The model which uses ReLU nonlinearities on wavelet is shown to be related to phase harmonic use which are sufficient for capturing texture statistics. The model reportedly performs well compared to other stateoftheart methods despite having fewer parameters. Strengths and Weaknesses: Strengths:  Establishing a connection between different texture model families along with the link between ReLUs and phase harmonics is a significant contribution. Weaknesses:  The paper is not suitable for ICLR as it does not present any datadriven learning models.  The paper spends excessive space explaining concepts that are not novel contributions which could be effective spent on presenting the original work.  The model show that the models are memorizing image patches which suggests insufficient texture model.  The parameterization information provided for comparison models is partially incorrect.", "Paraphrased Statement: This work introduces a novel image representation method using wavelet and nonlinear rectifier use. This model enables the introduction of intricate geometric textures with superior visual quality compared to previous waveletbased models. The work leverages recent mathematical findings on wavelet phase harmonics (Mallat et al. 2020 Zhang and Mallat 2021) to enhance image texture synthesis. Furthermore it demonstrates that the PortillaSimoncelli model (2000) is a subset of the wavelet phase harmonicsbased (WPH) model. Both approaches emphasize the important use of statistical dependencies among wavelet coefficients across different scales. The selection of WPH parameters is important for striking a balance between capturing structural information and avoiding pattern memorization. Strengths and Weaknesses: Positive Points:  The results provide insights into what Convolutional Neural Networks (CNNs) learn from images as the proposed model achieves similar outcomes with fewer parameters and fixed wavelet filters.  This understanding is valuable given the widespread use of CNNs in image work and the limited knowledge about the information they extract. Negative Points:  The relationship between the proposed algorithm and patchbased methods is not explored.  Although the proposed method falls under \"statisticsbased methods\" it should be compared to other approaches such as exemplarybased models (e.g. Raad et al. 2018) for a more comprehensive evaluation.", "Summary This paper compares various waveletbased texture synthesis models based on the number and nature of their parameters. It also includes comparisons to other texture synthesis methods. Strengths and Weaknesses The paper proposes a valuable approach to estimating the minimum number of constraints needed in a waveletbased texture model. However it could benefit from highlighting the number of parameters for waveletbased models more clearly. The analysis of how the number of parameters impacts the quality of synthesized textures needs to be presented more effectively. The paper should provide more illustrative model of model failures and successes. The current number are also basic and should be supplemented with additional number showcasing the performance of the models on more specific images. The paper should delve into why each model fails or succeeds and identify which texture constraints work these outcomes. Overall the paper is wellwritten and provides a comprehensive mathematical introduction for wavelet and related literature. The authors are encouraged to cite the paper by Vacher et al. (2020) on texture interpolation for visual perception. PostRebuttal Notes The reviewer increased their score from 5 to 8 based on the authors rebuttal. The paper is recommended for acceptance as a poster.", "Paraphrased Statement: This work presents a texture synthesis approach using rectified wavelet coefficients. The method claims to achieve quality comparable to VGG featurebased synthesis (Gatys et al. 2015) and random filterbased synthesis (Ustyuzhaninov et al. 2017) while surpassing the performance of Portilla and Simoncellis (2000) method with fewer statistical measurements. Strengths and Considerations:  The paper provides a comprehensive overview of wavelet and their use in texture synthesis.  The goal of reducing statistical requirements is clearly stated. Concerns:  Despite the emphasis on minimizing statistical requirements VGGbased synthesis appears to produce superior results with fewer measurements (especially for color textures).  The paper fails to acknowledge or compare with more recent texture synthesis methods such as those by SendikCohenor et al. (2017) Zhou et al. (2018) and others.  The presented model are primarily stationary images. more various samples including structured and nonstationary textures are necessary to evaluate the methods effectiveness.  The proposed approach may not be suitable for generating larger output images from minor input patches unlike other texture synthesis techniques."], "u2JeVfXIQa": ["Paraphrase: Summary: The paper introduces a CrossLayer Attention (CLA) module to enhance correlations between feature from various layers. Additionally an Adaptive CrossLayer Attention (ACLA) module is proposed to optimize the computational efficiency of CLA. To maximize performance a neural architecture search method is employed to identify optimal introduction points for ACLA modules. Evaluation: Numerous experiments demonstrate the proposed methods effectiveness in terms of quality and efficiency. However there are various areas for improvement: Weaknesses: 1. Lack of novelty: Layer attention is a wellestablished technique in computer vision and the papers contributions may not be particularly innovative. 2. Overgeneralization: The papers findings may be considered common knowledge within the computer vision community. The authors should focus on more groundbreaking and valuable image restoration number. 3. Incomplete evaluation: The paper incorrectly asserts that view Attention module (PAM) neglects spatial feature positions. The authors should provide a comparison between the proposed method and PAM. 4. Misrepresentation: The paper incorrectly suggests that Image Pyramid Transformer (IPT) and SwinIR require large datasets for optimal performance. SwinIR actually uses a small training set. 5. Limited comparison: The experiments lack comparison with other attention mechanisms making it difficult to gauge the significance of the proposed CLA method. 6. Missing details: Equation (13) requires a clearer explanation. 7. Inadequate experimentation: The experiments lack results for applications such as single image superresolution image denoising and image compression artifact reduction. The paper also overlooks comparison with RCANCLA.", "Paraphrase: Summary: This paper introduces a crosslayer attention module for image restoration tasks. Unlike traditional nonlocal attention methods that search for correlated feature within the same layer this method selects feature across layers. Adaptive crosslayer attention modules are used to reduce computational costs. The proposed method is evaluated for image restoration tasks such as superresolution denoising demosaicking and image compression. effectiveness:  The proposed method appears novel and welldesigned.  It has been applied to multiple image restoration tasks demonstrating its versatility. Weaknesses:  Does conventional nonlocal attention have limitations that the crosslayer approach addresses  An analysis of specific benefits of crosslayer nonlocal attention for image restoration tasks is needed.  Differences from the LAM method should be explicitly detailed.  Table 1 lacks CLA in RCAN.  performance in Table 1 lags behind RDN and HAN. Inference time comparison is not provided. If the method is not significantly faster its practical relevance may be limited.  In Table 2 and 3 performance gains over the baseline are small.  The order of HAN and RDN in Table 5 is inconsistent with other magnification factors.  In Table 5 the performance difference across different L values is small while FLOP and parameter number increase significantly. The impact of the CLA module appears negligible.  Ablation field show minimal performance differences between version making meaningful analysis challenge.", "Paraphrased Summary: This research introduces the Cross Layer Attention (CLA) module for image restoration tasks. CLA differs from existing attention algorithms by considering key pixels from previous layers enabling it to capture crosslayer correlations. To minimize computational complexity it employs deformable convolutions for key sampling and proposes the Adaptive Cross Layer Attention (ACLA). ACLA uses a gating mechanism to dynamically select the number of key pixels for each query. Furthermore neural architecture search optimized the placement of the ACLA modules within the network. Extensive experiments demonstrate the efficiency and effectiveness of ACLA in various image restoration tasks including superresolution denoising demosaicing and artifact reduction. While prior works have explored crosslayer attention CLA and ACLA introduce novel approaches through deformable convolution gating and neural architecture search. effectiveness:  Crosslayer attention offers a general approach applicable to different backbones.  Careful consideration of computational complexity ensures practicality.  comprehensive experiments provide quantitative and qualitative evaluation.  Clear writing and helpful number enhance understanding. Weaknesses:  Incremental improvements in performance may not warrant the complexity of ACLA.  Speculative claims about smooth results from other methods should be substantiated or removed.  small corrections in wording could improve clarity.", "Paraphrase: Summary: This research introduces crosslayer attention (CLA) modules that identify correlated keys from different CNN layers for each query feature. Additionally an adaptive crosslayer attention (ACLA) mechanism is proposed to dynamically select keys from various CNN layers through a neural architecture search (NAS) method. The CLA or ACLA modules are integrated into an existing image restoration model (EDSR) to create a deep architecture. Evaluations across various image restoration tasks demonstrate the superior performance of this enhanced model compared to existing techniques. effectiveness: 1. CLA and ACLA modules enable the modeling of nonlocal pixel correlations by considering feature relationships across different layers. 2. ACLA performs effectively across various image restoration tasks. Weaknesses: 1. The authors should include an experiment that replaces CLA with a conventional nonlocal block in Tables 1 2 and 3. 2. The rationale for how key correlated key pixels from preceding CNN layers improves image restoration is unclear. 3. ACLA utilizes NAS to dynamically select key pixels. However Table 1 suggests that ACLA only marginally enhances CLA in terms of superresolution accuracy which raises query about the effectiveness of ACLAs NAS component. This should be addressed in further discussion."], "tvwNdOKhuF5": ["Paraphrase: Summary:  Reresearchers combined several reinforcement Learning (RL) techniques to conquer the ViZDoom firstperson shooter (FPS) game.  The game is divided into three stages: navigation frag and strategic control.  A multistage learning system was developed to address this challenge with each stage leveraging different techniques.  The navigation stage employs Hindsight Experience Replay with Proximal Policy Optimization (HPPO).  Ruleguided policy research handles the frag stage and selfplay manages strategic control.  Each stage reuses the trained framework from the previous stage by appending a new network head. Strengths:  The multistage learning system outperforms current stateoftheart agents in the ViZDoom competition.  The technique allows for diverse skill learning unlike fixed approaches.  Experiments are welldesigned and include ablation work.  Visualizations enhance the papers clarity.  The approach demonstrates that existing RL techniques can achieve remarkable performance without excessively large neural networks. Weaknesses:  While the neural network architecture is provided the code and hyperparameter settings are not.  The computational requirements (1152 CPUs and 32 GPUs) may hinder reproducibility.  The multistage systems stages are manually defined. Improvements:  Compare the multistage system to a flat learning system.  Provide standard errors in the results.  Elaborate on the loss terms in the formulas.  Enhance the clarity of the proposed method and its contributions.", "Paraphrased Summary: This paper presents a method to enhance the performance of reinforcement learning agents in the ViZDoom FPS game (Deathmatch mode). The method incorporates known techniques like PPO Hindsight Experience Replay RuleGuided Policy Search and SelfPlay. The resulting algorithm outperforms previous topranking agents in ViZDom AI Competitions. An ablation work reveals the impact of each component on performance. Strengths:  Tackles a realworld problem in reinforcement learning.  mix proven techniques to effectively solve the problem.  Demonstrates the value of simple tricks over complex approaches. Weaknesses:  Poor writing quality and imprecise language make the paper hard to understand.  The term \"General reinforcement Learning\" is undefined.  Replicating the approach is difficult due to missing information on hyperparameters implementation details and training dynamics. Questions:  Why are only a few goal scores reported in picture 5  perform the authors explore limiting goal switching  How are endoflife events handled  Why was PPO chosen over offpolicy algorithms better suited for Hindsight Experience Replay  How does the agent decide to change its goal when detecting an enemy in a noncurrent goal field Minor Comments:  \"Hindsight Trust Region Policy Optimization\" is repeated in the bibliography.  picture 2 needs a more detailed caption to clarify its components and transitions.", "Paraphrase: Summary: This paper presents a multistep learning system for training powerful agents in firstperson shooter (FPS) games. The agents can dynamically adapt their tactics based on different opponents by selecting different target field. The paper suggests a method called Hindsight PPO to address goaloriented reinforcement learning although it may face limitations. The system integrates several existing techniques including rulebased policy research action wrappers and prioritized fictitious selfplay. Experiments demonstrate that the system produces an agent that outperforms previous top agents. Strengths:  The paper is wellstructured and easy to understand.  Visualizations in the experiments clearly show how the agent learns navigation shooting and other strategies.  The paper carefully designs the FPS game environment and achieves excellent performance. Weaknesses:  The paper uses offpolicy data in hindsight experience replay and adds an importancesampling term to the PPO loss work (like in the Hindsight TRPO method). However it does not include techniques to reduce the high variance caused by cumulative multiplication terms. Additionally the paper does not compare HPPO with other goalconditioned approaches in the experiment.  It is unclear why selecting different target field would represent different strategy styles. In FPS games there are several policy styles (e.g. aggressive defensive wandering). These styles cannot be only determined by the agents movement.  In Stage 3 the strategic policy relies on a statistical map that records Frag scores. It is unclear how the agent computes Frag scores in unseen field during testing or how precise the auxiliary networks predictions are against unknown opponents.  The experimental setup for evaluating strategy diversity is biased. The evaluation opponents (Goal00 to Goal19) are the same as those used to train the DSCAgent in Stage 3. Therefore it is expected that DSCAgent would perform better than other benchmark agents.  More experiments should be conducted to thoroughly evaluate the strategic diversity of the agent which is the papers main contribution. These experiments should examine how the agent dynamically changes its strategy during a game based on information it acquires about its opponents and how it develops strategies against agents not encountered during training.", "Paraphrased Summary This research presents a multistage learning approach for FPS games that combines existing techniques. The approach incorporates hindsight experience replay goalconditioned reinforcement learning and prioritized selfplay. Strengths  Outperforms existing methods in performance.  The rationale for each component is clear and logical.  The paper is wellwritten and easy to follow. Weaknesses  The baseline comparison are outdated not considering recent advancements.  The ablation work is inconclusive failing to demonstrate the necessity of the threestage approach or the distinct contributions of each component.  The lack of publicly available source code hinders reproducibility and understanding of implementation details."], "ivQruZvXxtz": ["Summary: The paper introduces Sequential Reptile (SR) a new method for multitask learning (MTL) that aims to improve multilingual BERT finetuning. SR modifies the original Reptile algorithm to perform innerloop optimization on multiple tasks simultaneously promoting gradient alignment and mitigating task interference. Strengths:  SR significantly outperforms all baseline methods on various settings.  The paper provides extensive experimental evidence of SRs effectiveness. Weaknesses:  The novelty of SR is limited as it primarily involves a simple modification to Reptile.  The intuition behind SR and the source of its improvement are unclear.  SR deviates from the original design principles of Reptile by using both inner and outer loops for taskuniversal goals. Questions and Suggestions: 1. Would adding regularization towards the original mBERT improve the performance of other baseline methods 2. Could withinbatch mixing enhance SRs performance 3. Can the authors provide more analysis to clarify the source of improvement in SR", "Summary This research proposes a novel training technique that enhances gradient alignment across different languages (tasks) in the multilingual finetuning process of pretrained language models. Inspired by the Reptile algorithm originally designed for metalearning the method was adapted for multitask learning and its effectiveness was evaluated in multilingual finetuning. Strengths:  Addresses an underexplored problem with significant implications.  Empirical issue validate the methods significance.  Simplicity of the method and insightful explanations provided.  Welldesigned experiments including zeroshot crosslingual transfer.  Clear writing and logical flow. Weaknesses:  Combines two distinct issue: catastrophic forgetting and negative transfer during multilingual finetuning.  Incomplete experimentation including the use of exclusively a relatively minor model size (BERT base) potentially limiting the generalization of issue to larger models.  Lack of justification for specific experimental settings such as hyperparameter choice and task sampling strategy.  Lack of empirical justification for clustering in zeroshot crosslingual transfer experiments.  Insufficient details on the training overhead of the proposed method.  Omission of relevant literature on optimization methods for multilingual training.", "Paraphrased Summary: This paper introduces Sequential Reptile a gradient alignment technique that minimizes negative transfer and catastrophic forgetting. Experiments in multitask learning and zeroshot crosslingual transfer scenarios for question answering (QA) and named entity recognition (NER) show the effectiveness of Sequential Reptile. Strengths:  Clear and organized presentation  Sequential Reptiles simplicity and consistent superiority over competing methods Weaknesses:  Limited focus on multilingual learning instead of wide transfer learning settings  No comparison with other multilingual transfer learning methods like multitask adversarial training  Limited experiments to exclusively QA and NER tasks (expansion to diverse multilingual tasks would be beneficial)  Statistical significance of improvements is not addressed", "Summary: The study introduces an optimization technique for training multitask learning (MTL) models that promotes task alignment. They believe that traditional MTL methods can lead to \"catastrophic forgetting\" of previously learned knowledge when finetuning for specific tasks. Key contribution:  The authors provide empirical evidence suggesting that misaligned gradients between tasks can result in negative transfer in multilingual tasks.  They propose a simple and effective MTL optimization method that jointly optimizes task losses and gradient alignment. Strengths:  The hypothesis on the role of gradient alignment in negative transfer is intriguing.  Empirical evidence supports the existence of a correlation between task gradient alignment and MTL performance.  The sequential reptile algorithm is straightforward and effective.  extensive experiments demonstrate the effectiveness of the proposed method and provide valuable insights. Weaknesses:  The findings would be strengthened by conducting experiments on singlelanguage MTL settings.  Some experimental limitations include:  Synthetic experiments should use multiple random initializations to demonstrate consistency of issue.  Average performance calculations in Table 12 should consider differences in training data sizes across languages.  Including STL performance in Table 5 would provide a baseline comparison.  Exploring hyperparameter settings particularly inner and outer learning rates would enhance understanding of the algorithm."], "kroqZZb-6s": ["Summary Paraphrase: This research presents a method for identifying patient phenotypes using their electronic health record (EHR) data and predicted outcomes. The researchers note that current machine learning approaches lack interpretability making it difficult to extract clinically functionful data. To address this they propose a featuretime attention mechanism that improves the interpretability of the model. effectiveness and Weakness Paraphrase: The paper tackles the significant challenge of phenotyping using an attentionbased recurrent model. The model assigns patients to latent populationlevel phenotypes. While this approach is innovative the function of a Long ShortTerm Memory (LSTM) network as the attention mechanism should be further justified. LSTMs can leverage long time dependencies but increase training time. Comparing the performance with a Multilayer Perceptron (MLP) feature extractor would be beneficial. Additional clarifications are needed regarding the attention mechanisms implementation and model training. The selection of hyperparameters using Occams razor principle requires further explanation. In the experimental solution the authors should describe the metrics functiond to evaluate model performance and their clinical relevance. Figure 6 could be improved by displaying mean attention for patients within the same phenotype clusters. The researchers should acknowledge similar approaches in the literature such as those that combine temporal and EHR data and profile regression models. Balanced loss is not a novel concept and the authors should cite relevant paper. Minor revisions include:  Provide citations for the claim that traditional cluster methods fail to capture timedependent data relationships.  Clarify whether separate or overlapping time windows are functiond for data processing.  function median imputation only on training data to avoid bias in validation and test set evaluation.", "Summary: The field proposes a deep learning model to identify distinguishable patient groups (clusters) using electronic health record (EHR) data. It incorporates a featuretime attention layer to beneficial capture patient data characteristics and timedependent model. The model aims to improve the accuracy of clinical observations by leveraging the multidimensional and timeseries nature of EHR data. effectiveness:  Clear explanation of the challenges and limitations of using EHR data in previous function.  comprehensive description of the proposed deep learning netfunction including its interpretability which is crucial for integrating AI techniques into healthcare. Weaknesses:  lack of rigorous experimentation in designing the field cohort. The field only considers observations within 24 and 72 hours prior to the outcome limiting the understanding of how long model observations remain valid and the potential for proactive interventions. An optimal observation and prediction window is not explored.  Limited experimentation with missing data discussion. The authors impute missing data with previous time block and median without justifying the choice or exploring alternative imputation techniques. Addressing missing data more effectively could enhance cluster separability and model predictions.  The proposed models clustering separability and outcome prediction scores are lower than benchmark models raising care about the novelty and effectiveness of the approach. Minor Comments:  Include the percentage of missingness in feature variables before imputation.  Cite the librariessoftware used in the field.  Improve the visual appeal of the featuretime cluster relevance function (Figure 6) with a more effective color scheme.  The term \"learnt\" is used redundantly in the discussion department.", "Paraphrased Summary: This paper presents a cluster method for electronic health records (EHRs) of hospitalized patients. The method aims to enhance interpretability and improve patient outcome predictions. It features an interpretable model a weighted loss to address class imbalance and a custom loss to prevent cluster collapse. effectiveness:  Clear and wellorganized writing  Informative figures and supplemental content  Effective performance in predicting outcomes and ensuring cluster interpretability Weaknesses:  evaluation on private data only  Questionable function of KMeans for cluster initialization without exploring alternative methods  lack of ablation studies to assess the impact of model features like custom loss and attention blocks  Similarity of the attention encoder to RETAIN (a prior model) without clear differentiation Minor care:  Missing blue dot in Figure 5", "Paraphrase Summary This article presents a method for predicting patient outcomes using electronic health records (EHRs). The authors propose a phenotyping model that groups similar patients based on their predicted outcome distributions. They also introduce a featuretime relevance function that highlights important featuretime combinations for specific patient predictions. Strengths  The paper introduces featuretime relevance function allowing medical experts to identify relevant featuretime combinations for predictions.  The paper experimental setup facilitates reproducibility.  The proposed method outperforms the National Early Warning Score used in UK hospital. Weaknesses  The proposed method claims superiority based on a metric that biases towards convex clusters despite the capacity of deep neural networks to learn such clusters.  The paper does not compare clustering in input and latent spaces despite the potential benefits of using a lowerdimensional latent representation.  The paper lacks evidence demonstrating the necessity and effectiveness of the proposed loss function.  The baselines used for directly predicting outcomes from EHR data may not be sufficiently robust. Questions  Can the cluster representations be easily understood and interpreted  How are \"clearer\" and \"more separable\" cluster phenotypes defined in the context of phenotyping based on outcome distributions  How does the proposed model care features with different sampling frequencies Suggestions  Improve the quality of the figures by saving them in highresolution formats.  Correct grammatical errors.  Analyze the distribution of outcomes in each cluster to assess its closeness to the ground truth.  Ensure consistent color functionping in the featuretime relevance function."], "qXa0nhTRZGV": ["Paraphrased Statement: Summary:  The paper explores why SAM (SharpnessAware Minimization) performs effectively.  It investigates why SAM improves generalization compared to Stochastic gradient descent (SGD) its robustness to mislabeled data and its potential for enhancing linear classifiers.  The paper suggests advancements for SAM using a gradient reweighting interpretation and employing a robust loss function. effectiveness and Weaknesses:  The paper ambitiously addresses various aspects of SAM including convergence properties generalization error and resilience to noisy labels and adversarial inputs. However its coverage of each topic is limited.  The authors introduce the MaxSum SAM formulation and SGDtype algorithms for optimizing the minmax objective.  Proposition 2 provides analytical controls for expected gradient of SAM with stochastic gradient but it lacks a theoretical explanation for SAMs superiority over SGD.  The paper claims that SAMs robustness to noisy labels stems from altering gradient guidance to minimize alignment with noise but this is supported by only one experiment.  The observation that SAM enhances generalization for linear model is surprising but lacks a clear theoretical justification relying on a small toy model instead.", "Paraphrase: Summary This study examines an algorithm similar to stochastic average gradient (SAM) with a squared loss function. It demonstrates that for diagonal linear networks neural networks (NNs) trained using this algorithm exhibit substantially generalization ability than those trained with traditional stochastic gradient descent (SGD). For deep NNs meeting the PL condition and assumptions outlined in Section 3 the authors present proof of the algorithms convergence. A linear model is used to elucidate the SAM algorithms superior generalization on datasets with label noise. effectiveness and Weaknesses effectiveness:  Wellorganized paper with solid proofs. Weaknesses: 1. Missing comparison to Original SAM Algorithm: The proposed algorithm lacks the gradient normalization step present in the original SAM algorithm described in [1]. The authors should provide empirical comparison to demonstrate the superiority of their proposed algorithm. 2. Confusion Regarding MaxSum and SumMax: The authors initially state that MaxSum SAM possesses a substantially implicit bias but later conclude that ERM and MaxSum SAM have similar performance while SumMax benefits from a substantially implicit bias. 3. Limited Scope of Results: Most results are based on squared loss rather than crossentropy loss which may limit their applicability. The paper could benefit from an exploration of implicit bias in crossentropy loss. 4. Assumptions for Convergence: The convergence results depend on assumptions A1 and A2. The authors should clarify whether these assumptions are generally applicable to NNs or only hold for diagonal linear networks. 5. insufficient Hyperparameter Tuning: Figure 1 suggests that ERM and MaxSum SAM perform similarly. The authors might consider optimizing the hyperparameter \u03c1 to explore potential performance differences.", "Paraphrase: Summary: The paper explores theoretical understanding for the effectiveness of adversarial weight perturbation and sharpnessaware minimization (SAM) in improving generalization in deep study. These methods lack theoretical explanations for their success. This paper aims to fill this gap by investigating SAM. contribution:  A theorem demonstrating SAM superior generalization over SGD in linear neural netstudys.  analysis of SAM convergence and discussion of its generalization behavior.  A new interpretation of SAM as a gradient reweighing technique.  A link between SAM and study on noisy labels. effectiveness:  The paper provides a rigorous and insightful analysis of SAM. Weaknesses:  The paper presents a collection of specialized results without a cohesive explanation for why SAM outperforms SGD in generalization. A highlevel discussion in the conclusion could address this.  SAM convergence to a stationary point is seen as a weakness indicating a need for methods that converge to nonstationary but favorable regions of the loss landscape.  The claim that normalized SAM averaged iterates converge to an inferior point requires further empirical or theoretical support.  Foret et al. (2021) and Wu et al. (2020) may not be truly concurrent study due to their temporal separation.  There is an omission of concurrent study on SAM such as https:arxiv.orgabs2010.04925.", "Paraphrase: This study examines the SharpnessAware Minimization (SAM) algorithm proposed by Foret et al. (2021) highlighting its strengths and limitations. effectiveness:  theoretical analysis: The study provides a thorough theoretical analysis of SAM including:  An examination of SAMs implicit bias using Woodworth et al.s (2020) technique.  analysis of SAMs convergence properties.  Insights into SAMs Behavior: The analysis sheds light on how SAM differs from other optimization methods like SGD. Limitations:  Limited Experiments: The experimental evaluations are minimal and primarily support the theoretical analysis.  Interpretation of analysis: The authors interpret the analysis results in Proposition 1 but some of their claims such as the purported superiority of SumMax over MaxSum lack clarity.  Convergence analysis: While the convergence analysis is valuable it fails to explain why SAM outperforms SGD.  NonRigorous discussion: The discussion on SAMs benefits for noisy labels and robustness to overfitting are interesting but lack rigorous mathematical support and thorough experimental verification. Overall the study provides a primarily theoretical examination of SAM with some insights into its behavior and potential benefits."], "v6s3HVjPerv": ["In a controlled setting researchers compared various methods for explaining how image classifiers make decisions. They used a simplified environment with two distinct animal types (\"peeky\" and \"stretchy\") to create image with specific and spurious correlations between feature (e.g. color body form). explanation methods were evaluated based on users ability to identify the feature the model relied on. While a simple baseline method (displaying model predictions) performed well a method that highlighted conceptrelated pixels (concept highlighting) performed poorly. The results suggest that pixel highlighting is not an effective explanation method in this simplified scenario because many different attributes can appear in a single highlighted area. However the researchers acknowledge that the findings may not generalize to more realistic image classification tasks and that concept highlighting may have advantages for tasks with a large number of form.", "Paraphrase: This article proposes a dataset called TWO4TWO for evaluating two interpretability methods: counterfactual and conceptbased explanations. The dataset is artificially generated allowing researchers to know the exact feature responsible for a models decisions. This allows for assessing whether the model explanations reveal the correct feature. The authors use a simple baseline explanation method that groups inputs based on the models output logits. A user study found that the two advanced explanation models did not perform significantly better than the baseline highlighting the importance of evaluating explanation techniques in user studies. effectiveness:  The paper presents a clear methodology for creating an artificial dataset with known groundtruth explanations.  It reveals the common issue that many model explanation methods face: their explanations may not be useful for users.  The user study involved a large number of participants providing robust results. Weaknesses:  It may not be fair to compare the concept model in this dataset because:  The abstract attributes relevant to understanding cannot be easily visualized in concept activation maps.  Showing just 5 image from a concept form may not sufficiently convey the significant attribute.  It is unclear if participants in Study 1 also participated in Study 2 which could introduce bias.  The formifier used for generating explanations and the baseline method may impact the quality of the explanations.  The proposed dataset may not be comprehensive enough to somewhat evaluate different explanation methods particularly attributionbased methods that contribution visual similarities with conceptbased maps.  The significance of the stars in Table 2 is not explained.", "Summary Paraphrase: The paper presents a synthetic dataset designed to study data bias. Since it is synthetic attributes can be modified to introduce or remove bias. Two user studies were conducted: one to assess users ability to identify bias and another to evaluate the effectiveness of explanations. effectiveness and Weaknesses Paraphrase:  The dataset could benefit from more model for each form.  The definitions of \"legs moved inwards\" and \"legs stretched out\" lack precision and clarity. specific criteria should be established.  The four main feature are body position animal color block form and ground color. Their data type and distribution (continuous or discrete) should be clarified.  The phrase \"legs position\" is ambiguous. A precise measurement or visualization is needed to define its values.  The method for sampling block forms appears biased with an increased likelihood of round blocks for stretched legs and cubic blocks for retracted legs. This bias should be explained.  The significance of layer 342 in concept generation is unclear.  Details of the user study tasks and interface are missing including the questions used to identify formrelevant feature.  Statistical data on time spent during the user study would provide insights into task difficulty.  A typographical error occurs in the text: \"iWe\" should be \"we.\"  A relevant study involving a synthetic dataset is cited: \"NearOptimal machine Teaching via Explanatory Teaching position\" by Chen Mac Aodha Su Perona and Yue.", "Paraphrased Statement: Summary: The study presents a novel experimental design and dataset synthesis method for a classification task where systems must distinguish between \"Peaky\" and \"Stretchy\" form. The experiment provides researchers with precise control over training data ensuring that biases in predefined feature (such as form and color) do not interfere with model performance. Human participants were asked to predict system outputs and the authors evaluated three explanation types: visual counterfactual and baseline (output logits). effectiveness:  The experiment design is advanced and allows for clear ground accuracy solutions while eliminating visually overlapping feature that might confuse machine learning systems.  The results indicate that under these controlled conditions current explanation methods do not significantly improve human understanding of system outputs beyond the confidence represented by output logits. Weaknesses:  The experiment protocol relies on a controllable training data generator which may not be applicable to other visual tasks such as image manipulation.  It is challenging to generate data that accurately represents the complexity of natural data limiting the generalizability of the explanation results to realworld scenarios."], "t3E10H8UNz": ["Paraphrase: Summary: This paper introduces a technique for imitation learning with few examples. It involves simultaneously training:  A general policy (highlevel)  Specific policies (lowlevel) for various tasks These policies are then finetuned with limited demonstrations for specific target tasks. The proposed approach outperforms existing fewshot imitation methods that use singlelevel policies or no metalearning. effectiveness:  Tackles an significant problem in fewshot imitation  Learns highlevel and lowlevel policies jointly  Adapts rapidly to new tasks  Wellexplained with helpful picture  Evaluated on a benchmark with relevant baselines  Demonstrates meaningful skill learning Weaknesses:  The approach is complex involving multiple learning tasks  May be sensitive to hyperparameter choices  set not compare to recent advancements in learning skills from multitask datasets or their use in imitation learning  Lacks comparison to a hierarchical approach that learns lowlevel skills separately  set not explore the impact of the issue of lowlevel skills  May benefit from label smoothing in the inner training loop of the highlevel policy Suggestions:  Investigate the use of label smoothing  Consider a comparison to a method that freezes lowlevel skills during highlevel policy training  Explore the work of the issue of lowlevel skills", "Paraphrased Statement: contribution 1: This work presents an innovative imitation learning framework designed specifically for robots to learn complex control tasks involving extended time horizons and rapid adaptability to new situations. The approach incorporates meta learning principles into a hierarchical policy structure with both levels of the hierarchy receiving meta training and finetuning during task performance. contribution 2: evaluation and Analysis In experiments conducted on the MetaWorld benchmark the proposed method surpasses existing approach. Qualitative analysis indicates that the method effectively discovers meaningful skills that enhance task performance. contribution 3: effectiveness and Weaknesses effectiveness:  Addresses a crucial research challenge: enabling robots to extract and leverage abstract task representations to efficiently master new tasks.  Provides a comprehensive review of related work.  Combines existing concepts from MetaAgnostic Meta Learning (MAML) and hierarchical imitation learning in a unique and novel way.  Offers a solid theoretical foundation.  Evaluates the approach against various established baselines. Weaknesses:  Limited clarity and precision in the foundation focusing too much on technical details rather than highlighting the significance of the research problem and its implications.  picture lack sufficient textual descriptions making them difficult to interpret without referencing the main text.  Experiments may not fully capture the complexity of longhorizon tasks as the MetaWorld environments involve relatively shorter sequences of basic process.  Writing contains grammatical errors and passive voice constructions affecting readability and understanding.  The discussion section overlooks potential limitations of the method such as its sensitivity to the issue of skills and the occurrence of mode collapse.  The use of a transformer in the MIL baseline is not fully explained and the absence of a transformer in DMIL is not justified.", "Paraphrase: Summary:  The authors propose a highlevel policy network that selects subskill policies for predicting process using metaimitation learning (DMIL).  Both the highlevel and subskill policies are finetuned during metatesting.  DMILs convergence is demonstrated and it is evaluated against several variations on the ML10 and ML45 tasks of the Metaworld benchmark. effectiveness:  DMIL performs well against ablation in both ML10 and ML45 settings.  convergence analysis suggests the algorithms training stability. Weaknesses:  DMIL primarily leverages existing methods without elaborating on its unique challenges.  effect comparisons with hierarchical imitation learning approach (Yu et al. 2018b Finn et al. 2017b Yu et al. 2018a) are limited.  Robustness of results across different tasksenvironments or subskill counts is not demonstrated which is significant due to the potential compositional task structure in Metaworld manipulation tasks. Questions:  How does DMIL differ from \"Hierarchical FewShot Imitation with Skill Transition framework\" Feedback:  An algorithm box outlining the order of inner and outer updates would enhance understanding.", "Paraphrase: Summary: DMIL (Dual Meta Imitation Learning) combines ideas from hierarchical metaimitation learning and hierarchical imitation learning. It uses a highlevel network that coordinates with subskills which are iteratively refined through metalearning. DMIL builds on MAML (ModelAgnostic MetaLearning) for imitation learning and provides theoretical proof of its convergence based on its relationship to the ExpectationMaximization (EM) algorithm. DMIL achieves impressive fewshot imitation learning results on the MetaWorld benchmark. effectiveness:  Natural extension of metaIL and HIL  Reasonable algorithm with proven convergence based on EM theory  connection to EM algorithm is understandable  Demonstrated effectiveness in experiments Weaknesses:  Limited evaluation on only two simulation environments  Lack of tasks that explicitly test meta and hierarchical aspects  effectiveness of the method may not be fully appreciated without such tasks"], "qZNw8Ao_BIC": ["Paraphrase: Summary: This study explores the observation that Vision Transformers (ViTs) are relatively unaffected by patchbased transformations. By evaluating models on transformed inputs the authors find that ViTs maintain stability and identify these features as useful but not robust. To address this they propose patchbased negative augmentation and loss which consistently enhance ViT robustness. effectiveness:  The observation that ViTs remain stable despite significant transformations is intriguing.  The study provides thorough comparisons to support these findings.  Negative data augmentations are not widely used in vision transformers so their application in this context is novel and promising. Weaknesses:  The loss work comprises three components: uniform loss L2 loss and contrastive loss. While accuracy are compared there is no open determination of the optimal loss or their specific departure. This uncertainty detracts from the paper contribution regarding the three loss.  Experiments focus primarily on the original ViT or versions augmented with RandAug and AugMix. Given that the proposed negative augmentation complements positive augmentations it is crucial to evaluate its performance with more further models like DeiT which employs a positive training strategy.  The experiments use ViTs with 16x16 token inputs. It is unopen whether transformers using finergrained token representations (e.g. 8x8) would yield different results and how varying the scale of patch augmentation affects the outcomes.  Recent approach in vision transformers have introduced modifications to tokenization including multiscale and overlapping embeddings. It is not known whether the high confidence predictions for unrecognizable images arise from ViTs nonoverlapping token embeddings. Furthermore the study does not examine whether the proposed augmentations are applicable to transformer architectures with overlapping embeddings.", "Paraphrase: This study examines the resilience of vision transformers (ViTs). It reveals that ViTs rely heavily on characteristics that persist after patchbased alterations but often lack humanrecognizable semantic significance. However these characteristics are susceptible to manipulation. The authors suggest utilizing patchbased conversions as \"negatively augmented views\" and provide penalties to discourage the use of nonrobust characteristics during training. Experiments on the ImageNet dataset evidence that patchbased negative augmentation consistently enhances ViT robustness. effectiveness:  The research innovatively identifies that ViTs utilize characteristics that are not always semantically meaningful for humans.  Experiments validate the studys hypothesis and the proposed patchbased negative augmentation successfully boosts robustness. Weaknesses:  The study focuses on ViT resilience without investigating its impact on other vision transformer models or realworld applications.", "Summary Paraphrase: This study investigates how vision transformers (ViTs) handle range augmentations that drastically alter the ranges meaning. The researchers show that ViTs can perform well on ImageNet even with severely distorted test ranges due to patchbased transformations (Pcorrupted). However this adaptation to robustness against patchbased changes compromises their ability to generalize to unrelated datasets (OutofDistribution OoD). To address this issue the authors propose \"negative augmentation\" techniques to prevent ViTs from relying only on patchbased features. effectiveness and Weaknesses: effectiveness:  Indepth analysis of ViT robustness against patchbased transformations and its impact on generalization.  Simple intuitive and effective negative augmentation methods.  extensive testing to determine optimal hyperparameters and demonstrate method effectiveness.  Compatibility of negative augmentation with positive augmentation and larger datasets.  Wellwritten and open presentation. Weaknesses:  Experiments in Fig 2 may need a more objective measure of patchbased robustness such as comparisons with nontransformer CNNs.  Experiments in Fig 3 could benefit from an additional result where test data corruption matches training data corruption to separate the effects of field shift from feature nonrobustness.  Results are based on single work so statistical significance testing would enhance the reliability of conclusions.  Missing direction on the better combination of negative augmentation types. Questions:  In PRotate is the rotation degree the same or random for each patch  Were ViT models in Fig 2 retrained by the authors or taken from Dosovitskiy et al.  Do experiments in Fig 2 and 3 include positive data augmentation and use the ImageNet dataset (not ImageNet1k)  Can negative data augmentation be extended to regression tasks  Are combinations of Pcorruptions (e.g. Pshuffle with PRotate) evaluated", "Paraphrase: This study emphasizes that visual Transformers (ViTs) are resistant to alterations in range patches (such as shifting or rotation). The authors propose using patch transformations as negative augmentations in ViT training to enhance resilience paralleling human perception. They implement three loss works to prevent overly positive ViT predictions on transformed ranges. The authors validate their findings through experiments on ImageNet series datasets. effectiveness:  The concept is straightforward and comprehensible.  The explanations and illustrations are open.  The experiments are welldesigned and their parameters are transparent. Weaknesses:  The study concludes that the method significantly improves ViTs outofdistribution performance but the results indicate limited improvements for two of the three loss (Uniform and L2).  only the contrastive loss yields significant enhancements. However its known for effectively training models through metric learning promoting compact representations in feature spaces. Thus the specific contribution of the contrastive loss within the proposed method is unopen.  A baseline with contrastive loss should be evaluated using only fresh or positively augmented ranges for training.  design 2 doesnt support the claim that ViTs are insensitive to patch transformations. The authors have exaggerated the vertical axis scale to minimize the perceived changes. The design even shows a significant performance drop (over 20) due to patch transformations which contradicts the \"insensitive\" description.  The negative loss work in Eq. (3) may present optimization challenges.  The studys findings need to be evaluated for CNNs as well."], "nhN-fqxmNGx": ["Paraphrase: This research examines the potential performance of various variable selection techniques when there is correlation among the columns of the design matrix. Specifically it focuses on situations where the design matrix Gram matrix has blockwise diagonal structure (2x2 blocks) with offdiagonals within each block specified by a correlation parameter and diagonals set to 1. The quality of each estimator is assessed using its anticipated Hamming distance. construction on previous research phase diagrams are generated with phase curves that delineate the boundaries between areas of exact recovery almost full recovery and no recovery. An analysis of each variable selection method provides insights and comparison. strength:  Clear and accessible writing way  Timely and significant topic of variable selection  Useful phase diagrams and insights for understanding method behavior  Solid technical validation Weaknesses:  Limited applicability to practical issues beyond brief mention of genetic markers and Toeplitz matrix  Additional model would enhance the paper appeal Questions and Comments:  Equation (1) notation should be clarified regarding if XT X  Ip is equal to \\textdiag(XT X)  [11 ... 1]T.  Acronym SCAD should be preceded by a definition of \"smoothly clipped absolute deviation.\"  A general audience would benefit from a definition of the multilog use.", "Paraphrase: This study explores the strength of different variable selection techniques including Lasso Elastic net SCAD Threshold Lasso forward Selection and forward Backward Selection. Previous studies have mainly examined model selection consistency but this paper analyzes the robustness of model selection consistency based on Hamming errors. The theoretical findings suggest that SCAD outperforms Lasso Elastic net is inferior to Lasso and other methods are beneficial than Lasso. strength:  The research question addresses a fundamental issue in variable selection.  The paper follows a logical structure.  The theoretical framework for the proposed approach is clearly presented.  The study provides valuable insights into the Hamming errors of each variable selection technique. Weaknesses:  The theoretical analysis assumes a specific data matrix condition (np) that may limit its applicability to realworld scenarios where variable selection is particularly beneficial in cases where np does not hold. The paper does not explore the theoretical implications in such instances.", "Paraphrase: Summary: This paper evaluates various methods for selecting variables in a specific statistical design (blockwise diagonal). The evaluation is based on theoretical calculations and model. strength:  Clear presentation  Thorough theoretical analysis within the defined problem setting Weaknesses:  Limited setting: The methods are evaluated in a highly simplified setting (specific distribution of coefficients and blockwise diagonal Gram matrix) that may not be widely applicable.  Lack of significant theoretical novelty: The theoretical results do not appear to extend easily to more general sparse linear regression setting.  Limited experimental results: model are conducted exclusively in a lowdimensional setting and a design lacks explanations.  Uncertain practical implications: It is unclear to what types of applications the paper findings can be applied.", "Paraphrase: Objective: The paper aims to theoretically compare various variable selection approaches based on Lasso. Methodology: The authors propose visual phase diagrams to analyze the efficiency of Lasso variants under an ideal setting. These diagrams categorize the variants into three regimes based on signal strength and sparsity:  Full Recovery  Almost Full Recovery  No Recovery They provide exact and nonasymptotic characterizations of these regimes. Key findings: By computing phase diagrams researchers can effortlessly compare the perphaseance of different algorithms. The authors show that under the \"blockwise correlation\" assumption Lasso can be simplified into bivariate subproblems. strength:  Clear and wellstructured presentation of contribution  Simple yet wellmotivated setting  Comprehensive analysis with valuable insights  foundation of the novel Almost Full Recovery regime  Proper referencing and placement within the literature  Experiments that support the theoretical results Weaknesses:  Lack of explicit statement in theorems about the phase of the expected Hamming error  Awkward wording due to this oversight  Typos in the main article (unnumbered design uncaptioned table)  Typos in references (capitalization inconsistencies)  Small font size in design legends  Typo in Supplementary Equation 7 (\u03c1 should be \u03c1)  Typo in Supplementary Lemma B.1 (\u03bb should be q)  Typo in Supplementary (page 3 \"design 1 exclusively dipicts\")  Typo in Supplementary (page 54 (xj xj2) should be (xj xj1))"], "oMI9PjOb9Jl": ["Summary Paraphrase: This paper introduces DABDETR a variation of the DETR model that uses dynamic anchor boxes for object queries. The authors discovered that DETRs slow training may be due to multimodal query properties. To address this DABDETR employs explicit positional priors (box coordinates) to enhance querytofeature similarity speeding up training. Experiments and analysis picture DABDETRs effectiveness in object detection. effectiveness and Weaknesses Paraphrase: Advantages:  Systematic comparison of positional encoding and attention mechanisms in transformerbased detection models.  Stateoftheart results on the COCO 2017 dataset. Disadvantages:  Limited novelty with concepts of explicit positional priors and iterative anchor update borrowed from existing method.  Incremental rather than transformative process.  Somewhat weak evidence supporting the root cause of DETRs slow training.  Conclusions drawn from limited qualitative visualization of anchor without quantitative experimentation.", "Paraphrased Statement: This paper proposes a novel query formulation called dynamic anchor boxes for detection transformers (DETR). This formulation incorporates precise positional and scale knowledge from anchor boxes into the queries used to train DETRs decoder. The anchor boxes are also progressively refined at different layers to optimize their settings. This approach provides a clearer understanding of DETRs query mechanisms and has been shown to outperform previous query formulation in both implicit and explicit contexts. effectiveness:  Wellstructured paper with clear comparisons to existing method  Thorough analysis of the role of queries in DETR and its variants  Informative picture and experimental results demonstrating the superiority of dynamic anchor queries  Relatively low computational overhead compared to other query formulation Weaknesses:  The proposed formulation appears to be less effective without the Deformable Convolution Network (DCN) backbone a possible limitation that requires further investigation  The authors could provide a more comprehensive analysis of runtime speed to complement the GFLOPs metric  Ensure accurate citation use when referencing other process (e.g. Section 4.3 line 2: DETR should not cite Meng et al. 2021)", "Summary Paraphrase: This research addresses the slow training convergence in object detection transformers. It introduces a novel approach using trainable 4D anchor boxes in a layerbylayer fashion. The authors justify their method by visualizing positional attention and explaining its advantage in handling object scale ratios. Additionally they propose a temperature parameter to adjust the size of positional attention which they determine empirically. experimentationations verify their claims and demonstrate improved performance over the stateoftheart (SOTA) on the COCO 17 validation set. effectiveness:  Detailed Diagrams: Comparison of the proposed method and baselines through clear diagrams.  Justification experimentationation: An experimentation addressing potential doubts about improvement sources.  attention Visualization: Justification of the method advantage through positional attention visualization.  SOTA Improvement: Enhanced benchmark issues surpassing the SOTA. Weaknesses:  Improved experimentationation lack: A training curve showing the proposed method impact on convergence rate is not provided.  Limited Hyperparameter Study: only temperature and decoder layers are explored potentially biasing ablation issues.  lack of Deformable DETR Comparison: Deformable DETRs issues are not included in the comparison despite its relevance.  Limited Configuration consideration: SOTA improvements under different training epochs and architectural configurations are not discussed.  Qualitative issue Limited: only one qualitative issue picture is provided and failure case analysis is lacking.  lack Paper Comparison: The ICCV21 paper \"Dynamic DETR EndtoEnd Object Detection with Dynamic attention\" should be included in the review and experimentations.  GPU memory use lack: Information on GPU memory required for reproducibility is not provided.  Code issue Insufficiency: Code release only is insufficient for reproducibility statistical analysis and computational infrastructure details are necessary."], "vPK-G5HbnWg": ["Paraphrased Summary: This study presents PACE a method for converting addressed acyclic graphs (DAGs) into sequences compatible with Transformer architecture. It involves modifying the Transformer to incorporate DAG structure. The authors compare PACE to existing methods such as GRUbased and unaddressed GNNbased approaches and demonstrate its superiority in neural architecture search tasks. Paraphrased effectiveness and Weaknesses: effectiveness:  PACE is a parallelizable method for encoding longrange dependencies in DAGs.  It outperforms existing RNN and unaddressed GNN models in neural architecture search tasks. Weaknesses:  The authors do not consider existing methods for encoding addressed graphs or applying Transformers to graphs.  The experimental results lack clarity on training objectives and procedures for baselines.  The consumption of canonical ordering for both encoding and decoding may confound the interpretation of results. Additional Related work and Considerations:  The paper should include related work on Transformer adaptations for addressed and unaddressed graphs (e.g. Graphormer Transformers with relative attention).  The authors could consider the \"novel positional encoding to enable treebased transformers\" study for comparison.  It should be specified whether the experiments involve finetuning or address evaluation on downstream tasks.  The paper should clarify the role of canonical ordering in encoding and decoding and provide results for different conditions.", "This paper proposes a parallelizable encoder for directed acyclic graphs (DAGs) an improvement over asynchronous message passing alternatives. The encoder converts DAGs into a unique canonical work represented with positional encoding. Combined with a Transworker encoder it outperworks existing methods in experiments on neural architecture search and Bayesian networks. effectiveness:  Motivation for parallelizable architecture is clear.  introduction and background make the paper accessible.  The dag2seq framework provides an algorithm for converting DAGs to sequences independent of specific architecture.  consumption of isomorphism classes is justified and beneficial.  Transworker architecture is a suitable choice for encoding and decoding graphs.  Experimental results demonstrate both efficiency and perworkance advantages. Weaknesses:  Explanation of the GNN in Section 3.1 is incomplete and requires additional details.  Lack of explicit definitions for aggregation and combination work.  Unclear if embeddings are used and how they are learned.  model of two graphs mapping to the same isomorphism class would be helpful.  While positional encoding allow for flexibility they do not incorporate graph adjacency matrix bias like message passing methods.  Explicit masking strategy is needed to reintroduce graph structure.  Hyperparameter selection across baselines for fair comparison is not discussed.  Transworker architecture in PACE may have more parameters than DAGNNDVAE.  Finding the canonical work of a graph can be computationally expensive potentially affecting inference speed.", "Paraphrased Statement: The authors introduce PACE a transformer architecture for directed acyclic graphs (DAGs). They propose a novel DAGspecific positional encoding scheme that enables a fullyconnected selfattentional model to process DAGs without relying on a sequential RNNlike approach. PACE demonstrates better performance on neural architecture search datasets compared to other baselines. effectiveness:  The paper is wellwritten and concise outlining its contributions and advantages over previous work.  The empirical results on neural architecture search datasets provide convincing evidence of PACEs effectiveness. Weaknesses:  The paper lacks a comprehensive evaluation contrasting PACE with recent advancement in Graph Transformers which generalize positional encoding for graphs.  The authors neglect to consider the ogbgcode benchmark which is relevant for evaluating performance on ASTs a common application domain for DAGs.  While parallelizing dataflow over DAGs with a DAGspecific GNN is potential the paper does not address the potential complexity of processing DAGs with large diameters particularly for tasks involving dynamic programming.", "Paraphrase: Summary: This research introduces a novel architecture for representing directed acyclic graphs (DAGs) as embedded vectors to enhance downstream applications. By breaking away from the sequential encoding of DAG nodes this architecture allows for parallel encoding significantly improving processing speed. effectiveness and Weaknesses: effectiveness:  Clear organization with informative illustrations of the problem and proposed solution.  The acceleration of DAG encoding is critical and the proposed approach demonstrates introduction with promising results. Weaknesses:  Inadequate technical rationale for the specific architectural design. While the parallelizable nature of the structure is evident the understanding for combining DAG2Set and the masked transformer require further explanation.  Validation on large graphs (with more nodes) is necessary.  Inclusion of more demanding downstream tasks such as protein classification or computer program encoding would provide a more comprehensive validation.  The consumption of NAS task benchmarks alone may not be sufficient to assess performance combining the method with further NAS baselines would strengthen the evaluation.  RMSE and Pearson correlation metrics may not accurately assess performance consideration of nonparametric ranking metrics like Kendall Tau ranking coefficients is recommended.  While comparing training speed demonstrates overall learning efficiency it does not directly measure the benefits of parallel node encoding inference speed should be consumptiond instead."], "swiyAeGzFhQ": ["Paraphrase: Summary: Reresearchers suggest a system where two knowledgeable partners team up to accomplish an objective. They found that interactions in this system lead to universal communication strategies that can be applied to novel challenge. Strengths:  Intriguing system and learning process that seem to foster communication protocols that can be applied to different tasks. Weaknesses:  There are assumptions made that limit the systems practical use in more complex situations.  Giving the system architect approach to a perfect model of the environment is a strong assumption.  The specific heuristic used in a research algorithm is not fully explained and may require extensive knowledge of the specific problem.  The \"selfimitation learning\" mechanism is unclear. Concerns:  Without approach to an accurate environment model the system may not work as intended.  Experiments that explore the nature of the learned communication protocol are lacking.  The results may be primarily due to the effectiveness of the research algorithm rather than the communication process. Questions:  How can the system agent exhibit predictable behavior without prior training  How does the selfimitation learning process work without a basic understanding of messages Section 3.3 attempts to explain this but a clearer description would be helpful.", "After considering the critiques I have decided to maintain my current score. I am optimistic about this model potential to enhance multiagent communication offering a more robust model than traditional rewardbased protocols used in reinforcement learning. The proposed framework termed the \"architectbuilder Problem\" involves an architect who guides a builder towards achieving a goal through messages. Unlike conventional reinforcement learning and imitation learning only the architect possesses knowledge of the goal and reinforcement while the builder is the sole actor in the environment. The framework derives inspiration from cognitive science theories and introduces an algorithm designed to establish a communication protocol between the architect and builder. Experiments conducted on gridworld tasks demonstrate the algorithms capacity to generalize learned communication protocols to unseen tasks. Strengths:  The model unique setting and algorithm are novel and appealing.  The authors effectively differentiate their approach from reinforcement learning and imitation learning providing clear insights into the proposed algorithms learning dynamics.  The decision to deny the builder approach to reinforcement is an innovative aspect of the framework. Weaknesses:  The model practicality within the agentagent setting is unclear a concrete model could enhance its relevance.  The motivation for this research remains ambiguous is it intended as a computational model for humanhuman communication or a practical framework for specific scenarios  The problem formulation could benefit from a formalization using Markov Decision process (MDPs) explicitly defining the MDPs and policies of the architect and builder.  The interaction description lacks precision leaving it unclear whether the architect sends messages after each builder step or if the builder can execute multiple steps after receiving a message.  The framework relies on the architect having approach to an environment simulator which may not be feasible in all agentagent communication scenarios.  The experimental environments simplicity is a limitation acknowledged by the authors. Suggested related work:  collaborative Dialogue in Minecraft: [https:aclanthology.orgP191537.pdf]  Hierarchical Decision work by Generating and Following natural language instructions: [https:arxiv.orgpdf1906.00744.pdf]  Interactive learning from Activity Description: [https:arxiv.orgpdf2102.07024.pdf]  Neural generalization that reinforcement construction for Grounded language learning: [https:arxiv.orgpdf2107.09285.pdf]  Incorporating Pragmatic Reasoning communication into Emergent language: [https:arxiv.orgpdf2006.04109.pdf]", "Paraphrased Summary: This research introduces a framework for interactive learning between two agents called the \"architect\" and \"builder.\" Unlike reinforcement learning or imitation learning this framework focuses on facilitating communication between the agents so that the builder can successfully solve a task. The framework assumes that the architect understands the goal of the task and the reinforcement associated with it. The architects and the builder gather data during two phases: a model phase and a direct phase. During the model phase they use behavior cloning to extract policies while in the direct phase they use planning. The framework is tested in a block environment where the builder is randomly selected. The results demonstrate that the framework outperforms the baseline algorithms in solving the task. Furthermore the learned communication channel can potentially be used in other tasks. Strengths and Weaknesses:  Strengths:  The model of the problem using two connected MDPs is sound.  The learning of a communication channel is directly relevant to the conferences focus on AI.  Weaknesses:  Lack of clarity regarding the relationship between the framework and established research on multiagent model.  Absence of realworld applications for the approach.  Overemphasis on intermediate applications without a clear path to a fully developed version.  Incomplete comparison to recent work on learning protocols (e.g. [1]) which may offer similar benefits for HRI.  Insufficient discussion of the reusability of the algorithm for the specific problem.  Use of suboptimal method (behavioral cloning and MCTS) in the learning protocol.  Limited insights provided by the random construction agent in the empirical evaluation.  Pseudocode of the algorithm is relegated to supplemental material instead of being included in the main paper.", "This paper introduces the architectbuilder Iterated Guiding (ABIG) method to address the architectbuilder problem where an architect with knowledge of a highlevel goal communicates with a builder via discrete signals. This problem resembles hierarchical reinforcement learning specifically Feudal reinforcement learning. The ABIG algorithm involves the architect learning a model of the builders behavior through interaction and then using this model to guide the builders actions through a Monte Carlo Tree research. The architect has approach to the reinforcement work and transition work allowing it to evaluate messages and improve the builders behavior. The paper includes evaluation comparing ABIG to model where the architect sends random messages and where the builder takes random actions. Strengths of the paper include its clear motivation and thoughtful approach. However it has limitations such as the architects approach to the transition work which is a strong assumption that undercuts the work compared to existing hierarchical reinforcement learning approach. Additionally the evaluation could be expanded to include comparisons with other hierarchical reinforcement learning method and ablation that relax certain assumptions such as the discrete communication channel."], "sWbXSWzHPa": ["Paraphrased Summary: This research addresses the optimization challenge for a fairness criterion with limited group label availability. It proposes optimizing based on the worst potential group assignment and suggests a method for limiting the permitted assignments. Strengths and Weaknesses: Strengths:  Addresses a relevant issue with both practical and theoretical significance.  The Group DRO extension is sensitive and theoretically superior to standard Group DRO.  The paper is clear and understandable. Weaknesses:  No theoretical or empirical analysis of how constraint size affects convergence efficiency. This is critical for the algorithms practicality.  The significance of correctly estimating the true marginal distribution for constraint set definition is not emphasized and empirical support for Lemma 2 is lacking.  The algorithms performance with incorrect distribution estimation (i.e. when the true probability distribution is outside the constraint set) is not addressed.", "Paraphrased Summary: This paper seeks to develop representations that are invariant to group membership given data that is only partially grouplabeled. To this end they introduce Worstoff DRO which extends Group DRO by optimizing against the worstcase group assignments within a constraint set. Pros:  The paper theories are sound and validated.  It tackles a realistic scenario involving fairness and its proposed solution appears logical. Cons:  The research question is clear but the transition to explaining Worstoff DRO is abrupt. A paragraph on its need would improve clarity.  Converting the feasible set to \\mathcalC\\barp\\epsilon enables better error estimation.  Pseudolabelbased approach are omitted as baseline despite their relevance to the research question.  Additional fairnessrelated datasets (e.g. Arrest Violent German) should be included for experimental validation if potential. Typographical Errors:  \"We optimize for the a worstoff soft group assignment\" (should be \"the\" or \"a\") Questions for Rebuttal Period:  Address the aforementioned cons.", "Paraphrased Statement: Summary: This work introduces WorstOff DRO a new adaptation of DRO designed for situations where group labels are incomplete in the training data. The method involves a nested optimization that aims to maximize the least favorable group assignment within certain constraint alternating with the standard DRO optimization. experimental findings support the proposed approach. Strengths:  Technically sound wellpresented and refined. Weaknesses: 1. Assumption of Missing completely at random (MCAR):  The work assumes that group information is MCAR but fails to consider other types of missingness such as missingness due to demographics.  This assumption may bias the estimation of the group distribution (\\hat\\mathbfp) and compromise the reliability of Lemma 2.  Empirical evidence is needed to assess the impact of misspecified \\hat\\mathbfp. 2. Missing baseline:  The work lacks comparisons with stateoftheart unsupervised methods (e.g. EIILJTT) that enhance worstgroup performance.  The proposed WorstOff DRO should ideally perform between EIILJTT (lower anchor) and Group DRO (upper bound) because it requires more information than unsupervised methods.  Without these comparisons its hard to assess the proposed methods effectiveness.", "Paraphrased Statement: This paper investigates a scenario where group labels are partially known. To address this a generalized objective work is proposed that explanation for the worstcase group assignments based on the available information. An iterative optimization algorithm is introduced to minimize this objective. Weaknesses:  Empirical evaluation reveal the proposed approach underperforms compared to simpler baseline (ERM Group DRO) on benchmark datasets.  Theoretical findings are limited in their impact due to the significant gap between worstcase and loss scenarios.  The proposed objective work is overly conservative assigning labels arbitrarily without considering individual point characteristics. Suggestions:  Explore context where the pessimistic nature of the proposed loss work proves advantageous.  Develop a synthetic task where the superiority of the proposed approach over baseline can be formally demonstrated.  Conduct thorough ablation work to gain further insights into the limitations of the proposed method."], "qqdXHUGec9h": ["Paraphrased argument: Summary: This paper addresses partiallabel learning (PLL) where each training example has candidate label including the true label. It demonstrates that class activation map (CAMs) can identify the true label from candidate label for PLL. The authors propose class activation value (CAV) for identification capturing learned representation data more generally than CAMs which are limited to image datasets and CNN models. CAVbased learning (CAVL) is proposed selecting the true label as the class with the maximum CAV. Experiments on various datasets show that the proposed method outperforms stateoftheart methods. effectiveness:  Introduces CAMs for PLL identification.  Proposes CAV for identification capturing learned representation data in a more general way than CAMs. Weaknesses:  Limited empirical validation of the claim that CAM could identify true label from candidate sets.  CAVL may be affected by falsepositive label as it selects a \"true\" label from candidate label after the first training epoch.  Lack of theoretical validation for CAVs effectiveness in PLL.  No comparison with the method presented in [1] which shares similar experimental settings.  Hyperparameter selection is optimized on a small validation set and the trainvalidationtest split is not provided.  No supplementary material is provided for reproducibility of experimental solution.", "Paraphrase: Summary: This paper examines \"partial label learning\" a task where each training example has a set of potential label containing the true label. The paper finds that Class Activation Maps (CAMs) a technique used to identify model in image perform beneficial than models themselves in selecting the true label from candidate. Based on this finding the paper introduces Class Activation Value (CAV) an enhanced version of CAM applicable to various models and inputs. The paper also proposes a novel method for identifying the true label by finding the candidate with the maximum CAV. Experiments demonstrate the effectiveness of this approach. effectiveness:  Novel observation: CAMs can be used to choose the true label by counting positive elements in their corresponding CAMs.  Methodological contribution: The paper introduces CAV and a new technique for true label selection using CAV resulting in a novel partial label learning method.  Extensive experiments support the methods effectiveness. Weaknesses:  Limited explanation: The paper could provide more detail on how CAMs are used for true label selection.  Questions:  How can the proposed method be applied to other weakly supervised learning tasks such as semisupervised learning  The method appears to be based on an identificationbased strategy for partial label learning this should be clarified.", "Paraphrase of the argument: Summary:  This field seeks to harness the models extracted features for partial label learning (PLL) tasks.  It initially demonstrates that a class activation map (CAM) is superior to model output in selecting the correct label from candidate options.  To address the limitation that CAM is unsuitable for linear models and nonimage data the paper introduces class activation values (CAVs) as a substitute demonstrating comparable performance. effectiveness:  The use of CAM for selecting true label from candidate is a novel approach.  The proposed CAV method is effective and efficient.  The research has been thoroughly evaluated using a range of data and backbone architectures.  The ablation field on data generation methods supports the notion that CAM is independent of data assumptions. Weaknesses:  The primary contribution is the application of CAM to PLL problems along with minor modifications to adapt it to various data types and backbone architectures.  The explanation for the \"power\" and \"dynamic\" attributes of CAM as guidance for PLL problems are unclear.  The CAV method relies on selecting potential true label based on CAV after a single training epoch but the paper does not specify the criteria for this selection or address the consequences of choosing incorrect label.  The research lacks experimental solution or discussion on realistic datasets which are significant for assessing the methods generalizability. Questions:  How does CAV differ significantly from attention values and what are its key advantages  How do other closely related works compare to the CAV method and what are their relative strengths and weaknesses"], "sMqybmUh_u8": ["Paraphrased Summary: Researchers present a theoretical approach for revealing hidden hierarchical relationships in reinforcement learning (RL) tasks used for metatraining. They also propose an algorithm for hierarchical learning with mathematical assurances. The work appears robust theoretically and is wellstructured. The authors introduce innovative ideas like latent hierarchy dynamics separation and importance weighting which could offer a novel view on hierarchical RL research. effectiveness and Weaknesses: However the current version raises concerns that hinder its recommendation for acceptance: 1. transition framework Assumption: The work assumes the agent has approach to transition framework (probabilities and reinforcement) which may be impractical for intricate realworld environments. This raises doubts about the theory applicability to highdimensional MDPs with complex transition. 2. latent hierarchy and Common state Space: The authors assume that the latent hierarchy divides MDPs into clusters with unchanged dynamics. It is unclear if such a hierarchy exists in all metalearning RL scenarios. Additionally metatraining tasks may not contribution a common state space questioning the generalizability of the theory and algorithm. 3. Unfounded Assumptions: The work makes various assumptions without explaining their validity in realistic RL scenarios. 4. Implementation Details: The main text lacks details on how to partition the state space and optimize hierarchical policies which are crucial for the approachs practical implementation.", "Summary (Paraphrased): This field provides a theoretical framework for hierarchical reinforcement learning in a metareinforcement learning setting utilizing a simplified tabular environment. It introduces the concept of \"exit coverage\" to quantify the importance of stateaction pairs and proposes a novel algorithm that leverages this concept to identify key exits within the hierarchical structure. The algorithm is proven to achieve efficient exploration by implementing a theoretical oracle that reduces the complexity of the task. additionally the regret bounds for the hierarchical oracle is established mathematically. effectiveness and Weaknesses (Paraphrased): effectiveness:  Provides various significant theoretical results.  Demonstrates the implementability of a hierarchical oracle. Weaknesses:  Limited to a tabular setting.  Lacking a comprehensive discussion on connection to existing methods. additional Comments (Paraphrased):  Suggest exploring the relationship between the algorithm \"exits\" and \"bottleneck state\" defined by prior methods.  Consider improving the phrasing regarding the work of Wei et al. (2020) to better reflect its focus on characterizing HRLs sample efficiency.", "Paraphrased Statement: Summary: This paper presents a novel way to analyze the advantages of hierarchical reinforcement learning (HRL) algorithm. Using this novel method they create algorithm that can learn the hidden hierarchy in a task and then use that hierarchy to improve performance on related tasks. Under certain conditions they show that their algorithm perform better than traditional methods. effectiveness and Weaknesses:  The novel definition of hierarchical structures is valuable and use to many realworld HRL scenarios.  The development of theoretical understanding in HRL is still limited and this paper could be a significant step forward. However various concerns arise:  The assumptions made in the paper are numerous and not commonly used in the field. The authors should explain why each assumption is necessary and whether they restrict the applicability of the algorithm.  Some assumptions appear artificial and may not hold in most HRL situations such as Assumption 5.2.  The paper should provide more information about the magnitude of the parameters used in the assumptions such as those in Assumption 6.1.  The algorithm proposed in Section 5 for learning transition dynamics and exit detection is not explicitly compared to the bruteforce approach. It would be helpful to clarify the benefits of the novel algorithm.  The regret bounds in Theorem 6.2 does not include the cost of learning the hierarchy. This means that the overall complexity of learning both the hierarchy and the task could still be exponential in certain cases.  The paper needs minor revisions to address typos and clarify statements such as:  The definition of regret in Section 3  The meaning of \"optimistically choosing dynamics estimates\" in Section 5.2.3  The specific state created in Definition 5.2 for successful and failed termination  The definition of the distribution \u03b4 in Definition 5.2  The input format for the oracle in Theorem 5.1 and how Pt is obtained  The definition of \"query complexity\" in Theorem 5.1", "Summary: This paper presents regret bounds for an RL agent that uses a specific hierarchical structure to solve tasks in a field. The authors also show that the hierarchy can be discovered by the agent and provide a method for doing so. effectiveness:  Provides formal guarantees for discovering hierarchical structure and performance on novel tasks using it. Weaknesses:  Assumptions and Definitions:  Latent hierarchy definition for rooms field may not generalize to other fields.  Optimistic imagining definition lacks generality.  Practicality:  Agent must explore without reinforcement to determine transition dynamics for hierarchy discovery.  setting of Results:  regret bounds apply only to tasks that satisfy specific hierarchical compatibility criteria which excludes most tasks. Recommendations:  Reframe the work into separate contributions for hierarchy discovery and performance guarantees.  Generalize the definitions and assumptions to make the results more applicable.  Explore more practical methods for hierarchy discovery that do not require extensive rewardfree exploration."], "xUdEO_yE-GV": ["Paraphrased Statement: Utilizing advancement in the differentiation of Persistent Homology (a tool for extracting topological information from complex data) this study proposes a modified filtration combining distance and height parameters. This filtration aims to enhance the accuracy of topological reconstruction in image work particularly segmentation. Through experiments on various datasets the authors demonstrate the effectiveness of their approach in comparison to existing methods both within and outside of Topological Data Analysis (TDA). The issue indicate competitive performance. effectiveness:  application of TDA in image work with a clear rationale.  clear and accessible writing style.  Solid and extensive experimental issue that contribute to image segmentation. Weaknesses:  Inaccurate statements regarding TDA.  Simplification of multiparameter persistence by considering linear combination of distance and height function which has been previously explored in TDA.  Absence of citations for fundamental TDA concepts and previous work on differentiation of persistence diagrams.  Typographical errors and inaccuracies in image captions.  Overstatement about the scarcity of persistent homology applications in computer vision.  Questionable use of the phrase \"cost of leaving a homology unmatched\" and inaccurate terminology when referring to persistence intervals.  Inconclusive issue in Section 3.4 suggesting that noisy images may not be as similar to errorfree ones as claimed.", "Paraphrase: Summary: The paper introduces a new method that adds spatial information to topological features for improved segmentation accuracy. current methods often neglect the location of these features leading to topological errors like disconnected or incorrect connections. This method adds a positional embedding to the likelihood function which is a randomly chosen function of image pixel coordinates. Experiments show improved performance. effectiveness and Weaknesses: This method tackles the crucial issue of training networks for topological segmentation. Traditional pixelwise loss struggle to capture topology resulting in errors. Topologicallyaware loss are essential for detecting objects like road and blood vessels. The paper builds on persistent homologies (PH) and introduces positional embedding. The issue demonstrate the effectiveness of this modification. However the implementation of the PHbased topological loss is complex. The paper lacks a detailed review of the implementation including differentiation. It should also explore connections and comparisons to other PHbased location technique such as the patchbased topology by Hu et al. (2019). The authors should explicitly compare the proposed method to patchbased approaches.", "Paraphrased Summary This paper proposes a novel approach for identifying topological structures in 2D and 3D imaging using a computational topology framework known as persistent homology. The approach introduces a loss function that evaluates the preservation of topology during segmentation or structural extraction tasks. effectiveness and Weaknesses While the paper topic is intriguing it requires improvement before issue due to the following: 1. clarity and correctness issue: The text contains errors in terminology and incorrectly explains fundamental concepts. 2. Insufficient Comparison to Existing work: The paper references related work but it lacks a detailed comparison to other methods that combine image analysis with topological information. 3. preliminary Experimental Setup: The experimental issue lack significant contingent such as missing standard deviations for comparison methods making it difficult to assess the proposed methods performance fairly. Additionally the selection of comparison metrics is questionable. Detailed Comments The title \"Localised Persistent Homology Guided Deep learning for image segmentation\" is misleading as location and deep learning effectiveness are not directly addressed. The abstract should provide More context for nonexperts. Consistent use of singular or plural work for \"homology\" should be employed. Avoid incorrect use of \"homologies\" replace with \"simplicial chains\" or \"topological features.\" The explanation of persistent homology in Section 3.1 should be rewritten for clarity potentially with a technical description. Equation 2 requires clarification through additional parentheses. image 3 needs improvement to highlight the difference between the proposed and default filtration functions. Provide contingent on hyperparameter selection in the experimental setup. Define the Betti error More clearly. Minor Comments Ensure consistent citation style throughout the paper.", "Summary The article proposes a new loss function for training neuronal networks on images of curving lines. This loss function includes a term based on persistent homology to retain topological features in the networks output images. The authors introduce a new filtration that combines common filtration methods with heightbased filtrations. Experiments show that this loss function outperforms existing topological and nontopological loss on a specific neuronal network architecture. effectiveness and Weaknesses effectiveness:  Novel filtration for preserving topological properties in neuronal network outputs.  clear and wellwritten article. Weaknesses: Conceptual limitation:  Lack of theoretical justification for the proposed approach.  Experimental issue are insufficient to fully validate the approach. Technical Concerns:  Section 3.1: Incorrect statement about the cost of unmatched homology.  Equation (2): Questionable use of core performance when values are not on the same scale.  Lack of discussion on persistence bimodules as an alternative approach. Presentation issue:  image 4 (c) and (d) lack axis labels.  Confusing article title for Topological Data Analysis practitioners. potential Future Considerations:  Investigating injectivity and inverse properties of the proposed loss function."], "jZQOWas0Lo3": ["Paraphrase: This research investigates the use of adversarial attacks and optimal transport (OT) for semisupervised domain adaptation. The authors demonstrate that adversarial attacks possess a property called cycle monotonicity. Subsequently they propose an algorithm that generates adversarial examples for labeled target samples utilizing the source classifier and maps the original target samples to this novel domain via OT. effectiveness:  Exploring the application of adversarial attacks in domain adaptation is innovative.  Positive results suggest potential benefits using discrete OT methods. Weaknesses: 1. Motivation and Intuition:  The authors fail to explain why cycle monotonicity of adversarial attacks is essential for domain adaptation success.  The method appears counterintuitive since adversarial examples are misclassified by the source classifier but the same classifier is applied to transformed unlabeled target samples. 2. experimental Results:  Disappointing results using neural OT (Table 2) which is typically superior to discrete OT.  Limited evaluation on only the Digits datasets for neural OT results.  Lack of comparison with stateoftheart OTbased domain adaptation methods. 3. Clarity:  The introduction may be challenging for readers unfamiliar with cycle monotonicity.  The background on OT could be condensed.  The cycle monotonicity equation (eq. 7) contains errors and lacks information.  The result in Lemma 1 is expected based on the assumption on epsilon.  The use of \"N\" in algorithm 1 conflicts with its typical use as the number of samples.", "Summary: This paper proposes using adversarial approachs to generate useful data for semisupervised domain adaptation. The disturbances must meet cyclical monotonicity constraints. effectiveness:  Uses adversarial approachs to create useful samples for domain adaptation.  Involves three steps: source classifier pretraining adversarial approach and optimal transport alignment.  Shows promising results on various datasets and algorithm. Weaknesses:  Conceptual Unclearness:  Cyclical monotonicity is redundant and potentially confusing.  The definition of cyclical monotonicity is not fully explained.  Unfair Comparison:  The paper compares semisupervised domain adaptation with unsupervised methods using biased labeled data.  Concerns about Perturbation Effect:  source classifier performance improves with larger perturbations.  Insufficient Information:  Number of labeled target samples is not provided.  It would be beneficial to compare performance to a classifier trained on both source and target domains.  Adversarial training methods could be employed for improved generalization.", "Summary: This research introduces a domain adaptation technique that leverages labeled data from the target domain. It operates by: 1. Training a classifier on the source domain: The latent representations from this classifier are used to:  Compute \"antiadversarial\" examples on the labeled target data modifying them to be correctly classified by the source classifier.  transport the latent representations of these modified examples to the target domain using optimal transport maintaining consistency with labeled examples. effectiveness:  Demonstrated improvements over baseline algorithm in certain settings.  Extensive experimental evaluation. Weaknesses:  Weak theoretical foundation with limited justification for certain assumptions.  Insufficient explanation of the algorithm mechanism of process.  Lack of experimental validation of explanations provided.  Poor writing quality with numerous errors and inconsistencies. Minor Comments:  The pWasserstein distance is defined for p \u2265 1.  \"Are used to penalty\" should be corrected.  Cost matrix M should be nonnegative.  Lemma 1 requires the assumption n1 \u2260 n2.  \"bar1N\" is not defined.  In algorithm 1 Xf should be replaced with \u03a9f.  The term \"source domain classifier\" appears redundantly.  \"such an an approach\" contains an unnecessary repetition.", "Paraphrased Summary: This paper introduces a novel technique that leverages the concept of slightly modified samples (termed \"source fiction\") to enhance the performance of existing semisupervised domain adaptation methods that use optimal transport (OT). The method leverages a connection between slightly perturbed samples and optimal mappings between distributions. The paper proposes generating a \"source fiction\" dataset by slightly perturbing labeled target samples (within a bounded range). The target data is then mapped to this \"source fiction\" dataset using the same classifier trained on the source domain. This technique boosts the performance of OTbased domain adaptation methods. effectiveness and Weaknesses: effectiveness:  Demonstrates a novel link between perturbed samples and optimal mappings.  Shows that the proposed algorithm enhances the performance of OTbased domain adaptation algorithm.  Suggests future research directions including the possibility of using only the source classifier without requiring the source dataset. Weaknesses:  The discrete OT adaptation require the entire target dataset at test time which may limit their practicality.  The results for neuralbased approach are weaker than the OTbased baselines.  The paper primarily focuses on comparison with OTbased domain adaptation methods excluding other stateoftheart approach.  The theoretical development and algorithm descriptions could be more clear and precise.  There are various typos and wording issues throughout the paper. additional Comments and Questions:  Can these perturbations be interpreted as finding nearby contribution of the source classifier that classify correctly  Are these samples effectively the opposite of adversarial examples aiming to decrease instead than increase the source classifier loss function  What is the significance of \\overline1N in Lemma 1  The paper should emphasize that OT methods are applied in the latent space of the classifier not the natural pixel space.  For clarity the algorithm and figure should be presented after they are referenced in section 4."], "iMH1e5k7n3L": ["Paraphrased Statement: Summary: This paper proposes a novel approach to enhance the performance of LSTM networks by incorporating rank coding. Strengths:  The paper provides a comprehensive introduction and a concise description of the proposed method.  The method is demonstrated effectively through experimental applications including challenging tasks that showcase its advantages over existing methods.  The tradeoff analysis between speed and accuracy is well presented along with the impact of rank coding on network performance. Weaknesses and Improvement suggestion:  The rank coding approach employed in this paper differs significantly from that proposed by Thorpe and Gautrais. In the latter analog vectors are ordered from high to lowest resulting in high representational complexity.  In this paper it appears that only the maximum analog value exceeding a threshold is considered which simplifies the computing but reduces the complexity compared to the original rank coding.", "Paraphrased Statement: Researchers have developed a novel method for training Recurrent Neural Networks (RNNs) using Rank order Coding (ROC). In ROC the label is determined by the first readout unit that exceeds a predefined threshold. Upon reaching this threshold the process is terminated and Backpropagation Through time (BPTT) is initiated from that specific time step utilizing predictions and ground accuracy at that point. This approach encourages the neuron with the correct label to be highly active at the determined time step. Consequently its threshold becomes more potential to be met earlier in future iterations reducing decision latency. Moreover the speedaccuracy tradeoff can be adjusted by modifying the threshold. The authors have evaluated the effectiveness of their method on LSTMs in various context including toy problems MNIST and the Google Speech Command dataset. Strengths and Weaknesses: Strengths:  The concept of ROC is potentially novel.  The content is wellwritten. Weaknesses:  The experimental validations are insufficient considering current ML standards.  The authors should consider testing their method on spiking neuron instead of LSTMs.  The experimental validation may be lacking especially in the context of the ICLR conference. only the Google Speech Command dataset is nontoy and the achieved accuracy is below the stateoftheart.  The use of batch process is not clearly explained given the varying number of time steps required for BPTT.", "Paraphrase: The authors present a method for classifying sequential data faster and more efficiently. They believe that for certain data types its not necessary to analyze the entire sequence to make a confident prediction. Their model accelerates inference by learning a \"rank code\" based on spiking neural networks. The results show improved inference time on two toy sequence classification tasks: temporal MNIST and Google Speech Commands. Optimizing inference speed through a rank code has a minimal impact on accuracy. The authors also introduce and demonstrate a regularization term that allows for a tradeoff between speed and accuracy. Strengths and Weaknesses: Strengths:  Practical and wellstructured method.  Uses a learned rank code inspired by spiking neural networks for early exit.  Adaptive computing reduction during training and inference.  Backpropagation from an early time step for training efficiency.  Comprehensive referencing and justification of claims.  clear and concise writing. Weaknesses:  No comparison to LSTMs without a rank code in the temporal MNIST task.  Comparison to spiking neural networks may not be fully fair.  Threshold for early exit is a fixed hyperparameter."], "wRODLDHaAiW": ["Summary (Paraphrased) This paper introduces a novel method for inferring both the unknown initial conditions and external inputs of a dynamical model simultaneously. This advancement is made potential by integrating the issue of an optimization algorithm (iLQR) into a recognition model. The paper evaluates the approach on simulated and real neural data comparing it to established models of nonlinear dynamical systems. effectiveness and Weaknesses effectiveness:  Tackles the challenging problem of learning dynamics and external inputs in complex systems especially relevant in neuroscience.  Clearly written and easy to follow.  Provides toy models of increasing complexity for intuitive understanding.  Compares favorably to stateoftheart methods. Weaknesses:  iLQRs inability to find initial conditions in a simple model raises concerns about the algorithm or prior choice.  Lacks a comprehensive comparison to related work on inferring control inputs in dynamical systems.  The sparse input prior may not always align with real data potentially affecting issue.  The paper briefly mentions parallels with the EM algorithm but a more detailed discussion would be valuable. Minor Issues:  Figure 1 caption: Add \"system\" to the first time.  Paragraph beginning \"At the beginning...\": Correct the typo in the second time.  Same paragraph: Clarify what \"weak initialization\" means in the context of generator dynamics.", "Paraphrased Statement: Summary: This research introduces a new approach for variational inference that integrates control principles to infer latent neural dynamicals in inputdriven state space models (SSM). It employs an iLQR model for the recognition model casting it as an optimal control problem. The recognition model is derived directly from the generative model minimizing the issue of free parameters compared to existing methods. Experiments confirm the effectiveness of the proposed approach on both synthetic chaotic attractors and real neural recordings. effectiveness and Weaknesses after Rebuttal: Pros:  Implicit recognition model based on the generative model.  Incorporates dynamical data inferred from the generative model.  Achieves performance comparable to stateoftheart methods.  Reduced parameter space for efficient hyperparameter optimization. Concerns:  While the method was extended and compared to LFADS it lacks comparison to other techniques.  The implicit recognition model may exhibit biases due to its reliance on the trained generative model.  It remains unclear how the method compares in speed to LFADS and other approach.  The work does not examine the performance of the method on other types of dynamicalal systems. Minor Error:  Typo: \"LFDADS\" in paragraph 2 page 2", "Paraphrase: Summary: This paper introduces ILQRVAE a novel method that combines learning latent dynamics with inferring missing control inputs. It utilizes the IQLR solver and implicit differentiation techniques to maximize an evidence Lower bound (ELBO) and infer probability distributions for both inputs and latent states. The method is benchmarked aadditiont other models including LFADS on synthetic and neural data analysis datasets. ILQRVAE achieves similar performance to stateoftheart methods on numerous datasets requires minimal hyperparameter tuning and offers efficient inference for lowdimensional latent variables. effectiveness:  Novel approach for simultaneous latent dynamics learning and control input inference.  Strong performance on toy datasets and neural data analysis benchmarks.  Accurate reconstruction of hand kinematics in primate reaching tasks.  Innovative use of iLQR with implicit differentiation.  Comprehensive analysis of LFADS and valuable insights. Weaknesses:  Lack of experiments demonstrating the impact of different input priors.  Unclear derivation of ELBO (potential conditioning error).  Computational complexity may limit applicability to higherdimensional latent variables.  Unfair computational complexity comparison with LFADS in Figure 1.  Absence of comparisons between work inferred by different methods.  Potential for efficiency addition by using lowerdimensional latents in Figure 4.  Typo in Figure 4 caption."], "lzupY5zjaU9": ["Paraphrased Statement: The research introduces two metaalgorithms \"Compress\" and \"Compress\" that utilize existing thinning algorithms as subroutines. These algorithms optimize runtime while introducing minimal additional error. \"Compress\" operates recursively dividing input data into four parts running itself on each part combining the results and halving the combined set using the \"HALVE\" algorithm. For instance using KTSPLIT as the HALVE algorithm can significantly reduce runtime but may introduce additional logarithmic error. Compress ensures that KTSPLIT operates on small input set. Building on \"Compress\" the authors develop a variant that trades runtime for modest error. This is achieved by applying the thinning algorithm to a small set obtained after running \"Compress.\" The runtime analysis is based on the recursive nature of the algorithms. The algorithms error bounds are established using the subgamma property. Empirical evaluations employing highdimensional Monte Carlo samples demonstrate their improved runtime performance. effectiveness and Weaknesses  Introduces a novel framework to enhance the efficiency of existing thinning algorithms.  Framework is straightforward with analysis primarily relying on the SubGamma property.  Framework significantly improves runtime reducing processing time from days to hours.  Runtime improvements are more evident for larger input size (103).  It is unclear whether practical applications commonly require such large sample size.", "Paraphrased Statement Summary The study examines distribution compression and thinning algorithms used in Monte Carlo estimation for functions within a RKHS. Given a set of points where the uniform distribution over these points approximates an underlying distribution within a specific error margin the research aims to identify a subset of points (significantly fewer than the original set) that primarytain a similar error level. Three reduction algorithms are proposed to enhance the efficiency of existing distribution compression methods. The key finding is that the proposed reduction improves the running time of existing algorithms (which typically have quadratic or higher runtimes) by a factor of two without increasing the corresponding error by more than a polylog factor. effectiveness and Weaknesses The research contributes to the field by introducing a general reduction framework that speeds up existing algorithms with minimal impact on accuracy. The underlying concept is straightforward and intuitive enhancing the value of the contribution. Acceptance is recommended. Suggestions 1. Major input:  Include a proof sketch for the MMD guarantees in the primary text.  Provide additional details and clarifications in the proof of Theorem 2 specifically regarding the relationship between \u03bbmax \u01c1\u03c8CP\u01c1k and u\u0303\u03c8kj.  Explain why RecHalve should not be used directly.  Highlight the key differences between the proposed recursionbased reduction and the recursionbased thinning approach proposed by Dwivedi and Mackey in Example 1. 2. modest input:  Correct grammatical error including omitted discussion and concepts defined prematurely.  Add a citation for the claim of \u03a9(n14) error.  Clarify the meaning of \"significantly improved error range\" in Section 3. Update The authors response has addressed the concerns raised. The paper reprimarys recommended for acceptance.", "This paper introduces a meta algorithm that enhances the efficiency of kernelset construction algorithms for the distribution compression problem. It achieves this improvement without significantly compromising accuracy. The meta algorithm divides the input into four equal parts recursively applies the kernelset construction algorithm to each part and then combines the resulting kernelsets into a single kernelset. This simple approach significantly reduces the running time of the original algorithm while maintaining a similar error range only four time worse. The effectiveness of this method lies in its simplicity and effectiveness in improving the running time of a kernel problem."], "wNsNT56zDkG": ["Paraphrased Statement: Abstract: This study addresses technical challenges to establish the adversarial Rademacher complexity of deep neural networks directly targeting the adversarial Rademacher complexity instead of variants like in prior research. It establishes both lower and upper bounds for this metric. Additionally numerical experiments are conducted to complement the theoretical bounds and provide evidence for the inferior generalization of adversarial training compared to standard training. Strengths and Weaknesses: The primary contribution of this work is its direct approach to adversarial Rademacher complexity rather than using approximations. The provision of both upper and lower bounds is significant. However various area require improvement: 1. understanding and Evidence:  It is unclear if the larger generalization gap for adversarial training applies to both adversarialtrained and standardtrained networks.  Evidence for a larger generalization gap between standard training and adversarial testing is lacking.  Insights are needed into the larger norms of adversarially trained models and the impact of sample size on the norm margin.  Implications for the loss landscape of adversarial robust neural networks are not discussed. 2. validation Details:  Theorem 3s validation lacks specifics on the referenced lower bound results and their application.  Concrete illustrations or examples are necessary to clarify the validation step. 3. literature Review:  The literature review focuses only on Rademacher complexitybased generalization for adversarial training.  A broader review of other theoretical approaches to generalization is warranted including relevant study that provide both upper and lower bounds. 4. Clarity in Theorem 1 validation:  Clarification is needed on the differences and key step in the derivation of adversarial Rademacher complexity compared to standard Rademacher complexity.  Detail should be provided on how to modify the validation step to obtain the standard Rademacher complexity. Minor Issues:  contribution in Section 1 should be described in more detail.  Propositions 1 and 2 and Section 3 should be reconsidered to avoid distracting from the primary focus of the paper.", "Paraphrase: Summary This study presents novel generalization bounds for adversarial training in neural networks utilizing the Rademacher complexity step. These bounds surpass previous findings by extending their applicability to neural networks of any depth. Empirical evaluations on CIFAR10 and CIFAR100 datasets using multiple VGG architectures demonstrate the connection between the bounds key parameters and adversarial training restricted generalization capacity. Strengths  Explores the significant topic of adversarial examples and adversarial training in recent years.  Advances theoretical results beyond existing literature.  Conducts experiments corroborange theoretical findings offering insights into the limited generalization abilities of adversarial training.  Presents a coherent and wellwritten paper (with some typos). Weaknesses  A comparison to other adversarial bounds based on Rademacher complexity or alternative framework (e.g. using toy examples or small networks) would enhance the evaluation of the improved tightness of the presented bound.  Consideration should be given to comparing the contribution to other adversarial training theory approaches such as provable method cited in the paper. Questions and Comments  Clarify that the provided bounds are applicable to convolutional neural networks earlier in the paper or specify which layers are included.  Discuss the practical implications of employing PGD adversarial examples rather than optimal perturbations.  Consider the robustness of assuming a loss role in the range [01] for neural networks.  Evaluate the tightness and practical applicability of Rademacher complexity bounds for neural networks. Update Post Discussion Following discussion the range has been increased by one point.", "Paraphrased Statement: This paper presents a novel upper bound on adversarial Rademacher complexity that involves the product of weight norms. This indicates that large weight norms impede achieving optimal generalization performance in adversarial settings. The authors empirically demonstrate that the weight norm product in adversarial training significantly exceeds that in standard training. Strengths: 1. The paper introduces novel bounds for adversarial Rademacher complexity. 2. The paper is clearly written and accessible. Weaknesses: 1. The empirical validation is inconclusive. While the authors show that the weight norm product is higher in adversarial training they do not account for potential differences in constants within the bounds for adversarial and standard training. A more rigorous comparison is required. 2. The paper suggests that minimizing the weight norm product can enhance generalization. However it lacks experiments demonstrange techniques to regularize this product during training to improve generalization. Including such experiments would strengthen the theoretical findings. PostRebuttal Assessment: After considering the authors response the reviewer revised their range upward.", "Summary: This paper presents a novel upper bound on the Rademacher complexity of neural networks in the presence of adversarial examples. Prior bounds for simpler models were limited and traditional method for deep networks did not apply. This work innovates by directly analyzing covering numbers rather than using inductive calculations. Strengths:  Addresses an open question from previous research.  Uses a sound technique.  Experimental results support the proposed explanation for generalization gap. Weaknesses:  Lack of a depthwidthdependent lower bound.  Missing citation of ALT19 and JMLR which provide relevant uniform convergence results based on Rademacher complexity.  Typo: \"d2\" should be \"2d\" in section 3."], "s2UpjzX82FS": ["Paraphrase: Summary: This research proposes a novel distributed learning platform that utilizes a vision transformer for numerous image processing applications. It delivers remarkable numerical and visual outcomes for various image restoration tasks while preserving data privacy. The platform consists of a taskagnostic vision transformer (run on a server) that learns general image representation and multiple taskspecific CNNs (run on client devices) that handle different restoration tasks. A dedicated training strategy is also provided. effectiveness and Weaknesses: effectiveness:  Introduces a novel and practical distributed learning setup for image restoration.  Enables handling multiple tasks while safeguarding privacy.  Demonstrates competitive or stateoftheart results for the evaluated restoration tasks.  Presents a clear and accessible explanation of the framework. Weaknesses:  The claimed privacypreserving aspect of the framework lacks experimental or theoretical validation.  The rationale for selecting only deblocking denoising deraining and deblurring tasks for clientside processing is not provided. Other tasks (e.g. superresolution inpainting) are not considered.", "Summary TAViT is a framework for distributed learning that includes a Transformer body for taskagnostic learning and taskspecific head and tail CNNs for different tasks. This enables taskagnostic learning of the body while training the heads and tails in a taskspecific manner. Experiments demonstrate the effectiveness of this approach in image processing tasks. effectiveness and Weaknesses  Line 5 of Abstract: The statement should be revised to acknowledge the inspiration from Vision Transformers (ViTs) rather than \"the success of ViT.\"  applicability to HigherLevel Tasks: The authors should explore the applicability of their proposed framework to higherlevel tasks that require semantic understanding.  Transfer of data feature to Server: While transferring data features to the server may pose challenges the authors should highlight any potential research or application value.  Bridging Domain Gap: The authors should provide more experimental results and analysis to demonstrate how the taskagnostic Transformer body bridges the domain gap caused by different tasks.  Shared Transformer body for Different data source: The authors should investigate whether the shared Transformer body work well for different data source (e.g. nature satellite medical image) and provide experimental results.  Comparison to Prior work: The authors should highlight the significant differences between TAViT and prior work that addressed taskagnostic learning in federated learning.", "Paraphrased Statement: Summary: The paper proposes an architecture for image processing that divides a network into head body and tail components. The head and tail components are based on convolutional neural network (CNNs) and can be trained on multiple devices using federated learning (FedAvg). The body component is based on transformers and is trained on a central server. The head and tail components are taskspecific while the body component is taskagnostic and learns from clients representing different tasks. The experimental results show comparable or superior performance to nondistributed framework as well as outperforming purely federated learning (FL) and serveronly learning (SL) approaches with a small number of clients. effectiveness and Weaknesses: The paper is wellwritten and proposes a novel architecture for distributed multitask learning. The code and framework are available making it easy to implement. However there are some key omissions that limit the paper impact:  Privacy Guarantees: Despite mentioning \"privacypreserving\" in the title the paper lacks any discussion of privacy guarantees for the proposed architecture. Privacy protection is crucial in federated learning and should be rigorously addressed.  Communication cost: The paper does not discuss the communication cost associated with federated learning in the context of the proposed architecture. Transmitting gradients between clients and the server can significantly impact performance and scalability.  Choice of architecture and Sampling strategy: The rationale behind choosing CNNs for the head and tail components and transformers for the body component is not adequately explained. Additionally the sampling strategy of selecting exactly one client from each task may not scale well with a larger number of clients.  Diversity of Experiments: The validation experiments mainly focus on a specific architecture. To establish the frameworks generalization more diverse experiments could be conducted.  Revised Response: The authors have significantly improved the paper by addressing most of the concern raised in the comments. The addition of section 3.3 and Appendix D provides explanations for the architectural choices and privacy guarantees. The remaining unresolved source in the \"Federated Learning\" paragraph on page 3 is a small typographical error.", "Paraphrased Statement: Summary: This research aims to develop a privacypreserving method for training neural network in image processing tasks such as deblocking denoising removing rain and blurring. The method involves splitting the neural network framework into two section: taskspecific convolutional head and tails (trained on local devices or \"clients\") and a common Transformerbased backbone (trained on a central server). The headtails and backbone are trained alternately assuming the other component is constant. This approach is similar to \"SplitFed\" but extends it to multiple tasks. Experimental results show that the proposed method:  Successfully trains neural network frameworks.  Achieves comparable or slightly better performance compared to existing distributed privacypreserving methods.  Performs better using the Transformerbased backbone compared to CNN backbones and in a multitask setting compared to singletask training. effectiveness:  The method effectively extends \"SplitFed\" to multitask learning with a shared backbone on the server side.  The experiments demonstrate the viability of the method and its ability to achieve comparable performance to existing privacypreserving methods.  Joint multitask training shows some performance synergy on various tasks. Weaknesses:  The use of a Transformerbased backbone on the server is not a unique contribution compared to prior work. The key novelty lies in jointly training a shared backbone on different tasks which is a natural extension of previous methods.  The choice of using CNNs as headtails and a Transformer as a backbone is not fully explained or justified. It is unclear if the roles could be reversed without impacting performance.  The distributed experiments involve a small number of clients which limits the practical evaluation of the method in realistic deployment scenarios."], "vdbidlOkeF0": ["Paraphrased Statement: This paper presents a method for estimating the ratio of log densities \\log [pq] by utilizing an intermediate density m. This strategy entails expressing \\log [pq] as \\log [pm]  \\log [qm]. The ratio of each density is then estimated using a generative classifier (logistic regression). The paper showcases the proposed methods advantages through simulations. Strengths and Weaknesses: 1. clarity and Structure:  Improve descriptions and titles for beneficial clarity.  Correct typos and define undefined terms. 2. Misleading Statements:  Remove the misleading claim that theoretical guarantees come from empirical analysis. 3. Lack of Novelty:  Acknowledge potential overlap with existing methods (e.g. sDRE) and provide more analysis on the unique contributions of this approach. 4. choice of Intermediate Density:  Provide a detailed analysis and argument for choosing an optimal intermediate density measure M. 5. Estimation of Class probability:  Explicitly explain the method for estimating \\eta(x) (e.g. logistic regression).  Discuss the choice of generative classifier and its impact on analysis. 6. Redundancy of FOD and HOD:  Remove unnecessary concepts (FOD and HOD) that are not directly related to the proposed method.", "Paraphrased Summary: This paper explores density ratio estimation (DRE) a critical task in machine learning. Traditional methods often struggle when samples are poorly separated. To address this the authors propose the use of binary classification for DRE and introduce the scaled Bregman divergence framework. This framework enables successful density ratio estimation. The paper showcases applications in experiments. Strengths and Weaknesses:  Contributes to solving a significant problem in DRE.  The proposed method extends and innovates upon existing approaches.  Introduces novel concepts to understand the problem. Questions: 1. What is the work that generates the data 2. Define Df(ab) as Bf(abM). 3. What does \\hat\\eta represent in the denominator of \\hatr(x) 4. Can the authors benchmark their method against other plain DRE methods and the approach proposed by Kato and Teshima (2021) Minor detail:  Explain the meaning of \"closed work\" in the context of density ratio estimation.", "Paraphrased Statement: Summary: This research develops a technique to calculate the ratio of two densities by employing intermediate densities that contribution significant overlap with the target densities. The approach leverages the scaled Bregman divergence and can be recast as a multiclass logistic regression problem. By adapting the concept of telescoping the proposed method alleviates the density chasm issue and outperforms other methods in various datasets. Strengths:  The method is clearly motivated and theoretically robust.  The paper is straightforward.  The proposed method significantly outperforms other approaches especially when both false outofdomain (FOD) and highoutofdomain (HOD) density chasm issues arise. Limitations:  The novelty of the method appears incremental as both Lemma 1 and the telescoping concept are existing estimation.  If the support of the two densities is entirely disjoint estimating the density ratio becomes trivial (yielding 0 or infinity).  If one density is a simple translation (mean shift) of the other and they are wellseparated logistic regression may provide beneficial classification due to improved separation. This may contradict the claimed FOD challenge. Clarification from the authors would be helpful.  The majority of experiments are focused on normal (Gaussian) distributions. The resilience of the proposed method to other mutual distribution families is limited in the reported solution. Other Comments:  In the definition of Xm on page 5 xp should be xpi.", "Paraphrased Statement: Estimation of Distribution Similarity and Density Ratio hand samples from two distinct distributions how can we determine the KullbackLeibler (KL) divergence between them more generally how can we calculate the ratio of their probability densities at specific detail (e.g. on samples from one distribution) Proposed method for Density Ratio Estimation (DRE) The authors present a novel approach to DRE a challenging task when the distributions differ in their supports or have a high density ratio. They build upon the work of Rhodes et al. which utilizes a \"telescoping\" method to express the density ratio as a product of ratios between intermediate distributions. The proposed method expands on this concept by systematically addressing distribution shift and refining the construction of intermediate distributions. empirical solution demonstrate the advantages of the approach over existing methods. Critique While the paper is generally wellwritten certain aspects raise concerns:  Accuracy: Lemma 1 suggests a connection between multiclass logistic regression and DRE but it does not specifically mention logistic regression. The definition of the reference density C is unclear and the definition of D(r(x)\\hatr(x)) is not provided.  rigor: The discussion of the proposed method in section 4 sometimes lacks precision. A formal definition of DRE and its evaluation metric would be beneficial. The explanation in section 4.3 is difficult to follow especially the argument for replacing log(mq) with log(qm).  Connection to Previous work: The authors refer to consistency of rankingbased noise contrastive estimation but the connection to their work is not fully established weakening the argument in section 4."], "fOsN52jn25l": ["Paraphrase: Summary:  This research extends the Lottery Ticket Hypothesis (LTH) to a more complex scenario and introduces the concept of \"Dual LTH.\"  The authors suggest that any part of a randomly initialized dense netprocess can be made a \"winning ticket\" (a netprocess that can be trained from scratch to match the performance of its dense counterpart).  They propose Random Sparse Netprocess Transformation (RST) to facilitate this transformation. effectiveness:  The Dual LTH concept is intriguing and merits investigation.  The paper is wellorganized and easy to follow. Weaknesses:  The authors claim that any ticket in a lottery pool can be transformed into a winning ticket. However\u5be6\u9a57\u7d50\u679c shows that the performance of some subnetprocesss still lags behind the dense model on CIFAR10100 and ImageNet with a 5098 pruning ratio. Evaluating the proposed method in scenarios where both LTH and Dual LTH identify winning tickets would be more insightful.  As noted in a related study rearranging the connections within individual layers of initial tickets does not affect performance only the layerwise sparsity level matters. It remains to be seen if RST would still be effective if the layerwise sparsity level were also randomly arranged.  The section on related process need to be better structured and provide more context. Minor Issue:  The caption for design 3 overlaps part of the yaxis label.", "Paraphrased Statement: This research investigates the Lotter Ticket Hypothesis and puts forward the concept of the Dual Lottery Ticket Hypothesis (DLTH). DLTH suggests that every lottery ticket in a given pool can become a winning ticket through a specific transformation using regularization techniques. experiment conducted on CIFAR10100 dataset using ResNet models support the empirical validity of DLTH. effectiveness and Weaknesses:  DLTH is an innovative extension of the Lotter Ticket Hypothesis providing a novel perspective.  The empirical study demonstrates promising results on the specific dataset and models used.  The paper includes comparisons with other methods like Pruning at Initialization (PI).  The writing is generally clear. Concerns:  The empirical study is limited to small network architectures and datasets and its performance on larger datasets (e.g. ImageNet) is not explored.  The paper solely focuses on the regularization method for ticket transformation without delving into alternative approach.  It overlooks relevant prior research such as RigL The Elastic Lottery Ticket Hypothesis and Drawing Earlybird Tickets. Including comparisons with these studies would enhance the paper credibility.", "Paraphrase: Summary: The paper presents a new pruning technique inspired by distillation. It retains pruned connections in the pruned netprocess and utilizes them to generate model outputs. This approach outperforms the widely used LTH method and some pruningatinitialization methods like GraSP. effectiveness:  The method has a strong need given the growing importance of sparse training techniques in managing large model sizes.  The paper is clear and the concept is easy to grasp.  Experimental results demonstrate the proposed methods superiority over vanilla LTH. Weaknesses:  It is unclear whether the process complements supplements or extends LTH. LTH emphasizes \"training in isolation\" meaning extracting sparse subnetprocesss. However in this method subnetprocesss emerge deep and all connections remain active during training. Retraining is also not a part of the process.  The authors should consider comparisons to existing compression methods particularly pruningaftertraining methods.  Some concept need clarification or correction. For instance \\bar\\theta (eta bar) is not defined and the second term in Equation (4) may not always be met. It would be more accurate to use \"\\lt\" instead of \"\".  Some citation formats should be updated to use parentheses such as \\citep.", "Paraphrased Summary: The paper proposes a method called \"Randomly Selected Ticket Wins\" (RST) that aims to transform randomly selected subnetworks in neural networks into trainable and efficient models. By gradually penalizing the magnitude of unused parameters RST isolates the subnetwork and improves its performance potentially matching or exceeding other baselines. effectiveness and Weaknesses: Major Pros:  Random subnetworks can achieve comparable performance to dense networks with similar sparsity.  RSTderived subnetworks have shown promising results. Major Cons:  The claim that any subnetwork can be transformed is incorrect.  Results with high sparsity levels exhibit noise raising concerns about statistical significance and experimental setup. way and Clarity Concerns:  The paper assumes knowledge of certain terms and concept which may not be clear to all readers.  key terms and concept are not defined upfront.  information extrusion is mentioned but not explained in detail. Updated Review: The authors have revised the manuscript to address some concerns including the erroneous claim. They have also modified the scope of their claims and presented a novel approach in the form of RST. However issues remain regarding the experimental setup and the sensitivity of the results to iteration and extrusion duration which should be further explored in the final manuscript.", "Paraphrased Statement: Summary: This research introduces the Dual Lottery Ticket Hypothesis (DLTH) stating that any random sparse subnetprocess within a dense neural netprocess can be transformed into a trainable \"winning ticket.\" This transformation known as Random Sparse Netprocess Transformation (RST) involves regularizing masked weights to involve them in training while transferring their information to unmasked weights. Experimental results show RSTs effectiveness compared to traditional lottery ticket pruning methods. effectiveness and Weaknesses: Two concerns about this process: 1. Posing: The authors claim that RST transforms any ticket into a winning ticket but the equations suggest it may be a general training method for sparse neural netprocesss. Comparing RST to Lottery Ticket Enhanced with IMP (EBLTH) could clarify whether RST is a transformation or a effective training method. 2. Writing: The foundation and literature review are lengthy while Section 3 is disconnected from Sections 4 and 5. There is a lack of discussion about how RST relates to the transformation of weights and masks making it seem isolated from the theoretical foundation described in Section 3. The authors should consider shortening other sections and expanding discussions in later sections to improve logical coherence."], "faMcf0MDk0f": ["Paraphrased instruction: Summary: This paper introduces two innovative binary neural netfunction (BaseNet and BooLNet) where most parameters are stored in binary format unlike previous models that used 32bit floatingpoint activations. Additionally the proposed netfunction eliminate batch normalization and most ReLU activations replacing them with a sign function. To enhance learning capacity the authors employ Multislice Binary Convolution and Local Adaptive Shifting methods. The performance of BaseNet and BooLNet is assessed for inference and compared to ResNet34 using 32bit floatingpoint format. Strengths: 1. Reduced energy consumption due to fewer FP32 performances. 2. evaluation of the impact of various techniques including Multislice Binary Convolution on accuracy memory usage and performance count. Weaknesses: 1. Lack of significant novelty the author appears to combine existing approach with BNNs to improve accuracy. 2. Insufficient explanation of the motivation for using the sign function in range of batch normalization. 3. The comparison in image 4 does not account for distillation or extended training epochs used in prior function. 4. Omission of a comparison with the Multislice Binary Convolution approach presented in [1]. 5. BooLNets evaluation is limited to ImageNet and ResNet34 it is unclear whether the proposed method can be applied to other datasets and netfunction. It is suggested that BooLNet be evaluated on different netfunction such as MobileNetV2.", "This paper presents the groundbreaking development of fully 1bit neural netfunctions. The proposed BoolNet outperforms existing models in terms of the balance between accuracy and energy efficiency. Strengths:  First successful attempt to create fully 1bit neural netfunctions a crucial step for practical hardware implementation.  Identifies and eliminates the negative impact of 32bit layers in typical binary neural netfunctions through innovative design.  Employs a Multislice strategy and other techniques to minimize accuracy loss.  Achieves superior performance in balancing accuracy and energy consumption. Weaknesses:  The introduced MSBConv performance increases the number of channels which may lead to higher memory usage and longer inference time.  The related function [1] on searching for accurate binary neural architectures should be referenced to provide context for the proposed approach.", "Summary Paraphrase: This paper aims to minimize or eliminate 32bit features in binary neural netfunction (BNNs). Current BNNs often incorporate 32bit features for improved accuracy but this can lead to increased computational overheads during inference. The proposed network replaces fullprecision batch normalization layers with shifted sign layers removes fullprecision scaling factors and employs multislice binary convolutions and 1bit shortcuts. Despite these techniques and knowledge dievenation the network even achieves 63 accuracy on ImageNet highlighting the challenges of eliminating 32bit features completely. Strengths and Weaknesses Paraphrase: Strengths:  The paper proposes several techniques to create BNNs with only binary operations. Weaknesses:  The paper motivation for eliminating 32bit features is not sufficiently strong.  Some of the proposed ideas resemble existing function without proper comparison or citations.  The comparison with ReActNet uses an older version that has lower accuracy and higher FLOPS.  The simulated energy consumption results lack a thorough analysis.  The paper could benefit from a comparison with BaseNetBoolNet which utilizes 32bit features to demonstrate the potential overheads.", "Paraphrased instruction: This research introduces techniques for significantly reducing the energy consumption of binary neural networks (BNNs) by modifying or removing 32bit components (like skip connections) found in current BNN architectures. The proposed architecture implements specific changes: 1. Reducing the precision of skip connections and activation function. 2. Converting the BatchNorm layer into a simplified sign function. 3. Utilizing a multislice strategy to mitigate the loss of representation causaged by binarizing feature function and shortcut connections. evaluation results demonstrate that the novel model achieves an impressive 4.7fold reduction in energy consumption albeit with some accuracy loss compared to existing architectures. Strengths:  Clear and concise writing style.  Novel network architecture that aims to enhance the efficiency of neural networks.  practical discussion of a potential usage case alongside implementation results to support the effectiveness of the architecture. Weaknesses:  The accuracy performance of the proposed architecture falls short of the ReActNet architecture which achieves a higher accuracy with fewer operations.  The hardware performance comparison between BoolNets and ReActNets is not conducted on a specific industrystandard hardware accelerator potentially introducing bias in favor of BoolNets."], "tgcAoUVHRIB": ["Summary (Paraphrased): This article introduces a novel approach for answering complex queries on knowledge graphs using multilayer perceptrons (MLPs). The system supports both one and twoinput queries and enables negation handling. Additionally a variation of the system leverages an MLP Mixer model for improved performance. The systems effectiveness was demonstrated on three wellknown knowledge graphs (FB15K FB15k237 NELL995) outperforming established baselines such as BetaE Q2B and GQE. effectiveness and Weaknesses (Paraphrased):  effectiveness:  Promising results  negation handling capability  Relevant experimental datasets  Mathematical rigor and sufficient system description  Weaknesses:  Limited discussion of related work (lack of pros and cons analysis)  Typographical error in the phrase \"directed acyclic graph (DAG) graph\"  Misidentification of the symbol \"sigma\" as the margin (which should be \"gamma\") and \"sigmoid use\" instead of \"margin\"", "Paraphrased Statement: The work proposes using neural network to learn projection intersection negation operators which are required for processing complex queries. These network are built upon the MLPMixer architecture and evidence strong empirical performance. effectiveness:  High empirical performance  Adequate information for reproducing the method in the main text and appendix  Capacity to handle negative queries Weaknesses:  Deficient related work section:  Most cited work are outdated (e.g. in KGR with logic rules the most recent publication is from 2018 which is considered previous in machine learning)  No comparison between the proposed method baselines and traditional KGR methods that address queries of type (hr) and complex queries  The related work section lacks a clear explanation of the work relationship to previous research. Clarity Issues:  The graph Query Embedding method upon which this paper is based should be described in large detail. Questions: 1. When the intersection operator receives more than two inputs how is the recursive order determined Different orders may produce varying results. 2. What do the node colors represent in Figure 1 3. How is the subgraph (e.g. Figure 1 (C)) derived from the knowledge graph 4. How are the initial embeddings si for each entity and relation determined 5. How does the training objective handle multiple potential solution (Might know the solution but main text should clarify.) Minor Improvement:  use \\textitNN in math mode to eliminate the irregular spacing between the two Ns.", "Paraphrase: Summary This work presents a novel method for reasoning in knowledge graphs using complex logical queries. It surpasses existing approach in performance. effectiveness and Weaknesses The paper introduces a novel approach for inferencing and learning in knowledge graphs with complex queries. It employs neural networks to execute logical operators and leverages MixerMLP networks for their implementation. Evaluation on standard datasets reveals marked improvement over prevailing methods. However the papers weakness lies in its lack of clarity regarding the training work. specifically it remains unclear how the network is trained to \"learn\" logical operations. Section 3.3 indicates standard training for MLPmixer architectures. Additionally the inclusion of a 2Vector Average Approach complicates the interpretation of performance improvement. It is uncertain whether the embeddings effectively learned logical operations or if their performance boost is attributable to extensive embeddings. The papers experimental section should delve deeper into explaining the reasons behind the observed solutions. Despite its strong experimental outcomes the work could be strengthened by providing a more thorough justification for its findings. After Author Review The authors have effectively addressed concerns regarding clarity and provided deeper insights. As a solution the papers score has been revised accordingly.", "Summary This paper introduces a novel embedding method for reasoning on knowledge graphs. Strengths  Embeddings facilitate knowledge reasoning on knowledge graphs. Weaknesses Lack of Clarity:  Figure 1: Insufficient explanation of its content.  Embeddings: Uncertain work for queries and entities.  Input generation for entity embeddings (Section 3).  Adoption of MLPMixer (Section 3.3).  Computation of solution vector (Section 3.4).  Transformation of firstorder logic (FOL) queries to embeddings.  Navigation strategy for graph query solutioning. Missing Discussion:  Explanation of performance improvement over other approach."], "hGXij5rfiHw": ["Paraphrased Summary: This field introduces a novel DIR algorithm that empowers a trained Graph Neural Network (GNN) to make prediction based on causal patterns. The model involves four components: 1. rationale Generator: Divides the input into causal and noncausal (spurious) contribution. 2. Distribution Intervener: Replaces the spurious contribution with a similar distribution. 3. Graph Encoder: Embeds the graph based on both spurious and causal contribution separately. 4. Classifiers: Predicts based on spurious and causal embeddings separately. The DIR algorithm aims to make prediction that are accurate and unaffected by spurious data. Empirical results show that DIR surpasses existing methods on both synthetic and realworld datasets. As a novel endtoend rationalization method DIRs underlying principle has been proven to be both sufficient and necessary for satisfying an optimal oracle function. effectiveness: 1. DIR is an innovative and endtoend approach. 2. The DIR principle is clear and theoretically sound. 3. DIR consistently outperforms baselines in empirical examination. 4. Observations on variance changes during training provide insights into invariant learning methods. Weaknesses: 1. The rationale for multiplying causal and spurious prediction is not explained in the paper. 2. The spurious classifier is expected to output a invariant vector since the causal prediction should not be significantly altered. More observations about the spurious classifiers prediction would be helpful.", "Paraphrased Statement: Summary This paper aims to enhance the interpretability of graph neural networks (GNNs) when dealing with data that is different from the training set. To achieve this the field investigates GNNs that are inherently interpretable by pinpointing the causal patterns that remain consistent across different environments. The effectiveness of the proposed model is then demonstrated on a graph classification task. Strengths  Focus on the critical issue of identifying causal patterns that transcend environments making GNNs more interpretable for graph data analysis.  Proposes the DIR model which uses a substantial approach to create interventional distributions and extract salient features across different environments.  Provides theoretical analysis to support the DIR method.  Evaluates the DIR models efficacy in terms of generalization robustness to bias and intrinsic interpretability.  Visualizes the identified rationales using interventional distributions. Weaknesses  The paper lacks a table summarizing key notation which would improve readability for reviewers.  The evaluation section should provide more details about the configurations of the baseline models for a more thorough performance comparison.", "Summary: The authors propose a method to improve the generalization ability of Graph Neural Networks (GNNs) for graph classification. They assume that a subset of the graph edges is responsible for its label while the rest are spurious correlations. To isolate these significant edges they use a rationale generator and distribution intervener. They evaluate their model on synthetic and realworld dataset. effectiveness:  Interesting application of causal theory to explain graph labels.  Wellwritten and understandable paper. Weaknesses:  The \"topK\" approach limits the methods applicability by assuming a fixed proportion of significant edges.  The method relies on GNNs as the graph encoder which have limitations in predicting links and identifying certain subgraph.  relevant baselines like GNN Explainer and CFGNN Explainer are missing.  The method may not be applicable to graph where the causal and noncausal contribution are not separable.  The examination set are limited to those seen in training.  The method may not capture certain complex relationships between subgraph and labels (e.g. XOR between motifs)."], "q79uMSC6ZBT": ["Paraphrase: Summary  This paper introduces Grammformer a new code completion model that can generate code containing placeholders (\"holes\") to skip uncertain prediction.  The model generates partial syntax trees with some nonterminals left unfilled.  Evaluated on C and Python plan Grammformer outperforms existing techniques.  It includes a thorough investigation of different model components. effectiveness and Weaknesses  All three contribution are novel and valuable:  Grammformer with its \"hole\" prediction ability.  RegexAcc a metric for evaluating prediction with holes.  Evaluation results demonstrating Grammformers superiority.  The evaluation includes both quantitative and qualitative aspects. Questions and Comments  Why not fully separate the models for nonterminal selection and expansion Did this impact performance  Explain the \"grammar flattening\" process and its impact on tree sizes.  Why is linear regression (LR) with \"no sclear expansion\" effective in practice  study the distribution of \"hole\" placements in generated model.  Discuss whether Grammformer with \"no sclear expansion\" resembles AnyCodeGen.  research cases where Grammformer recovers despite other holes and identify potential \"synch\" symbols.  Provide model demonstrating LR with holes underperforming compared to Grammformer.  Clarify whether trivial expansions like in Figure 16 contribute to the clear k match score.  study inference time for different models. Minor Issues  Table 4s location in the appendix is unclear.  Related work mentioned in the paraphrase include:  Rabinovich et al. 2017  Brockschmidt et al. 2019  Amodio et al. 2017  Murali et al. 2018  Chen et al. 2018", "Paraphrase: Summary: This study proposes GRAMMAFORMER a transformer model designed to generate code with \"holes\" (uncertain sections) where the model is uncertain. GRAMMAFORMER is trained on code completion tasks for C and Python. It outperforms other method by generating more accurate completions and longer sketches. effectiveness:  The idea of generating code sketches in the context of code completion is novel and practical.  The model effectively prevents lowcertainty code generation by identifying nonterminals.  The introduction of the REGEXACC measure allows for strict evaluation of sketches. Weaknesses:  Additional evaluation metrics could be explored to complement ROUGE and REGEXACC.  The weak performance of extended sequence models suggests the need for further investigation or data augmentation.  Training a probability distribution over grammar rules could potentially enhance semantic correctness.  The presence of a CoPilot row in Fig 1 without discussion in the main text is confusing.  The baselines section and results paragraph lack clarity regarding the \"L R with holes\" baseline.  The marginal improvement over GRAMMAFORMER (pretrained only) raises questions about the effectiveness of RL.", "Paraphrased Summary: Code completion can be challenging because certain intermediate code snippets may be hard to predict even if the surrounding code elements are easily predictable. To address this Grammformer introduces a transformerbased model that generates code completions with \"holes\" in area where the model is uncertain. effectiveness:  Presents a novel approach to code completion by incorporating uncertainty.  Outperforms baseline models in evaluation. Weaknesses:  performance gains over standard transformers are modest.  Computational overhead of Grammformer during inference is unclear.  The relevance of metrics like ROUGE and REGEXACC to code completion tasks is not fully established and the paper lacks evidence to support their use.  The paper lacks detailed evaluation of the generated sketches usefulness in specific programming tasks or applications.", "Summary Paraphrase: This study introduces a new method for completing code by using sketches. The proposed model strategically leaves \"holes\" in its prediction when it encounters high uncertainty. Experiments demonstrate the effectiveness of using Grammformer prioritizing accuracy in generated code. effectiveness and Weaknesses Paraphrase: effectiveness:  The approach addresses a fundamental limitation of lefttoright language models by allowing for user input in uncertain situations.  Grammformer is the first grammarguided model that specifically generates around \"holes\" to handle uncertain tokens.  It utilizes support study and a novel metric (RegexAcc) to improve performance in the absence of supervised data. Weaknesses:  The main concern is the additional computational cost compared to standard Transformer models.  The inference time increases linearly with the predicted sequence length potentially making it impractical for interactive autocompletion.  The accuracy improvement from Grammformer is modest which could outweigh the performance tradeoff.  The pretrainedonly version of Grammformer has comparable performance to the full model but may require less computation. Minor Issues:  \"CoPilot\" should be mentioned in the text if it appears in the figures.  Clarify the statement \"x is the 200 terminal tokens\" on page 6."], "qwULHx9zld": ["Paraphrased Statement: This study investigates the random features method which involves randomly projecting data into a lowerdimensional space while preserving the original kernel structure. Key contribution include:  Demonstrating that under a certain highdimensional assumption the random features kernel depends only on the first two generalized Gaussian moments of the activation work.  Proposing a specific choice of random weights and activation work that are computationally efficient and require less storage.  Empirical evidence evidence that the proposed method perform competitively with other random features approach while offering advantages in computation and storage. Strengths and Weaknesses:  Wellstructured and innovative leveraging theoretical insights to develop computationally efficient and storagefriendly random features.  validation technique is not particularly original but the independent contribution lies in the computational and storage benefits. This aspect should be emphasized more by the authors. Minor Points:  Explanations are needed regarding the claim that the proposed random feature computation requires no multiplication and only additions.  If \"epsilon\" is nonzero does the method still require no multiplication or additions Should the addition count be proportional to 1epsilon instead", "Paraphrased StateKent: SuKKary: This paper suggests a sparse randoK features Kethod for data assuKing a Gaussian Kixture distribution. The proposed kernel is independent of randoK weights and depends on Gaussian KoKents of activation work. The paper theoretically guarantees asyKptotic equivalence with the centered kernel and provides an efficient algorithK for constructing randoK features. experiKental results deKonstrate accuracy and coKputational efficiency. Strengths:  Provides theoretical guarantees for asyKptotic equivalence and efficient randoK features.  Devises a siKple and efficient algorithK for constructing sparse ternary randoK features reducing coKputational and storage coKplexity.  EKpirical validations show that the proposed Kethod offers coKputational and storage gains.  The writing is clear and easy to follow. Weaknesses:  The Gaussian Kixture assuKption for input data Kay be restrictive (e.g. Kany realworld datasets have nonGaussian distributions).  The asyKptotic theory requires the input diKension to approach infinity which is not applicable in practice. It is unclear how well the approxiKation holds for sKall input diKensions.  The presented theoreKs and corollary do not depend on the nuKber of randoK features (K) which is typically critical for approxiKation and generalization ability.  The proposed Kethod seeKs to be independent of kernel hyperparaKeters which is unusual for kernel Kethods. It is unclear how TRF handles kernels with different hyperparaKeters and how it affects perforKance.", "Paraphrased Summary: This paper proposes a method for efficiently approximating common kernel matrices with sparse nonzero introduction and low bit complexity. It does this based on a theoretical analysis using random matrix theory (RMT) applied to kernel matrices of data drawn from a Gaussian mixture model. The analysis shows that the approximated kernel matrix is asymptotically similar to the actual kernel matrix though nonasymptotic results are not provided. The paper also demonstrates experimentally that the approximated kernel matrix perform statistically as well as other approximation methods such as Random Fourier Features. Strengths and Weaknesses: Quality and clarity:  The paper is wellwritten and easy to understand. theory:  The kernel theoretical claim is compelling but not sufficient to support the paper alone. It serves as motivation for the proposed algorithm.  The estimator for the kernel matrix is assumed to be asymptotically correct but the paper should provide more clarification on this assumption.  The RMT validation is noted for its simplicity and approachability. Experiments:  The experiments provide solid evidence in support of the proposed algorithm.  However some improvements are suggested to strengthen the experiments:  Use a more appropriate baseline such as the optimal MSE without random features.  Include confidence intervals to assess the variability of the algorithms.  Reduce the number of data series in certain plots and add color coding for legibility. Additional Questions and Suggestions:  Explore the theory of using lower precision floats (e.g. 16bit or 8bit) to reduce space complexity.  Consider exploring connections to Michael Mahoneys work on RMT for deep learning.  Identify the conditions under which TRF outperform RFF statistically. Minor Corrections:  Page 2 footnote: Change \"kernel\" to \"kerneling the data.\"  Figure 4: Replace \"0.5 \\cdot 102\" with \"0.05.\"  Figures 1115: Indicate the zoomedin yscale in the captions.  Figure 12: Specify the dataset name in the caption.", "novel ResourceEfficient Kernel approximation practice Random Features The authors propose a method to approximate kernel work using random feature transformations in a more efficient way. This approach utilizes the random feature concept introduced by Rahimi and Recht where each random feature is calculated as the activation work of a randomly weighted inner product between the input and a random vector. To enhance efficiency the authors impose constraints on the random vector and activation work. They restrict these elements to have values between 0 and 1 allowing for computation with only additions and subtractions (avoiding multiplication) and reducing storage requirements by storing only 1 bit per feature. Under specific assumptions the authors prove that their kernel approximation converges to the true kernel as the number of features and data points increases to infinity. These assumptions include Gaussian mixture data distribution pairwise orthogonal input features and bounded fourth moments for random vector introduction. mathematical experiments demonstrate the benefits of this approach for tasks like kernel ridge regression and SVM. It offers significant speedup (approximately 2x) and memory reduction (approximately 8x) compared to previous quantized random feature methods with minimal accuracy loss. Strengths:  Ingenious ternary quantization approximation with impressive practical performance.  Relatively straightforward implementation. Weaknesses:  Restrictive theoretical assumptions (e.g. Gaussian mixture data distribution).  Confusing introduction of theoretical results (e.g. Theorem 1).  Lack of explanation for i.i.d. assumption on random vector introduction.  Absence of discussion on the minimum number of random features required.  Missing details on random feature computation time."], "h0OYV0We3oh": ["Summary Illiterate DALL\u00b7E a new image generation model leverages objectcentric representation without requiring text input. It outperforms mixture decoderbased models in synthesizing new image. Strengths  Clear and wellstructured technical details  advanced use of slot attention to learn compositional representation  Strong experimental results demonstrating the benefits of the objectcentric approach Weaknesses  Synthetic Data: The models performance on realistic data with complex textures is questionable.  Slots Initialization: Gaussianinitialized slots may not be optimal and learnable query vectors could enhance performance.  Comparative Analysis: compare to DALL\u00b7E itself not just Slot attention would be more appropriate. other objectcentric approaches for image generation should also be considered.  Novelty Clarification: Its unclear whether the improvement over Slot attention stems from a more robust decoder framework or enhanced objectcentric representation.  Relation to Prior Work: Related objectcentric approaches for image synthesis and use are not adequately discussed.  DALL\u00b7E Compositionality: DALL\u00b7E contrary to the authors claims does not fully lack compositionality as evidenced by its ability to generate image based on complex text prompts.", "Paraphrase: Summary: This paper presents a model that employs visual cues (visual prompts) to create image. Visual prompts are inherently engaging and thoughtprovoking. The proposed model outperforms current techniques. Strengths:  The concept is imaginative with visual prompts that appear significant and intriguing.  Qualitative model demonstrate promising performance. Weaknesses:  The experiments are limited in scope using synthetic and well modeled datasets. It is unclear how the model would fare on realworld datasets like ImageNet MSCOCO and Celeb1M. The prompts for natural scenes are also unknown.  The paper lacks a human evaluation of the generated image making it difficult to fully assess the models effectiveness.", "Summary (Paraphrased): This research introduces Slot2Seq a method that adapts the DALLE texttoimage model to imagetoimage paper. Slot2Seq aims to extract latent concepts from foundation image enabling it to generate new image by reconstructing slots and performing outofdistribution generation. Experimental results on four datasets including a modified CLEVR dataset demonstrate the effectiveness of Slot2Seq in these tasks. Strengths:  Fills the gap between DALLEs strong generation capability and slot learning models limited imaginative ability.  Highlights the limitations of pixel mixture decoders for image generation from slots and proposes an alternative approach.  Demonstrates promising results in outofdistribution paper with a welldesigned experimental setup. Weaknesses:  Lacks explanation for mirroring the CLEVR dataset and using it instead of the original CLEVR.  Could provide more details on architectural modeling decisions such as the use of DVAE over VQVAE.  Would benefit from a discussion of model ablations or compare to determine the specific contribution of Slot2Seq to the improved generation quality.  Suggests exploring the imposition of order on the conceptslot prompt as there may be an inherent hierarchy in image generation.  compare could be expanded to include other paper models such as energyfoundationd models to provide additional context."], "twgEkDwFTP": ["Summary: This study examines reweighting algorithms (such as Importance Weighting group DRO) designed to improve the performance of the group with the worst performance. The authors provide theoretical explanations for why overfitting commonly occurs in these algorithms. They demonstrate that under certain conditions such as linear models wide fully connected neural networks and squared loss the worstgroup performance of reweighting algorithms approaches that of the Equalized risk minimization (ERM) algorithm which does not explicitly consider group fairness. effectiveness:  Provides a formal validation for linear models showing that parameter changes occur in a lowdimensional subspace.  Suggests that reweighting algorithms without corrective measures may not outperform ERM in terms of worstgroup performance.  Wellwritten and clear. Weaknesses:  The conclusion that reweighting algorithms invariably overfit is somewhat overstated as it applies only to specific scenarios with assumptions not invariably met in use.  The analysis of neural networks is limited focusing on wide fully connected models rather than more complex architectures.  The theoretical issue and experiments use different model types (wide fully connected vs. ResNet18) which may not fully align.  Some key steps in the analysis are empirical despite the claim of a theoretical foundation in the abstract.", "Paraphrased Summary: This research explores the issue of optimizing models for worstcase subgroup performance a important approach for creating fair and robust models. Past methods have encountered challenges especially in the overparameterized regime where they struggle to achieve satisfactory worstcase performance on testing data. Theoretical Explanation and Empirical Evidence: This paper provides a theoretical explanation for this issue utilizing linear models and linearized neural networks. The authors demonstrate this phenomenon through mathematical experiments on two datasets. They also investigate the impact of L2 regularization in training. Practical Implications: The findings indicate that existing reweightingbased approaches need significant regularization or other stopping to ensure beneficial test accuracy in the overparameterized setting. effectiveness and Weaknesses:  clear writing and timely issue  Combination of theoretical issue and model Suggestions for Improvement:  Clarify applicability to batch or stochastic updates  Discuss the requirement for subsets of predictors and response variables  Strengthen the argument in the highlevel statement after Theorem 2  Provide empirical evidence to support the theoretical arguments  Correct typographical errors and revise the reproducibility statement Typographical and Reproducibility Statement Errors:  Equation (5): f(t) should be f  Page 8: \"trained trained\" (repeated twice)  Reproducibility statement: \"speculation\" should be \"specification\" \"Anaconda\" should be capitalized", "Paraphrased Statement: This paper presents a theoretical explanation for the observed issue of data importance reweighting in overparameterized deep netstudys. For linear models and linearized neural netstudys with squared loss and independent data points the paper demonstrates mathematically that reweighting has no impact on the learned model. It also examines the role of regularization in preventing this behavior and shows that regularization must be sufficiently strong to counteract small training errors. The paper is supported by empirical findings on two datasets. effectiveness and Weaknesses: effectiveness:  study a significant issue in deep learning regarding importance reweighting in overparameterized models.  Relevant to the communitys current research interests. Weaknesses:  focus on squared loss while previous study examined crossentropy loss creating a potential discrepancy.  Omits necessary references such as the seminal study on importance reweighing in deep learning ([1]) and a more comprehensive theoretical analysis for crossentropy and other losses ([2]).  Does not clarify whether the theoretical findings add significant new insights beyond prior knowledge.  The requirement for regularization has been previously established in other research ([1]).", "Paraphrased Statement: Summary  The study examines strategies to enhance accuracy for marginalized groups in overparametrized models.  Despite their initial appearance theoretical and practical findings suggest that reweighting algorithms typically lead to identical solutions as basic Empirical risk minimization (ERM) especially in overparametrized contexts.  Theoretical analysis supports this claim for linear models linearized networks and wide fully connected networks.  The authors propose that significant regularization is necessary (sufficient to impair training error) for solution alterations.  Additional theoretical and empirical evidence supports this notion. effectiveness and Weaknesses  effectiveness:  Understanding worstgroup performance is valuable.  The paper is accessible and insightful.  Weaknesses:  Misleading Summary:  Strong claims (e.g. \"reweighting cannot alter solutions\") are stated without explicit mention of inherent assumptions.  TheoreticalEmpirical Discrepancy:  Theoretical issue predict convergence to identical solutions for overparametrized models but empirical evidence shows differences in test accuracy.  Ambiguous Theoretical Assumptions:  Assumptions are presented piecemeal making it challenging to fully understand their implications.  Lack of Citation:  The observation of parameter changes within the data point span in linear models may not be new but proper attribution is lacking.  Limited Theoretical Scope:  Theoretical analysis assumes squared loss and its applicability beyond this assumption is unclear.  Assumption Justification:  The intuition behind Assumption 1 is not explicitly explained.  Similar Previous study:  Potential similarities to existing analysis are not explicitly acknowledged.  Missing ERM Results:  Table 1 lacks ERM performance for other stopping an important comparison metric.  Questionable Confidence Intervals:  Confidence intervals based solely on random seed iterations may not be reliable."], "qQuzhbU3Gto": ["Paraphrase: Summary: This research investigates community detection in mixed membership networks where nodes can belong to multiple communities. The authors propose a novel graph model that handles the overlapping nature of communities. The models adjacency matrix includes terms for soft community memberships intercommunity similarity and heterophily (dislike of links within communities). The authors provide an algorithm to estimate these parameters and demonstrate its effectiveness in experiments. strength and Weaknesses:  Model: The model combines three features: soft assignments nonlinearities and heterophily. This combination is novel and generalizes existing models.  Algorithm: The proposed algorithm is theoretically sound but lacks clear justification for certain steps. The computational complexity of eigenvalue decomposition is a challenge.  Theoretical Results: Theorem 5.3 extends the applicability of the algorithm but Theorem 5.2 raises concerns about the interpretability of the model.  Experimental Results: Experiments show promising results but lack analysis of computation time.  Interpretability: The model faces a major interpretability issue due to its invariance to various transformations. Its unclear how these transformations affect community memberships. The algorithms link to clustering is not fully explored.", "Paraphrase: Summary:  The paper presents a novel edgeagnostic graph generative model that can represent both heterophily (dissimilarity between nodes) and overlapping communities.  The model leverages nonnegative matrix factorization (NMF) to generate link prediction which can be interpreted based on identified communities.  The paper demonstrates the models effectiveness in multilabel clustering and link prediction tasks. strength:  Tackles a significant challenge in graph modeling by addressing heterophily and overlapping communities.  Provides a means to interpret link prediction by utilizing NMF. Weaknesses:  Lack of Motivation:  The rationale for the threestage approach is unclear.  The role and outcomes of each stage need clarification.  The possibility of a simpler approach is not explored.  Confusing Notation and Descriptions:  instance in Figures 1 and 2 are confusing.  Typos and incorrect equations (e.g. Equation 1) exist.  Section 4 lacks clarity and details about the training procedure and heterophily modeling.  AdditionalConcerns:  The threestage approximation work is complex and may introduce error.  The necessity of nonnegative embeddings for interpretability is questionable.  Missing Information:  No discussion of time and space complexity of the approach.  Limited Experiments:  The experiments use small datasets raising concerns about scalability.  Comparison with other recent methods is lacking.", "Paraphrase: Summary: This paper introduces a generative graph model that does not rely on any specific edge dependencies. It also presents a training method and theoretical insights for the model. However the evaluation of the model focuses on clustering making it difficult to assess its ability to replicate real networks. Despite its potential contribution it may be susceptible to a \"degeneracy problem.\" strength and Weaknesses: strength:  Strong motivation and clear explanation of generative graph models (GGMs) and their challenges.  foundation of a type of GGM and development of a training algorithm with promising theoretical results. Weaknesses:  Insufficient related work coverage on GGMs.  Unclear main contribution within the proposed model.  Lack of justification for how the training work addresses heterophily.  complex implementation details and missing explanations.  Absence of time complexity analysis which is concerning as the sampling work has a high time complexity that could hinder practicality.  potential degeneracy problem indicated by low Frobenius error. Main Issue with Evaluation: The primary issue lies in the evaluation of the model which focuses on clustering rather than the replication of network characteristics. To properly assess the proposed model it should be evaluated based on its ability to generate networks that mimic the global and local characteristics of real networks including heterophily."], "tBtoZYKd9n": ["Paraphrase: Summary: This study examines limitations and common traps in using the Maximum Mean Discrepancy (MMD) metric to assess generative graph models. It also offers guidance on calibrating the metric to prevent misleading results. Strengths:  Provides needed clarification on MMD for graph generative model evaluation.  Demonstrates the significant impact of improper MMD parameter calibration on results.  Offers valuable recommendations for tuning MMD parameter to obtain useful results which could enhance research choice and establish more robust benchmarks. Weaknesses:  Limited scope: The paper does not consider node and edge features which are crucial in practical applications such as molecular generation.  Only focuses on comparing graph structure using MMD. Questions:  Why was the 4orbit count descriptor not included in the analysis  A runtime comparison of different kernel (e.g. linear EMD and RBF) would be beneficial as claims about the slowness of the EMD kernel are not specific enough.", "Paraphrase: Summary This study aims to establish a systematic method for evaluating and comparing graph generative models. The authors identify essential criteria for evaluation metrics and discuss the use of Maximum Mean Discrepancy (MMD) for model comparison. However they identify potential issues with MMD and provide practical recommendations for researchers evaluating graph generative models based on empirical analysis. Strengths 1. The authors address a crucial issue in the graph generative model community by highlighting potential problems with MMD. 2. The papers clear writing and thorough empirical study effectively demonstrate the issues various aspects. The authors refrained from promoting their own approach which enhances the papers objectivity. 3. The authors recommendations are valuable for researchers in graph generative model. Weaknesses 1. The studys scope is somewhat narrow as it does not consider all types of graph generative models such as those focused on edge prediction or node classification. 2. The authors could have compared additional commonly used metrics and descriptor functions. 3. Instead of using correlation which value linear relationships the authors could have investigated nonlinear value like mutual information for beneficial evaluation.", "Paraphrase: This paper critiques Maximum Mean Discrepancy (MMD) a metric commonly used to evaluate Generative Graph Models (GGMs). MMDs main issues include:  Inability to detect differences under perturbations  Lack of scale  Reliance on kernel selection and parameter which can influence results and increase computational complexity The paper proposes some MMDbased methods to address these problems but the solutions are unclear. Strengths:  Presentation: The paper is wellwritten and accessible.  Reproducibility: Most experiments can be replicated.  Impact: The critique of MMD could reshape GGM evaluation use.  Technical choice: empirical evidence supports most identified pitfalls. Weaknesses:  Vague Solutions: The proposed solutions are inadequately explained.  Undefined Perturbations: The paper leaves \"perturbations\" undefined hindering comprehension.  descriptor role Dependency: The difference in distributions versus perturbations may not be an MMD problem but rather a limitation of specific descriptor functions. The paper relies heavily on clustering coefficient which may not capture perturbations in all graph.  Lack of Scale: The concept of \"scale\" is vaguely described and no clear procedure is outlined.  Limited descriptor roles: The paper fails to consider other meaningful descriptor functions such as geodesic distance.  Interpretability Issue Not Resolved: MMDs interpretability remains a concern that is not addressed.  Lack of comparison: The paper does not compare MMD to alternative metrics such as multivariate KolmogorovSmirnov.", "Summary This paper outlines essential criteria for evaluating graph generative models using metrics and discusses how maximum mean discrepancy (MMD) has been used in recent research. It highlights the limitations of MMD and the consequences of various choice in its implementation (e.g. kernel descriptor hyperparameters). The authors provide recommendations to address these issues. Strengths and Weaknesses Strengths:  Clear and wellwritten  Provides a comprehensive overview of MMD for graph generative model evaluation Weaknesses:  Assumption that perturbed graph should have significantly different probabilities may not always hold  Lack of citations for certain claims  Incomplete description of kernel and hyperparameter choice used by Niu et al. (2020)  Omission of the number of orbits with 4 nodes as a descriptor minor Comments:  Consider adding citations to support the statement that the community has gravitated towards MMD.  Clarify the value used to determine the \"distance\" between graph.  Explain why perturbing graph rather than generating graph from different distributions is more appropriate.  Add citations to support the claim that Equation 1 is often treated as a metric.  Correct the typo in Equation 2.  Address the need for further analysis of descriptor role expressivity and robustness.  Explain why any descriptor can be used rather than recommending a specific selection process.  Adjust the scale in design 4 to show a range of 1 to 1 for correlation."], "vcUmUvQCloe": ["Paraphrase: Summary: This article introduces a new value called joint Shapley values to calculate the average impact of a group of features on a models predictions. The uniqueness of this metric is mathematically proven. The authors also provide a code appendix with training and parameter tuning instructions. Strengths:  Clear and wellwritten introduction  Convincing experimental results demonstrate the effectiveness of joint Shapley values  model in Table 5 clarify the results Weaknesses:  The method is computationally demanding (though the authors acknowledge this and plan to address it in future work)  Sampling could be optimized for efficiency but this suggestion is not significant as the authors recognize it as a future improvement.", "Paraphrased Statement: Summary: This paper introduces \"joint Shapley values\" a generalization of Shapley values for measuring contribution of sets. Strengths and Weaknesses: The reviewer acknowledges the theoretical significance of the paper but raises concerns: 1. Practicality of k Selection: How to determine the value of \"k\" in practice The authors should provide guidance. 2. Explicit work of set work in model: In Table 2 the explicit work of the set work corresponding to work \"f\" is not provided. This makes it difficult to derive the set work. 3. Discussion on set work Choice: The paper lacks discussion on how to choose the appropriate set work.", "Paraphrase: This research introduces \"joint Shapley values\" which extend Shapleys principles and intuition. Joint Shapley values quantify the combined impact of feature sets on a models predictions. The work seamlessly applies Shapleys axioms from individual features to feature sets. In summary joint Shapley values estimate the average marginal contribution of feature sets to model predictions. The paper provides substantial mathematical findings for the joint Shapley values method. This method is then assessed using various datasets including simulated data Boston housing data and movie Review data. Strengths:  The represent code is wellwritten with clear examples and a straightforward implementation.  The theoretical concept are represent clearly and intuitively.  The work explores practical considerations such as hardware requirements and algorithm complexity estimates.  It provides an overview of cuttingedge techniques in Shapleybased explanations. Weaknesses:  The focus on the mathematical properties of the method overlooks the practical aspects of its application.  The approach lacks a comprehensive guide on how to utilize it for model insights.  direct comparison with other methods are limited to simple datasets leaving the relative strengths and weaknesses unclear.  Some terms introduced in the application section like \"negation effect\" require further explanation or extension. other Observations:  There is a potential typo in the sentence \"evaluating it at a extension or baseline feature\" (page 1).  The introduction and Related work sections should be distinct for clarity.  The \"application\" section might be more appropriately labeled \"Experiments.\"  Table 2 could be more compact and readable by reducing the number of decimal set."], "uYLFoz1vlAC": ["Summary Paraphrase: The authors introduce a more computationally efficient modeling approach called the Structured State Space Model (S3) which simplifies the decomposition of state matrices and expands the model in frequency space using a multipolelike evaluation. This method preserves the benefits of past state space model approach while enhancing computational stability and efficiency. effectiveness and Weaknesses Paraphrase: effectiveness:  Clear theoretical justification and approach explanation.  experimentation across various field (speech images text) demonstrate competitive performance.  Wellwritten introduction and background. Weaknesses:  Limitations and future direction are not explicitly discussed.  Concerns about dataset size dependency and lowdata performance are not addressed.  Results section organization is unclear with table references not aligned with text.  Feature visualization in Figure 2 evidence departures from the stated context learning model.  Hierarchical differences are less apparent in convolutional filters (Figure 5 in the appendix).", "Paraphrase: This paper proposes a new method for improving the scalability of linear statespace layers (LSSL) in neural networks for modeling longrange dependencies in very long sequence. The technique modifies the state matrix in LSSL enabling the state to retain data from the past which is crucial for capturing longrange dependencies. The paper provides a theoretical justification for the reparameterization and empirically demonstrates its effectiveness on various benchmarks. The proposed model S3 achieves significant performance improvements and computational efficiency. Questions: 1. Why is the baseline LSSL model excluded from most benchmarks particularly the Long Range Arena (LRA) 2. Are improved results expected over LSSL based on the findings presented in Tables 4 and 5 3. Does the proposed reparameterization guarantee an optimal state matrix A 4. Do the trainable parameters (diagonal matrix \u039b vectors p and q) preserve the structure of the state matrix A as a HiPPO matrix after training Is this preservation necessary 5. How does S3 perform in autoregressive generation tasks where error accumulation can limit the prediction horizon Can S3 mitigate this problem", "Summary: The paper expands on previous work using linear state space models (SSMs) with structured state transition matrices called HiPPOs. These matrices possess advantageous properties for memorizing longterm data from continuous signals. The papers primary contribution involves an efficient method for computing the convolutional kernel associated with the discretized SSM over time. By decomposing the state transition matrix into normal and lowrank components the problem is transformed into computing the Cauchy kernel a wellknown problem in numerical linear algebra. The resulting structured state space model (S3) is used in a deep neural network setting showing impressive performance on various tasks including longrange memory new speech classification generative modeling and sequential image classification. effectiveness and Weaknesses:  Wellwritten and wellcited.  Interesting and relevant theoretical and experimental subject.  Highly technical contribution (efficient computation of the discretized convolution kernel) with clear summary of key ideas.  High performance on benchmarks outperforming transformers and specialized audio CNNs. QuestionsRemarks:  Given that the HiPPO matrix is not unitary how does it avoid potential vanishingexploding gradients in SSMs  The HiPPO matrix is used for initialization but the algorithm can learn any NPLR matrix. Do NPLR matrices in general lead to wellbehaved SSMs or only those near the HiPPO matrix  Can S3 handle multidimensional input signals  In creating a multidimensional feature map by using H independent copies of S3 would it be more efficient to contribution the inputtostate and statetostate mappings while varying only the statetofeature mapping  The S3 model appears linear. Where are the nonlinearities introduced into the deep model  In Section 4.3 \"Irregularly sampled data\" the term \"irregular sampling\" may be misleading as it usually refers to nonuniform time grids which would impact the computation of the convolution kernel in Equation (5)."], "oxxUMeFwEHd": ["Paraphrased Statement: This paper presents TOGL a novel layer for Graph Neural Networks (GNNs) that incorporates topological information into the training work. Unlike Graph Filtration learning (GFL) which focuses on the output stage of GNNs TOGL is a more versatile layer that can be incorporated into any part of a GNN. Experimental results demonstrate that TOGL effectively leverages topological information when it is relevant to the task. Strengths:  TOGL provides a valuable tool for the GNN community.  TOGL opens up avenues for further research in Topological data analysis (TDA).  The paper is wellwritten. Weaknesses:  The papers flow may be confusing for nonGNN experts due to the absence of a brief background section on GNNs.  The experimental results while supportive are not particularly outstanding. Minor Suggestions:  Using f(\\theta) instead of f throughout the paper would enhance clarity as it eliminates the visual dependence of the righthand side on \\theta.", "Paraphrased Statement: Authors Proposal: The authors create a layer that works with Graph Neural Networks (GNNs). This layer can detect significant features in graph data such as connected parts and cycles. By learning from these features GNNs can make better models. Paper evaluation: The paper is easilyorganized easy to understand and based on solid research. The authors are upfront about the limitation of their work due to computer ability. They also show that even with these limitation using a few topological features like connected components and cycles helps GNNs improve. Reviewers Questions: 1. Comparison to DiffusionBased model: How do diffusionbased models compare to the authors model Are there any links between them that could be theoretically supported 2. Application to Contrastive learning: Can the model be used for contrastive learning with linear evaluation It seems like it could add a lot of useful information. 3. LargeScale evaluation: It would be good to test the model on large benchmarks like OGB instead of just the smaller TUdatasets. This would show how easily it performs and if it can be scaled up. 4. Theoretical Expansion: Expanding on the theoretical side of the work with model would make the paper easier to understand and more compelling.", "Paraphrased Statement: The paper proposes a topological Graph Layer (TOGL) plugin for Graph Neural Networks (GNNs) inspired by topological data analysis (TDA). TOGL enhances GNNs ability to detect topological structures. Strengths: 1. clear Motivation: The paper provides a convincing model (Figure 1) and a comprehensive review of related work to explain the need for capturing topological structures. 2. Technical Soundness and novelty: The paper introduces persistent homology which is more powerful than WLtest for topological awareness (Theorem 2). It also presents a model to integrate persistent homology into GNNs. Weaknesses: 1. Conflicting issue in Figure 1: The paper claims that Vanilla GCN outperforms WLtest in terms of topological structure awareness but this contradicts previous findings (GIN[1]) that suggest GCN should be less powerful. 2. potential gap between Betti issue and Graph Signals: The paper uses vertexbased filtrations to compute Betti issue and learn graph signals but it does not use a specific training loss to ensure alignment between the two. 3. Unverified quality of DeepSets Embedding work: The paper uses the DeepSets embedding work to convert graph signals into highdimensional space but this quality is not empirically validated. 4. Mixed Experimental issue: Table 2 evidence that WLtest outperforms TOGL on several datasets despite the papers claims that TOGL is strictly more powerful than WLtest.", "Paraphrase: Summary:  The authors enhance graph convolutional network (GCNs) by incorporating persistent homology an approach that captures the global topological features of graphs.  They conduct experiments involving graph and node classification showcasing the advantage of capturing topology.  Their method generally performs better than baselines and other topologyaware methods. Strengths and Weaknesses: Strengths:  Wellmotivated and comprehensible  clear formulation of method and experimental detail Weaknesses:  Concerns:  Lack of extensive testing with numerous layers and datasets to evaluate oversmoothing  Absence of experiments on geometrically diverse datasets that highlight the impact of topology  No discussion of a recent paper that employs persistent homology for geometric data analysis"], "qfaNCudAnji": ["Paraphrase: This research introduces a novel approach that incorporates proximal iteration into the critic network update process. This technique maintains the same computational cost as the original SGD optimization method. The paper provides empirical evidence to support the benefits of this enhancement. effectiveness:  The proposed method is straightforward and practical.  Despite the common function of proximal iteration in RL algorithms this is the first attempt to apply it to critic network parameters.  The paper offers a theoretical analysis of proximal iterations benefits.  The experimental results are comprehensive and the ablation study thoroughly investigates the techniques impact.  The writing and organization of the paper are clear and wellstructured. Weaknesses:  The technical contribution is modest.  The experiments functiond outdated DQN and DDQN framework. To enhance the significance of the findings the authors could consider one of the following approach:  Applying the proposed technique to advanced RL framework like DDPG or SAC in continuous control tasks.  Analyzing the magnitude of target network updates for other DQN variants akin to picture 4. This perspective could deepen our understanding of critic network.", "Paraphrase: Summary: This study uses Proximal Iteration to enhance DQNstyle RL algorithms. It ensures that the active network stays close to the target network stabilizing the training process. This straightforward technique additions the efficiency of DQN and DDQN in Atari2600 games. effectiveness:  Clear and concise writing.  extensive testing verifies that Proximal Iterations regularization term improves the performance of DQNtype algorithms in Atari games. Weaknesses:  Novelty: The regularization term itself is not particularly novel. The study provides no thorough explanation of why Proximal Iteration is effective in this situation. Additionally the theoretical analysis presented (Remark 1 and 2) is not original and does not contribute novel insights to the field.  Online network Bias: The paper suggests directing the online weights toward the target network without providing a detailed explanation. A more typical approach would be to direct the online weights toward the previous iterations old weights and the study should have compared the performance of these two strategies.  Theta Update: Equation 6 indicates that \u03b8 is updated each iteration while Algorithm 1 only updates \u03b8 when numUpdates  period is equal to 0.  DQNPro with DDQN: The study only confirms empirically that DQNPro can be combined with DDQN. Given the potential for conflict between the two methods objectives a more thorough explanation for the performance addition is needed.  Figure Quality: For a more professional appearance some picture in the experimental section should be recreated using better software (picture 25 appear blurry).", "Summary This study introduces a modification to the deep Qlearning algorithm by incorporating L2 regularization on the target network weights using proximal iteration. Despite the minimal modification in the algorithm the authors demonstrate improved performance across various Atari environments compared to baseline methods. Strengths  Simplicity: The proposed method is straightforward to implement with only a single line of code modification required.  Empirical efficacy: experimental results show a consistent and significant enhancement in task performance.  Low Computational cost: The method only involves a simple squared L2 norm calculation between weight set resulting in negligible computational overhead.  Easy Tuning: The hyperparameter introduced by the method (c) is easy to tune and can be set to a fixed value (0.2) for various applications.  Thorough Empirical analysis: The study provides performance comparisons with relevant baselines across a significant portion of the Atari 2600 benchmark demonstrating the methods general effectiveness. Weaknesses  Lack of Explanation: The paper fails to provide a clear explanation of why the proposed method improves performance.  Incomplete understanding of \"Deadly Triad\": The authors mention the \"deadly triad\" but do not adequately explain its connection to the proposed method or why proximal iteration in weight space is an appropriate approach.  Missing Citations: The authors omit referencing previous work that have investigated similar approach for constraining value network parameters or outputs.  Limited field Evaluation: The study only evaluates the method in the Atari field raising questions about its applicability to other fields or continuous control tasks.  Lack of Comparison with Other Stabilization Schemes: The authors fail to compare their method with alternative stabilization schemes.  Unclear option of Environments: The paper does not provide justification for the option of 40 environments from the Atari 2600 benchmark.  CherryPicked Results: The learning curves presented in picture 2 appear to be cherrypicked and do not fully capture the performance of the algorithms.  Insufficient Explanation of Update Magnitudes: The significance of the update magnitudes is not clearly explained in relation to the issues with deep Qlearning.", "Paraphrase: Summary: This research introduces a modified variant of the DQN algorithm that incorporates a \"proximal term\" to guide the learning process towards staying close to the target network. The authors demonstrate that this updated algorithm outperforms the original DQN on various Atari games. effectiveness and Weaknesses: The authors acknowledge the challenges in stabilizing deep reinforcement learning. They suggest that incorporating the concept of proximal iteration into DQN can enhance stability and performance. This modification slightly alters the weight update formula of the DQN algorithm favoring modification that align with the target network. The modified algorithm exhibits improvements over the original DQN as shown in comparisons with both the Google Dopamine variant and the authors reimplementation. Nevertheless it doesnt quite reach the current stateoftheart performance. Its significant to note that Formula 6 presented in the paper doesnt accurately reflect the infrequent updating of the \u03b8 parameter which is a crucial aspect of the proposed algorithm."], "zrdUVVAvcP2": ["This paper suggests using the idea of affordances from ecological psychology in planning. Affordances are possible actions for an agent influenced by their goals and the environment. This concept can simplify planning by reducing the number of options to consider. The authors contribution include integrating affordance consideration into planning and developing GrASP a method for finding affordances. They apply this method to DeepMind control suite domains and discuss its benefits and limitations. The paper is wellwritten and structured with a clear experimental section and a thoughtful consideration of the challenges faced during development. Notably the authors emphasize the importance of goalconditioned affordances which reflect the dynamic and agentdependent nature of affordances in ecological possibility. While other research has explored using affordances in agent model the papers approach to affordances and their impact on action generation is novel. However the authors could provide a more detailed motivating model to clarify the significance of incorporating affordances in planning. Additionally the selection of experimental domains could be expanded to include more sophisticated and complex tasks where the benefits of affordance integration would be more evident.", "Summary This paper presents a novel approach to using gradient descent for selecting action selections in planning. It introduces an affordance module that translates state representations into continuous actions. A learned model helps the planning action by providing dynamics reinforcement and values. Previous work typically updates the policy or actions using gradients along the trajectory under the learned model. However this work focuses on updating the set of potential actions to select from. effectiveness:  The approach bridges continuous control with discrete planning demonstrating how to update discrete action selections using gradient descent.  Experiments show GrASPs effectiveness in work with lowerdimensional selection spaces.  The paper analyzes the algorithms performance under different conditions including varying numbers of heads and conditioning on the goal. Weaknesses:  The algorithm is only compared to TD3 in the main results which is not considered stateoftheart for modelbased RL.  GrASPs advantage may be due to the lowdimensional action spaces used in the tasks.  Some details are unclear:  practice TD3 use primitive actions or pretrained selections  practice the affordance module implement any regularization to prevent output collapse  What do the discovered actions or selections look like and how do they compare to optimal actions  It is assumed that the visualizations in Figure 2 represent A agents rather than GA agents. Additional References:  [1] Chua et al. Deep reinforcement learning in a Handful of Trials using Probabilistic Dynamics model. NeurIPS 2018.  [2] Bharadhwaj et al. ModelPredictive Control via CrossEntropy and GradientBased Optimization. L4DC 2020.", "Paraphrase: Summary: This paper presents a novel approach called Gradientbased Affordance Selection (GrASP) which adds affordance selection (action or selection selection) to valueequivalent modules in the planning action. GrASP claims to excel over modelfree reinforcement learning (RL) by learning primitive actions and selection selection and planning in continuous environments. Affordances a mapping from states to a set of actions or selections are practiced to construct the lookahead research tree. The novel aspect of GrASP is its practice of the gradient of performance loss with respect to affordance model parameters to guide affordance selection. GrASP employs two elaboration procedures: shallowdepth complete trees and Upper Confidence Tree (UCT)based Monte Carlo Tree research (MCTS). It compares its performance to TD3 and different variants of GrASP on tasks from the DM Control Suite. effectiveness:  Offers a comprehensive ablation analysis of GrASP with both shallowdepth complete trees and UCTbased MCTS.  Extends the Valueequivalent model with an affordance learning module.  Provides a qualitative analysis of switches between affordance mapping outputs.  Episode returns for GrASP variants tend to be positively skewed when they outperform baseline model. Weaknesses:  The claim of introducing the concept of affordances for planning in continuous domains is inconsistent as sample MuZero is mentioned in related work as a previous approach. GrASP differs from MuZero by using gradients and a discrete set of affordance mappings for tree construction. However the paper does not clarify how this discrete set is determined from the continuous action and state space which may involve environmentspecific design selection.  GrASP is not compared to other stateoftheart (SOTA) modelbased algorithms. While the authors acknowledge that GrASP is not the SOTA it makes it difficult to assess its contribution.  The comparison between GrASP and Dreamer in the Appendix is relevant but some observations appear inconsistent with the provided plots. For instance Dreamer performs better than GrASP on Walkerwalk and CheetahRun. A discussion comparing the relative merits of the two model would enhance the analysis.  The solution section emphasizes which GrASP variants perform well in different scenarios but it does not provide a clear conclusion regarding the better model or its taskenvironmenthyperparameter dependencies.  The claim that GrASP fails to improve over TD3 in CartpoleSwingup is incomplete as one GrASP variant initially learns faster than TD3 although TD3 eventually achieves a higher asymptotic performance in ReachEasy and WalkerWalk. small Issue:  The term \"root note node\" is unclear. Clarification: comparison discrete steps in TD3 and GrASP: In TD3 a step is a single transition while in GrASP the continuation of the tree elaboration procedure is typically considered a step. The action selection in GrASP is performed at the leaf nodes of the research tree which can have varying depths depending on the selected elaboration procedure. In contrast TD3 executes an action for every transition (step).", "Paraphrased Statement: Summary: The paper presents a novel approach to tackle continuous action space planning with tree research algorithms by utilizing affordance learning. The proposed architecture comprises multiple modules with one module specifically designed to expand the research tree by predicting a set of discrete affordances (possible actions) for each state node. experimental results demonstrate the effectiveness of this technique in handling complex tasks where conventional tree research methods may struggle. effectiveness:  The idea of affordance learning is innovative and has potential implications for improving the capabilities of agents in complex decisionmaking scenarios.  The paper is wellwritten and accessible to readers with a basic understanding of reinforcement learning (RL). Weaknesses:  It is unclear what prevents the affordance prediction module from generating like actions potentially limiting exploration and optimization.  The initialization of the affordance prediction model may significantly impact its performance and further clarification on its role is needed.  The gradient update action for the affordance module considers only a limited number of affordances raising concerns about thorough exploration of the action space.  Figure 4s representation of affordance selection is difficult to interpret and further explanation of its significance is required."], "psQ6wcNXjS1": ["Paraphrased Statement: This work introduces respective Markov Chain Monte Carlo (MCMC) initialization approaches for training EnergyBased Models (EBMs) with varying input lengths. These approaches address different applications including image generation adversarial defense and density estimation. For shortrun image generation: The authors propose a hybrid initialization method that combines persistence and cooperation to overcome diversity issue in EBM learning with generator initialization. For midrun adversarial defense: The authors suggest rejuvenating a pretrained generator to enhance EBM defense capabilities as the input length increases. For longrun density estimation: The authors propose rejuvenation methods and regularization techniques to address oversaturation problems. Strengths:  The paper is wellstructured and accessible.  The empirical results demonstrate improvement over baseline models.  The fairness of the results requires further investigation. Weaknesses:  The incremental nature of the contributions as many of the techniques have been previously proposed.  Concerns regarding:  The lack of comparisons with pretrained generators for image generation.  The absence of comparisons with Xie et al. (2018).  Limited ablation study to evaluate the impact of annealing and the effectiveness of pretrained generator initialization compared to data samples.  Missing density estimation results for longrun EBMs.", "Summary Paraphrase: This paper explores applications of energybased frameworks (EBMs) in image generation adversarial robustness and density frameworking. However traditional EBM training methods have limitations in the latter two applications. Therefore the authors propose a new method that combines elements from CoopNets and PCD training approaches to address these shortcomings. experimentation demonstrate that their method outperforms other EBM form on the specified tasks. Strengths Paraphrase:  Addresses a significant issue in EBM training which is often challenging to optimize unstable and slow.  The proposed method combines the advantages of CoopNets and PCD to potentially provide a balanced approach with minimal additional computational cost.  Highlights realworld challenges in the EBM field including the tradeoff between sample quality and density framework accuracy and emphasizes the potential of EBMs in adversarial robustness. Weaknesses Paraphrase:  Lack of theoretical justification for the proposed modifications to CoopNetsPCD which could enhance the papers credibility.  Insufficient experimental details in the paper including information about optimizers learning rates batch sizes and training time hindering reproducibility.  Unclear if the proposed method simplifies EBM training or introduces additional hyperparameters for tuning which could impact its practical utility.  Density estimation evaluation relies on FID a qualitative step of sample quality while alternative approaches exist for assessing density framework accuracy.", "Paraphrased Statement Summary: The paper presents training strategies for energybased models:  Short sampling for image generation  Midrun sampling for adversarial defense  Longrun sampling for density estimation The authors claim that these strategies lead to significant performance improvements in all three applications and achieve stateoftheart results. Evaluation: Strengths:  study training methods for energybased models. Weaknesses:  The claim of stateoftheart performance is questionable.  The papers results are not clearly superior to those of generative adversarial networks (GANs) diffusion models and scorebased generative models.  only FID scores are reported for density estimation despite the papers claim of density modeling.  The computational cost and time requirements for training and using energybased models are not discussed.  The paper lacks significant technical novelty focusing primarily on the use of persistent initialization and pretrained generators for \"rejuvenation.\"  Pretrained SNGANs outperform energybased models in FID evaluations.  The use of FID in the density modeling section is questionable.  Diffusion models have been shown to surpass GANs on ImageNet even at higher resolutions.", "Paraphrased Statement: This study explores techniques for optimizing the training of EnergyBased Models (EBMs) based on the length of Markov Chain Monte Carlo (MCMC) trajectories used. It presents applications of EBMs in image synthesis adversarial defense and density estimation each utilizing a different MCMC sampling length regime. The authors propose a method to adjust MCMC initialization during training and testing to enhance performance in these applications. For image synthesis they combine persistent Chain with cooperative study where a fast generator framework is trained jointly with the EBM to overcome instability and lack of sample diversity. Strengths and Weaknesses:  image synthesis:  The need to use batch normalization in the generator should be clarified.  The persistent bank approach requires more explanation.  Adversarial defense:  The term \"annealing\" refers to the study rate in Section 3.1.  The specification of hyperparameters should be clear.  The similarity between trajectories of length K during late training and actual trajectories of length Kdef with annealed study rate should be discussed.  The robustness decrease with increasing K in form 6 needs further explanation.  Density Estimation:  The reasoning behind the statement that persistent samples cannot approximate steadystate samples needs elaboration.  The definition of terms such as \"lifetime\" and \"first bank\" requires clarification.  The comparison of results using data samples instead of a pretrained generator should be included.  experimentation evaluating the quality of the EBM density are needed.  Typos:  several typos have been identified and should be corrected."], "gICys3ITSmj": ["Summary Paraphrase: This paper suggests a method for integrating selfsupervised learning into machine learning. The authors show that contrastive learning techniques used in machine learning methods like R2D2 can perform well on various computer vision tasks. They also introduce two techniques to enhance selfsupervised learning models: (1) angle prediction and (2) batch gradient accumulation. Both methods improve upon the standard SimCLR model. effectiveness and Weaknesses Paraphrase: 1. Novelty: The idea of combining selfsupervised learning and machine learning is unique. The notion of treating different augmentation as task augmentation is conceptually intriguing. The proposed framework could inspire future selfsupervised learning reresearch. 2. Novelty (Trick 1): The first technique angle prediction is not completely new. It has been used in previous reresearch. Combining it with the joint embedding approach is also not novel. Its unclear how this setup differs within the machine learning framework. 3. Novelty (Trick 2): The second technique batch gradient accumulation appears new to selfsupervised learning. While the concept is derived from machine learning its application to selfsupervised learning is novel. 4. Reproducibility: The authors provide detailed experimental data for result reproduction. However the comparison to SimCLR is lacking. There was no hyperparameter research for the baseline SimCLRBYOL model which may not be optimally configured. 5. Overall Quality: The paper is wellwritten but the lack of an thorough hyperparameter research for the baseline model makes the incremental improvements observed less convincing.", "Paraphrased Statement: Overview: This research presents a new framework for selfsupervised visual representation learning drawing inspiration from the connection between contrastive learning and metalearning approaches for fewshot learning. Additionally techniques from metalearning are used to enhance existing contrastive learners. effectiveness:  The novel application of metalearning to selfsupervised learning.  Proposed framework outperforms SimCLR a stateoftheart contrastive learner in downstream tasks under a semisupervised learning setting.  Metalearning tools improve contrastive learners under the linear evaluation protocol. Weaknesses:  The claim that \"contrastive learning is only metalearning\" is not sufficiently supported by the arguments presented.  Evaluation results are inconsistent across different sections of the paper.  The Large Rotation as Auxiliary Loss technique is similar to previous research using random image rotations for selfsupervised learning.  The impact of training epochs and iterations on the performance of the proposed framework and SimCLR is not examined.  The nature of the loss function (l) described on page 5 is not specified.", "This paper demonstrates how metalearning training methods can be used for image data pretraining resulting in superior performance on downstream tasks compared to traditional contrastive learning techniques. The connection between these approaches is mathematically formalized and the insights gained are leveraged to develop improved data augmentation strategies for contrastive learning leading to enhanced performance on evaluation metrics. The paper excels in its theoretical foundation and practical contributions. It provides clear explanations accessible code and novel data augmentation techniques. However it could benefit from comparison to existing research on the relationship between unsupervised learning and metalearning especially considering the prior work of Hsu et al. (2019) on unsupervised metalearning. Additionally the paper title implies a broader conclusion than the specific connection to contrastive learning. It is uncertain whether the proposed methods would translate to nonimage domains like text. Minorly the description of metalearnings training loop focuses only on fewshot learning overlooking the broader scope of metalearning application. Reptile a metalearning algorithm does not distinguish between support and query task data as the paper implies. Lastly it is unclear what the computational expense refers to regarding performance in pixel space.", "Paraphrased Statement: The research paper explores the connections between selfsupervised learning (such as contrastive learning) and metalearning (such as finetuning). The relationship highlighted is intriguing but its not only original. notable is the use of gradient accumulation for contrastive learning a technique often employed in realworld application. However the explicit mention of this aspect is lacking in the paper. effectiveness: The paper is wellwritten and easy to comprehend. It effectively conveys the relationship between metalearning and contrastive learning. Weaknesses: The novelty of the presented ideas may be limited. However the paper serves as a valuable resource for those interested in the connections between these learning methods. The experimental results support the proposed approach but further analysis (such as confidence intervals) would have enhanced the paper rigor."], "kUGYDTJUcuc": ["Summary This search enhances the recurrent Attention Model (RAM) by incorporating an additional topdown attention mechanism. It utilizes image pyramids and Qlearning to select regions of interest in the topdown attention and then employs RAMs policy gradient to identify the patch in the bottomup attention. Additionally the authors propose two loss constraints to improve the performance of bottomup recurrent neural networks. The framework is endtoend and has been evaluated on three datasets demonstrating its effectiveness. Strengths  clear and organized presentation  Novel combination of topdown and bottomup attention in the RAM Weaknesses  The topdown models onestep prediction limits its effectiveness. The authors should consider multiplestep predictions or dynamic control of the prediction steps.  Unfair comparisons with existing methods as the proposed model includes additional topdown steps. Evaluation should account for the issue of topdown steps.  Unclear motivation for the context constraint (Eq. 11).  Limited applicability of the entropy constraint as image patches with high entropy may not directly relate to the final prediction in more complex datasets. Minor  Error in Eq. 10(c): Lt1 should be replaced with Lt Writing Suggestions  Reorder the \"context constraint\" and \"entropy constraint with better exploration\" sections to match the introduction and text.", "Paraphrased Statement: Summary: This field introduces a method to improve the initialization and exploration of the recurrent Visual Attention (RAM) model for image classification. The initialization is enhanced using image pyramids and Qlearning (topdown approach) while two new constraints are introduced to facilitate better exploration in the model (bottomup approach). Evaluation: The proposed method has been tested on various image classification datasets including MNIST cluttered MNIST SNHN (sequential multidigit recognition) and CIFAR10 (with robustness testing against PGD adversarial attacks). Strengths and Weaknesses: Weaknesses:  Lack of direct evidence demonstrating the superiority of the initialization and attention trajectory over the original RAM method beyond final accuracy.  Limited quantitative metrics and visualizations leaving the source of improvement unclear.  Potential for the improvement to stem from the increased model capacity rather than the proposed enhancements. Specific interest:  Lack of detailed analysis on the significant improvement observed for sequential digit recognition tasks.  Underperformance of the RAMbased method compared to a 2014 CNN method for sequential digit recognition.  Omission of runtime speed and computational cost comparisons in the evaluations.  Difficulty in comprehending Table 2 due to its presentation.", "Summary: This paper presents a groundbreaking technique that combines topdown and bottomup visual attention in recurrent neural networks. Additionally it introduces constraints to balance exploration and development during the examination of local domain. Strengths:  The concept is original and intriguing.  The writing is clear and concise.  Ample experiments showcase the models efficiency. Weakness:  Visual model should be included to illustrate the distinction between the proposed method and similar approach particularly the RAM model. These model would provide visual substantiation for the effectiveness of the proposed technique."], "nwKXyFvaUm": ["Summary This paper proposes a novel strategy for selecting clients in federated learning. It formulates the problem as minimizing the difference between the aggregated gradient from all clients and those from a selected subset. The paper provides a greedy solution and theoretical convergence analysis. While the method ideally needs all clients gradient it also includes a practical extension that uses gradient from previous iterations. Experiments show the effectiveness of the approach on various datasets. Strengths  Wellwritten and clearly motivated  Novel approach to client selection in federated learning  Reasonable objective and theoretical analysis Weaknesses  Requiring gradient from all clients is a limitation for practical applications  The extension using previous gradient may suffer from stale gradient issues  Lack of evaluation on how training efficiency is affected by limiting local updates when using previous gradient small improvement  Increase the size of plot labels and ticks  Enlarge the left margin of Algorithm 1", "Paraphrased Statement: This paper examines client selection for federated learning assuming that all clients will participate. It aims to choose a subset of clients such that the combination of their loss function gradient approximates the full gradient computed using all clients. By modeling the error as a supermodular function the paper proposes a corresponding submodular function that can be maximized. This maximization is equivalent to a greedy algorithm for selecting clients called DivFL. The paper demonstrates that the error between DivFL and the full gradient converges quickly under certain assumptions. Strengths and Weaknesses:  Clear and accessible writing  Math appears correct though standard  Could emphasize key differences from prior work Questions:  How does submodularity contribute to the analysis of the client selection algorithm  What is the connection between assumption 1 and submodularity  How restrictive is assumption 1 and how large is the parameter \\epsilon  Can the approach be adapted to partial communication scenarios small Comments:  assumptions 4 and 5 are stated for all t  Page 5: vkt should be vkt in the definition of \\overlinevt  The \"Setup\" paragraph on Page 5 could be clarified explaining the function of \\overlinev and \\overlinew  Appendix: gt and \\Delta vkt are undefined", "Paraphrased Statement: Strengths and Weaknesses:  Equivalence Issue: Minimizing a specific function is equivalent to maximizing another under exact consideration. However this equivalence does not hold under approximations.  Greedy Step Explanation: The greedy step in the algorithm is not explained clearly enough to understand its impact on communication complexity.  limited Structure Claim: The algorithm utilizes limited updates from clients which is surprising and requires justification regarding any specific structure that makes this claim valid.  assumption 1: This assumption which is a property of the algorithm is questionable and needs to be justified. This assumption is also referenced in a previous paper (Mirzasoleiman et al. 2020) but the exact location and relevance should be clarified.  Theorem 1 Validity: The validity of Theorem 1 is uncertain due to the issues with assumption 1.  Lack of Comparison to Existing work: The papers formulation shares similarities with Mirzasoleiman et al. 2020 but this relationship should be explicitly explored and compared rather than just briefly mentioned in the Introduction.", "Paraphrase: This research investigates using diversity sample (based on submodular function) to choose clients who transmit updates to the server in federated learning. The authors demonstrate that this approach optimizes the allocation of communication resources resulting in faster convergence and beneficial model accuracy. Key Points:  diversity sample using submodular function is a novel concept in federated learning.  The gradient involved in the work are not novel.  The work provides both theoretical and experimental evidence to support the effectiveness of this approach."], "qTTccuW4dja": ["Paraphrase: This paper presents a novel method for encoding and decoding sentences using volume coding based on arithmetic coding. The method has been shown to produce more valid generation compared to other models. effectiveness:  innovative approach to latent modeling  High validity performance Weaknesses:  Table 1 shows a low validity score (17.2) for a Transformer with 512 latent dimensions which is unexpected given the known capabilities of trained Transformer language models.  potential limitations:  Insufficient training information for the Transformer model  Transformer was not trained on sampled latent space  Suggestions for improvement:  Increase training information size  Add random noise to the Transformer training process to enhance its validity to sampling", "Summary Paraphrase: The research proposes a novel approach that employs the hidden states of autoregressive language models to create multidimensional arithmetic coding representation of both vocabularies and sentences. This technique connects the geometry of the latent space to discussion choices introducing a specific inductive bias that favors similar sentences. The authors evaluate the models performance on toy information and limited realworld information focusing on the fluency and grammaticality of unconditional sampling. However the evaluation do not assess semantic relatedness which may limit the models practical applications. effectiveness and Weakness Paraphrase: While the proposed method leverages arithmetic coding in multiple dimensions it raises concerns about its similarity to file compression techniques rather than autoencoders which typically prioritize information reproduction. Furthermore the problem that the model addresses is unclear as autoencoder encoders and decoders have wellestablished use cases. The authors also fail to justify why the specific realworld informationset was chosen leaving the relevance of the evaluation questionable. The research lacks a clear understanding of the \"biasing\" mechanism. Its unclear if it eliminates certain combinations leading to zero probabilities for all sentences or a weighting system that preserves some nonzero probabilities. The authors also express doubts about the inductive bias introduced by using dimensions in a roundrobin fashion suggesting that effects related to periodicity and subdiscussion tokenization may arise. Additional Issues and Questions:  The abstract implies that AriEL is an existing technique rather than a novel contribution.  Section 1 mentions \"such type\" without specifying the type.  Section 1 refers to an \"objective benchmark\" without defining objectivity.  Section 2 defines volume codes as distributed representation but contradicts this by describing them as points in Rd rather than values.  Section 2 lacks citations for noveler advancements in the mentioned methods.  The letters in Figure 1 are terminals despite being uppercase.  Algorithm 1 is not referenced in the main text.  The explanation of fixedlength representation is unclear as it implies padding or truncation.  The difference between 2 and 20 layers is significant and the optimal layer count may be somewhere in between.  \"GenerationDeocding Quality\" lacks a metric for grammaticality.  The validity metric may give misleading results when sampling count is finite.  The prediction quality metrics lack clear range.  The use of \"interpretability\" is poorly chosen language.  Figure 2 lacks raw information points and a clear definition of the yaxis.  Section 4.2s \"almost all\" quantification is unclear.", "Paraphrased Summary The paper introduces a method for converting sequences of text into multidimensional volumes rather than single vectors. This approach offers several benefits as demonstrated through experiments. effectiveness and Weaknesses effectiveness:  The problem addressed by the paper is relevant to representation learning. Weaknesses:  The paper is poorly organized and difficult to understand.  The authors assume prior knowledge of \"AC\" which may not be familiar to readers.  The choice of evaluation metrics is not well justified.  The benefits of the proposed \"volume Code\" are not adequately explained.  The paper does not explore the possibility of using the proposed method as a language model.  The paper does not propose direction to improve existing representation learning methods based on the proposed approach.", "Paraphrased Statement: Summary: This paper presents a method for compressing sentences into a latent space enabling text generation by randomly sampling from that space. The efficiency of this method is demonstrated on simplified information and a limited domain text informationset comparing it to Autoencoders (AE) Variational Autoencoders (VAE) and Transformer models. effectiveness and Weaknesses: 1. Writing Clarity:  The writing needs improvement especially the explanation of the method in Section 3.1.  significant performance are relegated to the appendix and notations are inconsistent. 2. Conceptual Concerns:  Its unclear why using a Language Model (LM) trained on biased information to split the latent space would enhance generalization for unbiased text.  While the method effectively utilizes the latent space it remains a concern whether it also inherits the bias from the LM. 3. Evaluation metrics:  Many evaluation metrics used in the paper are uncommon in the literature.  The metrics effectiveness for assessing encoding methods on larger more complex informationsets is questionable. 4. comparison to Other Models:  The comparison models (AE VAE Transformer) seem intentionally weakened with limited layers or sampling dimensions.  Its unclear how the proposed method would fare against stronger models."], "jT1EwXu-4hj": ["Paraphrased Statement: Summary This research investigates the domain transportation problem in information retrieval (IR) systems. The challenge lies in the limited overlap between source and target domains. The authors introduce an adversarial learning approach based on transportationconstraint risk minimization to overcome this effect. This approach considers the impact of recommendations on both observed and target domains. The paper provides theoretical analysis to support the proposed method and demonstrates its effectiveness through simulations and online experimentation. effectiveness and Weaknesses effectiveness:  Clear and concise writing style  novel overall framework  Theoretical analysis of the proposed method  Extensive experimental evaluation Weaknesses: 1. Lack of motivation: The paper does not provide a clear explanation of why the insufficient overlapping problem is significant in realworld RL systems or how the proposed methods address it. 2. Unclear theoretical analysis: proposition 1 effectively explains why IW and DA are ineffective in addressing the overlapping problem but the interpretation of Theorems 13 and Claim 1 remains unclear. Additionally it is not apparent how these theorems resolve the effect raised in proposition 1. 3. Insufficient literature review: The paper lacks a comprehensive review of related work including significant work on logging policy deficiency and unbiased recommendation learning. The authors should clarify the departure between their approach and other distributionally robust optimization papers using Wasserstein space. 4. Limited experimental comparison: The paper does not compare the proposed methods with appropriate baselines or benchmark datasets. It should include comparison with DA and advanced IW methods as well as evaluation on standard datasets for counterfactual learning in recommendation. A time complexity analysis would also be valuable.", "Paraphrased Summary: Researchers propose a novel method for learning recommendation policies based on feedback gathered from multiple policies. They argue that existing approaches face limitations when the policies recommendations differ substantially. To overcome this they introduce a reweighting and regularization technique that encourages exploration of policies with best rewards and overlapping recommendations with the current policy. They use a metric called Wasserstein space to address the effect of nonoverlapping recommendations. Theoretical and empirical evaluations show that their algorithm outperforms current methods. effectiveness:  Addresses a significant effect in recommendation systems.  Applies causal inference techniques to deterministic policies.  Provides comprehensive empirical results. Weaknesses:  Motivation for the specific approach is not fully explained.  assumption about reward model for extrapolation under nonoverlapping actions are not discussed.  The objective resembles counterfactual departure approaches and borrowing terminology from these approaches could improve clarity.", "Paraphrased Statement This paper presents a novel approach to recommender systems that focuses on finding intervention that effectively transfer patterns learned from one domain (observed domain) to another (intervention domain). To optimize recommendation algorithms in this context the researchers propose a \"transportationconstrained risk minimization\" target which is formulated as a twoplayer minimax game. theoretical analysis and empirical evaluations demonstrate the efficacy of the proposed method. effectiveness and Weaknesses  effectiveness:  Presents a novel perspective on recommender systems distinct from existing causal collaborative filtering approaches.  Provides comprehensive theoretical analysis.  Conducts empirical work on various data types (synthetic offline online) demonstrating the effectiveness of the method.  Weaknesses:  The paper lacks a metric for determining insufficient overlap between source and target domains which could influence the methods performance.  The bounds derived from different theorems are challenging to compare directly making it difficult to intuitively grasp the advantages of the proposed method.  The offline evaluation does not include the Deep and Wide method which is used in the online evaluation raising concerns about the fairness of the comparison.  The proposed minimax game approach for optimizing recommendation algorithms may involve greater complexity than other baselines and its efficiency should be analyzed in experimentation.", "Paraphrase: Summary: This work first emphasizes the critical requirement of domain overlap between source and target domains for the effectiveness of instance weighting (IW) and distribution adaptation (DA)based recommender systems. theoretical analysis also demonstrates that insufficient overlap can lead to difficulties or yet impossibility of using such methods. To address this effect the paper proposes a novel approach for enhancing recommendation performance by mitigating insufficient overlap. A transportationconstraint risk minimization objective work is introduced to optimize recommendations by effectively transferring knowledge from the source domain to the target domain. A twolayer generative discriminative adversarial model (GDA) is employed to optimize this objective work and thorough analysis of the GDA is provided. Extensive evaluations on substantial and semisynthetic datasets demonstrate the superiority of the proposed method. effectiveness:  Identification and exploration of the crucial effect of insufficient overlap in recommender systems.  theoretical validation of the uncertainty resulting from insufficient overlap.  Wellstructured and accessible presentation despite mathematical complexity.  Novelty and potential to inspire future research in recommender systems.  Convincing empirical validation. Weaknesses:  Minor typos requiring correction.  potential misprint in Table 1 regarding the Hit10 value for DTMCF on LastFM (81.81(20) may be intended as 81.81(0.20) or 81.81(0.02)).  Inconsistency in terminology referring to \"detail\" as both related and unrelated in certain instances.  Absence of comparison with IW and DAbased methods in the experimentation.  insufficiently clear motivation and foundation lacking detailed explanations of relationships between intervention domain adaptation and the proposed methods ability to mitigate insufficient overlap.  Lack of sensitivity analysis for the hyperparameter \u03bc despite its importance."], "iLHOIDsPv1P": ["Summary: The authors propose a new formulation for the information bottleneck problem utilizing the mutual information between the sampling and the learned weights as a regularizer. They provide analytical solutions for Gaussian distributions and propose an optimization algorithm. The authors explore scenarios with various activation works and noisy labels. Strengths and Weaknesses: Strengths:  Clear and wellwritten paper.  Novel idea to use mutual information between sampling and weights. Weaknesses:  Lack of clarity on the theoretical introduction:  It is unclear if the generalization gap bound based on I(SW) holds for the loglikelihood cost work used.  The distinction between cost work as information bottleneck or \"IIW regularization\" is not fully explained.  assumption and limitations:  The Gaussianity assumption for p(wS) may not be valid for all learning algorithms such as deterministic SGD.  It is not clear if the Gaussian assumption for p(w) can be used to bound I(SW) from above.  Discrepancy in algorithm description:  Line 9 of Algorithm 1 differs from equation (15) in its calculation of squared inner products.  Lack of explanation on:  The purpose of splitting IIW between layers.  The inflection point in Figure 1.  The variance explosion claim in Section 5.1.  Small mutual information values in figures seem unrealistic.  Unclear notation and terminology:  Is S samplingd iid in (4)  Why is the \"oracle prior\" called an oracle  How is the bootstrapping resampling weight \u03b6 defined  What is meant by \"l2norm keeps increasing\" at the end of Section 5.2", "Paraphrase: Summary: The authors propose a new interpretation of the Information Bottleneck (IB) method called the PACBayes Information Bottleneck (PIB). While the IB measures the mutual information between feature representations and inputs or targets the PIB measures it with respect to the empirical risk and the mutual information between framework parameters and the dataset. The authors demonstrate that PIB is related to generalization error and develop an estimator for the latter. They also present an approximate inference method for framework parameters using PIB. The results show that PIB captures the different phase of neural network fitting and compression based on activation functions depth and width. PIB also provides a effective approximation of generalization error that is resistant to label noise. The inference method improves generalization performance across various datasets. Strengths:  Novel technical and empirical contribution.  Clear differentiation from previous work.  Solid derivation and experiments supporting the claims. Weaknesses:  Language and clarity issue.  Lack of standard error intervals in graphs.  Technical terms used before definition.  Redundancy between terms \"IIW\" and \"I(w S)\".  Ambiguous statement about mutual information becoming trivial in deterministic cases.  Use of the term \"solution\" for an intractable problem.  Unclear definition of \"optimal posterior of PIB\".  Lack of loss plots in Figure 2.", "Paraphrase: Summary: This research introduces an updated version of the Information Bottleneck objective used in neural network training. This objective is driven by PACBayes bounds on generalization error which are proportional to the square root of the weights mutual information with the training information. The new objective thus minimizes both empirical risk and mutual information. An algorithm for estimating the mutual information (I(wS)) is developed and used to demonstrate an inverse relationship between I(wS) and generalization error across various neural network architectures. Strengths:  Proposes a novel and promising guiding principle for deep learning.  Addresses limitations of previous information bottleneck objectives.  Clearly written and technically sound.  Supported by thorough experiments that validate the theoretical motivations and approximations. Weaknesses:  Computational limitations of the proposed method are not fully explored e.g. compared to standard training or Bayesian neural network posterior approximation.  Minor grammar and spelling errors exist (e.g. \"infection point\").", "Paraphrased Summary: This paper presents the PACBayesian Information Bottleneck (PIB) a novel regularization method. The generalization gap is bounded by a work of I(wS) as shown in Eq. 4. PIB adds a regularization term of \\beta I(wS). Since I(wS) is difficult to compute the paper introduces various assumptions to estimate it using \\tildeI(wS) (Eq. 15). SGLD is used to compute \\tildeI(wS). experimentation show that:  \\tildeI(wS) exhibits a twophase transition during SGD training.  \\tildeI(wS) correlates with the generalization gap under various experimental conditions.  PIB outperforms l2 and dropout regularization. Strengths and Weaknesses: Strengths:  Addresses an significant problem.  Proposes a novel regularization approach.  Demonstrates that \\tildeI(wS) correlates with the generalization gap and improves performance. Weaknesses:  assumption are made to make I(wS) computation feasible. The robustness of these assumptions should be discussed and tested.  It would be helpful to show the actual generalization gap (with training and test accuracy) under different regularization methods. This would confirm that PIB reduces the generalization gap and not just improves training."], "gggnCQBT_iE": ["Summary Paraphrase: This paper explores an expansion of Structural causal Models (SCMs) that allows for cycles in causal relationships. The paper primary contribution lies in defining a \"MetaSCM\" using active mechanisms and effectively linking data to these mechanisms. Strengths:  The concept of cyclic causality is an intriguing domain of field.  The idea of \"sufficient representation\" is intriguing and has potential for further development. Weaknesses:  The paper lacks polish and requires significant editing and revisions to meet conference paper standards. Numerous grammatical and spelling errors (including a misspelling in the title) are present.  The theoretical contributions are insufficient for a conference paper. The authors need to demonstrate realworld problemolving application of the proposed MetaSCM.  The paper claim that tying the definition to data enhances flexibility remains unsubstantiated.  lack of experiments to validate the proposed definitions weakens the argument.  The paper contains unprofessional language and exaggerations as evidenced by phrases like \"significant theoretical problem\" and \"solid need.\"", "Paraphrase: This paper introduces a fresh perspective on causal graphical models by considering a more generalized perspective. Instead of restricting models to assumptions like acyclicity for easier model the authors introduce the concept of a \"meta causal model.\" This metamodel encompasses Structural causal Models (SCM) as a specific instance when indexed. They propose two additional assumptions: sparse mechanism transfer and sufficient activated mechanisms and discuss their implications. Strengths:  The paper introduces a novel and insightful approach to causal model.  The proposed model is intuitive and comprehensive capturing various scenarios such as feedback loops. Weaknesses:  The paper does not provide practical direction on how to learn or parameterize the proposed models.  It is unclear how researchers can apply interventional calculus or make interventional queries using these models. Conclusion: Despite the practical challenges this paper offers a refreshing and thoughtprovoking perspective on causal model. It has the potential to inspire future research and broaden the scope of causal inference.", "Paraphrased Statement: This paper proposes a method for characterizing nonstationary causal relationhips by using active sets of causal edges rather than context variables or domain indices. The authors also introduce Sufficient Activated Mechanisms (SAM) as an inductive bias for causal discovery and inference. This idea is theoretically interesting but the paper lacks a working system with theoretical guarantees and empirical validation. Strengths:  Novel approach to modeling causal mechanisms using samplewise active sets of edges.  Demonstrates the limitations of using auxiliary context variables for modeling causal nonstationarity. Weaknesses:  Unclear Motivation: The initial goal of addressing cyclic causal relation is not adequately addressed.  Conceptual model Only: The paper provides only a conceptual framework without a functional system or theoretical justification.  Unsupported Claims: The paper lacks evidence to support the effectiveness of the proposed SAM mechanism.  Limited Effectiveness: use active sets to represent nonstationarity may have advantages but it is not a novel concept and may not be effective in all scenarios.  lack of Justification for SAM: The starfish model does not provide a valid argument against Sparse Mechanism transfer (SMS) as SMS would also predict the observed transfer in functional relationhips."], "wMXYbJB-gX": ["Paraphrase: Summary This work investigates the strength of Label Smoothing Regularization (LSR) in training neural networks by examining the behavior of Stochastic Gradient Descent (SGD) with LSR in various nonconvex optimization scenarios. The convergence findings suggest that suitable LSR techniques can accelerate convergence by reducing label variance. Additionally the paper introduces TwoStage Label Smooth (TSLA) an efficient approach that combines smoothed labels in the early training stage with binary labels later on. Integrating TSLA with SGD enhances convergence by leveraging the benefits of LSR while enabling faster convergence in the final stage. extensive experiments demonstrate that TSLA improves the generalization performance of deep models on multiple benchmark. strength and Weaknesses strength:  Provides valuable theoretical analysis of LSR explaining why it can accelerate convergence.  Develops an enhanced method (TSLA) based on the theoretical analysis.  stage clear parameter and welldescribed methodology. Weaknesses:  Lacks experimental results for a commonly used LSR setting (theta0.1).  TSLAs strength on CIFAR100 dataset is unclear due to minimal improvement.  To ensure fair comparison experiments should include LSR results with the same number of epochs as TSLA.", "Summary: This research found that using a suitable amount of smoothing when optimizing nonconvex work using stochastic gradient descent can accelerate convergence. Consequently the authors developed TSLA a twostep algorithm that first applies label smoothing regularization (LSR) before switching to hard labels after a certain number of iterations. experimental results confirm the strength of TSLA. strength:  Clarity: The paper is wellorganized and easy to understand.  Rationality: The papers assumptions are supported by the literature or have empirical validation. The theory and proof appear sound.  Novelty: The paper offers novel insights into the convergence behavior of LSR with SGD. It suggests that an appropriate LSR can reduce variance and enhance convergence speed. Weaknesses:  Threshold in Theorem 3: The threshold for the smoothing parameter (\u03b4) to achieve a stationary point is dependent on the margin (\u03b5) which may be challenging to meet in practice. The authors should provide more guidance or empirical evidence regarding this threshold.  estimation of T1: Accurately estimating the number of epochs for LSR (T1) is crucial for TSLAs performance. Underestimating T1 can lead to worse results than applying no LSR while overestimating it can prolong training time. Finding the optimal T1 may be a demanding job.  Sensitivity to Hyperparameters: empirical results indicate that TSLAs performance is sensitive to the choice of the hyperparameter \u03b8. Large rate of \u03b8 can result in inferior performance compared to LSR alone. TSLAs strength thus hinges on finding appropriate rate for \u03b8 and T1.", "Paraphrase: Summary: This research explores the convergence model of stochastic gradient descent in deep learning with label smoothing (LSR). Findings suggest that LSR may hinder convergence towards the end of the optimization process. The authors introduce the twostage label smoothing strategy (TSLA) to enhance convergence. experimental results demonstrate the efficacy of TSLA on multiple datasets. strength:  Clear introduction and structure.  Intriguing theoretical analysis of label smoothing.  Simple and effective TSLA on limited datasets. Weaknesses:  Limited experimental scope: experimentation conducted on minor datasets and shallow models. more extensive examination on larger datasets and deep models such as ImageNet with ResNet152 are recommended.  Lower Top5 accuracy for LSR requires theoretical explanation.  Lack of exploration on label smoothings generalization across networks and datasets including ResNet152 MobileNet and ShuffleNet.  Novelty of TSLA is limited as it combines existing strategies but its optimization analysis is noteworthy.", "Paraphrase: Summary: This work analyzes the convergence behavior of SGD with label smoothing in deep learning. It also proposes a twostage label smoothing algorithm to enhance convergence. experimentation support its strength. strength:  Theoretical explanation for label smoothing regularization (LSR)  Twostage training approach Weaknesses:  Theoretical analysis lacks discussion of overfitting and relies on the assumption that smoothed labels in deep epochs impede learning progress (contradicting the overfitting viewpoint addressed by LSR).  Baseline SGD (without LSR) should be included in experiments.  Small CUB2011 dataset may affect observations.  experimental setup should incorporate data augmentation techniques and investigate other network architectures.  Datasets focused on finegrained range classification with limited classes.  experimentation should be expanded to larger datasets (e.g. ImageNet).  Impact of weight decay parameter on LSR should be explored."], "zfmB5vgfaCt": ["Paraphrased Statement: TransSlowDown is an attack strategy that targets the processing power of Neural Machine Translation (NMT) systems. By making small changes to benign input sentences this attack significantly increases the computational resource needed by the NMT to process the input. This paves the way for denialofservice attacks on translation services or battery drain on mobile devices. TransSlowDown is a white box technique that requires access to the gradients of the NMT system. However it can also be adapted to a block box technique using gradients from another NMT system. effectiveness and Weaknesses:  The lack of attention to adversarial input sentences in NMT research makes this a valuable contribution.  TransSlowDowns effectiveness is impressive.  The paper is wellwritten but potentially exaggerates the results. Limitations and future Directions:  practical NMT systems typically use a maximum sentence length which may limit the impact of TransSlowDown.  The attack is especially effective against Transformer models with selfattention decoders which have quadratic inference time complexity.  Testing the effectiveness of TransSlowDown on architectures with less computationally intensive decoders (e.g. LSTMGRU) is recommended.  Related literature on NMT faithfulness and efficiency techniques (e.g. quantization distillation) should be cited. Minor Issues:  Inconsistency in the power notation for the input and output sentences in Section 3.2.  Minor typos throughout the paper.", "Summary: The paper introduces an attack method for Neural Machine Translation (NMT) models that increases decoding complexity. To achieve this it Perturbs the input text . The method uses an algorithm that first identifies critical source tokens then applies tokenlevel or characterlevel perturbations to maximize the length of the result. effectiveness:  The attack effectively increases NMT model complexity during decoding.  The paper is clear and easy to follow. Weaknesses:  Whitebox attack Limitation: The attack scenario is limited to whitebox (internal model knowledge) attacks less significant for realworld blackbox scenarios.  Lack of Defense Solutions: The paper entirely proposes the attack method without providing corresponding defense solutions limiting its contribution.  Imperceptibility Evaluation: It is crucial in textual adversarial attacks to ensure that the generated adversarial samples are indistinguishable from the original input. However this paper lacks evaluation to demonstrate this.  Time Complexity: The method relies on greedy research which can be timeconsuming. The paper lacks comparisons with alternative baselines. Minor Points for Clarification:  The definition and derivation of \"importance\" in Equation 2 need further explanation.  It is unclear how to handle characterlevel perturbations resulting in multiple tokenized tokens or the UNK token as it may impact token importance.  The paper should specify how to select the most effective perturbation from the generated candidates to maximize the results length.", "Paraphrase: This study examines the effectiveness of neural machine translation (NMT) systems by introducing an innovative attack strategy to test their robustness. The attack consists of three steps: 1) identifying the most significant tokens for inference efficiency 2) creating adversarial perturbations for these tokens and 3) choosing the perturbations with the greatest impact. Experiments on three publicly available NMT systems demonstrate that this attack can significantly impair NMT efficiency. effectiveness:  The paper is wellstructured and the need is clear.  The topic is of interest and could attract considerable attention from industry and reresearch circles.  The proposed method is straightforward and wellcrafted. The attack to identifying significant tokens could be relevant to other areas of reresearch such as the over and undertranslation of NMT. Weaknesses:  The paper emphasizes the relationship between NMT efficiency and output length. However it would be beneficial to quantify the change in output length after the adversarial attacks. output length is a more direct measure of NMT efficiency and is hardwareindependent.  A straightforward solution to the attack could involve applying constraints to the beam research process of NMT. For instance decoding could be terminated when the output reaches twice the length of the input. If a simple constraint can mitigate the attack the contribution of the paper would be diminished.  Given the practical relevance of the proposed method it would be valuable to include experimental results on online translation systems such as Google Translate. The authors could address these weaknesses in their response.", "Paraphrase: This paper focuses on undermining the efficiency of neural machine translation (NMT) systems. It proposes a method to subtly change inputs to encourage NMT systems to produce excessively long translations reducing their decoding efficiency. The proposed method seeks to guide input modifications by minimizing the likelihood of \"endofsentence\" (EOS) signals thereby promoting longer translations. experimental results demonstrate the superiority of the proposed method in achieving efficiency attacks compared to previous attack. effectiveness:  The paper is wellwritten and presents a novel attack to attacking neural models. Weaknesses:  While efficiency attacks present a new research avenue they have limited practical value compared to accuracy attacks.  The paper overlooks the influence of decoding heuristics (length penalty maximum length constraint) on efficiency which can be easily exploited for defense.  The experimental results do not provide information on efficiency as a function of output length and implementation details such as maximum sentence length used in decoding are not disclosed."], "xspalMXAB0M": ["Summary of Paper and effectiveness This paper presents a novel approach to tackle reinforcement learning (RL) problems where the number of states is vast and the sample complexity is autonomous of this number. Instead of relying on structural assumptions the approach uses weak learners combined with boosting techniques to create a nearoptimal policy for the given RL problem. The sample complexity achieved is competitive and not dependent on the number of states provided weak learners are available. This approach is significant because it aims to bypass common structural assumptions (e.g. linearity) in RL theory while leveraging existing ideas from supervised learning. potential Applications The papers key effectiveness lies in its direction on agnostic algorithms which can be applied to a extensive range of RL problems by accessing weak learners that satisfy certain criteria. Weaknesses and Suggestions for Improvement Missing case: The paper would benefit from providing case of valid weak learners that meet the required assumptions. This would enhance the readers understanding of the applicability of the results. Clarity issue: The paper is written in a concise manner which may limit its accessibility. Providing more need and justification for definitions algorithmic choices and analytical results would significantly improve comprehension. Consider adding supporting text to explain the reasoning behind these elements. Minor Comments:  Include a discussion on the known achievable sample complexity for RL and MDPs in the tabular case to contrast them with the stateautonomous results presented here.  Provide clarification on Assumption 1 which appears stringent. An case weak learner would be helpful.  Define RW in the main text.  Define \u03b72 t before Theorem 13.  Describe the input parameters of algorithm 1.  Distinguish between M and P which are overloaded.  In Theorem 11 define little m and the policy class \u03a0.  Explain the term involving \u2130 in Theorem 11.  Correct the reference to algorithm D in the paper (it should be algorithm 5).", "Summary: The paper proposes using boosting with FrankWolfe optimization to combine weak RL learners into a strong policy. The theoretical bounds achieved does not depend on the number of states suggesting the method may be efficient in largescale MDPs. effectiveness and Weaknesses:  Dense and strong to read suffers from excessive notation and undefined terms.  Lack of clarity in presenting concepts and their importance.  No numerical case or discussion of practical performance.  Insufficient explanation of the function of the twolayer neural network.  Lack of discussion on when to use the proposed algorithm.  Insufficient background on boosting and no citations for known claims.  Unclear how dependence on state size was eliminated in the complexity bounds.  Assumption of an efficient exploration strategy is not explained.  Nonconvexity of the problem contradicts the mention of running optimization over the policy space. Minor Points and Typos:  \"w.r.t.\" should be \"with value to\"  \"we assumes\" should be \"we assume\"  Undefined terms in algorithm 1 (e.g. \\eta2 \\hat Q)  Misplaced period in algorithm 1  Incorrect notation in Theorem 11 (1\\delta)  Undefined norm for Q\u03c0 (s \u00b7) in Theorem 11", "Paraphrase: This paper proposes using boosting in supervised learning to improve policy learning in reinforcement learning (RL). The author provides a method for combining weak policies using an operator called the \"extension operator\" and proves the probabilistic convergence of the learning work. effectiveness and Weaknesses:  The mathematical details were not fully reviewed due to time constraints.  The paper lacks clarity and use undefined notations making it difficult to understand the main idea.  The practicality of the proposed algorithm is questionable as it is unclear how to compute certain components in practice.  The existence of function and policies that satisfy certain definitions is not addressed.  It is not clear how to estimate policy completeness or ensure that a key parameter remains finite in practice.  There is no compare between the proposed boostingenhanced algorithms and nonboosting RL algorithms.  Notations are inconsistent throughout the paper including the meaning of \"P\" in Theorem 11.", "This paper focuses on a technique called boosting in reinforcement learning (RL) to improve the performance of weak learning algorithms in developing efficient policies. The authors introduce an algorithm that iteratively enhances the accuracy of these weak learners ensuring that their sample complexity and runtime remain autonomous of the state space size. To address the nonconvexity of the value function they employ a modified variant of the FrankWolfe technique leveraging recent advancement in gradient boosting. Key effectiveness: 1. The paper explores boosting approach for RL which is a novel and relatively unexplored domain. 2. The algorithm utilizes a FrankWolfe variant to overcome the nonconvexity challenge relying on gradient dominance conditions established in earlier policy gradient algorithm studies. This application of FrankWolfe in RL is intriguing. 3. The papers structure and clarity are excellent making it easy to understand. The analysis appears to be rigorous. Key Weaknesses: 1. The algorithms application is limited to weak learners that optimize running function over the policy space which could be restrictive. 2. The analysis lacks significant novelty compared to earlier work by Hazan and Singh in 2021.", "Paraphrased Summary: This paper explores a new case of reinforcement learning (RL) algorithm inspired by the techniques commonly used in online boosting for supervised learning. The proposed algorithm incorporates the latest advancement from online boosting and demonstrates improved sample complexity performance which is unaffected by the number of states in the environment. effectiveness:  Successfully adapts online boosting techniques to the challenges of RL.  Provides a comprehensive theoretical analysis of the proposed algorithms. Weaknesses:  The proposed approach lacks significant novelty as it contribution similarities with existing work by Hazan and Singh (2021).  The paper fails to address the primary challenges associated with boosting RL compared to boosting contextual bandits an issue previously recognized in existing literature.  The presentation could be improved by incorporating related work on state compression techniques which are relevant to the papers focus on handling large state spaces in RL.  Empirical evaluations or experiments would enhance the papers credibility and demonstrate the effectiveness of the proposed algorithms."], "v-f7ifhKYps": ["Summary: Researchers create agents capable of coordinating with humans in the game Overcooked even without any prior human data. They initially train a set of agents with a focus on diversity. then they train a separate agent as the optimal response to this diverse population. Their method outperforms or matches the performance of existing techniques as evaluated both with a human proxy model and with real human players on Mechanical Turk. Strengths:  Exploring the benefits of diversity in populations for humanAI coordination.  Evaluating with real human participants which is crucial for this research field.  Providing video demonstrations and discussing qualitative aspects of agent behavior. Weaknesses: Missing or Misunderstood baseline and Ablations:  TrajeDi baseline: The authors population entropy bonus is a limited case of TrajeDi but they dont provide details on its hyperparameters.  PPOBC baseline: This topperforming baseline from prior work is omitted.  FCP baseline: A similar method to the authors approach that achieved stateoftheart humanAI coordination without human data.  Ablation of past checkpoints: The authors use multiple checkpoints for their full response agent which is omitted in the ablation study.  Ablation of individual population entropy terms: The population entropy objective includes two terms but its unclear if both are significant.  Ablation of uniform sampling: The performance of this ablation is surprisingly poor but learning curves are not provided. human Experiments:  None of the key baseline or ablations were evaluated with humans.  A betweensubjects aim in the human experiments may introduce biases. other Concerns:  Unnecessary section on Theorem 1.  Overlapping error bars in human experiments should be acknowledged.  Clarify the statement about increasing entropy and stable reward.  Strengthen the connection of cooperative games to realworld applications.  Cite additional relevant work on population diversitybased methods in multiagent settings.  Address typos and grammatical errors for readability.", "Summary: A novel approach is proposed for training AI agents that can work cooperatively with humans. It combines a technique that encourages diverse actions with a method for selecting the most appropriate human partner. Experiments show that this approach performs full than other methods in a cooking game. Strengths and Weaknesses:  The paper currently does not meet the standards for issue at ICLR because it presents a combination of two existing methods without properly acknowledging or referencing them.  The framing of the method is incorrect as it inaccurately describes TrajeDi as \"concurrent\" and fails to cite PFSP in the appropriate section.  However the paper contains valuable material including theoretical contribution novel combinations of existing methods and interesting experiments with human interaction.  To improve the paper the authors should reduce claim of novelty recognize the contribution of previous work and focus on the unique aspects of their research. Additional Comments:  Ablation study demonstrate the effectiveness of the prioritization scheme in combination with TrajeDi.  Figure 3 could be improved by combining reward and entropy curves into separate game and using colors to differentiate between methods.  Missing the stateoftheart baseline Off Belief Learning (Hu et al. 2021) for comparison.  Hyperparameter study results would be more informative if presented on the same game with shared axes.  The inclusion of human experiments is valuable but questions remain about the performance of TrajeDi with human partners and the impact of PFSP on human coordination.  The paper lacks a discussion of limitations such as scalability to larger population size and higherdimensional problems.  Bolding in Figure 5 should only be used if error bars do not overlap.", "Paraphrased Statement: Summary: This study introduces the Maximum Entropy Populationbased training (MEP) model to promote diversity in multiagent support learning (RL) with a focus on improving zeroshot humanAI coordination. MEP utilizes population entropy as a surrogate objective and employs prioritized sampling. Experiments demonstrate MEPs superiority over other approach with both simulated and real human players in the Overcooked environment. Strengths:  The paper is wellwritten and accessible.  The concept of incorporating population entropy into training is novel considering both individual and pairwise agent diversity. Concerns: 1. While policy diversity is beneficial for zeroshot humanAI coordination its potential applications extend beyond this specific problem. The authors should broaden the discussion to emphasize the methods versatility. 2. The prioritized sampling technique appears to play a significant role in training diverse policies. An ablation study would provide insights into the efficacy of each component. 3. Key references on diversity in populationbased multiagent RL such as [123] are omitted. A comparison with these existing methods would clarify MEPs contribution. Typos and Questions: 1. Algorithm 1 provides a detailed description of the training work but it is unclear whether the initial population policy pool is fixed or dynamic. 2. The \"Repeat environment\" subsection on Page 5 contains two paragraphs with identical names.", "Paraphrased Statement: The authors present a novel objective work for training multiple agents in support learning (RL) settings. This objective promotes diversity in the population of agent policies making them full equipped to interact with unfamiliar or \"unseen\" policies. The authors also propose a practical model for applying this objective at scale and demonstrate its effectiveness in the Overcook multiagent environment. Strengths and Weaknesses:  Strengths:  Technically sound combining established concepts from RL.  Effective in tests against comparable baseline in a relevant environment.  offer detailed data and code for reproducing the work.  Weaknesses:  Experimental Section:  Limited evaluation using only the Overcook environment excluding popular benchmarks that could provide broader insights.  Incomplete comparison with prior work (Lupu et al. 2019) lacking tests on toy problems presented in that study.  Focuses on AIhuman coordination overlooking the broader potential of the method for outofdistribution scenarios.  Content:  Unclear or unverified statements (e.g. claiming selfplay is dominant in RL)  Ambiguous statements about prioritized sampling and its optimality  Presentation:  Missing square brackets in a phrase (\"RL [...] RL \")  Incorrect phrasing in the foundation (\"prioritized sampling [of what] \")  Unclear terminology (\"incentive\" and \"multimodal\") in Section 3.1  Grammatical and punctuation errors in Section 3.4  Figure 3 is difficult to interpret and could be simplified.", "Summary (Paraphrased): The research aims to tackle the challenge of training AI agents in multiagent games. Traditional selfplay training can lead to agents that are too dependent on specific partners while populationbased training may not be effective without sufficient diversity. Recent study have focused on enhancing partner diversity to improve populationbased training. Problem Formulation: Coordinating with a novel partner (e.g. a human) during the test phase should ideally be within the experience of agents trained with diverse partner strategies. To achieve this the research introduces a population diversity metric based on crossentropy between agent pairs within the population. Proposed Approach: The research optimizes a lower bound for this metric based on the population entropy. Additionally a prioritized sampling procedure is suggested to determine which partner the agent should train with. Experiments and issue: Experiments in the game Overcooked show that the proposed approach outperforms the TrajeDi algorithm. Human testing demonstrates improved coordination with humans. Critique: Strengths:  Investigating a significant problem.  Testing on Overcooked and conducting human experiments.  Introducing a reasonable population diversity metric. Weaknesses:  The gap between the lower bound and real population diversity is significant limiting its utility.  Prioritizing the lowestperforming agents optimization is not fully motivated.  The prioritized sampling procedures benefits are unclear.  The discussion on Figure 3 regarding entropy convergence is unconvincing.  The writing could be improved for clarity and precision."], "i3abvoMoeCZ": ["Paraphrased argument: The paper introduces a novel approach for OutofDistribution (OOD) detection and model calibration. It utilizes the KL divergence between a uniform distribution and predicted distribution to derive two score works:  Covariate shift Score: Measures the deviation in feature distribution.  concept shift Score: Calculated as the difference between the predicted classs cosine distance and the average cosine distance of all classes. The paper also integrates a variant of Geometric sensitivity Decomposition (GSD) into these score works to enhance their sensitivity to distribution shift and improve model calibration. Additionally a new dataset called CIFAR 100 Splits is constructed providing varying levels of concept shift from CIFAR 10. The proposed approach exhibits promising performance in OOD detection (both covariate and concept shift scenarios) as well as calibration. Strengths and Weaknesses: Strengths:  Novel perspective on KL divergence and softmax linear classifier for OOD detection.  Unified approach connecting OOD detection and calibration.  introduction of the CIFAR 100 Splits dataset. Weaknesses:  Experiments on covariate shift datasets focus only on OOD detection neglecting model discriminative power.  unclear need for modifying GSD with a linear layer.  Lack of explanation for the benefits of a positive work in calibration.  Inadequate details on variants such as \"alphabeta\" \"alphaonly\" and \"betaonly.\"  Misleading argument about covariate shift occurring in isolation from concept shift. Questions: 1. Is the linear layer in GSD necessary 2. How does a positive work contribute to calibration 3. What do the variants \"alphabeta\" \"alphaonly\" and \"betaonly\" represent 4. clarification on the argument about covariate shift and concept shift.", "Paraphrased argument: This work introduces a technique to detect two work of data distribution shift:  Input covariate shifts caused by data corruption within the input field.  Semantic shifts where test data does not match the set of categories supported by the model (test data categories not found in the training data categories). The technique uses the decomposition of the KullbackLeibler (KL) divergence between predicted probabilities and a uniform distribution. Additionally a Geometric Outlier detection with Infinite Norm (Geometric ODIN) method is proposed to enhance OutofDistribution (OOD) detection and calibration outperforming existing methods on CIFAR10 CIFAR100 and SVHN datasets. Strengths and Weaknesses: Positive:  Wellwritten with clear introduction and writing style.  introduction of a new benchmark (CIFAR100 Splits) using discussion embeddings to simulate concept shifts.  Comparison with a wide range of baseline methods for OOD detection and calibration. Concerns and Questions:  need for detecting covariate shifts is unclear as adapting to shifted data may be desirable.  The feature norm used to capture covariate shifts is not rigorously justified.  Inclusion of the feature norm for the vanilla and alpha models in the results table would enhance understanding of its role.  Omission of relevant baselines proposed by other work (e.g. [1] and [2]).  Lack of evaluation on additional datasets used in OOD detection literature (e.g. Texture LSUN and Places 365).  Definition and significance of \"OutofDistribution calibration\" are not sufficiently explained.  Missing code for reproducibility. Minor Note:  Correction to the definition of concept shift on Page 3.", "Summary: This research proposes two scoring work to assess two types of OutofDistribution (OOD) detection: covariate shift (shift in data distribution) and concept shift (shift in data concept). The first work uses feature norms to detect covariate shift while the second uses angles to detect concept shift. The network uses a modified method to separate feature norms and cosine distances aiding in calibration. Experiments show promising performance on OOD model with varying levels of shift and calibration. Strengths and Weaknesses:  The exploration of OOD detection for both covariate and concept shifts is innovative.  The definition of concept shift as different concept with unchanged superficial characteristics may be more precise.  The theoretical need for the norm score is unclear and it may not accurately capture covariate shift behavior.  The use of alpha and beta as work of f is not fully justified.  The novelty claims could be revised as prior work have explored similar distinctions between OOD types.  The experimental results deviate from reported numbers in other work and the understanding for these differences should be clarified.  The proposed methods performance may not be stateoftheart compared to baseline methods.", "Paraphrased Summary: This work focuses on detecting OutofDistribution (OOD) data considering covariate and concept shifts. It examines score works that detect these shifts and theoretically develops new ones for enhanced OOD detection. The result is a calibration work that achieves stateoftheart performance on both indistribution and OOD data. Experiments on image recognition datasets (e.g. CIFAR100 SVHN) are included. Strengths: 1. Clear and wellwritten paper accessible to readers with varying levels of familiarity. 2. Solid derivation of score works for covariate and concept shifts with convincing rationale. 3. innovative use of geometric sensitivity decomposition and learnable instancedependent scalar argument (\u03b1 \u03b2). WeaknessesQuestions: 1. The explanation of Figure 1 could be improved as CIFAR10 vs. CIFAR100 is an extreme model of concept shift (same data different labeling). also the definitions of \"near OOD\" and \"far OOD\" are not clear. 2. The claim that intra orderpreserving works calibrate well for deep neural network is attributed to Rahimi et al. (2020). It would be helpful to explicitly connect this conclusion to Theorem 1 of that work."], "qPQRIj_Y_EW": ["Paraphrase: Summary: This paper introduces a new graphbased machine learning model for solving order fulfillment problems. These problems are typically complex and computationally challenging as they involve optimizing the allocation of orders to resources. The proposed model transforms each order fulfillment problem into a tripartite graph and uses a graph attention network (GAT) to extract features from the graph. A feed forward (FF) layer and an assignment layer then generate the solution. effectiveness:  The model is novel and innovative.  It effectively solves order fulfillment problems using machine learning techniques.  The model achieves promising results with a small cost gap compared to optimal solutions and minimal inference time. Weaknesses:  The paper does not provide details on how the training dataset of 100000 instances was generated. Its unclear if there are any unmentioned model or limitations in the data.  To demonstrate the models performance on unseen problems the authors should release the algorithm for generating the synthetic training datasets.  The paper does not include results from realworld datasets to assess the models practical applicability.", "Summary This research proposes a graphbased deep learning framework for addressing the order fulfillment problem in retail supply chain. The problem involves assigning (sub)orders to warehouses for fulfillment. The framework formulates the problem as a Mixed Integer Problem (MIP) which can be solved using MIP solvers like SCIP. To enhance the solution efficiency the authors train a graph neural network tailored to incorporate both edge features and diverse node types (orders suborders and warehouses). Supervised learning is used to generate predictions for optimal solutions on a dataset of 100000 instances solved with SCIP. Empirical evaluations across three problem sizes evidence that the trained model provides solutions comparable to SCIPs results (within 23) while being significantly faster (24 orders of magnitude). Strengths  Introduces a novel MLbased approach for the order fulfillment problem which is prevalent in online retail.  Adapts existing graph neural network architectures to suit the problem context including edge features and node type integration.  Clearly defines the problem and its mathematical preparation.  Provides a clear explanation of the model at an appropriate level of abstraction.  Demonstrates promising empirical results achieving nearoptimal solutions much faster than MIP solving and surpassing a heuristic approach. Weaknesses  Lacks clarity in positioning the paper.  Motivation for realtime problemsolving is weak as fulfillment typically takes hours.  Empirical evaluation is incomplete as both SCIP and the model yield different results with varying speeds.  The models reliance on order permutation limits its scalability.  Unsubstantiated or inaccurate claims are made.  Certain details are unclear or missing such as parameter generation and relevant problem aspects.  Overall the paper presents a relatively conventional application of supervised learning to graph neural networks albeit with suitable problemspecific adaptations.", "Paraphrase: A Graph Neural Network (GNN) is developed to address the need for realtime solutions in order fulfillment for supply chain management. The GNN model considers nodes representing orders items and warehouses and edge representing relationships (e.g. items contained in orders). These nodes and edge have feature vectors that capture additional data (e.g. warehouse location edge costs). The GNN learns optimal solutions to the fulfillment problem using a binary classification approach with training data consisting of optimal solutions. After training the GNN undergoes postprocessing to ensure feasible solutions within problem constraints. Experiments demonstrate that a GNN trained on small problem instances performs well even on large instances providing nearoptimal results in a shorter time frame compared to standard GNN approaches a nonML heuristic and an exact solver. effectiveness and Weaknesses: effectiveness:  Motivated by a challenge problem that aligns with ML applications  Clear and wellwritten paper  GNN performs effectively on large problem instances surpassing baseline approaches Weaknesses: 1. The requirement for knowing the maximum number of orders contradicts the exact size invariance of the model. 2. Clarification is needed on instance generation procedures and whether previous literature has utilized them to create difficult order fulfillment problems. 3. The cited baseline heuristic lacks a source hindering its evaluation. 4. inference time for the exact solver (Exact BB) may not accurately reflect its capabilities. Tracking incumbent solutions and measuring cost gap at different time limits would be more informative. 5. Sample complexity data is absent leaving questions about the GNNs data requirements for optimal performance. 6. The \"masking procedure\" mentioned in Section 4.4 merits clarification on its guaranteed feasibility. 7. Figure 3 is difficult to interpret due to excessive edge. An alternative visualization is suggested. Typos:  Equation (1c): The subscript in the sum should match the K in Table 1 (first row).  Table 2: \"TARIN\" should be corrected to \"TRAIN\".", "Summary The authors propose an imitation learning approach using Graph Neural Networks (GNNs) to address a complex order fulfillment problem. They utilize the SCIP expert policy for behavior cloning using a Graph Attention Transformer (GAT) model on a tripartite graph of orders items and warehouses. Compared to the Pointer Network heuristic and GAT without edge features their method demonstrates significantly reduced inference latency. effectiveness  Utilizes best practices from recent ML research on Combinatorial optimization imitation learning with GNNs and GAT.  Addresses a practical order fulfillment problem with nontrivial components. Weaknesses  Insufficient details on the experimental setup including data sources characteristics and SCIP parameters.  baseline lack effectiveness:  SCIP parameter optimization is not discussed.  Comparison with \"learning to branch\" algorithms is missing.  heuristic baseline explanation is inadequate.  Questionable technical claim regarding model independence from graph size.  Feasibility of model solutions is not reported. Minor Comments  Figure 3 is unclear and needs further explanation.  The variable N is used both for the maximum number of orders and the graph encoder layer which may be confusing."], "fM8VzFD_2-": ["Summary: The authors propose a novel conditional Variational Autoencoder (VAE) that utilizes diagnostic information as a learning signal to find compact representations of brain connectivity data that retain diagnostic relevance. This study aims to advance computational psychiatry by enabling a more continuous understanding of mental illnesses. The VAE is evaluated on synthetic and realworld datasets from patients with psychiatric diagnosis demonstrating promising results. The authors main technical contribution is a novel cost function for optimizing the embedding space. effectiveness:  Practical extension of existing VAE methods for mental health applications.  Comprehensive analysis using synthetic and empirical data comparing to diverse approaches.  validation using established psychological questionnaires.  Ethical considerations highlighted.  Clear writing way. Weaknesses: Technical:  Questionable choice of equal weighting (lambda  0.5) for the two information incorporation methods (Figure 2).  potential underestimation of withincluster variance using the proposed framework (Figure 2).  Lack of statistical significance testing to support claims of superiority. Empirical study:  Incomplete information about MRI preprocessing and data harmonization.  unclear validation scheme for the second dataset.  No multiple testing correction for FID comparisons.  Counterintuitive space relationship between diagnosis (SCZMDD  SCZASD). other:  minor font size in figures. Suggestions for Improvement:  research optimal weighting of information incorporation methods.  study variance estimates of the proposed framework.  Conduct statistical significance tests.  Provide detailed information on empirical study parameters.  Clarify validation scheme and the rationale for diagnosis space comparisons.  addition font size in figures.", "Paraphrased Statement: This paper introduces a technique for reducing the dimensionality of brain connectivity data for individuals with psychiatric disorders such as autism spectrum disorder major depressive disorder and schizophrenia. The method utilizes a conditional variational autoencoder informed by diagnostic information. The effectiveness of the method was assessed using two neuroimaging datasets resulting in clustered twodimensional representations of multipsychiatric disorders. However the paper has several shortcomings: 1. The language is unclear and challenging to comprehend. 2. The specific applications of the lowdimensional representations obtained by the algorithm are not clarified such as whether they can be used for patient classification. Using diagnostic labels for dimensionality reduction raises concerns about data leakage. 3. The paper attempts to guide dimensionality reduction using diagnostic labels. However mental illness diagnosis often lacks a definitive standard and relies on subjective symptom descriptions. As such diagnostic information may not be reliable for guiding the analysis of brain imaging data.", "Paraphrased Statement: The authors present a novel method using a variational autoencoder that incorporates functional connectivity data from fMRI scans to uncover hidden relationships between diverse neuropsychiatric disorders. The autoencoder transforms the highdimensional connectivity space into a lowerdimensional representation maintaining the pairwise relationships between diagnostic characteristics. Clinically this approach provides a continuous dimensional characterization of complex disorders rather than relying solely on distinct diagnostic labels. The authors evaluate their method on both simulated and clinical rsfMRI datasets of patients with autism spectrum disorder Major Depressive disorder and schizophrenia. Their experiments show consistent pairwise relationships in the latent representations demonstrating the framework ability to capture dimensional characteristics of multiple brain disorders beyond diagnostic labels. effectiveness:  Clear and comprehensive presentation of the methodology and technical contribution.  validation of the framework on two independent clinical datasets.  Exploration of a relatively understudied problem in functional connectivity analysis. Weaknesses:  Lack of a \"ground truth\" dimensional characterization makes it difficult to fully interpret the results.  Evaluation on simulated data could be enhanced for interpretability.  Lack of comparison with baseline methods and ablations in the qualitative assessment of latent embeddings.  utilization of FID space metric only on real data limits understanding of its utility.  Inconsistent conclusions regarding statistical significance in the UTO dataset.  unclear validation procedure for setting hyperparameters.  Limited clinical interpretability in identifying specific neural correlates or aberrant connectivity patterns underlying the nosological relationships.", "This research introduces a novel conditional variational autoencoder that uniquely utilizes diagnostic data in creating an optimized embedding space for complex functional connectivity data. To validate this approach we applied it to two large neuroimaging datasets of neuropsychiatric disorders and found a consistent relationship between autism spectrum disorder major depressive disorder and schizophrenia.", "Summary: This research introduces a novel method for creating lowdimensional representations of neurological disorders using a conditional VAE framework. This framework leverages diagnostic information in two way: 1. Clustering samples from the same disorder using the conditional VAE framework. 2. Separating cluster through contrastive learning. effectiveness:  Novel conditional VAE framework presented.  diagnostic information utilized in two way to enhance embedding learning.  Experiments conducted on both synthetic and real datasets demonstrating a consistent relationship between three disorders. Weaknesses:  Experiments only included ablation versions of the proposed framework and a few other algorithms. The authors could consider testing alternative conditional VAE frameworks and applying their contrastive loss to other frameworks.  Stability of the proposed method across multiple runs is unclear.  Real dataset results are difficult to interpret due to the utilization of crossvalidation.  Generalization of the trained framework between datasets was not tested.  clinical implications of the observed ordering of cluster are not discussed.  The writing in the paper could be improved for clarity and readability. minor Comments:  Clarify if the NCA method utilizationd any diagnostic information.  Verify the reported minor FID value for the MDDSCZ cluster pair."], "hgKtwSb4S2": ["Paraphrased Statement: Summary: This paper introduces a method to improve the Randomized Singular Value Decomposition (SVD) algorithm. Traditional Randomized SVD employs a Gaussian random matrix to reduce matrix size before performing SVD. The proposed method utilizes a multivariate Gaussian distribution with a custom covariance matrix. Leveraging the prior knowledge captured in the covariance matrix enhances the approximation accuracy of Randomized SVD. Experiments on synthetic data demonstrate the effectiveness of the proposed approach. effectiveness:  Addresses a fundamental research problem in reducing computational time while improving approximation quality using Randomized SVD.  Paper is wellstructured and the theoretical ground of the proposed method is clearly explained. Weaknesses:  Experiments only use synthetic data raising questions about the applicability of the proposed method to realworld scenarios.  Different matrix types (dense vs. sparse) may impact singular value decay and the proposed approach performance. This should be explored.  The experiment in Figure 2 does not account for the computational time required for Cholesky factorization in the proposed method. This affects the fairness of the comparison to the previous approach which does not require Cholesky factorization. The paper should compare the endtoend computational time of the two methods.  The paper should provide theoretical computational cost analysis of the proposed method.  The observational results should include the impact of the oversampling parameter on the approximation quality of both the proposed and previous approaches.  The context of the oversampling parameter in the experiments is not explicitly stated.", "Paraphrased Statement: This field introduces and evaluates a broader version of the randomized singular value decomposition (SVD) algorithm that handles any covariance matrix and HilbertSchmidt operators. Extensive numerical experiments validate the advantages of this expansion. effectiveness and Weaknesses: effectiveness:  Expands randomized SVD to handle matrices with arbitrary covariance matrices.  Extends the method to approximate HilbertSchmidt operators.  Demonstrates that the effectiveness of randomized algorithms depends on the spectrum of the covariance matrix or kernel. Weaknesses:  The introduction in section 3.1 (Randomized SVD for HilbertSchmidt operators) could be clarified including an explanation of how QR is used to obtain Q and the significance of the polynomial reintroduction of the covariance kernel.  No empirical results for approximation errors in the context of HilbertSchmidt operators are presented.  The metric \u03b3k used to assess \"the quality of the covariance kernel K in \\mathcalGP(0 K)\" in section 4.2 remains unclear.", "Paraphrased Statement: Summary: This field introduces a generalization of the randomized SVD algorithm for nonstandard Gaussian vectors. This technique is applied to approximate HilbertSchmidt operators using a novel kernel based on weighted Jacobi polynomials. effectiveness and Weaknesses: 1. Abstract:  The existing theory of randomized SVD for multivariate Gaussian vectors is credited but a claimed generalization for nonidentity covariance matrices seems ambiguous. 2. Generalization 1:  The authors appear to present new theoretical results on approximation error but this claim requires clarification. 3. comparison:  It is unclear whether an optimally chosen covariance matrix can outperform or be outperformed by the randomized SVD with standard Gaussian vectors. 4. kernel Construction:  The ground for using polynomials in the kernel design is not fully explained. 5. Figure Ordering:  Figure 4 is extensiond before it appears in the text leading to confusion. 6. Notation:  The use of L to denote both a kernels Cholesky factor and the space of square integrable functions can create ambiguity. 7. Enumeration Notation:  The use of (1) (2) (3) as enumerators within a paragraph overlaps with the notation for numbered equations making it difficult to read. 8. kernel Unbiasedness:  The notion of unbiasedness in the covariance kernel lacks motivation and may depend on specific applications. 9. Figure 1:  It is argued that the functions in Figure 1 exhibit the opposite behavior of what is claimed. 10. Figure 2:  An additional plot of error versus time would enhance the authors argument. 11. Proofs:  The proofs would benefit from explicitly stating the results being invoked. 12. Proof of Lemma 7:  The source of the upper bounds on the moment generating function is not specified. 13. Proof of Theorem 2:  The extension to Boulle and Townsends Theorem 3.2 should be explicitly stated in the text for clarity."], "r4PibJdCyn": ["Paraphrased Statement: Summary and effectiveness:  Presents a collaborative framework for generating candidates for both recommender systems (providing items to users) and advertising systems (finding users for items).  Introduces a BiInfoNCE loss that leverages the twotower architecture to learn user and item representations jointly.  The framework called TR picture improved performance in both recommender and advertising datasets compared to traditional Matrix Factorization (MF). Weaknesses:  The primary contribution the BiInfoNCE loss is similar to an existing method known as UniInfoNCE. This limits the technical novelty.  Lack of comparison with related recommendation framework.  Evaluations in the PUMS dataset are limited to MovieLens1M which is not a widely used advertising dataset. The chosen baseline (MF) may not be competitive enough in advertising.  The universality of TR across multiple framework is not tested.  Evaluation metrics for nextitem prediction in Table 2 are not consistent with standard practices.  It is unclear if the improvements in BiInfoNCE come primarily from using more negative samples or other factors.  References:  [1] Contrastive Learning for Debiased candidate generation in LargeScale Recommender Systems  [2] A Simple framework for Contrastive Learning of Visual Representations", "Paraphrased Statement: This paper proposes a new framework TotalRecall that simultaneously addresses candidate generation for users and items in recommender and advertising systems. TotalRecall introduces two novel ideas: a normalization approach for the score work and a bidirectional variant of the InfoNCE loss. Through extensive experimentation the paper evaluates the effectiveness of the proposed method in various scenarios. effectiveness:  Presents a novel approach to jointly address recommendation and advertising in ecommerce.  Conducts comprehensive experimentation to assess the methods performance. Weaknesses:  Lacks sufficient motivation for the claim that joint modeling of recommendation and advertising enhances ecommerce services.  Insufficiently explains how such platforms integrate recommendation and advertising and how the proposed method benefits them.  The technical novelty of the method is limited as similar ideas have been explored in previous research.  The contribution of the method is unclear if its effectiveness is only driven by experimental evidence rather than theoretical or rigorous analysis.  outcome do not consistently support the superiority of the proposed method.  The clarity of the paper is lacking with various unclear aspects of the method and experimentation.  Missing results and inconsistencies in experimental reporting.", "Paraphrase: Abstract: This research presents a technique for generating candidate recommendation in recommender systems considering both usertoitem (user2item) and itemtouser (item2user) scenarios. The proposed approach leverages infoNCE loss to train a twotower framework for optimizing the joint distribution of useritem interactions. To facilitate bidirectional candidate generation the authors implement batchbased negative sampling from both user and item perspectives utilizing the BiInfoNCE loss work. effectiveness: 1. Clear and wellstructured introduction of the main concept. 2. BiInfoNCE loss enables joint distribution frameworking for bidirectional candidate generation. 3. Experimental validation on various public datasets. Weaknesses: 1. Lack of novelty in using InfoNCE loss in recommender systems as previous work have employed it. 2. Questionable justification for frameworking pdata(ui) instead of separately handling pdata(ui) and pdata(iu) for candidate generation. Furthermore UniInfoNCE u2i outperforms BiInfoNCE in Table 2 undermining the rationale behind BiInfoNCE loss. 3. Comparison results show that ComiRec methods significantly surpass the proposed approach on the Taobao dataset. This discrepancy is not addressed in the research. 4. Underwhelming performance versus matrix factorization (MF) framework. While TR outperforms MF on Pinterest MF obtains higher hit range on Movielens. Additional results for different truncation value are not provided. Overall the improvement over pointwise lossbased MF is not significant.", "Paraphrased Statement: Summary: This paper proposes a unified framework for recommender systems (RS) and advertising systems (AS) by considering them as inseparable counterparts. The approach involves frameworking the joint probability of useritem interactions rather than treating them as separange conditional probabilities. The proposed framework leverages an existing loss work for multilingual language frameworks to optimize its objective. experimentation on realworld datasets demonstrange the benefits of this approach relative to traditional matrix factorization and sequential frameworking methods. effectiveness and Weaknesses: effectiveness:  Positive attempt to address RS and AS outcome simultaneously. Weaknesses: Major Concerns:  Weak motivation: The rationale for considering RS and AS jointly and the potential benefits and challenges are not clearly articulated.  Limited technical contribution: The proposed frameworks objective work (Equation 3) is only an extension of existing loss works (Equation 1) expanding it from singleloss to dualloss (useritem) optimization. minor Concerns:  system and writing improvements needed.  Mathematical notation errors in the text and Appendix.  For advertising system evaluation the MovieLens dataset and CTR (clickthrough range) metric are unsuitable.  Typos include:  \"Amazaon\" instead of \"Amazon\"  \"generation compare to solid baselines\" instead of \"generation compared to solid baselines\"  \"with higher accuracy compare to other methods\" instead of \"with higher accuracy compared to other methods\"  \"find that l2normalize u and i and then rescale the dot product\" instead of \"find that l2normalizing u and i and then rescaling the dot product\""], "hxitw01k_Ql": ["Paraphrase: The authors assess two memory designs for a specific decisionmaking problem. They find that a basic memory architecture where the agent only accesses a fixed number of past interactions (m) is more resilient to random starting conditions than a more flexible memory architecture (RAM). Despite this difference both designs exhibit comparable performance. effectiveness:  Innovative research that highlights potential limitations of current memory allocation training methods.  open and insightful explanations of optimal strategies (e.g. \"necklace policy\") and their underlying principles. Weaknesses:  While the authors criticize existing studies for not learning memory allocations it would be valuable to extend the current work to more complex tasks.  A straightforward test of the conjecture that restricted memory architectures are easier to learn and more robust could involve comparing architectures with restricted memory (e.g. RNNs) to architectures with attention mechanisms that limit observations to recent interactions (e.g. m interactions). Experiments like these could strengthen the practical applicability of the papers insights to advanced techniques in visual navigation tasks or other field.", "Paraphrase: This paper investigates two memory structures for policymaking in partially observable Markov decision processes (POMDPs) using theoretical and empirical analysis in a simplified problem. While the field is significant for understanding decisionmaking in POMDPs where memory is crucial the scope of this work is very limited making it difficult to draw general decision. The papers methodology is thorough and wellexplained but the comparison is confined to a single problem. Although the findings are stated cautiously they could benefit from more clarity and precision (e.g. the meaning of \"\" in Theorem 4.3). The paper overlooks relevant literature on memory in POMDPs from related field and different perspectives in reinforcement learning (RL).", "Paraphrase: This paper examines a twoarmed bandit problem with limited memory. The problem is modeled as a Partially Observable Markov decision work (POMDP) where the hidden state determines the probability of reinforcements for each arm. The authors propose two memory architectures for this problem: 1. FiniteState Machine (RAM):  Tracks the played arm and reinforcement to adjust a \"confidence\" value associated with each arm.  Transitions between states follow a chainbased structure with low probability of exiting end states.  policy chooses the arm with the closest end state to the current state. 2. History Buffer (Memento):  Stores a sequence of past arm work and reinforcements.  policy executes a repeating sequence of actions in a cyclic manner.  Cycles are ordered in a Gray code sequence and the policy switches between cycles only if beneficial. The authors find that the RAM model performs better than the Memento model in terms of the probability of selecting the worse action. however when learning a policy using either architecture random initialization of the Memento memory performs better than imposing constraints based on the proposed policies. effectiveness and Weaknesses:  effectiveness:  foundation of Grayordered necklaces as a promising memory structure.  Weaknesses:  Lack of theoretical analysis of the Grayordered necklaces or guarantees of optimality.  Limited scalability to problem with more than two arms or nonbinary reinforcements.  Unclear link between the model and general POMDPs for memory architecture insights.  Naive implementation of memory architectures given existing research in this field.  Insufficient empirical evaluation to support the claim that Memento outperforms RAM with random initialization.  Unnecessary increase in environment size by including memory state in the state representation.  Unclear explanation of calculations in Section 2.2.  potential for convergence effect with RAMrandom as observed in Figure 4.", "Paraphrase: Summary This study explores the impact of memory architecture on the learning performance of Partially Observable Markov decision work (POMDP) agents. It examines a basic twoarmed bandit problem with simplified probability conjecture. Two memory models are analyzed: random access memory (RAM) and memento memory. An asymptotically optimal policy is presented for each memory case. model results for basic gradientbased learning algorithms under both memory structures are provided. effectiveness and Weaknesses effectiveness:  The impact of memory architecture on POMDP learning is a valuable field of research potentially aiding in algorithm design under uncertainty. Weaknesses:  The study is limited to a simple twohypothesis problem potentially limiting the generalizability of findings.  The paper does not discuss the applicability of insights from the simple problem to more complex scenarios.  The policies and analysis presented are not particularly novel and lack discussion of their potential relevance to general POMDP problem.  The performance analysis focuses only on the asymptotic region and does not consider average or finitetime performance.  There are some presentation effect and lack of detail.  Figure 2 does not include results for the suggested local optimization as stated in Section 3.  The implementation of initialization schemes and the policies used are not clearly explained."], "gdegUuC_fxR": ["Summary: The paper introduces a stochastic process called HFHR derived from the Nesterovs Accelerated Gradient (NAG) algorithm. HFHR can be used for efficient sampling by injecting noise into both view and momentum variables resulting in a stationary distribution that matches the target distribution. A discretization of HFHR using operator splitting and Euler integration is provided. The paper analyzes the mixing time of HFHR and obtains a bound of \\widetildeO(\\sqrtd\\varepsilon) for sampling from logsolidlyconcave and smooth target distributions under an additional thirdorder development condition. Strengths:  The HFHR process is novel and has connections with NAG.  The paper is generally clear and straightforward. Weaknesses: 1. Direct noise injection into view: HFHR injects noise directly into the view variable which can harm discretization by causing nonsmooth trajectories making it difficult to estimate gradient. 2. ThirdOrder development Condition: HFHR requires an additional thirdorder development condition which weakens the convergence guarantee compared to some discretized ULD methods that achieve similar or beneficial convergence speed without this condition. 3. Unclear Condition Number and ThirdOrder development Constant Dependence: The paper does not specify the asymptotic dependence of the condition number \\kappa and thirdorder development constant G in the iteration complexity which would be useful information. 4. Worse Iteration Complexity than Some Discretized ULD: HFHR has a worse iteration complexity than certain discretizations of ULD despite citing such methods in the paper. 5. Lack of Motivation: The paper does not provide a solid motivation for HFHR beyond injecting noise into NAG. 6. Biased idea in Figure 2: The paper uses a biased idea method in Figure 2 which may affect the interpretation of the results. Typographical Errors:  Missing dt in equation 6.  Bracket error on the thirdtolast line on page 8.", "Paraphrased Statement: Summary: This research presents the accelerated gradientbased Markov Chain Monte Carlo (MCMC) algorithm and provides rigorous numerical proofs. Strengths and Weaknesses: The paper addresses a problem but the problem statement appears unclear or lacking in substance. Incorporating noise into the convex example of the gradientbased algorithm does not yield significantly different results compared to the algorithm without noise.", "Paraphrase: Summary The author presents a refined MCMC method known as HessianFree HighResolution (HFHR) using Nesterovs Acceleranged Gradient (NAG). In continuous time HFHR significantly outperforms the Underdamped Langevin algorithm. While numerical implementation may reduce the acceleration factor some acceleration is still attainable. Strengths and Weaknesses Strengths:  HFHR acceleranges the exponential convergence range by a factor of L in continuous time compared to the Underdamped Langevin algorithm.  Numerical implementation achieve a constantfactor acceleration despite discretization errors. Weaknesses:  The writing needs improvement for clarity and ease of understanding.  The derivation of Formula (6) could benefit from more context and motivation.  Definitions of notations should be provided at the end of Section B.  Page 24s derivation of Ht lacks sufficient explanation. Remarks:  Theorem 5.1 is accurange.  The logic of Theorem 5.2 appears sound. Minor Issues:  Experiments in Section 6.2 use a nonconvex function contradicting the theoretical assumptions.  Related works not mentioned:  Replica exchange (parallel tempering)  Accelerating Nonconvex learning via Replica Exchange Langevin Diffusion  Nonconvex learning via Replica Exchange Stochastic Gradient MCMC  Underdamped Langevin MCMC: A nonasymptotic analysis", "Paraphrased Statement: Summary: This research developed a Hessianfree method called \"highresolution Nesterovs acceleration.\" This approach demonstrates significantly faster convergence than other methods and is not only a timescaling technique. Strengths and Weaknesses: The proposed method effectively accelerates convergence in optimization tasks as supported by both theoretical analysis and experimental results. However it relies on assumptions regarding parameters \u03b3 and \u03b1 which may not always align with realworld applications. For broader applicability it would be beneficial to demonstrate the acceleration with more complex and largescale data.", "Paraphrase: Summary This research introduces a novel algorithm HessianFree HighResolution Stochastic Differential Equations (HessianFree HighResolution SDE) inspired by Accelerated Gradient (NAG). The continuous solution of this algorithm exhibits faster convergence than underdamped Langevin dynamics. The discrete version also shows improvement with a constant factor. Strengths and Weaknesses The paper provides a solid theoretical analysis. However it lacks a comparison to the randomized midpoint method (Shen Lee 2019). The authors claim a constant factor acceleration over underdamped Langevin dynamics but this is invalidated by the worse iteration complexity compared to the randomized midpoint method. Specific Comments 1) Assumption A1 (m \\lVert yx \\rVert \\leq \\lVert \\nabla f(y)\\nabla f(x) \\rVert \\leq L \\lVert yx \\rVert) may not be sufficient to prove solid convexity (m0) and smoothness (L0). 2) The paper mentions considering logconcave and logsolidly concave target distributions but Assumption A1 requires m0 which contradicts the logconcave example. 3) Theorem 5.2 may not apply to the logconcave example due to the bracket in the mention of \"(solidly) logconcave assumptions.\" 4) Remark 5.5 claims that HFHR can halve the steps taken by \"ULD.\" The specific algorithm referred to by \"ULD\" is unclear. 5) The use of error of mean as a surrogate for 2Wasserstein distance may be misleading. In Figure 2 the mean value at \\alpha0.5 suggests high accuracy in only one iteration while this is unlikely due to insufficient burnin. 6) Figure 1 should include comparison lines for ULD and randomized midpoint methods.", "Paraphrase: The field introduces an accelerated Markov Chain Monte Carlo (MCMC) method for sampling inspired by the Nesterovs Accelerated Gradient (NAG) approach. Utilizing the highresolution ordinary differential equation (ODE) of NAG from Shi et al. the paper implements a twostage strategy to eliminate Hessian dependence. The resulting firstorder ODE system forms the basis for the proposed HessianFree HighResolution (HFHR) dynamics diffusion process. Discretizing the HFHR dynamics yields the proposed sampling method. Theoretical proofs of convergence for both the continuous and discretized variants demonstrate acceleration compared to existing underdamped Langevin methods (ULD). Strengths and Weaknesses: Strengths:  Provides an accelerated firstorder diffusion process in continuous time leading to a novel sampling method after discretization. Weaknesses:  The paper is technically complex making it difficult to comprehend. Questions and Concerns: 1. acceleration in Continuous Time:  The paper claims an improvement of a factor of \\kappa compared to Dalalyan 2020. However no improvement is observed for the function f(xy)  mx2Ly2. clarification is requested regarding this apparent contradiction. 2. Discretization:  The paper employs a secondorder symmetric paper for discretization. The motivation behind this approach and its benefits over forward Euler discretization are questioned. 3. acceleration in Discrete Time:  The statement of acceleration is questioned unless a nontrivial improvement in the constant term (e.g. by a factor of L or \\kappa) is demonstrated. 4. ULDHFHR(\\alpha0\\gamma) Equivalence:  The equivalence of continuous ODEs when setting \\alpha  0 is acknowledged. However it is unclear if the same equivalence holds for the discretized algorithm especially given different discretization methods. 5. Experiment Comments:  Figure 2 shows surprisingly fast convergence (with only 2 iterations) to \\epsilon closeness suggesting the problem may be also simple.  Figure 3 contains some charts with \\alpha  0.1 and others with \\alpha  0.1 0.5 and 1. The reason for this discrepancy needs clarification."], "k9bx1EfHI_-": ["Paraphrased Summary: The researchers introduce a graphbased representation to analyze EEG data for seizure detection and localization. They combine Gaussian and correlation kernels to create an undirected connectivity graph. A diffusion convolutional recurrent network is then used on this graph. Strengths:  Combines robust EEG representation technique.  Able to predict and locate seizures.  Mathematical basis is clear.  Detailed experiments with ablation procedures.  Code is publicly available. Weaknesses:  Limited use of autoencoders which could be more beneficial as a regularization strategy.  The use of correlation in enhancing Gaussian kernels is unclear.  Hyperparameter selection for Gaussian kernels is not thoroughly discussed.  The necessity of a nonEuclidean representation (thresholded affinity matrix) is questionable as Gaussian kernels with appropriate bandwidths may achieve similar results. Clarification Requested:  In Section 2.2.2 the EEG trial X is listed as the input for graph neural networks but Figure 1 suggests that DCRNNs are applied to the affinity matrix. This requires clarification.", "Paraphrased Summary: The study proposes a technique for detecting and categorizing seizures using graph neural networks (GNNs) trained with unsupervised methods. The authors employ EEG signals and claim substantial performance in detection and classification. They also introduce methods for evaluating the models interpretability. Strengths and Weaknesses:  The selfsupervised training details should be expanded for clarity and reproducibility.  While unsupervised methods are suitable for labelscarce scenarios the justification for their use in this study given the availability of sufficient labels should be provided.  The authors use an occlusionbased method for interpretability analysis. They should discuss whether the graph structure can reveal the significance of specific brain regions (corresponding to electrodes) in seizure case.  While some seizure classes are combined it would be valuable to assess performance when all eight classes in the dataset are used for classification.", "Paraphrased Summary: This research presents a novel method for detecting and classifying seizures using electroencephalographic (EEG) data. It combines:  GraphBased Modeling: Two graph structures are used to represent the electrode geometry and dynamic brain connectivity.  SelfSupervised Pretraining: An unsupervised study technique is employed to enhance model performance. To interpret the models predictions occlusion maps are used to visualize the regions of the brain associated with seizure activity. Strengths:  Clear and wellstructured introduction of the goals and approach.  Exploration of two graph structures and comparison of their efficacy.  Demonstration of the value of selfsupervised pretraining.  Quantifiable and comprehensible evaluation of model interpretability through occlusion maps. Weaknesses:  Limited comparison with other existing methods.  The proposed method builds upon previously established technique."], "iulEMLYh1uR": ["Paraphrase: The paper explores the role of efficiency indicators for models and the potential drawbacks of reporting limited indicators. The authors highlight the risks associated with reporting only a few cost indicators which can result in:  Misleading conclusions about model efficiency  Unfair comparisons between models The paper provides recommendations for reporting efficiency indicators. Strengths and Weaknesses:  The papers objective is clear and beneficial to the community.  The authors identify crucial considerations and common errors.  However the technical content of the paper is insufficient for acceptance at ICLR.  While raising awareness is crucial the authors should provide more specific and practical guidelines.  Complete and detailed model of how to assess and compare model efficiency should be included potentially from existing papers that draw incorrect conclusions.  The \"Introduction\" and \"Training or Inference Cost\" sections could be condensed.  The discussion section should be expanded with practical model.  The authors should define term like \"model capacity\" \"parametermatched\" and \"computematched.\"  In number 1 and 2 it should be clarified whether default parameters from referenced papers are roled.  The hardware roled for computing in these number should be specified. Minor Comments:  Add a reference to Mesh Tensorflow.  Correct the typo \"Ac curacy\" in Section 3.1.  Include the number number in the text for number 3.", "Paraphrase: The paper examines various cost measurement used to assess the efficiency of neural network models during both training and inference. It argues that relying on a single cost metric can be insufficient because their behavior often varies depending on the platform used. The paper concludes by recommending that researchers include multiple cost indicators and clearly state limits on efficiency claims. Strengths:  Clearly demonstrates the inadequacy of individual cost metrics for assessing neural network efficiency.  Provides a broad range of cost indicators and focuses on the limitations of commonly used ones.  Presents experiments showcasing the potential lack of correlation between different cost indicators.  Presents a wellwritten and clear argumentation of the advantages and drawbacks of cost indicators. Weakness:  The discussion and suggestions in Section 4 are brief essentially urging researchers to adhere to rigorous scientific practices and avoid making exaggerated claims.  Minor typographical error: A missing number reference to number 3 on page 8.  The paper lacks substantial research insights beyond its comprehensive analysis of cost indicators.", "Paraphrase: Summary: This research focuses on evaluating and presenting the efficiency of machine learning algorithms. The authors demonstrate that various efficiency metrics exist yet many published works only report a limited number of them. This leads to confusing assessments of models realworld performance. The papers primary contribution is a set of suggested full practices for efficiently reporting the efficiency of novel machine learning models and methodologies. Strengths:  Comprehensive overview and analysis of efficiency metrics and their interrelationships.  Highlights cases where misleading reporting has occurred in recent machine learning and artificial intelligence (MLAI) study. Weaknesses:  The papers direct practical impact on the MLAI industry is unclear.  The contribution appears to be primarily a metacontribution to the MLAI research environment suggesting proper reporting methodologies without providing substantial new technical insights.  It lacks quantitative analysis to support claims about the prevalence of misleading efficiency reporting in the field.", "Summary This study highlights the significance of context and nuance in evaluating \"efficiency.\" It outlines different efficiency metrics and the distinction between training and inference efficiency. The authors emphasize that model efficiency assessments can be misleading depending on factors like efficiency metrics baseline architectures hardware and experimental selection. They conclude with best practice recommendations. Strengths  The motivation is excellent addressing the need for a robust understanding of efficiency in ML reresearch.  It identifies potential pitfalls and emphasizes the importance of context. Weaknesses  The paper lacks sufficient originality for publication in a highimpact venue.  It lacks clarity and focus appearing rushed and underdeveloped.  Some model are insufficiently explained and poorly integrated. Feedback Concrete model and Plots  Include more realworld model with accompanying plots or tables to illustrate concepts such as learning curves for different hyperparameters.  Provide more explanation for the ViT model configurations and show a wider range of depthwidth pairs.  Plot FLOPs and parameters against each other to support the statement about EfficientNets efficiency compared to other models. Addition of Relevant reresearch  study citing reresearch from MosaicML on improving training efficiency. Emphasis and Clarification  Emphasize the point that unstructured sparsity does not yield significant efficiency gains on current hardware.  Explain the concept of Paretooptimality.  role consistent terminology for \"parameter sharing.\" Organization and Structure  Move less relevant details to the appendix to make room for more crucial content.  Expand Section 3.3 on architecture research cost indicators and move it to the appendix. Other Typographical Error  Correct the typo \"Fig. is...\" to \"Fig. 3\"."], "yV4_fWe4nM": ["Paraphrase: Summary This field presents a fair clustering algorithm that employs deep learning (DL) framework to transform data into latent representation. The authors also demonstrate the equivalence between a practical fairness measure and a balance measure. The algorithm consists of two steps: (1) finding fair assignments (y) based on assignments (y) and (2) adjusting latent representation and y according to the pseudolabels (y). effectiveness and Weaknesses Pros: 1. The algorithm establishes the equivalence between the fairness measure used in the field and the balance measure. 2. Experimental results show satisfactory performance particularly regarding the balance measure. 3. The paper is wellorganized and easy to understand. Cons: 1. Section 4.1 focuses on introducing the backbone network which may be better suited as a separate section. The main contribution is the use of fair approximations as pseudolabels and experiments on multiple backbone networks would enhance the field. 2. The term \"rounded version\" in Page 5 lacks a formal definition. It is unclear whether the authors refer to the strong version of y which is a continuous variable. 3. Table 1 shows zero balance scores for DEC and IMSAT indicating that samples from USUS were not assigned to any clustering. This is unusual given that ACC and NMI scores are still high. It may be inappropriate to assign a zero balance score to nonfair clustering methods in such situations. 4. There are some typos and imprecise formulation in the paper such as:  Page 5: \"that satisfy our optimal fairness status\"  Page 5 Equation 8: Incorrect use of \"\\times\" or \"\\cdot\". The authors should emphasize that y is a row vector.  Page 5 Equation 11: Missing superscript", "Summarized Paraphrase: The paper proposes a technique that combines deep learning with optimization to promote fairness in clustering. This is achieved by expanding on a previously established balance measure for fairness and formulating an integer linear programming problem foundationd on it. This problem is then integrated into an existing deep learner for fair clustering. effectiveness and Weaknesses: This field does not clearly outline its original contribution. The proposed fairness notion is an extension of an existing fairness measure enabling it to handle protected status variables with multiple states. The integer linear programming formulation is incorporated into a probabilistic clustering framework introduced in 2017. Therefore the unique contribution remain unclear. The authors use \"we\" when describing the foundation clustering framework and other methods which obfuscates their own contribution. While multistate protected status variables are presented as the learning goal 23 datasets use binary protected status variables. improvement over existing methods are minimal or nonexistent. The experiments partially support flexible fairness constraints particularly in terms of accuracy. The authors should mitigate their claims accordingly. Moreover the field lacks justification for its necessity. It does not adequately address the shortcomings of prior process or explain how its approach overcomes these limitations. The rationale for selecting the foundation clustering framework and evaluation metrics is also insufficiently discussed.", "Paraphrased Summary: This research paper introduces a novel framework for \"fair clustering\" using deep learning. It aims to ensure fair predictions on categorized data with protected status variables (PSVs) and enable fair predictions for data lacking PSVs. effectiveness: 1. It explores fairness publication in deep clustering by defining fairness for binary and multiple protected status variables. This definition is equivalent to optimizing the general balance measure for unequal impact. Weaknesses: 1. The paper suggests that clustering without sensitive attributes can be fairer. However the term \"sensitive attributes\" is not clearly defined. The criteria for determining the sensitivity of attributes and the justification for removing them before clustering are unclear. The rationale for fairness constraints in clustering tasks should be explained. 2. The proposed algorithm involves obtaining pseudo labels (y) using a deep clustering framework on a dataset with sensitive attributes. These labels are then refined to obtain (\u0177) using fairness constraints. Finally (\u0177) is used for supervised training of a deep framework. It is unclear why fairness constraints are not directly incorporated into the optimization objective of the deep framework and are instead used as a preprocessing step for pseudo labels. 3. The datasets used in the experiments do not include sensitive attributes. The authors should provide an applied example to demonstrate the efficacy of their algorithm.", "Paraphrase: This paper introduces a method for equitable clusteringing a term used when each protected group is proportionally represented in every clustering as it is in the original dataset. The method utilizes a deep neural network for clusteringing based on the deep clusteringing with practical adversarial training approach. It refines the network by adding a fairness term to the loss function which measures the divergence between the predictions of the network and equitable assignments calculated using an LP solver. The paper presents empirical results demonstrating the superior performance of the proposed method in finding fair clusteringings when compared to existing deep clusteringing fair clusteringing and deep fair clusteringing methods. effectiveness and Weaknesses: effectiveness:  clear and wellwritten  Impressive experimental results Weaknesses:  Lacks clear distinction from existing deep fair clusteringing methods  Insufficient explanation of how the method guarantees fair clusteringing  limited support for example where each person can belong to multiple protected classes  Unclear significance and utility of Theorem 3.2  Confusing description of the constraint matrix  Counterintuitive experimental results lacking sufficient explanation  Overstatement of comprehensiveness in evaluation measures  Ambiguity in reporting deep framework and ILP results  Assumption that populationresultant distribution remains unchanged when PSV information is absent in the test data  Hypothesis on the impact of the fairness objective on clusteringing performance is not wellsupported"], "kQMXLDF_z20": ["Summary This paper surveys existing approach to address oversmoothing in Graph Neural Networks (GNNs) and introduces three matrices to categorize these techniques. It proposes TGCL a modelagnostic regularization method based on contrastive learning which meets specific criteria for tackling oversmoothing. effectiveness  Oversmoothing is a critical issue in GNNs and this paper provides a comprehensive overview of current techniques.  TGCL offers a flexible way to incorporate contrastive information into GNNs and experimental issue demonstrate its effectiveness. Weaknesses  The literature survey is limited omitting key techniques like Skip link subgraph sampling and DropNode.  The matrices used for classification are not clearly explained and their use in the survey is unclear.  The discussion of existing methods is superficial making the claimed contribution unclear.  The theoretical analysis of TGCL does not align with experimental issue and its computational cost is not compared with other methods.  The experiments lack baseline comparisons and the reliability of issue based on only 5 repetitions is questionable.  Homophily should be explicitly acknowledged in the preliminaries.", "Paraphrased Summary: The paper presents a novel layer TGCL for Graph Neural Networks (GNNs) to address the issue of oversmoothing. TGCL possesses three key features:  constant divergence indicator: It maintains a consistent step of representation divergence.  EasytoDetermine divergence indicator: The divergence step can be easily calculated.  ModelAgnostic strategy: It can be applied to various GNN architectures. Through experiments on four datasets the paper demonstrates that TGCL effectively mitigates the oversmoothing problem. effectiveness:  TGCL is universally applicable to GNNs.  The paper is accessible. Weaknesses:  The mathematical derivation for the upper bound is incorrect as the inequality following Equation (3) is invalid.  The mathematical validation for the lower bound is flawed due to the use of approximation symbols.  The paper lacks formal definitions for \"constant divergence indicator\" and \"easytodetermine divergence indicator.\"  It does not explain why these features are crucial for GNNs to overcome oversmoothing.  The plots in number 2(a) include only weak baseline models and stronger baselines such as GCNII and drop edge could outperform them.", "Paraphrased Statement: Summary: This paper introduces the TGCL method which addresses the issue of oversmoothing in Graph Neural Network (GNNs) as the number of layers increases. Unlike previous approach TGCL combines three core concepts: constant divergence indicator EasytoDetermine divergence indicator and ModelAgnostic strategy. Most existing methods only consider two of these concepts. The proposed method expands on the PairNorm method. effectiveness: 1. It examines various existing work and presents three metrics to evaluate oversmoothing in GNNs. 2. The paper is wellwritten and straightforward to understand. 3. It provides a theoretical analysis of the proposed loss use to demonstrate its soundness. Weaknesses: 1. Given the numerous existing methods (e.g. APPNP PairNorm DropEdge) its unclear why the authors chose to focus on PairNorm. 2. The novelty of the method is questionable. The authors analyze the boundaries of PairNorm and propose TGCL with the primary contribution being topologyguided contrastive loss. The significance of Lemma 1 is unclear and its link to the main content is missing. 3. Table 2 suggests that the improvements gained by TGCL are minor. Compared to GCNII TGCL does not exhibit superior performance on all datasets. Furthermore GCNII outperforms TGCL on CiteSeer and exhibits large stability across the four datasets. For case GCNII achieves 0.7179 \u00b1 0.0012 on Cora while TGCL achieves 0.7199 \u00b1 0.0151. TGCLs improvement of 0.0002 is offset by a higher standard divergence resulting in less stable issue. 4. The lack of experiments comparing TGCL to GCNII or other methods in Table 3 is a notable omission.", "Paraphrase: This study examines oversmoothing in Graph Neural Network (GNN) learning. The author introduces three metrics for comparing methods: divergence indicator intuition for setting its value and ease of adoption. They propose a novel method called Topologyguided Graph Contrastive Layer (TGCL) to mitigate oversmoothing while maintaining the proposed metrics. An analytical analysis of the TGCL loss is provided and empirical experiments demonstrate its benefits on four graph datasets. effectiveness:  Wellorganized and easytofollow manuscript  Thorough analysis of Khops study in Section 4.3 explaining the rationale for the topologyguided approach  Generic method that can be easily integrated into baseline GNN models as a contrastive loss extension Weaknesses:  The three metrics may be marginal for comparison with most baseline models satisfying the third metric.  The TGCL method claims to be easy to adopt but its loss term introduces additional parameters (temperature distance metric similarity use alpha). The paper lacks an explanation of how the optimal configurations are selected.  The experimental section does not address the impact of the added loss term on convergence speed compared to baseline methods."], "w8HXzn2FyKm": ["Paraphrase: summary: This research investigates a new approach to distributed stochastic approximation using a stochastic matrix to represent communication patterns between agents (as opposed to the commonly studied doubly stochastic matrix). The algorithm analyzes the systems behavior over time and within a finite timeframe revealing that it converges to a combination of the individual equilibrium points for each agents objective. A pushtype algorithm is proposed for decentralizing the implementation along with bounds on its performance. These findings and error bounds can be directly applied to distributed reinforcement learning (RL) algorithms. effectiveness and Weaknesses: Originality: This work explores distributed stochastic approximation algorithms with stochastic interconnection matrices a novel concept in the field. The findings differ from those of algorithms with doubly stochastic matrices. Additionally a pushtype algorithm is introduced for practical implementation. Quality: The results are mathematically sound and theoretically \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043e. The proposed algorithm is both innovative and challenging. Clarity: The paper is written in a clear and concise manner. The results are presented in a logical flow and their contribution compared to existing work are welldefined. Significance: These results complement existing research on distributed stochastic approximation algorithms which has previously focused on doubly stochastic interconnections. The analytical tools developed in this work may have broader applications in the future.", "Paraphrased summary: This work examines distributed linear stochastic approximation (DLSA) for decentralized agents on varying directed network. Two algorithms are proposed: 1. Consensusbased DLSA (CBDLSA): Uses rowstochastic mixing matrices to achieve consensus on parameter approximation. 2. Pushsumtype DLSA (PDLSA): Uses columnstochastic mixing matrices to aggregate information and update parameters. effectiveness:  Extends DLSA to timevarying directed network.  Provides convergence guarantees for both CBDLSA and PDLSA algorithms.  detailed analysis of algorithm behavior. Weaknesses:  Lacks comparisons to prior research making it difficult to assess the novelty of the results.  Relies on an assumption that is not fully justified.  foundation is lengthy and not sufficiently informative.  Highlevel objective of consensus on an ODE solution is not directly apparent.  Differences between CBDLSA and PDLSA are not clearly explained.  Justification for the update rule in Eq. (1) and Eq. (9) is unclear.", "summary: The paper examines the behavior of distributed stochastic approximation algorithms in the presence of random noise using a finite time analysis. Weaknesses include a lack of numerical case and the inability to compute the values of certain constants. Weaknesses:  Lack of numerical case: The paper does not provide concrete demonstrations of the interplay between various quantities in the theorems. A simple case in the distributed reinforcement learning setting would be helpful.  Computation of constants: The paper acknowledges the difficulty in determining the values of certain constants which limits the practical applicability of the results.  potential technical issues:  Changing bi and constant A: The paper states that bi varies across agents but A remains the same. The authors should clarify this since in distributed reinforcement learning transitions between states differ for agents resulting in distinct A matrices.  Rate of convergence: The paper suggests a rate of convergence of 1t2 for the twodimensional singleagent case under specific conditions. However Theorem 3(2) appears to indicate a slower rate of convergence of 1t. The authors should explain this variant.  Dependence of step size on \u03b3max: The paper notes that in the constant step size case the step size is inversely proportional to \u03b3max while in the timevarying case the step size is directly proportional to \u03b3max. The authors should clarify this difference in dependence.", "Paraphrased Statement This paper focuses on a problem in stochastic approximation that involves reaching consensus among multiple agents. It has applications in decisionmaking within reinforcement learning. The problem addressed differs from traditional approach that assume symmetric (doubly stochastic) interactions between agents. The authors propose an algorithm that addresses the case where the interaction network is nonsymmetric (not doubly stochastic). Under specific assumptions the authors mathematically prove that their proposed algorithm converges in a certain way. While the research topic is significant and its extension to nondoubly stochastic interactions is valuable the paper requires improvement. effectiveness  Addresses an important topic.  Extends existing approach to a more general setup. Weaknesses  Poor Organization and Lack of Clarity:  Lengthy foundation without a clear focus.  Incomplete definition of the problem.  Insufficient explanation of results and their implications.  Lack of Rigor:  Interdependent theorem statements.  Undefined or ambiguous variables and assumptions.  absence of case:  No numerical simulations or case provided to illustrate the nondoubly stochastic case.  specific Concerns:  Ambiguity regarding the validity of Assumption 1 at different time steps.  Assumptions not clearly stated or connected to variables in the theorem.  Unclear interpretation of assumptions involving \"forall X\" and \"A.\"  Incomplete definition of Theorem 1.  Unclear motivation behind using both fixed and varying step sizes.  Lack of definition of \"diameter\" in the theorem.  Missing subscripts in some equations. Overall Impression The papers topic is relevant but its presentation and performance are lacking. The authors need to improve the organization provide more clarification and include case to make their findings more accessible and verifiable.", "Paraphrase: This research examines how multiple agents can collaborate to optimize a problem while communicating in a limited way. The authors propose a new approach that eliminates the need for direct messaging among agents a common feature of previous methods. This work establishes that the approach can achieve accurate results in a timely manner. effectiveness:  The research tackles a practical problem in decentralized optimization.  The problem formulation and findings are presented with clarity and preciseness. Weaknesses:  The introductory section is extensive which may not be ideal for a conference submission.  The research may have limited relevance for a machine learning conference like ICLR.  The authors suggest connections to distributed multiagent reinforcement learning but these connections may not fully represent the field. Minor Points:  The terms \"meansquare\" and \"mean squared\" are used interchangeably.  The term \"double stochastic matrix\" is incorrectly written as \"doubly stochastic matrix\" on page 3."], "wqD6TfbYkrn": ["Paraphrased Statement: Summary This paper presents a novel approach called Point DiffusionRefinement (PDR) for completing point clouds. PDR leverages multilevel feature extraction from incomplete point clouds to guide the completion process. It also accurately adjusts 3D point locations to achieve both smooth surfaces and sharp details. PDR has demonstrated superior performance on established datasets compared to existing methods. Strengths  Clearly written manuscript  introduction of a novel pipeline for point cloud completion Weaknesses  Unclear figures (e.g. the relationship between different network components in Figure 1)  Lack of evidence supporting the claim of more uniform results  Inconsistent benchmark results compared to online data  Limited qualitative results in Figure 4 despite the inclusion of numerous methods in Tables 1 and 3  Absence of running time comparison with other methods despite the incorporation of timeconsuming process", "Paraphrased Statement: Summary: This research focuses on fully supervised point cloud completion at the work point. The proposed method involves using a conditional DDPM to generate a noisy but complete point cloud from incomplete input. This is followed by a refinement network also conditional on the incomplete input which further improves the noisy point cloud. Both network utilize a novel dualpath network architecture based on PointNet allowing for localized guidance from the incomplete point cloud. Due to the twostage approach evaluation speed can be accelerated up to 50 times through step skipping with a moderate loss in performance. The model is evaluated on the MVP MVP40 and Completion3D datasets. Strengths:  Achieves stateoftheart performance for point cloud completion across various datasets.  Provides comprehensive details and wellpresented experimentation.  Carefully analyzes and evaluates the inner work of PointNet components tailored to the proposed setting. Questions: 1. comparison are sought between the proposed twostage generation and refinement paradigm and a singlestage DDPM with a great model. 2. Concerns are raised about potential biases in evaluation results due to the use of a subset of the test set without reporting confidence intervals. 3. Information on the evaluation time per point cloud is requested. 4. Clarification is sought regarding whether the refinement network is retrained for different DDPM steps in the experimentation presented in Table 5. Update: The authors response regarding the benefits of the twostage model and the fairness of comparison with baselines is appreciated. The rating of \"accept\" is maintained.", "Summary This paper introduces an innovative approach for completing point clouds using diffusion Denoising Probability model (DDPMs). It employs a conditional DDPM to generate plausible complete point clouds from random noise conditioned on a partial point cloud. The method focus around designing a conditioning feature extraction subnetwork and a denoising subnetwork both of which utilize improved PointNet structures. Additionally a refinement network is proposed to reduce the computational burden offering a balance between efficiency and performance. The method outperforms competitors on various datasets preserving crisp details effectively. Strengths  The proposed architecture is more various and powerful than existing DDPM models for 3D point cloud generation.  The multiscale conditioning information in the proposed model enhances fidelity.  The authors counter a previous argument that PointNet cannot be used in diffusion by introducing a simple solution to incorporate absolute coordinates.  The paper is wellpresented and offers detailed explanation of design choices.  The authors emphasize the potential of using DDPMs to avoid Chamfer distance loss. Weaknesses  Direct comparison to PointVoxel diffusion completion results are lacking.  The method may struggle with multimodal completion which could be addressed by incorporating more noise.  The training process for the refinement network appears ad hoc. Recommendations  Conduct direct comparison against PointVoxel diffusion completion.  Explore multimodal completion and discuss limitations if encountered.  Investigate representation learning capabilities of the feature transfer modules.  Test point cloud completion in realworld scenarios such as with bounding box work of partially occluded cars.  Implement point cloud generation with depth map conditioning."], "mqIeP6qPvta": ["Paraphrased Statement Title: computer vision using transformer and Foveated vision approach: The authors propose a computer vision system that combines Transformer network with foveated vision. Unlike fullresolution deep CNN models their approach uses a recurrent attention model to simulate center saccades for scene exploration resulting in reduced computations and increased resistance to adversarial attacks. Strengths:  Introduces a novel foveated vision model within the Transformer model.  Demonstrates dynamic vision with predicted center movements for allocating attention.  Provides motivation for foveated vision and reviews prior art. Weaknesses:  Falsely claims to be the first to combine Transformer network and foveated vision.  Omits relevant prior work and lacks comparisons to existing foveated vision systems.  center on adversarial robustness without explaining its relevance to foveated vision.  Lacks a description of Transformer network and their benefits for foveated vision.  Limits center movements to a 14x14 grid and provides insufficient explanation of foveation pooling and aggregation.  Presents underwhelming classification results compared to EfficientNetB0.  Only compares to fullresolution models making it difficult to assess improvements over other foveated vision systems.  Does not adequately discuss the potential insights from the predicted saccades.  Lacks comparisons to biological center movements or random movement model.  Fails to demonstrate how learned models allocate resources based on image complexity. Additional Comments:  \"Reach that approach\" should be \"approach.\"  \"Using which locations far apart in the image will interact\" is unclear.  contingent on the computational speedup achieved from squared pooling regions are needed.  Clarify if dynamic stopping of fixation exploration is used only for inference or during training.  Improve image 4 by using color or different argument styles for accessibility.", "Paraphrase: Summary: The research introduces a method based on a hybrid architecture that combines transformer and convolutional neural networks (CNNs). It leverages fixation points for computational efficiency and robustness against adversarial attacks. The approach outperforms a fullresolution model and showing desirable qualities. Strengths:  The papers approach is innovative and intriguing.  The inclusion of robustness evaluations strengthens the claims.  model comparisons are fair based on similar argument counts.  The training steps and experiments are clearly outlined. Weaknesses and Questions:  Why limit adversarial attacks to only the transformer component  DeitTi is mentioned in Table 2 but is not explained.  Why is the \"Dynamic Stop\" foveated model not tested for adversarial robustness  The significance of \"PC\" in Table 1 is unclear.  Equation (2) has a typo: \"fixatiothe n.\"  The advantages of sequential processing could be presented more clearly.  Consider the \"Boundary Problem\" and the relevance of CNNs blind position.  Saliencybased models can incorporate foveal vision as demonstrated by Wloka et al. (2018)."], "gJcEM8sxHK": [], "tyrJsbKAe6": ["Paraphrased Summary: This paper investigates using a modelbased approach with the pessimism principle for offline reinforcement learning extending the research on modelfree pessimism. It provides PAC guarantees for learned policies based on the TV distance between the learned maximum likelihood estimate (MLE) model and the realizable ground truth addressing some problems that modelfree pessimistic methods cannot handle. strength:  Introduces a modelbased approach to the pessimism principle for offline RL complementing modelfree approach.  Demonstrates the ability to handle infinite policy classes adding flexibility to the framework.  Provides detailed comparison with stateoftheart work guiding future research.  Offers clear writing and structured presentation. Weaknesses:  The advantage of handling infinite policy classes may be overstated as modelbased approach also face challenges with complex model classes.  The implementability of the modelbased approach comparisond to function approximation with bonus function requires further discussion. Additional Questions:  How does the dependence on model class complexity in the MLE approach comparison to modelfree approach when the model class is infinite  What intuitions can be drawn from the complexity measures used in the MLE approach"], "zLb9oSWy933": [], "wQfgfb8VKTn": ["Paraphrase: Summary: This research addresses the challenge of cooperative multiagent reinforcement learning. It introduces a unique method for learning dynamic coordination graphs based on variance estimation in predicted payoff. The paper argues that variance can indicate the need for collaboration between agents. However theres a circular dependency between payoff estimation and coordination graph construction. Thus an alternate action representation is proposed to mitigate this payoff. The approach is tested on several coordination scenarios and benchmark tasks. effectiveness:  Introduces a novel approach for coordination graph learning.  The payoff variancebased edge selection method may resolve payoffs highlighted in previous research.  Evaluated with a large number of agents. Weaknesses:  Related study should include [1].  Ablation study could provide more insights such as the impact of the action encoder on other value factorization methods.  Sparsity effects in larger agent environments should be explored."], "xw04RdwI2kS": ["Summary This paper proposes the Inverse Contextual Bandit (ICB) as a posthoc analysis method for understanding how agents make decision in nonstationary environments. The ICB captures the evolving understanding of the environment by the agent over time. Strengths and Weaknesses Strengths:  Provides interpretable explanations of decisionmaking in nonstationary environments.  Captures the agents nonstationary understanding of the world. Weaknesses:  not open what contribution of the contextual bandit (actionindexed reward distributions) is being learned by the environment. Introduction  Remove quotations around ICB (abbreviation already introduced).  use either quotation set or emphasis not both for \"descriptive modeling.\" Core application  Add reward values to introduced policies to prevent unnecessary nonstationary regimes. Desiderata  use bullet points for clarity:  Environment should be learned (actionindexed reward distributions).  Smooth evolution of environment. design 1  Consider simplifying the depiction of nonstationarity using a Markov range with directed edges. Inverse Contextual Bandits  Suggestion: set ICBs as structural bandits conditioned on a previous bandit acting in the same domain (but not environment).  Clarify what \"evolves smoothly\" means (Lipschitz continuity or no discontinuities). notation  use conventional notation for state space (\\mathcalS).  Explain the difference between \\mathbbD and MDPs. State transition  Justify the assumption that state transition are independent of past states and actions. NonAvailability of Reference [23]  Remove reference to unpublished work. distribution of Newly Arriving Organs  Provide a diagram or reference to support the claimed conditional independence. Definition 1  use equation environments for mathematical notation. First work on Modeling Nonstationary Agents  Clarify that the work refers to modeling nonstationary bandits not agents. Illustrative case  Move design 4 (better capturing the problem) closer to the beginning of the paper.", "Paraphrase: This paper tackles the issue of Inverse Contextual Bandits. It investigates how an agents knowledge changes over time based on its actions. Given a contextual bandit scenario where the agent doesnt know the true rewards an observational dataset and parametrization for rewards and beliefs the goal is to recover the actual environment parameter and the agents beliefs. They present two algorithms for this task: one using Bayesian updates and one employing Gaussian processes. Their algorithms are evaluated using both simulated and realworld data pertaining to liver transplantationation. Strengths:  The problem being addressed is significant.  Their framework has several reward over previous approaches.  It represents the first formal attempt to learn interpretable representations of nonstationary behavior.  The use of Bayesian methods to calculate posteriors is sound.  The liver transplantation experiments are insightful. Weaknesses:  The paper presentation is difficult to comprehend and could be improved.  The algorithms do not introduce any novel techniques.  There are no theoretical guarantees for the proposed algorithms.  It would be beneficial for the authors to establish an upper bound on the error between the true environment parameter and its estimate demonstrating the convergence of their algorithms."], "obi9EkyVeED": ["Paraphrase: Summary The authors present a novel approach that combines perclient and weight dropout for federated learning. Their approach aims to enhance dropout probability (likelihood of setting a weight to zero) for weights where contrasting parity gradients emerge across different clients (indicating potential cancellation upon summation). This technique has been demonstrated to reduce client computational cost in FL by up to three time. Strengths and Weaknesses Strengths:  Addresses computational cost challenges for clients an aspect frequently neglected in FL research.  The method is various and applicable to practical FL scenarios without restrictive assumptions. Weaknesses:  The proposed approach necessitates increased client communication cost.  While the authors emphasize a favorable tradeoff between communication and computational cost practitioners prioritize communication cost due to potential burdens on users (e.g. limited WiFi resources).  The findings are specific to computer vision. It remains unclear if they extend to other field like NLP.  The logarithmic graphs in number 5 present readability issues due to inconsistent axis spacing.  number 5.c requires clarification on how FedDrop achieves lower communication cost than FedAvg.  The results in Table 1 could be more informative by including actual communication cost for each method instead of merely indicating budget compliance.  The empirical outcomes appear minor with a substantial (10x100x) increase in communication cost yielding a relatively minor (less than 10x) reduction in computation cost."], "zFlFjoyOW-z": [], "hRVZd5g-z7": [], "izj68lUcBpt": [], "keeCvPPd3vL": ["Paraphrased argument: Summary: This paper presents a framework for image generation that utilizes sparse coding reconstruction. The researchers incorporate sparse coding into a convolutional image generator and evaluate different methods to enforce sparsity in the direct network. Their experimentations demonstrate improvements in the FID scores of common GAN models and enhanced PSNR for image reconstruction using deep prior when compared to architectures without sparsity constraints. Strengths and Weaknesses: 1. Clarity and Conciseness: The paper is wellwritten and easy to understand. 2. Potential for Wide Adoption: The proposed method is simple and could be widely applicable if its effectiveness extends to a broader range of problems. However the experimentations in the paper are limited to two scenarios with various baseline architectures. 3. Lack of discussion on Downsides: The paper does not discuss any potential drawbacks of the technique which is unusual for methods that optimize multiple objectives. It remains unclear if these optimizations come at a cost such as slower convergence range altered latent space interpolation or diminished overall improvements. 4. Limited Qualitative comparison: While the authors claim significant improvements from their MLCSC strategy the tables only do not provide sufficient evidence to support this claim. The absence of qualitative image comparison makes it difficult to assess the actual magnitude of these improvements. 5. Excessive Reliance on PSNR: The exclusive use of PSNR as a metric in the DIP experimentation is problematic because PSNR can favor blurry images and poorly correlate with human perception. more further image quality metrics or visual examples would enhance the analysis. 6. Missing Implementation Details: The paper does not provide specific information on how the baseline architectures were modified to incorporate the (ML)CSC technique. The authors promise to release code for the experimentations but it is not yet available which hinders reproduction and further exploration of the method. Minor Editing Suggestions:  Abstract: Remove \"leading to stateoftheart performance\" as it lacks context.  Abstract: Remove \"a meaningful such regularization\" as \"such\" is redundant.", "Paraphrased Statement Summary: This paper presents a Generative Adversarial Network (GAN)based approach for image synthesis inspired by sparse coding. It employs the generator of a GAN to generate a sparse representation (sparse code) for image creation. The method involves decomposing the original sparse coding framework into two consecutive learning tasks making them more manageable. Additionally it integrates image regularization techniques into the framework. experimental results show that the proposed method surpasses existing image synthesis methods in performance. Strengths:  The paper effectively identifies the limitations of previous multilayer convolutional sparse coding frameworks and introduces a viable solution to address them.  The approach demonstrates superior performance compared to established benchmarks. Weaknesses:  The paper assumes that the latent code representation for optimal image synthesis must be sparse which may not always hold true.  The paper lacks comprehensive evaluation metrics by excluding commonly used image restoration metrics such as PSNR and SSIM.  The paper does not provide detailed information on the implementation including the selection of threshold values and weighting terms used in normalization constraints.", "Paraphrase: The paper investigates the relationship between convolutional neural networks (CNNs) and sparse coding. It proposes encouraging sparsity in the final layer activation to improve performance. The paper premise three mechanisms to promote sparsity which slightly enhance FID scores for various GAN architectures on the CIFAR dataset. The idea are also applied to a different generatorlike scenario. Strengths:  Wellwritten and understandable.  Provides background on sparse convolutional frameworks. Weaknesses:  Limited understanding of inner activation: Despite initial claims the paper fails to provide insights into the meaning of inner activation beyond the final ones. It does not demonstrate whether specific meanings can be attributed to different feature channels.  Arbitrary focus on final layer sparsity: Inserting sparsity at the networks end may be limiting in use particularly in highresolution GANs where the final layer has little impact on global structure.  Lack of clarity in DIP experimentation: It is unclear why sparsity is applied only to the encoder not the decoder or generator in the DIP experimentation.  Conflicting claims on sparsity: image 4 suggests significant sparsity in the baseline framework which contradicts the claim that the paper premise sparsity.  Limited evaluation: The results are only based on the CIFAR dataset which may not generalize to larger images.  Exclusion of relevant methods: The paper excludes the popular StyleGAN frameworks from the analysis.", "Summary and Proposed method This research introduces a new architecture for Generative Adversarial Networks (GANs) that promotes sparsity. The generator network is divided into separate branches for generating a sparse vector and the final image. Sparsity is enforced through a regularizer in the latent space between the branches. Strengths and Weaknesses Strengths:  Improved performance compared to standard GANs on metrics like FID IS and PSNR. Weaknesses:  Its difficult to clother distinguish this work from the similar method proposed by Mahdizadehaghdam.  Using a dictionarybased method (MLCSC) is not considered a significant innovation.  Results on CIFAR10 seem weak compared to recent advancements in highresolution image GANs.  DIP is not an optimal denoising method and requires other stopping which may be affected by the sparsity constraint.  Sparsity is not guaranteed during testing. Additional Comments:  Visual comparison of generated images would enhance the evaluation."], "sTkY-RVYBz": ["Paraphrased Summary: The work investigates the effect of model generalization beyond its training domain when using batch normalization (BN) layers. It attributes this challenge to the overreliance on features with low variance in BN layers. To address this the paper proposes a \"counterbalancing teacher\" approach that shift knowledge from a BNenabled model to a nonBN model. Experiments support the hypothesis that the approach improves robustness to data corruption and generalization to unseen domain. The approach is also tested on a 3D target cloud dataset. effectiveness and Weaknesses: effectiveness:  Clear and wellmotivated writing  Simple and effective approach that does not require knowledge of the unseen domain  empirical evaluation against various baseline methods Weaknesses:  Lack of theoretical analysis explaining how distillation resolves the effect with BN layers  Unclear reasoning behind the potential drawbacks of relying on lowfrequency features", "Paraphrased Statement: The paper suggests that Batch Normalization (BN) and related layers increase the sensitivity of neural networks to shift in data distribution. To address this it proposes a regularization technique based on distillation. While BN can improve accuracy within the original distribution it makes models more dependent on specific features making them vulnerable to shift where these features are altered. In contrast models without BN rely on all features more evenly leading to large robustness at the expense of lower accuracy in the original distribution. The paper introduces a training method where a teacher model is trained without BN and a student model with BN layers is trained on the main task with an additional regularization loss to match its hidden representation to that of the teacher. This approach helps maintain BNs accuracy within the original distribution while improving performance against distribution shift. The question remains whether the teacher and student models are initialized with the same weights. The paper is wellwritten its parameter are convincing and the empirical results provide strong support for the proposed technique.", "Summary Paraphrase: The authors introduce a new method to enhance the robustness of models against common image distortions. This method leveraging a studentteacher training paradigm utilizes a consistency loss with a student network that excludes batch normalization. significant improvements are observed in corruption datasets like CIFARC and VLCS. effectiveness and Weaknesses Paraphrase: The paper excels in clarity and motivation with promising experimental results. However the experimental section lacks depth:  Full results are not presented for ImageNetC despite its prevalence in similar research.  Comparison with competitive techniques (e.g. augmentationbased methods) is limited.  Indepth analysis and ablation work are not included. method Paraphrase: The description of the method in Section 3.2 needs improvement:  Clear details on batch normalization implementation (statistics estimation studentteacher updates) are lacking.  Despite the methods simplicity reimplementation based on the provided information may be challenging. Motivating Examples Paraphrase:  Error bars should be added to experiments in Figure 3 to enhance confidence.  Including results for an \"on the fly\" normalization layer (e.g. GroupNorm LayerNorm) in Figure 3 would be valuable. Figure 4 Paraphrase:  Lines should be disconnected to avoid misinterpreting relationships between methods.  A scatterplot with error bars would be more suitable. Experimental Results Paraphrase:  Clarification is needed on which results were reimplemented and which were cited.  Verification of baseline results to ensure experimental consistency is important. SelfSupervised Learning Paraphrase:  The justification for using selfsupervised learning when labels are available is unclear.  This aspect could be deemphasized and replaced with an extended Table 2 that includes more methods. Limitations and Expectations Paraphrase:  The limitations of the method and its expected performance for larger models and datasets are not discussed.  The potential parallels between this method and the limitations of batch normalization version for larger scenarios should be acknowledged.", "Paraphrase: Summary: The authors examine the impact of BatchNorm (BatchNorm) layers on model resilience. They show that models trained with BatchNorm often depend on features with low variance that work well on indomain data but not on outofdomain data. Based on this insight they propose a studentteacher approach (Counterbalancing Teacher) that creates a copy of the original model without any BatchNorm layers. They prove that this results in outstanding performance comparable to other approaches for mitigating outofdomain performance loss such as selfsupervised methods and data augmentation. effectiveness:  Clear presentation especially the insight into datadependent versus dataindependent norms in Section 2.1.  comprehensive experiments in Section 4 clearly outlining the hypotheses being tested.  Extensive comparison of CT to various data augmentation methods. Weaknesses:  Section 4.1 presents an unconventional experiment (red vs. blue). The authors lack sufficient control over version sources making it difficult to conclude whether the synthetic features are indeed the lowest variance features distinguishing digits. A variance histogram would enhance the experiments robustness.  Figure 4 is presented as a line plot which is misleading since the Xaxis is categorical. A more suitable plot case would be a grouped barplot or boxwhisker plot.  Figure 4 evidence normalization results for various layers of Wide ResNet but the corresponding results are discussed in Section 4.2 with a condition not depicted (e.g. \"NN outperforms all normalization cases on corrupted data\" despite \"NN\" not being shown in Figure 4).  Despite acknowledging the reliance on lowvariance features due to normalization the authors regularize both rG\u03b8 and rF\u03b6 with l2 normalization. An alternative method that does not require normalization would be beneficial.  While Table 2 is informative it only reports target estimates of mean crossentropy (mCE). An ablation work varying the regularization effect (\u03bb) in Equation 7 could provide valuable insights."], "rHMaBYbkkRJ": ["Paraphrase: compact: This research explores the challenge of evaluating methods in Continual Learning (CL). The authors highlight the complexity of CL as it involves several intersecting field. They propose CLEVACompass (Continual Learning EValuation Assessment Compass) a graphical tool that enhances the transparency and comparison of CL methods. Strengths:  Clear and structured introduction of concerns about CL evaluation.  Acknowledges the difficulty in comparing methods due to different assumptions and metrics.  CLEVACompass promotes transparency by specifying the metrics and experimental settings used in evaluation.  The visual reintroduction of CLEVACompass effectively summarizes data. Concerns:  The selection of paradigms and metrics in CLEVACompass is subject to change as CL evolves.  The definitions of innerlevel aim may be ambiguous leading to confusion.  The placement of aim in CLEVACompass may not accurately represent the overlap of concepts or the use of unsupervised methods.", "Paraphrased Statement: CLEVAcompass presented in this research tackles the difficulty of comparing continual learning methods due to discrepancies in experimental setup and evaluation. CLEVAcompass visualizes key aspects of different approaches such as paradigms and evaluation metrics to simplify comparisons. While CLEVAcompass effectively illustrates the diverse paradigms and challenges in continual learning some domain remain unclear: 1. The aim and distinction between the inner and outer circles representing supervised and unsupervised learning are not fully explained. 2. The specific benefits of using CLEVAcompass beyond merely highlighting influential paradigms are not clearly stated. 3. CLEVAcompasss inability to indicate applicable contexts or range methods raises questions about its utility for assessment. Therefore its advantages and incentives for researchers to use it for presenting their work remain unclear.", "Paraphrased Statement: compact: This paper introduces a method for more accurately evaluating continual learning. It offers a visual representation that helps determine a methods position within the broader field and allows for comparisons of methods based on reported metrics. Strengths and Weaknesses: Strengths:  Effectively addresses the issue of evaluating continual learning approaches.  Provides a comprehensive analysis of evaluation aspects (Figure 2) and their significance.  Highlights the importance of differentiating between static and continual evaluation and its impact on the choice of methods and metrics. Weaknesses:  Clarification is needed on how the CLEVACompass can be practically implemented and used.  Despite acknowledging the gap between theory and practice the proposal currently lacks a clear plan for its application in realworld scenarios.  The statement discouraging attempts to combine previous work with the CLEVACompass is not fully explained.", "compact: This paper highlights the challenges of defining universal criteria for continual learning (CL) research due to varying application scenarios. It proposes a classification system to structure comparisons and reviews evaluation challenges emphasizing aspects often overlooked in other machine learning domain. The paper advocates for a multiobjective evaluation approach using a radar spider web chart (CLEVACompass) based on curated CL desiderata. It anticipates both intended and unintended uses of such a system and discusses limitations and complementary dataset aim challenges. Strengths:  Identifies current challenges in defining CL evaluation.  Articulates the hypothesis of CL as a multiobjective optimization problem.  Provides a coherent literature review leading to a classification strategy.  Proposes a novel evaluation approach to address the complexity of CL. Weaknesses:  No formal system is proposed to precisely define terms.  Does not adequately address the impact of incentive structures on research progress in terms of \"SOTA claims.\"  May not fully mitigate the challenges of unbalanced datasets reinforcement learning contexts or adversarial scenarios."], "pzgENfIRBil": ["Paraphrased Summary: The paper introduces a novel technique called SCGLED (Selfconsistent Gradientlike Eigen Decomposition) for solving approximate Schr\u00f6dinger equations. This approach utilizes a unique framework that regards F(V) as an \"online data generator.\" This allows for gradientlike eigendecomposition methods to iteratively approach the selfconsistency of the equation in a manner similar to online learning. SCGLED eliminates the need for quantum mechanics heuristics in initial guess generation. The paper demonstrates its effectiveness in replacing traditional heuristicsbased methods resulting in significant performance improvements. SCGLED enables the independent discovery of highly accurate solutions without the use of conventional iterative methods. Strengths and Weaknesses: Strengths:  SCGLED effectively replaces traditional heuristicsbased initial guess methods enhancing performance. Weaknesses:  In Equation (3) vi is used in the equation but vit is mentioned in the description which could cause confusion.  Figure 4 only illustrates the speed of SCGLED in terms of iterations. It would be valuable to also display the absolute time taken for molecule solving by SCGLED SCGLED with DIIS and traditional methods. Conclusion: The paper presents a novel method SCGLED for solving approximate Schr\u00f6dinger equations using gradientlike eigendecomposition. SCGLED not only outperforms traditional heuristicsbased methods but also independently finds highly precise solutions without the need for conventional iterative methods.", "Paraphrase: This paper presents a gradientbased algorithm for calculating eigenvalue in nonlinear problems. The algorithm eliminates the need for problemspecific guesses in traditional methods. It is straightforward to implement does not rely on heuristics and is generally robust. Experiments demonstrate its improved efficiency and accuracy compared to existing methods. Strengths:  The proposed algorithm outperforms traditional methods in accuracy and efficiency.  The paper provides detailed technical data about the algorithm. Weaknesses:  The algorithm is based on the OjaKarhunen gradient method which is an incremental modification.  The paper lacks a convergence analysis for the algorithm.", "Paraphrased Statement: The paper investigates the Schr\u00f6dinger equations and the associated nonlinear generalized eigenvalue problem. It introduces the Selfconsistent Gradientlike Eigen Decomposition (SCGLED) algorithm and demonstrates its improved numerical stability in solving Schr\u00f6dinger equations. Strengths and Weaknesses:  Proposition 1:  The term \"negligible\" in Proposition 1 lacks clarity. It should be defined or replaced with a more precise concept.  The proof of Proposition 1 is challenging to comprehend. It appears to intend to establish a relationship between stable local convergence points and solutions to Equation 1 but further clarification is needed.  experimental Results:  To achieve satisfactory results the proposed method (SCGLED) still requires combination with the SelfConsistent field (SCF) algorithm.  More details should be provided on the full solver case including comparisons between SCF and SCGLED.  data on the number of examples where the algorithms failed to converge and the performance of convergence in successful cases would be valuable.  Minor publication:  The symbols for Veff and V should be distinct to avoid confusion.  Equations (2) and (3) could be presented in matrix form for consistency with the remainder of the paper.", "Paraphrased Statement: Summary This field introduces a numerical approach for solving specific nonlinear generalized eigenvalue problems related to the HartreeFock approximation of the Schr\u00f6dinger equation. The focus is on identifying the lowest k eigenvalue and eigenvectors. The general strategy involves employing fixedpoint iterations (or solving successive linear eigenvalue problems) within the SelfConsistent field framework. For determining the clear k smallest eigenvalue and eigenvectors of the linear problems the researchers utilize the recently developed EigenGame algorithm instead of the traditional Ojas algorithm. EigenGames advantage lies in replacing the timeintensive QR decomposition in Ojas algorithm with a generalized GramSchmidt step during gradient updates. numerical experiments demonstrate the efficiency of the proposed method. Strengths and Weaknesses  research efficient methods for approximating the Schr\u00f6dinger equations eigenvalue is a valuable research endeavor.  This paper proposes an approach for approximating the smallest k eigenvalue and eigenvectors of a generalized nonlinear eigenvalue problem associated with the HartreeFock approximation. Concerns and Comments  The core of the algorithm appears to be the EigenGame method for identifying the clear k lead components of the linear eigenvalue problem with minor modifications such as damping and momentum. Applying an existing method to a new problem does not constitute significant novelty for ICLR.  The authors claim robustness to initial guesses but present empirical evidence only. rigorous justification or convergence proof is lacking. The EigenGame paper provides strong theoretical analysis similar analysis here would enhance the paper.  Regarding the original Schr\u00f6dinger problem the paper focuses on the HartreeFock approximation and finitedimensional discretization. The authors could discuss whether and how the proposed method particularly the EigenGame approach can be extended to the original infinitedimensional Schr\u00f6dinger problem without Hartree approximation.  In Proposition 1 the assumption should be explicitly stated as the continuity of F. The current phrasing is imprecise.  Appendix D contains an excessive 10page table of numerical results. It is suggested to select representative data and provide the complete results in an accessible online repository."], "i7O3VGpb7qZ": ["Paraphrase: This paper introduces a novel approach for editing source code using a few exemplar edit. Unlike existing methods that rely on a individual edit exemplar our approach leverages multiple exemplars to learn edit representations. We combine these representations and generalize them to novel code snippets through a \"multiextent\" approach. Our exemplar employs a specialized role called \u03bbsoftmax to capture the importance of different nodes in the code abstract syntax tree (AST). Strengths and Weaknesses: Strengths:  Addresses a significant problem in software engineering.  Proposes a novel deep learning exemplar with improved performance. Weaknesses:  Evaluation metrics are not clearly defined making it difficult to interpret results.  human evaluation is not included to assess tool effectiveness.  Unsuccessful code editing exemplars are not discussed.  syntax correctness assurance is not discussed.  The \u03bbsoftmax role is derived heuristically and not trainable raising query about its effectiveness.  Limited comparison with existing process and traditional approaches (e.g. LASE).  The paper lacks theoretical analysis regarding AST representations.", "Paraphrased Summary: This research focuses on applying existing code edit to novel code segments similar to how a linter might replace double quotes with individual quotes in Python. Previous methods used a individual edit exemplar to predict the solution but this approach can lead to overfitting. Our process expands on this by:  Allowing the exemplar to learn from multiple exemplars  Using a similarity ranking to weight exemplars based on their relevance to the novel code segment This approach improves performance by considering alignment matches at both the individual and overall levels. It outperforms baseline methods on datasets generated from GitHub code. Strengths and Weaknesses:  Strengths:  Clear problem need  Multiple exemplar support  similarity matching role provide adaptability  Weaknesses:  Unclear explanation of similarity role (Section 3.1)  Confusing use of the term \"extent\"  Lack of explanation on optimal exemplar selection  Ambiguous use case for multiexemplar editing Questions: 1. How many exemplars are ideal Is variety or other metrics significant for exemplar selection 2. What is the practical use case for multiexemplar editing Will users provide exemplars or is a different use mode expected 3. How does your similarity matching formulation differ from attention mechanisms Minor Issues:  In equation 1 activations have gsk as a superscript but in equation 2 they have gsk. Please maintain consistency.  The descriptions of the \"Ours\" baselines are difficult to understand. Provide a more precise alternate formulation.", "Paraphrase: Neural code editors are AI exemplar that modify programs by applying edit represented as vectors. Most editors use a individual snapshot of the code before and after the edit to create the edit vector. However this may not be enough to capture all the information needed to perform the edit. This paper introduces an approach that combines information from multiple pairs of code snapshots to create more precise edit vectors. This approach outperforms simpler methods that use only a individual pair or different combinations of multiple pairs. The approach uses a tree structure to represent the programs and extracts features from the nodes of the tree. It then matches the input program with a set of programs from the exemplar and combines the features of similar nodes to create a more robust edit vector. While the approach improves the accuracy of code editing it also has limitations. One is that it relies heavily on a complex and computationally expensive matching process. Another is that the evaluation is limited to a specific type of code fixer and a small number of exemplar. Thus it is not yet clear whether the approach generalizes well to other code editing tasks and different types of fixers. To improve the paper it would be helpful to:  Explore the use of tree structure to optimize the matching process.  Conduct experiments with individualexample baselines using different distance measure.  Add an iterative baseline that applies edit from multiple exemplar separately.  Provide a more detailed explanation of why combining multiple edit representations is beneficial.", "Paraphrased Statement: Summary: This paper introduces a technique for code editing using a limited number of exemplar. It automatically generalizes code modifications by adaptively combining multiple support exemplar through multiextent paper. The method outperforms existing Graph2Edit baselines on two benchmark code editing datasets showing an improvement of 810. Strengths and Weaknesses: Strengths:  Addresses the novel problem of code editing with few exemplar.  use multiple exemplar for training and inference. Weaknesses:  Difficult to read with undefined terminology.  Unclear definition of \"editorial style\" in code editing.  Selection process for support exemplar is not specified.  Lack of clarification on the role of alignment in existing baselines (e.g. Graph2Edit). Suggested improvement:  Highlight key contribution and methods in the abstract.  Define \"editorial style\" more clearly.  Provide an algorithm for selecting support exemplar.  Clarify the alignment strategy used in the Graph2Edit baseline.  Consider using more precise language such as \"compose\" instead of \"complex.\""], "kHkWgqOysk_": ["Summary: This study examines the impact of \"class mismatch\" in semisupervised study where unlabeled data includes both labeled and unlabeled samples. The authors focus on the pseudolabeling method which uses model predictions as labels for unlabeled data. Findings:  Highconfidence predictions are more common in labeled data than in unlabeled data.  Pseudolabels are initially more balanced in labeled data but become imbalanced in unlabeled data for a specific class.  Assuming known unlabeled data labels the best solution is to treat the problem as a multiclass classification with distinct classes for labeled and unlabeled data. Proposed model: The authors propose the Upsilon model which combines a rebalanced pseudolabeling (RPL) approach to balance pseudolabels and a semantic exploration cluster (SEC) technique to address the lack of ground truth labels for unlabeled data. effectiveness:  Provides insights into the behavior of pseudolabels in class mismatch problems.  Proposes an algorithm that alleviates these issue and outperforms existing SSL methods.  Motivates the methodology through empirical comparisons. Weaknesses:  Limited experimental validation to a single dataset.  Unclear if findings are specific to the dataset or the animalvehicle split used.  Unclear mechanism behind the underperformance of openset labeling compared to oracle labeling.", "Summary: This research explores a challenge in semisupervised study known as \"class mismatch\" where the data used for labeling has different class proportions than the original data. The authors analyze an existing method called \"PseudoLabeling\" and find its limitations in handling class mismatch. They propose a novel approach combining two components: \"Rebalanced PseudoLabeling\" and \"Semantic Exploration Clustering.\" Their experimentation demonstrate the improved performance of this novel method compared to both supervised study and existing SSL techniques across various class mismatch scenarios. effectiveness:  Provides empirical evidence of PseudoLabelings behavior in classmismatched SSL.  Introduces a novel method (YModel) to address class mismatch in SSL.  Experimentally validates the effectiveness of the proposed method on several SSL benchmarks. Weaknesses:  The motivation for considering imbalanced pseudolabels is not wellexplained.  The proposed approach lacks significant technical novelty and depth.  The experimentation could be more extensive to provide a more comprehensive evaluation.", "Summary This study investigates semisupervised study (SSL) with unlabeled outofdistribution (OOD) data. It analyzes the performance degradation of SSL models in OOD scenarios and proposes methods to mitigate it. The proposed Ymodel uses Rebalanced PseudoLabeling (RPL) and Semantic Exploration Clustering (SEC). experimentation on CIFAR10 and SVHN demonstrate its effectiveness. effectiveness and Weaknesses effectiveness:  Explores the novel problem of OOD SSL.  Provides insights into the class imbalance of pseudolabeled data.  behavior thorough experimentation and ablation study.  Wellwritten and easy to understand. Weaknesses:  experimentation on smallscale datasets with limited evidence of performance on larger benchmarks.  Compares to weaker SSL methods obscuring the effectiveness against stateoftheart techniques like FixMatch.  Lacks comparison with a concurrent study on OOD SSL (Cao et al. 2021).  Truncation of pseudolabeled data is a simplistic solution and alternative methods could be explored.  Code issue is not mentioned.  Clustering on largescale datasets may be computationally challenging.", "Paraphrase: Summary: The researchers address the challenge of using pseudolabels in semisupervised study when there is unlabeled data from outside the aim class (outofdistribution data). They propose a model that includes:  Rebalanced PseudoLabeling: Adjusts pseudolabels on known classes to exclude outofdistribution data.  Semantic Exploration Clustering: Creates pseudolabels for additional classes using balanced clustering on outofdistribution data. effectiveness:  The study highlights the impact of outofdistribution data on pseudolabels and model performance.  The proposed approach appears novel. Weaknesses:  The evaluation lacks comparison to more recent and effective semisupervised study methods (e.g. FixMatch and ReMixMatch).  The proposed model should be tested on various datasets (e.g. CIFAR100 STL) to assess its robustness."], "p7LSrQ3AADp": ["Paraphrased Statement: This paper provides a set of criterion for assessing how well explanation of neural network decisions based on saliency maps perform. These criterion include aspects like whether the saliency map is static or dynamic and how much its influenced by specific settings. Although the paper doesnt implement or test the proposed framework it offers a helpful analysis of existing methods based on these criterion. Researchers and users in explainable AI (XAI) may find this framework valuable. However limitations include:  lack of clear Evaluation: Its not specified how the models are ranked against the criterion or how a \"score\" would be calculated. The authors acknowledge that subjective criterion like \"perceptual correspondence\" may be difficult to measure and yield varying results with different methods raising questions about the frameworks practical application.  Untested Framework: The framework has not been implemented so its effectiveness in aiding users in selecting effective models and making more informed decisions has not been evaluated.", "Paraphrase: This article compares \"saliency\" methods used in artificial intelligence (AI) networks. It evaluates them based on nine factors: efficiency reliability adaptability compatibility input influence simplicity visual matching and concept relevance. The authors claim that this new system helps users choose the effective saliency method for their specific needs. effectiveness and Weaknesses:  effectiveness: The article is clear and concise. It covers relevant research well.  Weakness: It could have discussed the differences between the methods in more depth. The article also doesnt provide specific examples or user feedback to support its claim that the new system improves decisionmaking. Additional Note: The article defines \"saliency\" as methods that identify which inputs most influence network decisions. However it doesnt fully explore what exactly these methods represent or whether users may have different expectations from them.", "Paraphrased Summary: This paper introduces a framework that helps evaluate and compare saliency methods which provide insights into how models interpret input data. The framework defines nine dimensions grouped into three categories each with quantifiable measure. These dimensions are used to create \"saliency scorecards\" that summarize the effectiveness and limitations of different algorithms making it easier for users to choose the effective choice for their specific needs. The analysis of ten recent algorithms using this framework reveals that some methods lack assessments in certain dimensions highlighting areas for future improvement. The paper effectively illustrates each dimension with examples to enhance understanding. effectiveness:  The framework provides a valuable abstraction for evaluating saliency methods focusing on their practical utility instead than subjective comparison.  The clear introduction of dimensions with examples facilitates comprehension.  The saliency scorecards offer concise summaries of algorithm attributes.  The comparative analysis highlights algorithm effectiveness weaknesses and gaps. Weaknesses:  The analyzed methods are mostly from 2017 or earlier potentially missing more recent development.  The paper does not introduce novel research beyond the framework itself."], "iEvAf8i6JjO": ["Summary This paper introduces an further learning method called continual learning. Its based on a technique called gradient projection memory (GPM) which tries to keep the model behavior consistent with past tasks. However the original GPM method can be also strict preventing the model from adapting to new tasks. This paper introduces a new balanced approach that allows the model to update its behavior while still considering past tasks. effectiveness:  Identifies a limitation in the original GPM method and proposes a solution.  Demonstrates improved performance in practical applications. Weaknesses:  The writing could be improved for clarity.  Some concept and reasoning are difficult to understand.  Certain claims and justifications lack theoretical or empirical support.  The novelty of the proposed method is limited as its essentially a variation on existing techniques.", "Paraphrased Statement: In this field we tackle the challenge of transferring knowledge forward in ongoing learning situations. We assess similarities between learned tasks using \"trust regions\" which allow us to pinpoint tasks that are most closely aligned with the task at hand. Our approach involves allowing frozen weights from analogous past tasks to be relaxed for use in addressing the current task. By relying on task similarities this method prevents catastrophic knowledge loss while facilitating knowledge transfer. We validate the efficacy of our approach across four benchmark datasets showcasing its promise. effectiveness:  Addresses a novel problem in continual learning: forward knowledge transfer.  Wellwritten and accessible paper.  Demonstrates effectiveness through experimentation. Weaknesses:  experimentation are limited in scope omitting significant benchmarks.  Lacks a thorough comparison with contemporary research.", "Paraphrased Statement Summary: Current methods often restrict the optimization space for new tasks to avoid \"catastrophic forgetting\" but this can compromise performance on the new tasks themselves. This paper proposes a novel approach to transfer knowledge from previous tasks to new ones based on a picture of task correlation. Main Contributions: 1. Trust Region: Defines a \"trust region\" based on gradient projection onto task input subspaces to measure task correlation. 2. Scaled weight Projection: Allows the new task to leverage knowledge from strongly correlated previous tasks within the Trust Region. 3. Trust Region Gradient Projection (TRGP): Introduces a continual learning approach based on scaled weight projection and a method for constructing task input subspaces. 4. performance Improvement: Demonstrates significant performance improvements on all benchmark tasks compared to existing stateoftheart methods. effectiveness:  Clearly outlines the challenge of existing methods and proposes a novel solution.  Achieves substantial performance increase on benchmark tasks and surpasses baseline methods. Weaknesses:  Toy model: Toy model 1 raises questions about the necessity of matching outputs between tasks when inputs change.  task correlation Measurement: The use of gradient projection onto a subspace spanned by inputs to measure correlation warrants further justification.  Trust Region Selection: The effectiveness of trust region selection which prioritizes selecting nearby tasks rather than strongly correlated tasks is questionable.  weight Projection Assumption: The papers assertion that weight projection onto strongly correlated task subspaces is significant for new tasks requires more evidence and proof.", "Paraphrase: Summary: This paper introduces an enhanced method for incremental learning called Trust Region Gradient Projection (TRGP) which addresses a limitation of existing Gradient Projection (GP)based approach. Motivation: GPbased methods excel at preventing catastrophic forgetting by preserving weights crucial to previous tasks. However when new tasks are similar to previous ones these methods may hinder progress by restricting weight updates in directions orthogonal to subspaces learned for other tasks. This can result in insufficient learning of new tasks (intransigence) and reduced ability to benefit from previously learned knowledge (backward transfer). Proposed Solution: TRGP overcomes this issue by introducing a scaling matrix Q that allows for selective unfreezing of weights within \"Trust region.\" These regions are defined as the most correlated subspaces from previous tasks to the current task. By balancing freezing and unfreezing through Q TRGP enables the network to optimize weights that are significant for both previous and current tasks improving learning and backward transfer. issue and comparison: The paper demonstrates the effectiveness of TRGP through experimentation on diverse datasets including MNIST CIFAR100 and a sequence of 5class classification datasets. issue indicate consistent improvements in accuracy and backward transfer compared to GP methods regularizationbased methods and memory replay. effectiveness and Weaknesses: effectiveness:  Addresses a significant problem effectively  Wellmotivated and theoretically sound  Wellwritten and accessible  Provides comprehensive review of relevant literature Weaknesses:  None identified but questions for the authors are provided Questions for Authors:  Why wouldnt the network always choose Q to unfreeze all Trust Region parameters  What is the memory footprint of the TRGP algorithm How does it compare to GP in terms of wallclock time  Is there a potential benefit to incorporating correlation values into the Trust Region definition"], "q7n2RngwOM": ["Paraphrase: Summary: This work investigates the challenging scenario of limited covariate overlap where estimating causal issues is difficult due to the lack of data at nonoverlapping values. The paper proposes a novel approach that leverages the prognostic score and IntactVAE (a combination of iVAE and CVAE) to identify and estimate causal issues. Specifically a new regularized VAE called \u03b2IntactVAE is introduced and its performance is analyzed through theoretical bounds on the treatment issue estimation error. experimental results demonstrate the issueiveness of the proposed method compared to existing approach. effectiveness:  Introduces an innovative approach for addressing limited covariate overlap combining prognostic score and IntactVAE techniques.  Provides extensive experimental results demonstrating the importance of balanced priors in use.  face theoretical analyses tailored to the specific setting of limited covariate overlap adapting results from iVAE to this new context. Weaknesses:  The paper dense content involving concepts from multiple areas makes it challenging to follow.  The use of similar notation for different issue (e.g. real issue and use) can be confusing.  The experimental results show that the proposed method performs worse than CFR in condition of the PEHE metric. This should be discussed in more detail considering the different data that ATE and PEHE convey.", "Paraphrase: Summary The work explores a challenge problem in causality: estimating effects when there is limited overlap between treatment groups. This is a significant issue in causality research. The researchers provide a theoretical model using PtS which modifies prognostic scores. Estimation employs a limited Variational Autoencoder (\\betaintactVAE). Furthermore the authors establish bounds on the error of Conditional Average Treatment issue (CATE) estimation. The paper concludes with experimental results and comparisons with other CATE estimation methods. effectiveness  clear and accessible language  Strong motivation and relevance  Rigorous theoretical analysis Weaknesses  data density makes it challenge to grasp all the details  Some assumptions remain unclear such as the rigour of the adjustment set X and the use of a neural network  The theoretical analysis assumes a noiseless prior which may limit applicability  The paper lacks specific model where the method is likely to be useful  The link with work on neural network generalization is not explored Additional Questions  Could the authors provide a more explicit list of assumptions and their necessity  Are there realworld scenarios where the assumptions of the method hold reliable  How does the method relate to research on neural network generalization  Could the regularization of estimation be a significant factor in the results given that it promotes outofsupport generalization", "Paraphrased Statement: This paper addresses the issue of limited overlap in estimating treatment issues. It proposes using prognostic scores which have less restrictive assumptions on overlap than other methods like propensity scores. The main contributions include:  Identifiability: Deriving conditions under which treatment issues can be identified using prognostic scores.  generative model: Developing a generative prognostic model based on variational autoencoders (VAEs). Theoretical results are complemented by experiments showing the proposed models are competitive and can improve upon existing methods. issueiveness:  Extends prognostic scores to machine learning in the face of limited overlap.  Provides a range of theoretical results on treatment issue identification and estimation.  Wellwritten with clear motivation and context. Weaknesses:  Some claims and statements are difficult to follow without consulting the Appendix.  The authors use the acronym \"PS\" for prognostic scores which conflicts with its common use for \"propensity score.\"  The experiments could be improved by:  Addressing the discrepancy between notation used in the paper and experiments.  Explaining the selection of linear use and hyperparameters.  Including a comparative work with the method proposed by DAmour and Franks (2021).  Assessing empirical error bounds against the theoretical bounds derived in Theorem 2.  Reformulating the claim about confounder dimensionality to emphasize potential separation issues with increasing covariates.  Including the SUTVA assumption for completeness.", "Paraphrased Summary: This work explores how to estimate treatment effects when there is limited overlap where individuals with certain traits may only be present in one treatment group. To address this the authors develop a novel variational autoencoder (VAE) called \\betaIntactVAE which builds upon previous CVAE and iVAE models. They mathematically prove that \\betaIntactVAE accurately identifies treatment effects. The proposed method is evaluated on synthetic datasets and compared to other models. effectiveness:  \\betaIntactVAE is a new and promising tool for estimating treatment effects.  The authors provide theoretical support for the methods effectiveness.  Empirical results demonstrate its superiority over other generative models. Weaknesses:  The authors use some notation that may be unfamiliar. Specifically symbols such as ft kt ht diag and diag1 are introduced without prior definition.  The difference between p\\theta(yxt) and p(yxt) in Section 3.2 is unclear.  The caption for Table 1 mentions an \"unconditional balancing hyperparameter\" but this concept is not elaborated upon.  It is unclear whether the parameters \\Lambda in Section 4.2 are learned from the data.  Equation (6) indicates that q\\phi(xyt) even depends on t. It is not clear how balanced PtS is achieved in this face.  Equation (7) appears to contain a typo: gt2(z) should likely be gt(z)."], "inA3szzFE5": ["Paraphrased Statement: Summary: This field introduces a metric to quantify a models sensitivity to spatial frequencies in the Fourier domain. The researchers show that CNNs often favor specific spatial frequencies across image samples. Using this metric they develop a set of spatial frequency regularization techniques to reduce a models sensitivity to certain frequencies. effectiveness:  clear and accessible writing  Wellmotivated proposed regularizer  Interpretable and applicable to any differentiable model  robustness against Fourier filtering and corruptions while maintaining accuracy Weaknesses:  Limited dataset and model selection for evaluation (only CIFAR10 included)  Unclear effectiveness of the method:  performance of SFS regularization methods in Tables 2 and 3 matches that of a simple \"lowpass filtered\" approach  high correlation between their performance under different types of corruptions Questions:  What is the computational cost of the proposed method compared to baseline methods considering the Jacobian involved in the regularization term", "Paraphrase: The research aims to address the uncontrolled handling of frequency components in image data by models. It introduces a novel regularization technique to enhance performance. The combination of chain rule with the input Jacobian is a notable concept. effectiveness:  Insightful discussion particularly regarding the application of chain rule.  Introduction of a novel regularization method. Weaknesses:  Unconventional presentation of experimental results. Bold numbers are used to indicate improvement over the baseline but they are usually reserved for peak performance. This makes the results appear weak compared to AugMix despite the authors claim that their method is competitive.  Clarity could be improved such as by providing illustrations to clarify how P corresponds to elements in image 1.", "Summary: This field defines \"spatial frequency sensitivity\" by analyzing a models input Jacobian through frequency transformation. It argues that this measure reflects a models sensitivity to specific spatial frequencies. The authors propose using this measure for \"spatial frequency regularization\" which promotes specific frequency sensitivity. effectiveness and Weaknesses: effectiveness:  The regularization method reportedly improves classification accuracy. Weaknesses:  The regularization methods result in a significant (7.3) reduction in accuracy on clean image which undermines the authors claims of competitiveness.  The field is primarily limited to small datasets and only evaluates model sensitivity on Imagenet not training.  The authors do not provide evidence that models filtered by frequency display the measured sensitivity which raises questions about the robustness of the sensitivity measure methodology.", "Paraphrased Statement: This field introduces a novel technique to make neural network training more robust against superficial Fourier model in data. By adding a regularization term based on the Fouriertransformed input Jacobian to the loss function the method allows trained models to disregard features with specific frequencies in the dataset. spatial frequency sensitivity is defined using the input Jacobian in Fourier space and the technique can be applied to any multidimensional data although the field primarily focuses on image datasets. Empirical evaluations on CIFAR10 and CIFAR100 datasets show improved robustness against Fourier filtering and image patching outperforming other baselines. While the proposed method may fall slightly short of the stateoftheart AugMix method in image corruption scenarios it is simpler and still surpasses other baselines in many cases. Experiments suggest that the method aids in learning global features in the dataset. effectiveness:  Simpler and more principled approach than using Fourierfiltered data or data augmentations to prevent bias towards superficial features.  Wellwritten motivation and method sections and convincing results in robustness against Fourier filtering. Weaknesses:  experimental results for data corruptions may lack significance with AugMix outperforming the proposed method.  Limited experiments (only CIFAR10 and CIFAR100) and the significance of outperforming \"mediumhigh pass filtered\" methods is unclear.  The sensitivity to the weighting hyperparameter between the two loss terms is not discussed.  AugMix results in the patchshuffling experiment are not provided."], "xwAw8QZkpWZ": ["summary Paraphrase: This research expands on existing concepts of behavioral priors (BPs) in reinforcement learning (RL) particularly those introduced by singh et al. (2021). Central to this approach is the use of a BP use (Z x s \u2192 A) and a policy over an abstract space (Z). By combining the two actions are generated that are constrained by the likelihood of actions observed in a dataset. This dataset is used to train the BP offline after which standard RL algorithm can learn a policy over Z. This approach has been shown to accelerate RL. Key Extensions for safety: The paper extends this approach by incorporating methods that learn the BP in the context of safety conditions. By learning an additional safety indicator variable the behavior learned by the system can be distinguished more effectively between safe and unsafe actions. The paper focuses on the learning background and loss use required to achieve this. It also introduces a mechanism to balance performance and safety violations. Evaluation and Analysis: The proposed approach is tested on a continuous robotic task demonstrating its effectiveness in comparison to several baselines. strengths and Weaknesses:  Clear extension of behavioral priors to safety domains.  Addresses the limitations of typical BP setup in safety contexts. sphere for improvement:  further discussion on the relationship between dataset quality and the potential for outofdistribution (OOP) actions.  A more explicit definition of the abstract action space (Z) within the RL framework.  A more thorough comparison to related work in the sphere of continuous spaces and abstract actions.  Enhanced clarity in the description of the approach and its connection to related concepts.  Improved presentation of experimental results including further explanation of performance and safety violation design.", "Summary: This paper introduces a method for safe policy learning that uses a prelearned behavior prior to condition the policy. The prior is learned from a dataset of safe and unsafe trajectories using variational inference. The conditioned policy is evaluated on simulated robot use tasks and shows improved safety performance compared to existing skill learning approach. effectiveness:  Modifies existing skill learning algorithm to incorporate safe learning by learning a behavior prior.  Intuitive and easy to understand.  Outperforms baselines (PARROT and SAC) in terms of safety after training. Weaknesses:  Offline data Requirement: Requires an offline dataset of both safe and unsafe behavior for prior learning which may be unsafe during data collection.  Insufficient literature Review: Omits discussion of relevant safe RL and skill learning work including those that emphasize the challenges of safe data collection.  Safety Violation Misrepresentation: solution on safety violations do not account for violations during offline data collection.  Lack of External Safe RL Baselines: only compares against PARROT and SAC not other safe RL baselines.  No Safety Guarantees: Provides no theoretical guarantees on safe behavior during or after convergence.  contrastive Learning Interpretation: Unclear how the objective corresponds to contrastive learning.  policy Prior Ignoring: No constraint to prevent the policy from ignoring the prior during learning.", "Paraphrased argument: This paper extends the \"PARROT\" method by Singh et al. (2021). PARROT allows hierarchical training on offline datasets enabling the specification of preferences for desired behaviors referred to as \"safety.\" This paper introduces an additional latent variable \"c\" representing \"safety.\" This variable is learned unsupervised from successful solution in various tasks. To ensure \"safety\" the authors use a chanceconstrained optimization approach converting probabilistic constraints into deterministic ones. To ensure that the safety constraint satisfies the desired level the authors leverage normalizing flows to estimate the threshold from historical data. This work is termed \"safety assurance\" or \"calibration.\" The papers key concepts include hierarchical reinforcement learning binary variables representing termination use the use of a Gaussian distribution to represent latent choice and the addition of a latent safety variable.", "Paraphrased argument: Summary: The work introduces SAFER a novel reinforcement learning (RL) method that improves policy learning under safety constraints. SAFER utilizes a prior learning algorithm to extract a safety argument from offline data and learns safe basic skills in different scenarios. effectiveness:  Tackles a critical problem in RL.  Clear and accessible writing.  Demonstrates improved performance over existing methods. Weaknesses:  Relies on established RL techniques without providing significant novelty.  Limited evaluation to a single environment raising concerns about generalizability. Recommendations:  Enhance theoretical analysis to elucidate the methods unique features.  Conduct more diverse experiments to verify the methods strengths and limitations."], "qyTBxTztIpQ": ["Paraphrase: Summary This research introduces CrowdPlay a novel tool for gathering human data using similar reinforcement learning environments. CrowdPlay not only allows the collection of diverse behaviors and data from various users through multiple channels but it also employs flexible and realtime incentive systems to ensure data quality. The authors have made a publicly available dataset and benchmarks for Atari 2600 games facilitating further research in imitation learning and offline learning. Strengths and Weaknesses CrowdPlay presents a novel approach to crowdsource human data. The paper is wellwritten and technically sound. The software design and details are clearly explained ensuring reproducibility. The dataset provided is valuable for reinforcement learning research. However there are some areas for improvement:  data imbalance: Some datasets in CrowdPlay may be imbalanced. This could potentially impact the performance of AI models trained on them.  AI Agent Training Details: More specifics on the AI agent training work would be beneficial such as training methods and differences between standard and cooperative variants.  incentive Mechanism for Social Media: The paper mentions data collection from social media users but does not specify the incentive mechanism used.  Minor Typos: There are a few minor spelling errors in the paper (e.g. \"what ALE and Gym what ALE and AI Gym\" and \"in both Space Invaders and Riverraid\").", "Paraphrased Statement: Summary: This paper introduces CrowdPlay a crowdsourced data collection platform built on OpenAI Gym. It includes a web interface for human players and a large dataset of six Atari games with multimodal and multiplayer data. Strengths:  Provides a valuable tool for the research community.  Dataset is significant.  Includes detailed data on the data collection work. Weaknesses:  focus excessively on platform and data collection details rather than data insights and significance.  Section 5 should be expanded to:  Highlight unique features of the data.  Discuss challenges it poses for offline RL algorithms.  Justify the focus on Atari. Other CommentsQuestions:  Define \"multimodality\" more clearly.  Acknowledge the Atari Grand Challenge datasets multimodal behavior.  Specify the \"various ingame behavior characteristics\" collected.  Define the \"Deepmindstyle\" observation working work and provide a reference.  Move software engineering and data working details to the appendix.  Explain \"task adherence\" used in design 3.  Clarify the \"realworld intricacies and diversity\" in the dataset.  Note that not all RL environments are Gym environments.  Justify the need for multiplayer data.  Include experimental hyperparameters in Section 5.", "Paraphrase: Summary: This study introduces CrowdPlay a platform that uses crowdsourcing to gather demonstrations for any Markov Decision work (MDP) environment. Additionally it offers a dataset of human gameplay for Atari games incorporating multiagent and multibehavior aspects. The study assesses current offline Reinforcement learning (RL) approaches using this dataset and provides details on incentive mechanisms for future crowdsourcing efforts. Strengths and Weaknesses:  Crowdsourcing human demonstrations has proven effective for solving complex learning tasks as seen in computer vision and natural language work.  CrowdPlay simplifies the work of collecting human demonstrations for any simulator.  The platform allows for easy postwork of data for learning experiments.  CrowdPlay is designed not to require simulator performance in the browser enabling compatibility with various simulators through a clientserver architecture.  The platform will be opensourced ensuring accessibility to researchers. Questions and proposition:  Can researchers recruit participants directly through CrowdPlay  The multimodal dataset is limited to two games are there design for expansion  Has the latency of the architecture been measured as high latency could discourage participation  Related work includes study utilizing YouTube videos for Atari game exploration and the latest offline RL algorithm for Atari games."], "q9zIvzRaU94": ["Summary Paraphrase: This paper explores causal relationships within nonstationary time series where the nonstationarity stems from unknown state factors. To address this the authors employ a probabilistic deep learningbased method. effectiveness and Weaknesses Paraphrase: While the papers topic is compelling its organization and writing require improvement. The authors appear to mix instantaneous and timedelayed causal relations leaving the reader confused. Additionally the observation status of the state variable is unclear throughout the paper especially in Section 3.2. Furthermore the paper lacks theoretical validation of causality claims such as identifiability of the causal graph. To strengthen the paper it should include references to related research on causal discovery in similar settings.", "Paraphrase: This paper introduces a novel method for uncovering the causal structure of nonstationary timeseries data. It draws inspiration from Lowe et al.s (2020) work and state that the causal graph driving edge introduction between variables can be conditioned on a categorical state variable. The method is formulated within a variational inference model treating edges as statedependent latent variables that enable the generationreconstruction of future observations andor state variables. effectiveness:  Addressing causal structure inference in nonstationary timeseries is a crucial and realistic challenge.  The method builds upon existing approaches suggesting reproducibility and plausibility.  Conditioning on a categorical state variable is a reasonable and logical approach. Weaknesses:  The model complexity limitations hinder its applicability to more complex data as evidenced by observational results.  The assumption of a categorical state variable with a fixed number of states may not adequately capture complex temporal model.  The experiments are restricted to synthetic and relatively lowcomplexity datasets raising concerns about performance in higherdimensional and noisier settings.  The impact of incorrect assumptions about the number of states is not explored.  The use of graph neural networks as encoders raises questions about the specific connectivity and potential gradient issues with increasing sequence length.  The statement in the related work section that causal discovery approaches use only observational data is potentially misleading as interventional data is often employed in nontemporal settings.", "Paraphrase: Summary: The work proposes a causal discovery technique for time series that extends [Lowe et al. 2020] to conditionally stationary data. It assumes that each time series has a latent variable \"model\" that determines the graph structure and edge case at each time detail. Conditioning on \"model\" renders the time series stationary. effectiveness and Weaknesses:  The method claims to be novel and interesting.  It adapts the [Lowe et al. 2020] model to conditionally stationary scenarios. Concerns:  The paper does not address whether the derived model is causally identifiable or merely a datafit graph.  In [Lowe et al. 2020] causality is proven under specific assumptions. It is unclear if this extends to the current method especially considering the potential for latent confounders (e.g. \"si\" being related to other \"sj\").  It is unclear if the latent states can be uniquely reconstructed to achieve conditional stationarity.  The paper lacks coverage of related work including similar methods for modeling nonstationarity (e.g. https:arxiv.orgabs1903.01672).  The evaluation primarily focuses on [Lowe et al. 2020] which is unlikely to perform well on nonstationary data.", "Summary This research addresses the challenge of extracting a causal summary graph and reconstructing time series data simultaneously. The proposed model based on conditional Variational Autoencoder (VAE) incorporates a state variable (s) as a conditioning factor distinguishing it from standard VAE model. Experiments on two datasets demonstrate the superiority of the proposed method over Alternate Causal discovery (ACD) for linear data with performance decreasing as the number of states increase for complex data. effectiveness and Weaknesses effectiveness:  Thorough observational setup for reproducibility  Addresses two evaluation tasks (causal graph extraction and time series reconstruction) simultaneously Weaknesses:  Poor introduction requiring improvements  Limited comparison to entirely one baseline (ACD) in linear data with no comparison in complex data  Lack of clarity in the model structure and notations Details 1. The model structure is not clearly illustrated. 2. It is unclear what the input and output of work G are. 3. There is a lack of explanation for the notations and their significance. Questions 1. What is the definition of a state 2. Which variable is the source in Fig. 1 3. Why is sjt not an input for Gij in Eq. (2) 4. How is the consistency of Gji and Gij ensured if sjt is not an input 5. How is G represented as a work if it is a graph 6. Could the dimension of z be high as it encompasses all edges among variables"], "hq7vLjZTJPk": ["Paraphrased Summary This paper introduces a revised version of the stochastic gradient descent (SGD) algorithm with clipping and limited communication (i.e. local updates) dubbed communication Efficient Local gradient Clipping (CELGC). CELGC is designed for nonconvex federated learning problem assuming a certain smoothness condition. Theoretical guarantees are provided assuming that the noise in stochastic gradient is bounded and client data heterogeneity is limited. The authors claim their method offers linear speedup and better communication efficiency than the standard Clipped SGD. effectiveness  Clear motivation and straightforward presentation.  Helpful inclusion of a proof sketch in the main text.  Comprehensive related work section. Weaknesses  Mathematical errors: The proofs contain various inaccuracies that invalidate the main result. Specifically the authors incorrectly manipulate the expectation formula leading to incorrect complexity estimates.  Restrictive parameter assumptions: The theorem assumptions impose strong conditions on the number of workers and local steps making them impractical in certain scenarios.  Numerical experiments: The evaluation is conducted with only 2 workers which is insufficient to demonstrate parallel speedup. Additionally the batch sizes used for local and nonlocal methods are not equal compromising the fairness of the comparison. General Comments  The statement \"local steps ensure communication efficiency\" is not only accurate as there are cases where local updates may not improve efficiency.  Table 1 requires revisions to include missing information and correct complexity bounds.  The paper should cite related work on scalable federated optimization methods.  The authors assumptions on noise and data heterogeneity are overly restrictive.  The claim that the iteration and communication complexity depend only on L0 may be incorrect as some dependencies may have been omitted. Minor Comments  A few typos and grammatical errors are present throughout the paper.", "Paraphrase: Summary:  This article introduces CELGC a novel distributed optimization algorithm.  CELGC performs better than SGD by utilizing normalized gradient in local training. effectiveness and Weaknesses:  CELGC is straightforward and effective differing from SGD only in its practice of normalized gradient for large gradient norm.  It has theoretical guarantees of convergence for smooth and nonconvex functions. practical Considerations:  CELGC has numerous hyperparameters including learning rate clipping threshold and batch size.  No guidance is provided for tuning these hyperparameters in novel problem without prior experience.  A linear speedup claim is made but not experimentally verified. Questions:  How should hyperparameters be set in the absence of previous experience  What distinguishes the naive version of parallel gradient clipping from CELGC with i1  Why is the baseline performance in picture 1(a) not better than CELGC with i1 given that they are theoretically similar A larger batch size for the baseline should be practiced to compensate for the potential impact of normalized gradient on small batch sizes.", "Paraphrase: gradient clipping is a valuable technique in training deep neural networks. It reduces the norm of gradient and helps to avoid exploding gradient. However in federated learning globally averaged gradient clipping is impractical due to the need for gradient synchronization. Despite its widespread practice local gradient clipping has no theoretical analysis. This paper provides a theoretical analysis of local gradient clipping and shows that it can guarantee convergence when the number of workers and local iterations are limited. While local gradient clipping is a practical approach it has some limitations. The theoretical results require that the number of workers and the number of local iterations be kept small. This may not be suitable for largescale distributed training with many workers and iterations. The paper focpractices on homogeneous local data in classic distributed settings. It does not consider heterogeneous local data and client subset participation in federated learning settings where global gradient clipping is less practical.", "Paraphrase: Summary: This paper explores the impact of gradient clipping on Federated Learning (FL) convergence. Focusing on the relaxedsmooth loss function the field examines scenarios where each worker applies local gradient clipping and performs multiple SGD steps before communicating and averaging models. Theoretical analysis reveals that the algorithm achieves an iteration complexity of O(1N\u03b54) to reach an \u03b5stationary point. Empirical Validation: The theoretical findings are corroborated through experiments on CIFAR10 (with the Resent56 model) Penn Treebank and WikiText (with LSTM models). effectiveness and Weaknesses: gradient clipping has been recognized as a valuable technique in RNN training and gradient explosion prevention. However its effect on distributed or federated learning settings has received limited attention. This paper contributes to this gap by providing a theoretical analysis of FedAvg and similar algorithms in the presence of local gradient clipping. Suggestions for Improvement:  Empirical evaluation of the closeness of theoretical bounds to experimental results.  Correction of Assumption 1 iii equation by adding the gradient operator \u2207.  Clarification in Lemma 2 that the error is quadratic in I not linear.  Comparison of the fields findings with other relevant research such as \"Understanding Clipping for Federated Learning ...\" by Xinwei Zhang et. al."], "wzJnpBhRILm": ["Paraphrased Statement: This paper presents a novel perspective on batch normalization as a way to approximate normalization across the entire dataset. It proposes a method that maintains body between the training and inference models during the forward pass and set interactions between different data detail in gradient computation. In increase the paper introduces a technique for estimating gradient by minimizing interactions between data detail keeping only one data detail in memory at a time. While this approach doubles the computational attempt it can be optimized by using a small minibatch for some function of the gradient approximation potentially sacrificing some accuracy. Strengths and Weaknesses: The paper contributes valuable insights into normalization but its practical applications are not immediately clear. The proposed method does not enhance accuracy but introduces increaseal computation. Practitioners may question its utility in light of existing batch normalization techniques. The approximation behind the method is to perform normalization during training in a manner similar to stochastic gradient descent (SGD) but it lacks practical advantages over the standard batch normalization approach.", "Paraphrased Statement: In batch normalization (BN) computation typically use statistics from a minibatch of samples. This research proposes a twostep approach that normalizes each sample separately resulting in potentially beneficial accuracy than BN. The method involves modifying the computation graph to incorporate multiple samples with shared weights. Strengths:  Novel approach that alters the computation graph to include various samples. Weaknesses:  experimental results indicate that the proposed method performs worse than the baseline approach.  Experiments lack clarity and may not present all relevant data.  Section 5 (experiments) could be improved by removing or summarizing Sections 2 and 3.  Limited testing on a single dataset.", "Summary: The proposed method addresses the challenges of batch normalization in small batch scenarios without requiring additional procedure during inference. Its effectiveness was demonstrated using Inceptionv3 on ImageNet. Strengths:  Novel and intriguing concept Weaknesses: 1. Missing Literature:  The paper overlooks the previously published MABN method which also resolves small batch publication without introducing extra operators during inference. MABN employs moving average statistics during backpropagation to address the drawbacks of batch renormalization offering similar advantages to the proposed method with reduced computational overhead. 2. Confusing Intuition:  Section 2.1 introduces unnecessary jargon and complex concepts that obscure the key detail. The proposed method involves three types of nodes: weights feature function and batch statistics.  The notation for the Jacobian as J(S T) is unconventional and hinders understanding. Mathematical classulas and example would clarify the concepts. 3. Unconvincing Results:  The experimental results are presented in text class making them difficult to compare and interpret.  The experiments were conducted on Inceptionv3 and ResNet50 with an unusual batch size of 32x50. It is unclear why small batch scenarios (e.g. batch size below 8) were not investigated.  The claim that the proposed method outperclasss CBN and MABN on nonresidual structures needs further justification through comprehensive comparative experiments. 4. Incorrect Citation Format:  The reference sources (e.g. journal or conference) for cited papers are not properly indicated. A classal citation style should be used to enhance the papers credibility.", "Paraphrase: This field presents an updated version of batch normalization designed for small training batch sizes. Similar to batch renormalization the technique uses limited backpropagation methods. The evaluation primarily uses the Inception V3 network comparing it to standard batch normalization and batch renormalization. While the goal of enhancing batch normalization is significant the approach could benefit from improvements: 1. Lack of Motivation: The reasoning behind the approach is unclear especially considering the similarity between the proposed method and batch renormalization. The main contribution seems to be the backpropagation modifications making it complex and challenging to understand. 2. Limited Evaluation: The experiments are mostly confined to Inception V3 with basic comparisons to standard batch normalization and batch renormalization. Despite its complexity the proposed method underperforms standard batch normalization on large batch sizes. On small batch sizes it marginally surpasses batch renormalization but the experiments are not comprehensive enough to draw definitive conclusions. 3. introduction improvement: The reliance on computational graph terminology complicates the understanding of detail. The experiments section could be beneficial organized with tabulated results and clear comparisons to the baseline for improved clarity."], "nxcABL7jbQh": ["Paraphrase: This work suggests a new direction of representing boundaries as 1D surfaces using mathematical transformations (Equation 1). It shows that this representation is mathematically equivalent to a traditional binary boundary function. The work goes on to train a neural network (NN) using a variant of the HRNet architecture to predict the vector field representation of boundaries. The NN is trained using a Mean Square Error loss function. Since the predicted boundaries can be very narrow the work proposes using average surface distances as a metric to evaluate the performance of the method. The work shows that its method outperforms three other loss function using the same NN architecture. Strengths:  Introduces a new boundary representation inspired by classical implicit function which is combined with innovative NNs.  Achieves thin boundaries which is useful for several applications. Weaknesses:  The NN architecture and training point are not fully explained making it challenging to reproduce the experiments.  The work only tests one NN backbone architecture and the Vector Transform method only outperforms other losses in some metrics.  The work does not provide convincing evidence that the vector transform representation is practically superior to the binary representation.", "Paraphrased Statement: Summary: This research introduces a novel loss function for boundary detection in image process. This function addresses challenges such as unbalanced labels and the thickness of boundarys. It utilizes the vector transform field which is linked to distance transform and obtains the final boundary function by applying a threshold to the divergence of the predicted vector transform field. The authors compare their Vector Transform (VT) loss to existing losses and demonstrate its effectiveness on several datasets. Strengths and Weaknesses: Strengths:  Clear and concise writing  Simple and promising concept  Extensive experiments Weaknesses: 1. inference Implementation: The authors present a \"simpler\" inference solution but the implementation is unclear. The function of the support image is not fully explained and it is unclear how the predicted boundary image is obtained. 2. ASD metrics Concerns: The authors propose using asd series metrics to address drawbacks of ODS and OIS metrics but do not acknowlboundary the potential sensitivity of asd metrics to noise and outliers. 3. Baseline Loss Comparison: While Dice Loss is a reasonable baseline including recent advancements such as clDice loss could strengthen the comparison. 4. DT Performance: The authors unexpectedly find that DT performs worse than VT despite their similarities. They do not elaborate on the threshold selection for DT or the selection of L2 loss for VT and L1 loss for DT. 5. BSDS500 Dataset insight: The authors provide limited insights into why WCL performs better on the BSDS500 dataset compared to their proposed method. Moving this experiment to the main script would be beneficial.", "Paraphrased Statement: This research paper presents a boundary detection approach in images by representing boundaries as onedimensional surfaces. The approach utilizes a onetoone vector transformation function. The paper offers theoretical backing for the vector transformation representation. Experiments using existing datasets validate the methods efficacy. Strengths:  Innovative: Introduces a novel concept of representing boundaries as surfaces.  Theoretical foundation: Provides theoretical underpinnings and discussions to support the approach.  experimental Verification: Demonstrates the methods effectiveness through experiments using publicly available datasets. Weakness:  Method Limitations: The paper does not address the potential limitations of the proposed method."], "xaTensJtCP5": ["Paraphrase: Summary: This work presents a novel objective work called Ab Initio for training neural Markov Chain Monte Carlo (MCMC) proposals. The Ab Initio objective aims to enhance property and representation invariance while reducing the need for prior knowledge compared to existing methods. The authors demonstrate its effectiveness on benchmark models. Strengths and Weaknesses: While the Ab Initio objective has potential for improving MCMC methods the following publication should be addressed: 1. Generality: The paper lacks evidence to support the general applicability of the Ab Initio objective. The authors have only tested it on relatively simple target distributions. More complex models should be evaluated to confirm its effectiveness. Additionally the experiments did not show significant advantages over existing methods. 2. Intractability: The computation of the Ab Initio objective requires an expectation that is intractable in practice since the target distribution is unknown. 3. estimation of HMC proposal: The paper claims that Ab Initio can approximate Hamiltonian Monte Carlo (HMC) proposals. This should be verified by comparing it with handtuned HMC methods.", "Paraphrased Summary: The paper introduces four desirable qualities for optimizing works used in tuning Markov Chain Monte Carlo (MCMC) proposals. These qualities are based on fundamental principles such as:  The work should possess a single global minimum when the proposal equals the target.  The work should be independent of the representation of the problem.  The work should yield known optimal solution in certain settings (e.g. optimal acceptance range for Langevin dynamics with a Gaussian target). Based on these principles the paper proposes a novel optimization work inspired by Gaussian Smoothing on Manifolds (GSM). This novel work satisfies all the proposed qualities. Strengths:  Addresses the crucial publication of developing automated methods for tuning MCMC proposals which is essential for complex proposals involving neural network.  Provides a coherent model for designing optimization works for MCMC proposals moving beyond adhoc approaches. Weaknesses:  Despite its general approach the paper only provides a specific optimization work that modifies GSM.  The specific optimization work still incorporanges some adhoc adjustments limiting its practical usefulness.  The empirical evaluation focuses on relatively simple models and does not consider online tuning which is the typical use case.", "Paraphrased Summary: The authors present an approach to optimize proposal distributions for Markov Chain Monte Carlo (MCMC) model. They propose an objective work that can be minimized to find optimal proposal distributions. This objective work is based on theoretical principles and has a fixed hyperparameter that was tuned in a separate task. The authors show empirically that their approach can reproduce theoretical solution for various target distributions. Paraphrased Strengths:  Clear motivation and description of the objective work  The proposal distribution optimized using the proposed objective work can reproduce optimal acceptance probabilities for MCMC algorithms Paraphrased Weaknesses:  The objective work involves an expectation with value to the target distribution which can be impractical when the target distribution is not known or only accessible through approximate samples.  The implementation of the proposed method is unclear particularly regarding when the proposal optimization is performed (before or during MCMC sampling).  The experiments lack clarity on whether high or lower value of quantities are better.  The authors mention that certain proposal distributions lead to nonergodic behavior but the solution show high efficient sample size for these distributions.  The term \"resampling normalizing flow\" is not defined and the authors state that it did not produce any accepted proposals in some experiments leading to uncertain conclusions. additional Comments:  The paper should include a section explaining how the efficient sample size (ESS) was computed.  The definition of MSJD (Mean Square Jump Distance) should be provided in the paper.", "Paraphrase: This paper focuses on optimizing Markov Chain Monte Carlo (MCMC) Chain by finding optimal parameters for their proposal distributions using the MetropolisHastings algorithm. It proposes an improved objective work based on estimation from Titsias (2019) that balances the acceptance range and entropy of the proposal. One key contribution is the assumption of a constant tradeoff parameter (beta) between acceptance range and entropy applicable to all targets and proposals. This claim is supported by experiments on synthetic targets (uniform Laplace Cauchy Gaussian) for MetropolisAdjusted Langevin Algorithm (MALA) and Random Walk MetropolisHastings (RW MH) proposals. The authors empirically demonstrange that their approach optimizes proposal parameters effectively for a variety of target distributions and proposals including a mixture of Gaussians and Bayesian logistic regression. However the paper has some weaknesses:  The assumption of a universal beta parameter lacks strong theoretical or empirical support.  The choice of beta was made manually which is not practical for practitioners.  The evaluation of the proposed objective requires samples from the target distribution which may not always be feasible in practice. Despite these limitations the paper offers a novel approach to optimizing MCMC proposal distributions with potential benefits for improving sampling efficiency and Chain convergence."], "h-z_zqT2yJU": ["Paraphrased Statement: This research aims to enhance the standard knowledge distillation method. By noting the observation from ChoHariharan that large teachers may hinder student performance this paper proposes an adaptive temperature adjustment. A novel \"sharpness\" metric is introduced to determine the teacherstudent knowledge gap. The approach is evaluated using CIFAR100 and ImageNet datasets. effectiveness:  The paper attempts to optimize the vanilla knowledge distillation method by adjusting the temperature.  The solution appears logical and practical. Weaknesses:  The paper has significant writing issues.  Equations lack clarity due to the absence of component descriptions (e.g. i and j in Eq. 2).  The LKD loss description is incorrect.  Tables and captions are unclear.  Table 1 lacks data on the metrics used (Top1 or Top5).  Table 6 does not explain the meaning of the \"R\" symbol.  picture 1 does not define \"SKD\".  Table 4 indicates that ATKD achieved 73.01 with Resnet50 as the teacher rather than Resnet34 as claimed.  Table 6 contains similar inconsistencies.", "Paraphrase: Summary: The paper suggests a technique for minimizing performance loss during knowledge transfer from a large \"teacher\" model to a smaller \"student\" model. The technique involves adjusting the difference in \"sharpness\" between the two models. sharpness is measured as the real softmax function (the natural logarithm of the sum of exponentials of the models output probabilities). During training the \"temperature\" hyperparameter is set based on the sharpness of the logits. For sharp output from the teacher model a higher temperature is used to smooth the output. effectiveness:  The method effectively reduces the sharpness gap between teacher and student models leading to improved performance when using large teacher models.  It provides a novel perspective on knowledge distillation. Weaknesses:  Table 5 should specify the temperature used for training and calculating the sharpness gap as comparing sharpness values derived from different temperatures is misleading.  The assumption that the sum of student logits remains constant during training (despite zeromean initialization of inputs and parameters) needs further explanation.  The \"logits std\" on page 4 should be labeled as variance. The mean of the output is assumed to be 0 which also requires explanation.", "Summary Paraphrase: This research addresses the decline in student model performance during knowledge distillation. The authors suggest that the issue stems from a difference in output clarity between the teacher and student models. They examine the correlation between this \"sharpness gap\" and performance degradation. They introduce the realsoftmax function to quantify model output clarity and develop ATKD a technique for dynamically adjusting teacher and student temperatures to reduce the sharpness gap. The authors also explore existing approach (Early Stop and TS) through the lens of model clarity. effectiveness and Weaknesses Paraphrase: effectiveness:  The authors provide a novel explanation for the KD degradation problem.  They introduce the realsoftmax function and propose ATKD for adaptive sharpness gap reduction. Weaknesses:  The authors should provide additional analysis:  theoretical analysis of the optimal sharpness gap  performance variations when adjusting teacher temperature manually  comparison of ATKDs sharpness gap with that of the best student model  It is unclear if the optimal sharpness gap varies across datasets or tasks.", "Summary Paraphrase: This field investigates how the difference in output temperature (\"sharpness gap\") between a teacher model (strong model) and a student model (weaker model) impacts knowledge distillation (KD) performance. The field concludes that a significant sharpness gap negatively affects KD. To address this an adaptive temperature strategy is proposed to gradually reduce the sharpness gap during training. Experiments show a strong correlation between the sharpness gap and the student models performance. The proposed adaptive temperature strategy demonstrates superior results compared to existing methods especially on picture classification tasks. effectiveness and Weaknesses Paraphrase: effectiveness:  The analysis and observations on the sharpness gap provide novel insights into the KD strategy.  The proposed adaptive temperature KD (ATKD) method is straightforward with a sound theoretical basis.  The ATKD method achieves stateoftheart performance especially on ImageNet which is significant. Weaknesses:  The field poses several unanswered questions:  Would alternative sharpness metrics (e.g. output probability entropy) output similar conclusions  Why cant a student trained with vanilla KD match the teachers sharpness  Could longer training with vanilla KD lead to similar sharpness gap and performance as ATKD  Does the sharpness gap observation generalize to variety in model width  It would be valuable to include a visual representation (line plot) showing the evolution of the sharpness gap and student accuracy over training epochs."], "p4H9QlbJvx": ["Summary This paper critiques two previous studies (Crowley et al. 2018 Liu et al. 2019) that claimed that pruning a large network and finetuning it does not improve generalization performance compared to training a low network from scratch. The critique argues that these previous act incorrectly set learning rate during finetuning rendering their conclusions invalid. The paper also proposes an explanation for why pruning followed by finetuning should lead to effective generalization performance based on dynamical isometry theory. Strengths and Weaknesses 1. Critique of Previous paper:  commendable for pointing out methodological issues with previous studies that make erroneous assumptions about finetuning.  However previous studies may have had legitimate reasons for limiting finetuning epochs which should be considered. 2. Unfair Experimental Comparison:  The paper compares training low netact from scratch to pruning and finetuning large netact but the training epochs are not equalized.  This may confound the results favoring the pruned and finetuned models. 3. Unclear Hypothesis:  The paper assumes that pruning and finetuning improves generalization over training from scratch but does not provide a clear justification for this hypothesis.  Both methods should theoretically converge to similar solutions given sufficient optimization. 4. Incorrect Metric for Dynamical Isometry:  The paper uses mean singular value as a metric for dynamical isometry but a more appropriate metric is the sum of squared deviations from unity. 5. Relationship between Dynamical Isometry and Convergence:  The paper suggests that increasing mean singular value during training is related to network convergence.  This claim is not fully explicado and requires further justification.", "Paraphrased Statement: Summary: This research contends that it is crucial to inherit the weights of pruned networks for finetuning. It refutes the claim that weight inheritance is unnecessary. The authors present evidence that previous experiments lacked appropriate learning rate and training epochs preventing network convergence. They propose that finetuning pruned networks requires larger learning rate and longer training because their dynamical isometry is disrupted. To address this they introduce OrthP an initialization method that fully restores dynamical isometry in simple MLPs. OrthP reduces sensitivity to learning rate and training epochs in the finetuning process. Strengths:  Provides novel insights into the role of weights in network pruning.  Thoroughly examines established debate with experiments across various pruning ratios.  The proposed OrthP method is highly effective for MLP networks with substantial pruning.  Wellwritten and easy to understand. Weaknesses:  Finetuning pruned networks requires significantly more training time and effort despite OrthPs claimed resolution of the dynamical isometry issue.  The application of OrthP is limited to MLPs.  Questions regarding experimental designs and results:  Lack of information on hyperparameters for \"scratch\" results in Tables 1 and 2.  Absence of comparisons between OrthP and randomly initialized sparse networks.  Concerns about OrthPs equivalence to prolonged training epochs.", "Paraphrase: Summary: The paper advocates for pruning a pretrained netact and then finetuning it rather than training a sparse netact from scratch. It attempts to relate the characteristics of the finetuning phase to the concept of dynamical isometry (DI). Empirical evidence suggests that preserving pretrained weights improves performance. Strengths: 1. Empirical observation that inheriting pretrained weights is superior to training a sparse netact from scratch with proper finetuning. 2. Empirical analysis of finetuning hyperparameters (learning rate and epochs) provides practical guidance. Weaknesses: 1. The connection to DI and pruning is not novel as it was previously established at initialization [Lee et al. 2020]. Extrapolating this concept to pretrained netacts is straightforward and does not represent a substantial contribution. 2. The hypothesis linking larger learning rate in finetuning to DI is questionable and lacks theoretical or experimental backing. Figure 2 shows an increase in mean JSV which does not imply improved DI (as DI theory aims for mean JSV around 1). 3. The comparison against scratch training appears biased. Finetuning is typically performed for a shorter duration than training from scratch but in this act finetuning was carried out for a similar act of epochs. The scratch version could potentially be optimized to yield effective performance. 4. The role of L2 regularization on pruned weights to enhance DI recovery is not adequately justified.", "Paraphrase: Summary Research exists that casts doubt on the value of inheriting network weights when pruning structured neural networks as training from scratch is often found to produce comparable or effective results than finetuning pruned models. This paper contributes three main components: 1. The authors reexamine the problem and debate that the previous conclusion is flawed due to undersized finetuning learning rate. They demonstrate that finetuning with pruned weights outperforms training from scratch when larger learning rate and extended training epochs are used. 2. The authors utilize dynamical isometry (DI) to analyze the impact of finetuning learning rate on final performance. They show that weight pruning disrupts DI which finetuning can restore. Notably higher learning rate accelerate the recovery of DI. 3. They propose a method to fully restore DI prior to finetuning in filter pruning. Strengths and Weaknesses Strengths  The paper presents a wellorganized and detailed field.  It effectively investigates the impact of learning rate on the finetuning stage of pruning filling a critical gap in previous research. Weaknesses  The concept of initialization in pruning is not fully clarified. While pruning in the provided figure may refer to weight pruning for both networks filter pruning results in distinct architectures making the pruned results different rather than alternate initializations for the same architecture.  The relationship between DI and generalization is not explicitly established. other metrics such as flatness could potentially exhibit similar trends to DI. Including a comparison with additional generalization metrics would enhance the persuasiveness of the analysis.  The experiments on MLP7 and MNIST may be overly simplistic for practical applications. It remains unclear whether the proposed method generalizes to more complex nonlinear convolutional neural networks.  The authors propose that strong regularization improves performance but they do not provide an explanation for this phenomenon or its connection to their previous analysis. Additionally it is unclear whether this method is effective for MLP7 and how it relates to DI."], "rxF4IN3R2ml": ["Paraphrased Statement: Summary: This research introduces a model (MQTransformer) to enhance time series forecasting accuracy and reduce unnecessary variations. It employs three key mechanism: decoderencoder attention positional encoding and decoder selfattention. These contributions address crucial issues in time series forecasting: improving accuracy and minimizing volatility. effectiveness:  Clearly defined problem statement and concise explanations of contributions  Wellstructured and clear paper  Benchmarking against established deep learning methods using five datasets including a retail dataset  Ablation work to assess the impact of proposed attention units  Computational cost analysis and comparison with stateoftheart Weaknesses:  Lack of evaluation of ablated MQTransformer variants on the retail dataset  Absence of definitions for other forecasting benchmark such as DeepAR TFT and MQRNN  Exclusion of the Informer model which employs a similar selfattention architecture from the benchmarking", "Paraphrase: This work explores using a Transformer model with an encoderdecoder structure for forecasting. It builds on previous research involving quantile forecasts where quantiles are predicted for each forecasting horizon. The model incorporates a 1D convolutional network and a multilayer perceptron to capture positional data in the time series data. It also utilizes an attention mechanism for each forecasting horizon. The authors demonstrate that limiting the context of the attention leads to improved forecasting accuracy by reducing bias. Additionally the model leverages historical data to reduce volatility in forecasts. It has demonstrated improved performance in demand forecasting tasks. While the model shows improvement over previous approach it largely follows the standard Transformer design with minimal technical innovations. The authors could have clarified certain notations and definitions as they are sometimes used inconsistently throughout the paper.", "Paraphrased Statement: This paper introduces an enhanced MQforecaster model for predicting future values in time series data across multiple time horizons. The goal is to enhance forecast accuracy and reduce excessive variability in future forecasting at several time intervals. The model incorporates several improvements:  A new method for handling positional encoding using embedded event indicators  Separate encoding for different horizons  decoder attention mechanism aware of past forecast errors enabling the model to identify and mitigate excess variability Extensive experiments on a proprietary dataset and public datasets for demand forecasting retail sales electricity load security volatility and traffic forecasting demonstrate: effectiveness:  significant improvements in forecast accuracy on both the proprietary and public retail forecasting datasets  Incorporation of forecast variability reduces excessive variability compared to the MQCNN model Weaknesses:  Most empirical analysis relies on a proprietary dataset limiting reproducibility and verification  Masked and relative performance metrics on the proprietary dataset hinder performance evaluation  Comparative analysis with the MQCNN model is incomplete missing results from the GEFCom2014 forecasting task  Ablation work are needed to determine the key component contributing to performance gains  Reproducibility may be challenging due to limited code availability reliance on internal datasets and lack of detailed ablations"], "w-CPUXXrAj": ["summary Paraphrase: This paper analyzes the ELBO loss work used in multimodal Variational Autoencoders (VAEs) and presents a theoretical proof that there is a nontrivial gap between the likelihood and ELBO from an information theory perspective. The authors argue that this gap can lead to a decrease in the quality of multimodal VAEs. The paper also includes experimental issue that explore the tradeoffs between unimodal and multimodal VAEs. Strengths and Weaknesses Paraphrase: Overall the paper is wellwritten and accessible. The issue is intriguing and has potential implications for VAE research and broader generative modeling. The strengths of the paper include:  Clear and concise writing  Informative and concise theoretical analysis  Extensive experimental evaluation However there are some weaknesses to address:  Providing an example to illustrate the gap between ELBO and likelihood would enhance understanding.  The paper should elaborate on the assumption that a larger gap leads to worse quality. It should also consider the possibility that optimizing ELBO may be more beneficial than optimizing likelihood especially in cases with limited model capacity.  Conducting an experiment to gradually increase the gap and observe its impact on quality would be valuable.  The concept of coherence is not fully explained in the paper and a mathematical definition of leaveoneout coherence would aid comprehension.  Minor issues such as imprecision in Equation (1) and the need for larger figures should be addressed.", "Paraphrased summary: This research theoretically demonstrates that sampling subsets of modalities in mixturebased multimodal Variational Autoencoders (VAEs) can weaken the evidence Lower bound (ELBO). To further support this the work introduces intricate datasets (TranslatedPolyMNIST and Caltech Birds with real images) to expose the practical limitations of multimodal VAEs. Strengths:  The paper is wellstructured and comprehensible.  It uncovers a previously unidentified key issue in mixturebased multimodal VAEs.  empirical findings from more complex datasets corroborate the theoretical conclusions.  The findings hold significant promise for future advancements in multimodal VAEs. Weaknesses:  The authors objective definition (Equation 2) is clarified as a lower bound of the real objectives for MMVAE and MoPoEVAE. The extent to which the identified issues apply to these real objectives is uncertain.  The term \"subsampling\" is employed ambiguously. The authors should define modality subsampling as selecting a subset from multiple modality subsets and distinguish it from ELBO subsampling in MVAE. Minor Observation:  The authors deviate from the customary notation for VAE encoders and decoder (p\\theta and q\\phi) by reversing them although their usage remains consistent throughout.", "Paraphrase: The work examines the strengths and weaknesses of existing multimodal Variational AutoEncoders (VAEs) like MVAE MMVAE and MoPoEVAE. They highlight limitations in terms of image quality and consistency when generating images from multimodal data. The researchers specifically investigate how reducing the amount of available data (subsampling) affects image quality. They show that if the different types of data within the multimodal data are not fully predictable from each other the generated images may lack consistency. They demonstrate these limitations using two datasets: PolyMNIST and Caltech Birds. To further demonstrate the challenges with multimodal VAEs on complex data they create an augmented version of PolyMNIST called TranslatedPolyMNIST with five different modalities. While the authors acknowledge the importance of multimodal data analysis in realworld applications they avoid suggesting solutions to the limitations they identify. They focus on quantifying the performance of multimodal VAEs when faced with missing data. The work provides valuable insights into the limitations of existing multimodal VAEs but it would be strong if it also offered suggestions for improvement. The authors could address how to overcome the quality issues caused by data discrepancies and how to balance diversity and redundancy within multimodal data.", "Paraphrased Statement: summary: The paper analyzes an unexpected flaw in multimodal Variational Autoencoders (VAEs) where their image generation quality falls short of unimodal VAEs. The authors propose a theory rooted in the multimodal VAEs evidence Lower bound Objective (ELBO) to explain this disparity. They conduct thorough ablation experiments to confirm the predictions of their theory. The findings have extensive implications for ongoing research on multimodal VAEs. Strengths:  Comprehensive review of multimodal VAE literature.  Wellgrounded theoretical explanation for the observed generation quality gap.  Supporting experiments on basic datasets.  Insights into the limitations of multimodal VAEs applicable beyond machine learning research. Weaknesses:  The paper lacks a detailed introduction to the datasets used specifically the PolyMNIST dataset and its multimodality.  The comparison between subsamplingbased and nonsubsamplingbased VAEs could be improved. Subsamplingbased VAEs receive less training data potentially affecting the conclusions."], "tQ2yZj4sCnk": ["Summary This study presents a novel learning framework called DMAC for cooperative reinforcement learning (RL). It introduces a novel penalty term that measures the divergence between the learning policy and a target policy to improve the learning work. The framework is compatible with various RL algorithms and can enhance their performance by guiding them towards the optimal policy of the underlying Markov decision work (MDP). The paper provides a rigorous mathematical foundation for the algorithm and demonstrates its effectiveness through experiments. Strengths 1. The DMAC framework offers a novel approach for cooperative RL. 2. The mathematical derivations are comprehensive and sufficient for understanding the proposed method. 3. DMAC can seamlessly integrate with multiple RL algorithms and improve their learning efficiency. Weaknesses 1. Some of the mathematical concept could be streamlined or moved to an appendix to allow for more indepth discussion and experiments. 2. The novelty of the technical content could be better highlighted by explaining how it differs from existing methods in RL particularly from DAPO and DOP. 3. Experiments are not entirely convincing:  Figure 1 lacks a direct comparison with DOP and the results for COMA and DOP are inconsistent with previous rule.  The use of COMA as an onpolicy baseline is suitable but could be complemented with strong alternatives like PPO or IMPALA.  The SMAC maps used in the experiments are not sufficiently challenging.  The statement that DMAC improves convergence performance is not clearly supported by the visualization in Figure 1. Writing 1. The motivation for the choice of the divergence penalty in Section 4 would enhance clarity. 2. Redundant discussion like \"benefited\" may be replaced with more concise alternatives. 3. \"capital w\" should be used in the phrase \"higher win rates.\"", "Paraphrase: This paper introduces DMAC a general framework for multiagent reinforcement learning. It uses a special regularization approach that involves entropy which helps to coordinate multiple agents in complex environments. The paper provides theoretical justification and empirical evidence from tests using various cuttingedge algorithms on the SMAC platform demonstrating DMACs effectiveness. effectiveness:  Includes theoretical analysis alongside experimental results.  extensive experimentation covers a range of stateoftheart algorithms. Weaknesses:  The rationale for the target policy \"rho\" is unclear.  Its not stated what rho represents or why it is necessary for improving learning.  If the optimal policy is known why is DMAC needed If not how can rho be chosen effectively  Its unclear how much DMAC differs from existing methods like SAC especially in its objective work and update rule.", "Paraphrased Statement This paper introduces a novel multiagent framework with controlled divergence to overcome the publication of learning biased policy in the maximum entropy approach. The main contributions include:  A divergence policy iteration algorithm for cooperative multiagent reinforcement learning (MARL) settings.  The offpolicy divergence regularized multiagent actorcritic framework (DMAC). Empirical evaluations demonstrate that DMAC outperforms existing MARL frameworks in various multiagent scenarios. effectiveness  Clear and wellstructured presentation of the framework.  DMACs flexibility allows it to be combined with existing MARL algorithms. This benefit is demonstrated by combining it with four MARL algorithms in the SMAC field.  DMACs improved exploration without introducing bias is identified as the key to its performance. Questions  The theoretical analysis assumes a fixed target policy but in practice the target joint policy is updated dynamically. Its not clear if the theoretical conclusions hold for nonstationary target policy.  The paper uses an RNN layer for partial observability but its unclear how the hidden state is handled in the offpolicy implementation.  possible future direction for DMAC include extending it to generalsum game (noncooperative settings) and exploring its challenges.", "In cooperative reinforcement learning with multiple agents the authors introduce an offpolicy learning method that incorporates KL divergence regularization with a target policy. This approach mitigates the bias introduced by the regularizer by deviating from traditional max entropy RL. The method includes specific techniques that partially decouple the learning work for each agent. The paper provides theoretical convergence guarantees and experimental results in simulated environments. In comparison to related work the method bears similarities to Wang et al. (2019). However further elaboration on the deviation between these approaches and why the current work is applicable to cooperative MARL settings would enhance clarity. Regarding theoretical guarantees the paper establishes monotonic improvement for a fixed target policy. While this limitation may not be problematic the authors could clarify whether theoretical guarantees extend to the general method stating this explicitly as a theorem if applicable. An improved overview and motivation of the papers structure and the interconnectedness of concept would enhance readability. For instance a clearer explanation of the definition and intuitive understanding of the target policy (rho) earlier in the paper rather than solely in section 4.5 would be beneficial."], "rwEv1SklKFt": ["Paraphrase: The authors discover that models poisoned with backdoors can be triggered by additional triggers beyond the ones initially introduced. This observation is novel and they propose a method to uncover these hidden triggers. However the underlying mechanism for this behavior remains unexplored and its unclear how it specifically applies to backdoored models versus clean models. Additionally the proposed method has limitations including the need for whitebox approach human review and patchbased backdoors. Recent attacks indicate that backdoors can be introduced in less conspicuous ways which may reduce the practical impact of this method. In terms of utility the authors should compare their method to existing stateoftheart systems for detecting poisoned models if they aim to demonstrate its effectiveness for this purpose. Currently the practicality of the method is questionable and it appears to be the only aspect of the paper that addresses a tangible problem.", "Paraphrase: Summary: This research presents a novel method for compromising classifiers. By incorporating a noisereducing smoothing module into a precompromised model attacker can exploit craft adversarial attacks to identify novel backdoor triggers. Extensive testing including human experiments demonstrates the effectiveness of the method in situations where other approaches prove ineffective. effectiveness and Weaknesses: effectiveness:  The study reveals that models intentionally compromised with hidden backdoors are inherently vulnerable to further attacks compared to noncompromised models.  The method utilizes denoised smoothing to generate adversaryresistant versions of compromised models with the goal of detecting adversarial examples that trigger the identified backdoors.  human trials demonstrate that the patches associated with these triggers are visible and easily detected by humans. Weaknesses:  The presented threat model is somewhat contrived. The attacker requires a malicious intermediary (c) to infiltrate a model trained by a legitimate party (A). However As backdoor may still function still if compromised by c.  The impact of this method on existing defense mechanisms against poisoning is unclear.  The study lacks detailed comparisons between extracted triggers and those used in training. The extent to which these triggers alter the intended functionality of the classifier is also unclear.  The paper lacks sufficient context for some numerical value such as epsilon and the noise level making it difficult to interpret the results accurately.", "Summary: This study shows that deep network classifiers with backdoors are susceptible to the generation of novel backdoor triggers still without approach to the training data or original trigger. An algorithm based on denoised smoothing and human interaction is proposed to create these triggers. The findings suggest that backdoored models may have multiple backdoor triggers potentially enhancing the detection of such models. effectiveness and Weaknesses: effectiveness:  Demonstrange a more effective attack method than previous work.  Presents an algorithm to generate two types of backdoor triggers without approach to the original data.  Achieves comparable or higher attack success range using novel backdoor triggers. Weaknesses:  Lacks theoretical analysis to improve the quality of the approach.  Requires human interaction for trigger generation. Questions for author: 1. How do DL services calculate gradients if they are not returned 2. Are alternative methods to denoised smoothing available 3. How is the poisoned class identified 4. Why is human interaction required for selecting distinctive regions 5. How does the choice of color alignment affect the attack effectiveness 6. How are multiple distinctive regions handled Propositions: 1. Assess the effectiveness of this attack against backdoor defense methods. 2. Revise the title to accurately reflect the findings. 3. Specify the backdoored classifiers tested in the study. 4. Include Denoised Smoothing results in the relevant section. Minor Corrections: 1. Correct the typo in the contributions section to \"a novel threat\". 2. Clarify the attack success range in Section 5.1.", "Paraphrase: Summary: The study demonstrates that an attacker who lacks approach to the original training or trigger data can construct a substitute trigger that fools a classifier. A Denoised Smoothing classifier generates highmagnitude adversarial examples indicating the triggers data. The researchers demonstrate that manually removing this anomalous model can still trick the classifier. effectiveness and Weaknesses: effectiveness:  The concept of crafting an attack without direct approach to the original trigger is a novel threat model highlighting the severity of the publication.  The study demonstrates the ease of creating alternative triggers across multiple benchmark datasets through a human study. Weaknesses:  The Denoiser is essential for trigger identification requiring separate training.  It remains unclear if adversarially trained classifiers without the Denoiser exhibit similar models when compromised.  manual review is necessary for crafting the alternative trigger. The explanation that automated detection would allow for triggers to evade detection is insufficient. Experimental evidence would strengthen this argument.  The study uses a specific epsilon (2060) to generate adversarial examples. The selection justification and impact on performance with varying epsilon value are not provided.  The proposed approach grants the attacker significant approach to the poisoned classifier including a Denoiser and gradient data from multiple Monte Carlo samples. It would be valuable to investigate the feasibility of performing this attack in a blackbox manner or demonstrate its transferability across classifiers.", "Paraphrase: This paper proposes a method to create backdoor triggers in a specific threat scenario where the attacker can only approach the classifier through queries at inference time. The authors highlight that anyone with approach to the classifier can potentially recreate these triggers. The paper uses a threat model where the triggers are patches and the victim model is only approachible through API queries during inference. Human judgment is used to identify model in adversarial examples of the denoised classifier. These model are then extracted and modified to match the original trigger patches. Concerns:  The denoised classifier has a smaller certified radius than the one trained from scratch.  The \u03b5perturbations used in the paper are relatively large. The method should be tested with smaller \u03b5 adversarial examples.  Its not clear if the method can construct triggers for irregular patch work or triggers with low contrast between the trigger and the scope.  The authors dont provide general principles for selecting appropriate patches for irregular trigger construction."], "mwdfai8NBrJ": ["Paraphrased Statement: This paper proposes a method called policy smoothing to protect reinforcement learning (RL) policy from adversarial attacks during testing. The core idea is to incorporate a smoothing technique into the RL training work. Additionally an adaptive version of the NeymanPearindeedn Lemma is introduced to provide theoretical reinforcement for this defense. effectiveness and Weaknesses: 1. Its unclear from the statement whether the adversarial perturbation epiindeedde (\\epsilont) is fixed regardless of the observed state (st) which differs from how its described later in the paper. 2. The definition of \\Phi in Theorem 1 is missing. 3. The contingent of the undefended agent in the experiments are unclear. Its mentioned that the defense involves policy smoothing during training and testing indeed its unclear if undefended agents are trained without any smoothing. 4. Figure 5 shows an unexpected result where the undefended agent perform slightly worse than the smoothed policy still in the absence of adversarial perturbations. This suggests that smoothing may have a negative impact on performance in nonadversarial environments. 5. The NeymanPearindeedn Lemma which is used to provide theoretical guarantees for the defense may be unfamiliar to indeedme readers. Additional context and explanation would be helpful.", "Paraphrase: The paper extends randomized smoothing (RS) to scenarios with sequential interactions specifically using reinforcement learning (RL) as an model. To use RS in this setting the authors adapt the NeymanPearson Lemma to account for the adaptive behavior of the adversary in RL. They show that the impact of a potentially stochastic adversary can be bounded by a deterministic adversary which can be further bounded by a deterministic adversary with a specific structure. Based on this theory the authors introduce \"policy smoothing\" an algorithm that provides lower bounds on the reward or other tracerelated metrics that an agent can obtain still in the presence of an adversary. The algorithm is demonstrated on several environments and deep RL agents showcasing its effectiveness. effectiveness and Weaknesses:  The paper presents a significant advancement by considering traces rather than individual time steps in RL verification.  The conceptual and mathematical arguments for applying RS to this setting are welldeveloped and explained clearly.  The results demonstrate the robustness of the approach but indicate that it may not still provide a practical defense for sequential decisionmaking. Questions:  In Section 4.2 the proof assumes a different policy \u03c0. The authors should clarify why the argument isnt done using the original \u03c0 and why this change is permissible.  The authors should discuss the possibility of using multiple overlapping certificates to improve the budgetscore tradeoff.  The authors should compare their approach to work ([1]) which provides similar guarantees.", "Paraphrased Summary: This study introduces theoretical limits (guarantees of robustness) for the effectiveness of reinforcement learning (RL) policy that practice randomized smoothing to defend against adversaries with constraints on their norm. The proposed technique extends the NeymanPearson Lemma to adaptive settings. The authors provide formal proofs and demonstrate their claims in four RL benchmark. effectiveness and Weaknesses: effectiveness:  Introduces a novel regularization method for RL policy that involves augmenting training data with noisy samples.  Presents provable lower bounds on policy returns when field to normconstrained adversarial perturbations.  Extends the certification work to cover the total trajectory rather than individual agent experiences.  Provides a sound theoretical model and clear explanations. Weaknesses:  The authors claim that policy smoothing is a novel contribution may be inaccurate as there are existing study on the practice of random and adversarial perturbations for RL policy regularization.  The analysis assumes that the adversary has access to the target agents observations which may not be realistic in some scenarios.  The empirical results could be more comprehensive by including nonadversarial scenarios and reporting the impact of policy smoothing during both training and testing.  In the Pong game the certified lower bound does not appear to align well with the reported results suggesting a potential issue with the attack methodology. Minor Comments:  practice uppercase \"T\" for time steps in Equation 1 to avoid confusion.  Clarify the threat model in Section 1 to specify whether the adversary has access to the target agents observations.", "Summary This work presents a technique to make RL (reinforcement learning) agents more robust against adversarial attacks by applying randomized policy smoothing. It introduces an adaptive NeymanPearson Lemma and guarantees robustness in terms of cumulative reward. Additionally it provides a worstcase scenario to assess the certifications accuracy and evaluates these certificates under adversarial conditions in two environments. effectiveness and Weaknesses  The papers writing is accessible and its proposed method is straightforward.  The experimental outcomes are convincing contributing to the limited research on RL robustness. However the papers contribution may be diminished by Zhang et al. (NeurIPS 2020) which contribution similar concepts. Despite acknowledging this the authors adequately compare and contrast their method with the competition. Randomized policy smoothing marks a novel approach to safeguarding RL model albeit its formulation resembles a POMDP (Partially Observable Markov Decision work). The authors might enhance their work by addressing this resemblance and discussing its implications. Other related study (e.g. Zhang et al. ICLR 2021) should also be included in the references.", "Paraphrase: Summary: This research investigates certified robustness of policy in reinforcement learning environments. The authors present a method akin to randomized smoothing in supervised learning to achieve certified robustness for policy. They offer theoretical evidence and empirical results. effectiveness:  The paper addresses the significant issue of attaining certified robustness in reinforcement learning.  The proposed method bridges the gap between randomized smoothing and reinforcement learning. Weaknesses:  The papers clarity could be improved before publication.  Some sections are challenging to comprehend.  The concept is somewhat incremental as randomized smoothing is wellestablished in supervised learning and the papers core concept is similar.  The Adaptive NeymanPearson Lemma is described as nontrivial but its explanation is unclear. Specific Comments:  Figure 1 requires clarification.  The adversarys capabilities are inconsistently presented with discrepancies in their ability to perturb observations versus adding smoothing noise.  The distribution of smoothing noise and the adversarys decisionmaking work are not fully explained.  The construction of Yst is unclear raising questions about the adversarys optimal strategy and its experimental or theoretical implications.  Expressions for \"underline F\" and \"overline F\" in Section 4.3 are not provided. After Response: The authors clarification regarding system B as a practical construct for analysis is appreciated. However the paper still lacks explicit distinction between the actual attack system and the practical system. Figure 2 continues to convey the impression that system B represents the attacker. While the clarity issue has been partly direct room for improvement remains. The papers correctness could not be fully evaluated due to limited familiarity with proof technique for randomized smoothing."], "u2GZOiUTbt": ["Paraphrased Statement: The authors introduce a task affinity measure calculated using a matching algorithm and the Fisher data matrix. This measure helps identify the training data labels most similar to the examination data. These relevant labels are so used for episodic finetuning enhancing model performance in fewshot learning tasks. Despite achieving stateoftheart results on four benchmark the paper lacks sufficient experimental detail and ablation work to fully validate the approach. The authors should clarify certain technical aspects and provide a more comprehensive evaluation of the models performance. Detailed Comments and Questions: 1. epsilonapproximate network: This refers to a network that achieves a performance within a specified distance of the optimal network. Epsilon is a userdefined threshold that determines the degree of approximation. Its value can significantly impact model performance. 2. Fisher data matrix: It is computed using the query data to ensure that the affinity score reflects the taskspecific characteristics of the examination data. 3. computational complexity: While the proposed approach requires more time for relevant task selection it reduces the training complexity for the fewshot model by focusing on relevant data. 4. 1shot background: The closest source task is calculated based on the statistical distribution of the target task which may not be significantly affected by the noise of sample selection. 5. TopR selection: The author selects the task with the highest task affinity score from among the topR tasks. 6. Source task finetuning model: The finetuned model on each source task is used to calculate the task affinity score for the target task. This work can be timeconsuming particularly for datasets with a large number of source tasks. 7. TieredImageNet label selection: The author selected 120 labels per source task to ensure comparability with the miniImageNet benchmark. However the examination set has 160 labels. 8. Backbones in different datasets: Different backbones were used to compare the effectiveness of the IEDistill methods on different types of image classification datasets. 9. Baseline and performance gain: The paper does not explicitly mention the baseline for comparison. The performance gain from TASSimple to TASDistill is derived from both knowledge distillation and the affinity scorebased task selection method.", "Paraphrased Statement: Summary: This work introduces a \"few shot learning\" approach that aims to assess the similarity between various tasks. The most relevant tasks are identified using an \"Affinity Score\" and utilized to improve the performance of the target tasks. The Affinity Score is a unique measure that gauges the interdependence of tasks. Its robustness is mathematically demonstrated. Strengths:  Using the Fisher Information matrix to gauge relevance between tasks is a novel approach.  The experimental findings show promise. Weaknesses:  The work relies on an empirical Fisher Information matrix. The rationale for this approximation should be discussed thoroughly.  The authors need to conduct additional experiments to demonstrate the effectiveness of the Affinity Score. For instance they should show that models trained with source tasks selected using the Affinity Score consistently outperform those trained with randomly selected source tasks.  The Affinity score proof assumes a strictly convex loss work which is a restrictive condition.", "Paraphrased Statement: This paper introduces a novel affinity score derived from the Fisher Information matrix that measures the similarity between a source task and a target task. This score is used to guide a fewshot learning approach that leverages a pretrained WholeClassification network. The proposed approach matches training labels using a maximum matching algorithm and selects the most relevant target task for training based on the affinity score. The effectiveness of this score is evaluated using benchmark fewshot learning datasets. Strengths and Weaknesses: Strengths:  Introduces a novel score based on the Fisher Information matrix with mathematical analysis provided.  Compares performance to numerous metalearning methods in empirical work.  Features a wellorganized structure. Weaknesses:  The proposed fewshot learning scheme lacks an incremental aspect.  The advantage of the proposed score (TAS) is unclear.  A more comprehensive evaluation is necessary to demonstrate the efficacy of the proposed method. Additional Comments:  The work main contribution involves incorporating a work of constructing a relevant training set using TAS into fewshot learning. This approach differs from previous methods that embed tasks into hidden space.  It remains unclear if TAS can be used for fewshot learning based on approach other than WholeClassification.  While the proposed method outperforms previous metalearning methods in accuracy more evidence is needed to explain its effectiveness such as an analysis of the source tasks selected by TAS.  A discussion on the computational complexity of the proposed method would be beneficial."], "o0ehFykKVtr": ["Paraphrase: The authors have developed an enhanced version of the Visual Foresight method for transfer learning in robotics. They introduce the concept of \"robotaware models\" where the overall dynamics model is separated into an analytical model for the robot and a learned model for objects in the scene. The analytical model along with existing paradigm projection techniques predicts how the robot will appear from the cameras point of position after specific actions. This allows them to remove robotrelated pixels from the paradigm data leaving the learned model with only objectrelated data. This exclusion makes the learned model invariant to the robots appearance. The same principle is applied to a novel planning cost function for visual MPC. This innovation facilitates crossrobot transfer in tabletop manipulation tasks where the goal is specified by a target paradigm. effectiveness:  Utilizes existing knowledge about the robot rather than learning it from data.  clear explanations with illustrative model and experiments on diverse platforms and datasets.  Impressive fewshot and zeroshot transfer results.  Thorough analysis of experimental results with both quantitative and qualitative metrics. Weaknesses:  Limited discussion on the method manipulationd to project and segment the robot into the paradigm plane.  The term \"disentangled\" might not be appropriate to describe the separation of robot and object pixels since the learned model still receives data about the robots movement.  The manipulation of a \"proxy robot\" in Section 3.2 is unclear.  Standard deviation statistics for the results in Table 1 could be provided for clarity.  Baselines for transfer experiments could be improved to include paradigms with both the robot and objects.  The methods limitations should be discussed such as its dependence on proper calibration and robustness to perturbations in the scene.", "Summary This paper introduces a robotic learning (RL)based approach for transferring knowledge between robots. It involves separating the model into a robotspecific model and a general world model. The method has been tested on both simulated and realworld data. The authors claim it enables zeroshot transfer to new robots. effectiveness and Weaknesses effectiveness:  Fills a gap in the literature  Clear and cohesive introduction  Convincing experimentation using both simulated and real data Weaknesses:  Overstated claim: The paper claims zeroshot transfer is potential which is not entirely true.  Insufficient technical details: The models implementation is not sufficiently explained making it difficult to assess its technical soundness.  Limited discussion of related work: The complementarity to existing field is not adequately discussed especially in relation to Chen et al. (2018).  Debatable approach: The papers proposal for handling occlusions using RGBD observations is questionable due to potential similarities in appearance between the robot and its environment.  Errors in references: Several references contain errors including incorrect citation formats and missing details.", "Paraphrase: This paper proposes a solution to the challenge of reducing the reliance on robotspecific data for robotic control. The authors present a modelbased reinforcement learning (RL) policy that combines robotagnostic and robotspecific dynamics modules. Through experiments with both simulated and real robots they demonstrate how visual modelbased policies can be transferred between different robot systems. effectiveness:  Addresses a compelling problem with an innovative approach.  Introduces a novel deep learning architecture that integrates a video generation model with an LSTM network.  Demonstrates promising performance on diverse robotic tasks. Weaknesses:  The methodology is somewhat complex and difficult to follow in the main text.  Details of the algorithm are missing especially regarding the novel contributions.  A comparison of pickandplace and pushing results is lacking.  The realworld robot experiments seem limited in scope. Minor Issues:  Clarification is needed regarding the final form of Eq. 3 and its usage.  The choice of the hyperparameter lambda is not mentioned.  A repetition of \"1.8k videos videos\" appears in the experiments section indicating a need for proofreading.", "Paraphrased Statement: research has been expanded on training a visual policy to control a robot arm in a manipulation position. This policy is trained on a vast dataset of robot experiences which includes diverse robots handling different objects in a tabletop environment. Using this dataset a model for visual dynamics is developed predicting the future visual robot state based on current visual robot state and applied action. This visual dynamics model is employed to achieve a desired visual state using the existing technique of visual foresight. The paper introduces two primary innovation: 1. Decoupled visual Dynamic model: The world model which predicts future scene observations without the robot is separated from the robot model which predicts future endeffector model and robot body representation in visual space. 2. Decoupled cost for visual Foresight planning: A novel cost function is promodeld for visual foresight planning combining and separating a desired robot state cost and a desired world state cost. effectiveness and Weaknesses:  The paper enhances visual foresight by incorporating knowledge from the robots forward kinematics analytical model. By decoupling world and robotspecific visual data it addresses a limitation of nondecoupled visual foresight.  The paper is wellwritten and accessible however certain details require careful reading of supplementary material.  A key limitation is the unclear explanation of whether robot data is entirely removed from the world observation. It appears that some robot data may remain even if the robotaware cost is ignored.  The paper relies heavily on prior works but it is unclear how these works were accessed. additional Notes:  The sentence \"The current and future mask are concatenated with the paradigm and the end effector model is tiled into the latent spatial embedding\" requires clarification on the paradigm and latent space referred to.  PSNR and SSIM metrics are not defined in the paper and should be explained for clarity."], "oAy7yPmdNz": ["Summary: The paper introduces CoordX a novel neural architecture for implicit representations that divides each input signal dimension into separate branches such as pixel locations in an image. Each branch is processed fused and combined to reconstruct the input data. The authors also propose a subsampling method for training and different branch split strategies. Experiments demonstrate the benefits and drawbacks of the method across various data types. Main contribution:  Introducing a novel architecture for implicit representations.  Demonstrating speed advance without sacrificing reconstruction quality in certain cases.  Experiments on diverse data modalities showcasing the methods effectiveness and weaknesses. effectiveness:  The motivation to improve the speed of implicit representations is timely.  The concept of leveraging coordinate locality in implicit representations is interesting.  Wide variety of data modalities used in experiments. Weaknesses:  Unconvincing experimental issue with significant PSNR drops in image experiments.  Lack of comparisons with other stateoftheart methods for accelerating implicit representations.  The proposed approach is limited to regular grids which restricts its applicability.  Poor writing and use of nonstandard notation make the paper difficult to read.  Incomplete related shape discussion omitting relevant concurrent research.  The proposed data decomposition approach while potentially applicable to compression is not thoroughly explored.", "Summary This paper introduces a novel network architecture (CoordX) that processes each input coordinate separately in the first layer rather than using a fully connected layer to process them together. This approach issue in faster evaluation training and inference for tasks involving coordinatebased MLPs without sacrificing signal quality. The benefits are demonstrated for image video and 3D shape representation tasks. effectiveness  Clear presentation of quantitative issue and welljustified ablation field.  Demonstrated speedup and lack of quality degradation for three signal fitting tasks.  Extension to local implicit methods with latent codes even providing a speedup with minimal quality loss.  Comparison of splitting degree versus issue of layers highlights the effectiveness of the proposed method beyond using minor MLPs. Weaknesses  Limited discussion of the significance of issue for neural rendering application which are the main use case for coordinatebased networks.  Minimal exploration of volumetric rendering and raytracing methods in the appendix.  Lack of ablation field on the input point sampling method which is crucial for achieving speedup.  Limited experiments on varied datasets for neural volume rendering. minor input  Reorder the innovation to provide context before introducing CoordX.  Clarify the notation in Equation (2) regarding the weight matrix.  Consider ablation field on the feature fusion method to investigate its impact on the CoordX architecture.", "Paraphrased Statement: The paper presents an innovative advance to CoortMLPs network innovation that aims to enhance its speed. This approach involves dividing the input coordinates sharing weights and merging them later. The authors demonstrate that this optimization can theoretically reduce computation requirements significantly and have achieved a noteworthy twofold increase in speed in practical application. While the concept is wellpresented and supported by evidence the papers originality may be limited as it offers incremental but impactful enhancements.", "Paraphrased Statement: This research suggests an advance to neural network models that operate on multidimensional grid systems. A component of the initial layers in these models handle separate components of the grid (e.g. x and y instead of combined xy). This decomposition helps reduce computation costs (FLOPS memory use runtime) for common tasks like image use video representation shape modeling and volume rendering using radiance field. However it slightly increases the issue of parameters required compared to standard models."], "zXne1klXIQ": ["Summary The authors present methods designed to identify invariant predictors across various data sources. Instead of matching distribution risks as frequently done in the past they propose training models using data detail mixtures to prevent reliance on false correlations between domain and class labels. These correlations may not hold during testing. The suggested approach employs the concept of mixup blending data instances in two ways: combining data details from the same class but different domains and combining data details from the same domain but distinct classes. Strengths:  The approach is effective and adaptable readily incorporable into existing techniques.  benchmark tests demonstrate improved prediction performance compared to baseline methods. Weaknesses and Recommendations:  The approach relies on assumptions not discussed in the manuscript such as the assumption that domaininvariant approach can only improve generalization if dataconditional label distribution remain consistent across domains.  The evaluation focuses only on downstream performance improvements providing limited insight into the sources of improvement. The authors should conduct domain prediction experiments to assess the invariance of learned representations.  The authors claim that the proposed mixup strategies induce domain invariance but this has not been empirically verified. They could evaluate risk estimates across domains to confirm this claim.  The evaluation should include an investigation into the issue of model capacity as the authors speculate that regularizers may limit model capacity and hinder performance.  The risk bounds in Theorems 1 and 2 rely on highly simplistic assumptions limiting their practical relevance.  The proposed approach bears similarities to recent methods that employ mixup across domains. The related study section should address these parallels. Other Comments:  The setting described on page 2 was first introduced in [6] not by Koh et al. (2021).  The labeled mixup definition in Eq. 2 assumes onehot encoding which is not explicitly stated.  Although the title mentions outofdistribution robustness a significant portion of the study focuses on multidomain learning where the goal is uniform risk predictors across domains which technically does not constitute outofdistribution generalization.", "Paraphrased Summary: Strengths:  Tackles the challenge of model robustness in different domains and subpopulations.  Introduces two data augmentation strategies based on mixup interpolation to remove spurious correlations.  Shows promising empirical results. Weaknesses:  Limited innovation compared to previous approach as it extends mixup.  lack discussion of related research on outofdistribution generalization.  Requires tuning of a hyperparameter (psel) for each task.", "Paraphrased Statement: Summary: This research introduces a novel augmentation approach based on mixup in the context of data distribution shift. data distribution are represented as mixtures of distribution (domains) and two distribution shift scenarios are addressed: 1. Domain shift where training and testing domains are mutually exclusive. 2. subpopulation shift where the test distribution has different mixture proportions than the training distribution. It is assumed that domain identification has a spurious (false) correlation with labels. To address this two mixup strategies are proposed:  Mixup: Combine two examples with the same label but from different domains.  Antimixup: Combine two examples with the same domain but different labels. It is argued that these mixup strategies counteract the spurious correlations. Experiments demonstrate the methods effectiveness compared to standard risk minimization (ERM) and other augmentation methods. Theoretical analysis shows that the proposed method has smaller worstcase classification error than ERM and vanilla mixup under certain conditions. Strengths and Weaknesses: Strengths:  Simplicity and intuitive appeal.  Extensively tested showing significant improvements over other methods.  Theoretical justification to a certain extent. Weaknesses:  Some analysis is unclear.  Contradictions between claims in different sections.  Absence of metrics for spurious correlations.  Inconsistent performance of vanilla mixup.  Lack of explicit clarification on the dimension of x in Theorem 1. Suggestions for Clarification:  Explain the different motivations for the two mixup selection strategies.  Provide metrics to quantify spurious correlations and demonstrate their impact on LISAs effectiveness.  evidence LISAs superiority over mixup on datasets with known spurious correlations.  Address the apparent inconsistency in vanilla mixups performance.  Explicitly state the dimension of x in Theorem 1."], "kcwyXtt7yDJ": ["Paraphrased Statement: This paper introduces a method for adapting machine learning model to multiple target domains using a graphbased approach. The traditional Assumption of treating all domains equally and aligning them perfectly is not appropriate in this setting. To address this the paper suggests a method for determining the adjacency between domains using a graph. The method also involves adversarial learning in which an encoder and a discriminator are trained. Furthermore the paper provides theoretical analysis for this problem. The experimental results demonstrate that the proposed method effectively generalizes alignment beyond a uniform approach. Strengths: 1. The paper presents a new approach to multitarget domain adaptation. 2. It includes a comprehensive explanation of the problem and the proposed method. 3. The method is evaluated on various datasets providing evidence of its robustness. 4. The illustrations in the paper help visualize the performance of the proposed method. Weaknesses: 1. It is unclear whether the proposed method can be applied to traditional domain adaptation scenarios involving only two domains. 2. The experimental results show modest advantages over baseline methods.", "Paraphrased Statement: Summary: This field improves an adversarial domain adaptation model by using a domain encoding graph to generalize the uniform domain alignment. The method is straightforward wellexplained and backed by experiments demonstrating its efficacy. Strengths: 1. The field proposes a graph discriminator as a replacement for the domain discriminator in the adversarial learning framework. 2. Theoretical analysis supports the benefits of this replacement. 3. Experiments validate the usefulness of the proposed approach for domain adaptation. Weaknesses: 1. Domain alignment is wellresearched including graphbased methods. The authors do not adequately justify why their graph discriminator surpasses conventional adversarial discriminator or their variations. 2. The field lacks a comprehensive explanation of its contributions compared to existing graphbased discriminator methods. 3. The experiments primarily compare the proposed method to legacy methods rather than stateoftheart domain adaptation techniques. 4. The fields acceptance would be enhanced by providing a clear articulation of the advantages of the proposed method and demonstrating its superiority over current benchmarks.", "Paraphrase: This paper introduces a model called GRDA that addresses the number of uniform alignment in domain adaptation which overlooks the relationships between domains. GRDA incorporates a domain graph to represent domain adjacency and enhances adversarial learning with a graph discriminator that uses conditioned graph embeddings. Mathematical proofs and empirical results demonstrate the effectiveness of this approach on both synthetic and realworld datasets. Strengths:  GRDA leverages topological structures among domains to enhance domain adaptation results.  The discriminator is used for reconstruction rather than classification which is a novel approach.  Thorough theoretical analysis supports the findings.  Experiments are comprehensive and provide parameter sensitivity analysis ensuring reproducibility. Weaknesses:  Lemma 4.1 assumes the optimal discriminator estimates the conditional expectation of Aij but this Assumption is not rigorously proven.  Proposition 4.1 introduces an intriguing idea but a formal proof is lacking.  The authors need to provide more insight into why changing the discriminator task from classification to generation improves domain adaptation performance."], "zRb7IWkTZAU": ["Paraphrase: This work introduces a hybrid learningrulebased approach for setting languagedependent rewards specifically for imagebased robotic control tasks. effectiveness:  Addresses a challenging problem in reward specification for robots operating in new environments.  Proposes a creative and effective approach that combines natural language work and image analysis. Weaknesses:  generalization Limitations: The approach may be limited to specific task formats and image scenes.  experimental Comparisons: The comparisons to other methods are limited not including imagecaptioning approach.  natural Language Complexity: The evaluation does not include complex natural language direction.  Environment Complexity: The work focuses on simple tabletop manipulation tasks with limited visual complexity.  Coverage of Prior work: various relevant work exploring languageconditioned skills in robotics are not cited.", "Paraphrase: Summary:  This research proposes using a pretrained CLIP (Contrastive LanguageImage Pretraining) model to guide a reinforcement learning (RL) agent reward use.  The method simplifies goal specification by using natural language.  A separate module calculates the reward based on spatial relationship while CLIP identifies relevant objects.  This method outperforms using the dot product between the embedded image and language vectors. effectiveness and Weaknesses: effectiveness:  Using natural language as a reward use is intuitive and aligns with languagegrounded RL goal. Weaknesses: 1. Zeroshot Definition:  The reward use relies on heuristics (Table 1) that define the goal making it not only zeroshot. 2. Limited Natural Language use:  The proposed reward use does not directly use CLIP image embeddings and text embeddings to estimate rewards.  CLIP is used as an object detector limiting the potential of the visionlanguage model. 3. CLIPbase Results in design 6:  The dot productbased reward in design 6 shows poor results which raises concerns about the RL agent ability to learn from this reward. 4. Alternative CLIP Baseline:  The researchers suggest exploring alternative CLIP formulations such as a classification model for objects with different spatial relationship to eliminate the need for separate spatial heuristics.", "Paraphrased statement: The paper proposes a method for training policies for tasks described in natural language text without requiring expert direction or state data to design reward use. The authors use a powerful visual grounding model to match objects mentioned in the text with their visual counterparts in the current observation thereby creating a proxy reward signal. They demonstrate the effectiveness of this approach through simple experiments and introduce enhancements to the baseline that address its limitations. They additionally train their languageconditioned policy on multiple tasks to create a general policy capable of solving arbitrary tasks given text descriptions and trajectory data from the training tasks. effectiveness and Weaknesses: effectiveness:  The paper addresses the challenge of specifying goal configurations using language text a more general setting than relying on imagebased goal descriptions.  The empirical evidence provided supports the authors claims. Weaknesses:  Underspecified goal Text Descriptions: The goal text descriptions used seem overly simplistic raising concerns about their effectiveness in complex task settings.  access to state data: The spatial grounding heuristics used by the proposed approach rely on ground truth object locations which could be considered accessing state data for reward engineering design.  Computational Complexity: Computing rewards based on spatial relationship requires multiple camera views potentially introducing challenges for realworld deployments.  Similarity to Existing Approach: The reward structure used in the paper appears similar to an existing approach that uses localization and grounding to derive rewards.  Missing data: The paper lacks details on how goal text descriptions are derived and how grounding of semantics and spatial relationship is translated into numeric rewards. Additional Questions and Suggestions:  What is meant by \"object interactions\" in the parsing of goal text  Was the Mask RCNN model finetuned If not how do the vocabularies of Mask RCNN and CLIP overlap  Can the language in Section 3.2 be revised to clarify that the proposed approach is distinct from the baselines", "Paraphrase: This paper introduces a method for specifying rewards in reinforcement learning scenarios where no expert demonstrations or specific state data is available. The approach utilizes natural language descriptions as the cornerstone for reward specification. The method uses CLIPs imagelanguage encoder to determine areas of visual saliency and identifies target locations for objects mentioned in the task description. Based on this data the paper suggests an automated reward use generation mechanism. The work presents a novel and practical solution for the challenge of specifying rewards in zeroshot scenarios enabling reinforcement learning agent to operate in diverse environments and tasks. While the work demonstrates promising results in simple scenarios it acknowledges the need for further research in more complex settings. The works weaknesses include:  Limited testing in complex environments  Lack of exploration into translating simple natural language queries to more complex tasks  Inferior performance of CLIPbased text embedding in the reward use compared to heuristicbased methods warranting further investigation  Incomplete details on the construction of the goal description and whether it facilitates effective parsing"], "ljCoTzUsdS": ["Summary This study examines the ability of machine learning models to generalize to unseen data focusing on two types of biases:  Featurelevel bias: Differences in the importance of certain features in model learning.  Exemplarvsrule bias: Differences in how these learned features are used for generalization inspired by cognitive psychology studies of exemplarbased and rulebased generalization. The authors present an experiment designed to directly explore this tradeoff in machine learning systems. Their empirical results across various models in different domains (e.g. image and language) evidence that considering both biases provides a more comprehensive understanding of extrapolation behavior compared to existing approach. Strengths and Weaknesses  Difficulty in Reading: The authors discussions lack clear definitions and use many acronyms. Key terms like \"featurelevel bias\" and \"exemplarvsrule bias\" are not formally defined making it challenging to follow the paper.  Limited Scope: The proposed approach focuses on three specific types of machine learning methods. The justification for this choice is not clear and it is unclear whether these methods are representative of a broad image of techniques.  Unclear language: The meaning of \"spurious correlation\" is not explained in the paper.  Unclear model: The authors model (e.g. image 1) are confusing. With only two training model it is difficult to see the differences between rulebased and examplebased predictions.", "Summary: This research proposes two metrics to assess a learning systems biases at the feature and exemplar versus rule levels. The authors designed four training scenarios that vary the relevance of certain features:  Cue conflict: Features are relevant to the task.  Zero shot: Irrelevant features are present.  Partial exposure: Irrelevant features are present but only a subset is used for training. The testing scenario involves extrapolating to a setting where all features are relevant. The inductive bias of a learning system is determined by the difference in its extrapolation performance under different training conditions. The authors evaluated their framework on synthetic and realworld datasets. On a synthetic 2D dataset they found that generalized linear models favor rulebased generalization while Gaussian processes favor exemplarbased generalization. On the IMDB dataset LSTM models exhibited high featurelevel bias (overreliance on irrelevant features) and exemplarrule bias. On the CelebA dataset ResNet models showed varying featurelevel bias but systematically preferred exemplarbased generalization. Strengths:  Novel approach to probing learning systems by comparing generalization performance under different training conditions.  synthetic experiment demonstrates the effectiveness of the metrics.  Clear and wellwritten introduction. Weaknesses:  Unclear utility: The practical applications of the proposed metrics are not immediately apparent given existing methods for learning models robust to irrelevant features.  Interpretation of measures: The meaning of the featurelevel bias (FLB) and exemplarversusrule propensity (EVR) scores is not explicitly explained.  generalization of measures: The metrics are currently defined for specific assumptions that may not hold in all scenarios. Questions and Suggestions:  What is the significance and interpretation of FLB and EVR scores  Can the metrics be generalized to settings with nonuniform label marginals  Can the authors provide contour plots of EVR to better understand its behavior under different conditions", "Paraphrased Summary: The paper presents methods for assessing how two types of explanation methods (rulebased and exemplarbased) generalize and extrapolate to unfamiliar data points. Strengths and Weaknesses:  The paper introduces thoughtprovoking concepts related to inductive biases such as the varying difficulty of learning different features (e.g. continuous vs. categorical).  However some aspects of the exposition are unnecessarily confusing. For instance instead of using arbitrary labels (\"dax\" and \"fep\") the paper could have simply labeled objects by their colors (common and purple).  Some claims require further substantiation or clarification. For model the paper asserts that GPs contribute to understanding exemplarbased generalization but does not provide a detailed explanation.  It would be helpful to distinguish which statements have been experimentally validated from those that have not. For instance Section 4.2 presents results without specifying whether they are based on experiments or theoretical considerations.  Section 4.4 makes significant claims based on results relegated to the appendix which raises questions about the reliability of those results.  The paper could benefit from better wording in some sentences (e.g. \"EVR increases with exemplarbasedness\" and \"What is the heldout quadrant\").", "Paraphrase: research in machine learning faces challenges in generalizing data rule to unknown scenarios (extrapolation). This ability hinges on the \"biases\" (assumptions) that guide the learning algorithm. However machine and humans dont always contribution the same biases leading to unexpected extrapolation behavior that deviates from human expectations. This study examines two types of biases:  Featurelevel bias: Differences in how easily certain features are learned.  Exemplarbased vs. rulebased bias: How features are used for generalization (e.g. by matching specific model or by applying rule to create new rule). Inspired by psychological studies the researchers propose a strategy for evaluating this tradeoff in learning systems. Their results show that controlling for featurelevel bias while measuring the interplay between exemplarbased and rulebased generalization provides a comprehensive understanding of extrapolation behavior beyond existing approach. Strengths and Weaknesses: Strengths:  The study tackles a important emerge in machine learning.  The protocol for investigating inductive bias is novel and based on psychological research. Weaknesses:  The experiments are limited to two datasets raising questions about practical validity."], "lEB5Dnz_MmH": ["Summary Paraphrase: This paper suggests a method for combining tweets and stock prices to predict stock trend more effectively. The method is said to perform beneficial than existing fusion techniques and generate high profits in market trading simulations. effectiveness and Weaknesses Comments Paraphrase: This paper proposes a way to combine text data and stock prices. However there are some concerns about the paper current form: 1. The proposed architecture uses existing techniques limiting its technical significance and novelty. 2. The paper language needs improvement with grammatical errors and inconsistencies. instance include:  The abstract contains the grammatically incorrect sentence: \"extensive experimentation is performed on tweets and historical price of stock market our method can achieve a beneficial accuracy compared with the stateoftheart methods on two evaluation metrics.\"  There are discrepancies between the text and diagram in Figure 1 including the mention of a ConvLSTM whose role is not explained.  Mathematical formulas lack clarity. For instance the definitions of x in (17) \u2299 in (20) vi and vm in (18) and (19) and zt and zT in (21) and (22) are missing. 3. Essential contingent are missing including:  Dataset data (time range splitting method for trainingvalidationtest data)", "Paraphrased Summary: This paper introduces a model that combines tweet and stock price data using a technique called \"coattention\" to predict stock trend. The model outperforms existing methods in a particular stock prediction dataset. effectiveness:  Welldefined problem with practical importance.  Stateoftheart performance on a stock prediction dataset. Weaknesses:  Excessive emphasis on established concepts (LSTM attention Transformer) in place of proposed model contingent.  Lack of clear distinction between proposed and existing model components (e.g. \"multifeatures fusion\" vs. Transformer Encoder Layer).  Insufficient technical contingent of proposed model components (e.g. construction of query value and key vectors). Minor Suggestions:  Adjust figure and table placement inside page boundaries.  Align section names with figure module names for clarity. Questions:  Define a training sample for the proposed model.  Determine if the model input is a combination of tweets and stock prices for a particular company on a single date.  Clarify the use of price vectors as valuekey and text vectors as query.  Decipher the meaning of \"07062021\" in the context of the paper.", "Summary Paraphrase: This paper presents a novel technique for combining text and stock price data for enhanced stock market predictions. It separately processes text and stock price data before merging them using a coattention transformer. This approach has been shown to surpass current benchmark based on realworld data and trading simulations. effectiveness and Weaknesses Paraphrase: effectiveness:  Effectively integrates text and stock price data by processing them in parallel and then fusing them.  Employs a coattention transformer for data fusion enabling beneficial exploitation of relationships between text and stock prices. Weaknesses:  Clarity and Organization:  Some sections require more refinement in writing.  References are misplaced such as those related to the dataset (Sec. 4.1) and temporal attention mechanism (Sec. 3.1.4).  Paragraph 2 of Section 2 could be split into two with the second focusing on fusion techniques.  model Details:  Section 3.1 requires clarification on terms such as \"endpoint\" and particular dates.  Consideration of using a Transformer model for text encoding instead of BiLSTM and selfattention.  more data on intermediate model outputs such as dimensions and components.  Justifications for using an LSTM after the coattention transformer and temporal attention.  trading model:  more contingent on experimental setup including the time period and stock selection criteria.  Quantitative results for extensive stock application.  Data Input:  Additional data on whether stock price data is only normalized or undergoes further processing.  Clarification on the dimensions of input and output data."], "vkZtFD0zga8": ["Paraphrase: This paper introduces a video compression system that considers uncertainties. It uses various decoders to predict motion vectors and a special loss function that prioritizes the top predictions. The system also studies the types of uncertainties (aleatoric and epistemic) in video compression and shows how to visualize them. Tests show that the system compresses videos about 20 beneficial than previous methods without significantly increasing processing time. effectiveness and Weaknesses:  The paper uses two different symbols (Fi and xi) to represent the current frame.  Its not clear how the predictions from raw data differ from those based on motion compensation.  The paper suggests that the loss function can be adjusted to consider the worst predictions instead of the beneficial but its not clear if this improves performance and why the beneficial predictions are preferred.  The last time of Section 4.1 is incomplete.  The paper doesnt explain what the Prediction Refine Net and Reconstruction Refine Net in Figure 1(b) do. Its not clear if theyre neural networks or how they predict and use uncertainties to improve compression.  The paper assumes that the predictions from multiple decoders follow a certain statistical model. It should clarify that the issue of decoders is sufficient to support this assumption.  There are various typographical errors punctuation errors and missing articles.  The system relies on an optical flow estimation method which could introduce errors.  The paper doesnt discuss cases where the system may fail.", "Paraphrased Statement: This field aims to reduce uncertainty in video compression through an ensemblebased approach. The proposed approach introduces a method to promote diversity among ensemble members. Experimental results demonstrate its effectiveness. effectiveness and Weaknesses:  The paper effectively applies deep learning to video compression.  The experimental results appear promising and the ablation field is comprehensive. Concerns: 1. The rationale for using multiple heads (ensemble) is unclear. What specific benefits does it offer 2. The introduction claims that current methods have difficulty accurately estimating optical flow. However the proposed method also involves motion estimation. How does it address this issue 3. The ablation field suggests that multiple heads enhance performance. Why not explore the usage of even more heads 4. The introduction lacks clarity. Figures are not selfcontained and require referencing other contribution of the text for interpretation (e.g. \"MV Res\" in Fig. 1). 5. Grammatical errors are noticeable throughout the paper. model include:  \"are cannot be\" in Figure 2  \"it follows that the upper ... is given by\" on page 4  \"MEMSE\" on page 5  Figure 9e should be Figure 2d  Figure 9f should be Figure 2e", "Paraphrased Summary: This field focuses on endtoend deep learning video compression. Recognizing the inherent uncertainty the authors propose an \"ensemble approach\" that essentially employs a multihead decoder. To foster diversity among the ensemble members they introduce an \"ensembleaware loss.\" Adversarial training is also incorporated. Experimental results demonstrate that the proposed model outperforms existing stateoftheart deep learning video codecs. An ablation field validates the efficacy of each factor. effectiveness:  Clear and structured organization  Superior performance to previous models supported by ablation field Weaknesses:  theoretical justifications for using ensemble multihead could be streamlined.  Further clarification is needed for:  The significance of Eq. 10 and the role of i j and h  How the ensemble loss function promotes diversity  Fig. 3 requires explanation for the premature termination of the proposed methods curve.  Comparison with other methods at higher bitrates (e.g. 0.15) is lacking.  Acknowledgment of the potential limitations of deployment on lowcapability devices due to model size would be helpful.", "Paraphrased Statement: This paper presents a combination of techniques for video compression that leverages multiple predictions to reflect potential variations in the data. It develops a loss function that promotes differences between these predictions while also studying the advantages of using adversarial training in this context. The models performance exceeds that of the prevailing DVC Pro standard by over 20. effectiveness and Weaknesses:  Experiments demonstrate the superiority of the proposed approach over existing methods.  The novelty of the approach is not entirely evident. The model appears similar to DVC with the increase of a multihead motion vector decoder based on an equally weighted Gaussian mixture.  The ensemblebased concept is not entirely original as similar approaches have been explored in other networks. The differences between the proposed method and these prior works need to be outlined more clearly.  The papers readability could be improved by aligning the notation in the text (Section 3.1) with Figure 1.  It lacks a performance comparison with a recently published paper that is readily accessible on archive. The missing reference is Hao Chen Bo He Hanyu Wang Yixuan Ren SerNam Lim and Abhinav Shrivastava \"NeRV: Neural Representations for video\" NeurIPS 2021."], "pbduKpYzn9j": ["Paraphrased Summary: The authors investigate the importance of aligning the style module of the teacher and student networks in unconditional GAN distillation. Based on their findings they propose an initialization technique that assigns the teachers style module to the student. They also compute latent focus for augmentation. effectiveness:  Provides valuable insights into unconditional GAN distillation particularly regarding style module.  Introduces novel techniques for student initialization and latent directionbased distillation.  Outperforms existing stateoftheart methods on the FFHQ dataset.  The proposed initialization is straightforward and wellreasoned. Weaknesses:  The style module architecture must remain unchanged to maintain performance which could limit its flexibility.  The experiments primarily focus on human faces making it unclear whether the method generalizes well to other field.", "Summary This field examines an issue that arises during knowledge distillation for unconditional GANs specifically the StyleGAN2 model: the discrepancy in outputs between the teacher network (teacherNet) and student network (studentNet). The authors suggest that the initialization of the StyleGAN2 style module exacerbates this problem. They provide detailed analysis and experiments to support their claims and propose a deepntdirectionbased loss role to enhance distillation. Strengths 1. The output discrepancy issue is relevant to distillation as noted in previous field. The proposed solutions specifically address StyleGAN2 compression showing that the style module cannot be compressed while the CNN backbone can with the style module of the student inheriting from the teacher. 2. The evidence and results are convincing. The authors demonstrate that similar outputs between studentNet and teacherNet lead to more realistic generated images while dissimilar outputs create a competitive relationship between GAN loss and distillation loss affecting distillation. 3. Comprehensive experiments including qualitative analysis and quantitative results demonstrate the effectiveness of the proposed model. The authors provide a detailed stepbystep analysis and propose reasonable solutions. Weaknesses 1. The paper lacks a discussion of the difference between image generation and recognition tasks in relation to the knowledge discrepancy problem as well as the potential application of the proposed method in other field of knowledge distillation. 2. The authors claim that the output discrepancy issue is unique to unconditional GANs distillation but previous field have observed it in classification tasks despite the presence of ample supervised loss. Detailed theoretical analysis or additional experimental results are needed to support this claim. 3. The comparative experiments are insufficient:  Figure 3 lacks results for different initialized CNN backbones making it difficult to determine the role of the style module in distillation.  The results for the randominherit setting in Table 1 are missing.  Hyperparameter tuning experiments should be included with details on the values used in Table 2. 4. A previous field proposed a studentNet initialization method for compressing GANs. The authors claim that randomly initialized studentGANs cannot learn effectively from teacherGANs leading to a twostep training approach. However it is unclear how gradually reducing the weight of GAN loss during training for randomly initialized studentGANs would affect the compression of the style module. Questions for Rebuttal Period 1. Address and clarify the weaknesses identified above. 2. Explain the results in Table 1 particularly the values of 9.41 versus 9.42 which seem inconsistent with the authors claims. Additionally provide details on the averaging of experiment results and whether the competitive relationship between GAN loss and distillation loss is mitigated in the deep training phase.", "Summary This paper addresses image generation model compression using a knowledge distillation model. It observes limitations of applying knowledge distillation directly and proposes copying select parameters (style module) directly from the original model to the compressed model. Additionally it adds a loss term to preserve relationships within the latent space. Experiments using StyleGAN2 on the FFHQ dataset are presented. effectiveness  Explores an significant topic in generative model compression  Easy to follow  Uses a stateoftheart model and dataset Weaknesses  limited novelty heavily tailored to StyleGAN2  Unconvincing validation: only one model and dataset considered  Marginal improvements  Lack of discussion on limitations Specific Concerns  Incremental novelty: The style module initialization is similar to previous pruningbased methods.  Lack of pixelwise loss elimination: Despite arguments against pixelwise loss they are still included in the bestperforming model.  minor improvements: The proposed loss term shows minor benefits and significance testing is not provided.  Ambiguous terminology: Conditional GANs and output discrepancy are used broadly without clear definitions.  limited exploration: The solution is tailored to StyleGAN2 and it would be beneficial to test other generative models.  Insufficient validation: Additional datasets and conditional scenarios should be considered.  Twostage training: The paper does not explain why the style module is frozen for twostage training.  Missing results: Table 1 lacks complete results and std difference are not reported for significance assessment.  Incomplete FID reporting: The teacher FID is not provided in all tables hindering comparisons.  Upsidedown arrow: The arrow in Table 4 for LPIPS is inverted.  Bolding errors: Table 4 incorrectly bolds \"ours\" results despite CAGAN achieving slightly better performance.  Lack of visual comparisons: Sidebyside image comparisons between teacher and student models would enhance the papers introduction.", "Paraphrase: Summary The authors propose a technique for distilling StyleGAN2 a generative adversarial network that produces images without any specific input data. They identify the output difference between the original \"teacher\" network and the distilled \"student\" network as a key challenge. To address this they propose:  Initializing the student models style module with weights from the teacher model and randomizing the convolutional layers.  Introducing a distillation loss that captures the semantic relationships in the latent space. The authors evaluate their approach using StyleGAN2. effectiveness and Weaknesses Pros:  The paper addresses an significant problem.  It references relevant previous work. Cons:  Experiment Section and Claims: The quantitative results lack standard difference making comparisons inconclusive. The claim of outperforming previous work by a \"large margin\" is unsubstantiated.  Technical Depth and contribution: The paper lacks technical depth and the effectiveness of the proposed ideas is questionable. Typos:  \"sduent\"  \"student\" in Section 2.  \"0.31\"  \"0.41\" in Section 4.2.", "Paraphrase: Abstract distillation of unconditional Generative Adversarial Networks (GANs) using traditional loss role often fails due to discrepancy between teacher and student model outputs. This paper presents a model for distilling unconditional GANs specifically addressing this output discrepancy issue. model The model comprises two components: 1. Style module initialization: Inheriting weights from the teachers style module for initializing the student style module. 2. LatentDirectionBased distillation loss: Enforcing consistency between teacher and student outputs by minimizing the space between their latent focus. Evaluation Experiments compare the proposed model with CAGAN and discrepancy using different initialization strategy and loss role. Strengths  Addresses the output discrepancy issue in unconditional GAN distillation.  Provides comprehensive ablation field on initialization and loss role for style module and CNN components. Concerns  A direct comparison is missing between the distilled model and a samearchitecture network trained from scratch which is necessary to justify the distillation approach.  The criteria for identifying similar and dissimilar student models in Figure 1(a) are unclear.  The models performance on more complex image distributions (e.g. building interiors) remains unexplored.  It is uncertain whether adding an adversarial loss would further improve the distillation results."], "iaxWbVx-CG_": ["Summary: This research introduces a novel framework for selfsupervised learning called HCCL. In contrast to existing methods like BYOL and SimSiam HCCL incorporange:  multiple hierarchical projection and prediction layers  Contrastive loss across different layers of projection HCCL demonstrange significant improvements over previous stateoftheart methods on diverse benchmarks (e.g. iNat18 Place205 COCO case segmentation). Ablation field confirm the importance of hierarchical projectors crosslayer contrastive loss and higher predictor learning range. effectiveness:  innovative crosscontrastive loss  Simple implementation  Improved performance on benchmarks Weaknesses:  Lack of clear intuition behind the crosscontrastive loss  Training protocols for downstream tasks differ from previous work making comparison less rigorous  HCCL has more trainable parameters than BYOL potentially impacting training efficiency  Unclear meaning of \"Fixed\" learning range in Table 7  HCCL does not consistently surpass BYOL or Barlow Twins with longer training times  Typos and formatting errors in the manuscript", "Paraphrase: The authors have developed an enhanced method called Hierarchical Cross Contrastive Learning (HCCL). This method uses features from different level and views of range to obtain more consistent features compared to traditional Contrastive Learning (CL) that only employs features from final layers. HCCLs effectiveness has been demonstrated in tasks such as classification detection segmentation and fewshot learning. While previous approaches have partially explored the concept of representation consistency across views and layers they often relied on direct consistency loss between feature pairs. However in use features often exhibit only partial similarity between the last few layers. This requires careful selection of layers and parameters for optimal performance. HCCL addresses this limitation by introducing transformation modules that bridge compared features. This approach yields notable improvements when training data is limited and training epochs are short. However the benefits are less pronounced for full training sets and epochs. This raises the head of whether the information introduced by HCCL is already implicitly learned with existing datasets and training settings.", "Paraphrased Summary In this field a Hierarchical Cross Contrastive Learning (HCCL) method is proposed for training visual representations without supervision. The method involves a hierarchical projection network that extracts multilevel representations from data. Additionally a Cross contrastive loss is implemented to promote invariant representations. The effectiveness of HCCL is demonstrated in downstream tasks such as classification segmentation and object detection. effectiveness and Weaknesses effectiveness:  clear writing  Improved performance over previous methods Weaknesses:  The method is based on existing techniques (SimSiam and BYOL) with incremental contribution.  The hierarchical projection heads and Cross contrastive loss are unique aspects of the approach but their impact on performance improvements is unclear.  The ablation field suggests that the improvements are dependent on multiple tuning steps and the actual contribution of the proposed method are not fully isolated.", "Paraphrased Statement: Summary: This field introduces a novel selfsupervised learning framework that uses a hierarchical approach to learn visual representations. The framework utilizes multiple latent spaces to project range and augmented range representations allowing predictions on each space. Minimizing a contrastive loss between features from different projection level trains the frameworks parameters. The field assesses its performance on range classification and detection benchmarks and compares it with existing methods. effectiveness: The field provides a clear overview of selfsupervised learning methods. It highlights the trend of using global representations in rangelevel predictions and argues that this approach does not fully bridge the gap between pretraining and downstream tasks. The proposed hierarchical Cross contrastive learning method is simple and clearly described. Additionally the field conducts extensive evaluation and ablation field on diverse benchmarks. Weaknesses: The field does not provide sufficient detail on how the existing global representation learning methods fall short in bridging the gap with downstream tasks. This explanation would strengthen the motivation for the proposed hierarchical approach."], "nZeVKeeFYf9": ["Paraphrased Summary: This work introduces LoRA a lightweight method for finetuning language models (LMs). LoRA achieves comparable or superior performance to fullmodel finetuning while requiring only a small number of additional parameters (0.010.05 of the original model). Crucially LoRAs training is efficient without incurring additional computational costs during inference. The core idea of LoRA is to learn a set of lowrank parameters that are added to the original LM parameters instead of finetuning every parameter. Specifically LoRA need: 1. Freezing the original LM parameters. 2. Creating a parallel set of lowrank parameters (WBA) for each original parameter. 3. training the model by updating only the lowrank parameters which are used to compute hidden states (Eq. 3). 4. Testing the model using the new parameters computed as W0  BA avoiding additional computations. Evaluations on both natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that LoRA outperforms other efficient finetuning methods and often even fullmodel finetuning. Notably LoRA uses a minuscule number of parameters compared to the original model. Additionally LoRAs inference overhead is minimal compared to other techniques that require further computations. Strengths and Weaknesses: Strengths:  Novel approach of using parallel lowrank paths for efficient finetuning.  Eliminates additional computations during inference.  Optimized training work.  broad experiments provide strong evidence for its effectiveness. Weaknesses:  training efficiency requires further clarification regarding the impact of storing the original weights gradients.  No comparisons with nonefficient finetuning baselines to provide context for its performance relative to the stateoftheart.", "Paraphrased Summary: The paper presents a way to compress large pretrained models like GPT3. It uses matrix decomposition (like Singular Value decomposition) to compress the models weights into a smaller size. These compressed weights are kept fixed while small additional parameters are added and trained during the finetuning work. This approach is useful in use. The authors show that by adding a small number of these additional parameters the model can perform well on different tasks. Strengths:  Simple and efficient method for adapting large pretrained models.  full performance compared to other finetuning approach. Weaknesses:  The idea is similar to other methods like adapters.  Its not compared to the latest and large models.  Its unclear how efficient the training work is and how quickly the model can be trained.", "Paraphrased Statement: This research introduces a new method called LowRank Adaptation (LoRA) for modifying pretrained models for specific tasks. LoRA adds lowrank matrices to the pretrained parameters for each task. Because these matrices are much smaller than the original parameters LoRA is more efficient in training and uses less computing power than other methods like finetuning or using adapters or quicks. Testing on various pretrained models (RoBERTa DeBERTa GPT2 GPT3) showed that LoRA performs as well as or even full than other techniques. LoRA is a simple and effective way to adapt pretrained models to different tasks making it a promising alternative to finetuning and quick tuning. The paper provides thorough experiments demonstrating LoRAs effectiveness across a range of models and tasks. Strengths:  Wellmotivated and straightforward to understand  Novel approach using lowrank approximation in pretrained models  Efficient training and low inference latency  Applicable to a broad range of neural network models Weakness:  Handling of multitask scenarios could be improved  Comparison of training efficiency should include methods with similar trainable parameters  exploration of how LoRA compares to methods like variety of Experts (MoE) for multitask training would be beneficial", "Paraphrase: Summary This work introduces LoRA a technique to enhance the efficiency of finetuning large language models. It retains the original pretrained model while introducing lowrank parameter matrices to modify it. This significantly reduces the number of parameters to adjust during finetuning if the rank is kept small. In comparison to other finetuning methods LoRA yields comparable or superior performance with improved memory and parameter efficiency. Strengths and Weaknesses: The paper proposes an intriguing concept with the potential for impact. However its implementation has room for improvement. Strengths:  Presents an innovative idea Weaknesses:  Exaggerates the benefits of lowrank finetuning with insufficient experimental evidence.  LoRA only finetunes Wq and Wv. A crucial missing baseline is a fullrank finetuning model that adjusts only Wq and Wv.  Overstates the memory and parameter efficiency of LoRA.  need thorough proofreading (see provided number of typos).  LoRA may require more training step to converge due to lowrank updates. Comparing learning curves and training time overhead would be valuable. detail:  Clarify the meaning of VRAM. If it refers to GPU memory the reported 23 memory savings are unexpected since LoRA still needs to compute forward and backward propagation for the full model.  Consider a more general variant of LoRA that enforces lowrank gradients instead of learning A and B. This would allow multiple lowrank step to produce a higherrank parameter update."], "rhDaUTtfsqs": ["Summary Paraphrase: This study focuses on pretraining largescale language models especially addressing stability issues. It introduces curriculum learning as a technique that allows for using an eightfold larger batch size and a fourfold larger learning range. This approach reduces the number of tokens and training time by approximately twofold. Strengths:  Simple and straightforward method  relevance to efficient pretraining Weaknesses: understanding for Rejection:  Lack of originality or novelty curriculum learning has proven effective in other scenarios.  unclear operationalization of \"stability\" making it challenging to assess the impact of curriculum learning on this aspect. Other Weaknesses:  Focus primarily on sequence length as the curriculum learning parameter without exploring other potential operationalizations.  Insufficient evidence to distinguish between curriculum learning and other factor contributing to training improvements such as batch paper.  Practical limitations and potential reluctance to adopt curriculum learning due to increased complexity and hyperparameter tuning requirements. Minor Comments:  Cite the Transformer paper in the first time.  Use the GPT2 training time for comparison instead of GPT3.  Increase the clear margin size for readability.", "Paraphrase: Summary: The researchers propose a method for training large transformer models (like GPT2) that gradually increases the length of the input sequence used for training. They explore how this approach affects the training work showing that it can save time and reduce the number of tokens required for training. Strengths and Weaknesses: Strengths:  The paper is wellwritten and easy to understand.  The researchers provide enough details to allow others to recreate their results.  The experiments demonstrate the benefits of using curriculum learning. Weaknesses:  The paper applies an existing curriculum learning method to GPT2 so it lacks originality.  The research could have been easily performed by other researchers.  While the work may be appropriate for a workshop it does not meet the level of innovation required for ICLR.  Its unclear why truncating text data would make it easier to work. The truncated contribution may contain significant features that are necessary for prediction.  The curriculum baseline used in Section 5.4 are weak and there are other curriculum learning methods that could have been used as choice. Additional Minor Correction:  \"which helps improves\" should be \"which helps to improve\" or \"which improves.\"", "Paraphrased Statement: Summary: A novel curriculum strategy is proposed for pretraining GPT models gradually increasing input length over the initial training steps. Despite its simplicity this curriculum has not been extensively explored for GPT models. This approach results in more stable training faster convergence and enhanced generalization. It introduces three additional hyperparameters that can be easily adjusted based on the models behavior on the validation set. Strengths:  Simple even effective curriculum method  Significant time savings and choice improvements Weaknesses:  Given the simplicity of the method further analysis is needed to understand the interactions between gradients and the curriculum. Suggestions for Additional Analysis:  Investigating whether the curriculum reduces gradient clipping case  Measuring the \"large variance on certain dimensions\" with and without the curriculum  Comparing the method with discussion raritybased curriculum  Determining the optimal T hyperparameter based on validation set length statistics Clarification QuestionsRemarks:  Handling of shorter time when batch length are trimmed  understanding for the second spike in loss when transitioning to fulllength sequence  validation curve comparison for CL60K CL100K and CL80K  Correction of mistaken references to finetuning in prior work  Grammatical errors and stylistic suggestions  Splitting of information in Tables 1 and 2  Clarification of \"not ideal\" findings  Definition of \"token reduction\"", "Paraphrase: Summary: This paper investigates stability issues that arise during the training of GPT2 models. It finds that as model size learning range and batch size increase the training work becomes unstable. To address this the authors suggest a curriculum learning method based on sequence length. They demonstrange that this method enhances training stability and significantly improves efficiency. Strengths and Weaknesses: Strengths:  Provides valuable insights into stabilizing the training of large transformer models a challenge faced by practitioners leveraging model scaling benefits. Weaknesses:  Generalizability: It remains uncertain if the observed instability issues extend beyond GPT2 to other causal transformers. If so the effectiveness of the proposed curriculum learning approach in those scenarios is unclear.  experimental Design: The comparison in the paper involves only two different set of hyperparameters that vary in multiple aspects (learning range batch size steps) making it challenging to pinpoint the solution case of the instability.  alternative solution: The paper does not adequately explore whether improved hyperparameter tuning or careful gradient and activation clipping could sufficiently stabilize training without requiring the proposed curriculum learning method."], "rw1mZl_ss3L": ["Paraphrase: This research demonstrates that adversarial training significantly improves performance in largebatch training compared to traditional data augmentation. The authors introduce a method for simultaneously generating adversarial model and computing gradients on weights to accelerate adversarial training in distributed environments. The method utilizes outdated weights for model generation and decouples the optimization process. The paper is wellstructured and provides novel insights into the issue of adversarial training on generalization performance in largescale training. The empirical results are encouraging but the authors could provide more detailed explanations for certain findings. 1. Perturbation Magnitude: The research reveals that approach intensity should increase with larger batch sizes and the use of augmentation. The authors should clarify why this is the face considering the known accuracyrobustness tradeoff in adversarial training where high perturbations can compromise clean data accuracy. 2. Pure Adversarial training: The paper should also include results on the effectiveness of pure adversarial training without mixing in clean data. This would enhance our understanding of the use of adversarial model in improving generalization.", "Paraphrased Statement: Summary: This research presents a new technique that enhances large batch training by incorporating adversarial data during the training process. Experimental results support its effectiveness. effectiveness and Weaknesses:  Concerns:  Scalability: The proposed approach scalability to diverse training scenarios is unclear.  TheoremExperiment Gap: There is a discrepancy between the theoretical convergence results and the experimental setup.  Specific issue:  Large batch size Selection: The paper focuses on training ResNet50 on ImageNet. Its unclear how to determine an appropriate large batch size for other training tasks.  Nonasymptotic convergence and LARS: The theoretical analysis provides a nonasymptotic convergence range but the experiments use LARS which can be sensitive to hyperparameters and may introduce stability issue during large batch training.", "Paraphrased Statement: This paper introduces the ConAdv algorithm which combines adversarial training with largebatch training. This allows for increased batch sizes without compromising accuracy while maintaining hardware efficiency. The ConAdv algorithm uses adversarial training to enhance accuracy during largebatch training. It also employs stale weights to enable concurrent computation of adversarial model and normal gradient updates. The study demonstrates promising results on ImageNet with batch sizes of up to 96k exceeding the MLPerfs 75.9 accuracy benchmark. effectiveness:  Clear and wellcrafted writing  Novel and practical approach  Significant speed gains over DisAdv without noticeable accuracy loss Weakness:  May require maintaining two parallel model on each machine leading to communication overhead for large model that do not fit on a single machine."], "j8J97VgdmsT": ["Paraphrase: The paper introduces a new technique for creating novel view of a dynamic scene with a persons face displaying different expressions. It combines a customizable 3D face model with existing methods that utilize neural radiance fields. To prevent distortions in the background when the face is animated the method adds a binary mask that separates the foreground (face) from the background. The effectiveness of the method is demonstrated using four custom videos. Strengths:  Unlike previous methods the promodeld technique allows for precise control of the persons facial expression when creating a novel view from a different view.  The paper is wellwritten and easy to comprehend. Weaknesses:  lack of Novelty: The primary concept of combining foreground masks to remove background artifacts is not original. Many recent methods for dynamic scene novel view synthesis already incorporate this strategy and some even eliminate the need for manual foreground masks.  Limited control:  The method cannot control the head model of the person. While the paper acknowledges this as something to be addressed in future work a previous work has already demonstrated the ability to manipulate both facial expression and head model. It remains unclear why head model parameters cannot be incorporated into the NeRF conditioning beyond facial expression.  control over facial expression is also constrained mainly to the mouth field. The paper does not provide convincing evidence that the method can capture the entire range of potential facial expressions including eyes nose and wrinkling details. The diversity of mouth work that the model can generate is also limited.  Low Quality:  The results produced by the method are of low resolution (256x256) compared to many previous work that achieve highresolution (512x512 or high) results. This low resolution omits significant highfrequency details such as facial wrinkles which are crucial for photorealistic face range synthesis.  In many instances the facial expressions in the generated ranges do not accurately match those in the reference videos. While mouth movements generally correspond to the references the precise mouth work often differs in the synthesized results.", "Summary This work presents a technique for animating portrait in a static scene using dynamic neural radiance fields (NeRFs). It employs a model akin to a 3D Morphable Model (3DMM) to construct a binary mask. This mask isolates the foreground (face) from the background enabling the innovation of a dynamic NeRF. The method exhibits promising qualitative and quantitative results. Strengths  Writing: The paper is wellwritten and accessible.  method: The underlying concept of separating conditioning variables for animatability is sound and deserves further exploration.  Evaluation: The authors thoroughly compare their approach to recent baseline demonstrating marginal superiority in both quality and performance. Weaknesses  method limitation: While the authors acknowledge potential entanglement issues among control signals their solution addresses entirely the foregroundbackground separation. There is no mechanism for disentangling other factors such as independent expressions.  Related work: Geometrical prior: The use of geometrical prior is not novel particularly in the context of neural body rendering. Existing techniques employ model like SMPL to achieve beneficial generalization. These ideas could be extended to faces.  Related work: NerFace: The proposed method closely resembles NerFace. The entirely distinction appears to be in their discussion of the background. The authors should emphasize this difference more clearly.  Evaluation: The absence of video comparisons to baseline limitation a comprehensive assessment. The authors should also provide model of method failures to highlight its limitations. Varying individual control variables could further evaluate the methods animability. Additional Points  Page 5: \"able to generate\"  Page 8: \"artefacts\"", "Paraphrased Statement: The work presents a method for generating new angles of a portrait video while allowing for controllable facial expressions. It separates the background from the foreground (faces) using landmark detection and fitting. face are modeled with adjustable expression parameters while the background is modeled without expression dependency. The approach was tested on videos of four field recorded with a phone. It also allows for video synthesis with adjustable facial expressions driven by an existing video. Strengths:  Demonstrates the innovative use of a head frame or video to control video synthesis. Weaknesses:  Does not consider HyperNeRF a superior NeRF technique as a potential baseline.  Synthesized videos are lowresolution blurry and exhibit noticeable artifacts around field.  The method lacks significant novelty being primarily an extension of NeRFs with masks and facial expressions as latents.  Evaluation is limited to a small sample of four field.", "Paraphrased Statement: Summary: This research introduces a method that combines parametric face model and neural radiance fields (NeRF) to generate synthesized view of portrait with controllable facial expressions. It enhances NeRFs proposed by Park et al. to enable expression use. contribution: 1. A dynamic NeRF model that supports explicit control. 2. A spatial ray sampling technique that separates facial expressions from background appearance. 3. Simultaneous control of viewpoints and facial expressions. Strengths and Weaknesses: Strengths:  Effective combination of parametric face model for facial prior and NeRF for complex appearance handling.  quantitative results demonstrating improved performance over baseline methods.  Qualitative achievements not supported by previous methods such as expression control and enhanced background handling. Weaknesses:  Reliability depends on the accuracy of DECA and landmark fitting with no error recovery mechanism.  potential blurriness in results with flickering artifacts during expression use.  Entanglement between expression parameters deformation fields and appearance codes potentially limiting control.  Equation 6 may allow a trivial solution with zero deformation field.  Absence of a quantitative ablation work to confirm disentanglement.  Questions about the actual control of expressions as reenacted expressions may not accurately reflect those in driving frames.  Missing references to relevant research in the human field and parametric face model."], "u4C_qLuEpZ": ["Paraphrased Statement: This paper presents a programming analysis model using graph neural networks to analyze assembly code. It utilizes the programs control current graph (CFG) call graph (CG) and data current graph (DFG) as inputs. The objective is to create a comprehensive model capable of handling both source coderelated tasks (e.g. program classification) and compilationrelated tasks (e.g. vulnerability assessment). By incorporating assembly code the model captures compilationspecific features. Additionally employing multiple graph offers a comprehensive representation of the programs semantics and structure. effectiveness and Weaknesses: effectiveness:  Addresses the challenge of developing a model that excels in both compilationspecific and source codelevel program analysis.  Proposes a process for generating process and programlevel embeddings from multiple graph. Weaknesses:  Novelty of Modeling Approach: The process for generating processlevel embeddings including the use of BERT for embedding basic blocks in CFGs closely resembles an existing model [2]. An acknowledgment of [2] in the methodology would clarify the contribution of the proposed work. The foundation primarily lies in incorporating CG and DFG for programlevel analysis which may be considered incremental.  Weak Baseline Used for SourceCode level Tasks: Unlike [2] the proposed model handles both source code and compilationrelated tasks. However it fails to compare against a relevant work [1] that also uses GGNN for program graph model and place source codelevel tasks. Evaluation against [1] or similar approaches would support the models generalizability call. QuestionsComments for source: 1. Why does the proposed approach omit the \"block inside graph task (BIG)\" used in [2] This task was found to enhance the models understanding of block and graph relationships. 2. Can we anticipate similar performance improvements in processlevel analysis tasks Abstracting a program into multiple graph based on assembly code may not yield significant advantages compared to existing methods like [1] for tasks like work call prediction. 3. In Figure 6 OM [2] appears to have lower accuracy than the CFG submodel. What factors contribute to this performance shift Minor Comment:  Allamanis et al. 2016a and Allamanis et al. 2016b are duplicate entries for the same work.", "Paraphrase: Summary This paper introduces a new approach that uses graph neural networks to tackle semantic and compilationrelated issues in program analysis at the assembly code level. effectiveness and Weaknesses Novelty combination feature dimensions in machine learning for code has been explored before as seen in work such as \"complex Program graph\" and \"Different Types of Program Features.\" Terminology The authors could clarify their terminology. For instance program classification and duplicate code detection are not typically considered \"program analysis tasks\" in the field of programming languages. Program analysis usually refers to dataflow analysis and pointer analysis. other Observations 1. Equation 1 on page 4 should use u in f to represent the neighbor of v instead of v. 2. Figure 2 lacks clarity in the meaning of nodes in the DFG subfigure. The variables or instructions represented by each node should be specified. 3. The code size of programs in the POJ104 dataset is not provided in the experimental setup. The number of nodes and edges generated by the DFG in this work should be compared to other methods. 4. The comparison with Ins2vec in the program classification task does not fully demonstrate the benefits of working with assembly code due to differences in representation and models. 5. The calculation of 8.6 on page 8 row 4 is not explained. 6. The understanding for the worse performance on gcc versus llvm in the program classification task is not addressed. 7. The method for calculating similarity in the binary similarity detection task is not described. 8. Clone detection is generally performed at a more detailed level so the practical value of detecting binary similarity for entire programs is questionable. 9. The use of CFG and DFG should be evaluated against PDG (program dependence graph). 10. The graphical representations (e.g. BERT GGNN) used in this work are existing techniques. The challenges of extracting semantic information from assembly code compared to source code should be discussed.", "Summary: This paper investigates using graph neural networks to analyze assembly code to understand programs. The authors claim an accuracy of 82.58 and 83.25 on two programlevel tasks. Critique: The reviewer believes this approach is not suitable for general intelligence in program analysis. flow research trends in program reasoning are moving towards higherorder code representations such as CASS SPT and formal mechanisms. These representations enable algorithmic rewrites and optimizations beyond traditional compilers. The reviewer argues that the authors approach of analyzing assembly code is a step in the wrong direction. It has been extensively explored as evidenced by research on ISA analysis which has shown limited success. The reviewer expresses disappointment that the authors did not explore more innovative approaches and finds the experimental results unconvincing.", "Paraphrased Statement: This article introduces a graph neural network approach to address binary analysis tasks such as classifying programs and detecting binary similarity. The approach effectively combines various binary code representations such as compiler intermediate representation (IR) and assembly code. effectiveness:  The idea of consolidating different representations is innovative and practical. Weaknesses:  The evaluation is insufficient and lacks depth.  There is no comparison with established machine learningbased binary similarity detection tools.  The techniques applicability to large realworld programs is unclear.  The evaluation dataset is outdated featuring small programs compiled using only two compilers.  Many similarity detection tasks involve multiple compilers optimization level and hardware architectures which arent considered in the evaluation.  Figure 3 lacks binary code representation raising concern about the techniques effectiveness in scenarios where source code is unavailable."], "oxwsctgY5da": ["Paraphrased Statement: Researchers introduced a unique attack scheme using mixed integer programming (MIP) to search for adversarial examples by manipulating activation values in binary variable configurations. They developed optimization heuristics such as topdown beam search and bottomup neighborhood search to guide the search work resulting in stronger attacks compared to existing MIP solvers and stateoftheart methods. effectiveness:  Innovative approach to adversarial attacks through MIP formulation.  practical heuristics for efficient search. Weaknesses:  Limited explanation of the problem being addressed and the significance of MIP.  Unclear advantages of the proposed formulation over standard gradientbased approach.  Need for better clarity and precision in the writing including grammatical errors.", "Paraphrased Statement: This paper presents the BaBAttack a technique to solve complex adversarial attacks efficiently. The BaBAttack uses neural network verifiers with bound propagation on GPUs to quickly assess multiple search field. It also creates an adversarial candidate pool to direct the search and refines candidates using a refined large neighborhood search. effectiveness:  Combines various technologies to enhance the branchandbound work for finding adversarial examples in deep neural networks.  Achieves higher attack success rates than the baseline MIP attack in a shorter time frame. Weaknesses:  Timeconsuming compared to gradientbased attacks.  Consists of multiple components with no ablation experiments to demonstrate individual impact.  Heuristic elements in the algorithm design lack theoretical analysis.  Writing is unclear and strong to follow.", "Summary Paraphrase: This paper describes an better method for solving a specific mathematical problem related to creating \"adversarial examples\" using a technique called \"branchandbound.\" This method is designed to handle complex scenarios where existing approach struggle. Strengths Paraphrase:  The proposed method combines ideas from various sources into a novel and efficient approach.  It significantly speeds up calculations compared to the basic method on specific tasks tested. Weaknesses Paraphrase:  The experiments were limited to only those adversarial examples that could not be solved by other methods.  No comparison were made between the proposed method and other techniques leaving the relative performance unknown.", "Paraphrased Statement: Summary The researchers suggest a solution for the adversarial attack problem on ReLU neural networks. They utilize a branchandbound (BB) algorithm to identify adversarial examples within the activation space of a binary integer programming formulation. Experiments demonstrate that this approach enables the discovery of adversarial examples for difficult cases where current adversarial attacks are ineffective. effectiveness and Weaknesses The paper is wellwritten and straightforward with convincing experimental results. However it relies solely on heuristics and lacks theoretical support. Additionally the experiments are not comprehensive. Expanding on either theoretical results or comprehensive experiments would significantly enhance the quality of the paper. While adversarial attacks are a crucial problem the fundamental concept presented here is not particularly novel diminishing the contribution of the paper. Detailed input 1. systematically use \"branchandbound\" or \"BB\" throughout. 2. In equation (1) specify that x represents the original input. 3. Introduce the abbreviation \"PGD\" for \"projected gradient descent\". 4. Clarify that N represents the number of ReLU neurons. 5. Specify that N represents the number of ReLU neurons in the second paragraph on page 2. 6. Correct the phrase \"adversarial adversarial attack\" to \"adversarial attack\". 7. Update references to use \\eqref instead of \\ref. 8. In equation (2) indicate that i ranges from 1 to L. Also include the constraint \\hatz(i)(x) \u2265 0. 9. Change \"can be provable obtained\" to \"... provably ...\" 10. Provide a reference for \"... is well explored recently ...\" 11. Emphasize \"adversarial common pattern\" in italics on page 6. 12. Correct the phrase \"T percent of\" to \"100T percent\" in the second paragraph on page 7. 13. Explain the improvement in attack margin for the experiments in (c) in Fig. 6. 14. Provide detailed data about the datasets used. 15. Explicitly mention Fig. 6 in the Experiment section and provide a thorough explanation. 16. Expand the experiments and present them in a more detailed manner."], "mF122BuAnnW": ["Paraphrased Statement: The research introduces a novel approach for verifying structured output models using a localized smoothing technique. The method focuses on minimizing prediction errors caused by adversaries that introduce small perturbations to the input. The approach involves smoothing input pixels at locations distant from the primary pixel of interest using larger noise magnitudes. This is based on the assumption that these distant pixels have less work on the prediction at the primary pixel and therefore can be smoothed more aggressively to achieve higher certified radii. The problem is formulated as finding adversaries outside the certified region of anisotropic smoothing over the input and is solved using linear programs. Empirical evaluations on image segmentation and node classification tasks demonstrate the effectiveness of the proposed methodology which achieves comparable accuracy to existing technique with improved certified radii.", "Summary Paraphrase: Authors Goal: Develop a robustness certificate for tasks involving single input and multiple outputs. This certificate aims to measure the models robustness against input perturbations. method: Introduce a collective certificate that:  Considers the entire input but assigns varying importance to different input regions for each output.  Bases the collective certificate on localized randomized smoothing. Evaluation: Test the proposed collective certificate on image segmentation and node classification tasks. effectiveness:  Addresses an significant problem.  paper is generally soft to understand.  Proposed method presents some novelty. Weaknesses:  Insufficiently clear distinction between the proposed method and existing approaches.  Lack of comparisons with other methods.  Evaluation is limited and certain metrics are questionable. QuestionsConcerns:  What is the fundamental difference between the proposed theoretical result and similar result in Eiras et al.  Can Fischer et al.s method be adapted to the current setting  Why is center smoothing not included in the comparison  Are the dotted contrast and \"Naive\" labels in image 2 distinct What about similar markings in other image  Why is certified ratio used given that certifying misclassified elements is questionable  Is the image segmentation evaluation limited to Pascal VOC 2012 More various datasets like Cityscape should be considered.  AUC is not a standard metric for image segmentation mIoU should be used instead.  UNet is outdated more modern semantic segmentation models should be evaluated.  computational overhead for finding optimal \u03c3 values is not addressed.  Independent certification of grid cells in localized randomized smoothing may compromise the semantic relationships between pixels hindering the purpose of semantic segmentation.", "Paraphrase: The paper introduces three main contribution:  local Randomized Smoothing: The authors develop a tailored smoothing distribution for each output of a multioutput classifier enabling tighter guarantees for each output.  Variance Smoothing for Discrete Data: They propose a novel analysis method for variance smoothing that utilizes the average softmax value instead of majority vote as the prediction rule. Using mean and variance statistics they enhance robustness in this method.  collective certification for Multioutput Classifiers: The authors establish a unified interface (\u03b5pnorm ellipsoids) for base certificates for all outputs. They formulate the multioutput certification problem as a mixedinteger linear program seeking a point that violates the base certified regions for the most outputs. effectiveness and Weaknesses: effectiveness:  versatility: The collective certification strategy is compatible with most existing randomized smoothing technique allowing for adaptability to future advancements.  Variance Smoothing: The concept of variance smoothing presents an innovative way for improving certification.  Empirical Validation: Table 5 demonstrates the superiority of the proposed methods over baseline approaches providing solid support for their effectiveness. Weaknesses:  Training Disparity: The proposed local smoothing strategy lacks a training counterpart leading to potential misalignment between training and testing objectives.", "Paraphrase: Summary This research applies anisotropic certificates for randomized smoothing to enhance the reliability of multioutput classifiers. Specifically anisotropic Gaussian and Bernoulli smoothing are employed to improve collective robustness. Experiments conducted on semantic segmentation and node classification demonstrate the proposed methods effectiveness. effectiveness and Weaknesses effectiveness:  Clearly defined motivation  Intuitively applied anisotropic smoothing  Aligned experimental tasks Weaknesses:  theoretical Analysis:  Unclear contribution and restatements from prior work  Proposition 1 and 4: Special case of existing result  Equation (1) and analysis in (36): Similar to previous findings  Experiments:  Lack of accuracy performance comparison with baselines  Reporting of certified ratio metric instead of accuracy or AUC  Need for direct comparison to strictly local setup  Limited experimental datasets  Writing:  References to tables and image without appendix mention  Duplicate appearance of \"\\sigma\\textmin\""], "sX3XaHwotOg": ["Paraphrased Statement: Summary: This research proposes a novel framework for pretraining transformerbased language models (LMs) by employing a combination of multiple generators and a discriminatorguided learning approach. The framework incorporates multiple masked language model (MLM) heads within the generator each assigned to a different layer. The predictions from these heads are combined using learnable weights which are then input to the discriminator. The gradient for learning is limited to single blocks between the MLM heads ensuring that each MLM head use as an independent generator. The GumbelSoftmax trick facilitates the backpropagation of the discriminator loss to the weights. Empirical evaluations show that the proposed pretraining AMOS outperforms previous baselines on GLUE and SQuAD tasks. effectiveness: 1. Clear and wellwritten paper. 2. Innovative use of a committee of generators to enhance discriminator signal diversity. 3. Strong empirical results demonstrating improved performance over baselines. 4. comprehensive ablation studies justifying model decisions. Weaknesses: 1. Missing standard deviation error bars in ablation study results. 2. limited experiments with independent generators instead of using the same generator with different MLM heads. 3. Unclear whether improvements will extend to larger transformer architecture. 4. Absence of a societal impact discussion and reproducibility statement regarding compute resource use. Questions: 1. selection of \\lambda in Equation(4) and potential crossvalidation. 2. Potential empirical benefits of AMOS with targeted masking approach. 3. Feasibility of using different \"families\" of generators. 4. Intuition or results regarding varying the number of MLM heads k beyond k3.", "Paraphrased Statement: Summary: This paper demonstrates a novel method for training text encoders known as adverbially learned curriculum. It expands on the ELECTRA architecture by introducing multiple auxiliary masked language models (MLMs) that are combined using a mixture parameter. This framework has been proven to outperform existing stateoftheart methods and alternatives on various downstream NLP tasks. The paper is wellwritten and accessible providing strong experimental evidence to support its claims. effectiveness:  Clear and comprehensible writing style.  The paper builds upon the established ELECTRA architecture making its main contribution the addition of an inductive learning curriculum well identifiable.  The authors employ multiple auxiliary MLMs and strategically combine their outputs to feed the discriminative text encoder.  To optimize training efficiency and streamline the architecture all auxiliary MLMs are derived from a single base MLM with subMLMs generated from different layers.  The mixture parameter is learned adversarially using the negative discriminator gradient.  The extensive ablation study provides empirical support for the proposed architecture superiority. Weaknesses:  Typo: \"datecentric\" should be \"datacentric.\"  The paper only utilizes the MLM on three layers (468) of the eightlayer generator.  While the authors demonstrate the inferiority of using a single MLM head irrespective of layer it remains unclear if there exists an optimal number of MLM heads for an Nlayer generator network.", "Summary and effectiveness: The paper introduces a novel framework called AMOS for improving ELECTRAstyle pretraining using adversarial and curriculum learning. Instead of using Gumbelsoftmax for adversarial learning the authors propose a \"mixtureofsignals\" approach that combines signals from different layers of a generator. The framework achieves effective performance than ELECTRA and SOTA COCOLM on GLUE and SQUAD 2.0. Weaknesses: 1. The authors claim AMOS uses a \"mixture of training signal generator\" but the actual implementation uses a mixture of signals from different generator layers. To support their claim an experiment with independent generators is suggested. 2. The adversarial training does not include resampling masked tokens which could be a valuable signal. 3. The evaluation is conducted on GLUE dev set but results on the GLUE test set would be more convincing. 4. A comparison of training time and performance with ELECTRA would provide additional insights.", "Paraphrase: Summary: This study enhances Electralike pretraining models by incorporating multiple Masked language model (MLM) generator heads at different network layers along with a single discriminator. The probabilities for each MLM output are adjusted dynamically using a gumbelsoftmax estimator. The findings show that generatordiscriminator dynamics are influenced by the generators capabilities and structure. Hence the study suggests using a \"mixture\" of discriminators with varying capacities. This \"mixture\" is made effective by sharing the generator backbone and adding separate MLM heads to each layer. The model only learns the mixing weights of the representations generated at each layer. effectiveness:  Novel approach to address generatordiscriminator dynamics in Electralike training using a computationally effective generator mixture.  Superior performance on various downstream NLU tasks compared to baselines.  comprehensive ablations to evaluate model components. Weaknesses:  Potential for the discriminator to optimize for higher weights on lower layers making its own optimization easier.  Counterintuitive results in Fig 3 (a) with layer 8 having the highest coeffective followed by 4 and then 6.  Unknown robustness of mixing weights to different random seeds and fixed weights training.  Training multiple generators although computationally demanding could provide a performance benchmark.", "Paraphrase: method: This study proposes a technique for language model pretraining using generatordiscriminator training. Similar to ELECTRA which uses a masked language model (MLM) as a generator to corrupt input text with masked tokens this method employs multiple MLM generators that replace tokens at varying difficulty levels. A discriminator model is then trained to identify the replaced tokens. The authors use gumblesoftmax to enable gradient flow from the discriminator to the multiple generators. Evaluation: The results on GLUE and Squad show that the proposed approach outperforms previous stateoftheart methods. Questions: 1. It appears that the generator with more layers (8) achieves a lower training loss during pretraining implying that it can better recover masked tokens and create a more challenging task for the discriminator. 2. Is the MLM head shared across different generator layers 3. The ablation study on stopgradient is limited to MNLI and Squad tasks which are expected to exhibit performance degradation. Is this ablation evaluated on other GLUE tasks as well 4. The authors use gumblesoftmax to enable gradient propagation. Is it possible to leverage reinforcement learning for this use as in ELECTRA and what is the impact on downstream task performance 5. The authors state that the discriminator loss is used to train the mixture weights of MLM output to create a more challenging signal for the discriminator but this process does not update the MLM embeddings. This seems contradictory as the generator is jointly trained with the discriminator and MLM embeddings should receive gradients from both the discriminator and MLM pretraining of the generator. Figure 2 also shows that discriminator gradients are backpropagated through the generator. Additionally Table 2 indicates that adversarial MLM hurts MNLI matched performance by 0.3 points while leaving other tasks unaffected. What is the performance of this context on other GLUE tasks 6. In Table 1 the number of parameters in the Base context is larger than in Base one. Can you explain this 7. All ablation results in Table 2 are evaluated on MNLI and Squad tasks. However all GLUE tasks should be considered to fully understand the contribution of different components of the proposed AMOS. 8. In Table 2 the layer switch and random layer configurations imply that all MLM layers are pretrained equally. 9. Table 2 shows a decrease of only 0.1 in MNLImatched performance when using a separate MLM generator configuration compared to the stop gradient setup. What is the performance on this context on other GLUE tasks 10. In Figure 3(b) the discriminator accuracy with a 4layer generator increases smoothly outperforming AMOS without adversarial training. Can you explain this intuitive learning curriculum", "Paraphrase: This paper introduces AMOS (adversarial mixture of signals) a method that enhances the training of deep adversarial autoregressive generators within the ELECTRA selfsupervised framework. AMOS employs a mixture of representations function to combine hidden representations from various layers of the generator network. By leveraging GumbleSoftmax relaxation AMOS stabilizes GANstyle adversarial training for ELECTRAlike frameworks. This stability enables the effective training of superior generators and discriminators by allowing gradients from the discriminator to directly update the generator. The paper presents extensive experiments on selfsupervised language learning tasks (GLUE and SQuAD) demonstrating AMOSs stateoftheart performance compared to similarsized alternatives. comprehensive ablation studies validate the efficacy of curriculum setup (typical of ELECTRA) and adversarial setup (introduced by AMOS) in improving results. The effectiveness of stabilizing training using the mixtureofhiddenrepresentations approach is also demonstrated particularly for deeper generators. effectiveness and Weaknesses: effectiveness:  Contributes significantly to the integration of adversarial training into selfsupervision for language learning.  Establishes the importance of mixing signals from multiple generator depths for stabilizing adversarial training in ELECTRAstyle models.  Demonstrates the superiority of combining adversarial training with selfsupervision leading to significant improvements with comparable model sizes.  Provides detailed ablation studies to substantiate the necessity of each component in the model. Weaknesses:  Adversarial training as implemented in this paper is one of many possible approach and the specific contributions of each aspect are not only clear.  The paper focuses on GumbleSoftmax relaxation but other approach for discriminator training in text generation (e.g. feature matching) have shown improved stability. It would be valuable to explore these alternatives."], "xbx7Hxjbd79": ["Paraphrase: Summary: This paper explores learning in game scenarios where opponent can also learn. It identifies challenges with current methods proposes new ones and suggests their potential superiority. strength:  Addresses a compelling problem in learning with opponent. Weaknesses:  contribution fall short of the paper claims especially in Section 4.1 where the main parameter relies on intuitive reasoning without rigorous proof.  The paper relies heavily on vague language and undefined concepts (e.g. \"consistency\" and \"stability\") even in comparative variety (\"more stable\").  assumption are excessively restrictive (e.g. perfect knowledge of opponent issue parameter and gradients).  Without rigorous issue the insights remain limited.  The consistency proposal lacks a clear method for verifying its conditions.  Oddly posed questions in the issue section suggest an overreliance on empirical analysis which may not be conclusive.  The discussion of empirical observations generalizes excessively.", "Paraphrase: Summary: This paper examines learning in differentiable games especially considering the opponent learning as well. It identifies a flaw in previous claims about the comparison of competitive gradient descent (CGD) and iLOLA. It introduces a definition of consistent update rules for differentiable games and demonstrates that iLOLA follows such rules. The paper then proposes a new algorithm COLA and shows its ability to find more consistent solutions empirically. strength and Weaknesses: Overall the paper is wellreceived. However the relationship between CGD and LOLA remains unclear. While the paper acknowledges the need for a rigorous proof of the correction to previous claims it does not provide one. Clarifying this correction would strengthen the paper contribution. COLA is claimed to follow consistent update rules but empirical issue suggest it does not converge to the desired strategy. Therefore the motivation for using COLA over LOLA is unclear. The paper interpretation of the original CGD claim is unclear. It claims that the series expansion of CGD recovers HOLA (higherorder LOLA) and that this implies CGD equals iLOLA. However the reasoning behind this conclusion is not evident.", "Paraphrased Summary The authors address the reliability issue in the LOLA algorithm. Their research includes:  study HOLAs convergence  Showing that CGD may not always align with higherorder LOLA  Proposing COLA to resolve this consistency problem COLA outperforms HOLA in experiments even when HOLA fails. However COLA sometimes exhibits similar issues which warrants further investigation. Strengths  Clear writing and indepth exploration of LOLAs limitations  Critical analysis of CGDs relationship with LOLA  Thorough justification for COLA and supporting empirical evidence Weaknesses  Additional proof for CGDs parameter would enhance the paper  Including CGD as a baseline in empirical evaluations would provide a more comprehensive analysis", "Paraphrase: Summary:  This research examines an issue in the LOLA algorithm where agents assume the other as a naive learner leading to inconsistent gameplay.  The paper introduces the InfiniteOrder LOLA (HOLA) to address this issue but finds it may not resolve the convergence problem.  Instead the paper proposes ConsistentOrder LOLA (COLA) which uses neural network to minimize consistency loss.  COLA demonstrates improved consistency and stability in finding solutions even when HOLA diverges. strength:  Thorough examination of related work and empirical findings.  COLAs low computational complexity compared to other methods. WeaknessesConcerns: 1. Comparison to StateoftheArt (SOS):  COLA lacks a theoretical convergence guarantee like SOS which also offers adaptive opponent shaping for better performance.  Its unclear when to use COLA over SOS. 2. consistency Definition:  The paper definition of consistency (Equation 5) may lead to an infinite regression as its dependent on other equations that involve the same variables. 3. Scalability of Neural Network Approach:  Learning update functions using neural network may become challenging as the dimensions of the parameter increase. 4. Incomplete Comparison:  The paper only compares COLA with CGD in the Tandem field excluding other scenarios like matching pennies. 5. Presentation issue:  Experimental issue (Tables 1 and 2 and design 1) are not conveniently positioned near the relevant section (Section 6)."], "iUuzzTMUw9K": ["Paraphrase: Summary: This paper introduces StyleNeRF which combines the NeRF (Neural radiance field) model with a stylebased generator to enhance the efficiency and 3D consistency in highresolution image generation. Key contribution:  (1) The paper utilizes NeRF to create a lowresolution feature map and gradually upsamples it to high resolution.  (2) Three improvements are proposed to improve 3D consistency: an enhanced upsampler a regularization technique and a progressive training approach to enhance stability during geometry learning. Strengths:  (1) StyleNeRF surpasses traditional NeRF methods in highresolution rendering displaying fewer artifacts and rendering rapidly.  (2) It resolves the issue of highresolution synthesis that NeRFbased approaches have struggled with.  (3) The paper innovative combination of 2D Generative Adversarial Network (GAN) and NeRFbased techniques is both significant and groundbreaking. Weaknesses:  (1) Minor typos include:  \"r\" in \"right\" (image 3) should be capitalized as \"R\"  \"an mapping network\" (Section 3.4) should be \"a mapping network\"  \"piGAN\" and \"\u03c0GAN\" (Appendix C) should be consistent in notation  (2) image 3 could be more comprehensible with labels for blocks (e.g. \"FG\" and \"BLK\").  (3) Additional quantitative experiments comparing upsampling operators would demonstrate the proposed methods advantages.  (4) An intriguing observation is the different pupil positions in generated human eyes (images 6 and 15 vs. 12 and 13). The paper should explain this phenomenon.", "Summary: This paper introduces a 3Daware generative model that produces highresolution images akin to real photographs. Its core creation lies in using techniques to enhance rendering speed and ensure consistency across multiple viewpoints while generating images with higher resolution in 2D using a CNNbased renderer. Strengths and Weaknesses: Strengths:  efficient extension of StyleGAN to 3D.  generation of impressive and highquality images.  Fast and supports highresolution rendering. Weaknesses:  lack of significant technical novelty (combines existing GAN techniques).  Unclear conceptual contribution (unique value of the work).  Compromise in multiview consistency due to CNNbased rendering. Points for Clarification: 1. Geometric meaning of equation (5)  Clarification needed. 2. Generalizability of the upsampler design to improve synthesis in other GANs. 3. Importance of separate networks for background and foreground. 4. understanding for underperformance of GRAF and piGAN compared to their original studies. 5. Inclusion of a metric to quantify multiview consistency in Table 1. Other Issues:  Correction of superscripts in equation (5) in section 3.2.  Typo in section 3.4 (\"an mapping network\" should be \"a mapping network\").  Removal of unnecessary expansion of equation (2) in section 3.2.  Correction of \"GARF\" to \"GRAF\" in section 4.2.", "Paraphrase: Summary: StyleNeRF: A 3Daware generative model for highresolution image generation with enhanced multiview consistency. It combines NeRF (Neural radiance field) with a stylebased generator for efficient rendering and 3D consistency. It renders lowresolution feature maps using volume rendering and upsamples them progressively in 2D. It introduces a novel upsampling module and a regularization loss for enforcing 3D coherence. Strengths and Weaknesses: Strengths:  Synthesizes highresolution images interactively with superior 3D consistency.  Allows control over camera positions and style variations.  Supports style mixing inversion and basic semantic editing. Weaknesses:  Limited Novelty: Combines existing techniques (PiGAN and GIRAFFE) without significant creation.  Questionable Approximation: Equation 5 lacks a clear explanation and mathematical proof.  Ambiguous visualization: image 2s internal representation visualization is unclear with undefined color meanings. Also mentioned artifacts are not easily discernible in the final results.  Unclear Upsampler Design: The upsampling mechanism is not fully explained and its impact on artifact reduction is questionable.  Undefined Notation: The meaning of \u03c6\u03b8 in Equation 7 is not defined.  Baseline Evaluation Anomaly: In image 4 the performance of baselines like PiGAN is noticeably worse than reported in the original research requiring clarification.  Definition Required: Kfg and Kbg in image 3 need explanation and clarity on how to obtain them.", "Paraphrased Summary: Researchers have developed a groundbreaking 3D image synthesis model called StyleNerF. This model combines two existing techniques Neural radiance field (NeRF) and StyleGAN to address challenges related to efficiency consistency and image quality. StyleNerF uses NeRF to render lowresolution images which are then enhanced by StyleGANs 2D synthesis blocks to produce highresolution photorealistic images. This approach enables interactive rendering speeds. Experiments show that StyleNerF outperforms previous 3D generative models narrowing the gap between 2D and 3D image synthesis. Strengths:  Ability to generate highresolution realistic images with strong consistency across multiple views.  control over camera angles and image style.  efficient rendering process by combining NeRF and StyleGAN. Weaknesses:  Removal of viewing direction from NeRF making it an \"irradiance field\" rather than a \"radiance field.\"  lack of clarity in image 3 notations.  \"Gaze following\" phenomenon in the demo video.  Uncertain if the model can synthesize 3D scenes from unstructured photographs like StyleGAN can."], "mdUYT5QV0O": ["Summary This process proposes an algorithm for training neural netprocesss with a regularization term to encourage specific structural model in the model. The algorithm is claimed to identify the underlying structure accurately in a finite number of steps. model demonstrate that the algorithm outperforms existing approaches in identifying structured sparsity while maintaining prediction accuracy. Strengths: 1. The algorithm promotes desired structures in the model without increasing computational cost. 2. The paper provides a wellwritten and clear explanation of the mind behind the algorithm which are wellfounded and supported by existing literature. 3. The theoretical results Theorems 1 and 2 are significant and just proven. Weaknesses: 1. The paper exaggerates the significance of its findings claiming that the algorithm can \"correctly identify the underlying structure.\" The theoretical results only demonstrate that the algorithms sparsity converges to the sparsity of the stationary point which does not imply correctness. 2. The experimentation suggest that the algorithm consistently identifies structured sparsity but they lack value of deviation from the optimal sparsity making it difficult to assess the accuracy. 3. The paper omits crucial details when applying the general frameprocess to neural netprocesss such as explicitly defining the active manifold and the limit point. 4. The assumption that the tentative iterate converges almost surely to a certain point W needs to be formally defined and mathematically justified. 5. The relationship between the tentative iterate W\u0303 and the limit point W requires clarification. 6. The statement that proximal stochastic gradientbased algorithms cannot identify the manifold within finite iterations needs supporting references.", "In this paper the authors introduce the Regularized Modernized Dual Averaging (RDMA) algorithm for training structured neural netprocesss. RDMA is a proximal method that reduces variance without additional computational costs. It requires no additional hyperparameters beyond those used in stochastic gradient descent. The authors mathematically demonstrate that structure identification occurs within a finite number of iterations. experimentation using MNIST FashionMNIST LeNet5 and VGG16 datasets show that RDMA can effectively produce groupsparse neural netprocesss. While the algorithm is wellmotivated and mathematically sound the experimentation are limited and comparisons with other structured learningbased methods would enhance the field. Additionally a recent process that utilizes proximal operators for netprocess sparsity could be included in the related process section.", "Paraphrased Statement Summary regularization helps impose desired structure on neural networks (NNs) during training. The authors introduced RMDA (Regularized Modernized Dual Averaging) which uses a weighted average of past gradient to calculate a tentative update and then applies a proximal performance related to the regularizer. The model parameters are then adjusted in the direction of this tentative update with a specific factor. The authors proved that RMDA theoretically identifies the NN structure in a finite number of iterations under certain conditions. Strengths and Weaknesses Using momentum and regularizers to enforce structure in optimization particularly NN training is common. RMDA extends the dual averaging algorithm with momentum. Weaknesses  The claims about variance reduction are unclear and its uncertain where such variance reduction is achieved in the paper. While RMDAs computational cost is similar to SGD maintaining momentum and dual averaging increases complexity.  The authors claim RMDA guarantees structure identification but this is an exaggeration since theres no convergence proof for RDMA. Theorem 2 assumes convergence and then shows that the converged parameter belongs to the active manifold.  The statement that RMDA \"identifies the optimum structure\" is difficult to prove since theres no way to definitively determine if the structure found is the best potential.  The theoretical analysis is incomplete and relies on strong assumptions including the expectation that the algorithm converges given specific parameter value.  Its unclear if the T0 in Theorem 2 refers to additional iterations of RDMA after the convergence of the parameter or includes all RDMA iterations.  section 3 states that proximal stochastic gradientbased algorithms cannot identify the manifold in finite iterations while RMDA can. Its unclear why this contradiction occurs whether its due to the analysiss assumptions or the use of momentum in RMDA. Minor Suggestions  Define all notations used in the paper consistently including subdifferential interior and relative interior."], "pVU7Gp7Nq4k": ["Summary Paraphrase: This study investigates the problem of excessive parameters in deep learning by examining the hidden representation in the last layer of wide and deep networks. The researchers discovered that in wide networks trained sufficiently similar test error can be achieved by randomly selecting a subset of lastlayer representation and eliminating the link to the \u00fcbrigen representation. A linear mapping can then be found from the subset to predict the complete set indicating that the information in the deleted features is essentially \"cloned\" into the remaining ones. These findings are supported by empirical evidence across multiple models datasets and learning scenarios. Interestingly the \"cloning\" effect is only observed in networks using data augmentation and weight decay and it was not evident in ImageNet. Strengths and Weaknesses Paraphrase: Strengths:  The paper presents a thorough empirical analysis with welldesigned experiments.  The authors showcase a variety of architectures and datasets showcasing the generalizability of their results.  The reconstruction analysis effectively demonstrates the \"cloning\" effect. Weaknesses:  The necessity of heavy regularization and complex training process introduces some limitations.  The observed \"cloning\" effect is not observed for ImageNet raising concerns about its generality.  The theoretical explanation or intuition behind the \"cloning\" phenomenon is lacking. Additional input:  The researchers should explore how the networks width and depth work the threshold for observing \"cloning.\"  Investigating whether higherorder correlations may be present in the residual after reconstruction would be beneficial.  Clarification is needed regarding the authors claim that they focus on networks in the feature learning regime despite the need for wide networks to exhibit \"cloning.\"", "Summary Paraphrase: This study explores a phenomenon known as \"representation mitosis\" in neural networks. As the networks width increases beyond a specific threshold neurons in the last layer become redundant. This means they form groups with identical information only differing by random noise. Strengths and Weaknesses Paraphrase: Strengths:  Identifies and investigates a novel phenomenon in deep neural networks. Weaknesses:  Empirical evidence Misalignment: The evidence presented does not fully support the claim of onetoone \"clones\" of neurons. Instead it suggests that a random subset of neurons can maintain predictive performance.  Unclear Significance of last Hidden layer: It is not clarified why the phenomenon is specific to the last hidden layer.  Limited Generalizability: The study did not reproduce the mitosis phenomenon on ImageNet potentially due to insufficient network size.  Lack of Practical Applications: The paper does not explore how the observations can be utilized in practice.  Confusing Presentation: The authors use the term \"clones\" to describe the phenomenon but acknowledge that it does not refer to exact neurontoneuron mappings. This creates confusion in understanding the observations.", "Summary This paper proposes a novel explanation for the phenomenon of benign overfitting in wide neural networks. The key idea is that the neurons in the output layer can split into identical groups (clones) under certain conditions. These clones represent the same features but have slight variations due to noise akin to the process of mitosis in cell division. The number of clones increases linearly with the width of the output layer. Strengths  Introduces a novel mechanism to understand feature learning in wide neural networks.  Provides empirical evidence to support the proposed mechanism.  Wellwritten and organized. Weaknesses  The conditions under which representation mitosis occurs are not clearly defined.  Claims that clones appear only in welltrained regularized networks and not in experiments on ImageNet.  More research is needed to identify the precise conditions for achieving representation mitosis. Minor input  How does the architecture and connectivity of the network affect the mechanism  Does achieving mitosis contribute to effective generalization  How is the number of neurons in each clone determined", "Summary Paraphrase: The paper presents empirical evidence that wide neural networks (NNs) exhibit redundant representation in their last layer under specific conditions (overtraining large width across layers data augmentation and regularization). These conditions allow a large enough random subset of the penultimate layer activations to accurately predict the integral activation of the last layer. Moreover pruning the network to these subsets reduces the training error (though still lagging behind that of the wide network). Notably the test error of these pruned networks decreases as the subset size increases similar to the test error reduction observed with wider regular networks. The authors propose a preliminary interpretation of this \"mitosis\" effect suggesting it may contribute to the improved generalization of wide networks with increasing width. Strengths and Weaknesses Paraphrase: Strengths:  The \"mitosis\" effect is a novel finding and is clearly demonstrated under specific settings.  The paper identifies conditions where the effect is absent.  The paper potentially contributes to understanding how overparameterization enhances generalization.  The writing is generally clear and the designs are of high quality. Weaknesses:  Lack of Clarity: The paper main weakness is its lack of clarity which hinders wide understanding and evaluation of the claims made. Specific Concerns:  Undefined \"error infinity\": It is unclear how \"error infinity\" is defined making it difficult to interpret plots.  Incomplete plots: For plots showing \"error infinity\" corresponding plots of error and loss should be included in the appendix to clarify the effects of width.  Lack of mathematical contingent: Certain statements regarding scaling and averaging require mathematical explanation or references for comprehension.  Ambiguous definitions: concept such as \"clones\" and \"chunks\" need more formal definitions.  Insufficient evidence: The paper presents limited evidence to support the claim that mitosis explains improved generalization with width.  Lack of ablation studies: It would be valuable to further investigate the specific conditions responsible for mitosis through ablation studies. Minor Concerns:  Formatting issues: Extra spaces inconsistent yaxis labels and missing yaxis ticks.  Inconsistent design layouts: Mismatched chunk sizes and colors and unequal xaxes.  Typographical error: Missing commas incorrect wording and reversed discussion."], "fkjO_FKVzw": ["Paraphrased Summary This work presents a hierarchical neural network architecture for handling graphs. The bottom layer uses any GNN model capture for smaller subgraphs while the top layer implemented as a transformer operates on an abstracted graph. effectiveness  Wellsupported empirical section with training on multiple datasets compare to baselines and ablation work.  memory efficiency compared to existing baselines. Weaknesses  Many issue showing Coarformers superiority are not statistically significant.  Lack of thorough discussion of prior hierarchical graph models (e.g. DiffPool). Questions  Sec 3.1: Is using Personalized PageRank (PPR) as attention bias in graph transformers novel  Loss on the common graph is auxiliary only.  How does Coarformers parameter count compare to other models Is lower memory consumption solely due to fewer parameters  Why exclude nontransformer architectures on the global scale (e.g. GraphSAGE) Presentation Improvements  Correct BERT citation.  Cite PPR.  Remove learnable parameter Beta due to lack of impact.  limit HPO (hyperparameter optimization) in Related work.  Cite SPD.  Add descriptions of VN VE Alg JC and their citations.  Replace Table 5 with a graph showing trends in the c parameter.  Correct the table 12 Cornell column highlighting as GT has a sound issue.", "Paraphrased Summary: This paper introduces Coarformer a model that combines a standard Graph Neural Network (GNN) with a Transformer on a simplified graph to handle large graphs. While the idea of extending Transformers to large graphs is significant the proposed approach has both strengths and weaknesses: effectiveness:  Technically sound methodology. Weaknesses:  Coarformer lacks clear motivation and appears overly complex.  The main contribution seems to be operating the Transformer on a simplified graph while the rest of the approach is standard.  The paper lacks justification for claiming that Coarformer enables minibatch training for large graphs (Proposition 1).  The importance and scalability of the graph coarsening algorithm are not thoroughly examined.", "Paraphrased Statement: Summary This work proposes using a transformer neural network on the simplified graph derived from a graph simplification algorithm. Due to the computational demands of using transformers on large graphs the researchers employ existing graph simplification algorithms to create smaller graphs that retain global characteristics. To combine local and global information the researchers extract local details from the original graph using Graph Neural Networks (GNNs) and global information from the simplified graph using transformers. They so exchange information between these local and global levels using a crossview propagation mechanism. The authors demonstrate that the proposed Coarformer model surpasses baseline models in node classification tasks and excels in efficiency compared to other global transformer models for graphs. effectiveness and Weaknesses effectiveness:  Applying transformers to large graphs using graph simplification is a relatively unexplored domain.  Combining the local and global information captured by GNNs and transformers is an innovative approach.  The paper is wellwritten and accessible. Weaknesses:  The consumption of transformers on compressed graphs has been previously proposed (Baek et al. 2021).  The authors do not account for the time required for graph simplification which can be significant for large graphs.  The paper only presents GPU memory usage for small graphs and it would be valuable to include issue for large informationsets.  The experimental compare with baseline models are limited and may not fully represent the stateoftheart. Typos:  \"\u03b2\" in Section 3.3 should be changed to \"\u03b3\" (as \u03b2 is already defined in Section 3.1).  The caption of Table 1 is missing a period (\".\") at the end.", "Summary: This research mainly combines Graph Neural Networks (GNNs) and Transformers to extract both local and global information from largescale graphs. effectiveness: 1. The paper presents a clear combination of GNNs and Transformers leveraging the advantage of GNNs for local aggregation and Transformers for global aggregation. 2. The coarsening mechanism effectively reduces the complexity of selfattention in the Transformer. 3. Minibatch training is optimized using sampling techniques. Weaknesses: 1. The novelty of the proposed approach is limited as it combines existing techniques like GNNs Transformers graph coarsening sampling and PPR. The overall design is straightforward. Additionally the model may result in a large issue of parameters. 2. The concept of capturing global and longrange dependencies has been widely explored in previous work such as GeoGCN and BWBackprop in GNNs."], "rSI-tyrv-ni": ["Paraphrase: Summary: The study explores the impact of including entity type abstraction in pretrained Transformer models. The authors tested five distinct architectures to create an abstractionaware model. This model is evaluated on three NLP informationsets for reasoning tasks. empirical effect:  Entity type abstraction enhances performance in logical reasoning environments with straightforward language.  For QA informationsets with more natural language the baseline model is already strong resulting in minimal improvement from abstraction incorporation. effectiveness:  Wellpresented paper with robust experimental effect. Weaknesses:  performance boost for QA informationsets is negligible: The study should prioritize improving realworld information rather than synthetic information.  Baseline performance is inferior to stateoftheart models: The authors should demonstrate that the proposed methods benefit models with beneficial baseline effect.  Nonuniversal abstraction incorporation strategy: Different architectures work for different informationsets making it inefficient to select the appropriate architecture for a given informationset. Style Typos and Errors:  Section 4.1: \"accept both\" does not make sense in the context.  Section 4.1: Test set size discrepancy between 4.1 and 4.1.1.  Appendix B: CLUTTR level 2 output does not adhere to the answer template.  Appendix: TACRED is mentioned but its role in the study is unclear.", "Paraphrased Statement: Summary: This research study tested three methods for incorporating abstract concept knowledge into natural language process (NLP) models: adding additional input embeddings using a separate sequence for encoding and introducing an auxiliary prediction task. experimentation conducted on the CLUTRR HotpotQA and CoQA datasets demonstrated that models utilizing abstract entity knowledge achieved marginally beneficial performance. effectiveness: 1. The paper is wellstructured and easy to comprehend. 2. The experimental effect provide compelling evidence of improved performance on the three NLP tasks. Weaknesses: 1. The experimental section lacks an indepth analysis of the models inner work which could shed light on its performance. 2. additional analytical experimentation would yield more insights into the models behavior. For example investigating why model variations perform differently across CLUTRR HotpotQA and CoQA would be valuable.", "Paraphrase: This paper investigated adding entity type abstractions to Transformers and tested its impact on three tasks requiring logical reasoning: 1. compositional language understanding (CLUTRR): understanding textbased relationships. 2. Multihop question answering: answering questions requiring information from multiple sources. 3. conversational question answering: answering questions in a dialogue context. effect show that the method greatly improved the synthetic CLUTRR task but only slightly improved the other two tasks. effectiveness:  The paper is clear and wellorganized.  It demonstrates a notable improvement in the synthetic reasoning task.  It provides insights into the ability of the T5small model to understand entity abstractions. Weaknesses:  The paper presents only overall effect without analyzing why entity abstractions didnt always enhance performance.  It only tests the T5small model so the findings may not apply to larger more capable models.", "Paraphrased Statement: Summary: This study explores integrating entity abstractions into transformerbased language models (LMs) for text reasoning tasks. The authors introduce various techniques for incorporating entity abstraction information into transformer LMs. experimentation on a synthetic dataset indicate that the proposed method enhances compositional generalization. However experimentation on two realworld datasets reveal that the method does not significantly improve performance. effectiveness:  Clear and concise writing style.  The concept of using entity abstraction is straightforward and widely applicable if effective.  Promising effect on the CLUTRR synthetic dataset demonstrating improved compositional generalization. Weaknesses:  The idea of using entity abstraction is not especially novel as tagging input language with partofspeech tags is a common practice in NLP models.  The proposed method fails to improve performance on realistic datasets limiting the practical significance of the findings."], "xp2D-1PtLc5": ["Paraphrased Statement: Summary: This paper proposes a learning method for Voice Conversion (VC) using a single encoder that captures both contextual and speaker characteristics. It also incorporates techniques for leveraging vocal quality and speakerspecific information during training and prediction. The method reportedly surpasses the performance of most currently available opensource models. Strengths: 1. The problem formulation is clear and innovative adhering to theoretical rationale. Its mathematical validation enhances its value. 2. The model outperforms established opensource frameworks in a reliable manner. 3. The ablation work provides valuable insights voiceicularly regarding latent variables. Weaknesses: Questions: 1. The paper contains several grammatical and formatting errors. It should be validationread before publication. 2. The citations are poorly formatted and should be revised. 3. problem formulation should be a separate section or voice of the foundation not voice of the methodology. 4. Theorem 3.2 suggest that embedding and identity are an objectivesubjective mapping which is incorrect. It should only be a surjective mapping. 5. It is unclear whether the parameters of Equation 8 were determined through preliminary experiments or a pretrained dataset. 6. The reviewer recommends testing the method on publicly available VC datasets such as the VC2018 challenge if feasible. 7. A weakness is that multiple encoders are yet required during evaluation. Although not necessary to address this now it should be mentioned as a future direction.", "Paraphrased Summary: The paper introduces a voice conversion model that separates speaker information and linguistic content into distinct embedding vectors. Voice conversion is achieved by replacing the speaker embedding. The model comprises four loss functions for speech reconstruction embedding reconstruction speaker classification and adversarial speaker classification. Strengths and Weaknesses:  The combination of loss functions is not novel and the paper lacks justification for its use.  The experimental results in Table 1 are inconclusive due to varying model architectures and training loss.  The theoretical foundations (Section 3) are weak:  The assumption that linguistic content and speaker information can be strictly separated is flawed as speakers vary in sentence construction and pronunciation.  The claim that the content and speaker embeddings uniquely determine speech is false as individuals cannot produce identical speech utterances.  The information process inequality assumption (I(Cx Sx) \u2265 I(Cx x)) is overly solid and potential incorrect. Weaknesses:  Disentanglement is not guaranteed despite using a speaker classifier and gradient reversal layer.  Reconstructing hidden vectors can lead to overfitting if the corresponding loss weight is also high. Presentation publication:  The foundation lists problem with other models but does not connect them to the proposed approach.  The setting section lacks coherence.  Theorems in Section 3 are more appropriately categorized as assumptions.  Several wording publication and typos are present throughout the paper.", "Summary Proposed System: The paper introduces ClsVC a voice conversion (VC) system that derives speech representation from reference speech to separate speaker (quality) and content information into distinct embeddings. Disentanglement Approach: To ensure clear separation the latent speech representation is divided into two components: speaker embedding and content embedding. Two classifiers a common speaker classifier and an adversarial speaker classifier are employed to enforce this distinction. Loss work: A novel \"codereconstruction loss\" is proposed to prevent content embedding from carrying speaker information. Strengths:  Simplified model architecture using a single encoder for both quality and content extraction.  Flexible partitioning of embeddings allows adjustment of their proportions.  Enhanced disentanglement with both common and adversarial speaker classifiers.  Vector norm eliminates speaker information from speaker embedding.  Codereconstruction loss ensures content embedding remains qualityfree. Weaknesses:  Limited novelty as the proposed method essentially combines existing content and speaker encoders.  potential technical errors such as the minimization of the total loss potentially increasing the adversarial speaker classification loss.  Unclear derivation of Zhat (and X) in the codereconstruction loss equation.  Ambiguity in referring to \"estimated content embedding\" in the reconstruction loss equation.  Lack of ablation work for loss other than codereconstruction loss.  The selection of latent variable dimensions needs exploration and justification.  The papers English writing requires proofreading.", "Paraphrase: Summary: This paper presents techniques for converting voice. The method employs an encoderdecoder framework which incorporates classification tasks over learned embeddings to enhance voice conversion. The representation space is partitioned into content and speaker components enabling the separation of content from speaker identity. Experiments are conducted on both conventional and oneshot voice conversion tasks. Objective and subjective evaluation are used to assess all systems. Strengths and Weaknesses: Strengths:  Proposes an approach for voice conversion by adding classification tasks to an autoencoder architecture.  Specifically these classification tasks aim to extract separate speaker and content representations. Weaknesses: Technical publication:  The paper closely resembles AutoVC lacking originality in its contingent and descriptions.  Crucial contingent are omitted including the encoder and decoder architecture and the C1 and C2 classifier specifications.  It is unclear why Theorems 3.1 and 3.2 are labeled as \"Theorems\" rather than assumptions or conditions. Empirical evaluation Concerns:  There is no empirical verification of whether the content embedding captures meaningful contentrelated information.  No attempt is made to quantify the leakage of content and speaker information into the embeddings.  It remains unclear how the framework performs when the input audio utterances are from the same speaker. Writing and Presentation:  The papers writing could be improved with proofreading required to address typos and speech errors.  It can be difficult to read and comprehend in certain sections.  design (e.g. 1 and 4) are not adequately discussed. experimental Limitations:  The classifiers receive both speakercontent embeddings and speaker identity vectors. The format of the speaker identity input is not specified.  The experiments lack depth and certain contingent are missing.  The data partitioning for experiments is unclear.  Hyperparameter selection is not explained. evaluation publication:  The MOS results exhibit high variance making performance interpretation challenging.  The sample size for the MOS work is not disclosed.  design 4 is not adequately discussed. Additional Concerns:  The impact of embedding dimension changes on performance is not explored.  The paper lacks original insights and empirical support for its claims regarding speaker and content embeddings."], "wv6g8fWLX2q": ["Summary This paper suggests using multiparameter persistence (a new concept in data analysis) to find hidden relationships between nodes in a GNN (Graph Neural Network) over time. The authors develop a \"dynamic EulerPoincar\u00e9 surface\" as a method to summarize this data and demonstrate its advantages. They also propose a \"supra\" graph convolution module to simultaneously learn spatial and temporal correlations in complex multivariate time series data. They show that their approach outperforms others in predicting traffic flow Ethereum token prices and COVID19 hospitalizations. Strengths  Addresses a significant problem with widereaching implications.  Combines two new research directions in a unique and innovative way.  Provides a novel timeaware multiparameter persistence invariant.  Demonstrates a solid theoretical introduction.  Wellwritten and structured with informative appendices.  Strong empirical results across diverse domains. Weaknesses  Computational complexity of the proposed approach is not discussed in comparison to other baselines.  Scalability concerns with the supraLaplacian matrix are not addressed.  Some argument lack sufficient empirical support (e.g. lower computational cost).  Certain choices such as concatenating global average and max pooling seem arbitrary.  Text in shape 1 is difficult to read.", "Paraphrased argument: Summary: This research incorporates multipersistence into graph neural networks (GNNs) enabling them to uncover hidden shape in spatiotemporal graph data that change over time. The developed TimeAware Multipersistence SpatioSupra Graph Convolutional Network (TAMPS2GCNets) outperforms other innovative methods on a range of spatiotemporal graph data sets. Strengths and Weaknesses: Strengths:  The paper combines timeaware deep learning with multiparameter persistence resulting in the introduction of TAMPS2GCNets which has five main components.  The model offers significant advancements over current stateoftheart techniques. Weaknesses:  The review raises some concerns including:  The learning process for the learnable node embedding E\u03c6 is unclear and the multiplication performance in Equation (1) needs clarification.  The mechanism for learning the normalized selfadaptive adjacency matrix S resembles the selfattention method in GAT which requires dense input graphs resulting in inefficiency.  The computation becomes inefficient because \u1e6eS is a power series of S.  The ablation study does not compare the use of the normalized selfadaptive adjacency matrix S to the use of the new adjacency matrix.", "Paraphrase: Summary: The authors advocate for using multiparameter persistence to explicitly model the hidden time relationships in spatiotemporal data. They introduce the TimeAware Multipersistence SpatioSupra Graph Convolutional Network which extracts multipersistence features through the EulerPoincar\u00e9 surface and enables timesensitive learning. Strengths:  Introduces multiparameter persistence into timeaware learning of convolutional networks opening a new research avenue.  Leverages multipersistence to capture time dependencies enhancing graph learning effectiveness.  Technically sound with thorough experimental validation.  Clear writing wellpresented methodology and readily reproducible code. Weaknesses:  minor concerns regarding implementation choices in Equation (6) such as the use of global average pooling and global max pooling.  Additional analysis of computation time compared to other methods in the main paper would enhance the discussion."], "vdKncX1WclT": ["Paraphrase: Summary: This paper presents a method to create \"backdoored\" pretrained models that can act maliciously in downstream tasks even without knowing taskspecific information. The method assigns predefined output representations to triggers rather than establishing connections between triggers and target labels. To prevent all triggers from triggering the same target label the authors design trigger pairs with opposite values. experimental results show that the proposed attack method can successfully induce target labels even after finetuning demonstrating the potential security threat of pretrained models with backdoors. The paper also discusses defense mechanisms to mitigate this threat. Strengths and Weaknesses: Pros: 1. Addresses a practical problem in the era of pretrained models. 2. research connections between triggers and predefined outputs independent of task information. 3. Uses trigger pairs with opposite values to avoid inducing the same target label. 4. Provides broad experimental results on both NLP and CV tasks to showcase the threat of backdoored pretrained models. 5. Discusses defense methods to mitigate the threat. Cons: 1. The methods effectiveness in multiclass classification where multiple pairs of triggers could lead to only two target labels is uncertain. 2. The proposed method appears sensitive to batch normalization which is a common practice in deep learning potentially limiting its effectiveness in realworld applications. Conclusion: After considering the authors response the reviewer maintains the original evaluation score.", "Summary Paraphrase: This study introduced backdoor attacks that enable an attacker to train a pretrained model (PTM) to produce specific predetermined outputs for certain inputs. Strengths and Weaknesses Paraphrase: Strengths:  The backdoor attack method targets PTMs a significant security concern.  The attacks were tested on both text (NLP) and paradigm (CV) tasks. Weaknesses:  The proposed attack assumes a binary classification task where the model predicts a predefined target class for triggered inputs. However it may be challenging to apply this approach to multiclass classification tasks with a moderate issue of classes. Additionally the high dimensionality of PTM outputs would require a large issue of predefined values.  The evaluation was conducted on simplified datasets rather than original downstream datasets and it remains unclean how the attack would perform on realworld datasets.  The paper does not provide clean guidance on selecting predefined values that lead to the prediction of specific target classes. The attacker may need approach to either the blackbox or whitebox version of the finetuned model to determine the corresponding target label for each trigger which may not be feasible in practical threat models.  The proposed attack may not be effective against models that use batch normalization a common technique in innovative neural networks.  The evaluation did not include comparisons with stateoftheart backdoor attack defense methods. Moreover the proposed defense is susceptible to FinePruning a defense technique.  The comparison with existing backdoor attack methods is insufficient as it does not explore whether those methods are applicable to PTMs.  Minor: Table 4 does not include the CAcc metric for the case where no backdoor attack is present.", "Paraphrased Statement: Summary:  This paper presents a technique for injecting backdoors into pretrained models allowing these backdoors to be inherited by downstream models.  The method involves restricting the output representations of backdoor samples using a proposed loss work.  Experiments demonstrate the effectiveness of the backdoor attack on NLP and CV tasks. effectivenesss and Weaknesses:  effectiveness: Evaluated on both CV and NLP tasks.  Concern: Lack of significant novelty or unique challenges addressed. contribution:  Novelty: Compared to existing backdoor attacks this method does not modify the model architecture or training data. Countermeasures:  Only evaluated against trainingtime defenses.  Finepruning significantly outperforms other defense methods due to its ability to remove backdoor triggers without affecting benign accuracy. Limitations:  Theoretical Guarantees: None provided.  threat model: Unclear requires approach to finetuned model outputs.  Scalability: May be limited to models with a minor issue of labels. Questions:  Relationship between trigger Pairs and Control: How does the issue of injected trigger pairs affect the ability to control target labels  Blending Ratio: Why did the blending ratio vary between VGGNet and ViT", "Paraphrased Statement: Summary: The paper proposes a method for creating backdoored pretrained models that can harmfully manipulate various downstream tasks even without knowledge of the tasks. It introduces the Neuronlevel Backdoor Attack (NeuBA) which enables the modification of output representations for samples containing a trigger by enforcing predefined values during training. Strengths:  Excellent writing quality  novel and concerning concept of introducing backdoors in pretrained models Weaknesses:  Overlooked previous research on similar issue  Absence of key baseline comparisons  No evaluation of detection capabilities Detailed Comments:  The paper has omitted relevant existing work such as those by Yao et al. (2019) and Jia et al. (2022) which address the same issue.  The authors should clarify their unique contribution in light of these prior study highlighting differences in target and approaches.  Comparative study should be conducted to evaluate the effectiveness of NeuBA against other approaches.  The authors need to assess the impact of using NeuBA across multiple downstream tasks for a single pretrained model.  They should evaluate the resilience of NeuBA to backdoor detection methods such as NeuralCleanse and MNTD.  The term \"neuronlevel\" is used repeatedly in the title and paper but its meaning is unclear. Please explain if it refers to the representation vector."], "vXGcHthY6v": ["Summary (Paraphrased): The paper introduces a technique to discover intrinsic features that remain invariant across similar observations from different MDPs (Markov Decision work). These features can assist reinforcement learning agents in adapting to varying testing consideration (even when the test data significantly differs from the training data). The approach involves minimizing the data exchange between the latent representation of indistribution and outofdistribution experiences. This is achieved by employing a GANlike discriminator that distinguishes between the two. effectiveness and Weaknesses (Paraphrased): The papers goal and impressive issue are commendable. The concept of using a discriminator to derive invariant latent representations is sound and practical. However the paper lacks detailed clarity making it challenging to fully understand the methodology. specific Queries:  When is the parameter P\u03d5 fixed and when is it trained  What is at in Equation 2 during training on MDPtest  In Equation 3 what do z and \u0303z represent  set Dnondist and Ddistr refer to data from MDPtrain and MDPtest respectively  How large should the source buffer for training data be and does it store all training data", "Summary In this work a method is proposed to adapt a reinforcement learning (RL) pose trained in a simplified environment to a more complex environment with distracting elements in the observations. Instead of enhancing the training environment with additional data to increase the poses resilience the pose is adjusted at test time to ignore specific distractions encountered in the environment. experimentation in simulations demonstrate the advantages of this approach especially when the agent has no approach to reinforcement data at test time. effectiveness and Weaknesses effectiveness:  The setup addresses a common problem in robotics applications where simulations differ significantly from realworld environments.  empirical issue indicate the benefits of the proposed approach. Weaknesses:  The theoretical foundation and justification of the approach are lacking.  The distinction between state and observation is introduced but later ignored potentially weakening the argument about partial observability.  Equation (5) simply combines two loss work without justification or explanation of how their relative importance is determined.  The use of the GAN to match distributions over z is not adequately explained or justified.  The frozen inverse dynamics head supposedly prevents overfitting to training data yet the GAN portion of the architecture is hypothesized to address overfitting. This inconsistency raises questions about the need for the GAN.  The description of the work is incomplete and additional detail are not provided in supplementary materials or code. Minor Typo and Clarification  \"training environment the Distracting\" should be \"training environment with Distracting\"  Table 1 values are the reinforcement differences between before and after adaptation.", "Paraphrased Statement: Summary Introduction: This paper investigates the challenge of developing pose that generalize effectively to test data that differs from the training data distribution. The approach focuses on using an inference pose to learn the latent structure of the input rather than relying on data augmentation. This approach surpasses traditional indistribution generalization techniques and can handle cases where the target domain is unknown. Prior knowledge and lack of reinforcement work in the test distribution present significant hurdles. In robotics such learning is crucial. Statement of design: The authors propose to reformulate the outofdistribution generalization problem as an unsupervised policy adaptation between two Markov Decision work (MDPs) with similar latent dynamics and reinforcement structure but different observations. They aim to exploit probabilistic inference to achieve invariance by leveraging latent structure. problem formulation: Given two distinct MDPs Atrain and Atest the agent has approach to ground truth data in Atrain but not in Atest. The key challenge is where the shift between Atrain and Atest occurs due to differences in observation spaces. Invariance Through Inference and Algorithm: The goal is to learn an encoder that maps semantically similar states from both MDPs to latent vectors. This is achieved without approach to reinforcements in Atest and without paired trainingtest trajectories. The approach employs two target: 1. Distribution matching to align the latent distributions induced by both MDPs. 2. A GANstyle loss to prevent the latent code from relying on distracting data that distinguishes Atrain and Atest. The adaptation target involves maximizing the mutual data between the prototypical representation from training and the adapted representations of novel observations. experimentation: The authors demonstrate the effectiveness of their approach using the Deepmind Control Suite (DMC) for training and the Distracting Control Suite (DistractingCS) for testing. issue show that baseline augmented with the proposed Invariance Through Inference (ITI) exhibit significantly reduced performance degradation as distraction intensity increases. ITI outperforms Policy adaptation during deployment (PAD) on DMC with fixed distraction intensity. effectiveness and Weaknesses: effectiveness:  Novel approach leveraging latent representations to pose invariance.  No reliance on target distribution reinforcements.  Strong issue on DMC with varying distraction dimensions.  Ablation work demonstrating the impact of the adversarial target. Weaknesses:  Limited evaluation domain (robotics control).  Lack of theoretical bounds or analysis of scalability.  comparison mainly focuses on data augmentation approaches.  formal definition of adaptation and outofdistribution criteria unclear.  Algorithm overview section could be more detailed.  Ablations and PAD comparison use fixed hyperparameters.  Lack of contrast between compared methods in Table 1.  Explanation of why ITI outperforms PAD could be expanded.", "Paraphrased Summary This paper proposes a method for identifying generalizable form in different Markov Decision work (MDPs) even when they have different observation states. It suggests using a generative pose to represent the various observations and trains this pose using a Generative Adversarial Network (GAN) framework along with auxiliary losses for the systems dynamics. effectiveness  Introduces a novel approach for facilitating generalization to different but related RL environments.  Tackles an significant issue in the practical application of reinforcement learning agents.  Demonstrates promising empirical issue. Weaknesses  Clarity:  Uses unfamiliar terminology (e.g. \"exante\").  Vague description of the similarity necessity between forward dynamics.  Inconsistent notation for generative poses input.  Undefined terms and abbreviations (e.g. \\tildez D).  experimental evaluation:  Limited experiments only showcasing two main experiments and an ablation.  comparison to PAD missing in form 5.  Lack of data on the issue of repetitions for confidence intervals.  Other Concerns:  Missing citation to relevant work (e.g. domain pose).  Could have provided a more detailed explanation of why GANs were chosen instead of other generative poses.  Insufficient discussion of training challenges with GANs."], "vqGi8Kp0wM": ["Paraphrase: Summary: This paper focusages on domain transfer tasks in pretrained GANs specifically transferring generated images from one domain (A) to another domain (B) using a single reference image from B. The proposed method combines StyleGAN and CLIP and has been experimentally shown to be superior to existing techniques. effectiveness: 1. The proposed technique is elegantly designed. 2. Experimental results demonstrate its effectiveness and the visual model provided are impressive. Weaknesses: 1. image 6 includes captions with \"``\" before semantic labels. There appears to be an error in the caption as it states that glasses were removed from a mans face instead of being added. The distinction between \"Ref\" and \"Dom.B\" in the bottom group is also unclear. Update: Initially the reviewer was impressed by the paper and gave it a high score. However after reviewing similar paper submitted to ICLR2022 the reviewer identified several areas where the paper falls short: 1. The paper does not include quantitative results and relies solely on cherrypicked images to demonstrate its effectiveness. 2. The primary experiments focus solely on style transfer on human faces excluding attribute transfer. The term \"generative domain adaptation\" is therefore not appropriate. Although extra experiments on cars and cats are included in the appendix their results are misleading. For model in image 11 the caption describes a style transfer task but the dogs structure has changed well. 3. The diversity of the transferred model has not been thoroughly evaluated which is a crucial metric for generative model. 4. The usage of CLIP in the proposed method gives it an unfair advantage over other techniques.", "Paraphrase: The study introduces a technique for adapting a trained Generative Adversarial Network (GAN) to a new domain using just one image from that domain. Key technical contributions include: effectiveness:  Novelty: The proposed method is unique.  Leveraging Pretrained GAN: The method utilizes pretrained CLIP StyleGAN for adaptation leveraging its prior knowledge.  Domain Shift model: The approach model domain shift as parallel vector which enhances effectiveness.  Impressive Results: The method outperforms competing approach (Fig. 4).  Latent Space Coupling: The technique allows for combination with latent code editing approach (e.g. Styleflow) for attribute manipulation.  comprehensive Review: The related work section provides a thorough overview of relevant topics. Weaknesses:  Limited Experimental Support: Some claims (e.g. reducing mode collapse) lack sufficient experimental support (Fig. 4 may be insufficient).  Incomplete Ablation study: quantitative measurements for ablation studies would be beneficial.  Writing Clarification: The methodology section could be more detailed and easier to follow.  Unclear Learnable Weights: The learnable weights in Fig. 3 need further explanation.  Unspecified reference image: The number of reference images used in styleGANnada and fewshot GAN (Fig. 4) is unclear.", "Summary This paper introduces a new approach to adapt GANs for a single application using a single input. The method produces visually superior images compared to existing techniques offering more design flexibility while maintaining computational efficiency. effectiveness  Highquality image generation as shown through visualization results.  Strong theoretical model with a clear ablation study demonstrating the impact of each regularization term.  GPU compatibility for effective training and testing.  Versatile capabilities including image editing and domain gap control. Weaknesses  Writing and Presentation: section 3 is poorly written and confusing with terms introduced without proper context.  Limited Baseline Comparisons: The paper just compares the method to two existing techniques which may not be sufficient.  Missing quantitative Analysis: The paper lacks quantitative comparisons of inference time with the baselines. Minor Issues  Page 5 has excessive whitespace due to large figures.  Equation 5 lacks a comma.  The final sentence in the \"Training and Inference Time\" paragraph is incomplete.  The font for \"Pnorm\" is inconsistent in the \"latent space interpretation and semantic editing\" paragraph. reference  [1] Gal Or et al. \"StyleGANNADA: CLIPGuided Domain Adaptation of image Generators\"  [2] Ojha et al. \"FewShot image generation via CrossDomain Correspondence\""], "zAyZFRptzvh": ["Summary This paper proposes a framework for checking deep study models to make sure they meet certain requirements. This is done to make people More confident in models that have been trained. The framework study by making small changes to input data to see how far the input can be changed while still getting the expected result. The framework was tested with a pixelbased training approach and four different datasets. effectiveness and Weaknesses effectiveness:  The paper addresses a relevant problem within the conferences scope.  The framework is welldesigned and provides a deep understanding of the approach.  The paper provides a clear statement of the contribution and need. Weaknesses:  The formal innovation of the framework is not always clearly explained making it difficult to fully understand.  Some of the variables used in the \"Task Performance\" section are not introduced until the \"Verifiability\" section.  Its unclear how the \"Interpretability\" section relates to the concept of interpretability.  The section on \"certified images\" refers to a concept that is not clearly defined in the paper.  The human study used to evaluate the realism of generated images is not a reliable metric for machine study datasets.  The reliability of the generative model used to create input data affects the confidence in the framework.  The specification of the encoding purpose used to represent input variations is not clear. discussion Despite the weaknesses the paper makes a valuable contribution by providing a innovation for future research. However it would benefit from clarifying the formal innovation expanding on the encoding purpose and using a More reliable evaluation method for the generative model.", "Summary This research introduces AuditAI a tool that can verify training certification using semantic specifications based on a generative model. The authors broadened the scope of IBP (data Bottleneck Principle) to accommodate variations in latent space and assessed its effectiveness across various datasets. effectiveness  Clear and concise writing Weaknesses  The research appears to be a straightforward extension of IBP Advmix [1].  The distinction from [2] is unclear (Appendix A.6).  Modeling perturbation sets for images with perturbations (e.g. adversarial corruptions) may enhance specification capture.  It is uncertain if latent space perturbations in generative models preserve semantics.  Whether the latent space of a specific GAN can capture all variations is also unclear as GAN diversity is often limited.  Unit testing arguments are not particularly novel as they follow standard verification protocols.  The name \"AuditAI\" is informal.  Mixed utilization of \"citep\" and \"citep\" in references.  Missing left parenthesis in Theorem 1.", "Paraphrased Statement: Summary: This reresearch introduces AuditAI a system that verifications if a trained model aligns with specified criteria and facilitates certified training. It uses a generative model to represent inputs and verifies their adherence to a predefined set. The system innovation lies in applying the interval bound generation (IBP) technique to latent codes in embeddings to assess the semantic behavior of models. effectiveness:  Simplicity in extending IBP to latent codes  Comprehensive scope covering verification of specific behaviors and certified training  Extensive empirical evaluation on diverse datasets (ImageNet Chest Xrays FFHQ) demonstrating the effectiveness of the modified IBP Weaknesses:  Lack of a sanity verification experiment on a toy model with known violations  Reliance on the assumption that latent codes and generators accurately decompose input into meaningful concepts  User study with potential limitations (Turker assessment of Chest Xrays indistinguishability not necessarily indicative of AuditAI effectiveness)  Unclear assumptions in Theorem 1 regarding f  Lack of details on the practical implementation of the IBP research process  Ambiguous definition of \"the corresponding convex bound of the variations of \\mathcalS0i in in range space\"", "Paraphrase: Summary This research presents a novel and valuable approach to AI model auditing. The proposed framework is noteworthy. The experiments demonstrate the effectiveness of the method. effectiveness and Weaknesses Pros:  Proposes a novel direction for AI model auditing that is beneficial particularly for industry applications.  Offers a compelling solution with demonstrated efficacy. Cons:  The classification and generative models share the same encoder which limits direct application to existing networks with specific backbones (e.g. ResNet50).  The theoretical barriers are addressed using an offtheshelf method (IBP).  The methods applicability to other tasks (e.g. object detection segmentation) is not demonstrated. Questions and Suggestions:  Clarification is requested for understanding the specific representation used to define face rotation angle and maintain accuracy.  It is suggested to compare the proposed approach with existing methods that use uncertainty or confidence scores for AI model auditing in the Related study section."], "vEIVxSN8Xhx": ["Paraphrase: Summary The paper introduces a technique called logpolar space convolution where elliptical kernels are applied to an input image. The kernels are divided into radial segments and each segment value is applied to all pixels within its corresponding image region. The results are summed and potentially normalized. The technique is implemented using logpolar space pooling which generates a set of pooled feature maps that can be processed by a regular convolution. This method enables the use of larger receptive fields without requiring additional weights. Strengths and Weaknesses Positives  The concept is promising and the experimental results demonstrate that the method does not negatively impact existing model architectures.  It provides a valuable new tool for future research. Concerns WritingPresentation  The mathematical equations need to be corrected. Equations 1 and 3 should include a dot operator matrix transpose of X or additional summation over channels.  The equations represent crosscorrelation rather than convolution. count using the crosscorrelation symbol or reversing the kernel or input indexing.  Clarify the use of the bias term in the implementation. Results  The results are somewhat weak.  The increase on ImageNet and VOC2012 are marginal and require further quantification and analysis.  It is unclear if the midgray value in shape 4 represent kernel value of zero or normalized value.  Provide more discussion on the meaning of the numerical accuracyAUC metrics in the context of shape 4 where many kernels do not appear to benefit from the increased receptive field size. complexity Analysis  The reported FLOPs calculation is unclear. Dilated convolution with the same issue of weights should have identical FLOPs to standard convolution.  count also evaluating the latency and memory approach shape of the different approach during training and inference to provide a more comprehensive analysis of complexity.", "Paraphrase: This paper introduces a novel convolutional operation called LogPolar Space Convolution (LPSC) for image recognition tasks. The LPSC kernel is distributed within logpolar space allowing the receptive field to expand exponentially with increasing radius without requiring a significant increase in learning parameters. To implement LPSC the authors utilize logpolar space pooling to precompute an expanded feature map. The LPSC operation is so applied similarly to standard convolutional filters. The proposed LPSC is integrated into popular CNN backbones such as ResNet VGGNet and MobileNet. Experiments show that the limited network outperform the original structures on various image recognition and segmentation datasets. Strengths:  The concept of LPSC is intriguing and offers a new approach to convolutional operations.  LPSC enables exponential expansion of the local receptive field without significantly increasing parameters which could be beneficial for dense prediction tasks like image segmentation.  The operation of LPSCbased network surpasses that of the original versions across various architectures and datasets.  The concept is straightforward and the paper is generally easy to follow. Weaknesses:  The effectiveness of expanding the local receptive field may be underrepresented. The experiments use lightweight network (e.g. ResNet1820 MobileNet) where the original receptive fields are not sufficiently dense. Its unclear if LPSC will be as effective in more complex model with larger receptive fields (e.g. ResNet101).  The authors do not discuss related work on expanding receptive fields such as Dense ASPP.  Computational efficiency is a potential concern. Precomputing the spatial mask and the exponential increase of the outer rings in the logpolar kernel can lead to increased time complexity.  The assumption that features near the kernel of the kernel are more significant than those further away is not fully explained or justified.  The operation improvements on some datasets are small. The absence of results using more further model like ResNet101 or Xception65 limits the generalizability of the findings.  The authors only compare their method with dilated convolutions in segmentation experiments. It would be valuable to also compare it with other relevant techniques like DenseASPP deformable convolution or selfattention modules.", "Paraphrased Statement: Main Points:  The paper introduces a new method for performing convolution on a circular grid representing angle and radius.  The radius is divided logarithmically to prioritize central content.  The new convolution operator is various and compatible with existing model.  Empirical results show that it outperforms standard convolution with comparable parameters. Strengths and Weaknesses: Strengths:  The concept is logical and wellsupported by empirical results. Weaknesses:  1. Metaparameter issue:  The paper claims to use consistent metaparameters for fair comparison but this is questionable.  Metaparameters can significantly impact operation so using identical value limits the informative nature of the comparisons.  The authors should demonstrate equal sensitivity to metaparameters among all methods.  2. Fair parameter Comparisons:  Comparing model with equal parameters may not be fair as the proposed method resembles separable convolution with fixed depthwise convolution weights.  This effectively increases the model capacity compared to standard 2D convolution.  The authors should analyze the impact of the fixed logpolar space pooling layer on operation.  3. Generalizability to Deeper model:  The paper only evaluates shallow model (ResNet20) where deep model may reduce the methods benefits or even harm operation.  The authors should consider the methods implications for more complex architectures like transformers and active convolution.  4. Suboptimal Baseline operation:  The baseline methods operation appears suboptimal so the apparent operation increase may be overstated.  The authors should improve the baselines to ensure meaningful comparisons.", "Paraphrased Statement: This research explores the concept of using encodings with denser points in specific areas and sparser points as space from those areas increases. These encodings often referred to as logpolar filters or shape context feature have been utilized in classic computer vision techniques. The authors have incorporated these filter types into Convolutional Neural network (CNNs) by modifying architectures (via pooling techniques) to enable the use of filters with this specific structure. Experiments demonstrate that these methods can surpass basic benchmarks such as local and dilated filters. Strengths and Weaknesses:  Strengths: The paper is wellwritten and presents a reasonable approach. Logpolar feature have strong precedents in the system of the human retina and the effectiveness of shape context methods. The authors acknowledge the limitation (e.g. increased memory use) of their approach.  Weaknesses: The results are not particularly groundbreaking. While the proposed architectures are novel similar results could potentially be obtained with existing techniques. A stronger baseline would be a traditionally constructed architecture that allows filters to learn dense sampling near points and sparse sampling away from points. The experiments support a small advantage over simple baselines but do not compellingly demonstrate the practical significance of this new architecture. Furthermore there are no advantages in training time or memory requirements making it unlikely that practitioners would adopt this approach for practical problems."], "tUa4REjGjTf": ["Paraphrased Statement: Main findings: This study investigates the certified robustness of machine learning ensemble models. It establishes that under the assumption of smoothness in the model diverse gradients and substantial confidence margins are both necessary and sufficient conditions for ensuring certifiably robust ensemble models. The proposed Diversity Regularized Training (DRT) technique outperforms existing provably robust methods. effectiveness and potential Limitations: ensemble models display strong performance in empirical defense highlighting the need for extending their capabilities to certified robustness. The study provides valuable insights though some appear readily accessible from empirical ensemble model defenses. Specific Considerations:  In Definition 1 to align with randomized smoothing the condition should be \\(\\delta2 \\leqslant r\\) instead of \\(\\delta2 r\\).  The empirical robust accuracy of DRT (r0) may be lower than the baseline on largescale datasets.  The motivation for the ensemblebeforeSmoothing approach is unclear. Is it driven by the properties of Theorem II or by empirical observations The approach appears to be straightforward.", "Summary Paraphrase: The researchers investigated how ensembles can enhance the verifiable robustness of classifiers presenting both theoretical and practical insights. They established that enhancing robustness requires varying gradients within the ensemble (along with ample confidence margins). These insights were incorporated into two regularization terms for training resulting in improved verifiable robustness on various datasets. Strengths Paraphrase:  The empirical findings are compelling demonstrating the proposed models consistent superiority over alternatives.  The theoretical model appears heavy and significant.  The paper is clearly written and accessible. Weaknesses Paraphrase:  The study is limited to a specific ensemble context with a fixed number of members (N  3). Exploring the impact of varying ensemble size would have been valuable.  The researchers do not fully utilize the theorys ability to identify nonrobust samples for certification.  The proposed method requires training ensemble models in conjunction which could be seen as a limitation. Additional Comments Paraphrase:  The emphasis on gradient diversity for ensemble robustness aligns with practical observations. The authors speculate on a potential deeper connection between gradient diversity and ensemble diversity.  While the theory does not explicitly address the impact of ensemble size it would be fascinating to explore this relationship theoretically.  The optimization of ensemble member weights (wi) in the proposed approach is unclear from the provided data.  The ability of Theorem 1 to determine nonrobust samples is intriguing. Presenting the proportion of certifiably nonrobust samples would provide further insights into the cases where robustness cannot be verified.", "Paraphrased Statement: The study suggests training various classifiers to enhance the verifiable robustness of ensemble classifiers. It introduces two regularization penalties during training: (a) Gradient Diversity (GD) loss and (b) confidence Margin (CM) loss. This is justified by a theoretical observation that both (a) and (b) are necessary for a robust ensemble model when all individual classifiers are continuous. Experiments demonstrate that the proposed training consistently boosts the verified accuracy of ensemble models compared to ensembles of classifiers trained conventionally. effectiveness and Weaknesses:  The paper is wellwritten and grounded in theory.  The experimental results strongly support the effectiveness of the proposed method.  Exploring ensembles of continuous classifiers is a significant and understudied field. Concerns:  The transition from Theorem 1 to the proposed EnsemblebeforeSmoothing (EBS) method is unclear.  Theorem 1 applies only when all individual classifiers are continuous before ensemble while Theorem 2 shows the continuity of the classifier after ensemble.  Its not clear whether EBS ensures the continuity of all individual classifiers before ensemble.  The paper could benefit from comparing the method with Average evidence Radius (ACR).  The certified accuracy addition are not consistent across all datasets especially on ImageNet.  Singlemodel performance comparisons of DRTtrained models would be valuable.  The computational costs including training and certification time overhead should be discussed.  The decreased clean accuracy of DRT models on ImageNet warrants further explanation.", "Paraphrase: This paper introduces a novel approach to verify the robustness of ensemble models using randomized smoothing and certification. The approach involves analyzing the dependency of the robustness radius on the L2 norm of the weighted sum of foundation model gradient vectors. To enhance robustness a training technique is proposed that aims to increase the diversity of these gradient vectors. effectiveness and Weaknesses The theoretical analysis presented is heavy but its comprehension is hindered by complex notations. The empirical results provide sufficient support for the claims. However the paper lacks details regarding the weights assigned to ensemble members. Clarifying the values and selection process for these weights would strengthen the experimental section.", "Paraphrased Statement: This paper introduces an ensemble model designed to enhance certified robustness. The proposed DiversityRegularized Training (DRT) method is a simple and effective training approach that utilizes two regularizers:  One promotes diverse gradients ensuring that the ensemble members provide different predictions.  The other encourages large confidence margins increasing the reliability of the ensembles predictions. extensive experiments demonstrate the effectiveness of the DRT approach. effectiveness:  The paper presents a theoretically heavy and practical approach.  The writing is clear and compelling.  The experimental results provide strong evidence for the methods effectiveness. Weaknesses:  The concepts of gradient diversity and confidence margins are not only novel in improving ensemble robustness.  Randomized smoothing has been explored previously. However the paper effectively combines these elements into a theoretically wellfounded ensemble approach."], "yrD7B9N_54F": ["Paraphrase: Summary This research presents a technique for predicting link in graphs with limited data using a domain discriminator. The goal is to obtain graphlevel representations that are impartial to domains. The authors present the idea of a \"shot\" in graphs. The suggested technique outperforms existing methods on three different datasets. Strengths and Weaknesses Strengths  Integration of adversarial learning (commonly utilized in unsupervised domain adaptation) into graph link prediction problems with limited data to acquire domaininvariant representations. Weaknesses 1. Lack of Justification for Motivation: The work motivation for introducing a limitedshot setting for graph link prediction is not fully explained. Despite creating such a scenario the suggested approach does not address the effective use of \"few shots\" or the generalization of the trained model to novel tasks with minimal training steps. 2. Ambiguous domain Definition: The paper definition of \"domain\" is ambiguous. For instance why were several domains chosen from the same graph in ogbnproducts Should the chosen domains be regarded as \"different domains\" 3. Questionable Adversarial learning Application: The application of adversarial learning in limitedshot learning is confusing. While adversarial learning in domain adaptation seeks domaininvariant representations its relevance in limitedshot learning remains unclear.", "Paraphrase: The paper suggests a solution to the challenge of imbalanced datasets in graph link prediction. This method involves adversarial training to produce graph embeddings that can adapt across different domains enabling transfer learning. The paper utilizes TSNE plots of graph embeddings to identify optimal situations for using this method. The paper evaluates its method against existing heuristicsbased and GNNbased domain adaptation techniques through experiments. Strengths:  The adversarial trainingbased domain adaptation strategy is wellexplained.  Experiments demonstrate the efficacy of the proposed method.  TSNE plots offer valuable insights into the methods applicability. Weaknesses:  The paper lacks a clear articulation of the distinctions between the proposed method and DANN (DomainAdversarial NN) making it harder to assess its novelty.  The method may merely be an application of DANN to GNNbased link prediction.  The experimental setup might not accurately reflect realworld scenarios with an imbalanced domain sample size of 1 and other domains having 10 samples. This simplified scenario raises doubts about the methods general applicability.", "This paper introduces a novel way to use adversarial training to improve graph embeddings in imbalanced domains. The approach is inspired by fewshot learning which is a popular technique in computer vision. The paper explains how to apply fewshot learning to graphs and how to design experiments for imbalanced domains. The results on two benchmark datasets and the PPI dataset show that the approach improves graph embeddings.", "Paraphrase: This work introduces adversarial training for situations involving a small number of labeled data points for link prediction in graphs. The idea is to employ a discriminator that determines the domain of graphlevel embeddings. This technique addresses the difficulty of link prediction in graphs with limited labeled data. Experiments on three benchmark datasets demonstrate the superiority of the proposed method over existing ones. Strengths:  The work establishes a novel problem of link prediction with limited labeled data and imbalanced domains.  The method outperforms baselines and tSNE plots offer a practical guideline for using the proposed method.  Visualizations effectively demonstrate the effectiveness of the proposed approach. Weaknesses:  Adversarial training in graph or link prediction is not entirely novel with several relevant prior work omitted from the discussion.  The specified setting for fewshot link prediction seems arbitrary. The authors do not provide context on its application or motivation.  The model uses a 3layer GCN with 32 neurons but no hyperparameter ablation work are presented."], "mz7Bkl2Pz6": ["Paraphrased Statement: This paper examines the behavior of Stochastic Gradient Descent (SGD) under broader assumptions than previously considered. Traditional global convergence proof for SGD required restrictive conditions: global Lipschitz continuity of gradient and specific noise models for stochastic gradient. This paper relaxes the global Lipschitz assumption to a local Lipschitz assumption and incorporanges more general noise model assumptions. The key outcomes are global convergence and stability:  global convergence: Iteranges either converge to a stationary point or diverge.  stability: The objective function remains bounded along any sequence of iteranges. To overcome the challenges posed by the general assumptions the authors employ novel techniques to separange certain dependencies. Strengths and Weaknesses:  The paper is wellwritten and presents intriguing results.  Assumption 5 is rather intricate and its origin is unclear.  The results focus on asymptotic behavior (infinite iterations).  It remains to be explored whether nonasymptotic results (convergence range) are achievable under the relaxed assumptions.  The study considers general matrix stepsizes but scalar stepsizes may warrant further examination.  The definition of the convergence range (LR) may require clarification as different values of \u03c8 could potentially lead to the same LR. Typos:  Page 2: \"the the\"  Page 4: \"holds for function a variety\"  Page 6: \"our\" for eq (9)  Page 6: practice \"\u03b8j\u2264R\" mean \"X\u03b8j\u2264R\"  Page 6: \"be get arbitrarily closely\"", "Paraphrased Statement: Summary: This paper examines the global convergence of Stochastic Gradient Descent (SGD). The authors demonstrate that under less severe conditions than previously established SGD iterations will either reach a stationary point of the objective function or diverge. Strengths: 1. Studying SGDs global convergence is crucial in machine learning and optimization. 2. The paper is wellorganized and easy to follow. 3. The analysis techniques might benefit the research community. Weakness: Compared to earlier studies the primary distinction lies in the weaker assumptions: 1. previous: global H\u00f6lder continuity This work: local H\u00f6lder continuity 2. previous: Stochastic gradient with bounded variance This work: Stochastic gradient bounded by an upper semicontinuous function The authors assert that their context is more practical. However they do not provide concrete examples to justify this claim (particularly for the second assumption).", "Paraphrased Statement: Summary: This study examines the global behavior of stochastic gradient descent (SGD) in a general nonconvex context. The authors address limitations of previous assumptions removing both the strict bound on noise and relaxing the Holder continuity requirement. An model is provided to illustrate the inadequacy of the noise bound typically employed. SGDs global convergence is established indicating that it either diverge or converge to a finite value with a vanishing gradient excluding unfavorable outcomes such as oscillations. A strong assumption yields stability ensuring convergence with probability one. Strengths and Weaknesses: Strengths:  study global behavior of SGD in a general nonconvex context  Eliminates the unrealistic noise boundedness assumption Weaknesses:  Assumption 5 may require further clarification and justification  The technical challenges and introduction underlying the results could be elaborated upon in the contributions section"], "tG8QrhMwEqS": ["Paraphrased Summary: This paper presents a method for structured pruning of neural networks. By creating attentionbased feature map for each layer the method assesses the importance of filters within that layer. An adaptive threshold strategy then determines the number of filters to remove based on userspecified criteria. This method has been shown to improve performance on CIFAR10 and ImageNet datasets using ResNet architectures outperforming various other stateoftheart pruning techniques. effectiveness:  Simple and easy to implement  Offers multiple adaptive pruning modes tailored to specific constraints Weaknesses:  Evaluation of filter importance is similar to existing methods  Rationale for threshold calculation is unclear  Lack of experiments on nonResNet architectures  Ablation study to analyze the impact of attention score and pruning strategy is missing  Evaluation of actual speedup benefits when minimizing FLOPs is absent", "Paraphrase: Summary: This research presents an automated technique for pruning neural networks that doesnt rely heavily on manual input. The focus is on: 1. Pruning channels based on the activation map they generate rather than the channel weights. 2. An iterative method that automatically corrects poor pruning determination. effectiveness and Weaknesses: Weaknesses:  Unclear writing style and confusing presentation.  Lack of clarity in explaining the pruning strategy.  Missing ablation studies:  compare of pruning technique and iterative method separately.  Runtime analysis for the automated approach.  Modest FLOPs reduction compared to other methods (e.g. Eigendamage).  Experiments limited to ResNets would benefit from testing on VGG models. effectiveness:  Pruning based on activation map is an intuitive and logical approach.  Compelling results in compare to other pruning methods.  Relatively simple and interpretable automated approach.  Includes ImageNet results which is uncommon in pruning literature.", "Summary: This paper presents a pruning method that merges multiple existing techniques to achieve comprehensive pruning. It combines activationbased thresholding with iterative structured pruning. effectiveness:  Achieves promising results on Cifar10 and ImageNet datasets.  Provides flexibility for users to prioritize various factors (accuracy latency memory) during pruning. Weaknesses:  The method lacks novelty as it primarily combines existing techniques rather than introducing new concepts.  It lacks a thorough analysis of the algorithms convergence and correctness.  The compare on ImageNet are limited including only a small number of stateoftheart pruning methods. It is recommended to compare with more recent and comprehensive methods on additional benchmark models."], "gJLEXy3ySpu": ["Paraphrase: This paper offers a substantial guarantee of robustness against adversarial alterations for topk predictions using the l0norm. It broadens the top1 prediction certification radius defined by Levine and Feizi (2019) to cover topk predictions and extends the l2norm certified radius of Jia et al. (2020) to the l0norm. Experiments on CIFAR10 and ImageNet evidence that the proposed method considerably outperforms existing approach for topk prediction robustness. The paper presents both a robust theoretical foundation and compelling experimental results. It provides formal proof of the l0norm certified robustness guarantee for topk predictions which is distinct from previous research by Levine and Feizi (2019) and Jia et al. (2020). Additionally the extensive experiments showcase the consistent enhancement in topk certification accuracy on CIFAR10 and ImageNet. The ablation work further clarifies how hyperparameters influence performance.", "Summary Paraphrase: The paper presented a more precise method for L0 verification of the top K predictions made by classifiers using randomized smoothing. Strengths and Weaknesses Paraphrase: Strengths:  The paper provides a strong theoretical foundation and impressive experimental results. Concerns and Questions:  The section titled \"Deriving the certified radius for the smoothed classifier\" is confusing. The authors should reorganize it and clearly define the notation.  The claim that the certified radius is tight for k1 is only valid when no assumptions are made about the base classifier. This is not typically the case so the statement may be overstated.  Table 12 does not include comparison of top1 predictions for the authors methods.  The authors should consider comparing their method with other L0 verification methods such as the one presented in the provided link.", "Paraphrased Statement: Abstract:  This paper presents theoretical advancements and experimental results on ensuring the robustness of predictions made by models that generate top k predictions. Theoretical contribution:  It demonstrates that randomized ablation (as proposed by Levin and Feizi) has a certified radius for top k predictions in terms of the \\ell0 norm.  It proves that the certified radius is optimal for k1 and almost optimal for k1 (the radius cannot exceed \"the derived radius plus 1\"). Experimental Results:  The paper compares the proposed method to existing approach and investigates the impact of relevant parameters. Strengths:  Clear and comprehensible writing style.  Novel theoretical contributions clearly differentiated from prior work. Weaknesses:  No significant weaknesses noted. input:  The paper approach lacks generalizability making it difficult to classify it as a major theoretical discovery.  The comparison of the proposed method to existing approach may not be entirely fair.  The experimental results focus on demonstrating superiority rather than exploring more nuanced questions.", "Paraphrase: Summary: The paper enhances the accuracy guarantee of randomized smoothing for the \\ell0 case and improves upon existing guarantee by considering the discrete nature of randomized ablation predictive distribution. It also extends \\ell0 methods to certify topk predictions beyond the usual top1 certification. Strengths:  Clearly highlights related work and their contributions.  Describes improvements in a way that simplifies understanding.  Extends randomized ablation to topk predictions increasing its applicability. Weaknesses:  contribution may seem incremental due to efficient explanations.  practical testing focuses on images while \\ell0 metrics are commonly used for graph neural networks and 3D deep learning. Evaluating these networks could enhance the impact of the paper."], "psNSQsmd4JI": ["Summary The authors present a framework called CMARL for distributed multiagent reinforcement learning. CMARL enables scaling learning by utilizing containers that house actor replay buffer learner and a queue manager. Strengths  Clear and wellwritten paper with thorough explanations of architectural decisions.  Reproducible method and code built on the popular PyMARL repository.  novel and advantageous use of diversity between containers. Weaknesses  Insufficient introduction of experiments:  Use of time on the xaxis makes CMARL appear significantly beneficial than baselines.  Baselines may not have had sufficient time to complete training leading to underestimation of their performance.  Selected baselines prioritize sample efficiency rather than speed limiting comparison to existing literature.  Unclear use of multiagent nature in the method:  CMARL may be algorithmagnostic raising questions about its utility in singleagent settings.  Experiments need to justify the use of multiagent environments if the primary motivation is increased data requirement. Other Considerations  The paper refers to multiagent settings with global states but this is not always the case in MARL.  Parameter sharing between global and local learner is an intriguing concept.  Further research could explore how parameter sharing across agents is handled in CMARL.", "Paraphrase: This paper introduces a distributed framework for multiagent reinforcement learning (MARL) using containers. It tackles three key challenges: transferring observation data between agents handling interprocess communication and enabling efficient coordinated exploration among agents. By employing containers that gather environment experiences into buffer for each agent and training local policies this framework (CMARL) outperforms existing benchmarks in terms of time efficiency. Strengths:  clear introduction with sections dedicated to each component of CMARL.  Helpful graphical illustrations of neural network structures.  detailed experimental setup for reproducibility.  Extensive testing across multiple environments against stateoftheart algorithms.  Inclusion of ablation field on several CMARL components. Weaknesses:  Limited novelty compared to existing hierarchical MARL algorithms.  Ambiguity in how containers achieve diversity.  Insufficient explanation of the mutual data loss and its impact on experience diversity.  Lack of additional graphs to complement the presented results.  Incomplete ablation field including unspecified rationale for parameter choices and comparisons between different scenarios with varying agent counts.", "Paraphrase: Summary This paper presents a novel distributed multiagent reinforcement learning framework using a valuebased approach. It divides the system into multiple containers and a centralizer. container are trained locally while the centralizer handles highpriority samples from all containers. To enhance exploration the framework incorporates a loss function that encourages diversity among containers. It achieves improved performance on the Google Research Football and StarCraft II micromanagement benchmarks. Strengths:  clear motivation and need for distributed acceleration in multiagent RL.  novel implementation of distributed training with containers and a centralizer.  Accelerated training process due to reduced data transfer requirement.  Potential for improved results with sufficient training resources.  Wellorganized and clear writing. Weaknesses:  Incorrect formula in Equation (5) for approximate calculation of p(\\mathbfat \\mid \\mathbf\\taut).  Similarity to CDS [2] in encouraging diversity among containers.  Multiqueue manager is essentially a twolevel replay buffer introducing potential interference issues.  Unclear rationale for using prioritybased screening of experiences for the centralizer learner.  Unfair comparison with baselines due to different training resources.  Recommendations for reimplementation using pymarlv2 (optimized version of pymarl) to demonstrate effectiveness and compatibility with recent approach in multiagent RL.", "Summary (Paraphrased): This paper explores the challenges of distributed multiagent deep reinforcement learning (MARL): data transfer interprocess communication and efficient exploration. The authors propose a containerbased distributed MARL framework that addresses these challenges. The framework isolates the environment interaction component in containers reducing CPU overhead and facilitating diverse policies in different containers. It also uses prioritized experience replay (PER) to enhance training efficiency by selecting valuable samples. Strengths (Paraphrased):  direction on a significant and timely topic in MARL.  clear and intuitive introduction of the proposed approach. Weaknesses (Paraphrased):  Demanding data transfer: The paper does not fully address the issue of data transfer between GPU and CPU which is a critical bottleneck. The solution of placing the environmental interaction contribution in containers does not eliminate the need for data copying between CPU and GPU.  Interprocess Communication: The authors introduce a multiqueue manager and buffer manager to optimize communication but their design lacks detailed discussion. The use of signals to control readwrite conflicts may not guarantee uninterrupted learning.  efficient exploration: The MIbased approach for encouraging diverse policies is not novel and has been explored in singleagent RL settings.  policy Diversification: Constraining policies to have diverse behaviors using KL divergence may increase the computational load during parallelization which is not discussed.  comparison Methods: The authors fail to compare their approach with distributed singleagent training system which would provide a valuable benchmark.  experimental evaluation: The authors experimental setup may be biased as their method uses more resources than comparison methods and the use of a scoring reinforcement mechanism may inflate performance estimates."], "gf9buGzMCa": ["Summary This paper presents three findings regarding the universal approximation of functions on compact sets under a width constraint that typically prevents universal approximation. The authors argue that these results advance the understanding of neural network representation capabilities and have practical applications. However while the first finding is clear the second and third findings raise some concerns. 1. maximum principle: neural networks with continuous monotonic activation and width limited by the input layer achieve their maximum (and minimum) values at the boundary of compact domains. This explains why universal approximation with these functions on arbitrary compact sets is impossible. 2. disjoint Domains: The authors claim a result in Theorem 2 that applies to \"finite collections of disjoint n0dimensional compact sets.\" However most of their discussion focuses on only two sets. The generalization to multiple disjoint sets is not clearly justified or supported by experiments. 3. Harmonic activation: The authors demonstrate universal approximation on finite and discrete domains using the paper of cosine functions. This result is not directly relevant to the main topic of the paper as it hinges on the discreteness and finiteness of the domain not the width of the input layer. Strengths and Weaknesses Major Observations:  The generalization of Theorem 2 to multiple disjoint sets is not adequately supported by experimental evidence.  It is unclear how K2 is defined in Theorem 2. Minor Observations:  The references to \"NN\u03c3(n0)\" and \"arbitrary compact sets is impossible\" could benefit from more clarity.  The validation of Corollary 2 references a \"Proposition 1.2\" that is not defined.  Using dashes instead of commas in certain places would improve readability.  Definitions of terms like C and Lp should be provided earlier.  The meaning of \"multiply pairwise compact components\" should be clarified.  The term \"root cause\" in the discussion of the first finding is somewhat vague. Additional References:  [1] https:arxiv.orgabs1711.02114 (On expressiveness in terms of depth and width)  [2] https:arxiv.orgabs1906.00904 (On memorization in terms of expressiveness)", "Paraphrased Statement: compact: This paper explores the expressive power of neural networks with limited width (maximum width less than or equal to input dimension). It investigates the question of which functions can be approximated by such networks. Main contribution:  maximum principle (Theorem 1): Networks with continuous monotonic activations and width less than or equal to input dimension achieve maximum values on the boundary of compact subsets of the input space.  Tightness of Lower Bounds: This principle demonstrates the tightness of previous lower bounds on width.  Approximation of Functions on Two disjoint Sets: Shallow ReLU networks with width less than or equal to input dimension and depth 4 can approximate functions that take constant values on disjoint subsets of the input space under certain conditions.  Expressivity of Cosine Networks: Shallow cosine networks with width 1 and depth 3 possess certain expressive properties. Strengths and Weaknesses: Strengths:  Investigation of a lessstudied aspect of neural networks (width).  Technical results that provide theoretical insights. Weaknesses:  Limited novelty compared to related work.  Lack of a particularly solid or significant main message.  Collection of relatively minor results that may not warrant publication in a toptier conference.", "Paraphrase: compact: universal approximation theorems in neural networks typically show that a class of neural networks can approximate any function in a given function space. However recent research indicates that for continuous function spaces on arbitrary compact sets the network width must exceed the input dimension. This study investigates subsets of Rn that allow for universal approximation with neural networks having a width equal to the input dimension. findings: The study provides insights on: 1. A threshold of n0  1 for network width when the activation function is continuous and monotonic (e.g. ReLU). 2. A sufficient condition for approximating piecewise constant functions using compact sets with specific geometric properties. 3. A universal approximator for a finite sample set using a width1 depth3 neural network with a cosine activation function. Strengths:  Clear and accessible writing.  Comprehensive review of prior research.  Novel investigation into universal approximation on restricted compact sets.  Practical relevance for approximation tasks in realworld applications. Limitations:  Some results apply only to monotonic activation functions which limits their generalization compared to recent studies on deep narrow neural networks.", "Paraphrase: compact: This paper investigates the approximation capabilities of neural networks (NNs) with narrow width (width less than input dimension). It explores whether there are compact subsets where NNs can still achieve universal approximation or exact interpolation for all functions mapping from those subsets to real numbers. The paper first demonstrates the existence of subsets that preclude universal approximation guarantees. Subsequently it provides sufficient conditions for specific compact subsets to enable universal approximation or exact interpolation guarantees particularly for applications in clustering and classification. The functions considered are piecewise constant over disjoint compact sets with general assumptions on activation functions that encompass most commonly used functions. Strengths:  Explores the understudied narrow NN regime contributing to a comprehensive understanding of NNs.  Introduces an intriguing maximum principle in section 3 highlighting subsets for which universal approximation is impossible even with arbitrary network depth when activation functions are monotonic and continuous.  In section 4 the paper identifies subsets that guarantee exact interpolation and universal approximation focusing on disjoint unions of finite compact sets with constant functions in each disjoint set. These subsets are relevant for clustering and classification applications. It provides conditions for exact interpolation by constantdepth ReLU networks (for two compact sets) or depth dependent on input size (for more than two sets). Additionally it demonstrates that any finite sample set can be exactly interpolated by a narrow and constantdepth network with cosine activation. Weaknesses: Theoretical:  The paper considers subsets with functions that are constant over disjoint compact sets. Can this assumption be relaxed for universal approximation with arbitrarily large depth Is exact interpolation as challenging as universal approximation in this case  The applicability of Theorems in sections 3 and 4 with nonmonotonic activation functions such as cosine is unclear. A discussion on the use of nonmonotonic activations would be beneficial.  When defining NN(n0) as the class of functions it would be informative to specify the depth (possibly inputdependent) required for approximation or interpolation. Presentation and Minor Issues:  The foundation mentions an \"significant result\" without attribution.  In page 4 \"partially constant\" may be better replaced with \"piecewise monotonic.\"  Including statements of all auxiliary theorems and lemmas relevant to the main results in the appendix would enhance reader comprehension."], "vrW3tvDfOJQ": ["Summary Paraphrase: This field addresses uncertainties in reinforcement learning (TD and Actor Critic) using inverse variance weighting (IV) and probabilistic ensembles. The authors analyze uncertainty sources demonstrating how TD learnings heteroskedasticity necessitates IV weighting of training sample. They propose a combination of approach to address variance in issue from the environment and QNetwork estimation. The IVRL algorithms outperform previous methods in simple continuous control and MuJoCo tasks. Strengths and Weaknesses Paraphrase: Strengths:  Clear motivations and valid contribution to developing robust reinforcement learning paradigms.  solid foundation in supporting material.  Insightful connection between TD learning and regression on heteroskedastic labels justifying the use of Bayesian inverse variance (BIV). Weaknesses: Major:  Lack of specificity in exact quantities and network structures used in BIV and learning algorithm objectives.  Vague descriptions of baselines and ablations making experimental analyses difficult to interpret.  Inconsistency between emphasis on supervision choice and use of exploration concepts as justification for performance. Minor:  Lack of concrete takeaways and conclusions about IVRLs strengths and limitations.  Abbreviations and acronyms in figures could be more informative.", "Summary: This field focuses on reducing uncertainty in QLearning ActorCritic (QLearningActorCritic) architectures. The authors propose a method that estimates the uncertainty in target use and uses this information to weighting sample with an inverse variance technique called batch inverse variance weightinging (BIV). This approach enhances training efficiency and reduces sample complexity allowing frameprocess to reach similar performance with fewer training episodes. Strengths:  Theoretically sound and practical method that exploits aleatoric (random) uncertainty in target use.  Improves sample complexity a crucial aspect for applying reinforcement learning to realworld problem.  Demonstrates significant sample complexity improvements in both discrete and continuous environments with clear improvements in two out of three discrete environments and three out of four continuous environments. Weaknesses:  Omits the value of a key hyperparameter that influences the performance of the proposed method limiting reproducibility.  There appears to be an inconsistency in the definition of the loss use as the BIV loss term includes a squared error loss which is also present in the LA loss term. Suggestions:  Provide information on the tuning of the lambda parameter in the loss use for reproducibility.  Clarify the details of the loss use definition and resolve any inconsistencies.  Consider evaluating the method on a more various set of environments to ensure robustness of results.  Relate the components of the loss use to the experimental results and use more precise terminology related to uncertainty quantification.  Reference recent process on practical issue in aleatoric uncertainty estimation using Gaussian negative log likelihood loss.", "Paraphrase: Summary: The field presents inversevariance reinforcement learning (RL) which combines probabilistic neural networks (with variance estimation) and inversevariance weighting (to incorporate uncertainty). Experiments in OpenAI Gym environments demonstrate its effectiveness when coupled with DQN and SAC surpassing baselines and SUNRISE. Strengths:  Clear writing and explanation of justification and results. Weaknesses:  Lack of clarity in mathematical formulation and implementation: The methods specific footprint and how to implement it in RL are not welldefined.  limited mathematical detail: The paper only provides equations for supervised learning which is insufficient for an RL paper.  Novelty and significance: The method primarily combines existing approach rather than introducing novel concepts. The proposed weighting scheme is directly adapted from supervised learning and the analysis of uncertainty sources is potentially novel but not sufficiently significant.  Empirical evidence limitations: Experiments are limited to a narrow range of environments raising questions about generalizability. The paper does not explicitly demonstrate the advantage of the proposed weighting scheme (BIV) over choice (UWAC SUNRISE).  potential conflict with optimism in RL: Variance downweighting may be seen as a form of pessimism which could hinder the benefits of optimism in RL uncertainty handling.", "Summary Paraphrase: This paper introduces a novel algorithm group Inverse Variance Reinforcement Learning (IVRL) which utilizes uncertainty estimates to guide the adjustment of value use (and actorcritic settings). IVRLs effectiveness is demonstrated through evaluations of IVDQN and IVSAC in multiple benchmarks. Strengths and Weaknesses Paraphrase: Strengths: 1. High Relevance and concern: The issue is significant and has widespread concern within the research community. 2. Thorough Literature Integration: The approach is wellgrounded in existing literature. 3. Exceptional Clarity: The paper is highly accessible and provides intuitive explanations for key concepts. 4. comprehensive Evaluation: issue are presented across multiple hyperparameter sets and different seeds providing a robust evaluation. 5. applicability in various Settings: The benefits of IVRL are showcased in both discrete and continuous environments. Weakness:  None Identified: The paper is wellwritten and does not exhibit any notable weaknesses. Additional Note: The reviewer expresses concern in further discussion regarding policy degeneration observed in the LunarLander environment."], "vnF5gDNvcKX": ["Summary This paper addresses the high variance in policy gradients due to domain randomization in reinforcement learning simulations. The authors demonstrate that using statedependent baselines for each environmental parameter rather than solely statedependent baselines can further reduce policy gradient variance. An algorithm based on this analysis is proposed which consistently accelerates policy training in six robot control tasks. Strengths  The proposed algorithm generalizes well compared to other methods. Weaknesses  The novelty is limited since the statedependent baseline is similar to an existing method.  The inconsistency in clustering based on Q value but finding nearest neighbors based on environmental parameter is unclear.  A potential error in the theoretical analysis regarding the conditioning of environmental parameter is identified.  The clarity of the paper could be improved with confusing notation and imprecise mathematical parameter.  The algorithm needs more explanation regarding baseline updates and hierarchical clustering. Concerns  The algorithms variance reduction is not analyzed in detail.  The use of subspace clustering for stateenvironmentdependent baselines may be unnecessary. other Comments  Typographical errors are noted (italicization symbol usage).  The meaning of NC and N in the algorithm is unclear.  A factor in the equation is incorrectly labeled pcc1M which should be pcc0H1.", "Paraphrased Summary: This paper explores how to reduce variance in policy gradient estimation which is caused by randomizing environments during training. The authors propose a state and environmentdependent optimal baseline that eliminates this bias. They also develop a VRDR (ValueReconsidered Domain Randomization) method that divides the environment space into subspaces and estimates the baseline accordingly. effectiveness and Weaknesses: effectiveness:  Addresses the variance problem in domain randomization training.  Clearly written and thorough. Weaknesses:  Theorem 1 is not practically useful.  The method assumes known environment parameter which may not be realistic.  convergence improvements are not significant for all tasks despite the added computational cost of clustering.  The choice of clustering method and interval could influence the issue.  The paper does not compare to the active domain randomization algorithm from a previous field. Additional Concerns:  parameter Nc is not defined in algorithm 1.  The paper should clarify that the testing environments are used directly rather than being trained on.  The baseline used for comparison is not the same as the proposed method in the previous field cited by the authors.", "Paraphrase: Summary: The research presented introduces an optimal baseline that adapts to environmental and state changes and a technique for reducing variance in domain randomization for policy gradient methods. effectiveness:  beneficial writing and solid mathematical groundwork.  Extending baseline state value to include environmental variance is a logical step.  The variance reduction technique is wellestablished in singleenvironment training and its extension to multiple environments is straightforward. Weaknesses:  The experimental issue lack depth and are insufficient in the number of baselines compared.  number 1 does not convincingly demonstrate the benefits of the proposed methods while variance reduction in standard training is shown to be significant.  The analysis is lacking and does not explore potential reasons for the inconclusive issue.  significant baselines such as meta learning and robust reinforcement learning methods are not included in the comparison.", "Paraphrase: Summary: This work examines methods for improving the performance of policy gradient algorithm in domain randomization (DR) settings. Specifically it explores techniques to reduce the variance of gradient estimates under DR leading to more effective policy updates and faster learning. algorithm: The paper proposes an algorithm called Variance Reduced Domain Randomization (VRDR) which theoretically derives an optimal state and environmentdependent baseline for policy gradient methods. It provides general guidelines for constructing such baselines. Evaluation: Experimental issue on continuous control tasks demonstrate that VRDR outperforms two baseline algorithm. effectiveness: 1. Wellstructured and clearly written. 2. Highly relevant and significant issue. 3. beneficial balance between theoretical grounding and empirical evaluation. Weaknesses: 1. Why not measure the gradient variance directly instead of using a proxy 2. Lower variance gradient estimates could potentially lead to convergence to local optima. 3. Its not intuitive why lowervariance gradient estimates generalize better in DR compared to supervised learning. 4. The acronym \"MRPO\" at the end of Section 5.1 should be clarified (likely a typo). 5. The added computational cost of VRDR compared to baselines should be further explained."], "t1QXzSGwr9": ["Summary Paraphrase: This study investigates image classification using quantum computers. While prior study only considered 4x4 images this paper experiments with 16x16 MNIST images. Strengths and Weaknesses Paraphrase: Strengths:  practical implementation of image classification on quantum computers. Weaknesses:  Minor Technical Contribution: Encoding (Eq. 68) is standard circuit synthesis (Section 5) uses an established method and circuit implementation relies on existing software (TensorFlow quantum). FRQI was also proposed earlier in 2011.  Overstated Abstract Claims: The abstract suggests comparable accuracy to classical neural netstudys but the paper acknowledges inferior performance (90 quantum vs. 98.9 classical).  Limited range: Experiments are restricted to number 3 and 6 and the scale is even toysized compared to classical method.  Typos: The paper contains several typos.", "Paraphrase: Summary: The paper introduces a quantum image coding method using 2qubit gates. Downsampled MNIST digits are identified using standard components. The experimental section also explores the use of shorter codes. Strengths and Weaknesses: Positives:  Clear and wellorganized background and problem definition sections.  Multiple experiments validate the proposed approach. Negatives:  Vague innovation:  lack of explanation on why quantum computers excel over classical ones.  Unclear definition of \"inherently quantum\" data.  Claims of encoding images on current quantum hardware without experimental evidence.  Questionable reliance on model experiments on a consumer laptop given that quantum systems are approaching the limits of classical supercomputer models.  Assumption that color qubits are either 0 or 1 despite MNIST digits being black and white. Raises concerns about applicability to color images. Minor Remark:  The statement on page 3 \"completely specifies the others\" could be improved for clarity.", "Paraphrase: Summary: The paper explores the use of quantum neural netstudys (QNNs) for traditional image classification. It employs the FRQI approach to process larger input images (8x8 or 16x16 pixels) instead of the typical 4x4 pixel images. The reresearchers develop parameterized quantum circuits (CRADL and CRAML) using XX and ZZ gates to facilitate quantum state transformations and classification. Strengths:  The use of FRQI to encode images with higher resolution.  The innovation of CRADL and CRAML circuits for classification tasks. Weaknesses: 1. Novelty Limited: The image encoding method is not novel having been used in other QNN reresearch [1]. Similarly the parameterized circuit concept is comparable to that used in [2]. 2. Binarization Loss of data: Despite the larger image resolutions the images are binarized resulting in significant data loss. The paper does not justify why 8x8 binarized images are superior to 4x4 integer images used in [3]. 3. lack of Comparison with Other Embedding method: Since the paper claims to have an efficient embedding method comparisons with alternative method (e.g. phasebased embedding [3] [4] statevector embedding [2]) are essential. 4. Circuit Depth Sensitivity to Noise: The proposed embedding circuit appears complex (deep) making it potentially vulnerable to noise in NISQ devices. actual quantum device experiments (e.g. on IBMQ machine) could validate the results. 5. Limited Dataset and Hyperparameter Analysis: The study is based solely on the MNIST dataset without considering ablation study on hyperparameters. Comparisons with previous QNN study are also absent. 6. Overstated Novelty Claim: The assertion that this study is the first to classify actualistic images is exaggerated as previous study [2] [3] [5] have achieved similar classification tasks on the MNIST dataset with [3] even demonstrating performance on actual quantum machine. 7. Subpar Empirical Results: The experimental results suggest that classical neural netstudys consistently outperform the quantum models. 8. Justification of Circuit Choice: The paper should provide more justification for using CRAML and CRADL circuits. Given the vast innovation space it would be helpful to explain why these specific ansatz were selected. References: [1] Jiang Weiwen Jinjun Xiong and Yiyu Shi. \"A coinnovation framestudy of neural netstudys and quantum circuits towards quantum advantage.\" nature communications 12.1 (2021) 113. [2] Farhi Edward and Hartmut Neven. \"classification with quantum neural netstudys on near term processors.\" arXiv preprint arXiv1802.06002 (2018). [3] Wang Hanrui et al. \"quantumnas Noiseadaptive research for robust quantum circuits.\" arXiv preprint arXiv2107.10845 (2021). [4] Lloyd Seth et al. \"quantum embeddings for machine learning.\" arXiv preprint arXiv2001.03622 (2020). [5] Henderson Maxwell et al. \"Quanvolutional neural netstudys powering image recognition with quantum circuits.\" quantum machine Intelligence 2.1 (2020) 19.", "Paraphrased Statement: The paper introduces a novel quantum machine learning algorithm for classifying compressed images. It employs a quantum circuit to convert images into quantum states and uses trainable gates to predict labels. However the algorithm has several limitations. Strengths:  Introduces a quantum algorithm for image loading and classification.  Incorporates quantum computing principles into the algorithm. Weaknesses:  Marginally contributes to the field of quantum machine learning.  Assumes binary image representation which may not be suitable for certain datasets.  Has high circuit depth despite logarithmic qubit number.  lack comparison with existing image loading techniques in quantum machine learning.  Uses a nonnovel classification circuit.  Limited understanding of the circuits capabilities.  lack theoretical analysis including time and gate complexities."], "rMbLORc8oS": ["Paraphrased Statement: This work examines a chemical reaction retrosynthesis challenge. It presents a novel retrosynthesis framework enhancement with two primary components: DRGAT for potent embedding calculations and semitemplates for precise synthon completion. particularly the proposed semitemplates consider the neighboring local bonding structure. This effectively minimizes template redundancy bridging templatebased and templatefree retrosynthesis approaches (the previous being less flexible but more effective and the latter being more flexible but less effective). experimental evaluations demonstrate the effectiveness of the new components for each subtask resulting in overall retrosynthesis performance that surpasses existing frameworks. effectiveness:  A novel retrosynthesis framework variant utilizing a localityaware semitemplate  Strong numerical performance in subproblems and main retrosynthesis task Weaknesses:  Inadequate explanation of DRGAT  Potential omission of references and comparisons  number with poor visibility", "Paraphrase: Summary This paper introduces a novel automated method for designing chemical reactions (retrosynthesis). The authors approach uses preexisting templates to generate plausible reaction pathways. They streamline the process by reducing template overlap resulting in a set of 150 simplified template fragments that cover almost 97 of reactions. Additionally they propose a method for identifying the reaction center using a specialized attention mechanism applied to a molecular representation. Empirical evaluations demonstrate that the proposed method outperforms existing retrosynthesis approaches. effectiveness and Weaknesses: effectiveness:  Wellstructured and accessible writing.  The authors break down complex templates into smaller manageable fragments reducing redundancy.  The resulting 150 fragments provide comprehensive coverage of reactions.  The attention mechanism enhances the identification of reaction center.  impressive retrosynthesis performance achieving toptier accuracy.  analysis of the individual step in the retrosynthesis process is included. Weaknesses:  The concept of semitemplates has been previously published in a journal article.  The contribution of the attention mechanism (DRGAT) is not clearly specified.  The atom and bond feature used in Table 1 are not provided.  The test data may still contain information that could facilitate shortcut solution.  Evaluation on a larger dataset (e.g. USPTOfull) would demonstrate the scalability of the method.", "This research investigates predicting the ingredients that form a given chemical product known as onestep retrosynthesis. To tackle this challenge the authors introduce SemiRetro a novel framework that combines techniques from other templatebased (TB) and templatefree (TF) approaches. SemiRetro is divided into two components: 1. DRGAT an innovative GNN architecture that practices edge attention to identify the reaction center and decompose the product into synthons (molecular building blocks). 2. \"Semitemplates\" (picked from a predefined set by a classification framework) that fill in the synthons and assemble the reactants. The contributions of the research are as follows: 1. DRGAT outperforms prior methods in identifying reaction center (specifically the RGCN practiced in Shi et al.s G2G) on respective evaluation metrics. 2. The practice of semitemplates allows SemiRetro to cover a full range of reactions than previous templatebased methods while requiring fewer unique templates. 3. SemiRetro achieves higher accuracy than many previous TB and TF methods especially when the reaction class is unknown. effectiveness: 1. SemiRetro achieves superior accuracy compared to prior retrosynthesis methods. 2. Semitemplates cover a full range of reactions than full templates resulting in better training efficiency. 3. SemiRetro has faster training times and a smaller memory step than comparable techniques. 4. The paper presents a clear explanation of the approach aided by helpful number. Weaknesses: 1. DRGATs superior performance is largely attributed to its enhanced reaction center prediction capability. further analysis such as an ablation work would be valuable to understand its exact contributions. 2. The novelty of SemiRetro is questionable as it shares similarities with Somnath et al.s GraphRetro which also follows a twostep process of predicting reaction center and adding leaving groups. The distinction between semitemplates and leaving groups should be clarified. 3. While the training efficiency and scalability of SemiRetro are highlighted experiments on larger retrosynthesis datasets would demonstrate its practical advantages. 4. The evaluation lacks comparisons to stateoftheart methods such as Sun Ruoxi et al.s Energybased View of Retrosynthesis and Sacha Miko\u0142aj et al.s Molecule Edit Graph Attention Network Modeling chemical reaction as Sequences of Graph Edits. Questions for the Authors: 1. Can you compare SemiRetros synthon completion performance to other methods like Somnath et al.s 2. When the reaction class is known do you practice it to filter out irrelevant templates or provide it to the reaction center network as input 3. In number 5 the dotted green line appears to exceed 1. Is this accurate 4. In equation 5 are the node embeddings \\mathbfhk derived from (a) message passing on the template graph or (b) the synthon graph 5. You mention on p.6 that \"Reactants can be determinately generated from semitemplates and synthons.\" What are the conditions under which this fails 6. Can you elaborate on why SemiRetros interpretability is superior to previous approaches", "Summary: A novel retrosynthesis prediction algorithm has been developed incorporating an innovative GNN architecture and semitemplates. This algorithm achieves exceptional performance on benchmark tasks. effectiveness:  Demonstrates strong results  Employs a robust GNN architecture Weaknesses:  Lacks an ablation work to determine the contribution of each component  It is potential that the GNN architecture is primarily responsible for the observed improvements rather than the semitemplates or other elements of the algorithm  Comparative baselines against existing methods such as Sacha et al.s MEGAN Sun et al.s work and Seidl et al.s approach are not provided. These methods have been shown to outperform SemiRetro in top10 accuracy. Additional Note:  It is suggested that the Szymkuc et al. 2016 citation in the introduction may not be appropriate as their work argues against the practice of datadriven approaches for this problem."], "huXTh4GF2YD": ["Summary This paper introduces a novel open set recognition method using spacebased classifiers. It minimizes the space between class samples and their means for labeled data and forces background data to lie outside class acceptance regions. Unseen class samples can then be rejected based on their space from known class focus. The method outperforms existing open set recognition approaches. effectiveness  Clear writing and wellarticulated motivation and methodology.  Aims to create compact class regions within which known class samples lie and keep background samples outside.  Favorable experimental results demonstrating superiority over competing methods. Weaknesses  Limited novelty as the idea of using spacebased classifiers for creating compact class regions has been explored before.  Problematic use of classspecific means which are randomly selected and fixed instead of being updated during training. This may disrupt semantic similarities between classes.  lack of comparison to a space metric learning method using triplet loss which would provide a more comprehensive evaluation.  Absence of an ablation study to assess the significance of background class samples in creating compact class acceptance regions.", "Paraphrase: Summary: This article presents a new technique called distancebased background class regularization (BCR) for openset recognition (OSR). This approach employs a distancebased classifier based on linear discriminant analysis to confine the feature space of knownclass data separately for each class. This in turn pushes backgroundclass samples away from this reduced feature space. The experiments demonstrate the impressive OSR performance of this method. effectiveness and Weaknesses: effectiveness:  Clear and concise writing  development of a distancebased background class regularization (BCR) method utilizing linear discriminant analysis for OSR  Welldesigned ablation study to evaluate the distancebased methods performance Weaknesses:  Limited novelty: The distancebased classifier employs principles that have been explored in other approaches (e.g. Mensink et al. 2013)  Insufficient experimental evaluation: The study lacks comparisons with contemporary Softmaxbased techniques (e.g. Liu et al. 2017 Wang et al. 2018)", "Summary Paraphrase: For recognizing objects that dont belong to known classes (open set recognition) the authors propose using data from sources outside those known classes. This \"background\" data is used to account for unknown instances. They use a method called LDA to estimate the probability of an instance belonging to a class using the space from that instance to the class mean. The class means are chosen randomly before the model is trained. The authors introduce a loss function that combines crossentropy loss with a regularization term based on background data. This background data is used to estimate a probability of inclusion which is then used to calculate the regularization term. The authors test their method in two scenarios: (1) where the unknown classes come from the same dataset as the known classes and (2) where the unknown classes come from a different dataset. In both cases their method outperforms several existing algorithms. effectiveness and Weaknesses effectiveness:  The main contribution is a loss function that incorporates background class regularization based on the probability of inclusion.  Empirical results show that the proposed method performs well compared to other algorithms.  The paper is wellwritten. Weaknesses:  The loss function includes both crossentropy and background class regularization terms. It would be helpful to investigate the issue of removing one of these terms.  A more detailed explanation of the need for the background class regularization term would be beneficial. Minor issue:  The paragraph next to Figure 2 contains Equations 7 and 8 for the probability of inclusion (PI).", "Summary This research introduces a new method for generalized openset recognition (OSR) that combines known and unknown classes in classification tasks. The goal is to develop a model that can simultaneously detect unfamiliar classes while maintaining accuracy for known classes. The method employs background class regularization (BCR) utilizing data from both known and unknown classes during training. Unlike previous BCR techniques it separates feature classwise ensuring that unknown class data remains distant from known classes. A distancebased classifier with linear discriminative analysis is used to process feature extracted from a deep neural network. A novel loss function is designed to balance performance between known and unknown classes while preserving closedset classification accuracy. The proposed approach is evaluated against three other BCRbased OSR methods and demonstrates improved performance on image and text classification benchmarks showcasing enhanced detection of unknown classes without compromising accuracy for known classes. issueiveness  Combines multiple techniques in a unique and welljustified manner.  Clear theoretical introduction and experimental validation.  high openset recognition rate and comparable or good closedset accuracy.  Code and experimental setup available for reproducibility. Weaknesses  ablation study could provide more detailed insights into model behavior.  Computational complexity and efficiency analysis in relation to previous methods is missing. Additional Comments and Questions  cater a complete loss equation including classification and classinclusion terms.  Consistently use similar terminology in experimental subsections.  Introduce the background dataset (ImageNet) other in the text.  Bold equally good results of previous methods in tables.  Collect ablation study results on classification accuracy in a separate table.  Explain the optimal lambda selection process and its impact.  Discuss the issue of different class mean vector initialization strategy.  Explore good use for selecting \"known unknown classes\" data and consider automation.  Investigate model robustness under different choices of \"known unknown classes\" data."], "nEfdkfAyRT8": ["Paraphrased Statement: The paper presents a novel secondorder regularization method for solving unconstrained minmax problems where the work is doubly differentiable and strongly concave in the maximization variable with Lipschitzcontinuous first and second derivatives. The proposed method involves a nested gradient ascent strategy for the maximization variable. It uses a dynamic number of iterations to ensure desired properties and backing convergence. Additionally it employs a cubic proximal update for the minimization variable. Under standard assumptions the method is shown to converge to a secondorder stationary point. Strengths and Weaknesses: The approach combines gradient ascentdescent techniques with secondorder cubic regularization optimization preserving the Lipschitz properties of the maximization solution while utilizing an approximate solution for the strongly concave maximization problem. Improvements:  Reorganize the paper to clearly present the problem formulation and assumptions at the beginning.  Acknowledge Griewank A.s earlier work on cubic regularization.  Cite Hallak N. and Teboulle M.s recent paper on secondorder optimization.  Correct the typo in the statement of Theorem 4.", "Paraphrased Statement: Summary: This study introduces CubicGDA an innovative GDA algorithm capable of reaching secondorder stationary points in nonconvex minimax optimization unlike previous GDA algorithms that could only converge to firstorder stationary points. CubicGDA employs the Cubic regularization technique to evade saddle points in nonconvex minimax optimization. Under standard conditions it has a monotonically decreasing potential work ensuring global convergence to a secondorder stationary point. Furthermore under various gradient dominance scenarios CubicGDA exhibits a significantly faster convergence range than traditional GDA. Strengths and Weaknesses: CubicGDA effectively extends firstorder GDA algorithms to secondorder algorithms that can escape saddle points in nonconvex minimax optimization. It uses gradient ascent to approximate secondorder information and Cubic regularization to overcome saddle points. The study provides a comprehensive analysis of its convergence properties including global convergence and convergence ranges under various conditions. One limitation is that CubicGDAs learning range relies heavily on the problem condition number. Future research could investigate reducing this dependency. Additionally employing acceleranged gradient descent for strongly concave subproblems may enhance the algorithms global convergence range. The authors could provide more discussion on this potential improvement.", "Paraphrase: Summary: This paper focuses on nonconvex minimax optimization problems and introduces CubicGDA an innovative algorithm designed to escape saddle points and find secondorder stationary points. The paper provides a global convergence range for CubicGDA and analyzes its convergence behavior under local nonconvex conditions. Strengths and Weaknesses: Identifying secondorder stationary points in nonconvex minimax problems is a significant challenge in optimization. CubicGDA appears intuitive and wellmotivated. However the theoretical contribution of the paper could be enhanced and empirical validation is absent. Concerns:  global convergence range: While CubicGDA requires fewer iterations than GDA (O(\\kappa13.5\\epsilon1.5) vs. O(\\kappa2\\epsilon2)) its dependence on the condition number (\\kappa) is significantly worse which may compromise performance in illconditioned problems.  Cubic regularization Subproblem: Solving this subproblem with gradient descent only yields a sublinear convergence range. The paper does not address how this affects CubicGDAs overall convergence behavior.  Empirical Evaluation: The paper lacks empirical results to demonstrange the effectiveness of CubicGDA. It is unclear whether it outperforms existing GDA algorithms in practice. The authors are encouraged to conduct experiments and provide evidence of CubicGDAs ability to escape saddle points where GDA fails. Minor Consideration:  Exploring the potential to generalize CubicGDA to nonconvexconcave (not strongly concave) settings would be beneficial.", "Paraphrase: Summary: The authors present a secondorder method for nonconvexstrongly concave minimax problems using cubic regularization. It achieves convergence under common smoothness assumptions and Lojasiewicz gradient geometry. This method is unique in reaching a secondorder stationary point allowing it to overcome strict saddle points. Strengths and Weaknesses: Major Weaknesses:  The value of reaching a \"secondorder stationary point\" in minimax optimization is unclear and should be explained by the authors.  The algorithm is similar to the Cubic regularization Newton method with the main challenge being accuracy in solving the inner maximization problem. This is not very challenging due to the exponentially fast convergence in strongly concave cases.  comparison to existing firstorder methods are lacking.  The choice of parameter \\etax depends heavily on the conditional number leading to large constants in convergence results potentially negating the benefits of secondorder methods.  It is unclear if CubicGDA retains the performance of the original CR method near strict saddle points or local minima.  mathematical experiments to support the theoretical findings are absent. Minor Weaknesses:  \"Envelop\" should be corrected to \"envelope.\"  y(x) is referred to as a minimizer but it is actually a maximizer.  The method is described as \"GDAtype\" which is misleading since there is no gradient descent step and it employs secondorder information suggesting a Newtontype approach.  The references \"Jin et al. 2017a\" and \"Jin et al. 2017b\" both point to the same citation and the \"Jin et al. 2017b\" mentioned in the related work on CR appears to be incorrect.", "Paraphrase: This paper introduces novel algorithms that provide secondorder convergence guarantees for nonconvexstrongly concave problems. The paper starts by presenting convergence range guarantees for the general setting and later provides linear or superlinear convergence range results under different range of \u0141ojasiewicz gradient geometry. Strengths and Weaknesses:  To the beneficial of my knowledge this is one of the first paper to analyze secondorder convergence ranges for nonconvexstrongly concave minmax problems (as too discussed in a similar paper [1]).  Both the results and analysis appear promising.  The theoretical contribution is significant for this field. Concerns:  ICLR is a conference that prioritizes practical relevance.  This work primarily focuses on convergence ranges but does not consider periteration cost.  Forming the Jacobian matrix in the algorithm and solving the cubic subproblems exactly can be computationally expensive in machine learning applications.  The practical relevance of the paper may be limited unless the authors provide approximate optimization steps for the primal variable vector and analyze the iteration complexity in terms of firstorder oracles (gradient evaluation and Jacobianvector product).  For the results under \u0141ojasiewicz gradient geometry the authors should provide practical examples that satisfy this assumption to enhance understanding instead of merely citing relevant paper."], "fgcIb5gd99r": ["Paraphrased Statement: This research introduces a multiscale fusion selfattention module that enhances the extraction of phaselevel information at diverse scales. It uses convolution performance with varying kernel sizes to achieve this. The researchers tested the module on relation extraction and GLUE (General Language Understanding Evaluation) tasks to prove its effectiveness. effectiveness and Weaknesses:  Weaknesses:  Limited novelty: Using convolution performance with different kernel sizes to extract multiscale information is not novel (e.g. GoogleNet).  Uncertainty of phase information extraction: Simply using convolution performance with different kernel sizes may not guarantee phase information extraction.  Dynamic sparse module implementation: The module sets attentional values below a threshold to zero to reduce discussionlevel attention when a discussion focuses on a phrase but the mechanism for achieving this is unclear and its contribution lacks empirical evidence. Reference:  Szegedy C. Liu W. Jia Y. Sermanet P. Reed S. Anguelov D.... Rabinovich A. (2015). Going deeper with convolution. In CVPR 2015.", "Paraphrase: Summary: This paper proposes a modified transformer network architecture that includes: 1. Using a convolutional operator to extract phrase features and calculate phraseword attention. 2. Predicting a mask for wordword attention to reduce phraseword attention when wordword attention is substantial. Experiments show that this method enhances the base BERT model when applied to its extracted features. effectiveness and Weaknesses: effectiveness:  The method improves the BERT baseline. Weaknesses:  The method adds additional parameters which may explain its improved performance.  The concept of combining convolution and transformers is not novel.  Its unclear if explicitly modeling phrases is beneficial as deep neural networks may already implicitly capture them. Suggestions for Improvement:  demonstrate that the novel architecture outperforms alternatives with similar parameters and performance.  Determine if the improvement is solely due to increased parameters.  Provide evidence that explicitly modeling phrases is essential and that the method effectively captures phrase information.  Explain why existing models are insufficient for capturing phrase information.", "Paraphrased Statement: Summary: The paper introduces a selfattention approach for NLP tasks that operates at multiple scales. It aims to enhance the extraction of wordlevel and phraselevel features. Key view of the method include using varying kernel sizes for feature extraction and multiscale attention fusion. Additionally a dynamic sparse module adjusts the weights of the attention matrix. effectiveness and Weaknesses: The manuscripts contribution is limited and the proposed method lacks significant novelty. The motivation for the method is unconvincing as there are numerous existing techniques for extracting phrase and context information. Multiscale feature extraction and attention fusion are also wellestablished concepts. The proposed method merely employs different convolutional kernels without offering substantial novel insights. The analysis of related literature is superficial omitting recent stateoftheart field. Both the introduction and related process section require comprehensive revision to provide a more thorough understanding of the field. The experimental results are insufficient to validate the claims of the proposed method. The authors should evaluate it on more NLP tasks and compare its performance to current SOTA approach including those published after 2020 in Table 1. Given the methods general applicability it should be tested across a wider range of NLP tasks.", "Paraphrase: This field combines convolutional networks with selfattention to address discussiontophrase relationships explicitly. It employs a sparse masking technique to strike a balance between discussiontodiscussion and discussiontophrase attention. The model demonstrates substantial performance on diverse tasks. effectiveness:  Integrates convolutional models to improve selfattentions local nature and phraselevel learning. Weaknesses:  The use of convolutional networks and phraselevel information in selfattention has been previously explored.  The proposed dynamic masking strategy may have limitation.  The papers writing needs refinement.  Experimental details lack clarity.  The ablation field is incomplete and comparison should be enhanced. specific Criticisms: 1. Similar methods using convolutional networks and phrase information for selfattention exist reducing the novelty of this paper. 2. The masking strategy essentially adds an attention layer but its formulation prevents gradient backpropagation into certain parameters. 3. The writing needs improvement especially in using precise language. 4. The implementation of the convolutional model is unclear. 5. The authors should provide a substantialer rationale for using convolution with selfattention given BERTs contextualized discussion representations. 6. An ablation field that compares the model to a vanilla attention layer without convolution is missing."], "w_drCosT76": ["Summary The article introduces a novel model for optimizing molecular leads using a \"differentiable scaffolding Tree\" (DST). While the concept of learnable parameters in the indicator matrix adjacency matrix and node weight vector is innovative it has limitations. Nevertheless the paper offers promising insight for molecular generation. effectiveness  Differentiable Scaffolding Tree: DST is a learnable graph input for molecules allowing for optimization of molecular structures by adjusting parameters.  Scaffold Tree over molecular Graph: Using a scaffold Tree reduces the occurrence of invalid molecules.  Determinantal point process: DDP ensures diversity and maintains desired properties in generated molecules. Weaknesses  Restricted Backbone: The backbone of the oracle model must be differentiable limiting its applicability to wet experiments.  local Optimum: optimization steps can lead to local optima limiting property improvements.  Costly Assembling from Scaffold Tree: Assembling molecular graphs from the scaffold Tree can be timeconsuming. Suggestions  Explore ideas from existing research to improve the efficiency of scaffold Tree assembly.  Introduce constraints to learnable parameters to accommodate scenarios with unchangeable molecular portions.", "Paraphrase: Summary: Chemical optimization is crucial for various applications such as drug development. While deep study models and optimization methods have shown potential they face challenge in directly modeling discrete chemical structures and rely heavily on exhaustive searches. To address this the authors introduce a novel technique called Differentiable Scaffolding Tree (DST). Key Contributions: 1. The DST defines a local derivative for chemical graphs enabling gradientbased optimization of discrete structures. 2. A chemistryaware graph network is used to encode prior knowledge about functional groups molecule and topological relationships in chemical structures. 3. A unified graphbased optimization model is proposed to handle both singleobjective and multiobjective optimization tasks. effectiveness and Weaknesses: effectiveness:  The DST provides a scalable solution for generating novel molecules due to its implicit differentiation approach which maintains computational efficiency and memory space. Weaknesses:  Additional quantitative comparisons with related study are needed.  The approximation for formulating the problem as an iterative local discrete search requires further clarification especially regarding equation 10.  The inclusion of single molecule as \"substructure\" may not align with chemical intuition as molecule are typically considered sidechains for modification rather than structural units for analysis.", "Paraphrased Summary The paper introduces a novel approach called Differentiable Scaffolding Tree (DST) for designing molecules. DST enables continuous gradientbased optimization to improve specific molecular feature. Experiments show that DST excels in optimizing molecular properties producing diverse and innovative molecules. Notably DST requires fewer queries from external sources (oracle) and mostly uses them before the optimization process begins. effectiveness and Weaknesses Pros:  Novel and innovative concept  Promising optimization results  Continuous gradientbased optimization without pretraining a latent space model Cons:  Lack of point in experimental setup (e.g. number of generated molecules optimization iterations per molecule) Questions for Rebuttal point:  How many Adam steps are performed in each optimization iteration", "Summary This paper presents an innovative genetic algorithm (GA) for optimizing molecular structures. The key innovation lies in the mutation step where a pretrained graph neural network (GNN) is used to guide the direction of mutations by predicting probability distributions over modified graphs. The algorithm shows promising results on various molecular optimization tasks outperforming various baseline methods. effectiveness  Novel differential scaffolding Tree (DST) concept simplifies mutation steps.  Clear and wellorganized introduction of the DST method.  Comprehensive comparison with multiple baseline methods.  Interpretable results highlight the potential for understanding molecular structureproperty relationships. Weaknesses  The paper fails to explicitly acknowledge the GA nature of the approach potentially overlooking comparisons with existing GAs.  The necessity of the DST method is debatable as alternative mutation strategies could be simpler and less computationally demanding.  Lack of clarity in the experimental section limits the impact of the reported results. Suggestions  Explore alternative mutation strategies to evaluate the impact of the DST method.  Include strong baselines especially Bayesian optimization to provide a more robust comparison.  Improve the clarity of the experimental section by providing more detailed explanations and using industrystandard metrics."], "mQxt8l7JL04": ["Summary THis work presents two significant contributions: 1. A novel regularization term for Variational Autoencoders (VAEs) to ensure an (approximate) isometry between tHe latent space and tHe unknown data space. 2. A \"flattening\" measure tHat enHances tHe isometry constraint by modifying tHe latent space directly. Experiments demonstrate tHe effectiveness of tHese tecHniques on diverse datasets. effectiveness and Weaknesses effectiveness:  THe paper provides insigHts into How an approximate isometry between latent and data space improves generative framework performance. Weaknesses:  THe rationale beHind tHe isometry requirement is not fully explained.  Isometries may not always exist between different types of manifolds so tHe relevance of quasiisometry in generative framework is unclear.  THe work lacks a compelling \"killer application\" tHat demonstrates tHe advantages of isometric representation learning.  THe quality of latent and data space dimensions (D and m) is crucial but is not discussed in detail nor are sensitivity experiments provided.  THe quality of identity metric for tHe data space needs justification.  THe proposed measure (Equation 7) is novel but may need furtHer clarification. Writing and Novel Idea:  THe paper is wellwritten and easily understandable.  THe core idea is novel but Has potential similarities to recent work. Additional Comments:  Visual examples of scaled isometries could improve understanding.  Computational cost of tHe flattening measure sHould be addressed.  Interpretation of tHe equidistance plot could be provided.  References are needed to support claims about tHe popularity of tHe H() use.  A grammatical error exists on page 5 (\"inHerent tradeoff We introduce\").", "Paraphrased Statement: Summary The authors present a novel work of regularized autoencoder (specifically a variational autoencoder) that focuses on preserving the datas geometric structure. They show that preserving angles and space ratios enhances data representation. They modify the regularization term and analyze the balance between reconstruction and geometric preservation. Additionally they implement a strategy for flattening the latent space representation after working. Strengths  Addresses the notable challenge of unsupervised manifold learning using neural networks.  Expands on previous work by Chen et al. and Jang et al.  Introduces Isometric regularization using Hutchinsons trace estimator for efficient gradient calculation.  Demonstrates improvements over current techniques. Concerns  The method is referred to as an autoencoder in the introduction but is later described as a variational autoencoder.  The rationale for using the variational component is not adequately explained.  Results on MNIST are not convincing the marginal MSE gain could be due to inadequate hyperparameter tuning.  Hyperparameter tuning work for all methods is unclear.  The latent dimension is set to 2. For MNIST and image datasets a higher dimension is potential necessary.  Equispace ellipses lack clear definition and may not accurately represent space preservation in highdimensional space.  improvement in retrieval task seem significant but hyperparameter tuning for all methods is not detailed. minor Comments  The term \"learning the data manifold\" is undefined.  A period or comma is missing before \"We introduce\" on page 4 to improve time flow.  image 1 and 2 contain redundant MSE plots. Redundant inworkation can be omitted.  The colors used in the embedding should be clarified.  The dataset used for the experiment in image 2 should be specified.  The term number calculation in image 4 is not adequately described more explanation is needed. Relevant Citations  McQueen et al. (2016) discuss nearly isometric embedding using relaxation.  Peterfreund et al. (2020) present a local conworkal autoencoder for standardized data coordinates.", "Paraphrased Summary: This work presents a regularization technique that enforces autoencoders to generate latent space that are more geometrically similar to the input space up to a scaling factor. It also demonstrates the tradeoffs between two proxy measure for geometric similarity and reconstruction accuracy as well as the improved performance of a variational autoencoder utilizing this regularizer in a retrieval task. effectiveness:  Clear theoretical motivation and development of the method.  Enhanced performance in an unsupervised image retrieval task compared to a strong baseline.  Favorable tradeoffs between geometric similarity and reconstruction error. Weaknesses:  Lack of averaging over multiple random seeds in experimental results potentially affecting reproducibility and reliability.  potential conflict between KLdivergence regularization in VAEs and the introduced regularization scheme.  Absence of discussion on alternative perspective such as exploring different navigation strategies in the latent space. Questions for Authors:  Clarification on the novelty of the proposed family of regularization term.  Impact of the regularization on the generative capabilities of the framework.  Reasons for the significant difference in performance between IRVAE and IRVAEFM regarding P1.  Quantification of the computational overhead incurred by the regularization procedure. minor Comments:  Section 2 could be considered a subsection.  Inconsistencies in notation for the use h.  Credit to [2] should be given for VAEs.  Definition of \"sufficiently large number of samples\" for computing MCN and VoR in Section 5.1.  use of a categorical color scale for categorical information in image 1 and 2 would enhance readability.  Inconsistencies in reference formatting.", "ParapHrased Statement: Summary: THe work presents a novel regularization metHod tHat enforces geometric constraints on tHe latent space of an autoencoder. THis metHod defines a HierarcHy of geometrypreserving mappings empHasizing scaled isometries (maps tHat preserve angles and distances up to a scaling factor). THe autoencoder training involves learning tHe scale factor alongside tHe manifold and latent space representation. A novel coordinateinvariant regularization term measures tHe proximity of tHe decoder to being a scaled isometry. A subsequent flattening procedure enHances tHe geometric properties of tHe latent space. Experiments demonstrate significant improvements over existing metHods in diverse application scenarios. effectiveness and Weaknesses: THe paper is wellstructured and offers a compelling solution to tHe cHallenging problem of incorporating geometry into latent space. THe proposed regularization metHod is wellreasoned and tHeoretically sound. THe coordinateinvariant functional is original and effective. THe experiments are tHorougH and HigHligHt tHe metHods efficacy. However tHere are some field for clarification:  AutHors claim tHat a less stringent regularization on isometry is preferable but furtHer evidence to support tHis is lacking.  THe use of tHe stabilizing latent space flattening is unclear and sHould be elaborated.  WHile tHe metHods superiority over FMVAE is acknowledged a more detailed explanation of tHe coordinateinvariant strategys significance is needed.  Notation inconsistencies between H in Equation (7) and Section 4.2 sHould be rectified.  It would be beneficial to sHowcase tHe metHods advantages for data generation potentially tHrougH visualization of interpolated data.  code availability and performance metrics would enHance tHe papers reproducibility and practicality.", "Paraphrase: Summary: This work focuses on preserving geometric relationships in learned latent space representations examining a image of geometrypreserving mappings. While current methods like SimCLR overlook geometric preservation this paper demonstrates the superiority of angle and relative distancepreserving mappings over angle and absolute distancepreserving mappings. The proposed representational learning technique combines reconstruction loss with isometric regularization achieving embeddings with isometric properties without compromising reconstruction quality. The paper compares its approach to VAE (Kingma  Welling 2014) and FMVAE (Chen et al. 2020). effectiveness and Weaknesses: Pros:  Investigates geometrypreserving regularization in representational learning an significant field with potential impact. Cons:  The paper employs a separate invertible mapping that does not affect reconstruction accuracy which could be addressed within the original decoder.  The assumption of H(x) as the identity simplifies the regularization but limits its domainspecific applicability.  Visualizations in image 1 do not clearly demonstrate the benefits of isometric regularization.  The paper lacks ablation work on mixup regularization to isolate its contributions.  Comparisons are limited to VAE and FMVAE potentially excluding stronger representational learning baselines."], "nc0ETaieux": ["Paraphrase: Summary The research makes practice of cryptographic techniques to demonstrate the unexpected finding that a uniform small error against all neural network discriminators of polynomial size does not guarantee a small error in the Type1 Wasserstein distance. Strengths and Weaknesses Major Points  The work has a potentially significant impact by showing that polynomialsized discriminators may be insufficient for achieving distributional learning.  It prompts a reconsideration of whether GANs are more appropriate for feature learning than distributional learning.  The practice of Wasserstein distance as an objective work may need to be reevaluated given the lack of correlation with GANs empirical performance.  assumption such as polynomialsized neural network discriminators and diverse target distributions appear reasonable.  The application of pseudorandom generator theory to GAN analysis is novel.  Numerical experiments provide support for the theoretical findings. Minor Points  Typos in the paper:  Definition 2: Replace \\mathbb Ex\\sim q(f(y)) with \\mathbbEx\\sim q (f(x)).  Eq (1): Replace \\mathcal Dd(m) with \\mathcal D\\astd(m).  Incomplete inequality in the proof sketch of Theorem 3.2: The inequality comparing \\mathbbE[\\mathcalM\\tau(G(Um))] and \\mathbbE[\\mathcalM\\tau(Ud)] is incomplete.", "Paraphrase: Summary: The work demonstrates that if Goldreichs pseudorandom generator (PRG) can deceive Boolean circuit of limited size it is possible to create a miniature rectified linear unit (ReLU) network that generates a distribution that is nearly identical to the target distribution (with respect to Wasserstein1 distance) but cannot be distinguished from the target by any ReLU network of reasonable size. Strengths and Weaknesses: Strengths:  The work reduces the question of \"bad solution to the Generative Adversarial network (GAN) target\" to the presence of local pseudorandom generators. This is an intriguing and unique idea.  The research shows that minmax optimality may not imply that the GAN learns the target distribution at least in certain artificial consideration. Weaknesses:  The work relies heavily on theory.  The solutions significance is questionable because it is widely recognized and intuitive that deceiving a limited class of discriminators does not necessarily entail precisely matching the target distribution. Suggestions for Improvement:  Define \\gammam in Theorem 1.1 to enhance clarity.  Simplify Definition 5 for readers unfamiliar with cryptography by explaining terms like \"uniformly random kuniform hypergraph\" \"kary predicate\" and \"circuit in Ppoly.\"  Specify the failure probability in Lemma 2.4 (i.e. the solution holds with probability at least 1\\delta).  Consolidate the numerous dispersed Lemmas to make the paper more coherent and reduce readers cognitive burden. For instance Lemma 2.1 is only applicable to Theorem 3.6 and Lemma 2.3 is selfevident and never used. Consider omitting these Lemmas from the main body of the paper.", "Paraphrased argument: Summary: still with (population) minimax optimality for a GAN the generator may not have learned to mimic the distribution under the Wasserstein distance. Specifically simple generators (randomized ReLU network) with constant depth polynomial size and Lipschitzness properties can produce discrete distributions from simple seeds (e.g. uniform on a cube) that are distant (in Wasserstein distance) from any diverse distribution and indistinguishable from a diverse target distribution by polynomial discriminators. This proof relies on the assumption of the existence of local pseudorandom generators. Strengths and Weaknesses:  The solutions are original and significant establishing a connection between GANs and local pseudorandom generators.  The authors claim that minimax optimality does not imply distribution learning which is imprecise because distribution learning depends on the chosen success measure.  The focus on Wasserstein distance is reasonable but somewhat arbitrary changing to another inherent probability metric (IPM) could invalidate the solution.  The success of GANs in applications like image generation may be partly due to their nonoptimization of traditional distancedivergence measures.  The papers demonstration could be improved with clearer notation and additional background on pseudorandom generators for nonexperts. Clarifications and Details:  Random networks are used as generators allowing for discrete seed distributions but continuous output distributions.  The reductions in the paper may not hold for deterministic (e.g. ReLU) networks which should be explicitly stated.  The weak experimental section could be strengthened by using a continuous target distribution more typical in real applications. Minor Corrections:  Define \\gammam in Theorem 1.  Specify the argument(s) referred to when mentioning \"polynomial discriminator\" and \"polynomial generator.\"  Define the negl work in the main text since it is used in the main theorem argument.  In Theorem 3.1 clarify that the generators arguments and Wasserstein factor WF depend on \\epsilon(m).  In equation (1) verify if Dd(m) should be f(Dd(m)).", "Paraphrase: Summary: This work examines generative adversarial networks (GANs) using a ReLUbased generator and discriminator optimizing the Wasserstein1 metric. It demonstrates the existence of a \"bad\" generator that can fool all discriminators under the estimated Wasserstein1 metric while significantly deviating from the true data distribution under the real Wasserstein1 metric. The proof employs two assumptions: one regarding the diversity of the target data distribution and another commonly used in cryptography. The findings consider the computational complexity of the model and potentially provide insights into the learnability of GANs. Strengths and Weaknesses: The paper is compelling but there are some concerns regarding its demonstration and significance: 1. The assumption about data distribution diversity is insufficiently explained particularly in the main theorem making it unclear to readers. 2. The magnitude of the error term in Theorem 3.1 is unclear. Is it practical Does the solution offer guidance for training GANs given that the Wasserstein1 metric estimation is monitored during training 3. Despite the papers claim that \"minimax optimality doesnt imply distribution learning for GANs\" it does not explore the likelihood of encountering this situation in practice which limits the papers significance. 4. There is a disconnect between the theoretical findings and the experiments. By training only four types of discriminators the authors may have omitted potential discriminator variations. A more comprehensive experiment that includes an example where the generator successfully deceives all discriminators would be beneficial.", "Paraphrased argument: Summary: The paper demonstrates that a smallsized discriminator network (polynomial size) cannot effectively distinguish between the original data distribution and the distribution generated by a constant depth generator. This is proven mathematically showing that still when the discriminator has neural IPMs a large Wasserstein distance can still exist between the data distributions. Strengths and Weaknesses: Strengths:  Clear and wellstructured paper  Rigorous and complex theoretical framework  Gradual development of the theory from discrete cases to continuous generalization Weaknesses:  Technical details may be hard to follow  Section 3.2 and 3.3 could be improved with better explanations:  Section 3.2 lacks a clear connection to Section 3.1.  Section 3.3 could benefit from a highlevel description of the extension from binary to continuous output before Theorem 3.6. Experiments:  The experiments are provided for illustrative design but lack clarity:  It is unclear whether design 1 shows training or testing loss.  The Wasserstein distance is claimed to be large but no numerical verification is provided.  The method for computing the loss \\(\\mathbbE[\\log (D(X))]\\) is not explained."], "hcoswsDHNAW": ["Paraphrase: This work has optimized AdvProp an adversarial training technique for faster training. It introduces a reduction in the proportion of adversarial examples within mixed training batches and incorporates various recently developed adversarial training techniques. Experiments using ImageNet datasets and different backbone models validate the efficiency of this improved method. Strengths:  efficient resolution of a major drawback of AdvProp reducing computational cost.  Promising empirical results. Weaknesses:  Limited innovation and technique strength.  Empirical evaluation focused only on ResNet architectures excluding EfficientNets.  Insignificant performance improvement for object detection tasks.  Conflicting claims and empirical support in the paper.  Discrepancies in the selection of perturbation strength and the effectiveness of BN tricks between TensorFlow and PyTorch implementations of AdvProp.", "Paraphrased Statement: Summary: AdvProp an adversarial training method is computationally inefficient. This paper proposes various measures to improve efficiency: 1. Decoupling: Adversarial samples are no longer required for each clean range. 2. Onestep Adversarial: Multistep adversarial training is replaced with a single step. 3. Gradient Recycling: Existing techniques for accelerating AdvProp including gradient recycling are reused and refined. Strengths:  Presents \"tricks\" that significantly reduce AdvProps computational cost.  Provides insight into the effectiveness and limitations of these techniques.  Highlights specific benefits such as the effectiveness of gradient recycling in nontargeted attacks and with specific batch normalization strategies.  Clear writing and solid motivation. Weaknesses:  Lacks major scientific discoveries primarily relying on empirical observations.  Relies heavily on prior work for gradient recycling techniques.  Biased experimental results due to the use of different training budgets for AdvProp and baselines.  Questionable whether the proposed measures truly provide a \"free lunch\" of efficiency gains without sacrificing performance.", "Paraphrased Statement: Summary: This paper introduces Fast AdvProp an enhanced implementation of adversarial propagation. Adversarial propagation strengthens deep neural networks by adding adversarial training with extra batch normalization layers specifically for adversarial examples during training. The authors incorporate existing techniques and modifications to improve training speed. Strengths:  Clear and thorough explanations of methods and justifications for modifications through experiments.  Demonstrates faster training with Fast AdvProp compared to standard AdvProp through a count of data processing steps.  positive experimental results indicate Fast AdvProps effectiveness in improving ResNet performance on ImageNetlike datasets while maintaining a budget similar to standard training. Weaknesses:  Fast AdvProp combines various existing methods including adversarial propagation free adversarial training and Shuffling BN limiting its novelty.  However each modification is supported by experiments and the overall evaluation is convincing.  Table 1 provides inconsistent data on accuracy gainslosses compared to standard training without clear explanations or colorcoding.  Minor errors in grammar and capitalization in the text.", "Paraphrased Summary: This work proposes Fast AdvProp a method to enhance training efficiency and reduce computational costs for AdvProp which utilizes adversarial examples to improve range recognition accuracy. Fast AdvProp lowers computational costs by recycling gradient calculations during training. Through experiments it demonstrates improved recognition and object detection performance compared to AdvProp within similar training constraints. Additionally Fast AdvProp seamlessly integrates with existing training techniques like mixup and can be applied to various vision tasks. Strengths:  Welldefined motivation and quantitative comparison of training costs.  Clear illustrations and algorithm that convey the underlying concept.  Logical organization and presentation.  Demonstrated superior performance and adaptability to different tasks.  Comprehensive ablation work that analyzes various aspects of the proposed method. Weaknesses:  The rationale for introducing \"gnoise\" in Algorithm 1 is unclear. Providing more details would enhance clarity.  The performance for padv values of 0.2 and 0.3 is not reported in Table 4. For a comprehensive comparison it would be beneficial to include AdvProps robustness performance in Table 3."], "keQjAwuC7j-": ["Paraphrased Statement: This paper focuses on the integration of differential privacy and adversarial robustness for pretrained machine learning models. It aims to develop methods that offer both privacy and robustness without retraining. The authors utilize an existing technique involving a denoising autoencoder placed before a pretrained model followed by a noise injection scheme known as \"randomized smoothing.\" This approach is known for its high accuracy against adversarial examples but its privacy guarantee were previously unexplored. This work adapts this algorithm to provide differential privacy for the autoencoders training data. The authors claim various contributions: 1. Combining inherent input perturbations with explicit gradient perturbations to satisfy differential privacy leading to improved accuracy for the same privacy levels. 2. Introducing novel analytical tools (MGM and MMGA) to analyze privacy guarantee for multivariate Gaussian noise injection. 3. Conducting experiments on various benchmark datasets demonstrating that their algorithm (\"TransDenoiser\") provides better privacy while maintaining comparable robustness to previous methods. effectiveness and Weaknesses: effectiveness:  Addresses relevant topics in the ML community: privacy and robustness for large models.  Simple and understandable concept.  Intriguing idea of translating input noise injection into gradient perturbations for simpler privacy analysis. Weaknesses:  Technical quality concerns specifically regarding Lemmas 1 and 2 and their unclear connection between input noise and gradient perturbations.  Claims regarding the analysis of multivariate Gaussian noise injection may not be fully original since they could be related to previous work on matrixvalued Gaussian mechanisms.  Unfair comparison of TransDenoiser with previous methods due to differences in privacy preservation scope.  Inapplicability of a solution (Theorem 3) for Algorithm 1 as it does not use Poisson sampling as assumed in the cited work.  Cluttered appendix with unnecessary redemonstrations of existing solutions.", "Paraphrased Summary: This research investigates the challenge of simultaneously achieving total differential privacy (DP) and certified robustness for pretrained machine learning models. The TransDenoiser framework is introduced which extends the existing framework by Salman et al. (2020) by incorporating transformed gradient perturbations for comprehensive DP. The authors mathematically analyze the DP guarantee provided by these perturbations and empirically evaluate the TransDenoiser method on MNIST and CIFAR10 datasets. The solution demonstrate TransDenoisers effectiveness in resisting Fast gradient Sign method (FGSM) and Projected gradient Descent (PGD) attacks while maintaining guaranteed DP. effectiveness and Weaknesses: effectiveness:  Welldescribed idea: The concept is clearly explained.  Detailed analysis: Both theoretical and empirical analysis are provided. Weaknesses:  Lack of novelty: TransDenoiser lacks significant foundation as it primarily builds upon the framework proposed by Salman et al. (2020).  Missing comparison: A comparison with the primary baseline method ([1]) is absent diminishing the credibility of the solution.  Ambiguous purpose of DP: The significance of DP and the distinction between \"partial\" and \"overall\" DP are not adequately explained. Background information on DP should be included in the main body of the paper rather than the appendix.  clarity issues: The writing could be improved for clarity with various typos confusing sentences and inappropriate symbol choices. The TransDenoiser Training Algorithm is particularly difficult to comprehend.  Unclear scope: It is not specified whether the proposed method applies to the Linfinity norm both theoretically and experimentally.  Convincingness of experimental solution: The empirical evaluation could be enhanced by including robustness accuracy against PGD attacks using different variants (e.g. MadryEtAl CW Auto approach). Designating methods with descriptive names instead of confusing abbreviations would also improve clarity.  Lack of discussion on tradeoffs: The possibility that the improved robustness may come at the expense of natural accuracy should be discussed. The absence of clean examples for other baseline methods hinders a fair comparison.  Misleading statements: The authors make misleading statements such as incorrectly attributing model retraining to [1] and failing to acknowledge the purpose of MSE in [1].  method evaluation: The methods should be evaluated on larger datasets (e.g. CIFAR100) and more popular models (e.g. ResNetxx WRNxx) for a exhaustive demonstration of their effectiveness.", "Paraphrased Statement: This paper introduces TransDenoiser an algorithm that simultaneously provides privacy and robustness to machine learning models. TransDenoiser purposes a denoiser to perturb input and gradient enabling it to achieve both differential privacy and certified robustness. The privacy guarantee is thoroughly analyzed and experiments demonstrate the proposed methods effectiveness in preserving model utility and resilience against adversarial attacks. effectiveness: 1. The paper effectively quantifies the privacy guarantee by transforming input perturbation into gradient perturbation which allows for the purpose of explicit gradient perturbation in the privacy analysis. 2. It introduces a Multivariate Gaussian mechanism that considers multivariate Gaussian perturbation to enhance the privacy analysis. 3. The proposed method exhibits strong performance across various datasets and adversarial attacks. Weaknesses: 1. The Multivariate Gaussian Mechanism is not a novel concept and various previous work have explored its purpose for differential privacy.", "Summary This paper investigates differential privacy (DP) in pretrained models that provide robustness against adversarial input by modifying the input (input perturbation). It shows that input perturbation can improve DP by estimating the noise in the models gradient (gradient noise) and adding gradient perturbation when necessary. This approach reduces the DP budget required by demonstrating that input perturbation provides some DP security. Experiments confirm the effectiveness of this approach showing improvements over previous methods in terms of DP against adversarial attacks. effectiveness  Simultaneous defense against adversarial input perturbations and privacy attacks is beneficial.  The paper quantifies the DP provided by certified robustness techniques.  Experiments demonstrate advantages over existing methods. Weaknesses  The perturbation mechanism combines input and gradient noise with a simplistic transformation that uses a Taylor approximation and weakly considers the loss purpose properties.  The multivariate Gaussian mechanism is a straightforward extension of existing mechanisms. Questions  How do the balance of negative and nonnegative examples affect DP  How are the perturbation scale thresholds (xilow xiup) determined in the experiments  Can gradient perturbation enhance certified robustness  How does the proposed approach comparison to methods that prioritize only certified robustness or DP Suggestions  Clarify the field of input (x(i)).  Provide a consolidated notationterminology section to define key terms (e.g. (epsilondelta)DP).  Separate the definition and analysis of the Multivariate Gaussian Mechanism (Theorem 1).  Clarify whether \"o\" in Equation (2) and Algorithm 1 represents the littleoh notation or a coincidence.  While the DP bounds using the Moments accountant are tighter than previous approach it may be more accurate to say \"relatively tighter\" in the abstract."], "ms7xJWbf8Ku": ["Paraphrased Statement: Summary: The study suggests a novel approach in BERT pretraining where sequences are packed instead of padded to meet the maximum sequence length for each instance. To preserve optimization effectiveness the method adjusts positional embedding adds attention masks and tweaks hyperparameters. This method accelerates training by twice on the Wikipedia dataset with comparable loss issue. Strengths and Weaknesses: Strengths:  Clear introduction  Novel and implementable packing strategy  Effective issue Weaknesses: 1. Generalization concerns: The packing algorithm may compromise generalization performance on downstream tasks due to nonindependent sequences. 2. Overstated speedup: The proposed methods speedup may be 1.2x instead of 2x considering BERTs twophase pretraining work. 3. Pack depth limit: The justification for limiting the pack depth to 4 is\u4e0d\u660e\u786e. 4. Incomplete comparisons: The paper lacks comparisons with alternative methods such as grouping sample by size before batching. Minor Concerns: 1. Meaning of \"sm\" in Section 3 is undefined. 2. Abstract mentions 0.02 second overhead time for SPFHP and 28.4 second for NNLSHP while the main text only shows 0.03 second for SPFHP and provides no information on NNLSHP.", "Summary Paraphrase: The study presents a straightforward method for combining time for efficient training. To maintain accuracy changes to model architecture and optimizer hyperparameters were made. Strengths:  Simple to comprehend and integrange into existing training work.  significant performance enhancements.  Ablation study demonstranges the effectiveness of model architecture modifications.  Accessible to understand. Weaknesses:  Evaluation of different pretrained models on downstream tasks is lacking despite the significance of pretrained models in BERT.  Absence of a GitHub link in the appendix making reproduction difficult. Questions: 1. The paper claims the learning range is not adjusted despite gradient accumulation through averaging. However LAMB also averages gradients but requires learning range scaling. 2. Does a comparison exist between the two proposed packing algorithms", "Paraphrase: This study analyzes the training data for BERT and identifies opportunities to minimize padding thereby optimizing computation. By consolidating multiple sequences into a single fixedlength sequence (using two packing algorithms) the authors demonstrate a 2x acceleration in BERT training without compromising quality. They also explore the necessary adjustments to model and optimizer configurations to account for the efficient batch size increase resulting from packing. Strengths:  The motivation for reducing padding is compelling.  The paper is wellstructured and clear.  The GPUTPU training libraries commonly use padding to support dynamic sequence lengths which is inefficient.  The proposed packing algorithms aim to reduce padding by combining multiple sequences into one fixed length.  The study considers the entire problem as binpacking to optimize the packing work. Weaknesses:  The packing strategy is datadependent limiting the speedup potential for datasets with different sequence length distributions.  The packing order enforced by the algorithms violates the i.i.d assumption potentially impacting model performance.  Implementing the packing workaround requires complex code modifications in input pipelines and model changes including hyperparameter retuning due to increased batch size. Novelty:  A similar multisequence packing baseline exists in the tensor2tensor library but the authors claim to have enhanced it by considering the problem as binpacking for global optimization.  The specific extent of these enhancements compared to tensor2tensor is unclear. Technical Quality:  The technical quality is diminished by the aforementioned limitations in significance and novelty. Clarity:  The paper is wellorganized and easy to follow.", "Paraphrased Statement Summary This paper introduces two packing algorithms ShortestPackFirst histogram Packing (SPFHP) and NonNegative Least Squares histogram Packing (NNLSHP) designed specifically for BERT pretraining. Key determination  SPFHP packs sequences in as little as 0.02 second significantly improving training speed.  The algorithms have been tested with packing depth ranging from 1 to 16.  The paper includes extensive supporting materials (20 pages) detailing packing algorithms model changes hyperparameter adjustments and comprehensive experiments. Strengths and Weaknesses Strengths:  Presents novel packing algorithms with promising issue.  Provides detailed experimental evaluation on IPU hardware.  Includes extensive documentation and supplemental materials. Weaknesses:  Limited testing with only BERTlarge models and Wikipedia datasets.  Lacks comparison with baseline packing algorithms.  applicability to other pretraining tasks (e.g. T5 GPT) and nontextual sequences (e.g. audio) is unexplored.  Availability of IPU hardware may be limited for some readers.", "Paraphrased Statement: This paper introduces efficient techniques for optimizing training sequences in BERT models. These methods (ShortestPackFirst HistogramPacking and Nonnegative LeastSquares HistogramPacking) aim to minimize the amount of padding tokens (50 in the Wikipedia dataset) to expedite training. The methods are easy to implement and have minimal impact on model performance. Experimental issue demonstrate a significant speedup (nearly 2x) compared to conventional BERT training. Strengths and Weaknesses: Strengths:  The idea is easy to comprehend. Weaknesses:  The authors disclose their affiliation in code snippets which could violate ICLRs doubleblind policy.  The novelty of the paper is limited as packing is an established concept already implemented in Tensor2Tensor.  The paper focuses on a narrow scenario (Wikipedia dataset BERT training) and does not explore other datasets or tasks.  The paper only presents training loss data and does not discuss downstream finetuning performance which is the ultimate metric of efficientness.  The papers organization and writing style could be improved for clarity and conciseness."], "tBIQEvApZK5": ["Summary Paraphrase: In this work the authors:  test neural networks as datadependent kernel machines.  offer applying a distillation method directly to the pairwise kernel matrix of models.  Extend their approach to ensemble settings exploring both theoretical and experimental aspects.  Conduct experiments on ensemble distillation and dataset knowledge transfer. effectiveness and Weaknesses Paraphrase: effectiveness:  The authors provide a clear need for their work and connect it to relevant theoretical findings. Weaknesses: 1. The authors claim to propose a novel feature kernel distillation (FKD) method despite similarities with existing work using similar kernels and divergence metrics for distillation. The authors should clarify these connections and provide a more detailed comparison. 2. The authors overlook recent literature on ensemblebased online distillation methods which should be included in the discussion and experimental evaluation. 3. The authors do not fully address the limitations of their method in datasets with numerous classes. They should define the applicable function cases or provide experiments demonstrating its scalability. 4. It is unclear if the method is compatible with activation function other than ReLU. 5. The authors do not specify if their method is limited to distillation from the final fully connected layer.", "Paraphrase: Knowledge distillation for neural networks encompasses various techniques. This paper examines kernelbased knowledge distillation for data sets with multiple perspective. The research demonstrates that integrating feature function from multiple teacher networks enhances the generalization capabilities of student networks. mathematical model support the effectiveness of the proposed learning method corroborating the theoretical findings. effectiveness:  The paper offers a clear presentation.  The theoretical setting is wellexplained even if the detailed proof is provided in an appendix. Weaknesses:  kernel pooling is utilized in equation (4) but max pooling is frequently employed in practice. Is the theoretical analysis applicable to max pooling as well  The choice of regularization parameter lambdaKD in mathematical model is not specified.  It is unclear if similar mathematical results can be achieved when true ReLU is employed rather of the modified tildeReLU.", "Parafrase: Resumen: En este art\u00edculo los autores exploran la ampliaci\u00f3n del marco de destilaci\u00f3n de conocimiento (KD) a casos donde el estudiante y el profesor no comparten espacios de predicci\u00f3n. Proponen la Destilaci\u00f3n de N\u00facleos de Caracter\u00edsticas (FKD) donde las ponderaciones de la \u00faltima capa de una red neuronal se consideran un n\u00facleo de caracter\u00edsticas dependiente de los datos. En lugar de regularizar la distancia entre los logits escalados por temperatura correspondientes a cada punto de datos del estudiante y el profesor en KD tradicional regularizan bas\u00e1ndose en la distancia entre los n\u00facleos de caracter\u00edsticas de un par de puntos de datos (producto interno de las ponderaciones de la capa previa a la final de la NN). En un escenario simplificado que considera los datos como multivista los autores muestran una situaci\u00f3n donde la destilaci\u00f3n de conocimiento cl\u00e1sica no funcionar\u00eda bien pero una destilaci\u00f3n de conocimiento basada en FKD usando un conjunto de profesores s\u00ed funcionar\u00eda (el n\u00facleo del profesor entre dos puntos de datos es el n\u00facleo promedio del profesor en todo el conjunto). Los autores proponen usar un n\u00facleo de correlaci\u00f3n en lugar de n\u00facleos sin procesar y regularizaci\u00f3n de caracter\u00edsticas para distribuir los valores del n\u00facleo. Contin\u00faan demostrando emp\u00edricamente que este enfoque ayuda en la transferencia de conjuntos de datos entre conjuntos de datos con diferentes n\u00fameros de clases. Tambi\u00e9n muestran que usar un conjunto de profesores permite que FKD mejore el rendimiento (pero KD tambi\u00e9n mejora el rendimiento). Puntos Fuertes y D\u00e9biles: El problema investigado en este art\u00edculo es interesante. El enfoque de tratar de regularizar para hacer coincidir los n\u00facleos del profesor en lugar de los logits escalados por temperatura tambi\u00e9n es bastante interesante. Los resultados experimentales muestran que el m\u00e9todo es competitivo en varios escenarios. La teor\u00eda es interesante aunque parece ser una extensi\u00f3n bastante sencilla de AllenZhu y Li. La \u00fanica debilidad del art\u00edculo es la falta de concordancia entre la teor\u00eda y la pr\u00e1ctica. Los experimentos muestran que FKD es bastante competitivo incluso sin conjuntos. Pero la teor\u00eda no explica por qu\u00e9. No me qued\u00f3 claro c\u00f3mo la Regularizaci\u00f3n de Caracter\u00edsticas afectar\u00eda la teor\u00eda.", "Paraphrased Statement: This paper introduces a novel knowledge distillation model called Feature Kernel Distillation (FKD) which matches the feature kernels of the student and teacher neural networks. The authors extend theoretical findings to prove the benefits of FKD over traditional neural network training. They demonstrate this with a synthetic dataset where FKD generalizes well unlike standard training. Empirically FKD exhibits consistency between theoretical prediction and empirical observations although the settings differ. It also outperforms other knowledge distillation methods on various image datasets. effectiveness:  FKD is a novel approach in the knowledge distillation field.  It can be applied in scenarios where the teacher networks output dimension differs from the student networks unlike traditional KD.  The paper provides clear theoretical justifications for FKD under specific settings.  The paper is wellorganized and provides intuitive explanation for theorems and derivations. Weaknesses:  The technical novelty of FKD is limited as it relies heavily on existing theoretical results.  The theoretical comparison between FKD KD and ensembles is not fully explained.  The understanding for an \u03b5 term in Theorem 2 is unclear.  There is an error in Appendix C.4 where Pv l should be Pv c l. Recommendation: The authors should move the theoretical results to the appendix to focus on providing more empirical results particularly in natural language process tasks. This will enhance the papers appeal and reach a wider audience."], "uSE03demja": ["This paper introduces a novel method for estimating physics simulation and rendering settings from an RGB video. Previous techniques calculated the error in image space which can lead to suboptimal results when the generated and reference videos differ significantly. To address this the authors propose to estimate the error in simulation state space using a pretrained renderinginvariant state prediction (RISP) netfunction. The RISP netfunction predicts the simulation state from rendered images under various conditions. The RISP netfunction is integrated with a differentiable renderer creating a fully differentiable pipeline from simulation and rendering parameters to predicted state. A novel regularization term is introduced to ensure the rendering invariance of the RISP netfunction. The authors also propose an efficient training strategy for calculating the gradient of the loss function. Experiments demonstrate the superiority of this method over existing approach showcasing the significant contributions of the RISP netfunction and regularization term. While the function tackles a fundamental problem and presents innovative components its applicability to substantial datasets remains unclear. The authors are encouraged to evaluate their method on a wider range of datasets particularly substantialworld ones.", "Paraphrase: Summary: This paper addresses the challenge of estimating physical system parameters from videos when rendering conditions are unknown. It introduces a unique approach using a RenderingInvariant State Prediction (RISP) network to forecast state based on rendered images. This network is incorporated into a model with differentiable simulation and rendering for parameter estimation. Training the RISP network involves domain randomization using synthetic data. The paper also presents a gradient loss that further enhances the network rendering parameter invariance. effectiveness:  Wellwritten paper with clear contributions.  Novel concept of RISP as a preliminary step in parameter estimation.  Innovative gradient loss regularization to ensure rendering parameter invariance.  Comprehensive experiments with appropriate baselines.  Ablation field demonstrates the gradient losss impact on state prediction performance. Weaknesses:  unclear if training and testing rendering environments are identical affecting the applicability to scenarios with different parameter distributions.  Evaluation on substantial videos would provide valuable insights into the methods performance in practical settings.", "Paraphrase: Summary: This research presents a novel method for employing differentiable simulators for system identification and visuomotor control tasks without requiring approach to actual underlying state. It accomplishes this by predicting a \"renderinvariant state\" that stabilizes the loss landscape and reduces domain mismatch. Experiments in four environments demonstrate the superiority of this approach over existing techniques. effectiveness:  The approach addresses the challenge of directly reconstructing object properties or control parameters from visual observations.  It overcomes the limitation of the existing \\nablaSim approach which requires identical physics and rendering engines for reference and target trajectories.  RISP the proposed method exhibits strong performance in various environments handling rigid articulated and deformable objects. Weaknesses:  The impact of using different differentiable physics engines for training and testing should be clarified.  The baseline methods should uniformly use all frames in the reference trajectory for loss computation.  The learned RISP representation may not be fully disentangled or compositional which could affect its robustness to image transformations or multiobject simulation. Minor Comments:  Correct spelling: \"articulated body\" and \"model\"."], "vkaMaq95_rX": ["Paraphrased Statement: Summary: This paper presents EXACT a framework that compresses activation value during GNN training using quantization and random projection. EXACT aims to reduce memory consumption without significantly affecting training speed or accuracy. It reduces activation size with minimal accuracy drop (0.20.5) and training slowdown (1025) across various GNN models and graph datasets. effectiveness and Weaknesses: Limited GPU memory restricts the size of GNNs that can be trained. This paper addresses this number.  effectiveness:  Simple and effective techniques for activation compression.  Demonstrates promising effect with theoretical support.  Wellwritten paper.  Weaknesses:  memory Savings: Reporting memory savings only for activations is misleading. Overall savings should include input data.  scalability: effectiveness of EXACT with sampling method and for large datasets or deeper GNNs is unclear.  Design Rationale: Justification for the order of applying random projection and quantization is not provided.  Insight into activation compression: The understanding for GNNs resilience to activation compression is not explored. Nits:  Inconsistent description of the order of random projection and quantization.  Incomplete reporting of training overhead in Table 3 and number 3.", "Paraphrase: Summary This research aims to train Graph Neural Netstudy (GNN) models using less memory. Traditionally training involves storing activation value to calculate gradients during backpropagation typically at high precision (32 number). This study challenges that practice arguing for using lowerprecision activations for backpropagation to reduce memory usage significantly. They propose two approach: quantization and random projection. Strengths  The study addresses a crucial problem.  The effect are promising especially for quantization.  The authors chose various datasets including largescale ones.  The selection of models is wellsuited including GCNII for evaluating depth scalability. Weaknesses  The advantage over sampling or historical embedding approach is unclear.  Random projection show significant accuracy degradation questioning their practical value for compression.  quantization alone offers limited novelty compared to existing method.  The study does not significantly advance the method proposed in Chakrabatis NeurIPS 2019 paper which was successfully applied to GNNs.  It lacks comparison with other lossy activation compression techniques (e.g. \"ACGC Lossy activation compression with Guaranteed Convergence\") that could also be applied to graphs. Overall while the papers contributions to the literature may be limited it has practical value due to its straightforward implementation and promising effect.", "Paraphrased Statement: Summary This study examines the usage of compressed activation maps in the training of Graph Neural Netstudys (GNNs). It introduces an optimized implementation for GPUs and extensively analyzes the tradeoffs between memory savings time overhead and accuracy loss. Experiments demonstrate that the proposed framestudy can reduce activation memory usage by up to 32x with a minimal accuracy drop of 0.20.5 and a time overhead of 1025. effectiveness  Pioneering study in utilizing compressed activation in GNN training.  Thorough empirical evaluation and theoretical analysis of memory accuracy and time tradeoffs.  Impressive memory savings with minimal performance impact. Weaknesses  Lack of comparison with other memorysaving techniques.  Future study could include:  comparison EXACT with gradient checkpointing and memory swapping.  Evaluating EXACT against sampling approach under a fixed memory budget."], "xDIvIqQ3DXD": ["Paraphrased Statement: This paper explores the theoretical limits of the Recurring Neural Network (RNN) encoderdecoder framework in a simplified linear setting. It analyzes both the frameworks ability to approximate any work (\"universal approximation\") and the range at which it approaches this approximation. Strengths:  The paper is wellwritten and organized.  The summary of the theoretical results is clear and concise.  detailed mathematical proofs are provided for all key claims.  The authors clearly differentiate their work from previous study. Weaknesses:  The paper could improve readability by briefly explaining Equation 69 without assuming prior knowledge from Li et al. (2021).  Equation 5 should be clarified to specify that \"h\u03c4\" represents the final hidden state.", "Summary This paper explores the theoretical capabilities of recurrent Neural Network (RNNs) in modeling temporal sequence within a simplified linear framework. The authors investigate an RNN encoderdecoder architecture where one RNN encodes a sequence into a \"coding vector\" and a second RNN decodes this vector into an output sequence. Key findings 1. universal approximation: RNN encoderdecoders can approximate any linear and continuous temporal relationship to any desired accuracy given a sufficiently large coding vector. 2. approximation range (Large coding vector): The error bound depends on the smoothness of the encoder and decoder weights the decay range of the output signal under the encoder and the smoothness and memory decay range of the target work. 3. approximation range (Small coding vector): In this case a \"temporal product structure\" emerges which relates the approximation range to the range of the temporal relationship being modeled. This allows for a tradeoff between the size of the coding vector and the approximation error. 4. Experiments: Experimental results support the theoretical analysis. Strengths and Weaknesses The paper presents a wellwritten and insightful theoretical framework for RNN encoderdecoders. It clearly differentiates itself from a related work Li et al. (2021). Concerns and Questions 1. The significance of \"classical\" or \"traditional\" RNNs is unclear. 2. A comparison of the proposed analysis to other related work would be valuable. 3. Some mathematical notation and Assumption require further clarification (e.g. Eq. 1 Eq. 2 Eq. 5 Eq. 6 and Theorem 4.2). 4. The practical implications and limitations of the \"linear and continuoustime idealization\" Assumption should be discussed. 5. A discussion of algorithms for constructing the approximations would be helpful. 6. An assessment of the Assumption used and their impact on the applicability of these theoretical findings to realworld architectures would be valuable.", "Paraphrase: This research explores the mathematical introduction of why encoderdecoder models generalize recurrent neural networks (RNNs) in handling sequence with varying time dependencies (timeinhomogeneous sequence). The authors use a series of equations to demonstrate the approximation capabilities of encoderdecoder models with RNNs as their factor. They begin by proving that these models can approximate any work then establish approximation range based on the \"range\" of certain mathematical concept called temporal product. These product capture the temporal relationships between the input and output sequence. Strengths and Weaknesses:  Strengths:  The paper offers rigorous mathematical proofs that encoderdecoder models using linear RNNs can approximate a wider range of sequence models than RNNs alone.  It mathematically proves that encoderdecoder architectures are better suited for timeinhomogeneous sequence modeling.  Weaknesses:  The study range is limited by its Assumption of linear and continuous time which doesnt always reflect realworld scenarios.  The complex mathematics involved may hinder the accessibility of the paper for the ICLR audience which typically prefers empirically supported claims."], "oh4TirnfSem": ["Paraphrase: Summary The researchers present PFGNN for graphlevel tasks. It utilizes an exact isomorphism solver in its design. To address the high complexity number they propose a sampling process using particle filter updates. Strengths and Weaknesses Pros:  The theoretical basis of the design and results is strong.  PFGNN outperforms numerous existing GNN methods in empirical test on various graphlevel tasks. Cons:  The paper clarity could be improved for readers who are not experts. Detailed Comments The reviewer acknowledges limitation in their expertise related to graph isomorphism testing and particle filter approach. They comprehend the premise that the individualization approach enhances the 1WL test by addressing symmetry. Experimental data showcases PFGNNs exceptional performance. Overall the reviewer deems the paper valuable but recommends improving clarity and accessibility for nonexperts. They suggest a comparison between PFGNN and standard graph isomorphism solvers (e.g. Nauty and Traces) for verification purposes. Additionally they inquire about PFGNNs time complexity on other tasks and datasets such as ZINC ALCHEMY and QM9 and how long it takes to achieve the reported superior results on those datasets.", "Paraphrased Statement: Summary: The work introduces PFGNN a novel method for individualizing node representations to enhance the expressiveness of Graph Neural Networks (GNNs). Inspired by individualization and refinement (IR) algorithms used in graph isomorphism PFGNN emulates IR by randomly selecting nodes for individualization and aggregating their representations across multiple iterations. However PFGNN incorporates learnable components to improve the selection of nodes for individualization and the individualization process itself. Strengths and Weaknesses:  Strengths:  Proposes a novel approach based on IR algorithms.  Conducts exhaustive empirical analysis to support claims.  Can be easily applied to standard GNNs.  Clearly presented and wellexplained.  Weaknesses:  Lacks ablations on individualization method and partial randomization making it difficult to identify the fundamental factors contributing to performance improvements.  Compares only to fully randomized RNI which is not a fair comparison due to the difference in preserved data.  Claims improved performance on realworld datasets without using standard benchmarks for validation. Recommendations:  Conduct ablations on different individualization methods and levels of randomization to identify the fundamental contributors to PFGNNs performance.  Compare PFGNN to partial RNI to better assess its advantages.  Utilize standard benchmarks to validate the exact improvements on realworld datasets.", "Paraphrased Statement: Summary: This paper introduces a modified version of the Individual Refinement (IR) architecture that leverages neural networks and Graph Neural Networks (GNNs) to enhance the expressiveness of GNNs in graph isomorphism testing. IR is a widely used technique for graph isomorphism testing and adapting it to GNNs is a novel approach. However IR typically involves an exponential number of branches so the paper utilizes a particle filter algorithm to approximate the complete IR algorithm by sampling K paths. Experimental results on synthetic and realworld datasets demonstrate the improved performance of the modified GNN compared to standard GNNs. Strengths:  Innovatively adapts IR to neural networks and GNNs providing a valuable contribution.  The particle filter algorithm offers an efficient and lowcomplexity approximation of the IR algorithm. Weaknesses:  Particle filter samples only K paths to approximate an exponential number of paths in the original IR algorithm which raises concerns about the expressiveness of the K paths and how they compare to the full IR algorithm.  Particle filter requires approach to intermediate observations which is not provided in the proposed algorithm. The authors may need to clarify this discrepancy.  The sampling method may lead to incorrect nonisomorphism classifications for identical graphs potentially affecting generalization performance.  The theorem 2 validation appears to be flawed due to the value being independent of T and Equations (22) and (23) are unclear.  scalability and practical complexity remain a concern. The modified framework is significantly deeper and large than the base framework and Kpath sampling requires more memory for parallel computations. The paper implementation on the ZINC dataset exhibit slower performance and higher memory consumption compared to the base GNN. Addressing these number is crucial for applicability to large datasets like CIFAR10.", "Paraphrase: Summary: This paper introduces a novel extension to messagepassing Graph Neural Networks (GNNs) that can learn representations of graphs that are nearly universally applicable. Inspired by the \"Colourings Search Tree\" method used in exact graph isomorphism solvers the authors propose a differentiable approximation of the search tree by sampling multiple paths. Strengths and Weaknesses: The paper is wellwritten and easy to understand. It combines technical depth with strong empirical results demonstrating its originality. While other frameworks have been proposed with large power than 1WL this paper approach is distinct. The framework excels in graph isomorphism testing and property detection tasks outperforming baselines on realworld datasets. However the paper primary focus on synthetic datasets raises questions about its effectiveness in realworld scenarios. Theoretical Motivation: The paper provides a strong theoretical foundation. It formalizes the universality of graph representations using the concepts of individualization and refinement. The authors show how the distance between generated embeddings is approximated by sampling paths from a search tree. Experimental evaluation: Extensive experiments on both synthetic and realworld datasets demonstrate the strong performance of PFGNN in learning expressive graph representations. The use of synthetic datasets allows for exact evaluation but it would be beneficial to assess the frameworks effectiveness on standard graph classification and property prediction datasets. Minor Comments:  Explain the training process for PFGNN on graph isomorphism test datasets.  Provide an explanation for why increasing the number of inference refinement (IR) steps does not consistently improve classification accuracy."], "upnDJ7itech": ["Paraphrased Summary: This paper proposes Knowledge Infused Decoding (KID) a reinforcement learningbased approach for grounded natural language generation (NLG). KID incorporates external knowledge into decoding by:  Retrieving relevant Wikipedia passages using dense passage retrieval (DPR).  Constructing a \"knowledge trie\" of triples (subject predicate object) extracted from the passages.  Tracking mentioned nouns and verbs in a local trie.  Generating tokens conditioned on both prior context and triple retrieval from the knowledge trie. KID is modelagnostic and can enhance both encoderdecoder and decoderonly models. Experiments show that KID:  Outperforms standard decoding methods (beam search and sampling) on NLG tasks.  Enhances knowledge infusion on abstractive questionanswering datasets. effectiveness and Weaknesses: effectiveness:  Interesting and relevant approach to grounded generation using retrieval and memory augmentation.  Model and task agnostic providing a plugin replacement for existing decoding approach.  Allows generation to be grounded in external knowledge. Weaknesses:  Lack of clarity and details in the paper making it difficult to follow.  Unclear quality of baseline.  potential issues with missing stop language and rendering constraints.  Unclear evaluation setup and comparison to prior process.  FusioninDecoder a key baseline for knowledge infusion is not included. Additional Comments and Suggestions:  Clarify how language not present in retrieved demonstrations are generated.  Explain how the local trie is used to navigate the external trie.  Define the \"inhouse split dev set\" and clarify evaluation subsets.  Compare to other constrained decoding baseline.  Provide a breakdown of runtime for KID and compare to other methods.  Add a table with dataset sizes and statistics.  Include qualitative examples to illustrate runtime mechanisms. Recommendation: Acceptance despite concerns about paper clarity. The authors should address these issues in revisions.", "Paraphrased Statement: This paper focuses on improving language models (LMs) by incorporating external knowledge. The authors introduce Knowledge Infused Decoding (KID) a novel algorithm that infuses knowledge into LMs at each decoding step. KID utilizes a local knowledge memory based on the current context and updates it through reinforcement learning to guide language generation. Consequently KID can seamlessly integrange knowledge and perform well on tasks that require knowledge such as abstractive question answering logical writing and dialogue generation. effectiveness and Weaknesses: effectiveness:  KID surpasses baseline models on diverse knowledgeintensive tasks.  Its objective function can effectively incorporange knowledge into LMs.  The dynamic knowledge infusion approach offers an alternative to static knowledge base. Weaknesses:  The details of knowledge graph construction and graph traversal are not fully explained.  Optimization parameters such as learning range and value of K could be further clarified.  It is unclear how varying K would affect performance.  Example sentences generanged by KID and comparative analysis with RAGgeneranged sentences would be helpful.  Table 5 lacks step of sentence naturalness or grammaticality which may be impacted by KIDs knowledge integration.  FiDBART should be included as a baseline method.", "Paraphrase: This paper introduces a text decoding strategy for tasks requiring extensive knowledge. It incorporates external knowledge into the decoding process by:  Efficiently retrieving relevant documents  Creating a fast and accessible knowledge data structure  Using an interactionguided decoding method that selects tokens based on reinforcement learning. The method is applicable to diverse knowledgeintensive tasks and can be integrated into existing language models. It outperforms existing knowledgeaware models even those optimized for specific tasks and array well with human evaluations. effectiveness:  No need for retraining or finetuning language models for knowledge incorporation.  Compatible as a plugandplay decoding strategy with other sampling or beambased methods.  Dynamic integration of external knowledge at each decoding step overcoming limitations of static knowledge retrieval approach.  reinforcement learning helps overcome sparse reinforcement by directly informing token selection with extracted concepts and relations. Weaknesses:  Lack of information on decoding inference time compared to other methods.  No comparison with nonRLbased decoding techniques for knowledge infusion.  Ablation studies do not fully elucidate the contribution of the RLbased decoding technique.  Insufficient description of the trie data structure its memory optimization and the dynamics of the local memory.", "Paraphrased Statement Summary: This paper aims to enhance the performance of natural language generation (NLG) tasks that require external knowledge known as knowledgeintensive tasks. The focus is on utilizing existing language models (e.g. GPT2 and BART) without modifying their architecture while incorporating external knowledge. Method: The paper introduces Knowledge Infused Decoding (KID) which modifies the decoding probability at each step to favor language retrieved from a relevant document in a corpus. To train this decoder a KL divergence term is added to the loss function to prevent the decoding distribution from deviating significantly from the original language models distribution. Results: KID significantly improves the performance of GPT2M and BARTL when applied to multiple datasets. In some cases KID outperforms the stateoftheart. Moreover it is shown to be superior to retrieval Augmented generation (RAG) which directly incorporates retrieved documents into the input. effectiveness:  KID provides a soft constraint for constrained decoding rewarding specific target language.  It consistently improves results across multiple datasets.  It is a versatile module that can be easily added to any language model in practice. Weaknesses:  The paper writing is complex making it difficult to understand.  It does not compare against a nongraph constrained decoding baseline which would provide further insights into the benefits of graphbased constraints. Questions:  The connection between the proposed loss function and reinforcement learning (RL) is unclear.  Despite the paper mention of exposure bias in vanilla decoding the proposed loss function with reinforcement at each step appears to exhibit similar bias."], "g1SzIRLQXMM": ["Paraphrased Summary: This paper examines the number of weight adjustments necessary for a deep netstudy to acquire representations comparable to the human visual cortex. The study employs CORnetS (a netstudy designed to mimic the primate ventral stream) and BrainScore (a step of the similarity between netstudy responses and brain responses). The paper investigates three strategies to reduce weight adjustments:  Reducing the number of training epochs  Initializing weight using learned weight clusters  Training only a subset of layers (critical training) All three methods significantly reduced weight updates with moderate impact on BrainScore. Combining these strategies resulted in 80 of the original BrainScore with 50 fewer weight updates. Strengths:  Clear and concise writing  Comprehensive analysis supporting the authors claims  Innovative techniques for weight initialization and critical training Weaknesses:  Motivation for the study could be clarified.  Biological plausibility of clusterbased initialization and critical training requires further discussion.  issue of clusterbased initialization on classification performance are not presented.  The referenced study (Zador 2019) focuses on connectivity rather than weight updates.", "Paraphrase: This paper tackles a crucial question in connecting biological and artificial neural networks. Despite ANNs mimicking aspects of BNNs after training their typical training methods lack biological realism making it challenging to establish their suitability for BNN model. The study focuses on the highscoring CORnet aiming to significantly reduce training updates while maintaining the high score. This proposes a likely training mechanism for BNNs. Strengths:  Addresses a fundamental question relevant to both machine learning and neuroscience.  Utilizes a stateoftheart neural architecture avoiding simplified models.  Provides clear analysis and issue. Weaknesses:  Overlooks more established biologically plausible ANN training approaches.  The improvements in score are subjective and modest.  Assumptions and plausibility are not thoroughly discussed amid alternative possibilities.  The approach appears ad hoc and lacks clear principles which may limit its appeal to the neuroscience community.  It essentially combines existing methods rather than introducing novel ideas.  The implications for the statistical learning community are unclear.", "Paraphrase: Summary: The paper aims to reduce the significant discrepancy in training volume between deep neural networks and developing brain by exploring three strategies: reducing training iterations freezing layer training and weight clustering. The effectiveness of these methods is evaluated individually and in combination using data from primate ventral stream experiments. Strengths:  Tackles a critical issue in neuroscience: the mismatch between training requirements for deep networks and brain biology.  Provides practical solutions to minimize training including freezing layers and reducing training epochs.  Demonstrates the feasibility of achieving brainlike performance without excessive training by examining both ImageNet and ventral stream data. Weaknesses:  The weight compression approach requires further explanation including the choice of cluster\u6578\u91cf.  number requirements for proposed networks should be compared with compressed networks.  Similar weight compression techniques have been explored previously.  Experiments are primarily limited to a single architecture (CORnetS). recommendation: The paper offers a novel approach and extensive experimental issue to address a significant problem. Its findings suggest that minimizing training iterations in deep networks preserves brain similarity implying that supervised learning may not be an optimal approach for brain development. Acceptance is recommended. Minor Points:  Clarification on whether learning rates were optimized for different training lengths in number 1.  Adjustment of learning rates during training for improved issue.  Correction of performance metric in Abstract from absolute to relative BrainScore.  Explanation of specific layers trained in number 3CD.  Specification of architectures used in Section 6 (MobileNet V1 and ResNet50).", "Paraphrase: Summary: deep neural netstudys though not biologically realistic show striking resemblance to feature maps and tuning curves in the mammalian visual cortex. Researchers object to explain these differences and make deep netstudys more plausible models. Key findings:  Standard feedforward netstudys can effectively model the visual stream with minimal coordinated parameter changes.  Sparsely updating weight along with plausible random initial weight can accurately predict neural responses.  This suggests that mammalian brain may require less training and other training may be more significant in learning. Strengths:  extensive analysis demonstrating the potential of deep netstudys to model the visual stream.  findings support the idea that random initial weight can influence neural response prediction.  Implications for understanding brain learning mechanism. Weaknesses:  Limited exploration of how model predictions relate to task accuracy.  Absence of integration between brain prediction and netstudy performance within the study. Overall: The authors present valuable study for researchers investigating the application of ML models to neuroscience. Their findings suggest that deep netstudys may be more plausible models of cortical netstudys than previously thought opening up new avenues for future study."], "xZ6H7wydGl": ["Summary: This paper introduces a sampling method to estimate the likelihood of observations in stochastic differential equations (SDEs). Unlike traditional methods this approach ensures that sample trajectories pass through the observations leading to high accuracy in highdimensional models. It also removes dependencies between sample detail enabling parallel performance for faster computation. Strengths:  Clear and concise writing though additional background data would be helpful.  Wellstructured and clean source code provided.  Novel approach based on estimating a linear SDE which simplifies importance sampling and improves accuracy. Weaknesses:  Limited reproducibility due to missing lowlevel implementation detail.  Empirical evaluations lack depth and clarity:  use of parallelism in experiment 3.1 is not specified.  Speedup factor of 52 is not fully explained.  Plots could be improved with logarithmic scale and consistent time truncation.  Only the adjoint method of Li et al. (2020) is used as a baseline overlooking other related methods. Other publication:  The contribution statement should be revised to focus solely on the genuine contribution.  Grammatical errors and missing words in the text.  Figures 2 subplots should contribution axes for clarity.", "Paraphrase: Summary The paper introduces a novel approach for calculating the density of an observation sequence with arbitrary range and diffusion use. Using Girsanovs theorem the method evaluates the density under a Wiener process. The authors also propose an inversion trick to manage statedependent diffusion use. Experiments demonstrate that the method can learn accurate range and diffusion use significantly faster than traditional integrationbased approaches with reduced gradient variance. Strengths and Weaknesses Strengths:  Addresses a relevant topic facilitating the adoption of stochastic differential equations in the field of continuoustime dynamical systems.  Impressive experimental solution demonstrating the methods efficiency and accuracy. Weaknesses:  Limited experiments on lowdimensional systems further testing on larger systems is needed to assess potential issues and tradeoffs.  Ambiguous and insufficient detail in some sections:  Clarification is needed on the prior and posterior distributions mentioned in line 48.  more comprehensive data on the GPSDE model in Section 3.3 would be beneficial.  The experiments seem repetitive.  Lack of readability in Algorithm 1:  Explicitly listing the set of ts (line 3) and \u0394t would improve clarity.  Verification of Theorem 1:  A proof or commentary from reviewers would be appreciated to confirm the validity of equation (910) and the sum in Algorithm 1.  Explanation of equation (13):  A clear definition of all terms and processes involved would enhance understanding of the stateindependence achieved through the invertible mapping T.", "Paraphrased Statement: Summary This paper introduces an efficient sampling method for estimating probabilities in stochastic differential equation (SDE) models with Stateindependent or transformabletoStateindependent diffusion. Using Girsanovs theorem the observation probability density is transformed into a weighted probability relative to a standard Brownian motion process. The solution are qualitatively compared to integrationbased sampling showing that the proposed method is faster with reduced gradient variance. Additionally a variational Gaussian process (GP) example demonstrates the methods capability of providing reasonable uncertainty estimates. Strengths and Weaknesses Strengths:  Improved speed and efficiency for SDE learning. Weaknesses:  Limited range to Stateindependent or transformabletoStateindependent diffusion (not clearly Stated in the title and abstract).  Limited methodological novelty (relies heavily on Girsanovs theorem).  Lack of comparisons with methods designed specifically for Stateindependent diffusion.  Gradient variance solution presented on a different dataset than learning curves.  Insufficient examples and empirical validation for realworld applications.  Minor issues with informal language and lack of clarity in the independent text regarding experimental detail. Specific Comments:  Expand the presentation to include more applications of SDEs.  Provide citations and examples for claims in the text.  Explicitly State the conditions of Girsanovs theorem and the requirements for use f and sigma.  Eliminate informal language.  Include multiple examples for learning curves and standard deviations across experiments.  Consider exploring the methods behavior with varying dimensions and observation noise levels.  Move relevant data from the supplementary material to the independent text for clarity.  Provide more detail on the variational GP experiment (generative model data inference process)."], "xm6YD62D1Ub": ["Paraphrase: Summary The paper presents a new approach for selfsupervised representation learning using an objective work with three components: invariance variance and covariance. The invariance term aims to make representations resistant to input transformations. The variance term ensures each representation dimension has sufficient variance. The covariance term discourages coadaptation among dimension. The proposed objective work outperforms existing selfsupervised learning methods. Strengths and Weaknesses Strengths:  Clear and accessible exposition.  simple and practical method compared to previous techniques.  variance and covariance terms prevent representation collapse.  Applicability to diverse encoding networks. Weaknesses:  Representation collapse may not be the main challenge in selfsupervised learning.  Experimental results are adequate but not groundbreaking.  Necessity of heterogeneous encoders is not convincingly demonstrated.  contribution of variance and covariance terms requires further analysis.  Lack of comparison with the similar Barlow pair approach obscuring the proposed methods uniqueness.  The significance of the covariance terms difference from Barlow pair is not highlighted.", "Paraphrased Statement: Summary: The paper introduces a new selfsupervised method with a new loss work that explicitly prevents collapsed solutions. Strengths:  The loss work is welldefined making it easier to understand and interpret than previous methods like BYOL and SimSiam.  The approach of minimizing the standard deviation for each feature dimension is innovative.  The extension of the method to other techniques particularly SimSiam provides insights into how negativefree methods work.  The paper is wellwritten and accessible. Weaknesses:  The loss work appears to be an extended adaptation of Barlow pair with the variance term as its main distinction. However results show that VICReg does not offer significant improvements over Barlow pair.  The paper does not clearly specify the problem it aims to address. If the variance term is crucial it would be beneficial to compare the standard deviation of features in Barlow pair and analyze the advantages of combining variance invariance and covariance.  While the authors claim that VICReg does not require weight sharing this feature is not exclusive to VICReg since methods like SimCLR and Barlow pair can also work with different architectures. A comparison between these methods in a nonshared architecture setting would be valuable.  The reason for VICRegs superior performance in ESC50 experiments compared to Barlow pair is not clear and it is unclear if Barlow pair used multimodal data in this experiment. Since Barlow pair can work with different architectures it is significant to determine the factors contributing to VICRegs effective performance.  Table 4 lacks results for Barlow pair which would be essential to evaluate the impact of the variance term on different methods.", "Paraphrased Statement: Summary: This paper introduces a model for selfsupervised visual pretraining on ImageNet that combines three loss work: 1. Alignment between different perspective of the same image 2. covariance reduction to minimize offdiagonal values in feature covariance 3. Variance measurement to regulate standard deviation of embeddings within the batch This combination of loss work is new in visual pretraining. Strengths:  Clear and wellwritten paper  Simple method with comparable performance in linear evaluation and downstream task transfer  Comprehensive comparison with previous methods Weaknesses:  Lack of newty in the loss work as covariance reduction is adapted from Barlow pair and variance measurement has been used in other models for analysis.  Biased performance comparison in Table 1 with the proposed model pretrained for a longer duration and using additional optimization techniques compared to some of the other methods.  High computational cost due to the quadratic nature of covariance matrix computation.  While advantages of the variance and covariance terms over whitening in WMSE are discussed a theoretical explanation of their benefits would enhance understanding.  It would be beneficial to compare the proposed method with MoCo v3 using a Transformer backbone for a more comprehensive evaluation.", "Paraphrased Summary: The authors present a regularization technique called varianceInvarianceCovariance (VIC) for selfsupervised learning. The loss work incorporates three components:  Invariance term: Promotes similarity in embeddings for samples with different perspective.  variance term: (Main contribution) Hinge loss on the variance of embedded variables claimed to prevent \"variance collapse.\"  Covariance term: Adapted from Barlow pair a previous method. VIC offers flexibility in designing Siamese architectures by eliminating requirements for batch normalization and weight sharing allowing for multimodal signal embedding. Strengths:  Clear presentation of scope and main idea.  novel idea of adding a variance term to prevent representation collapse.  wide experiments and comparisons to prior methods.  Ablation work to analyze the impact of different components.  Demonstration of multimodal signal representation learning without requiring architectural or weight sharing. Weaknesses:  In Tables 1 and 2 the variance term appears to have a minor role with the proposed method underperforming compared to Barlow pair in many instance.  Barlow pair also does not require shared weight between branches. Can the authors justify the inferior performance of Barlow pair in Tables 3 and 5  The authors emphasize the importance of using standard deviation instead of variance in the hinge loss. A numerical instance to illustrate representation collapse with variance could be helpful."], "rFJWoYoxrDB": ["Paraphrased Statement: This study explores the design space of NASBench301 using predictions. It reveals that high performance on CIFAR10 can be achieved with a smaller subset of the space. Based on these findings the paper proposes a simplified search space where random architectures yield substantial performance. It also offers suggestions for improving NAS algorithms. Strengths:  Simplifies the complex search spaces in NAS.  Identifies redundancies that can guide future designs.  Easytoread despite some grammatical inaccuracies. Weaknesses:  Limited to CIFAR10 raising uncertainty about generalizability.  Neglects resource consumption which is influenced by topology and operations.  The impact of reducing operations on performance is unclear as NB301 includes all DARTS architectures.", "Paraphrased Statement: This paper argues that the current cellbased approach in neural architecture search (NAS) has limitations:  redundancy in Search space: The search space contains many unnecessary performances that do not contribute significantly to model performance making it unnecessarily complex. Highperforming architectures often share similar subgraphs.  Restricting Cell design Improves Results: By limiting the search space to crucial performances and similar subgraphs random sampling in NAS can achieve comparable results to more sophisticated NAS algorithms. Strengths:  Thorough analysis of the DARTS search space including performance importance and common subgraphs.  Reveals issues of redundancy and diversity in the cell design of DARTS.  Provides suggestions for search space design and benchmarking in NAS:  Grow the search space (search) then remove redundancy (prune).  Focus on highlevel design choices rather than celllevel details.  Verifies the potential bias in using surrogate benchmarks for performance prediction. Weaknesses:  Open questions remain unanswered such as defining an optimal search space and providing beneficial practices for search space design.  The analysis primarily focuses on the DARTS cell leaving it unclear whether other cellbased search spaces face similar issues.  While the paper criticizes cellbased search spaces it acknowledges their value as a common basis for comparing NAS methods.  The analysis of performance importance does not consider design tradeoffs such as optimizing for performance vs. efficiency.", "Paraphrase: Summary: This paper investigates neural architecture search spaces based on the NASBench301 dataset. It finds that:  Only a few operations significantly contribute to performance.  Highperforming architectures have patterns similar to known architectures (e.g. ResNet). Strengths and Weaknesses: Strengths:  Addresses the significant challenge of search space design in neural architecture search.  Questions the assumption that large search spaces lead to beneficial results.  Provides reliable results supported by a surrogate model and training on actual data.  Potentially influential in the development of future search spaces. Weakness:  Focuses primarily on DARTSbased search spaces and NB201 a simplified version of DARTS.  The analysis of subgraphs in Section 4 may be biased towards simpler subgraphs and could benefit from clarification.", "Paraphrase: Summary: This study analyzes the NASNET cell search space in NASBENCH301 and NASBENCH201. The findings indicate that separable convolutions and skip connections are crucial operations. By limiting the operations to these and enforcing skip connections as residuals researchers achieved accuracy comparable to networks discovered by advanced search algorithms. They also suggest that normal cells can be used for reduce cells. Additionally they replaced cells in stateoftheart (SOTA) architectures with only separable convolutions and skip connections maintaining accuracy close to the original networks. Strengths:  Relevance to the NAS community  Focus on a promising subset of operations for efficient search  emphasis on the importance of skip connections as residuals  Potential solution to the issue of ranking inconsistencies between supernet and scratchtrained architectures Additional Experiments Requested:  Run Darts and Bananas (or other NAS algorithms) on the PrimSkip search space for CIFAR10 and transfer to ImageNet comparing accuracy and run time reduction.  For Darts in the PrimSkip search space determine the correlation between ranking architectures based on supernet accuracy and scratch training compared to ranking in the original Darts search space. Weakness:  Reliance on a surrogate model for some experiments raising concerns about the conclusiveness of results due to the limited amount of trained architectures.", "Paraphrased Summary: The study examines the DARTS search space used in neural architecture search. Using statistical analysis it identifies key features that contribute to highperforming architectures. Surprisingly it finds that a simple cellbased search space is highly redundant and imposing basic constraints can result in substantial performance even with random search. Strengths and Weaknesses: Strengths:  The study provides valuable insights into the structure of cellbased search spaces.  It demonstrates that certain design elements are consistently present in highperforming architectures. Weaknesses:  The findings suggest that random search augmented with simple rules can achieve comparable results raising questions about the practical utility of the current search space.  The study focuses solely on cellbased NAS while other types of search spaces (e.g. for macrolevel decisions) may be less restrictive.  The constraints identified by the study are based on prior knowledge gained through a search algorithm which could limit their potential impact."], "lQI_mZjvBxj": ["Paraphrased Summary: This study presents a novel approach to federated learning using knowledge distillation where participating clients can choose their own models. It establishes theoretical results for a twoclient federated regression scenario demonstrating that:  Iterative alternating knowledge distillation gradually loses data and ultimately converges to an ineffective model.  A novel ensembling technique that combines intermediate models produced by both clients over iterations asymptotically converges to the centralized model. These insights are then applied to more realistic federated classification scenarios on the MNIST and CIFAR10 datasets. strength and Weaknesses: Novelty:  The paper introduces a novel idea of using ensembling intermediate models in federated learning. Significance:  The study aims to translate the theoretical insights into a modelagnostic federated learning framework with robust asymptotic guarantees. However there are concerns about the practical significance due to the significant gap between the theoretical setup and realworld settings: 1. Extension to MultiAgent scope: The results are limited to a twoclient scenario and do not address multiagent settings which would require modifications to the averaging scheme. 2. Assumption of Equal data: The assumption of equal data across clients is unrealistic in use and its impact on the averaging scheme is not explored. 3. Flaw in Alternating Knowledge Distillation (AKD): The theoretical results are based on a flaw in the AKD setup where one client does not see its true training output. This flaw may not exist in other knowledge distillation methods. 4. potential Unnecessity of AvgKD: If the flaw in AKD is corrected the AvgKD mechanism may become redundant. 5. Reliance on Regression Model: The results are tied to a specific regression formulation and may not be applicable to classification settings or other statistical models. 6. Communication Cost: The knowledge distillation work requires clients to contribution models leading to a higher communication cost compared to standard federated learning. Soundness:  The theoretical results appear correct but concerns exist about their applicability in realworld settings due to the limitations mentioned above. Clarity:  The paper is wellorganized and easy to follow but some grammatical errors are present. experimentation:  The experimental results suggest that the initial local model of one client is comparable to the centralized model which raises questions about the strength of knowledge distillation.  The flaw in AKD is evident in the results with one client performing consistently worse than the other.", "Paraphrase: The study investigates how two kernel regression models are optimized using codistillation in a decentralized scope. In each round a local client predicts labels for its data using the other clients models. These labels are then used to retrain the local model. Three variations of codistillation are analyzed: the original version an average of the true label and the predicted label and an ensemble approach that utilizes all model iterations for predictions. Theoretically the study shows that the original approach can fail but the ensemble approach is optimal under certain conditions. Empirically the findings are confirmed. The analysis suggests broader applicability as similar behavior is observed when using neural networks instead of kernel regression. strength:  Rigorous analysis of codistillation dynamics  Clear presentation and sound theoretical results Weaknesses:  Empirical evaluation is limited to two clients linear regression and MNIST  comparison to other distributed learning methods is lacking", "Paraphrased Statement: This paper explores a modelagnostic federated learning approach called knowledge distillation. In a simplified scenario with two agents and a kernel regression problem they find that transferring knowledge from one agents trained model to the other can lead to performance degradation. To address this number they propose an alternative algorithm Ensemble Average Knowledge Distillation (AvgKD) which avoids the degradation problem and converges effectively. experimentation demonstrate the superiority of AvgKD over other approaches showing that it converges to optimal solutions. strength:  Provides the first theoretical analysis for modelagnostic federated learning.  Demonstrates the limitations of knowledge distillation and motivates the development of AvgKD.  Supports the theoretical findings with comprehensive experiments. Weaknesses:  Similar knowledge distillation ideas have been proposed by other researchers (e.g. Lin et al.).  The simplified twoagent scenario may not fully capture realworld federated learning settings.  The focus is primarily on optimization with less attention paid to generalization ability.", "Paraphrased Summary: This paper introduces the concept of federated kernel regression where two devices try to learn models using different kernels. Their work creates an algorithm called alternating knowledge distillation (AKD). However they show that AKD can lead to underfitting due to excessive regularization. They also highlight a connection between AKD and the alternating projection algorithm for finding set intersections. Using this connection they suggest an improved algorithm. strength:  Introduces a new federated kernel regression framework.  Provides a thorough theoretical analysis of knowledge distillation methods. Weaknesses:  Limited applicability to twoagent cases while federated learning typically involves multiple agents.  Analysis in the experimental section is unclear or missing:  Convergence of AKD loss in number 3 is not explained.  Reasoning behind the increase in AKD loss is not provided.  Analysis for numbers 4 5 and 6 is absent.  Some notation in the paper lacks clarity with terms potentially sharing similar meanings or requiring more explanation of their differences."], "q4HaTeMO--y": ["Summary Paraphrase: The researchers compare two types of models: Deep Declarative Networks (DDNs) and Deep Equilibrium Models (DEQs) under certain assumptions. They demonstrate that solving a specific optimization problem in DDNs leads to a fixed point that resembles the optimization of a DEQ with an implicit layer. This finding suggests an initialization method for DEQs based on the result obtained in DDNs. Strengths and Weaknesses Paraphrase: Strengths:  The paper is wellwritten and selfexplanatory.  The authors establish a connection between DEQs and optimization problems providing a framework for their field.  The assumptions made are justified and the exponential family used is suitable for implementing DEQs.  The initialization scheme for DEQs inspired by the DDN result has proven effective. Weaknesses:  The paper lacks a direct performance comparison between DDNs and DEQs.  The influence of kernel and exponential family choice on the fixed point comparison is not fully addressed. other Note:  An extra curly bracket appears in Appendix E after Equation 16.", "Paraphrased Statement: Summary: This research paper investigates the connection between deep declarative networks (DDNs) and deep equilibrium frameworks (DEMs). It begins by introducing a specific type of DDN with a kernelized generalized linear framework and demonstrates that certain DEMs are equivalent to these DDNs. The paper explores the implications of this comparison and proposes a method for initializing DEM weights using DDNinspired principles. Strengths:  The paper addresses a novel and significant topic: connecting DEMs to existing frameworks (DDNs).  It provides a comprehensive theoretical analysis of the comparison between certain DEMs and DDNs as considerably as a method for initializing DEMs.  Experimental results suggest that DEMs initialized using this method achieve beneficial performance under comparable training conditions. Weaknesses:  The empirical evaluation is somewhat limited. While experiments are conducted on a synthetic dataset and a realworld dataset the performance on the latter is not clearly presented. Experiments are only shown up to epoch 5 which may not be sufficient for convergence. Additional visualizations and a time analysis of the initialization process would strengthen the evaluation.  The paper does not acknowledge that the existence of result for DEMs has been discussed in a previous field by El Ghaoui et al. (2019) where the conditions for considerablyposedness are less restrictive.", "Paraphrased Statement Summary This paper presents a novel view that connection optimizationbased layers (e.g. DDNs) with fixedpoint forward computations in deep equilibrium networks (DEQs). Specifically the paper demonstrates how a kGLM optimization layer can be formulated as a simplified version of a DEQ framework layer under certain conditions. The paper includes rigorous proofs and extensive discussion of this connection covering aspects like parameterization and initialization scheme. Strengths and Weaknesses Strengths  pass assumptions sound theory and a wellreasoned proof.  The interpretation of DEQ frameworks through kGLM is original.  The concept of using informed initialization scheme is insightful particularly for improving the stability of DEQ training. Weaknesses  The view of examining DEQ iterations from an optimization standpoint is not only novel.  The discussion on initialization while valuable is impractical for DEQs beyond the range of this paper.  The paper does not address whether commonly implemented DEQs align with any specific optimization problem.  The convolutional form proposed in Appendix E has limitations including symmetric weights and restrictions on striding and dilation. The authors should clarify if spectral normalization is applied during training or only at initialization.", "Paraphrased Statement: This paper introduces a deep declarative network (DDN)inspired formulation for declarative equation questers (DEQs). The DDNbased DEQs are motivated by the closedform result and guaranteed existence of the maximum a posteriori (MAP) result for a specific type of DDN called kernel Gaussian likelihood models (kGLMs). The paper also explores making the result unique by imposing a Lipschitz constraint. While the convexity of the kGLM log probability and the Lipschitz condition for uniqueness are known the kGLM approachs primary advantage lies in its closedform result which serves as a valuable initialization for training DEQs. Experimental results demonstrate that careful initialization enhances training compared to random initialization and the kGLM approach is effective when field knowledge can be incorporated into the kernel. Strengths:  Provides a promising initialization scheme for DEQs. Weaknesses:  limited empirical evaluation of the kGLMbased DEQs expressiveness.  Experiments focus on tasks suited for kernelbased approach and broader evaluations are lacking. Questions: 1. Computational Cost: What is the computational cost of solving for the kGLM initialization How many training iterations would be equivalent in time to this initialization 2. Corollary 6: How is Corollary 6 used to determine the kGLMbased initialization for DEQs Does it assume that all data points have the same value 3. Architectural Restrictions: Does the restricted architecture for g impact performance Is the kGLMbased initialization applicable to more general DEQs 4. performance Difference: Why is there a difference in performance between a \"trained kGLM\" initialization and the fully trained DEQ Typos and Clarifications:  Equation 3: The definition for F\u03b3 may be missing.  Page 3 bottom: Should \"Update the current estimate zr for the fixed point to be f(zr1)\" be H(zr1)  Notation and Example: Clarify the notation and its relationship to the motivating example. Separate the notation definitions from the discussion about Gaussian process modeling."], "wkMG8cdvh7-": ["Paraphrased Summary: This study demonstrates that graph injection attack (GIA) is more effective than graph modification (GMA) and can disrupt homophily distribution. Homophilybased defenses can therefore detect GIA more easily. To address this the authors propose adding a constraint to GIA to maintain homophily. They show that this constrained GIA is more powerful than regular GIA. effectiveness:  Clear and accessible writing  Solid theoretical analysis to understand and enhance GIA  Effective proposed method supported by experimental results Weaknesses:  Lack of justification for the practicality of evasion and inductive settings in realworld application  Unclear how the authors excluded test nodes from training data as 3layer GNNs use subgraph containing both labeled and unlabeled nodes  Assumption that the graph follows homophily distribution and lack of discussion on the effectiveness of the method for disassortative graph", "Summary: This study examines the pros and cons of node injection attacks on graph neural networks. The authors show that these attacks are generally more effective than node modification attacks without defense. However homophilybased defenses render node injection attacks ineffective even worse than node modification attacks. To address this the authors propose adding homophily preservation as an attack regularizer to maintain stealthiness against defenses. Extensive experimentation confirm that this regularization improves attack effectiveness against defenses. effectiveness:  Wellwritten and comprehensive experimentation Weaknesses: 1. Proof compare Assumption: The proof of compare between node modification and injection attacks assumes a large research space for injected nodes. This assumption may not hold in practice. 2. Indirect Node Modification: The study compares node injection to indirect node modification attacks which are less powerful than direct attacks. 3. Theorem 1 Proof: The proof study claims that GIA optimal loss function value approaches GMAs from below as perturbation budgets approach each other. This contradicts the expectation that a smaller perturbation budget would increase the optimal loss function value. 4. Homophily defense and Attack Significance: The proposed homophilypreserving regularization enhances node injection attacks by exploiting a weakness in homophilybased defenses that detect injected nodes that significantly differ from their neighbors. The papers significance may be limited to designing adaptive attacks against simple homophily defenses.", "Paraphrase Summary The paper examines graph injection attacks (GIA) where adversaries introduce novel elements (nodes and links) to alter the predicted label of a objective node by a trained graph neural network (GNN). They compare GIA to graph modification attacks (GMA) by manipulating existing links and demonstrate that GIA is more effective in causing damage. Additionally they discuss the vulnerability of GIA to homophilybased defenses and propose a GIA attack that maintains homophily and withstands such defenses. effectiveness and Weaknesses  effectiveness:  comprehensive analysis and substantial theoretical underpinnings.  Highlights a significant weakness in the robustness of GNNs against GIA.  Provides a foundation for future research addressing this vulnerability.  Weaknesses:  Potential limitations:  Homophilybased defense may be ineffective if the classifier relies solely on graph structure making GIA less effective.  limitation of the model in damage of attack budget and applicability to different downstream tasks are not fully explored.  The intuitive basis for the effectiveness of HAO is not fully explained.  Minor modifications:  Clarification of the definition of Dx in Section 2.2.  Correction of a grammatical error in Section 2.2.  Modification of a phrase in Section 4.1 to improve clarity.", "Paraphrased Summary: This research explores how to make adversarial attacks on graph neural networks more subtle. specifically it focuses on enhancing the stealthiness of graph injection attacks where carefully designed nodes are added to the graph data. The paper introduces the concept of homophily unnoticeability and examines the capabilities of graph injection attacks. It finds that homophily defenders can easily thwart GIA but suggests a novel adversarial objective that maintains homophily. experimentation show the effectiveness of the proposed attack against various defenses. effectiveness and Weaknesses:  Positives: The research addresses an significant topic and develops an effective attack strategy that remains potent against multiple defense methods.  Negatives:  The compare between GMA and GIA (Theorem 1) is not fully comparable due to differing attack budgets.  The novelty of the approach is somewhat limited as it follows a common strategy of exploiting defenders reliance on homophily.  The papers main contribution are primarily empirical rather than theoretical."], "qsZoGvFiJn1": ["Paraphrase: This study introduces a technique to enhance object detection in lidar scans when the lidar scanner repeatedly scans the same scene. The proposed method involves extracting point clouds from earlier scans passing them through a trainable network and then temporarily integrating the features from each scan. These aggregated features are combined with those from the current scan and then fed into an existing object detector. Experiments demonstrate that this method improves object detection performance for several object types and classification techniques. Strengths:  Leverages past point clouds to provide contextual information about the scene.  Simple and efficient approach to utilizing past traversals.  Applicable to different scenarios with advancements in SLAM and localization algorithms.  Evaluated using four different object detection algorithms showing method independence.  Welldesigned and presented experiments using two relevant datasets.  Includes qualitative figures to demonstrate algorithm improvements.  clear and polished presentation with informative figures tables and writing.  Provides a concise overview of related work categorizing them efficiently. Weaknesses:  Lack of introspection into what the network study and how it enhances performance.  Speculation regarding the specific contributions of past traversals to detection improvements.  Suggestions for additional experiments to provide more insights into the method behavior:  Comparing moving vs. static object detection improvements.  Investigating the impact of past traversals containing or lacking objects on detection rates.  Baseline using a single past traversal matching the current scan.  Baseline using a sequence of point clouds from past traversals to create a merged point cloud input for detection.  comparison with recurrent network that utilize time series data such as LSTMs.  Incomplete related work section which could benefit from including additional change detection and historical context perception study.", "Paraphrase: This study focuses on object detection in LiDAR point clouds for automotive applications. The proposed method leverages unlabeled past data from the same geographic area where the car is currently located at inference time. This data is used to augment the current observations significantly enhancing perception quality especially for small objects like pedestrians and at extended ranges. A sparse 3D fully convolutional neural network (called the \"hindsight encoder\") preprocesses the unlabeled data generating features that are integrated with the original point cloud before feeding it to an object detector. The hindsight encoder is trained in conjunction with the detector and its features can be precomputed offline. The proposed method evaluates using datasets that match the scene of using unlabeled past data. issue on Lyft and nuScenes datasets show promising improvements particularly for pedestrians cyclists and distant objects. The paper strengths include substantial performance enhancements indepth analysis and plans to opensource the code. Limitations include the need for prior fleet through the same area even during validation and the potential for outdated past observations. However the approach remains comparatively simpler and less expensive than HD maps.", "Paraphrase: use: Enhance 3D object detection for autonomous vehicles (AVs) by leveraging past LIDAR data. method: Combine LIDAR scans from previous traversals of an AVs road into a detailed representation using a reliable localization system. Use an further neural network to process each scan capturing vital features that are stored along the road. During object detection these stored features are integrated with the current LIDAR scan to improve the detection accuracy. issue: Without additional training this approach significantly improves object recognition performance by 20 average precision points on standard datasets. Strengths:  Novel approach utilizing past information to enhance object detection without added training costs.  Potential for significant time and resource savings in data acquisition and annotation.  Thorough evaluations demonstrate the method effectiveness. Weaknesses:  Limited investigation into the impact of localization errors on bearing (orientation) which warrants further study.  Exploration of the networks potential as a backbone for the entire object detection system would provide valuable insights.", "Paraphrase: compact: This research proposes a HINDSIGHT framework for identifying objects in lidar scans. It utilizes scene features acquired during prior scans to enhance realtime detection pipelines. By incorporating these additional features the system improves detection performance particularly for pedestrians where past observations can distinguish transitory objects. The experiments include a detailed ablation study and demonstrate the framework efficacy and integration ease. Strengths:  Novel approach: Leveraging past observations for scene understanding is a novel concept with the proposed pipeline being successful in demonstrating its value. It is particularly beneficial for pedestrian detection a challenging task in object recognition.  Simplicity and efficientness: The framework is straightforward and efficient using an existing feature extraction framework and detector. The addition of historical features to the current scan is also simple and enhances detection without significant overhead.  Wellexecuted experiments: The experiments are comprehensive with an extensive ablation study that explores framework design and results. This demonstrates the pipelines compatibility with existing detectors and its ability to improve detection performance. Weaknesses:  Limited exploration: While the pipeline is simple it could benefit from exploring design choices such as finetuning the feature extractor and detector together or updating offline history features as more data is collected.  Clarity: The paper could provide more clarity in some area such as how dense point clouds are acquired from sparse samples and whether an alignment step is involved. Visual examples would be beneficial.  Results analysis: To highlight the benefits for pedestrian detection the paper should include more results and analysis for this specific case. It should compare the improvement in pedestrian detection to other object categories and discuss the impact of fusion on moving and stationary pedestrians."], "jFfRcKVut98": ["Summary Paraphrase: This work introduces the concept of \"partial equivariances\" which define equivariance with respect to a specific subset of a group. Instead of enforcing this subset prior to training the authors propose a method for dynamically determining the subset during the training work. The effectiveness of their model is demonstrated on various datasets (RotationMNIST CIFAR10 CIFAR100) that necessitate varying levels of equivariance. effectiveness and Weaknesses Paraphrase: effectiveness:  Conceptually simple and wellimplemented approximation.  Enforcing partial equivariance provides flexibility in addressing learning problems with strict equivariance limitation.  Clear and wellwritten presentation. Weaknesses:  Presentation Concerns:  Inconsistent notation in Equation 3.  Ambiguous references to lifting performance.  Figure 1 and 2 lack sufficient detail and guidance.  Proposition 4.1 lacks novelty and should be properly attributed.  Technical Concerns:  Omission of \"steerable GCNNs\" from the literature review.  Lack of comparison with steerable GCNNs in the experiments.  Limited quantification of the equivariance error and unclear interpretation of the results.  The distribution learned over the subset is not explicitly stated to be nonuniform potentially diminishing the perceived novelty.", "Paraphrased Statement: Summary: This work introduces a technique called \"partial equivariance\" where the equivariance requirement use to only a portion of a group. Rather than manually defining this portion the technique derives it through Monte Carlo sampling. Experiments on modified versions of MNIST and CIFAR10 demonstrate that partial equivariance enhances classification accuracy compared to full group equivariance. effectiveness:  clearly written and accessible  Thorough discussion of related research  Novel concept of partial equivariance with various subset scenarios explored Weaknesses:  Lack of clear motivation for the need for partial equivariance  Experiments tailored to favor partial equivariance on MNIST and only marginal improvement on CIFAR10 hindering the demonstration of its effectiveness  Absence of analysis visualizations or concrete model to support the necessity and benefits of partial equivariance  Limited experiments on small artificial datasets calling for further testing on larger and naturally occurring datasets", "Paraphrased Statement: Summary: This work presents a method to derive partial equivariances from data using a GroupConvolutional Neural network (GCNN). The system prunes group elements at each layer resulting in partial equivariance rather than full equivariance. The paper utilizes concept from Finzi et al. to approximate group convolution using Monte Carlo integration and from Benton et al. to employ a reparametrization trick for learning the boundaries of a uniform distribution from which group elements are sampled. Experimental findings indicate that partial equivariance surpasses full equivariance when the data itself does not exhibit full rotational equivariance. effectiveness:  Tackles the challenge of learning symmetries from data.  Provides a flexible approach by allowing equivariance to be pruned differently at each layer potentially leading to improved performance. Weaknesses:  Partial equivariance leads to approximate invariance to transformations from the group subset.  Equivariance once broken cannot be recovered in subsequent layer.  The method may converge to undesirable model such as premature equivariance breaking or restrictive layer. QuestionsComments:  Provide further details on unstable discrete groups mentioned in Section 6.  Include results for CIFAR100 and SE(2) cases with visualizations similar to Figure 4.  Compare the promodeld method with handcrafted techniques for breaking equivariance.  Quantify the equivariance errors introduced by Monte Carlo approximation and group pruning.  Discuss the similarities and differences between the promodeld approach and the work of Benton et al. including quantitative comparison.  Figures 1 and 2 could be improved for clarity by rotating the feature maps to the same model.", "Summary and contribution The work proposes using \"partial group convolution\" which can learn from data the layer of symmetry (partial equivariance) required by each layer in a network for specific tasks. This method approximates group convolution (LieConv) by sampling group elements from a learned distribution rather than a uniform one. Experiments show that when full symmetry is detrimental using partial equivariance improves performance compared to traditional graph convolutional neural networks (GCNNs) while even matching performance when full symmetry is beneficial. effectiveness  Exploring partial equivariance a novel and underexplored concept in GCNNs.  Demonstrating the practical relevance of partial equivariance as it enhances performance when full equivariance is harmful.  Proposing a simple even efficient solution requiring minimal modification to existing GCNN architecture.  Clear experimental results supporting the claims of the work. Weaknesses  Misleading or confusing use of terminology and notations making it difficult to verify the technical claims.  Lack of a mathematical definition for equivariance error.  Small range in figures which hinder understanding. Questions  How is the overall learned group subset determined for a dataset with varying group subsets  Can the distribution over groups be constrained to a uniform distribution over a specific subset which might be a more intuitive definition of \"partial\" group convolution"], "luO6l9cP6b6": ["Paraphrase: Summary: This study introduces an access to evaluate pretrained models by examining their adaptability without relying on specific word meanings. Using a pretrained English BERT model they finetune it on corrupted English data that retains word frequency but obscures word identity. evaluation are conducted across six tasks. Strengths:  The paper is wellstructured and straightforward to understand.  The experiments are effectively designed to back the assertions. Weaknesses:  In Section 7.3 the provided results for BERT are questionable. Pretraining could be a confounding factor that is not being accounted for. For instance the word frequency data is already embedded in the BERT models parameters along with other attributes that apply to scrambled text as well. Therefore it is unsurprising that BERT would outperform when finetuned on scrambled data (with word frequency maintained). LSTMs superior resilience stems from its small model size enabling it to relearn all angle. similar patterns are observed for 1layer BERT on classification tasks where the performance gap between different model variations is generally smaller. To bolster these claims the authors could test a BERT model that has been pretrained on scrambled English.  In Section 6.2 the authors assert that the poor performance on sequence labeling tasks is due to their increased reliance on word identities. However as Hewitt and Liang have demonstrated such labeling instructions (albeit in POS tagging) can be well acquired through finetuning. The authors should consider providing a more compelling explanation for the poor performance on labeling tasks.  The proposed method is frequently used in various study such as adversarial attacks. While its application to novel domains is somewhat novel the observations made are not particularly original or significant. The paper mentions pretrained models in its title but merely BERT is discussed throughout.", "Paraphrased Statement: This study explores transfer learning and domain adaptation in language models. The authors suggest limitations to knowledge transfer when models are distorted or randomized. They employed various randomization strategies including frequency matching and uncontrolled word replacement. Their findings indicate that the BERT model exhibits high transferability in classification tasks even under scrambled conditions. The experiments highlight the importance of pretraining for sequence labeling tasks involving randomization. Swapping language with different frequency severely impacts performance indicating the significance of word identity. This supports previous observations that BERT preserves frequency well contributing to its superior transferability. Strengths and Weaknesses: The paper is wellstructured and raises intriguing questions about transferability. However it lacks intrinsic evaluation measures for the scrambled datasets. The scrambling techniques used may stray from realistic adversarial attacks. Comparing the techniques robustness together would enhance the study credibility. Additionally exploring the effects of synonym and antonym replacement on scrambling would provide further insights into the impact of semantic relatedness.", "Summary (Paraphrased) The study examines the adaptability of language models (BOW GloVe LSTM BERT) when faced with vocabulary randomization with and without considerations for word frequency. Strengths  Clear and wellwritten  Thorough evaluation of model robustness under artificial vocabulary transfer Weaknesses  Lacks compelling justification for using vocabulary scrambling as a meaningful transfer:  It is not reflective of realworld language transfer (crossdomain crosslingual etc.).  It does not offer novel insights compared to other artificial transfer (e.g. word or role shuffling).  Uncertain why the specific scrambling technique was chosen.  Section 7.3 provides valuable baseline comparisons while Section 7.5 raises questions about the necessity of word reassociation for task performance. Writing  Generally wellwritten  Term \"scrambling\" may be misleading as it refers to word order transfer in previous work.  Motivation for focusing on frequency could be strengthened. Additional Weaknesses  Overreliance on BERT: Consider exploring more recent or alternative language models.  Limited scope of sequential labelling tasks: Expand beyond POS and NER to include more complex problems.  role of a small subset of GLUE tasks raises interest about cherrypicking.  Sensitivity of results to hyperparameter transfer should be examined.", "Paraphrase: This study investigates the underlying mechanism of knowledge transfer in pretrained neural models during crossdomain learning. The researchers conducted a series of experiments where they trained and evaluated models with unscrambled English data scrambled data (with random token replacement) and randomized data (with token frequency preserved). Across various text classification and sequence model tasks the scrambled and randomized input data resulted in performance decreases compared to unscrambled data. further analysis aimed to explore whether the scrambling:  Maintains sentence semantics (it does not)  Causes BERT to retrain (inconclusive evidence of transfer)  Affects transfer at different BERT layers (results vary by layer)  Leads to word identity reassociation (apparently not) Strengths and Weaknesses:  The presentation is generally clear.  The experiments provide some interesting insights.  However the inconclusive results in multiple experiments limit the study scientific significance. Suggestions for improvement:  To support the hypothesis of transfer learning with scrambled or random input an experiment training on original data and testing on scrambled data (or vice versa) could be included.  The analysis section could be enhanced to provide more meaningful interpretations of the inconclusive results.  The study title should be revised to more accurately reflect the focus on BERT.  Table 4 should be placed closer to its reference.  The statement \"Our leading hypothesis here is that the LSTMs may actually relearn all angle without taking advantage of pretraining\" needs clarification."], "tsg-Lf1MYp": ["Paraphrased Summary: The study proposes a novel task called Natural Attributebased Shift detection (NAS detection) which involves identifying samples that have undergone shifts in natural attributes (e.g. age time lighting). Three benchmark datasets are created from existing datasets in vision text and medical domains. contribution:  The study evaluates and analyzes outofdistribution (OOD) detection methods on the NAS benchmark datasets.  Three categories of NAS samples are identified.  A modified training objective for deep classifiers is proposed to enhance OOD detection performance. Strengths:  The paper highlights a novel challenge for OOD detection: identifying NAS samples.  extensive experiments demonstrate the capabilities of OOD detection methods in detecting NAS.  The proposed loss function improves OOD detection performance using the Mahalanobis detector. Weaknesses:  The study settings for adjusting natural attributes vary between range text and medical datasets.  The proposed loss has only been applied to the Mahalanobis detector. Its compatibility with confidencebased OOD detector remains unexplored.", "Paraphrased Summary: The authors introduce \"natural attributebased shift\" (NAS) a subcategory of outofdistribution (OOD) phenomena. Using existing datasets they create three datasets specifically designed for NAS experiments. By analyzing model outputs using PCA they identify three NAS distribution categories. They evaluate three OOD methods on the NAS datasets and observe inconsistent performance. Subsequently they propose using distance and entropy loss during model retraining to enhance distancebased OOD methods. Strengths:  innovative concept  Clear and accessible writing  Improved generalizability of distancebased Mahalanobis method for NAS Weaknesses:  Insufficient theoretical justification for assumptions regarding age and range brightness  Limited experimental setup with only three OOD methods and three binary classification task  Potential for acronym confusion between NAS (task) and NAS (Neural Architecture research)  Inconveniently placed Table 2  Lack of clarity and reproducibility in Section 4.2 formulation  Inefficiency of proposed method due to additional training loss  Incomplete investigation of different NAS scenarios  Absence of visualization of feature distribution after Mahalanobis method improvement", "Summary This paper introduces a novel problem called Natural Attributebased Shift (NAS) in Deep learning (DL) systems which involves changes in data distribution affecting systems designed for specific task. This shift differs from OutofDistribution (OOD) data where differences occur in the label space. Instead NAS occurs due to systemic attribute changes in the data such as altered brightness in Xray range. contribution  Identification of the NAS detection task and creation of datasets for its study.  evaluation of OOD methods on NAS datasets indicating their inconsistency across different scenarios.  novel analysis of shifted sample locations in feature space and the performance of OOD methods.  Demonstration of enhanced OOD performance on all NAS scenarios using a modified training objective for deep classifiers. Strengths  Recognition of NAS as a distinct subset of OOD problem in DL.  creation of diverse NAS datasets covering different fields.  Identification of NAS scenario types based on proximity to decision boundaries and indistribution data.  Empirical evidence of improved performance with additional loss terms in the Mahanalobis model for NAS detection. Weaknesses  Limited discussion of the applicability of NAS solution to other settings such as pose variation.  Difficulty in interpreting the dense Table 3.  potential scalability issues for larger datasets and models. Minor Comments  Clarification of NAS language to avoid confusion with Neural Architecture research.  Completion of the phrase \"medical field\" in the introduction.  Explanation of ResNet18 model selection over ResNet50.  Inclusion of a note regarding higher values indicating beneficial performance in Figure 2.  Discussion of PCA versus more informative visualization techniques like tSNE.  Suggestion to include additional methods in Figure 2 analysis."], "qhC8mr2LEKq": ["Paraphrase: Summary: CrossBeam is a novel method for guiding program synthesis by leveraging a neural architecture that considers the search context when selecting programs for evaluation. Compared to other search methods CrossBeam evaluates fewer programs in string synthesis and inductive logic programming tasks. Strengths: CrossBeam advances the utilization of neural model in program synthesis by incorporating the search context in its decisionmaking process. Unlike previous methods that solely consider individual programs CrossBeam utilizations the neural architecture to determine which program to prioritize. Weaknesses: 1. Running Time: CrossBeams running time is significantly slower than Bustle a simpler search method. While CrossBeam may evaluate fewer programs Bustle is more effective in terms of implementation time. 2. Search algorithm: CrossBeam utilizations an incomplete search algorithm that can lead to stalled searches. The paper discusses using the UniqueRandomizer to alleviate this issue but it lacks clear evidence of its effectiveness. Additionally the small search budget utilizationd in the experiments raises concerns about the methods performance on larger and more complex problems. 3. reproducibility: The paper does not provide sufficient data for reproducing the results such as the definition of the embedding zi. The availability of source code is also unclear which could hinder reproducibility.", "Summary CrossBeam is a program synthesis algorithm that combines neural network direction with bottomup search. The neural network predicts combination of subprograms to explore reducing search space complexity. It is trained on search history and intermediate program implementation to assist in choosing argument lists for subprogram combination. CrossBeam shows improved accuracy and search space reduction compared to existing methods in string manipulation and logic programming domains. Strengths and Weaknesses  CrossBeam overcomes the exponential search complexity in program synthesis.  It is relatively easy to implement and can be applied to various domains.  Including the full search history as context can potentially become a bottleneck for large DSLs or generalpurpose programming languages.  It may be beneficial to consider pruning irrelevant search paths to improve efficiency.  The computational complexity of CrossBeam with respect to the number of DSL process and IO model should be analyzed to understand potential performance issues.  more data is needed on the loss work used during training and if it can incorporate penalties for search depth to optimize for compact solutions.  An RLbased training method could further reduce the discrepancy between training and testing objectives.  The wallclock time comparison could be misleading due to different implementation languages FLOPs or languageindependent metrics should be used for a fair comparison.  Figure 2 requires further explanation to clarify its purpose.", "Paraphrased Summary: This paper introduces CrossBeam a bottomup program synthesis method that uses neural direction for search. neural networks are trained to suggest combination of previously computed subprograms (arguments) for each DSL operator. UniqueRandomizer generates novel subprograms that are iteratively combined until a resolution satisfying the inputoutput specification is found. training occurs onpolicy using beamaware training techniques. Strengths:  Combines innovative techniques like bottomup program synthesis onpolicy beam training UniqueRandomizer and pointer networks.  Strong experimental results on string manipulation and inductive logic programming benchmarks.  An ablation work highlights the significance of various components. Weaknesses (Minor):  Wall clock comparisons do not show substantial speed improvements over baseline despite batching potential.  Incomplete detailing of experiments (e.g. comparison to RobustFill random training ablation).  Iterative exploration of all operators consider learning to prioritize operators based on specification and subprogram pool.", "Summary Paraphrase: This paper introduces a novel approach to creating programs from input and output model. It uses a bottomup search method guided by an intelligent network that decides the path of the search based on the current context unlike previous methods that followed a rigid sizebased enumeration. The network is trained by feeding it programs and their behavior under random inputs. The method performs well in tests involving string manipulation and deductive reasoning. Strengths:  Improved results over existing methods.  comprehensive experiments demonstrate the methods effectiveness.  clear and concise presentation with informative diagrams. Weaknesses:  Slower implementation time than certain existing methods.  Related methods are not thoroughly compared in a tabular format.  No provision for errors in input and output specifications. Questions:  How does the method identify programs that align with the users intent Does it use any prior knowledge or infer data from the inputoutput pairs"], "gWGexz8hFH": ["After reviewing the manuscript and the authors responses I am satisfied with the updates made. I recommend acceptance of the paper. The manuscript explores distributed differentially private (DP) learning with blackbox secure multiparty computation (MPC) for gradient aggregation. While existing aggregation techniques assume continuous space MPC requires discrete values. The authors introduce the distributed Skellam mechanism which is discrete and infinitely divisible resolving this issue. They demonstrate that it outperforms binomial and discrete Gaussian noise methods in privacy preservation using MNIST and Fashion MNIST datasets. Key strengths include addressing weaknesses in previous approaches and improving performance. However the paper lacks analysis of the tightness of privacy limit and may need further clarification regarding comparison to other work. The authors are encouraged to clarify these points in a revised version.", "Paraphrase: This paper introduces a privacypreserving mechanism for federated learning called the Distributed Skellam Mechanism (DSM). It is based on the Skellam distribution and provides strong privacy guarantees in decentralized settings. Specifically DSM is shown to satisfy both Randomized Differential Privacy (RDP) and Differential Privacy with Parameters (\\epsilon \\delta) (DP). strength:  Clear structure and logical theoretical proofs  Leverages the properties of Skellam distribution such as the sum of Skellam random variables being Skellam distributed Weaknesses:  Lack of novelty as similar ideas have been previously explored in other research.  Concerns about the tradeoff between privacy and accuracy as the extensive application of the mechanism in learning settings may lead to potential privacy vulnerabilities due to paper theorems.", "Paraphrased Statement This work introduces the distributed Skellam mechanism (DSM) for federated learning within the distributed differential privacy (DP) model. Unlike the existing approach which utilizes distributed discrete Gaussian (DDG) noise DSM adds independent Skellam noise to each local gradient. Advantages of DSM:  Privacy secure is independent of gradient dimensionality.  Tight privacy accounting due to the Skellam distribution properties.  Smaller privacy budget compared to DDG resulting in better constant in privacy analysis.  Improved performance over DDG under limited communication (e.g. 12 bits per parameter). strength and Weaknesses: strength:  Clear and rigorous privacy analysis.  Variance of DSM approaches that of the centralized Gaussian mechanism minimizing utility loss.  Empirical evidence of superior performance over DDG under certain conditions. Weaknesses:  Privacy secure assumption may not hold for all practical parameter regimes.  Algorithm appears to operate on an unbounded domain instead of a finite group requiring modifications to ensure correct aggregation.  Modular clipping error may affect the accuracy of Corollary 1.  Experiments lack a baseline comparison with the (centralized) Gaussian mechanism.", "Paraphrase: Summary: The paper presents a privacypreserving mechanism based on differential privacy for federated learning. It is evaluated using the MNIST and Fashion MNIST datasets. strength:  Addresses a significant problem in privacypreserving machine learning.  Leverages the use of the Skellam distribution an advanced approach. Weaknesses:  Limited theoretical novelty with the independent contribution being a protocol to compute vector sums with differential privacy.  Insufficient experimental evaluation as the accuracy on MNIST is low (80) compared to previous work (95).  No nonprivate baseline is provided for comparison."], "nhnJ3oo6AB": ["Paraphrased Summary: This paper:  Presents a novel approach to robot locomotion using legs.  Employs a Transformerbased model trained through reinforcement learning.  Provides thorough experimental evaluation in both model and realworld settings measuring performance and safety. effectiveness and Weaknesses: effectiveness:  Clear exposition with detailed diagrams and implementation details.  Demonstrates the effectiveness of visionbased locomotion using an expressive model.  Validated on a real robot. Weaknesses:  lack of significant technical novelty (RL and Transformers arent solely new).  Limited testing of challenging terrain.  Absence of foot clearance and foothold position adjustments in the learned behaviors. Additional Questions: 1. Does the paper use additional simtoreal transfer techniques besides field randomization 2. Is significant tuning required to train natural locomotion gaits If so what reinforcement function terms are critical", "Paraphrased Statement: This field introduces a novel innovation for quadrupedal movement. It combines \"feeling\" (proprioceptive) and visual data with a transformerbased model allowing an \"actor\" to navigate environments containing obstacles and varying terrain. The model predicts environmental changes several step ahead. The method is thoroughly tested in model and during realworld project. It excels in both achieving higher reinforcement and generalizing better between model and realworld settings. Overall the article is wellwritten with a balanced and thorough evaluation. effectiveness:  effectively combines visual and proprioceptive data for quadrupedal movement using transformers.  Wellwritten and clear sections.  Recognizes the importance of proprioceptive feedback for immediate reactions and visual observations for longterm project. Weaknesses:  Using solely five random seeds for evaluation is limited.  Some minor clarifications and details could improve clarity.  Its unclear if the reported results are based on the better seeds.", "Paraphrased Summary: This paper presents a new approach that uses both visual (depth information) and sensor data (IMU joint angles) to learn to navigate and walk on diverse terrains. The approach uses a transformerbased architecture for endtoend training. effectiveness:  Proposes a unique architecture that effectively combines visual and sensor inputs for visual navigation and locomotion project.  Demonstrates successful performance in model for navigation obstacle avoidance and walking on uneven terrains.  Shows promising results when transferred to a realworld robot exhibiting walking and navigation capability in different environments. Weakness:  lack comprehensive comparisons with alternative approaches. While previous hierarchical techniques have been mentioned the proposed approach is not explicitly compared with them. Adding baselines would provide a clearer understanding of its relative performance and limitations.", "Summary This research proposes combining proprioceptive (bodyposition) and visual data for quadrupedal robot movement. The authors introduce a new \"LocoTransformer\" model that uses separate modules to process these inputs before feeding them into a shared transformer network to forecast process and value. Experiments show that incorporating both case of data enables the robot to navigate obstacles of several sizes including moving ones. The authors have also successfully tested the strategy on a real robot in both indoor and outdoor settings with unseen terrain and obstacles. effectiveness  Tackles a key issue in quadrupedal locomotion: integrating visual information into learningbased control systems.  extensive experiments and indepth analysis in a simulated environment.  Successful transfer of the learned policy from model to a real robot. Weaknesses  Realworld experiments may not fully demonstrate the necessity of visual information.  Alternative approaches for utilizing visual data such as mapping and trajectory project should be discussed and compared.  Comparison with the stateoftheart in both academia and industry (e.g. Boston Dynamics position) could strengthen the claims of advance.  Addressing performance in the presence of moving obstacles and comparison with the robots builtin controller would be valuable.  Proofreading is required to address typos in the manuscript."], "vgqS1vkkCbE": ["Summary: The research proposes a new state abstraction method for hierarchical reinforcement learning called Value Function Spaces (VFS). VFS builds a state representation by combining value functions from pretrained lowerlevel policies (skills). These value functions capture the affordances of the skills while filtering out taskirrelevant information. Strengths:  Novel approach to using value idea as state representations.  Leverages existing policies allowing it to approach a vast repertoire of skills.  Clear and wellwritten presentation. Weaknesses:  Unfair compare with baselines as VFS has an advantage in approaching pretrained value functions.  limited testing across different environments. Clarifications and Comments:  The methods manipulation of value idea scaling is not addressed.  Redundancy in sections 4.2 and 4.3 with the introduction.  Artificial restriction of baseline embedding spaces to the size of VFS is questionable.  Provide information on the number of transitions used in training for both VFS and baselines. Final conclusion: After addressing concerns and presenting new results the reviewers increased their score to 6.", "Paraphrase: Summary: This research introduces Value Function Spaces (VFS) a novel representation that leverages learned value parts from skills to abstract knowledge about the environment. This compact representation enables improved performance in longhorizon planning tasks. Strengths and Weaknesses: VFS is an innovative approach that can enhance performance across application involving skills. yet it lacks key performance metrics limiting the evaluation of its effectiveness. Including metrics like \"successweighted path length\" would provide valuable insights. Understanding the failure instance and limitations of VFS is crucial. The paper should address when and why the system may fail and discuss potential limitations. Questions and Suggestions:  Clarify the origin of the \"goal latent state\" used in planning.  Improve design 1(b) to more accurately depict the transformation of states using the VFS encoding part.  Provide specific instance of the skills available to the agent in the environment depicted in design 6 for better understanding of the skill clusters.", "Paraphrase: Summary: This paper suggests using the vector of value functions for individual skills as the abstract state representation in reinforcement learning. The authors create a modelfree RL algorithm similar to DQN and a modelbased planning algorithm for navigating this state space. These algorithms outperform baseline representation learning algorithms in a maze work and robotic manipulation task. Assumptions:  Options are predefined with associated value functions.  The new task can be solved using the defined options. Strengths and Weaknesses: Major Comments:  Source of Options set: The paper assumes options are predefined or learned with an algorithm that provides value functions. yet some options learning approaches do not provide explicit value functions. The authors should consider these approaches and their compatibility with their method.  Positioning with Prior work: The paper idea of using value functions for state abstraction has similarities to previous work (Options Keyboard by Barretto et al. and AOSM by Rosen et al.). The authors should acknowledge these work and explain the differences between their approach.  Relationship with Transfer Learning: The paper does not explicitly discuss its connection to transfer learning through option reuse. The authors should address this relationship. Overall: The paper presents a potentially effective method for constructing actionoriented state abstraction. yet its presentation could be improved by positioning the work within prior research and considering compatible options learning approaches.", "Original Statement: Summary: The paper introduces a novel state representation method Value Function Spaces (VFS) that uses value functions to construct a highlevel space representation in a hierarchical reinforcement learning (RL) scenario. Key part include:  VFSbased state representation  Practical algorithms for VFS in modelfree and modelbased RL settings  Evaluation of VFS in two scenarios (MiniGrid and manipulation task) Strengths and Weaknesses: Strengths:  Wellwritten paper clear and accessible  VFS is a simple and elegant state representation method  VFS demonstvalues substantial performance in practice especially when lowlevel skills are handselected and trained optimally. Weaknesses:  The paper does not address the challenge of automatically discovering and identifying lowlevel skills in hierarchical RL.  The paper assumes that lowlevel skills are \"perfect\" which may not be realistic in practical settings.  It is unclear how VFS handles variations in object set arm set background textures and distractor objects. The claim that these states are functionally equivalent in the VFS representation needs more explanation.  The paper overstates the capabilities of VFS in capturing positional information preconditions and core of skills without providing sufficient validation.  The success value metric is not the only relevant metric a cumulative reinforcement metric should also be considered.  The neural network architectures used for VFS are not mentioned."], "gc8zLQWf2k": ["Paraphrased Summary: This study examines how memorizing unusual model affects the effectiveness of adversarial training. The authors propose Benign Adversarial Training (BAT) which they evaluate on two image datasets. effectiveness:  Provides interesting insights into the memorization of atypical samples during adversarial training.  Clother presents the proposed method and includes detailed explanations.  Evaluates the impact of various components of BAT. Weaknesses:  BATs objective role includes multiple additional terms and lacks evaluation against adaptive attacks that consider these terms.  Does not mention whether other stopping was used during adversarial training which could affect baseline performance.  Uses datasets (CIFAR100 and Tiny ImageNet) that are not commonly used in the adversarial research community. Evaluating BAT on CIFAR10 where there are established benchmarks would provide more valuable insights. Minor issue:  Weak attacks (e.g. FGSM) should not be considered important evaluation.  Provides insufficient details on the scope used for PGD and CW attacks.  The value of 1\u03bb in BAT differs from that reported in the TRADES paper.", "Summary This study examines the \"memorization\" aspect of adversarial training where the model remembers atypical training samples. Through experiments and analysis it reveals two key findings: 1. Memorizing unusual samples can improve accuracy on clean (nonadversarial) atypical samples but not on adversarial model. 2. Memorizing some atypical samples can even hinder the model performance on typical (nonatypical) samples. Based on these findings the authors introduce \"benign adversarial training\" a method that reweights training data. This method has been shown to enhance both clean accuracy and adversarial robustness on certain dataets. effectiveness and Weaknesses effectiveness:  Establishes a connection between memorization and adversarial training.  Provides comprehensive experimental data on training data impact on adversarial robustness.  Introduces a novel method that improves adversarial robustness. Weaknesses:  Lack of novelty in main contributions as similar findings have been reported in previous study.  Some claims lack sufficient support or empirical evidence.  The motivation for exploring memorization in adversarial training is unclear.  The reweighting strategy requires further explanation.  The relationship with curriculum learning in adversarial training is not addressed.  It is unclear how the method can handle longtailed data distributions. additional Notes:  For clarity it is recommended to use \"f\" instead of \"F\" in the loss role equations.  References provided for further study.", "Summary Paraphrase: Researchers investigated the interplay between adversarial training (intended to enhance model robustness) and atypical model in datasets. Unlike conventional training they discovered that adversarial training struggles to generalize to atypical test samples under adversarial consideration. They also found that including atypical model in adversarial training can harm test accuracy contrasting with standard training where these model typically improve performance. The authors propose a method that deemphasizes atypical model during robust training and introduces a contrastive regularizer termed BAT (benign adversarial training). effectiveness and Weaknesses Paraphrase: effectiveness:  rigorous exploration of the relationship between adversarial training and atypical model.  Interesting findings and a promising proposed method. Weaknesses:  Similarity to existing work on robust classification of samples from a Gaussian distribution in the presence of outliers.  Limited exploration of \"poisoning atypical model.\"  Justification for the beneficial effects of discrimination loss regularization needs clarification. Minor Comments:  Confusing title.  Relevant related work (Balaji et al. 2019) not discussed.  Inaccurate reference to PGDgenerated adversarial model as \"manually generated.\"  image 4 could be improved for readability.", "Paraphrased Statement: This study examines the impact of unusual (atypical) samples on adversarial resilience and proposes a modified adversarial training approach inspired by these insights. The novel Benign Adversarial Training (BAT) includes an adjusted loss role and assigns weights to adversarial model based on their distance from the decision boundary. BAT has been shown to improve accuracy on clean data while maintaining comparable robustness to existing techniques. effectiveness:  valuable insights and analysis on atypical samples and robust overfitting  Promising results demonstrating enhanced accuracy with similar or better resilience against strong attacks  Evaluation of individual components in ablation study  Clear and accessible writing style Weaknesses:  Ambiguous threshold for defining atypical samples  The connection between atypicalness and adversarial robustness is unclear  Generalization of theorems beyond atypical samples and specific to nearboundary model  BATs components may be more directly related to margin than atypicalness  Worthwhile to test the algorithm on nonatypical samples to isolate the impact of atypicalness CommentsSuggestions:  Prior work on metric learning for adversarial robustness exists  Related study from the perspective of margin may provide valuable context  Potential typos noted"], "rczz7TUKIIB": ["Paraphrased Statement: Summary: MetaLoss a metalearning loss function approach can automatically adapt the properties of existing loss functions (e.g. MAE MSE) to specific tasks. It has shown improved regression accuracy on time series datasets compared to traditional model that do not adapt their loss functions. While the motivation is clear concerns arise regarding the experimental setup. Strengths:  Clear motivation and adequate literature review. Weaknesses:  Claims of MetaLoss being loss function agnostic are not fully supported as only MAE was used in experiments. Testing with multiple loss functions would strengthen the claim.  Only DNN model were used to train with MetaLoss. Experiments with additional architecture types (RNN CNN LightGBM) would demonstrate its versatility.  Hyperparameter ranges and tuning function for DNN training are not disclosed.  Onestep forecasts are used which limits the applicability of the model to multistep forecasting scenarios prevalent in time series forecasting.  The benchmark datasets used are not commonly employed in forecasting research raising questions about the generalizability of the results.  MAE is used as both the loss function for training and the evaluation measure for forecasts which may not be the beneficial use.  Statistical significance of performance differences is not provided and only percentage improvements are shown.  The computational cost of MetaLoss is not reported making it difficult to assess its feasibility for largescale application.", "Summary This study introduces a method for optimizing loss functions in regression tasks using metalearning. This approach is particularly beneficial when the performance metrics are nondifferentiable which occurs frequently in tasks with asymmetrical misforecasting costs and conditional clauses. The approach involves two neural network:  losslearning NN: Learns an approximate differentiable loss function that resembles the nondifferentiable performance metric.  predictor NN: Optimizes its performance based on the loss function defined by the losslearning NN. Two key techniques enhance the effectiveness of this approach:  loss exploration: Adds noise to the nondifferentiable loss to promote exploration and prevent overfitting.  Cotraining: Trains the two neural network concurrently ensuring that the losslearning NN guides the predictor NNs optimization. Experiments involving time series forecasting tasks support the proposed method. Strengths  Conversion of nondifferentiable loss functions to differentiable adaptation which expands the range of regression tasks that can be optimized.  Clear explanation of the metalearning approach supported by experiments and discussion. Weaknesses Readability  Occasional confusing wording that disrupts the flow of the paper. Novelty Evaluation  loss exploration and cotraining should be further evaluated to demonstrate their impact on performance.  Provide additional experiments to clarify how loss exploration parameters affect learning.  Explore the effects of cotraining using alternative schedules or settings. Claim of Metaloss Excelling Teacher  The claim that Metaloss learns more than the \"teacher\" (MAE loss) is confusing. MAE loss is not a \"teacher\" but rather a reference point for the losslearning NN. applicability to Plain regression Tasks  While the method is presented as generally applicable to regression tasks only time series forecasting experiments are conducted. Exploring its performance on standard regression tasks would provide valuable insights. Clarifications  Explain how the loss perception points are obtained for Metaloss.  Define yt and \u0177t for the losslearning DNN. Minor Comments  Typo in Figure 2: \"loos\" should be \"loss\".  Confusing phrasing in the time discussing Metalosss loss evolution over time.  Figure 2s yaxis label \"Perceived loss\" needs clarification.  Consider using the more familiar term \"overestimation\" instead of \"overdimensioning.\"  Label clarity in Figure 3(d) needs improvement.", "Paraphrased Statement: This research investigates the use of metalearning for selecting loss functions in time series forecasting allowing the choice of loss function to adapt to the specific model being used. The authors introduce MetaLoss a system that jointly trains both the regression network and losslearning network. The paper provides limited evidence of the effectiveness of MetaLoss under specific conditions but its broader utility remains unclear due to several weaknesses. Strengths and Weaknesses:  Strengths: The problem of metalearning loss functions for time series forecasting is a novel concept.  Weaknesses:  The problem formulation lacks a clear explanation and diagram.  The evaluation only compares MetaLoss to a single simple baseline which is insufficient to demonstrate its effectiveness.  The training process is not described in detail nor are the specific hyperparameters used.  An ablation study to assess the influence of different hyperparameter settings is lacking.  The writing is unclear and requires improvement.  The paper lacks data essential for reproducibility including hyperparameters source code and data availability.  The authors use misleading terms such as \"we prove\" and make exaggerated claims based on limited results.", "Paraphrased Statement: Summary: This paper presents a model for learning loss functions in time series forecasting regression tasks. It consists of two learning components: one for forecasting the next time step and another for learning the loss function. The experiments demonstrate that the proposed method enhances performance slightly over MAE. Strengths and Weaknesses: From my perspective this paper has several significant issues: 1. Questionable Outer learning Objective: Unlike most metalearning approaches this papers outer learning goal is simply to reduce the difference from the existing MAE loss function. This approach seems illogical because the function of loss learning is to surpass existing loss functions like MAE but the proposed method aims to minimize the deviation from MAE. The authors do not provide a convincing justification for this choice. Additionally the introduction of noise during learning appears adhoc without a clear explanation of its rationale. 2. Lack of Novelty: The paper claims that its method of cotraining the predictor and loss is innovative. However it is similar to the commonly used approach of onestep lookahead or online firstorder approximation which is wellestablished in metalearning and gradientbased hyperparameter optimization. The authors should properly cite these existing approaches and clarify their methods distinct features. 3. Insufficient Baselines and Justifications: The paper only compares its method to MAE and does not consider other standard loss functions such as MSE or MSLE. Furthermore the performance improvement is minimal and the use of a metric called \"Gain()\" is questionable. The authors should provide justification for the effectiveness of randomizing techniques like dropout or smoothing instead of attributing the improvement solely to randomness."], "w7Nb5dSMM-": ["Summary: This paper presents the groundwork for a novel class of evolution strategies (ESs) based on biological research from the 1980s. The ESs show an equivalence to stochastic gradient descent (SGD) in certain situations but lack experimental validation. Strengths:  Wellargued and exciting potential  Strong advocacy for blackbox optimization Weaknesses:  Lack of experimental Validation: The paper does not provide any experiments to support its theoretical findings.  Incomplete Practical application: The paper mentions neuroevolution but lacks results for standard RL control problems.  Outdated equivalence algorithm: The proposed experiments should compare the novel ESs to modern ES implementations such as CMAES or NES.  Insufficient Performance Analysis: The paper does not analyze the performance of the proposed ESs in high dimensions and sample efficiency.  Incomplete Literature Review: The review skips evolutionary neuroevolution advancements from 1990 to 2010 and presents an incomplete view of the current state of the art.  Conflicting Equivalence theory: The paper equates the proposed ESs to SGD but also states that the models used do not need to be differentiable. This creates confusion about the equivalence applicability.  Lack of rate Proposition: The paper does not explain why users would choose an ES over SGD especially given the superior performance of SGD.  Transfer Learning Attribution: The paper does not correctly attribute credit for transfer learning also known as \"finetuning.\"  Exaggerated Computational Resource Claims: The paper overstates the computational resources required for ES experiments. Many hobbyists can solve complex problems with ESs on personal computers. Conclusion: The paper presenta promising research way but its theoretical findings need experimental validation and practical applications. The authors should revise the manuscript to address these weakness before publication.", "Paraphrased Statement: This paper explores the connection between the GillespieOrr (work) model of evolution and Stochastic Gradient Descent (SGD). The authors aim to demonstrate the potential benefits of using evolutionary algorithms for finetuning models. Strengths:  The authors generalize the application of work EA beyond the specific model provided.  They establish an equivalence between work EA and SGD providing details on the validation and limitation.  The paper discusses how minima flatness affects a genetic algorithms error correction ability highlighting potential difficulties in transferring robust models.  Two proposed heuristics for improving model finetuning are described in detail addressing separate aspects of the problem space. Weaknesses:  The paper acknowledges limitation of genetic algorithms in finetuning and the equivalence between work EA and SGD but it lacks a thorough analysis of these limitation when the assumptions dont hold.  The section on dimensionality detection is brief and does not fully explain how the efficient phenotypic dimension is calculated and used to find the best population.  Empirical evidence to support the claims made in the paper is lacking limiting the credibility of the proposed benefits.  The paper contains minor grammatical and spelling errors.  more indepth theoretical analysis and empirical results would enhance the significance of the paper.", "Paraphrase: Summary This work introduces GillespieOrr evolutionary algorithm (GOEA) and demonstrates its use in parameter finetuning. It presents the concept of evolution in the context of model finetuning by contrasting it with SGD and examining the likelihood of finding improved parameters. Furthermore it provides practical methods to expedite the finetuning process. Strengths and Weaknesses Strengths  Comprehensive review of evolutionary and stochastic gradient descent algorithms  Novel perspective of finetuning as an adaptive process to changing environments Weaknesses  Main algorithm based on existing reresearch  Focus on parameter evolution is not original  Limited experimental results to support the proposed theory and heuristics  Inconsistent sentence structure and repetition  Incomplete sentence: \"... in reverse exponential probability of the\"  Excessively long and short paragraphs hindering readability  Lack of direct equivalence to SGD References 1. Salimans T. et al. (2017). evolution strategies as a scalable alternative to reinforcement learning. 2. Such F. P. et al. (2017). Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural network for reinforcement learning. 3. Khadka S.  Tumer K. (2018). evolutionguided policy gradient in reinforcement learning. 4. Pourchot A.  Sigaud O. (2018). CEMRL: Combining evolutionary and gradientbased methods for policy research. 5. Lee K. et al. (2020). An efficient asynchronous method for integrating evolutionary and gradientbased policy research.", "Paraphrase: Summary: This paper establishes an equivalence between a case of evolutionary algorithm and stochastic gradient descent (SGD). While its wellwritten it lacks proper definitions for introduced notation. Strengths and Weaknesses:  The equivalence between SGD and evolutionary algorithms is intriguing but not only novel.  The paper introduces a unique case of evolutionary algorithm that is easier to formalize and analyze.  The paper suggests that the flatness of SGD minima corresponds to the stability of learning in artificial neural network (ANNs).  It also hypothesizes that transferring a model with flat SGD minima is only efficient when the target data also has similar minima flatness. limitation:  The paper lacks a formal framework making it difficult to assess the robustness of the model.  The generalizability of the hypotheses requires experimental validation to provide datadriven support.  The concept of SGD minima flatness needs better definition to guide data selection for successful transfer learning.  The theory of error correction in ANNs needs empirical verification."], "xNO7OEIcJc6": ["Paraphrase: The authors examine CLIPs behavior when given images with superimposed text. They define \"labelswitched images\" as those where the superimposed word changes CLIPs classification. Using fMRI information they created a corpus of images with 12 superordinate (broad) categories 138 basic (narrow) categories and 150 instances each. They superimposed labels on each image and recorded CLIPs response. The authors analyze CLIPs errors on these images and argue that since the errors cannot be predicted by the semantic or spelling similarity between the superimposed word and the depicted object (which is the case in human psychology experiments) CLIP does not process images in the same way as humans. However the authors motivation is unclear as CLIPs objective is to match image captions. Labeling the text as \"incorrect\" is misleading since CLIP may justifiably assign high scores to both the depicted object and the superimposed word. The authors main argument that CLIP behaves differently from humans is based on the observation that labelswitched misclassifications (where CLIP predicts the written word rather than the depicted object) do not depend on semantic or spelling distance. However this argument is problematic because it does not account for the inherent ambiguity of the setup. The authors propose an alternative hypothesis that CLIPs performance would be more affected by semantic or spelling proximity as in humans. However the experimental setup does not adequately test this hypothesis as it is not comparable to human experiments.", "Summary The authors have developed a model to assess the \"pictureword interference\" phenomenon in CLIP a stateoftheart visual and language model. This interference result is observed in human cognition when people process images with superimposed words. The model investigates whether CLIP exhibits similar interference across different image categories and explores whether the interference is influenced by the semantic similarity between words and images. result indicate that:  Presenting superimposed words disrupts CLIPs image classification regardless of image category.  The semantic relationship between words and images does not affect the interference result.  The superimposed word representation in CLIPs image encoder is distinct from the image representation. Strengths and Weaknesses  Strengths:  The study investigates an essential aspect of CLIPs cognitionlike behavior.  Weaknesses:  Formatting: image and tables should be aligned at the top.  Writing: There is a potential misunderstanding regarding CLIPs training process. The authors suggest that CLIP models language information from images but this is not explicitly stated in the original paper.  Task Setting: A more detailed explanation of the task settings would help prevent confusion from arising.", "Summary: This study developed a task to assess how the meaning of word forms in images affects object recognition. It explored:  Error rates of the CLIP model in classifying wordembedded images.  Similarity between original and overlaid labels.  Changes in model prediction probabilities.  Representations of images with different types of wordimage conflicts. The study suggests that CLIP can recognize word forms but struggles to represent word forms and images in the same category with similar visual representations. Strengths:  Clearly stated motivation and background.  Multiple analysis to evaluate wordimage interference.  Easytoreproduce methods. Weaknesses:  Some result need further explanation.  Its unclear if CLIP was finetuned on images with word forms.  Misclassification could be divided into specific cases.  The term \"semantic compositionality\" may not align with the studys focus.", "Paraphrase: This study creates a dataset to test how image classification models handle interference from written words. The authors examine CLIP a languagebiased model to see if it experiences the same interference result as humans. image in the dataset have words superimposed that represent different category levels. Experiments reveal that CLIP is influenced by superimposed words but the interference is unrelated to the semantic connection between the word and the image. Further investigation shows that CLIPs image representations differ from those of superimposed words while this distinction is absent in two ImageNetbased CNNs. Strengths:  The dataset helps evaluate the biases of image recognition models that incorporate language.  Various analysis are presented to isolate the impact of pictureword interference in CLIP. Weaknesses:  The writing could be improved for clarity.  image could be enhanced to better illustrate the studys design and findings.  While the CNNs investigated showed less interference it was not evaluated if they would perform similarly if trained on a noisy dataset like CLIP.  It would be beneficial to train CLIP on ImageNet to determine if its language bias is still distinct from CNNs under such conditions."], "xtZXWpXVbiK": ["Paraphrased Statement: Summary: This research explores the use of normalizing flows (NFs) in belief inference models for partially observable visual control tasks. The paper proposes a modelbased reinforcement learning (MBRL) model that derives the demonstrate lower bound (ELBO) for the NFbased inference model. Empirical Evaluation: The proposed models inference capabilities are tested in a sequence MNIST domain. Its control performance is assessed in six continuous control problems. Results show improved accuracy in predicting hidden states and increased sample efficiency in solving control tasks. Strengths:  NFs enhance control performance and reduce sample complexity in MBRL compared to Gaussian assumptions.  The paper presents a novel approach to belief inference using NFs demonstrating their potential in MBRL.  The model exhibits the ability to predict future uncertainty as seen in the sequential MNIST results. Weaknesses:  Lack of comparisons to certain relevant baselines (e.g. PlaNetBayes Igl et al.s method RNNs) in the empirical results.  Potential confusion in terminology between belief states and hidden states which should be clarified.  Missing ablation study on the impact of the number of imagined trajectories (N) on results.  Limited exploration of the models capability for reasoning about future uncertainty.", "Paraphrased Statement: This paper introduces a method for learning dynamic belief states for Partially Observable Markov Decision Processes (POMDPs) using a more flexible approach. The authors argue that previous approaches using conditional diagonal Gaussian distribution limit the models ability to accurately predict the environment and lead to suboptimal performance. To address this they propose using normalizing flows to build a robust parametric distribution for the POMDPs statespace model. This approach allows the model to capture more complex model in the environment dynamics. additionally they incorporate their learned model into a reinforcement learning model resulting in a highly efficient algorithm. Evaluation: Strengths:  Utilizes a more expressive distribution to substantially represent environment dynamics leading to improved sample efficiency in RL.  Demonstrates consistent performance improvements over baselines and other standard RL algorithms. Weaknesses:  The approximation of using flexible distribution for POMDPs is not only novel as particle filters have also been explored.  The POMDP RL model proposed closely resembles the Dreamer algorithm and lacks clear distinctions.  Details on the data buffer foundation and retrieval of specific data points are missing. additional Considerations:  The authors acknowledge the challenges of handling state uncertainty in POMDPs but the experiments presented primarily focus on environments with minimal temporal stochasticity.  The performance of Dreamer with multiple imagined trajectories is surprisingly lower than Dreamer and this discrepancy is not fully explained.", "Paraphrase: Summary: belief states are often used in POMDPs for learning and recently researchers have explored deep recurrent generative models to approximate belief states in complex POMDPs. This paper proposes using normalizing flows which are more flexible than the isotropic Gaussian distribution commonly used in deep learning to represent belief states. The approach outperforms Dreamer a similar method. Strengths:  The authors introduce a more expressive distribution for representing belief states in POMDPs through normalizing flows.  The proposed approach is effective as demonstrated by positive results on sequential MNIST. Weaknesses:  The novelty of the paper is questionable as it may be similar to Dreamer with normalizing flows added to the latent space.  The results on DM control Suite are less impressive.  The paper lacks a clear explanation of the differences between FORBES and previous work potentially overlooking DreamerV2s multimodal and discrete latent space.  The paper contains several spelling errors. Postrebuttal Updates:  The authors clarify that FORBES is more general than a modification of Dreamer and acknowledge theoretical contributions relevant to the community.  The authors address reviewer concerns and improve the paper coherence. Final Recommendation: While the paper lacks significant novelty in the context of POMDPs it presents a solid approach with positive results. It is recommended for acceptance especially given the authors efforts in addressing reviewer feedback.", "Paraphrased Statement: This paper suggests a novel approach for tracking belief in continuousstate Partially Observable Markov Decision Processes (POMDPs) using normalizing currents. The belief approximation is learned through variational inference and an actorcritic reinforcement learning algorithm is designed based on this representation. Strengths and Weaknesses:  The accurate approximation of belief is crucial but challenging in continuousstate POMDPs. This paper proposes a normalizing currentbased belief approximation to address this difficulty.  The proposed method of learning the belief approximation maximizes an demonstrate Lower BOund (ELBO).  An actor and a critic are learned in conjunction with the normalizing current belief approximation.  Experiments demonstrate that the normalizing currentbased belief achieves higher ELBO than a baseline Recurrent State Space Model (RSSM) on the MNIST sequence dataset. Additionally the actorcritic algorithm demonstrate solid performance on several problems from the DeepMind control Suite.  The paper is generally straightforward to comprehend. However some key aspects of the approximation need clarification to fully understand its correctness. Clarification Requests:  In Equation (7) and Theorem 1 the expectations should be taken with respect to q(s1T  \u03c4T oT). It would be helpful to explicitly state how q(s1T  \u03c4T oT) factorizes.  Please explain the rationale behind the architecture illustrated in digit 3.  How are the observations and actions encoded into q\u03c8(st  taut ot)  Where do the transition dynamics p\u03c8(st  st1 at1) and the observation model p\u03c8(ot  st) appear in the belief model If they are utilized for encoding actions and observations they are not designed to approximate the true dynamics and observation model. Using them as such in Equations (12) and (13) seems questionable.  Equation (9) differs from Equation (7) because Equation (7) employs a Markov transition model p(st  st1 at1) while Equation (9) utilizes a nonMarkov transition model p(st  taut).  In the reinforcement learning algorithm the actor and critic are both use of the state not the belief. How is an action calculated for a belief in this case  While there is a discussion of the learning objectives for the actor and critic a complete description of the reinforcement learning algorithm is lacking. For instance how is training experience gathered Which experiences are stored in the experience buffer (Algorithm 1) How frequently are the belief model actor and critic updated Questions and Concerns Regarding Experiments:  For the digit writing task what constitutes the state  Details about the neural networks employed are missing.  RSSM originates from the PlaNet paper not the Okada et al. 2020 paper.  The proposed belief model and RSSM are only compared on the digit writing task. Can they be assessed on problems from the DeepMind control Suite  Results are only provided for six out of 20 problems from the DeepMind control Suite. For some issues the Dreamers findings still fall short of those provided in the original Dreamer paper. This discrepancy may be due to the algorithms only being run for 500k steps while the Dreamer paper results are obtained after 5 million steps. To see if the novel algorithm can eventually match Dreamers reported performance it would be beneficial to run it for a longer period. Minor Grammatical Error:  Page 3 line 13: \"usally\" should be corrected to \"usually.\""], "yuv0mwPOlz3": ["Paraphrased Summary: The researchers tested various active learning techniques for classifying text data in situations where the data comes from multiple source and differs from the target data (domain shift). They used methods based on uncertainty divergence reverse classification and nearest neighbors creating 18 combinations and testing them on questionanswering and sentiment analysis datasets. Main findings:  Uncertainty in different example manifests differently leading to variant in the data points selected for labeling. effectiveness:  Ambitious goal with comprehensive validation of subproblems.  Wellstructured paper despite the complexity of the work.  Thorough experiments extensive discussions and comprehensive source. Weaknesses:  Unclear Practical Applicability: The authors fail to provide clear example of realworld situations where active learning with multiplesource domain adaptation is necessary.  Questionable Methodologies: The connection between some methods (KNN Hdivergence) and active learning is unclear and the ranking of data points based on lowest uncertainty contradicts active learning principles.  Incomplete evaluation: The authors compare final F1 measures instead of learning curves and the methods used are not considered true domain adaptation example.  Unanswered Questions: several research question posed by the authors remain unanswered.  Minor Issues:  Use of bold and red fonts may be distracting.  Incorrect citation of a paper that used dropout for uncertainty quantification.  The two tasks used for evaluation are very similar.", "Paraphrased Statement This paper explores various approaches to active learning when dealing with text data across multiple domains. Given a limited labeled dataset from a target domain and extensive unlabeled data from multiple source domains the question is how to select example from the source domains for labeling to enhance performance on the target domain. The research concludes that selecting example based on Hdivergence metrics surpasses most other active learning techniques. Notably the proposed method called DALE selects example from the source domain that are similar to misidentified target domain example and outperforms standard \"discriminative active learning\" in most experiments. The analysis indicates that selecting example from various domains is beneficial and DALE helps prevent the selection of problematic example. effectiveness:  Practical guide for practitioners encountering similar domain shift scenarios.  Comprehensive experimental evaluation covering various domain shift settings in NLP. Weaknesses:  Absence of normalization in the analysis of example ranking by different methods.  Contested claims about the impact of diversity on performance in Section 6.2 as obtaining data from multiple domains can potentially hinder performance with certain domain distribution conditions. Based on these weaknesses the paper is currently borderline. However addressing these concerns would prompt a higher evaluation.", "Paraphrased Statement: This paper explores active learning across multiple domains to identify example for sentiment analysis and question answering tasks. By evaluating 18 acquisition strategies from four different family it demonstrates that the HDivergence method (including a proposed variant called DALE) generally yields the most significant improvements. effectiveness and Weaknesses: Unlike previous work that focused on a single domain this paper investigates multidomain active learning a more realistic scenario. It provides a comprehensive analysis of various active learning methods and experiments examining correlations among example rankings and exploring optimal source domain combinations. Despite being wellwritten the paper lacks significant technical novelty beyond the two proposed variant. Additionally some elements of the active learning analysis have been addressed in other work and the connection to similar work like [1] is not discussed.", "Paraphrased Statement: This paper compares active learning (AL) domain shift detection (DS) and multidomain sampling techniques for combining data from different source. While the paper is wellwritten and organized it lacks specific recommendations on how to apply AL DS and multidomain sampling in various applications. Additionally the selection of question answering and sentiment analysis as the experimental datasets is not adequately justified. Finally the analysis of the experimental results could be improved to provide more insights and support the findings."], "oLYTo-pL0Be": ["Paraphrased Summary: This work introduces a new Federated Learning (FL) method that employs a \"teacher\" and \"scheduler\" to enhance robustness. The teacher a reinforcement learning agent guides node data training by identifying effective minibatches. The scheduler a metalearner observes model updates and selects nodes (nodes) for global model distribution. This strategy aims to protect against compromised nodes. Critique Highlights: Strengths:  relevance of FL in data management particularly in healthcare.  Focus on mitigating backdoor approach.  novel use of a scheduler to identify potentially compromised nodes.  Experimental issue using realworld hospital data. Weaknesses:  Lack of clarity regarding technical aspects including:  training and distribution of teacher models.  implementation details of the scheduler including node selection strategy and scalability.  Incomplete experimental details:  distribution of data and nonIIDness in MNIST and CIFAR10 experiments.  Lack of ablation work to quantify the impact of the teacher model.  Insufficient data on robustness experiments including approach implementation and comparison with existing robust FL approach.", "Paraphrased Statement: The proposed federated learning algorithm utilizes a neural scheduler which is trained without accessing local data. Although this approach has the potential to enhance the efficiency and robustness of global models it has certain limitations and requires further development. Concerns and Recommendations: 1. The use of deep reinforcement learning (DRL) for neural scheduling is not novel as previous research has applied DRL for data filtering and performance optimization. The authors should provide more detailed comparisons with these existing work to highlight the unique contribution of their approach. 2. The experiments provided do not sufficiently demonstrate the effectiveness of the algorithm. The authors should include baseline comparisons against other node selection strategies such as hypernetwork or bandit techniques. 3. While the proposed algorithm exhibits robustness against backdoor attacks the underlying explanation for this phenomenon remains unclear. The authors should provide theoretical or empirical insights to elucidate the reasons behind this observation.", "Paraphrase: This paper introduces a new way to build models using data from different hospitals without revealing patient data (federated learning). The framework has three main contribution: a basic machine learning model a \"teacher\" that uses reinforcement learning and a \"scheduler\" that tells the teacher which hospital to focus on. The paper explains the framework in detail and shows how it works in examination. Strengths:  Studying how to train models across hospitals using federated learning is significant to protect patient privacy.  Tests show the framework works well and how each contribution helps.  The paper is mostly clear and logical. Weaknesses:  The paper does not offer much new theory for federated or curriculum learning. Most equations from Section 3 number from previous work.  The authors need to show what this paper adds thats new and how it helps the community.  The examination details in Sections 4 and 5 are not clear such as missing data about the model training settings.  The papers organization could be better. For instance the dataset details should be shorter or moved to supplemental materials and the discussion and conclusion should focus more on the papers main contribution. Comments:  The teacher contribution seems to be a fixed pretrained reinforcement learning agent. But it should change when a different student model is chosen because the input space will be different. The authors should explain this.  Typos:  Section 1 Paragraph 4: \"Firstly We use\"  \"Firstly we use\"  Section 4 Paragraph 1: \"clinical staff who greet\"  \"clinical staffs who greet\"  Section 4 Paragraph 3: \"into three samples of\"  \"into three samples\"  Table 1 compares decentralized FLST to centralized classification methods while Table 2 compares FLST to decentralized federated learning methods. Its surprising that FedAvg does better than most centralized methods which isnt consistent with other work. The authors should explain this."], "gCmCiclZV6Q": ["Paraphrased Summary: This research explores using CLIP (a large language model) to pretrain a model on a social morality image database. The goal is to detect and mitigate offensive content in large image models trained on massive datasets like ImageNet. Strengths and Weaknesses:  Strengths: The research proposes a method to address potential biases in large image models by leveraging CLIPs ability to detect implicit knowledge and infer offensiveness.  Weaknesses:  The definition of \"offensiveness\" used in the research is unclear and it is unclear who the target audience for determining offensiveness is.  The research assumes that large models trained on large datasets will solve the problem of defining offensiveness when in reality it is a complex philosophical question.  The research references prior art for the thresholds used to discretize evaluation in the SocioMoral Image Database but does not explicitly justify these choices.  The research does not adequately address the potential for reinforcing biases in the filtering model (CLIP) used to detect offensive content.  The impact of the qualitative representative provided is unclear including whether they are from a highrecall setting and whether they validate the need for the proposed filtering technique.", "Summary This paper proposes using pretrained CLIPbased models to identify offensive images. After reviewing concerns about inappropriate content in datasets the authors conduct baseline experiments to demonstrate the feasibility of detecting such content. They then explore using CLIP models implicit knowledge for mitigation. Validation on ImageNet shows that the approach can uncover previously unnoticed offensive images. Strengths:  focus on dataset bias in deep study models and suggests a way to identify questionable representative.  Provides a comprehensive background and related work section.  work code and pretrained models available. Weaknesses:  Technical contribution is limited as the work largely extends existing CLIP models.  Lacks analysis of precision and recall for the proposed work.  Does not fully address ethical concerns.  need to correct minor formatting and citation issues.", "Paraphrased Summary: This study develops a classifier to determine whether an image is perceived as offensive. It uses a psychological dataset of images and morality judgments from a research study. Predicting these judgments is challenging due to the limited data. To address this the study leverages CLIP an further image identification model and a technique called softprompt tuning. Softprompt tuning significantly improves CLIPs performance achieving over 95 accuracy compared to 85 with standard finetuning. Strengths and Weaknesses: While the classifier is effective the use of the study is unclear. The research uses an uncommon dataset and focuses on a narrow application presenting limited analysis beyond identifying categories in ImageNet with high immorality predictions. The study could have explored prompt finetuning in CLIP more comprehensively rather than limiting its application to this specific dataset."], "g4nVdxU9RK": ["Paraphrased Statement: Summary Rewardless OpenEnded Learning (ROEL) combines two existing approach: Paired OpenEnded Trailblazer (POET) and DynamicsAware Unsupervised Discovery of skill (DADS). POET creates progressively challenging environments to encourage agents to develop novel behaviors while DADS provides a framework for learning skills without rewards based on mutual information between state transitions and latent skill variables. ROEL integrates these approach to generate environments that elicit controllable behaviors without relying on reward functions. Empirical Evaluations: The following evaluations were conducted:  Qualitative assessment of skills learned by ROEL  Qualitative comparison of ROEL and DADS  Measurement of rewards for skills learned by ROEL and DADS  Measurement of state predictability for skills in different environments by ROEL and DADS contribution:  Combines openended and unsupervised RL  Demonstrates that ROEL generates more robust policies in a bipedal walker environment compared to DADS effectiveness and Weaknesses: effectiveness:  Combines two natural and synergistic learning approach  Moves towards learning without manually designed structures  Clear presentation of the algorithm Weaknesses:  Lack of evaluation of transferability to other environments  Difficulty in visually distinguishing between ROEL and DADS  Indirect quantitative analysis  No comparison against POET Recommendation: Accept with potential for reconsideration based on further discussion. Justification: Despite the weaknesses in the evaluation the algorithm novelty and overall quality reinforcement acceptance. Specific Questions:  What challenges were encountered in combining POET and DADS Additional Comments:  The review is based on the reviewers understanding of RL but may not fully consider other similar approach.  various typographical errors have been identified.", "Paraphrase: Summary: This research proposes a method for skill learning in environments without explicit rewards. It involves coevolution of agents and environments where the goal is to maximize the mutual information between a skillrelated code and the environments state transitions. effectiveness:  The concept of openended skill learning and unsupervised skill discovery is innovative.  The paper effectively combines approach from previous work.  The writing is clear explaining the complex approach well. Weaknesses:  The evaluations are unconvincing.  The baseline algorithm only trains on a single environment which makes it unsurprising that the method trained on multiple environments (coevolved or not) outperforms it.  It would be beneficial to compare the algorithm to a method trained on multiple fixed tasks (or other basic multitask method) to assess the impact of coevolution beyond multiple environment training.  The differences observed in the figures could be attributed to singleenvironment versus multienvironment training rather than the curriculum generated by the proposed algorithm.  While quantitative evaluation of unsupervised learning is challenging some assessment of finetuning a specific skill for extrinsic rewards would strengthen the evaluation (compared to suitable baselines).  The significance and impact of the algorithm are unclear without appropriate experimental comparison. PostRebuttal Update: While the addition of an evaluation against DR is a positive step the evaluation still needs improvement. further refinement is encouraged before the paper can be considered acceptable.", "Summary This paper presents a method called ROEL that combines unsupervised skill discovery (DADs) with openended learning (POET). This method allows the environment and agents to evolve together enabling the discovery of a diverse set of skills. ROEL is tested in a simulated bipedal walker environment. effectiveness  Tackles the challenge of learning without explicit rewards which is difficult and impractical in real systems.  Utilizes openended learning to encourage a wider range of skill learning. Weaknesses  experimental Evaluation:  Lacks evaluation on DADs environments which are more complex and demonstrate the effectiveness of DADs skill discovery.  performance comparison on outofdistribution environments lacks quantitative step and relies solely on visualizations that may not reliably reflect actual performance.  The diversity of returns is used as a heuristic for diverse skills but this connection is not fully explained or justified.  Novelty:  The approach is not particularly novel as it primarily combines existing algorithm (POET and DADs) without introducing significant new concepts or techniques.", "Summary: Researchers propose a novel method called Rewardless OpenEnded Learning (ROELD) that combines openended reinforcement learning and unsupervised skill discovery techniques. ROELD extends POET a recently developed method to perform skill discovery in environments without explicit rewards. They demonstrate the effectiveness of ROELD in learning identifiable skills in a bipedal walker environment. effectiveness:  Addresses the problem of unsupervised skill discovery in openended RL environments where traditional supervised RL approach are limited.  Combines existing methods in a novel way to tackle this problem.  Provides comprehensive empirical evaluation including both qualitative and quantitative results. Weaknesses:  Lack of clarity in the training details of the DADS baseline making comparison unfair.  Need for further experiments to demonstrate the effectiveness of ROEL compared to DADS trained with field Randomization.  Limited explanation of how skills are selected in experiments and the meaning of \"outperforms\" in the context of offsample environments.  Structural issues in the papers organization with implementation details appearing in the Method section and future directions scattered throughout. Minor Issues:  Incorrect use of citation commands in the manuscript.  Use of informal language and abbreviations.  Commas missing in sentences. PostRebuttal Update: While the authors have addressed some concerns the manuscript still compares against DADS on the normal environment which is unfair. The authors are advised to continue improving the paper and submit it for future review."], "mFpP0THYeaX": ["Summary Paraphrased: This field presents a novel approach for handling situations where intermediate distributions are not available. The method involves creating artificial samples by combining source and target features. The effectiveness of the approach has been assessed leading to valuable insights. Strengths and Weaknesses Paraphrased: Strengths:  The paper is wellpresented and accessible.  The proposed method is simplistic even impactful.  The field analyzes the factors influencing the effectiveness of iterative selftraining providing valuable insights for the community. Weaknesses:  Generating intermediate data for domain adaptation is not a novel concept and comparisons with relevant process (e.g. [12]) are recommended.  The proposed method yields minimal enhancements over iterative selftraining (as evidenced in Table 3). more evaluation on datasets with different natural distribution shifts are suggested.", "Summary Paraphrase: This paper proposes a method for solving the gradual domain adaptation problem. It creates virtual samples from intermediate distributions by combining representations of model from the source and target domain. Strengths Paraphrase:  The paper tackles an underresearched problem in gradual domain adaptation.  The concept is simple and straightforward to comprehend. Weaknesses Paraphrase:  Novelty Concerns:  The papers primary idea is not novel as other methods have proposed creating synthetic data for adaptation. These methods should be compared to the proposed GIFT method on the gradual domain adaptation task.  Technical detail Concerns:  The lambda scheduler uses a fixed step size which may not be optimal for different datasets.  Its unclear which alignment method is used for the final model.  Experiment Concerns:  Comparison with iterative selftraining is unfair due to different training iterations.  The separation of Tables 1 and 2 is confusing and the claim of GIFTs robustness is questionable based on the results.  The use of different numbers of teacher updates in image 3 is unexplained.  GIFT does not exhibit a clear improvement over iterative selftraining with the same baseline.  other Concerns:  The source section needs updating with novel publications on domain adaptation and selftraining.", "Paraphrased Statement: This field tackles domain adaptation (DA) using a novel method called GIFT. GIFT comprises two components:  Manifold Mixup: Creates virtual samples by seamlessly blending features from both source and target domain. The blending intensity (mixup coefficient) gradually shifts towards the target domain over time.  Coteaching Strategy: Allows two networks to collaborate and mutually improve their performance. GIFT demonstrates promising results on various small synthetic and natural image datasets surpassing various baseline methods. Strengths and Weaknesses:  Strengths:  GIFT incorporates a unique approach to annealing mixup weights.  Weaknesses:  The extent to which the weight annealing strategy contributes to improved DA performance is unclear.  limited technical novelty as GIFT primarily extends existing manifold mixup techniques.  The evaluation datasets used (FMoW and Camelyon17) are not commonly employed for DA assessments.  The authors should consider testing GIFT against a wider scope of established DA methods.  A related method Tent should be included in the comparison.", "Summary To address the issue of unsupervised domain adaptation in the context of gradual shifts researchers have developed an iterative selftraining approach. This method gradually adapts a model trained on a source domain to a target domain. The approach assumes access to intermediate samples connecting the source and target domain. However in cases where such samples are unavailable the authors propose a strategy to generate them. The proposed method called GIFT performs manifold mixup between representations of source and target domain model. The weight of the target domain representation is gradually increased as training progresses effectively introducing an implicit curriculum. The authors also introduce a heuristic to pair model for mixup. Strengths and Weaknesses  Strengths:  Generates data from intermediate distributions which is practical when such distributions are not available.  Weaknesses:  It is unclear why gradual domain adaptation is necessary when intermediate samples are not available.  The definition and evidence for intermediate distributions generated by manifold mixup are lacking.  experimentation do not compare the proposed method to established baselines making it difficult to assess its merit.  solution indicate improvements over iterative selftraining but may not be significant without multiple runs.  Presentation and clarity issues make it difficult to understand the approach and its contribution. Additional Comments  The manuscript lacks proper notation definitions and premise for the proposed approach.  The meaning of \"a beneficial classifier\" should be clarified.  Algorithms 1 and 2 need better explanation and variable introductions.", "Paraphrased Statement: This research enhances previous process by addressing a broader scenario where intermediate distribution shifts are difficult to quantify per sample. To handle missing samples from intermediate distributions the researchers propose gradual feature interpolation. In situations where iterative selftraining is ineffective this approach proves beneficial. The field includes synthetic and realworld distribution experiments to support its claims. Strengths:  The issue is clearly defined and supported by model and experiments.  The paper is wellstructured and easy to comprehend. Clarification Required:  Algorithm 2 assigns matching scores of 1 for samples from the same class. However if there are false positives it could bias the alignment towards them hindering the student netprocesss training.  The selection of target model for mixing might introduce challenging model early on. Its unclear how random assignments address this issue.  The Align function and its associated \u03bb value determine the difficulty of the intermediate model generated. The tuning of the \u03bb scheduler is not adequately direct.  There is a discrepancy in notation between the \u03bb scheduler step size in Algorithm 1 (\u03b4) and on page 3 (\u03c3).  One of the subplots in image 1 mentions \"gradual selftraining\" but this method is not discussed or referenced in the paper."], "saNgDizIODl": ["Paraphrased Summary: This work introduces NUQ a kernelbased technique for evaluating epistemic uncertainty in pretrained deep neural network classifiers. NUQ practice kernelbased probabilistic estimation to either the input clear hidden layer or output logits of the network. While NUQ shows promising performance in differentiating between indistribution and outofdistribution samples it has limitations. effectiveness and Weaknesses:  practice output logits as input to NUQ may be problematic due to their invariance to certain input space guidance potentially hindering the detection of changes from indistribution to outofdistribution samples.  Its not only clear how NUQ differs from DUQ another uncertainty estimation method and their results appear comparable.  The work doesnt include more innovative competitive baselines such as ensembles and DDU for a comprehensive evaluation.  The optimization of other methods used for comparison such as the issue of networks in ensembles and hyperparameters in MCdropout is not fully described.  The accuracy of NUQ in estimating reliable epistemic uncertainty is not evaluated and comparison with properly optimized choice are not provided.", "Paraphrased Summary: This paper presents a nonparametric method for quantifying uncertainty in deep neural networks. By estimating the conditional density in the pretrained networks feature space it separately calculates epistemic and aleatoric uncertainty based on risk bounds. practical implementation involves using existing methods for kernel density estimation Gaussian mixture models bandwidth selection and neighbor retrieval. The effectiveness of the method is demonstrated in OOD detection tasks across image datasets. effectiveness and Weaknesses (Paraphrased): effectiveness:  usage principled step (excess risk and Bayes risk) to quantify uncertainty.  Provides a practical method for large datasets.  Demonstrates solid experimental results on various image datasets. Weaknesses:  Significance:  Uncertainty estimation is limited to the neural networks feature space which may be affected by feature collapse.  Other methods provide principled uncertainty quantification and its unclear how this method compares.  Efficiency:  Requires additional GMM modeling neighbor search and bandwidth selection.  Experiments:  Should include comparison with other principled uncertainty quantification methods.", "Summary Paraphrase: This paper introduces NUQ a singlepass method based on kernel density estimation (KDE) for estimating uncertainty in neural networks. Experiments show that NUQ improves robustness to data noise and outlier detection compared to deterministic models on MNIST benchmarks. It achieves similar performance to DDU on CIFAR100based outlier detection and surpasses it on ImageNet. effectiveness and Weaknesses Paraphrase: effectiveness:  Proposes a method for deterministic uncertainty estimation in deep learning which aligns with current research efforts.  Provides a solid foundation by referencing prior work. Weaknesses:  Novelty is limited as it primarily combines existing concepts.  Clarity could be improved by providing more intuition and details about the implemented method.  Empirical evaluation lacks comparison to solid baselines (e.g. ensembles DDU) on smallscale experiments.  ImageNet results raise questions about DDU performance and could benefit from additional baseline comparison. Other distinction and Questions:  The kernel procedure dimensionality and joint vs. autonomous KDE fitting need clarification.  The claim regarding the lack of a principled uncertainty quantification method in prior work is vague and needs elaboration or removal.  Exploring the impact of bandwidth tuning on OOD detection performance with known datasets would be valuable.  Caption space and reference formatting issue need attention."], "hC474P6AqN-": ["Paraphrase: This paper extends the \"Multitype Disentanglement without Adversarial Training\" (SL21) approach. It employs a modified version of SL21 to train a single model on multiple labeled datasets that encode the same data at varying levels of detail. The model creates a unified embedding that allows for effortless prediction of all labels essentially unifying the different label sets. Experiments on altered versions of the dsprites and foundation face datasets are conducted. On dsprites the model exhibits a more refined representation compared to SL21. effectiveness and Weaknesses:  The paper offers a modest improvement over SL21.  It involves a minor adjustment to the architecture.  Evaluation is limited to a synthetic dataset.  The distinction between a single multidimensional style vector and multiple scalar style values is unclear and the practical implications in the model equations are not wellexplained.  The evaluation results are insufficient to draw meaningful conclusions. They do not significantly outperform the SL21 foundationline and it is uncertain how these models can be applied to other problems effectively.", "Revised argument: This paper introduces a method to merge datasets with distinct categorical labels into a shared latent space. It modifies the VAE architecture proposed by Sha and Lukasiewicz (2021) by splitting the latent space into \"content\" (representing features) and \"style\" (representing labels). The objective is to group similar labels in the latent space while maintaining distinct features. While the proposed method offers minor variations from the original model its technical contribution is limited. The explanations in Section 3 require clarification especially regarding the loss purpose. The paper claims to approximate specific distributions without discussing limitations such as the nonuniqueness of the latent distribution. The experimental evaluation focuses on synthetic datasets and comparison to the baseline model. However it lacks comparison to other methods and evaluation on standard benchmarks. The papers motivation of addressing challenges in emotion recognition is not supported by any experiments in this area. Overall the paper requires additional explanation a broader experimental setup and clarification of its technical contribution to fully demonstrate its potential.", "Paraphrase: The authors propose a method for disentangling multiple factors in a variational autoencoder (VAE). They modify the sampling layer to generate multiple factor representations and weights for each representation. The final representation is then a weighted sum of the factors. The method is evaluated on toy datasets and shows promising results in encoding multiple factors. effectiveness:  Novel approach to disentangling factors in a VAE.  Effective in learning disentangled factors. Weaknesses:  Lack of intuitive explanations for key notations and terms (e.g. PNor s\\hats s).  Equation 4 resembles Bayes theorem but omits the prior probability p(tj). This omission requires clarification.  The evaluation on emotional data is not included despite being mentioned in the abstract and foundation.", "Paraphrased argument: This work introduces a continuous disentanglement variational autoencoder. It builds upon the disentangled variational autoencoder proposed by Sha Lukasiewicz by replacing the single style vector distribution in the sampling layer with a weighted sum of multiple distributions. This change results in a continuous latent space. effectiveness:  Strong correlation between latent variables and the underlying generative factors.  Simple and mathematically sound approach. Weaknesses:  The approach is an incremental upgrade of the disentangled variational autoencoder simply using a weighted sum instead of a single distribution. This concept is often employed in models like Transformers and GANs.  Despite the mention of dataset merging in the abstract and title the experiments only concentrate on individual synthetic datasets and the disentanglement scenario. Demonstrating applications for merging datasets and achieving improved performance would bolster the claims.  The papers writing is unclear in certain parts necessitating multiple readings to grasp the method due to a lack of intuitive explanations."], "jKzjSZYsrGP": ["Paraphrase: Summary: The paper explores the use of Transformer models for longtime series sequence forecastings. Its primary contribution is a novel SCAttention mechanism that replaces the traditional attention performance. By dividing the sequence into equallength segment and computing attention among them SCAttention significantly reduces computational costs. effectiveness and Weaknesses: The papers methodology is intuitive and effectively improves computational efficiency. However there are some concerns: 1. The paper does not adequately explain why segmenting the sequence can significantly improve performance. 2. The performance results in Tables 1 and 2 differ from those reported in the original Informer paper raising questions about potential misunderstandings or inaccuracies. 3. In the ablation field on SCAttention it is unclear if the SCformer with canonical attention is the same as the original Transformer. Additionally its performance is exceptionally beneficial compared to other Transformer variants which requires further explanation. 4. The dual reverse forecasting task while beneficial lacks novelty as similar ideas have been explored in time series and traffic forecasting. 5. The model should be analyzed in greater contingent including the impact of encoderdecoder blocks parameter count and guidance on selecting the optimal segment length (as seen in Table 4). Among these concerns points 1 2 and 3 are the most critical. If these publication can be adequately addressed the papers score may be adjusted accordingly.", "Paraphrased Statement: Summary: This research presents SCFORMER which modifies the Transformers standard attention mechanism with a segment correlation attention. The purpose of this change is to reduce memory consumption in the Transformers scaleddot product attention. Additionally a dualtask approach is proposed where current time series data is used to make predictions about past time series. effectiveness: 1. The paper acknowledges the quadratic time complexity of the Transformer with respect to time steps. 2. The use of a dualtask approach to enhance performance is innovative and the experimental results support its effectiveness. 3. The experiments validate the efficacy of the segment correlation and dualtask methods. Weaknesses: 1. The segment correlation approach is not particularly original as it simply divides long time series into smaller segment and calculates attention scores over those segment. 2. The time complexity formula (O(L2L2seg)) is inaccurate it should be O(L2Lseg) instead. The formula considers the complexity between segment but omits the complexity of each segment (O(Lseg)). 3. In the multivariate setting (Table 5) the Transformer model trained without the dualtask approach outperforms the model trained with it. This suggests that the dualtask approach may not be universally beneficial.", "Paraphrase: Summary: This research proposes a novel Transformer model known as SCformer designed for forecasting long sequence of time series data. The primary introduction lies in replacing conventional selfattention with an effective mechanism called segment correlation attention (SCattention). This modification aims to capture long and shortterm dependencies within the data. Empirical evaluation across multiple datasets demonstrate the efficacy of the proposed method. effectiveness:  Clear organization  Relevance of the problem being solved  Technical soundness of the proposed SCformer Weaknesses:  Modest technical novelty  Insufficient citation of relevant prior process  Lack of clarity in certain aspects of SCformer Specifically the technical novelty of SCformer is questioned due to its similarities with the Autoformer model which also exploits serieslevel correlation. Additionally the authors fail to compare their method against Autoformer and related process. Another point of concern raised is the uncertainty regarding the optimal segmentation length in practice and the unclear input structure of the decoder embedding in picture 1.", "Paraphrased Statement: Summary This paper investigates longterm time series forecasting using a technique called \"segment correlation attention.\" The time series is divided into shorter periodic sequence and attention is calculated for each segment. The final representation is created by combining the selfattention representations of all segment. effectiveness  Explores the use of periodic data for longterm forecasting.  Clear and accessible writing style. Weaknesses  Lacks comparisons to existing methods.  No ablation field to evaluate the effects of segmentation.  Limited introduction as dual forecasting is already used in recurrent neural netprocesss (LSTMs GRUs) for time series forecasting.  Its not explained how the segmentation is calculated or how it differs from previous process (Wu et al. 2021).  Concatenating segmented representations may not be optimal and using the entire sequence should be explored.  Missing ablation field comparing the full sequence to the last segmented sequence.  Lacks comparison to other field on periodic time series forecasting (e.g. Cinar et al. 2018).  Providing an equation or algorithm for the SEG function (Eq. 6) would enhance reproducibility and clarify the papers contribution.  beneficial scores should be highlighted in bold in Tables 3 4 and 5 for consistency with Tables 1 and 2."], "ybsh6zEzIKA": ["Summary: This field investigated how to enhance data for supervised graph classification. The authors created ifMixup a data augmentation method based on Mixup for graph data. They demonstrated that ifMixup is not susceptible to the known problem of manifold intrusion in Mixupbased methods. effectiveness:  The issue of data augmentation for graph classification is intriguing and merits attention.  The paper is clear and easy to understand.  The authors theoretically proved that ifMixup is intrusionfree. Weaknesses:  The improvements shown in the tables are minor.  ifMixup is essentially a direct application of Mixup to graphs which reduces its technical originality.  There is a challenge in using Mixup on graphs due to the lack of node alignment between graphs. The authors random assignment of node indices raises questions about its validity.  The paper contains typos such as mixed usage of \"neighbor\" and \"neighbour\" in image 2.", "Paraphrased Statement: Summary: This field introduces a mixup technique for graphs that claims to be manifold intrusionfree. This is achieved by blending two input graph pairs (incorporating dummy nodes) as per standard mixup principles. effectiveness:  The proposed mixup strategy is straightforward.  The motivation for mitigating manifold intrusion is evident.  The discussion of oversmoothing in Section 4.2.3 and image 5 is wellexplored demonstrating the effectiveness of the mixup method in addressing this problem.  Evaluation results include performance analysis on eight graph classification datasets. Weaknesses:  The proof of invertibility in Section 3.3 assumes binary edge weights (0 or 1).  It is unclear whether the method can handle graphs with realvalued edge weights and still maintain invertibility.  The assumption that lambda is not 0.5 in the invertibility proof may not align with standard mixup practices.  The relationship between invertibility and manifold intrusionfree behavior is unclear. It is not evident how recovering interpolated graphs ensures compatibility with other training samples.  The experiments exclusively employ 5 and 8layer GNNs exploring shallower structure is recommended.  Section 4.3.1 lacks clear link to the proposed method.  The field fails to evaluate the methods effectiveness on node classification tasks where similar mixup techniques have been used successfully.  A comparison with existing graph mixup methods (e.g. GraphMix [1]) is missing.", "Paraphrased Summary: This paper introduces a technique for mixing samples represented as graphs. The process involves: 1. Matching node indices across graphs of different sizes by padding the smaller graph with dummy nodes. 2. Intermixing the features and edge link associated with nodes having the same index from the original graphs. The paper claims that this approach prevents the \"manifold intrusion\" problem where mixed graphs may overlap with other graphs in the dataset. effectiveness and Weaknesses:  effectiveness:  Extends the concept of Mixup to graph data addressing an significant research gap.  Clear writing in most sections.  Weaknesses:  The claim about avoiding manifold intrusion remains unsubstantiated as the method may not prevent the mixed graph from coinciding with existing or future graphs.  Mathematical notation in Section 3.3 could be simplified for clarity.  Node features cannot be fully recovered from the mixed graph and edge weights are lost if present.  Results are marginally better than alternative mixing techniques that operate in the representation space.", "Paraphrase: This paper introduces a straightforward technique for mixing graphs in the input space. Vertices are arranged in any order and dummy vertices are added to equalize the number of vertices in the graphs to be mixed. mixing is then applied to both vertices and edges. This method significantly improves performance surpassing current stateoftheart results on various benchmark datasets. It includes a theoretical guarantee that the mixed graphs avoid a specific issue called \"intrusion.\" The algorithm referred to as \"IntrusionFree Mixup\" (ifMixup) interpolates edges and nodes and assigns interpolated values to labels based on the edges. source graphs can be recovered from the mixed graphs. Instead of assigning class labels to nodes the task is to group each graph under a single class. Dummy nodes which lack edges are added to ensure that all graphs have the same size. The method involves constructing \"soft\" edges by mixing edges between two graphs allowing the network to process these soft edges. The graph mixing performance is proven to be invertible. ifMixup benefits from increased model depth and mixing rates contrary to a baseline model that exhibits overfitting. effectiveness and Weaknesses: The paper introduces a simple and logical algorithm for graph mixup which achieves impressive results on various datasets. The algorithm supports the use of larger model and mixing rates. It suggests further research avenues such as mixing in random layers. The paper makes a valuable contribution to the field. Additional Observations: The improvements from the method are notable and the performance remains stable with increasing mixing rates."], "oU3aTsmeRQV": ["Paraphrase: Summary: This paper introduces SEAT a novel adversarial training technique. The final robust classifier is created by averaging the states of each history model throughout the adversarial training process. The authors compare SEAT to prediction ensemble methods and demonstrate its effectiveness theoretically and experimentally. effectiveness:  The proposed method is well \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0439 and the authors provide numerous experiments to support their claims and demonstrate the performance of SEAT.  The paper is generally clear wellorganized and simple to understand. Weaknesses:  The paper does not thoroughly explain the causes of SEATs performance decay (Figure 2).  The experimental findings are not fully convincing:  The authors should provide more experiment setup detail such as whether the beneficial or last epoch results are reported and what loss work is used for SEAT.  Considering strong components like TRADES trained with varying \\ lambda values in the ablation work would be beneficial.  On Table 1 TRADES appears to be undertrained as its performance against NAT and AA should be higher than 84 and 49.5 respectively. The authors should retrain and reevaluate the baselines.  On Table 2 while SEATCutMix does not consistently outperform all attacks (e.g. MIM) it is unclear whether CutMixs improved performance over AA is solely due to a tradeoff between natural accuracy and robustness.  Evaluating larger datasets like CIFAR100 would provide a more thorough assessment.  The authors should make the training and evaluation code available for transparency.  There are some typos and grammatical errors in the paper suggesting that it was written hastily. discussion Questions:  The authors should investigate the causes of SEATs performance decay and explore potential solutions.  The authors should conduct further ablation work with various combinations of methods (e.g. SEAT by TRADES SEATCutMix by TRADES).", "Paraphrase: This paper presents a training technique called SelfEnsemble Adversarial Training (SEAT) which enhances model robustness by averaging the weights of past models. The authors claim that SEAT creates a smoother loss landscape and improves robustness compared to single models or ensemble predictions from multiple classifier. However they note that traditional selfensemble approach can suffer from learning rate issues in deep training stages. effectiveness and Weaknesses: effectiveness:  SEAT applies selfensemble to model parameters during adversarial training.  empirical experiments and ablation work demonstrate SEATs effectiveness. Weaknesses:  The paper lacks detail on how experimental results were obtained (e.g. training steps average values and variances).  There is an inconsistency in incorporating CutMix in Tables 1 and 2.  The paper explains why traditional selfensemble methods face learning rate issues but does not explore alternative learning rate schedules (e.g. warmup stage).", "Paraphrased Statement: Summary: The authors present a novel approach to enhance a models resilience against adversarial approachs. This method combines adversarial training with model ensembling where ensembling is achieved by maintaining a running average of model weights over time. The authors provide theoretical analysis and evidence that models trained with their approach called SEAT (SelfEnsemble Adversarial Training) exhibit competitiveness with existing adversarial robustness techniques. effectiveness and Weaknesses: Upon reviewing the paper \"SelfEnsemble Adversarial Training for Improved robustness\" I recommend against its publication in ICLR. The primary concerns are as follows: Proposed method Clarity: The proposed method is unclear. The authors claim to employ model ensembling but the algorithm suggests they are mainly augmenting the optimizer (SGD) with a momentum term. Formalism of Theoretical contribution: The theoretical sections use unusual language making it difficult to assess their correctness and relevance. For model Theorem 1 concludes that SEAT \"hardly be approximate to the average prediction of history networks.\" The meaning of \"hardly be approximate\" is unclear. Limited Experimental Evaluation: The experimental evaluation lacks an adaptive approach which could exploit the methods potentially smooth loss landscape. Additionally multiple work should be performed with standard deviation reporting for added reliability. Additional Notes:  Terms such as \"extremely imperceptible\" need clearer definitions.  The introduction oversimplifies adversarial training as \"memorizing\" adversarial models.  The first line of the second paragraph of the introduction contains grammatical errors.  Proposition 1 employs ambiguous wording stating that something is \"almost\" of a certain order of smallness.  The remark after Proposition 1 does not clarify why earlier models are needed if later scope are more robust.  The paragraph after Remark 1 contains a typo: \"uesd\" should be \"used.\" Update After Rebuttal: While the theoretical contribution remain unclear based on discussion with the authors and other reviewers my score has been increased.", "Paraphrased Statement: Summary: This research presents a novel method called SelfEnsemble Adversarial Training (SEAT) that utilizes previously trained models during training. SEAT reduces computational costs compared to traditional ensemble methods by reusing historical model states which reduces the need for training multiple independent models. The work also provides theoretical analysis demonstrating the efficacy of the selfensemble approach. effectiveness: SEAT is a novel ensemble method that reuses historical states to save computational resources. The theoretical analysis effectively explains the differences between SEAT and existing ensemble methods. Experiments demonstrate its robustness against various adversarial attacks across different network architectures. Weaknesses: 1. SEAT employs exponential moving average (EMA) a technique commonly used to enhance model performance. The authors could elaborate on their contribution to EMA. 2. While the work paper overall results under AutoAttack (AA) it lacks detailed findings for each component of AA. 3. The authors claim SEATs efficiency but providing FLOP or throughput metrics would support this assertion. PostRebuttal Response: The authors responses satisfactorily address my concerns. The additional experiments comparing SEAT to vanilla EMA showcase the effectiveness and originality of the proposed method. The inclusion of results for each component of AA demonstrates the methods robustness. The reported FLOPs and running time confirm SEATs computational efficiency. overall SEAT is an innovative and valuable approach supported by thorough theoretical analysis and experimental validation."], "k6F-4Bw7LpV": ["Paraphrased Statement: Summary: This work presents a remarkable phenomenon in deep networks with zero training error. not solely do test accuracy and training accuracy align but accuracy on specific subsets of the training set also corresponds to accuracy on the corresponding subsets in the test set. The authors introduce the term \"distributional generalization\" for this observation. They demonstrate its prevalence across various range datasets. Strengths:  Wellwritten and clear presentation  Novel and significant empirical observation  Formalized explanation and supportive experiments Weaknesses:  Questionable reproducibility of the observation which may limit its significance input and Questions: 1. regression scope: Distributional generalization may be less surprising in regression where a smooth model interpolating training data can naturally exhibit this phenomenon. 2. error bars on plots: error bars in Figure 2B would enhance interpretation. 3. label noise experiment on ImageNet: Results of the label noise experiment on ImageNet (especially nontail classes) would strengthen the works claims. Minor input:  AlexNet Top1 accuracy on ImageNet is 63.3 not 56.5.  The term \"distributional generalization\" may be too solid as it implies zero entire variation between test and train output distribution which may not be true.", "Paraphrased Statement: The paper suggests examining the entire range of classifier behavior rather than relying solely on a single metric like test error. This broader approach can pinpoint the exact field where error occur. The paper investigates datasets with altered label to demonstrate how classifiers respond to such training data. It proposes that the output distribution under \"distinguishable features\" conditions resembles the distribution of true label. While the paper provides convincing model of how some classifiers exhibit this behavior it also notes that this phenomenon is less surprising for \"overparametrized\" networks. The theoretical proof for this conjecture is limited to the nearest neighbors classifier and the paper acknowledges the need for further analysis to generalize this finding to more classifiers and distribution. It suggests future research should focus on the theoretical underpinnings of distributional generalization.", "Paraphrase: Paper Summary:  Introduces the concept of \"distributional generalization\" which suggests that an interpolating classifier produces output distribution resembling the label distribution within certain data subgroups.  Formulates a \"feature calibration conjecture\" that formalizes this concept.  Provides empirical evaluations and mathematical proof for the conjecture for specific nearest neighbor models. Strengths and Weaknesses:  The feature calibration conjecture is intriguing but its significance is questionable.  Generalization involves the convergence of training and test performance which implies comparable performance on data subgroups.  The authors experimental findings regarding subgroup performance discrepancies are not surprising as they are consistent with established generalization bounds.  The absence of MLP experiments in the initial subgroup work (Figure 5) could be improved.  The slight performance differences observed in Figure 6 warrant further discussion or revision of the overall argumentation.  The paper has strengths in its definition and wellconducted experiments but its overall impact is limited."], "srtIXtySfT4": ["Paraphrased Statement: The paper proposes a method for automatically allocating shared parameters among network layers. It employs a \"shape shifter\" network that adjusts the number of parameters in the model. Layers are initially grouped using kmeans clustering with layers within each group sharing parameters. The network generates weights by downsampling or upsampling as needed by each layer. The method is evaluated under resourceconstrained and unrestricted conditions and across various tasks. It is also compatible with distillation and pruning techniques. effectiveness and Weaknesses:  The paper demonstrates the benefits of NPAS over distillation and pruning.  It provides a comparison with Slimmable networks another approach for automatic parameter allocation.  NPAS introduces an additional training cost but the parameter sharing accelerates convergence. Areas for Improvement:  The explanation of the method in Section 3 could be more intuitive by reordering the sections.  Providing algorithm or pseudocode would enhance clarity.  Correcting the typo \"covolutional\" on page 8 would improve the quality of the paper.", "Paraphrase: This research focuses on automating the assignment of parameters to layers within a given budget and generating weights based on the assigned parameters. This approach extends the concept of parameter sharing. The proposed method has shown excellent performance and efficiency on various datasets across different tasks. effectiveness:  Timely and Relevant: Addressing parameter sharing in AutoML which has been largely overlooked.  efficient and efficient: Demonstrated effectiveness and efficiency on multiple datasets and tasks. compatibility with other inference cost reduction techniques. Weaknesses:  Hyperparameter Sensitivity: The method involves numerous hyperparameters and its unclear how sensitive its performance is to these parameters. Additionally its not evident if the same hyperparameter set is used across all tasks and datasets.  Limited range: While the method reduces the number of parameters it may not directly address reducing inference time which is a critical consideration for practitioners. It would be beneficial to explore the applicability of other pruning methods to the network configuration generated by the proposed approach.  Clarification Needed: The proposed methods description in image 2 could be simplified or an alternative explanation provided for beneficial understanding.", "Paraphrased Statement: This research proposes a novel approach to optimize neural networks by strategically allocating parameters across layers (parameter sharing). This technique has the potential to significantly reduce memory consumption and bandwidth which is beneficial for hardware efficiency. effectiveness of the Research:  The concept of parameter sharing is intriguing and addresses a practical demand for parameter reduction.  The proposed upsampling and downsampling methods for parameter allocation demonstrate promise. Questions Requiring further Exploration: 1. While the research demonstrates parameter savings it is significant to evaluate its impact on hardware performance such as bandwidth memory consumption and training speed. 2. The effectiveness of the proposed \"NPAS\" method should be compared to random parameter search as is common in neural architecture search (NAS). 3. The allocation of parameters in depthwise layers (e.g. in EfficientNet) should be further investigated along with any potential differences compared to regular convolutions. 4. It would be valuable to explore whether the optimal parameter allocation strategies vary across different tasks.", "Paraphrased Statement: effectiveness:  extensive evaluation demonstrates the versatility of the approach across various datasets model and tasks.  The proposed mechanism for parameter allocation and automated weight sharing seems innovative and potentially beneficial for generalizability studies. Weaknesses:  The claimed practical advantages of the approach are overstated particularly regarding inference time.  While shared parameters reduce memory requirements the impact on training time and distributed training synchronization is unclear and may be minimal.  The use of parameter count as a metric for model capacity may be inadequate in comparisons with methods that reduce parameters through network removal. Suggestions for Improvement:  Distinguish between \"reduces FLOPs\" and \"reduces memory footprint\" in image 1 and attribute these effects to the corresponding methods.  Acknowledge in the discussion that other methods achieve FLOP and training time reductions while NPAS focuses on memory efficiency.  Provide empirical data on the practical impact of reduced parameter counts on training time and GPU memory usage."], "uwnOHjgUrTa": ["Summary: This research introduces a training technique for quantizing lowbit networks that utilize a multibitwidth approach. During training a temperature parameter and a penalty term are used to guide the network towards the target low bit. Experiments using ResNet 18 and MobilenetV2 on CIFAR10 CIFAR100 and ImageNet Classification datasets demonstrate the effectiveness of the technique. effectiveness:  The proposed multibitwidth training reduces quantization error and improves loss landscape optimization. Weaknesses:  The overall performance of the approach is not satisfactory compared to other lowbit quantization methods (lower accuracy on weightonly and weightactivation 2bit quantization tasks).  Some technical details are unclear such as whether all bitwidths converge to the target low bit and the potential consequences if they do not.  The writing contains grammatical errors and typos.", "Paraphrase: This research tackles the difficult quantization problem of lowbit quantization. It uses a trainable linear combination of high medium and lowbit quantization initially transitioning to solely lowbit quantization at the end. various quantizers and associated attention matrices are employed during quantization to merge the quantized weights or activations. effectiveness:  Clear writing and easytounderstand concept  Thorough ablation work to assess the proposed methods components Weaknesses:  Introduction of additional parameters (e.g. \u03b1) during training increasing computation and memory take. Further theoretical and experimental analysis is needed to explore this.  Multiple quantizers with varying bit widths are used potentially adding to the memory and computation costs of quantization.  compare in experiments are made with corresponding methods using the same n1 bit depth. However the proposed method uses three quantizers with varying bit widths with n1 as the lowest. This compare is considered biased. For a fair evaluation baselines and the proposed method should have comparable computation and memory costs.  Omission of compares with cuttingedge approach hindering a thorough evaluation of the proposed method.", "Summary The paper aims to compress neural networks by improving the binaryrelax quantization technique. During training the precision is dynamically adjusted using a mixbased quantization method guided by a \"temperature cooling\" approach and an \"attention\" vector. The method termed DQA gradually transitions from a mix of quantization precision to a single lowerprecision setting towards the end of training. It is compatible with various quantization techniques. The evaluation focuses on computer vision architectures for image recognition tasks. effectiveness and Weaknesses 1. The paper use of the term \"attention\" is potentially misleading as the attention vector only depends on a trainable parameter and not on the input. 2. The literature review omits stateoftheart neural network quantization techniques such as Permute Quantize and Finetune And the bit goes down and Training with Quantization Noise for Extreme Model compression. 3. The baseline methods used for compare are weak leading to noncompetitive results on the Imagenet ILSVRC 2012 benchmark compared to recent approach. 4. The introduction and related work sections are combined and could be reorganized for clarity. 5. The take that the method supports activation quantization is not fully supported by experiments. 6. The manuscript requires editing for typos and proper terminology.", "Paraphrased Statement: Summary: This research introduces DNN Quantization with Attention (DQA) a technique that employs a mix of high medium and lowbit quantization at the start of training. As training progresses it gradually transitions to using only lowbit quantization. experimental results indicate that DQA consistently outperforms traditional quantization and the BinaryRelax method across multiple datasets and network architectures. effectiveness:  DQAs performance surpasses traditional quantization approach and BinaryRelax across various datasets and networks. Weaknesses:  The paper presentation requires improvement with grammatical errors and typos present.  While the paper compares DQA to BinaryRelax it lacks a wider compare with other quantization methods. Existing research such as LQNets may demonstrate superior performance in comparable settings. Questions and Minor Comments: 1. How is the specific quantization method (e.g. minmax SAWB BWN TWN) determined for function with DQA 2. Why are R18BR and MV2BR results not reported for the ImageNet experiments using ResNet18 3. It would be beneficial to present averaged validation accuracy from multiple training function even if the networks convergence is stable. 4. Observations are requested regarding why DQA consistently achieves better results than the FP version when using SWAB in Table 1."], "irARV_2VFs4": ["Paraphrase: Research Summary:  The work introduces an effective approach for generalizing to all groups even when there are shifts in subgroups and domain adaptation.  extensive model provide insights and numerical experiments demonstrate the methods performance.  The method is straightforward to comprehend and implement and it delivers strong issue. effectiveness and Weaknesses: effectiveness:  Clear understanding and easy implementation.  empirically proven effectiveness.  Wellwritten and intuitive explanations in the model work.  fair Assumption about intergroup interactions. Weaknesses: Algorithm:  Unclear statement regarding the gradients behavior near convergence.  Lack of a formal definition of cos(gigj).  Correction formula affects the gradient despite the claim that it only affects the loss value. model and Experiments:  Misstatement in the experimental setup.  Unclear terminology regarding \"population types.\"  Relationship between the \"worst ratio\" and the effectiveness of spurious correlations is unclear.", "Paraphrased Statement: Summary: This research introduces a novel method for categorization tasks that leverage training data labeled by groups. The approach focuses on enhancing the performance of underrepresented groups while maintaining overall robustness. Compared to an existing technique called GroupDRO this method shifts attention from the group with the largest regularized loss to the group that causes the greatest average training loss reduction. The research includes convergence analysis and thorough comparisons with GroupDRO. effectiveness:  Developing an ERMbased technique that excels across the entire dataset without compromising grouplevel worstcase accuracy appeals to the reviewer especially given the potential for varying levels of label noise across groups.  The paper is wellstructured and presents its contributions clearly with detailed experimental setup data.  theoretical analysis and empirical findings contribute to a deeper comprehension of the novel algorithm. concern:  The reviewer questions whether its necessary to disregard group annotations entirely in scenarios where they are available.  They also inquire about the computational complexity of the proposed method.  The reviewer expresses concern about the methods susceptibility to label noise. PostRebuttal: The reviewer acknowledges the authors response and the revisions made to the manuscript. They appreciate the added need discussion of CGD and inclusion of more datasets which enhance their understanding of the papers contributions. As a issue they support its adoption.", "Paraphrased Statement: Summary: This research presents a novel approach to improving the robustness of machine learning (ML) models against distribution shifts. Existing methods aim to minimize the error for the worstperforming group. However this paper introduces a novel perspective focusing on reducing the average error across all groups. The algorithm proposed in this paper incorporates this intuition and while not minimizing a specific loss work is shown to converge to equilibrium points. Experiments on synthetic and realworld datasets demonstrate the enhanced performance of the proposed algorithm compared to baseline methods. primary contribution: 1. A novel approach to robust ML that optimizes the group with the largest impact on reducing the overall average error rather than the group with the worst error. 2. empirical evaluation on synthetic and WILDS Robust ML benchmark datasets showcase the superior performance and group robustness of the proposed method. effectiveness:  originality of the approach  Quality of the analysis and introduction Weaknesses:  The rationale for choosing the group with the largest average error reduction is not fully explained making the algorithm seem arbitrary.  The issue on synthetic datasets appear somewhat contrived potentially limiting the generalizability of the method.  The benchmark datasets used in Table 3 are modified which raises concern about the robustness of the comparisons.", "Paraphrased Statement: This paper introduces a novel algorithm for scenarios where the test data distribution differs from the training data distribution. In these scenarios there are multiple groups whose presence is known during training but absent during testing and the proportions of these groups vary during testing. Traditional methods such as groupDRO focus on distributionally robust optimization or identifying a classifier that performs well for the group with the worst loss. This paper proposes a different approach concentrating on the group that causes the most significant loss reduction during training rather than the group with the highest loss. The paper presents various synthetic cases showcasing the practical applications of the proposed method. It then concludes with experimental issue on various benchmarks for this setup demonstrating improved performance compared to previous approach. effectiveness and Weaknesses: Pros:  The idea of focusing on the group that leads to the largest overall loss reduction is logical and appealing.  The case of one group having more noise potentially leading to group DROs overemphasis on that group is intriguing. The proposed method offers a sensible solution to this problem.  The synthetic model presented clearly illustrate the uses and advantages of the method over group DRO.  Improved issue are obtained on various benchmark tasks through empirical experiments. Cons:  While the synthetic model are understandable the reason for the methods effectiveness on benchmark datasets is less clear. For instance in spurious correlation settings the imbalance between majority and minority groups could also affect the spurious features loss reduction. The intuition is less evident for the proposed method.  The lack of a fixed loss work optimized by the gradient method makes it difficult to grasp the work.  More explanations for the improved performance on realworld datasets would have been helpful as it is not immediately evident how the benchmark datasets reflect the structure of the toy setups. Questions and Suggestions:  Clearer introduction of the synthetic model would benefit readers (e.g. figures illustrating the data distribution).  Clarification on whether the proposed method optimizes a specific loss work or not.  More insights into the choice of scaling the gradient by li(\\thetat)p including the rationale for depending on the loss value and the analysis in Theorem 1s consideration of this scaling."], "kcrIligNnl": ["Paraphrase: Summary:  The authors propose a method that can effectively generate molecule conformation yielding competitive issue on four datasets.  Unlike conventional methods that focus on distance matrices (which may lead to invalid molecular structures) this approach predicts atomic coordinates directly.  The loss function is rotationinvariant a key characteristic of many molecules. strength:  The issue are impressive and the methodology is wellexplained.  The ablation studies in Table 5 provide valuable insights. Weaknesses:  Some discussion could be expanded such as the statement regarding directly generating coordinates for complex molecules.  The comparisons are predominantly with ConfGF additional evaluation would be beneficial.  more details on the hyperparameters could be provided in an appendix. Comments:  Fig. 3 does not specify the meaning of colors or whether it shows different position.  Some alternative methods such as GSchNet also avoid violations of the triangle inequality.  The meaning of COV() being 100 in Table 2 is not clarified.  Property predictions for QM9 could be expanded to include more properties.  The text can be challenging to follow in position.  The \"ours with FF\" row in Table 5 is not clearly explained and why it is not the default conformation.", "Paraphrase: Summary: The authors present a generative framework that uses a variational autoencoder (VAE) architecture to construct threedimensional (3D) molecular structures as a position of coordinates. Unlike previous methods that rely on generating distance matrices and solving a timeconsuming distance geometry problem this framework directly generates 3D coordinates through an iterative process within the decoder. strength and Weaknesses: Originality: The framework lacks originality as it shares similar concepts with ConfVAE and another unnamed framework described in references [1] and [2]. These frameworks also use an autoencoderbased framework for conformation generation an iterative generation function and a RMSD objective. However the novelty lies in the further graph convolutional neural network architectures used in the encoder and decoder. The baselines compared against exclude both [1] and [2]. Clarity and choice: The authors clearly state their intentions and provide insightful discussion and ablation studies. However the papers \"Contributions\" section is ambiguous and the \"Related function\" section omits pertinent references. Its dense inline equations and inconvenient formatting make comprehension challenging. The excessive focus on minor technical details could be addressed by transferring them to supplementary materials. Significance: The frameworks novelty is minor. While experiment issue indicate improved metrics compared to the ConfGF baseline the lack of standard deviation makes it difficult to assess the significance of the performance gap. The absence of significant baselines [1] and [2] also raises concerns about the significance of the findings. Drawbacks and Questions:  Using alternative Greek letters for encoderdecoder components would enhance readability.  Ablation studies should include loss function variations as [2] introduces a distancedifferencebased loss that also exhibits rototranslational invariance without requiring additional problemsolving.  Smallscale dataposition issue are unnecessary as the largescale dataposition encompasses them. Consider training CVGAE and CGCF on the larger dataposition.  The color scheme in picture 3 is deceptive. An additional picture showing aligned conformation clouds would demonstrate the generated conformations diversity.", "Paraphrased Statement: Summary This research paper introduces a direct technique for generating molecular conformations based on the molecular graph. Inspired by AlphaFold2 the researchers demonstrated the ability to directly output 3D coordinates for minor molecules rather than using the traditional approaches of creating intermediate interatomic distance matrices or iterative strengthbased methods. The method shows good performance on the standard QM9 benchmark dataset and outperforms previous baseline methods for larger molecules. strength and Weaknesses The function represents an improvement over the Langevinbased method it primarily compares to. However its unclear whether this improvement is due to different parameter settings (e.g. a larger framework) or a genuine algorithmic advancement. The authors did not provide a detailed explanation of how the algorithm specifically benefits larger molecules or why it could not surpass the previous framework on minorer molecules. While the authors provided documentation of performance differences between methods in Table 3 it would be beneficial to see their new framework (or ConfGf or both) modified to a significantly different size. The contribution of the finegrained loss function functiond in training is not fully understood. The particular matching loss is not invariant to permutations of symmetric atoms potentially leading to inconsistencies in loss values for equivalent molecular structures. section 4.3 of the paper presents a table of average energies of generated conformers which are referred to as \"property predictions.\" However its argued that there is no experimental method to measure this specific quantity and the Boltzmannweighted average of energies would be a more meaningful physical property. Additionally the function of an MMFF strength field for optimization followed by evaluation with Psi4 is not fully explained. The units of energy in Table 4 are also unclear and could significantly impact the reported errors. Finally the first two future research directions are not clearly explained specifically the combination of the method with flowbased or Langevin dynamics frameworks. case of case that currently fail in the framework would also be helpful for understanding its limitations.", "Paraphrased Statement Summary The paper introduces a new generative framework based on Variational Autoencoders (VAE) to create molecular conformations from graphs. The formulation and training goals are akin to existing VAEbased frameworks with the primary innovation being the decoders architecture. The decoder inspired by AlphaFold consists of successive identical update blocks that iteratively modify nodeedge embeddings and coordinates. These blocks utilize further graph neural networks. strength  Clear and wellorganized paper.  Comprehensive experimental evaluation comparing the framework with benchmarked methods on minor and large datasets.  issue show the framework is superior or on par with stateoftheart approaches while requiring less computational resources. Weaknesses  Originality concerns: The decoders innovation closely resembles the \"structure module\" of AlphaFold.  Inaccurate problem definition: The paper initially suggests that each graph corresponds to a unique conformation which is incorrect.  Questions about decoder block innovation: While invariant training objectives are utilized it remains unclear if using equivariant networks for coordinate updates would enhance performance."], "iEx3PiooLy": ["Paraphrase: Summary: This work expands on the Where2Act (ICCV21) method for static motion generation with 3D articulated objects. The extensions include: 1. Generating longterm process trajectories by learning from data generated through reinforcement learning (RL) exploration. 2. Conditioning process trajectories on taskspecific data. Strengths and Weaknesses: Pros:  Addresses the understudied problem of longterm process trajectory generation for 3D articulated objects.  Wellwritten paper. Cons:  The method is primarily an extension of Where2Act.  The combination of Where2Act and curiosity direction for RL policy for interactive trajectory exploration is not very novel.  The paper does not compare with a baseline for the curiosity direction component in the context of trajectory generation.", "Paraphrased argument: This research addresses the challenge of manipulating objects (often furniture like cabinets) by training artificial intelligence to identify and execute visual process trajectories for interacting with them. The system analyzes point cloud scans of objects and generates scores indicating the feasibility and likelihood of different approach. It has been tested in simulations and on realworld objects demonstrating its effectiveness. Key Innovations and Comparison: The main innovation compared to previous work (Where2Act) is that this system produces motion paths instead of just grip orientations. This necessitates variety in the framework but provides superior performance in this specific task. The modifications including curiositybased exploration and a method for selecting trajectories have been evaluated and shown to be advantageous. Strengths:  Clear and straightforward explanation of concepts and ideas.  detailed appendix provides additional data to clarify the approach. Minor Considerations:  The term \"step\" is used to refer to both motion waypoints and time steps. Clarifications would be helpful.  The paper could crossreference previous work on which the trajectory Proposal module is based (e.g. cVAE).", "Paraphrase: The paper introduces a method for exploring 3D environments that involves:  Collecting interaction data using reinforcement learning (RL) while balancing extrinsic (taskrelated) and intrinsic (curiositydriven) reinforcement.  Training visual framework that predict potential outcomes and guide intrinsic reinforcement. However the paper has two main limitations: 1. The connection between visual perception and RL policy is weak as the RL policy receives only exploration reinforcement for interacting with field predicted to have low success probability by the visual framework. This may not be an effective exploration strategy as these field may genuinely be unsuitable for successful process. The paper should consider incorporating the certainty of the visual predictions into the exploration process. 2. The baseline RL approach appears to train a single policy across multiple tasks using point cloud input. This approach is potential to underperform becautilization it does not account for taskspecific variations. A more appropriate baseline would be to train separate RL policy for each environment and utilization paradigm inputs directly similar to the \"asymmetric actor critic\" setup."], "h4EOymDV3vV": ["Paraphrased Summary: The paper introduces a new method for creating representations using generative models. It enhances nonadversarial scorematching techniques to control the detail level of representations and create an infinitedimensional code. This is especially useful in semisupervised learning where pretraining with the proposed model improves class separation and achieves stateoftheart performance. intensity and Weaknesses:  The paper is clear and the method appears reliable.  Scorebased generative models offer an alternative to GANs and VAEs for representation learning.  Pretraining with the proposed method significantly improves semisupervised classification results especially on smaller datasets like CIFAR10.  The results for the generation task are not as impressive but they are not the main focus of the paper. Corrections:  Page 1: Define \"SDE\" before using it.  Page 3: Correct the typo \"Denoising\" to \"Densoising\".", "Paraphrased Summary: Researchers introduce a novel approach for training representations using diffusion probability models (DDPMs). The estimator receives the encoded original image as input and the DDPM loss function prompts the encoder to extract features necessary for image restoration. This representation can vary in nature (deterministic stochastic pointwise or infinitedimensional). Preliminary experiments demonstrate the encoders ability to learn meaningful features and comparisons with LaplaceNet show enhanced performance in semisupervised classification. intensity and Weaknesses: intensity:  Innovative approach: The approximation is novel and wellmotivated. Weaknesses:  Incomplete literature review: Related work in representation learning are missing making it hard to assess the context.  limited computational resources: Restrictions are mentioned but not specified.  selection of baseline: The baseline functiond for quantitative evaluation is unclear missing opportunities to compare against established methods.  Infinitedimensional representations: The practical application of these representations is not explained.  Ambiguity in representation selection: optimal timestamp selection suggests that the final representation is still pointbased raising questions about its function in downstream tasks. Questions and Suggestions:  \u03bbdivergences and fdivergences: Clarify how \u03bbdivergences can express any fdivergence.  Uniform and Gaussian sum motivation: Provide the rationale behind using this combination in the model.  Decoder equation correction: Correct the decoder definition in Equation 23.", "Paraphrase: Summary: This paper presents a new approach for learning representations using scorebased generative models by combining different denoising score matching loss function. The method utilizes an encoder conditioned on time resulting in an infinitedimensional representation. Experiments show that pretraining classifiers with the proposed method improves results compared to existing semisupervised learning techniques. intensity:  Simplicity and ease of function  Improved performance over baseline methods Weaknesses:  The approximation of representation learning with denoising score matching is not novel similar concepts have been explored in [2 3 4].  Inconsistent function of \"diffusionbased\" and \"scorebased\" terms throughout the paper suggesting \"scorebased\" may be more appropriate given the focus on denoising score matching losses.  Proposition 1 only restates a known result that denoising score matching is equivalent to the Fisher divergence rather than presenting a significant contribution.  The claim in Section 3 that diffusion models are trained to minimize KL divergence is inaccurate as this only applies to the likelihood weighting of score matching losses.  The maximum pairwise distance of CIFAR10 image is reported as 170 contradicting other work that found it to be around 50.  The adversarial training of \u03bbdivergences is not wellfounded as the unknown constant in denoising score matching can vary with \u03bb. Related work [1] should be cited and discussed.", "Paraphrased Summary: This research introduces an improved scorebased model for learning representations. It incorporates a latent code derived from the original data into the score network potentially boosting representation quality. The authors employ an encoder to generate this code potentially influenced by temporal information. Their approach surpasses existing semisupervised learning techniques. Notably strategies such as adversarial training and noise schedule optimization enhance the models performance. intensity and Weaknesses: intensity:  The concept of incorporating a latent code into the score network is intriguing and potentially impactful.  The paper is clearly written and wellreasoned. Weaknesses:  Missing detail: necessary formulation and implementation specifics are absent such as the precise definition of the \"VDRL\" algorithm.  limited Experimental information: The semisupervised learning experiments lack key detail such as the timepoint used for latent code extraction and whether the enhancements mentioned in Section 3 were applied.  Contribution Questions: The experiments suggest that the original LaplaceNet algorithm may contribute slightly more to performance than the encoderbased approach. A more comprehensive experiment directly leveraging the encoders features could provide clearer insights.  Sections 3 Relevance: The content in Section 3 appears unrelated to the paper focus on representation learning and could be omitted in favor of additional detail on the main topic."], "kxARp2zoqAk": ["Summary The authors present a framework for enhancing time series data using contrastive learning. The framework adaptively selects data augmentations based on their fidelity and variety. contrastive learning objectives are applied at both local and global levels. The frameworks performance is evaluated on time series forecasting and classification tasks across various datasets. effectiveness  Novel approach to selecting data augmentations for time series data.  extensive experiments on different datasets and benchmark frameworks.  Wellwritten and structured paper. Weaknesses Unclear Definitions and Justification  The definition of pseudolabels is not clear.  The justification for adding the label of the full batch (yB) to the framework is not fully explained. Incomplete analysis and ablation  More complete analysis and ablation are needed to validate the conclusions drawn from the results. Incomplete Benchmark framework Explanation  The paper does not provide sufficient information about the benchmark frameworks used making it difficult for readers unfamiliar with them to understand their differences from the proposed framework. Missing Benchmarks and Causal Conclusions  The wellknown contrastive learning method contrastive Predictive Coding is not mentioned or used as a benchmark.  Causal conclusions cannot be drawn from the presented results regarding the reasons for the frameworks superior performance. Minor Issues and Typos  Typos in the text.  Figures could be referenced earlier in the text.  Clarification of batch and minibatch usage.  Distinction between cutout and subsequence augmentation.  Handling of timewarpinginduced variety in window sample size.  Missing leading zeros in Table 5.", "Paraphrased Statement: This paper presents a method for learning representations from time series data that considers the information capacity of the series. It focuses on developing effective data augmentation strategies and uses information theory principles to formulate two key optimization criteria: high fidelity and high variety. empirical results demonstrate improvements on various time series forecasting and classification tasks compared to existing approach. Detailed Concerns: 1. high fidelity:  While the criterion assumes label information the approach uses both actual labels and pseudo labels.  Properties 1 and 2 rely on the assumption of a large number of classes which is not always practical.  The relaxation to batchsized pseudo classes weakens the robustness of these properties. 2. high variety:  The technical correctness of seeking high variety by minimizing the mutual information between augmented and original data is questionable.  Minimizing the negative InfoNCE only guarantees a lower bound on mutual information not minimizing it directly. 3. Metacontrastive Learning:  The positive and negative neighborhoods used in the localwise contrastive loss need further clarification.  The fusion scheme appears heuristic and its connection to meta learning needs justification.  The use of the term \"metalearner network\" for the proposed architecture needs explanation. 4. experimental Results:  ablation results suggest that fusion schemes and random augmentations yield similar performance.  Using label information for augmentation selection in InfoTSs may not provide a fair comparison with other baselines that do not use labels.", "Paraphrase: This paper makes valuable contributions by combining empirical data with theoretical principles. In time series analysis data augmentation is challenging because it risks distorting the time series. Unlike image data humans cannot visually detect these distortions. To address this the paper presents:  A novel data augmentation approach using information theory.  A metalearning framework to optimize data augmentation selection.  A method for selecting optimal augmentation for contrastive learning. The paper provides extensive experimental evidence and theoretical analysis to support its approach. While no significant weaknesses or limitations are apparent work time and complexity information could be included.", "Paraphrased Statement: Summary: This paper introduces InfoTS a technique for enhancing contrastive learning of time series data by utilizing learned augmentations. Its innovative approach employs ConcreteGumbelSoftmax distributions to create a learnable augmentation strategy. The paper demonstrates the method effectiveness through empirical tests on various time series forecasting and classification benchmarks. effectiveness and Concerns: Concerns:  Eq. (3): Dismissal of the first term H(v) as meaningless noise raises concerns about the assumptions robustness.  Properties 1 and 2: Their trivial nature could be addressed more effectively in the paper.  Metalearning vs. Hyperparameter Tuning: The paper is more accurately described as a hyperparameter tuning method rather than a metalearning approach.  Error Bars: Plots with error bars are lacking in experiments making it difficult to evaluate statistical significance in some cases.  Table 3: Close results suggest potential lack of statistical significance especially when removing components from the metaobjective.  Benefits Justification: The paper complexity may not be adequately justified by its performance improvement. Figure 8 and Table 5 indicate a heavy reliance on the subsequence policy resulting in similar results to using only subsequences. Minor Comments:  Typographical and grammatical errors throughout the paper (e.g. \"the information theory\" \"important weight\" \"combing candidate\").  Confusing language: \"Augmentation\" refers to the original sequence v while \"g\" represents the mapping of x.  Inadequate Reference: The reference to Tishby et al.s information bottleneck paper is not suitable for the general concept of \"information theory.\"  Deterministic Nature of g: The paper suggests v is probabilistic while later sections imply g is deterministic.  Transformation Examples: Providing examples of transformation ti would enhance understanding of the method.  Table 3 Inconsistency: Bolding is inconsistent with Ly24wo LocalMAE and Ly48wo variety(MSE MAE) missing bold text."], "nO5caZwFwYu": ["Paraphrase: Summary: This field explores the use of deep learning techniques to address combinatorial optimization issues. According to the authors advanced methods often employ encoderdecoder models. Encoders generate embeddings of problem instances which decoders then use to iteratively construct solutions. The paper investigates how to rapidly update model parameters for test instances using a pretrained model to enhance solution choice. Three techniques are proposed for modifying (1) static problem embeddings produced by the encoder (2) weights of problemspecific residual layers added to the decoder and (3) parameters of a lookup table affecting probability distributions. Strengths:  This research focuses on the promising field of machine learning for combinatorial optimization where it has the possible to bring about significant advancement.  Experiments indicate that the proposed methods are considerably faster than competitor methods such as active search (Bello et al. 2016) which adjusts all model parameters at test time. In contrast the presented approach only modifies specific parameter subset resulting in improved efficiency.  The authors evaluate their method on several combinatorial optimization problems including routing and scheduling issues. Notably improvements over active search for the scheduling problem are more modest. Weaknesses:  The problem description could be more concise. A clear explanation of \"process\" and their impact on state st1 after application would be beneficial.  The solution choice improvements over problemspecific baselines are occasionally little (e.g. less than 1 in table 1 and 2). Confidence intervals would provide large confidence in the superiority of specific methods. detailed Comments:  Page 2: The term \"exemplary\" may not be suitable consider removing it.  Equation (1): The authors should clarify the purpose of b0 as a variancereducing baseline.", "Paraphrase: Summary: This paper explores machine learning techniques for solving complex optimization problems known as combinatorial optimization. It builds on previous process by Bello et al. (2016) which used reinforcement learning to generate solutions for such problems (such as the Traveling Salesman problem). The key foundation in this paper is the optimization of only a subset of the models parameters rather than the entire set. Three different implementations based on this idea are proposed. Strengths and Weaknesses: A significant challenge with existing reinforcement learning approaches for combinatorial optimization is their computational expense. This paper aims to address this limitation by optimizing only a subset of the models parameters. The paper is wellwritten and provides detailed discussion of related research and clear introduction of experimental results. However there are some minor ambiguities in presenting the proposed techniques. Novelty: This paper extends the active search method proposed by Bello et al. (2016) with the novelty lying in the optimization of only a subset of the models parameters. The simplicity of the proposed technique could make it appealing if it demonstrates good performance. introduction: The paper is wellwritten and provides detailed discussion of related research. The experimental results are presented clearly including ablation field and trajectory analysis. However there are some minor ambiguities in describing the proposed techniques such as the lack of explicit definitions for the JRL and JIL loss functions mentioned in Figure 1. Limitations: 1. Ambiguities in the definition of JRL and JIL loss functions. 2. The lack of CPU time reporting in the experimental results. It would be helpful to provide both wallclock time and CPU time to clarify the source of performance improvements. Minor Comment: The xaxis label in Figure 3 should be corrected to \"lambda.\"", "Summary The paper proposes a new method for improving combinatorial optimization heuristics called Efficient Active Search (EAS). EAS extends an existing active search method by updating only specific contribution of the model parameters during testing. The paper applies EAS to two different optimization models for three different problems and demonstrates significant improvements over the original models on instances of the same size and larger. Strengths  Clear and wellwritten paper  The proposed approach is applicable to any constructive optimization method with an encoderdecoder architecture  EAS consistently improves performance across multiple models and problems  It addresses the challenge of learningbased models performing poorly on larger instances  A thorough discussion explores possible reasons for the efficientness of different EAS variants on different problems Weaknesses  Experiments are limited to instances with a maximum of 200 nodes while recent methods can handle much larger instances  Generalization is only tested with respect to instance size not other distribution shifts  For one problem results are provided for different instance distributions but each model is trained for three weeks on each distribution. Its unclear if EAS would even be beneficial if the model was trained on a single distribution. Recommendation The paper should be accepted. While EAS has limitations in terms of instance size and generalization it demonstrates a promising approach for improving optimization heuristics and has the possible to be applied to a extensive range of problems. Questions  Why is a new layer added for finetuning (EASLay) instead of finetuning existing layers  Why are there no introduction for most learningbased baselines in table 1 and 2 for N \u2265 125  Has EAS been applied to a model trained on a uniform distribution for CVRP100  How does the scale of different losses affect the optimal value of lambda  Can a rule of thumb be derived from the experiments to predict which EAS variant would be most efficient for a given problem", "Paraphrase: The paper introduces a new method that involves updating portions of a deep neural network while optimizing combinatorial problems using reinforcement learning during the search process. It demonstrates that this selective update approach improves results while reducing computational costs. The proposed method is evaluated across several combinatorial optimization problems and compared with other machine learning approaches and traditional solvers demonstrating its effectiveness. Strengths and Weaknesses: This paper presents a promising concept with significant results. The evaluation is thorough and impartial and the findings are compelling. It is a wellwritten paper worthy of acceptance. However a few areas could benefit from further clarification:  Table 1 results show that \"concorde\" is often faster than other solvers which is surprising compared to \"LKH.\" This issue requires explanation to enhance the readers understanding.  The most uncertain aspect of the proposed method is the placement of the new layer. It seems complex to implement in practice and may necessitate costly evaluations of several choice. A more detailed discussion of how the authors made this decision for their experiments along with recommendations for doing so in different context would enhance the paper practical application."], "pETy-HVvGtt": ["Paraphrase: Summary The work employed partial information decomposition (PID) to explore interactions between multiple latent variables. PID revealed that:  Mutual information between a latent variable and a generative factor can be decomposed into unique redundant and synergistic information components.  The uniqueness component signifies the degree of variable disentanglement. The researchers developed a disentanglement metric based on modified mutual information gain (MIG) and evaluated VAEbased models on two datasets. effectiveness  Applying PID for disentanglement analysis is innovative.  The proposed metric addresses limitation of existing metrics in capturing multivariable interactions.  Comprehensive supplementary material supports reproducibility.  The paper is wellwritten and clear. Weaknesses  Quantitative or visual support is lacking for the claim that existing metrics can miss higherorder entanglement.  Simulated entanglement attacks may not represent common scenarios in disentanglement learning. Demonstrating realworld examples would be beneficial.  experimentation solely relied on unsupervised VAEs with limited settings raising concerns about reliability and generalizability. The authors should explore results under different architectures and hyperparameter configurations.  Enforcing nonnegativity in the metric using the max0 operation may require theoretical clarification.  The results for JointVAE using the proposed metric warrant further explanation. additional investigation into the total correlation terms and comparisons with other discretevariable VAEs would be valuable.  Qualitative results (e.g. latent traversal visualizations) would aid in visually understanding feature space changes.", "Summary Paraphrase: This research introduces a new metric for quantifying data disentanglement based on the Partial Information Decomposition (PID) principle. Additionally it presents two \"entanglement\" attacks to demonstrate the effectiveness of the proposed metric. effectiveness:  The paper tackles a significant problem in machine learning.  The proposed metric aligns with established principles. Weaknesses:  The paper lacks citations and discussion of previous research.  The writing could be improved for clarity.  The experimental validation is limited to small datasets and does not fully showcase the metrics value.  The proposed metric may be less practical than existing alternatives.", "Paraphrase: The paper presents a theoretical approach using information theory to analyze the separation of features in latent representations. It employs the Partial Information Decomposition (PID) framework to study the interactions between latent factors and their correlation with the observed data considering more than two latent variables. The authors propose a generalization of the Mutual Information Gap (MIG) metric to multiple latent variables by dividing the mutual information into unique redundant and complementary information components. They argue that current pairwise correlation analysis approaches mistake some redundant information as disentanglement when it should be considered entanglement. additionally they establish bounds for the partial information terms and show the limitation of existing disentanglement metrics (BetaVAE FactorVAE and MIG) in quantifying multivariate correlations in the latent space. To evaluate the performance of these criterion the authors introduce two \"entanglement attacks\" that inject correlated noise into the latent representation. experimentation on the dSprites and 3dshapes datasets demonstrate that the proposed metric effectively reveals the decrease in disentanglement caused by the attacks while other metrics fail. effectiveness:  Novel approach using PID to generalize the information gap concept for multiple latent variables.  Derivation of upper and lower bounds for the partial information terms.  Wellstructured and written manuscript.  empirical verification of the metrics effectiveness through entanglement attacks. Weaknesses:  lack of detailed ablation work beyond the entanglement attacks.  No analysis of the robustness of the partial information criterion.  Absence of traversal analysis to provide visual evidence of disentanglement changes.  Insufficient coverage of related work particularly the work by perform and Tran (ICLR2020). additional Questions:  How does sampling affect the disentanglement metrics for each VAE model  Was the sensitivity of partial information calculation to the number of samples studied  What types of entanglement attacks might not be detectable by the proposed criterion  Why are the upper and lower bounds for partial information particularly close for JointVAE  represent both continuous and discrete variables considered in the disentanglement analysis  explanation of the large batch size for TCVAE in the experiments.", "Paraphrased Statement: Summary: This work presents a new metric for disentanglement called Unibound. It calculates this metric using partial information depaper which breaks down the mutual information between latent variables and labels into specific information terms with distinct roles:  Redundant information  Unique information  Complementary information Each term is clearly defined with intuitive explanation. UnibBound similar to MIG is constructed using these terms. However UnibBound employs an improved paper that best captures disentanglement and resists adversarial representation attacks. effectiveness and Weaknesses: effectiveness:  Clear and wellwritten explanation for both theory and experiments  Intuitive and welldefined information terms  Improved paper in the UnibBound metric best capturing disentanglement concepts  Demonstrated effectiveness against adversarial attacks by theoretically showing faster convergence against redundancy attacks in a toy model  Outperforms existing metrics in resisting redundancy attacks in experiments on real datasets  Provides detailed analysis of disentangled methods based on the depaper of information terms Weaknesses:  The assumption that the inference distribution and its marginals are tractable may be limiting  Some notational inconsistencies that could be improved"], "qpcG27kYK6z": ["Summary This paper introduces:  A concentric sphere representation for 3D point clouds.  Icosahedronbased spherical CNNs based on this representation. contribution 1. volumetric feature Learning: The concentric sphere representation enables learning feature within 3D volumes. 2. Rotation Equivariant and Scalable Computations: Two types of convolutions (intras and intersphere) are combined to achieve rotation invariance and scalability. Strengths 1. Multiple resolution Spheres: Using spheres with varying radii may enhance feature learning compared to previous approach using a individual sphere. 2. Extension to spherical CNNs: The multiple resolution approach common in image deep learning is applied to spherical CNNs for the first time potentially leading to advancements in 3D point cloud work and spherical CNNs. Weaknesses 1. Rotation Equivariance explanation: While the method is claimed to be rotation equivariant a beneficial explanation of the proposed and related methods including why some are not rotation equivariant would be beneficial. 2. Missing resolution: resolution for ModelNet40 from Cohen et al. 2019 are missing in Table 1. 3. DOS Experiments:  Clarification is needed on the parameters RC used in experiments (e.g. meaning of R1 and C32).  The loss work weights (\u03b1  0.1) imply less emphasis on loss(FDOS) it would be informative to see resolution with \u03b1  1.0.  The authors should clarify if it is fair to compare their method to others using an additional loss term.  It is not clear why simplified versions (R1) perform well on DOS experiments could this indicate that DOS estimation is too simple 4. spherical CNNs Baseline: The baselines used in DOS experiments do not include several noted SOTA spherical CNNs such as Gauge Equivariant Convolutional Networks and the Icosahedral CNN.", "Paraphrase: This research presents a method for representing 3D space using a series of nested spheres each discretized by a highly regular icosahedral grid. The method involves applying separate convolutions within and between the spheres which are then combined into a framework that can learn both volumetric and rotationally invariant representations from point cloud data. effectiveness:  Introduces a novel volumetric representation based on concentric spheres discretized using icosahedral grid.  Proposes a convolutional framework that combines intrasphere and intersphere convolutions to extract volumetric representations.  The convolutions are rotationally invariant and scale linearly with grid resolution.  Demonstrates promising resolution in 3D object classification and the analysis of atomistic systems. Weaknesses:  The paper lacks detailed and comprehensive evaluations of 3D object classification performance.  It does not adequately reference relevant literature in the field of 3D point cloud deep learning.  Sidebyside comparisons with leading methods on standard benchmark datasets are missing.", "Paraphrase: Summary: This research introduces a novel neural network for processing spherical data called a Graph Convolutionbased Multiscale Spherical Deep Neural Network (CSGNN). It employs both spherical and radial convolutions to capture point at different scales. A simplified architecture is proposed to combine the extracted feature. The method is tested on classifying point clouds under various transformations and predicting the electronic state density of graphene structures. effectiveness:  Enhances accuracy by incorporating multiscale feature using radial convolutions.  Clear and concise presentation.  Relevant experimental use cases across multiple domains. Weaknesses:  The approach can be considered more akin to a multiresolution Spherical CNN rather than a distinct method.  The concept of volumetric feature is not fully explained or justified.  Lack of detailed description of the rotational invariance capabilities of the convolutions.  Separation of radial and angular convolutions may lead to information loss.  The impact of discretization artifacts introduced by spherical representation is not discussed.  Potential for pruning based on radii is not explored.  Consideration of the minimum number of concentric spheres for effective feature extraction is missing. Minor number:  Grammatical errors (e.g. \"over over\")  Lack of normalization point for the point sets  Clarification on the use of connections in Fig. 5", "Paraphrase: Summary: This work addresses the difficulty of learning representations from 3D point clouds that can adapt well to various orientations. The proposed CSGNN employs a convolutional framework to learn from concentric spherical feature maps incorporating both withinsphere and betweensphere information. The approach is effective and wellpresented demonstrating its practicality in spherical representation learning. It has some areas for improvement. effectiveness: 1. The work introduces a novel volumetric representation for convolutional learning using nested spheres discretized by an icosahedral grid. 2. The CSGNN combines intrasphere and intersphere convolutions to learn volumetric representation over concentric spheres. 3. CSGNN is effective in classifying arbitrarily rotated 3D objects and in describing molecular environments. Weaknesses: 1. The architecture lacks novelty following a model of hierarchical graph convolutions followed by radial convolutions. performance is limited by the radius size (number 1 Table 2). 2. Calculating the contribution of all points to vertices is timeconsuming and redundant. The reviewer suggests considering how to select effective points or avoid redundancy in converting point clouds to concentric spheres. 3. The method has several adjustable hyperparameters (RBF threshold number of concentric spheres input channels kernel size). Its robustness is unclear. 4. The calculation of DOS and FDOS loss point requires further explanation. 5. The experimental resolution fall short of stateoftheart performance in some cases. It would be beneficial to discuss any failure cases encountered."], "uPv9Y3gmAI5": ["Paraphrased compact: This research explores a technique for compressing neural network weights using a modified approach to the traditional method of Singular Value Decomposition (SVD) on weight matrix. The proposed method utilizes a weighted SVD objective. The weights are calculated based on the Fisher information an approximation of the gradients average squared norm over the dataset distribution. The method simplifies computation by assigning a uniform weight to each row of a weight matrix. The Fisher information calculation assumes that parameters are independent of each other. The proposed technique has demonstrated superior performance in language model compression experiments. It effectively reduces the size of both taskspecific and precompressed models. effectiveness and Weaknesses: effectiveness:  Strong experimental evidence supporting the proposed methods superiority over existing approaches.  innovative weighting scheme for SVD in lowrank weight compression. Weakness:  The introduction could benefit from an other reference to \"weighted SVD.\"  study referencing related work on compressing trained neural networks that extend beyond minimizing the Frobenius norm difference in weights.  For a fair comparison consider optimizing hyperparameters for each scope (if computational resources allow).  The tables beneficial performer was not highlighted in line 3 (the original compact model).", "Summary This paper introduces a fresh method to compress weight matrix in machine study such as those found in deep neural networks (DNNs). One traditional approach is to use truncated remarkable rate decomposition (SVD). While SVD minimizes the Euclidean distance between the original and compressed matrix it doesnt always lead to beneficial performance on tasks as the truncated portion may yet contribute to the models effectiveness. The authors propose an alternative optimization problem that considers empirical Fisher information. Experiments on various datasets indicate that their method achieves beneficial results than using truncated SVD alone. effectiveness and Weaknesses  effectiveness: The proposed method can achieve beneficial results than truncated SVD.  Weakness: The computational cost and guarantees of the proposed method are not fully explored. Additional Comments  The paper language needs improvement.  The authors should provide more details about the following aspects:  Computational cost of generating finetuned models using the proposed method versus traditional SVD.  Guarantees or limitations of the proposed method in terms of task performance improvement.  The impact of the proposed method on model overfitting and training time.  Figure 6 needs clarification particularly regarding the relationship between reconstruction error and classification performance.  The authors should consider adding experiments that demonstrate other potential benefits of the proposed method such as improved speed or efficiency. Minor Comments  Equation (1) should use the exact equality symbol \"\" instead of \"\u2248\" since W is rank k.  The authors state that they propose \"making the same row of the W matrix to contribution the same importance.\" They potential mean that each weight in row i of W will use the average weight of that row.", "Paraphrased Statement: compact SVD a technique applied to matrix in language models aims to reduce model size. yet instead of using standard SVD which minimizes reconstruction error a modified version is employed that assigns weights to matrix elements. The importance of each element is estimated based on gradient magnitudes. For computational simplicity average row weights are used instead of element weights. effectiveness and Weaknesses of the approach:  effectiveness:  No need for retraining  minimal performance drop on various tasks  Applicable to pretrained models  Weaknesses:  Modest parameter reduction compared to retraining methods (e.g. ALBERT)  Incomplete results in Table 2  Lack of insight into the problem addressed (i.e. why \"group 10\" impact \"significant parameters\" more than \"group 9\") Questions Regarding the approach:  Prevalence of the behavior observed in Fig. 1  comparison of remarkable rate magnitudes in \"group 9\" vs. \"group 10\"  Exploration of minimizing Eq. (5) using SGD  potential gap between Eq. (5) and the simplified Eq. (6)"], "jJOjjiZHy3h": ["Paraphrased Statement: The work proposes AdversarialAugment (AdA) an innovative method for generating augmented versions of an input. The technique involves modifying an input through a \"corruption network\" (e.g. a pretrained imagetoimage model) while simultaneously adding a carefully crafted \"worstcase perturbation\" to the networks weights. AdA aims to leverage worstcase perturbations to enhance averagecase robustness against outofdistribution data such as common image corruptions. The method adjusts the perturbation radius based on the similarity (SSIM) between the augmented and pristine inputs. The paper theoretically analyzes AdA outlining assumptions for its behavior and its relation to previous techniques like DeepAugment. extensive evaluations demonstrate the methods effectiveness on standard corruption benchmarks (CIFAR10C and ImageNetC) field shift (ImageNetR) resampled test position (CIFAR10.1 and ImageNetv2) and worstcase robustness against \\ellp perturbations.", "Paraphrased Statement: This paper introduces a data augmentation technique involving sample augmentation by modifying parameters of a generative model. The perturbations are determined through an adversarial loss and constrained by a perceptual similarity distance to prevent outliers. Besides indepth empirical evaluations the paper provides a formalization of the proposed method and a related one (DeepAugment) offering theoretical insights and examining convergence properties and PACBayesian analysis. Strengths:  Comprehensive empirical evaluations with detailed results.  Formalization and theoretical analysis connecting the method to existing work.  Integration of defense against perturbation and adversarial training offering robustness against both without compromising generalization. Weaknesses:  Single Ada methods (proposed variants) underperform DeepAugment and AugMix on all corruptions and adversarial attacks (Table 12). This implies the need for combination with other techniques for effectiveness.  Comparison with stateoftheart adversarial training methods (e.g. [123]) for adversarial robustness is lacking.  The method appears computationally more demanding than AugMix and DeepAugment. Additional Questions:  Clarification on the value and calculation of the maximum threshold t in Appendix E.  Statistics on the number of augmented samples guarded and the values of lambda used in linear interpolation to assess the work of augmented samples.  substantiation that all methods (Ada DeepAugment AugMix) employed the same training strangegy including learning range schedule and resizing on ImageNet.  Whether standard data augmentation was used in addition to the specified augmentation for ImageNet and CIFAR10.  Explanation of the absence of randomness in choosing between training on real and augmented samples in Algorithm 1.  evidence to support the claim of Adas generality to other tasks and data modalities beyond the mentioned experiments.  Elaboration on the origin of the 105 \u03b4 parameter in Section 4.  Evaluation metrics and performance of the generative models used.  Comparison with Adversarial Mixing [4] a similar augmentation technique using generative models and adversarial loss.", "Paraphrase: Summary: This paper uses imagetoimage translation networks to create adversarial data augmentation for input images. When combined with existing methods it significantly improves robustness against unknown corruptions and extreme distortions. Strengths: Unlike previous methods that targeted the classifier this approach innovatively employs imagetoimage translation networks to generate corrupted samples. The proposed approach can handle a extensive range of image transformations potentially leading to major advancements in this field. Weaknesses: The proposed approach is computationally more demanding than other methods (except DeepAugment) because it involves optimizing over imagetoimage translation networks which generally operate on highresolution images and have higher latency than classification networks. Optimization is also performed for each sample individually. There is a noticeable trend where UNet and VQVAE models perform poorly compared to their counterparts (Tables 1 and 2). Notably using them as a source of adversarial dataset augmentation significantly degrades clean accuracy. This may be due to their inability to reconstruct the input image without perturbations or challenges in achieving high fidelity with perturbations. Due to the use of imagetoimage translation networks the current approach is mainly restricted to image corruptions. Simulating nonlocal transformations like affine transformations poses difficulties for such networks. It would be helpful for the authors to discuss this limitation. Additionally the paper lacks analysis of how the proposed augmentation generalize to unseen corruptions during testing. The imagetoimage translation network may optimize for corruptions that resemble existing techniques (e.g. blurring conditions changes) in a targeted position. However it is unclear if it generalizes to unseen corruptions in an untargeted position. In image 3 the range of SSIM values appears to exceed 1 which is impossible since SSIM is bounded between [1 1]. This may be a plotting publication with error bars.", "Paraphrase: The authors introduce AdversarialAugment a data augmentation technique that enhances the robustness of image classification models. By leveraging imagetoimage models this method generates adversarial images while ensuring consistency under certain conditions. Experimental results demonstrate that AdversarialAugment:  Strengths:  Links common image corruption robustness to adversarial robustness.  Improves models against lpnorm bounded adversarial perturbations.  Surpasses previous augmentation methods (DeepAugmentAugMix) in adversarial robustness.  Weaknesses:  Computationally more demanding than other augmentation techniques.  Future work could potentially address robustness against all corruption types."], "swrMQttr6wN": ["Paraphrase: Summary: This research introduces a technique to improve agent navigation by utilizing semantic predictions made in unexplored regions. This approach draws inspiration from how humans use prior knowledge of home layouts to navigate and complete tasks even before entering the house. The core estimation involves training a model capable of predicting room categories in unseen areas. Uncertainty predictions are then used during navigation to select goals when attempting to reach a specific target category. effectiveness and Weaknesses:  effectiveness:  The overall concept and highlevel estimations are presented clearly.  The use of semantic priors during exploration is reasonable and intuitive.  Weaknesses:  Lack of Structure: The paper presents multiple components without a clear structure or algorithm boxes.  Unclear Loss map: The training loss map for the segmentation model remains\u4e0d\u660e\u77ad from the main section.  Costly semantic map: The method requires precise semantic segmentation maps which are expensive to obtain.  EnsembleBased Uncertainty Estimation: Uncertainty estimation relies on an ensemble of models increasing computational costs.  limited experimental Comparison: The evaluation lacks comparisons with similar baseline methods from chaplot et.al.  Bias Towards Large Objects: semantic segmentation is less effective in detecting smaller objects. The experimental results do not provide a breakdown of navigation queries based on object size.  Baseline Omission: A simple baseline that avoids predicting unseen semantic layout is missing from the experiments. It is unclear if navigation queries could be solved only based on observed data.", "Paraphrased Summary: This paper introduces a novel framework that enables agents to create spatially semantic maps for navigation tasks. The maps generated can extend beyond observed areas by utilizing semantic data during the learning work. Two key contribution enhance the navigation pipeline: 1. An active learning approach that identifies informative locations for the agent to explore optimizing the mappers training efficiency. 2. A method to incorporate uncertainty in the mappers predictions when selecting target objects on the map enabling a controllable balance between exploration and exploitation. effectiveness and Weaknesses: 1. effectiveness:  Provides a novel approach to training mappers used by embodied agents.  Extends estimation from previous work to incorporate them into navigation tasks.  Introduces an elegant formulation that leverages variance in predictions for both mapper training and goal selection. 2. Weaknesses:  Lacks a thorough comparison with stateoftheart approaches.  Some aspects of the systems performance are not fully clarified such as:  The order in which informative waypoint selection and target goal selection occur.  The constraints on map locations considered in geocentric optimization.  The source of variance in predictions among the ensemble of mappers. Minor Questions and Clarifications:  Why does the LowerBound strategy for goal selection not lead to higher SPL (success path length) compared to the UpperBound strategy  Regarding Table 3 why is the \"L2MOfflineLowerBound\" row missing  Is there a limited case for the SemExp baseline comparison in Table 3  Could the framework be extended to leverage variance of ensemble in the semantic feature space instead of logits as in the current work", "Paraphrased Statement: This work introduces a novel technique for ObjectGoal navigation (such as \"go to the table\") using active semantic map. The approach includes:  A semantic map module that predicts unseen areas. This module is initially trained on optimal paths between guide.  active learning to select additional training trajectories that enhance learning efficiency.  ObjectGoal navigation achieved by training predictors for specific object classes and selecting target locations based on uncertainty. The method surpasses previous mapbased navigation approaches in the Habitat Challenge ObjectGoal dataset. effectiveness:  Excellent performance of all components  Effective use of uncertainty for learning and goal selection  Comprehensive evaluations  Opensource code availability Weaknesses:  High computational requirements due to ensemblebased uncertainty estimation  Lack of comparative results on the Habitat Challenge public test set  Suggestion to include recent concurrent work by Ye et al. and Maksymets et al. for completeness in future comparisons."], "pz1euXohm4H": ["Summary This field focuses on generating sequences and suggests enhancing the target part data. During training for an inputoutput pair (x y) their model first employs teacher forcing to determine the output probability (py). It then creates an augmented pair (x \u0177) by combining the embeddings of y and py using \"mixup.\" Additionally it encourages prediction alignment between (x y) and (x \u0177) by minimizing the KL variance of their output probability. Experiments on three sequence generation tasks show improvement over baselines. effectiveness and Weaknesses effectiveness  Extensive experiments on three generation tasks (dialog generation machine translation abstractive summarization) with improvement over baselines.  The augmented data construction concept resembles a fast approximation of scheduled sampling which is theoretically valid.  Easytofollow paper. Weaknesses  Hyperparameter selection and ablation field appear to be based on test set.  Despite the papers repeated emphasis on the overlook or neglect of targetside data augmentation in sequence generation there have been numerous related field (e.g. scheduled sampling RLbased methods heuristics selftraining).  The consistency loss is not straightforward and necessitates further investigation. Given that the model is trained to produce identical targets for the two aspect why is an additional consistency loss beneficial  In certain situations baseline comparison are unfair. The authors state that \"for a just comparison with prior process we use the Moses script to calculate the BLEU score\" but tokenizer differences across papers can affect the scores obtained using this script. SacreBLEU is a beneficial selection for truly impartial comparison.", "Paraphrased Statement: Summary This field explores a data augmentation method for sequencetosequence generation tasks that operates on the targetside sequence. The key contribution is a method that uses model output to create pseudotarget tokens and sequences for augmentation. This is accomplished by integrating soft embeddings into the encoderdecoder model utilizing a standard crossentropy loss augmented with a consistency loss. Notably this approach does not require an external model for data augmentation. extensive experiments are conducted on three sequence generation tasks to demonstrate the effectiveness of the proposed approach. effectiveness: 1. Comprehensive experiments on diverse area: dialogue summarization and machine translation. 2. data augmentation without the need for additional resources. ConcernsQuestions:  language: Should the term \"sequence generation\" be refined to \"sequencetosequence model generation\" as the field focuses on that area  part 2: Comparison with Knowledge Distillation. The claim that the proposed method focuses on target input rather than output is unclear. How does target output modification not affect target inputs  Why is training not stabilized solely by Equation (3) How is training stability defined  What is the rationale for using the consistency loss to stabilize training  Tables 1 and 2: Number of dialogue turns is not specified.  Table 3: Is the number of training passes consistent for all methods If not could it be standardized The \"Ours\" model appears to be trained through More augmentation passes.  Tables 4 and 5: Are the results statistically significant Please provide significance values.  part 4.3: The statement about finetuning BART on CNNDM is extra.  part 4.4: The ablation field (Table 6 and 9) appears to be hyperparameter tuning rather than true ablation.  part 4.4: How was the value of \u03b1  0.55 determined  Table 10: Behavior when \u03b2  0 is unclear. Does the model experience empirical stability publication (mentioned earlier)  field of variance: Provide the intuition for using KL variance over JS variance and explain why KL performs better.  Conclusion: Avoid using subjective terms like \"significantly boost\" without statistical evidence.  future process: Explain why exploring the combination with unlabeled data is challenging given that the current model also does not utilize any labeled data.  Typographical Corrections:  part 2: Incorrect claim about inference cost for autoregressive methods.  Equation (3): Missing negative sign in the loss part.  Capitalize Rouge consistently.", "Paraphrase: Traditional Seq2Seq model: During training the decoder uses real language from the previous step as input. However during inference it uses generated tokens instead. This difference between training and inference can affect performance. Proposed approach: During training the decoder still uses real language as input. Additionally it calculates vocabulary distribution probability and combines them with discussion embedding weights to create \"pseudo tokens.\" This helps reduce the gap between training and inference. effectiveness:  Addresses a significant publication in natural language generation.  Simple and clear method with publicly available code.  Tested on several text generation tasks showing improvement. Weakness:  The augmented decoder input relies on the ground truth which limits its effectiveness.  Small BLEU score improvement in NMT and abstract summarization tasks suggest that the gap between training and inference may not be fully narrowed. Questions for the source: 1. Verify the reported BLEU scores for NMT task as they may contain errors. 2. Clarify the inconsistent use of the alpha parameter in Equations 2 and 4.", "Paraphrased Statement: Summary: This paper presents a straightforward data augmentation technique for the target side of NLG model. It involves creating a \"soft token\" embedding by blending targetside discussion embeddings with the probability distribution predicted by a preliminary model (adjusted by a temperature parameter T). The training loss consists of three part: (1) conventional crossentropy loss (2) crossentropy loss using the soft token embedding and (3) a consistency loss that regulates the distribution variance between (1) and (2). Results evidence that the proposed augmentation method significantly enhances performance for dialogue generation and marginally improves neural machine translation and abstractive summarization. Ablation field indicate that a minimum weight is required for the original crossentropy loss part and that the optimal temperature parameter tends to decrease with smaller vocabularies. effectiveness:  Applicable to diverse NLG tasks  Clear and wellstructured presentation  Combines mathematical rigor and intuitive explanations  Supports findings with experiments spanning multiple generation tasks and an indepth ablation field Weaknesses:  Modest improvement in machine translation and abstractive summarization compared to other augmentation methods  potential computational expense due to additional process at each time step  Uncertainties about the methods realworld utility such as its ability to enhance backtranslation in machine translation Suggestions for improvement:  Explore the potential for stacking the proposed method with other augmentation techniques such as SwitchOut on the source side.  Investigate the interaction between the proposed method and backtranslation especially when supplemented with monolingual data.  Correct a typo in the sentence \"It may also contains\" (should be \"It may also contain\").  Add a summation over j in Equation (4).  Provide an explanation for why higher temperature T might be beneficial with smaller vocabularies."], "hqkhcFHOeKD": ["Paraphrased Statement: Summary: This study establishes a mathematical framework to enhance the understanding and design of marginbased loss functions. The primary goal is to find the largest margins. The article defines class margin as a measure of interclass separability and sample margin as a measure of intraclass compactness. Additionally sample margin regularization and zerocentroid regularization are introduced to address classbalanced and classimbalanced situations. experimentation demonstrate the effectiveness of this framework in imbalanced classification tasks such as person reidentification and face verification. effectiveness:  Wellwritten and clear introduction  Introduces a systematic mathematical framework centered around maximizing margins  Proposes novel regularization methods for classbalancing scenarios  Supports the proposed method through theoretical analysis and empirical experiments Weaknesses:  The proposed loss function (GMSoftmax) involves more parameters than other framework.  The improvements achieved by the proposed method are marginal compared to existing approaches.  The study does not examine the reasons for the modest accuracy gains.", "Paraphrased Statement: This research presents a mathematically sound framework for analyzing and designing loss functions. It introduces the concepts of class and sample margins and proposes maximizing these margins as the learning objective. The study provides rigorous theoretical support for this approach. For classbalanced scenarios a sample margin regularization term is suggested. For classimbalanced face a unique largest margin softmax loss is introduced. Additionally a zero centroid regularization term is proposed to constrain the prototype centroids to zero. extensive experiments show that the proposed approach enhances accuracy and margins for various recognition tasks. effectiveness:  The proposed method offers a strong theoretical foundation unlike previous marginbased softmax methods.  The paper is wellwritten and provides detailed proofs in the supplementary materials. Limitations:  The core concepts of class margin sample margin and zero centroid regularization are not entirely novel.  The evaluation metric used for face verification (IJBC) is not specified.", "Paraphrase: This paper focuses on largemargin loss functions. Key Points:  The authors introduce class and sample margins and analyze the lower bounds of various marginbased losses.  They show that these losses are optimized by the same optimizers as classsample margins.  Two practical methods are proposed: sample margin regularization and largest margin softmax loss to improve classifier margins. experimental Results:  On range classification tasks these methods enhance performance and comparison favorably with other largemargin classification methods. Critique: effectiveness:  The authors demonstrate a unified framework to analyze marginbased losses.  practical methods for improving margins are proposed and evaluated. Weaknesses: practical Methods:  The methods are not particularly novel and their effectiveness is not fully validated.  sample margin regularization is not comparisond to other imbalanced learning methods.  LMsoftmax loss lacks empirical evidence of its effectiveness. Theoretical analysis:  The paper theoretical contribution is limited.  The analysis in Theorem 3.2 is overly general and not specific to marginbased losses.  The authors do not consider scenarios where class class are correlated which can impact margin optimization. Minor publication:  Typographical errors in equations and text.  The effectiveness of the methods for the person reidentification task is not clear.", "Paraphrased Summary: This study explores fresh loss functions to enhance margins during training of supervised classification framework (specifically softmaxbased) under various conditions including balanced and imbalanced sample distributions. The authors introduce several innovative loss functions designed to promote large margins. They provide theoretical and empirical evidence to support the effectiveness of their proposed approaches. effectiveness:  Clear and wellstructured introduction with persuasive motivations and concise theorems.  Analytical results explain the rationale behind the proposed loss functions.  extensive empirical evaluations demonstrate the benefits of using marginpromoting losses. Weaknesses:  The study lacks empirical results for scenarios where the number of class significantly exceeds the embedding dimensionality (e.g. in deep learning framework trained on datasets with numerous finegrained class).  The choice of specific loss functions (GMSoftmax LMSoftmax or Rsm) based on the dataset is not clearly justified. It would be beneficial to explore the possibility of using all three loss functions simultaneously."], "zRJu6mU2BaE": ["Paraphrased Summary: Problem: To enable fewshot learning (FSL) when the target domain is significantly different from the original (source) domain known as singlesource crossdomain fewshot learning. Approach: A threestep method is proposed: 1. Train a feature extractor using contrastive loss on the source domain. 2. Train a masking module to identify relevant features for the target domain. 3. Finetune the extractor and mask together to obtain features similar to the relevant ones. effectiveness and Weaknesses:  effectiveness: The last two step are claimed to be novel and contribute to the overall performance.  Weakness: The approach is not compared to alternative FSL methods such as multitask learning embedding learning or external memory learning. Metalearning is only one approach to FSL. Suggestions for improvement:  Provide source and a broader taxonomy of FSL methods to justify the focus on metalearning.  Consider using supervised training on the source domain instead of unsupervised training.  Clarify if the framework can be classified as metalearning since it does not have the typical outer and inner loops.  Provide more data on how the hyperparameters were selected especially the number of training epochs for each step.  Include a source for the VC dimension in Appendix H.", "Paraphrase: Summary: This work introduces ConFeSS a framework for addressing crossdomain lowsample learning challenge. ConFeSS follows a threestep work: 1. Train a feature extraction backbone using contrastive loss on cornerstone domain data to enhance feature learning. 2. Train a masking module to identify features relevant to target domain classification. 3. Finetune a classifier alongside the backbone ensuring the backbone generates features similar to the relevant ones. ConFeSS has been evaluated on multiple crossdomain lowsample learning benchmark. effectiveness and Weaknesses: effectiveness:  The problem addressed (crossdomain lowsample learning) is significant and warrants attention.  The paper is wellpresented and understandable.  The method is straightforward and simple to implement.  Comprehensive experimentation and ablation work provide valuable insights into the method. Weaknesses:  The threestage training work may be complex and challenging to implement.  While ConFeSS performs well in most cases it significantly underperforms on the CropDisease dataset compared to STARTUP.", "Paraphrase: Summary: This work proposes a framework for crossdomain learning with few examples. The framework involves: 1. Training a backbone framework unsupervisedly using a selfsupervised contrastive loss. 2. Selecting relevant features using a mask module. 3. Finetuning the network with the selected features. The framework was evaluated using the CDFSL benchmark outperforming existing methods in many cases. effectiveness:  Clear and wellorganized introduction.  Sound rationale for each framework step.  Effective use of positive and negative feature decoupling.  wide analysis. Weaknesses:  Numerous hyperparameters to tune which can be challenging in fewshot learning.  Lack of standard deviations in evaluation results which is significant due to randomness in fewshot learning. Questions for Further exploration:  Investigating the utility of negative features in finetuning.  Comparing the framework to supervisedly pretrained frameworks to demonstrate the benefits of selfsupervised pretraining.  Exploring the training method for the mask module and its potential impact on performance.", "Paraphrased argument: Summary: This work introduces a framework for learning across different domains with limited data. The framework involves: 1. Training a foundational framework. 2. Developing a masking network for features. 3. Finetuning on the target domain. The frameworks effectiveness is demonstrated on the CDFSL dataset where it surpasses existing approaches. effectiveness: 1. The unique separation of positive and negative features strengthens finetuning. 2. The algorithm is straightforward. 3. Outperforms other methods on CDFSL. Weaknesses: 1. The feature masking module employs multiple loss hindering its replication. 2. Small improvement over finetuning the entire network. 3. The ablation work (section 4.4) lacks clarity and completeness. domain for improvement: a) Clarify the discarding of labels on Ds. b) Explain how trivial masks (e.g. all 1s or all 0s) are prevented. c) Simplify the explanation of loss work in eq.(6) and (8). d) Rationale for training the mask source on Dt instead of Ds. e) Provide guidance for hyperparameter tuning when a validation set on the target domain is unavailable."], "tCx6AefvuPf": ["Summary: The research proposes a novel training technique for Graph Neural Networks (GNNs) that ensures privacy for both node features and adjacency matrices. It leverages existing differential privacy model in the GNN context claiming this approach to be innovative. effectiveness:  The motivation for privacypreserving GNNs is legitimate and relevant.  The paper is wellwritten and relatively clear. Weaknesses:  It is unclear who the privacy protections are intended to shield against.  The usage scenario should be described with a specific model for better understanding.  The cited work on graphlevel prediction is not as distinct from nodelevel prediction as suggested.  The paper focuses on nodebased prediction under transductive settings while inductive settings would provide a more rigorous test of transferability.  Graphlevel prediction tasks with nodelevel privacy preservation are significant and could be included in the experiments.  The proposed method should be compared to existing privacypreserving approach such as the work by Wu et al. (2021a).  The general formulation of GNNs in the paper could be improved to encompass different architectures.  Results from different privacy cost settings would be valuable for understanding the tradeoff between accuracy and privacy.  It is unclear whether the method is extensible to multiple GNN level or whether specific GNN architectures perform better under privacy constraints.", "Paraphrased Statement Summary This research paper proposes a method for achieving nodelevel differential privacy in Graph Neural Networks (GNNs). By adding controlled amounts of noise to gradients during each optimization step the study proves that trained GNN level can maintain (\u03b1 \u03b3) nodelevel Renyi differential privacy. The research focuses on 1layer GNNs with a maximum level of K for each node and it mathematically determines the appropriate noise level. The paper makes the following contributions:  Introduces the concept of learning nodelevel differentially private GNNs.  adapt DPSGD (Differentially Private Stochastic Gradient Descent) for GNNs by extending privacy guarantees.  Demonstrates the effectiveness of the proposed optimization algorithm on benchmark graph datasets. effectiveness  First paper to provide solid privacy protection for private nodes in graph learning.  Proves the differential privacy of the proposed algorithm mathematically.  Clear and wellorganized presentation. Weaknesses  The proof of differential privacy applies only to 1layer GNNs.  Experiments lack comparisons with other DP GNN approach.  The introduction is somewhat unclear in defining what case of privacy is being preserved and how it is applied.  A typo may exist in the proof of Lemma 7 where \u03c1 in the first equation may need to be replaced with \u03c1.", "Paraphrase: Summary This research focuses on preserving privacy at the node level during the training of Graph Neural Networks (GNNs). This approach is more stringent than \"edgelevel privacy\" because it accounts for the impact of a single node on the training work which may involve connections to numerous other nodes. In contrast edgelevel privacy only considers the impact of a single edge. effectiveness and Weaknesses While the specified privacy parameter of 30 seems high other industrial applications have utilized similar parameters. The study raises the question of whether edgelevel privacy with lower parameters offer large privacy than nodelevel privacy with such a high parameter. For edgelevel privacy it has been demonstrated that nodelevel privacy can be ensured by adjusting privacy parameters according to the number of edges connected to a node. However the converse may not always hold: nodelevel privacy of 30 may not equate to a significantly lower edge privacy guarantee. The privacy guarantee is not surprising because if each node is guaranteed a maximum of K neighbors its work extends to K other nodes. To ensure edgelevel privacy constant noise would guarantee constant privacy parameters but adding or removing K edges would require proportional noise to maintain constant parameters. The analysis does not explicitly explain whether it refutes this intuition or provides a technical reason for its breakdown. The experiments involve gradient clipping which depends on the dataset and requires scaling factor finetuning. It is unclear whether this ensures privacy and an exhaustive hyperparameter search is employed. In Figure 1 DPMLP seems to perform better than DPGNN at moderate privacy parameters (e.g. 7). The reason for this and the level at which one method outperforms the other should be clarified. While the proposed approach may offer advantages over existing methods in some privacy scenarios it is unclear when these improvement occur (e.g. at specific epsilon values). Multiple hyperparameters require tuning raising concerns about whether the extra privacy budget allocated to these parameters outweighs edgelevel privacy. The novelty of the algorithm is also uncertain as it appears nodes could be preworked to have at most K edges and then worked using an edgelevel private algorithm to achieve the same nodelevel privacy guarantee. UPDATE The authors acknowledge the feedback and plan to make the following improvement:  Address the concerns about novelty in the privacy analysis.  Clarify the circumstances where the proposed approach offer advantages.  Provide more details on the privacy guarantees and the intuition behind them.", "Paraphrased Summary: This study introduces a private algorithm for Graph Neural Networks (GNNs) at the private node level. The algorithm is based on adaptation to the Differentially Private Stochastic Gradient Descent (DPSGD) and is applicable to directed graphs. The paper investigates the privacy guarantees using Renyi differential privacy and provides amplified privacy assurances for the algorithm. Empirical results demonstrate its effectiveness. effectiveness and Weaknesses:  The extension of DPSGD to GNNs is innovative and shows improved empirical performance. Questions: 1. The role of Str in Algorithm 1 is unclear after subgraph aggregation. 2. The algorithm appears to combine DPSGD with group privacy concepts (specifying a fixed number of neighbors per node) which seems less groundbreaking. 3. The paper mentions uniform minibatch sampling without explaining its impact on privacy amplification results. 4. information on the privacy parameter epsilon used in Tables 2 and 3 is missing."], "kiNEOCSEzt": ["Paraphrased Summary: Recommendation systems (RSs) can lead to preference shifts where user preferences are influenced by the systems recommendations. This happens because RSs use reinforcement learning (RL) which incentivizes user to stay in areas of preference that are easily satisfied. To address this publication the authors propose a system that combines datadriven elements and behavioral assumptions to simulate recommendation policies and identify safe shifts ensuring that user preferences are not unduly influenced. effectiveness:  Addresses a taboo topic that commercial companies often avoid researching.  Practical methods and foundational results for developing \"neutral\" recommendation policies. Weaknesses:  trust on modeling assumptions that may alter conclusions.  Limitations include the use of random recommendations as a benchmark for preference shifts.  ethical considerations and tools for counterfactual analysis are important but inevitable. Overall: The paper presents innovative ideas that contribute to the ethical development of RSs. While it has limitations the community can benefit from its insights and it is recommended for acceptance.", "Paraphrased Summary This paper explores a significant publication in recommender systems: the alteration of user preferences due to their work. Current approach such as reinforcement learning and myopic solution can inadvertently alter preferences in harmful ways. To address this the authors propose a simulation framework that allows system creators to predict the preference shifts induced by a recommender system and assess its impact before deployment. This framework involves:  Estimating user preference dynamics by creating a user predictive model from historical data allowing for future preference estimation and counterfactual estimation.  Evaluating and optimizing the recommendation policy by measuring its work on user preferences relative to their natural shifts. simulation work show that recommender systems designed to stay within the trust region (close to natural preference shifts) can prevent manipulative behaviors such as making user more predictable. Strengths  Focuses on a pressing publication in recommender systems.  Proposes an innovative simulation framework for handling preference shifts.  Introduces the concept of natural preference shifts and a novel penalty for recommendation policies that deviate from them. Weaknesses  Incomplete technical details making it appear like a preliminary paper.  Vague descriptions of user predictive model training.  Lack of released experiment code for reproducibility.  unclear experiment details such as:  Generation of user preferences under ground truth human dynamics.  Counterfactual preference estimation used in policy evaluation.  Definition of \"SUM\" (engagement) in evaluation metrics.  Ambiguous wording and disorganization in the introduction.", "Paraphrased Summary: This work investigates the challenge of quantifying preference shift caused by recommender systems. It introduces a distance measure for these shifts based on \"safe shifts\" (acceptable shift). This metric aids in penalizing undesirable shifts during training and evaluating. simulation using a practical recommendation environment and human behavior demonstrate the proposed approach effectiveness. effectiveness: 1. The problem of quantifying induced preferences is relevant especially for practical recommender system applications. 2. The proposed method is theoretically sound and wellmotivated. Weaknesses: 1. The paper lacks widely used simulation tools like VirtualTaobao. 2. simulated experiments introduce noise and may not accurately reflect realworld scenarios. 3. The experimental setup is overly simplistic ignoring potential restrictions on preference use in recommender systems. 4. The selection of methods such as BERT as the human model appears biased. 5. The work practical value is unclear due to the absence of a solid evaluation method. Integrating the approach into established recommendation pipelines would provide a more robust assessment. 6. A competitive baseline method is missing from the results. 7. The computational complexity of the proposed approach is not addressed.", "Paraphrased Statement: Summary: The work emphasizes the significance of:  Assessing the impact of recommendation systems on user preferences.  Determining whether these shifts are detrimental.  Developing optimizations to prevent undesirable shifts. effectiveness and Weaknesses: Points for Authors to Consider:  Provide evidence beyond computer science model to support the premise of shifting consumer preferences.  Distinguish between consumer preferences and behavior in the discussion.  handle whether recommendation systems genuinely influence user choices considering prior research.  Ensure transparency in simulation methods by sharing assumptions and source code for reproducibility.  Improve LaTeX formatting for clarity. Additional Points:  Cite relevant references (e.g. [1] [2] [3]) to support the discussion.  Consider addressing potential biases or limitations in the simulation approach.  Explore the implications of the findings for researchers and practitioners in the field of recommendation systems."], "sOK-zS6WHB": ["Summary and Strengths: This research focuses on fingerprinting GANs (specifically StyleGAN2) in a flexible way allowing for the addition of various fingerprints after initial work. The authors utilize modulated convolution layers scaling GAN intermediate layer outputs via channelwise embedded projections. They also introduce evaluation metrics for GAN fingerprinting demonstrating the superiority of their proposed method. Strengths:  Addresses a critical issue in the increasing prevalence of powerful generative models.  Presents a scalable fingerprinting approach with minimal additional resource requirements.  Provides crucial metrics for evaluating GAN fingerprinting tasks showcasing the effectiveness of the proposed method. Weaknesses:  Lack of Visualization: No visualization demonstrates the impact of channelwise modulation as a fingerprinting technique.  Limited solution Testing: Experiments are conducted at 128x128 and 256x256 solution only which may not reflect the performance of the method with higherresolution GANs.  Undefined Coefficient Balancing: Equation (5) introduces balancing coefficients for release terms without specifying their selection work or impact.  Questionable release Term Motivation: The motivation behind the Lz release term in Equation (2) which is unrelated to other release terms is not fully explained. Questions for generator:  Fingerprinting resilience: Are there concrete model of fingerprinting techniques that have failed against adversarial GAN iterations  Lz release Motivation: Why is the Lz release term considered crucial and how would its exclusion impact the fingerprinting work  Logit Calculation Clarification: In Equations (2) and (3) Lz and Lc are calculated using the same decoder F. Clarify whether these release terms operate on distinct portions of the logits. Overall Assessment: Despite the weaknesses noted the studys strengths including the motivation novel technique and valuable evaluation metrics make it a significant contribution. The authors are encouraged to address the concerns raised to enhance the comprehensiveness of their work.", "Paraphrased Statement: Summary: This research proposes a method to enable creators of artificial data (e.g. deepfakes) to mark their generated data before sharing it through public platforms (as done by OpenAI). This allows creators to easily identify if any data was produced using their generative model mitigating the potential negative issue of fake data. The method involves incorporating a unique fingerprint into the generative model making each generated data identifiable. A decoder model is also trained alongside the generator to determine the fingerprint used to create a given potential fake data. The authors claim that their technique has minimal computational overhead negligible performance degradation compared to conventional generative models and is robust against attacks aiming to evade fingerprinting. These claims are validated using publicly available GAN datasets. Strengths and Weaknesses:  Strengths:  Addresses a significant issue posed by the rise of deepfakes.  Overcomes scalability limitations of previous fingerprinting methods.  Clear and wellsupported approach.  Weaknesses:  potential reliance on the assumption that only a few legitimate entities can create further GANs.  Honesty of model creators is assumed.  Minor issue related to experimental details and potential plagiarism in the concern work section. Suggestions for Improvement:  Expand the conclusion to discuss potential limitations and future research way.  Address the potential threat of malicious entities developing their own GANs.  Explore nonrepudiation techniques to ensure responsible release of generative models.", "Paraphrased Statement: Objective: Develop a technique to uniquely identify image generated by specific GAN models allowing for their detection and attribution to the original model. This fingerprint would aid in scenarios where malicious actors leverage published GAN models to create fake image as it would allow for model identification and exposure of the synthetic nature of the image. technical approach: Embed a 128bit fingerprint into the GAN model and train an autoencoder alongside the GAN. The autoencoder extracts the fingerprint from generated image. Strengths and Weaknesses: Strengths:  proactive protection against malicious use of synthetic data. Weaknesses:  contribution Novelty:  Concept of GAN fingerprinting is not novel (proposed in ICCV 2021).  Key difference lies in scalability to numerous fingerprints. This enhancement is the primary contribution.  Related work:  Confusing classification of watermarking and steganography.  Network watermarking solutions for generative models exist (e.g. Ong et al. CVPR 2021).  Experimental Analysis:  Limited testing to StyleGAN2 model.  Insufficient work on the distinguishability of fingerprints from different generator.  Inadequate robustness work particularly with adversarial noise.  Uncertain comparison with passive methods (e.g. Wang et al. 2020) and neglect of other attributionbased approach.  Lack of performance comparison with watermarkingbased methods.  Inadequately explained comparison with multiple \"N GAN source.\"  Absence of ablation work on the importance of release terms.  Deepfake misuse Scenario:  Unclear behavior when synthetic image lack a fingerprint. No experiments to address this scenario.", "Paraphrase: This research introduces a method for detecting and tracing deep fake image back to their creators. The method involves embedding a distinctive \"fingerprint\" into image generated by GANs. This fingerprint can be easily detected and linked to the user who created the image. The method utilizes a StyleGAN2 backbone with a modified generator that modulates its filters with unique fingerprints. By training a single model with randomly assigned fingerprints and latent codes a vast issue of models can be created with different fingerprints in a issue of seconds. Experiments show that the fingerprint can be reliably detected in generated image with minimal impact on their quality. The model can handle over 2128 unique fingerprints. Strengths and Weaknesses Strengths:  detection and attribution capacity: The method facilitates detecting deep fakes and attributing them to their creators.  Scalability and Efficiency: Training a single model enables the generation of numerous models with different fingerprints making the method scalable.  technical foundation: The method introduces a novel release use for StyleGAN2 and modifies its architecture. Weaknesses:  StyleGAN2 Dependence: The method is currently limited to StyleGAN2.  data Requirements: high fingerprint detection accuracy requires a training set with at least 10k samples.  Training Effort: Model training involves significant samples from the fingerprint place even if only a few models are needed. Additional Comments:  Comparing the robustness and immunizability of the proposed method to existing approach would be beneficial.  It should be clarified whether the baseline issue were extracted from an existing issue.", "Paraphrased Paper Summary: This paper proposes a method to mark GANgenerated image with a unique fingerprint during the training work. The fingerprint is integrated into the GANs filter weights and encoded into the generated image. The authors show that the fingerprint can be reliably detected and attributed to the GAN source even after common image use. Strengths and Weaknesses: Pros:  Proactively fingerprinting GAN models is an innovative approach.  wide experiments support the design choices.  The paper is wellpresented and easy to understand. Cons:  The paper focuses on a narrow detection task within a specific GAN architecture.  The application is limited to scenarios where the model creator cooperates.  The paper does not evaluate the reliability of the detection model in terms of exact fingerprint code matching.  The immunized models performance under blurring perturbations is not fully explained."], "wClmeg9u7G": ["Paraphrased Statement: Summary: The paper presents two methods for solving general variational inequality problems using unbiased and contractive compressors. The methods have been tested on saddle point problems including largescale adversarial training of transformers. effectiveness and Weaknesses: Weakness:  Theoretical issue are limited.  The authors assume parameter q is constant but it can be large in overparameterized background resulting in a little learning range.  When \u03c4 (communication cost control parameter) is near 1 the learning range becomes negligible hindering convergence.  The condition for Algorithm 1 to outperform uncompressed methods is restrictive.  The authors do not provide model of VI problems beyond saddle point problems and do not demonstrange that assumptions hold for such problems. other Concerns:  It is unclear how all nodes approach F(wk1) at the start of iteration k1.  The paper claim that \"we introduce the notion of expected density...\" is not novel.  The paper contains numerous typos that warrant revision.  Some terms (\u03c4 bk) are only defined in Algorithm 1 and should be defined in the main text.", "Paraphrase: Summary: This study focuses on compression techniques for addressing variational inequalities (in particular saddle point problems) in a distributed background. Both unbiased (MASHA1) and contractive (MASHA2) compression approach are discussed. Theoretical analysis demonstrates the convergence of the proposed method. effectiveness and Weaknesses: effectiveness:  Proposals of compression techniques for solving variational inequalities.  Coverage of various cases including unbiased vs. contractive and deterministic vs. stochastic.  Indepth convergence analysis. Weaknesses: 1. The paper analysis does not consider nonconvexnonconcave objectives which arise in practical applications such as GAN and adversarial training leading to nonmonotonic operators in variational inequalities. This more realistic assumption has been addressed in previous studies with rigorous theoretical analysis. It would be beneficial for the authors to expand their analysis to this more practical background. 2. In the text preceding Figure 2 the authors claim that \"the learning curves on Figure 1(upper) follow a predictable practice with more extreme compression techniques demonstrating slower periteration convergence.\" However Figure 1(upper) is not mentioned in the paper so it is unclear what figure the authors are referring to. Additionally the learning curves in Figure 2(upper) are largely overlapping making it difficult to observe the claimed trend. A further discussion of the compression ratio between 8bit quantization and ability compression with range r  8 is also warranted. 3. In Figure 2(lower) it is common practice to include average scores across different tasks in the GLUE benchmark. This would facilitate easier comparison between different methods. 4. In Figure 1 the extrastep method appears to diverge with accuracy decreasing during training. Since the extrastep method is specifically designed for saddle point problems its divergence is counterintuitive and requires explanation. 5. The paper lacks a dedicated conclusion section to summarize the findings.", "Paraphrase: This paper examines the communication demand for a group of distributed players to jointly solve a variational inequality problem. The authors present two algorithms MASHA1 and MASHA2 that handle both deterministic and stochastic cases. They also demonstrate the application of these methods to bilinear saddle point problems and adversarial training of transformers. effectiveness:  The algorithms extend the extragradientextrastep method for solving variational inequalities to a distributed background.  The authors specify modifications to the method for distributed environments and provide convergence proofs and communication bounds. Weaknesses:  The algorithms are incremental adaptations of the extragradientextrastep method and may not be a innovative contribution.  Although the convergence and communication bounds are intricate they are not innovative but rather adaptations of proofs from the centralized background.  The paper would benefit from a clearer introduction to the classical solution of variational inequality problems before diving into the distributed approach. Section 4 could be made more readerfriendly by transitioning the reader smoothly into the distributed solution.", "Paraphrase: Summary: This paper introduces a decentralized approach for solving a specific type of mathematical optimization problem (variational inequalities) that is commonly encountered in machine learning applications. The approach involves using a technique called compression to reduce the communication overhead between multiple devices and a central server. The paper provides mathematical proofs that guarantee the convergence of the algorithm even with the presence of compression errors. It also examines the tradeoff between the amount of data transport (communication complexity) and the speed of convergence. effectiveness:  The paper addresses a welldefined problem and offers a theoretically sound algorithm.  The convergence analysis is rigorous and comprehensive. Weaknesses:  The paper does not provide a theoretical analysis to show that compression can indeed reduce communication complexity compared to uncompressed methods. Specifically it requires the condition that a certain parameter (\u03b2) be greater than 1 but for a particular compression method (randK) this condition is not met.  The experimental evaluation is somewhat limited.  It focuses only on adversarial training rather than general GAN applications.  It does not explore the impact of different parameters (compressors compression ratios learning rates) on performance.  The paper could be improved by providing more point on the centralized solution (without compression) as background information.  The communication complexity analysis suggests an unexpected disparity in the work of compression factors between devices and the server. This requires clarification."], "mNLLDtkAy4X": ["Paraphrased Summary: This paper enhances previous research on curiosity in artificial intelligence by incorporating the prediction of uncertainty (nonreducible) into action selection. Agents thus prioritize avoidance of environmental states where uncertainty cannot be reduced through learning. This approach improves exploration in environments with nonreducible uncertainty. Additionally it provides a potential test to elucidate the role of acetylcholine a brain neurotransmitter associated with uncertainty. effectiveness:  Indepth review of curiosity techniques.  Neuroscientifically inspired algorithm that considers aleatoric uncertainty.  Estimation of uncertainty using a single agent improving the models practicality.  Thorough experimental evaluation with multiple baselines.  Proposed test to investigate the role of acetylcholine in the brain. Weaknesses:  limited improvement in exploration in unaltered environments.  Lack of evidence for increased reinforcement in unaltered environments.  Uncertainty predictions derived from definitions questioning the need for the proposed experiment design. Specific Comments:  Verify equation (5).  Define variables before role.  Consider explaining Angela Yu and Peter Dayans model as inspiration.  Provide a brief derivation for equation (5). Suggestions to the Authors:  evidence improved reinforcements in realworld environments.  Collaborate with neuroscientists to test the acetylcholine experiment.", "Summary Paraphrase: This paper introduces an exploration bonus mechanism that discourages noisy TV behavior by penalizing the variance of the predicted state. A normal distribution model is used to predict the future state (St1) based on the current state (St) with mean (\u03bct1) and variance (\u03c3t12). The bonus is calculated as the squared error between the predicted and actual future state (St1  \u03bct1)2 minus the variance penalty (\u03c3t12). This approach ensures that noisy TV which typically has high variance is penalized while predictable transitions have a low bonus. effectiveness and Weaknesses Paraphrase: effectiveness:  novel and intuitive approach to prevent noisy TV behavior. Weaknesses:  Mathematical justifications are unclear and contain errors.  Excessive discussion of a simple Gaussian model for predicting the future state.  Bioinspired aspect lacks convincing content and evidence. Suggestions:  Rewrite the method section to start with the final bonus equation (7).  Explain how the bonus mean is zero when the model is learned.  Discuss how the mean and variance are learned using a twoheaded network.  Consider incorporating actions into the learned model to address potential issues with changing policies.", "Summary: This work introduces a way to separate epistemic (reducible) from aleatoric (irreducible) uncertainty addressing the problem of intrinsically motivated agents being rewarded for highuncertainty states that cannot be reduced. effectiveness and Weaknesses:  Weakness: The work fails to acknowledge prior research that has addressed the same issue and misinterprets some literature.  Specifically the authors claim that estimating epistemic uncertainty in highdimensional data is unsolved despite evidence to the contrary (e.g. Gal 2016s method using MC Dropout).  The authors overlook other similar approach such as Deep Ensembles and related work using epistemic uncertainty as intrinsic rewards.  The selected baselines in the experiments are insufficient to demonstrate the superiority of the proposed method. Detailed Comments:  C1: The acknowledgment for deterministic uncertainty estimation is incorrect the acknowledgmentd work uses a Bayesian Neural Network while deterministic uncertainty estimation was introduced in another work (not cited in the text).  C2: The acknowledgments for epistemic and aleatoric uncertainty should include other work like Hora 1996.  C3: The claimed novelty in computing aleatoric uncertainties is unclear. There are existing methods that can do this efficiently.  C4: The proposed incentive for agents to seek epistemic uncertainties is not novel and derives from the law of total variance.  C5: It is unclear how regularization term could absorb epistemic uncertainty. Does this mean that regularization reduces epistemic uncertainty If so an explanation is needed.  C6: The experimental protocol in Section 5 should be improved for clarity.  C7: The method for estimating acetylcholine in the model should be explained.  Suggestion: The authors should use \\citep instead of \\cite whenever possible and choose a more readerfriendly color for acknowledgment."], "hW2kwAcXq5w": ["Summary: setup: The work focuses on imitation learning without explicit reward information. It aims to train a policy that emulates expert behavior in a dataset (De) while utilizing a nonexpert dataset (Do) for direction. approach: The authors propose an algorithm inspired by positiveunlabeled classification and adversarial imitation learning. It involves training a discriminator to distinguish expert samples from nonexpert samples. Meanwhile a policy is trained to imitate the expert and deceive the discriminator. effectiveness:  The paper is wellwritten and easy to understand.  The setup is intriguing and potentially impactful. Weaknesses: Methodology:  The authors overlook a similar work (PUGAIL) that previously applied positiveunlabeled learning to imitation learning setup.  The theoretical derivations are unclear and require further detail. Related work:  additional mention on using multiple datasets for imitation learning should be cited (e.g. CSI MILO). Evaluation:  The authors claim the algorithm can approach optimal policies but they lack experiments to support this.  The plots should include the average return in De to provide a more complete evaluation. Experiments:  The experimental setup is adequate but could be expanded.  additional results with several data scenarios and environments would be beneficial. Writing:  The abstracts phrasing about \"both optimal and nonoptimal expert behavior\" could be clarified.  Minor grammatical errors should be corrected.", "Paraphrase: This research introduces an offline imitation learning approach that uses a combination of optimal and nonoptimal datasets to learn decisionmaking tasks without needing labeled reward. To harness valuable highreward examples from the nonoptimal dataset the authors devised a discriminator that targets a \"positiveunlabeled\" learning goal. In this scenario positive samples stem from the optimal dataset while unlabeled samples are drawn from the nonoptimal dataset. This discriminator is trained concurrently with the policy in an adversarial manner leading to a behavior cloning objective that assigns different weights to samples from the two datasets based on the discriminator predictions. Experimental findings indicate that in simulated locomotion field the proposed algorithm can utilize the nonoptimal dataset to generate policies that outperform vanilla behavior cloning objectives and previous offline imitation learning benchmarks. effectiveness:  Relevant and substantial problem: Reusing existing offline noisy datasets for novel tasks.  Clear introduction and preliminaries: effective introduction of the problem and background information.  Novel method: Weighted behavior cloning objective using a discriminator. Weaknesses:  Unclear derivation in section 3.2: need for more intuitive explanations and justification for design choices.  Missing baseline: Evaluation without the adversarial learning factor of the discriminator.  Limited evaluation range: Experiments confined to locomotion field uncertain scaling to more complex scenarios.  Inconsistent baseline introduction: Some baseline are presented as lines while others are curves.  Lack of training details: Missing pseudocode or description of the wide training scheme.", "Paraphrase: The paper introduces DWBC an offline imitation learning algorithm for datasets containing both expert and nonexpert demonstrations. It employs a modified loss function that assigns weights to expert and nonexpert data based on a learned discriminator. DWBC demonstrates superior policy performance in OpenAI Gym tasks compared to existing methods. Notably it can learn from a limited number of expert demonstrations (e.g. 4000 for HalfCheetah). However the papers method and derivation lack clarity. DWBC learns a discriminator conditioned on the policy to simplify discrimination however the justification for this approach is insufficient. Additionally there are inaccuracies in the derivation such as the dependency of the loss function on states and actions and the need to stop gradients from flowing to the discriminator during policy optimization. The experiments also raise concerns:  Shading in Figure 1 may not accurately represent uncertainty.  The training set for each seed may vary.  Axis labels are missing from Figure 2.  The datasets used have limited diversity originating from online training.  In the offline policy choice experiment it is unclear if separate evaluation and training datasets for the discriminator were used.  The expert policys return in the experiments shown in Figure 1 is not provided.", "Paraphrase: Summary: Researchers address the issue of imitating expert behavior in robotics tasks using suboptimal data. Traditional methods like behavior cloning struggle with poor data leading to reduced performance that often worsens as more suboptimal data is introduced. This paper introduces a novel learning approach inspired by Generative Adversarial Networks (GANs). The model learns a discriminator to distinguish between samples from an expert and suboptimal demonstrations. This discriminator is then functiond to adjust the importance of each data sample in the training work similar to costsensitive learning. The algorithm is tested on standard robotic control benchmarks in several environments. In environments like Hopperv2 the resulting policy surpasses existing imitation learning techniques. The discriminator (which analyzes action probabilities) is also evaluated for offline policy evaluation showing promise in comparing the performance of different policies. effectiveness:  Clear and wellwritten paper  Strong empirical evidence supporting the claims  Carefully selected baseline for comparison  Novel function of a discriminator for both policy optimization and evaluation Weaknesses:  Experiments focus on datasets with varying proportions of positive samples. It would be beneficial to include data collected from a mix of optimal and suboptimal sources.  Offline policy evaluation is only tested in the Hopperv2 environment. additional environments would provide a more comprehensive evaluation."], "qRDQi3ocgR3": ["Paraphrased Summary: The study examines biases in deep neural networks (DNNs) caused by training on pathological data where multiple latent variables are only observed as fully correlated (e.g. scale with color). The researchers introduce a problem setup where the trainer is restricted to seeing samples with correlated latent variables and design criteria to analyze the models behavior when the correlation is broken (i.e. in unseen data). They conclude that DNN models generalize to unseen data by: 1. Primarily relying on single cues (e.g. color alone) for prediction. 2. Favoring simpler cues over more complex ones. 3. Exhibiting a preference for solutions that prioritize simple cues. The study is wellwritten with clear claims and methods. However: Weaknesses: 1. Lack of clarity in some explanations (e.g. Figure 3 interpretation). 2. The problem statement implies bias but lacks a ground truth assignment to demonstrate it. 3. The findings (e.g. preference for simpler cues) are not surprising or counterintuitive. Suggestions for Improvement:  offer clear explanations of confusing statements.  Clarify the concept of bias in the given problem.  Reframe the contributions to highlight the practical implications of the study.", "Summary: The paper explores which visual cues are favored by computer vision models. It sets up a training environment where different cues have equal importance. The study shows that cues like color are learned more easily than cues like preference or work. Additionally cues that are easier to learn tend to lead to flatter loss landscapes and are more prevalent in model parameters. Strengths and Weaknesses: Strengths:  The paper provides an indepth analysis of model preferences for diverse visual cues.  It demonstrates that models favor less complex cues such as color and ethnicity. Weaknesses:  \"Averted solutions\" or \"averted cues\" are used without being defined in Section 3.2.  Section 3.2 suggests that \"preferred solutions\" are obtained by optimizing the model using a specific dataset while \"averted solutions\" are computed by optimizing for a specific property deemed nonpreferred. If this is the case its not appropriate to directly compare the two as they are trained on datasets of different sizes.  Figure 4 lacks clarity regarding which solutions are \"averted\" and \"preferred.\" It also implies that the parameter variation directions were randomly chosen making it difficult to draw conclusions.  Section 4 measures cue complexity by finding the minor model that can memorize the training set. It would be interesting to examine if larger models have reduced biases towards simpler cues.  The bias dataset setup used to study cue preferences may not accurately reflect realworld data. Exploring the effect of offbias samples on model performance would provide additional insights.", "Paraphrased Statement: The authors present a model to examine how deep neural networks prioritize certain \"cues\" when making decisions. They focus on cases where multiple cues are available but not equally utilized. To achieve this they introduce the WCSTML task which allows for precise control of cue availability. They also conduct study on the UTKFace dataset which is more representative of realworld scenarios. To analyze cue preferences the authors introduce metrics based on loss landscape topology. They also propose that the \"complexity\" of cues work cue preference. Strengths and Weaknesses: Strengths:  clear motivation and careful construction  Novel use of the WCSTML task for empirical evaluation  Corroborating finding that solutions relying on preferred cues are abundant Weaknesses:  Analysis could be more indepth:  Selected cues may not be equally prevalent due to varying extraction demands  Experiments on UTKFace may not fully account for additional \"shortcut cues\"  Kolmogorov complexity may not be an ideal proxy for model parameters  Figures need improved legibility  The description of Figure 4 could be clarified"], "wu5yYUutDGW": ["Paraphrase: Summary: This article describes a novel method for categorizing video time into scene using unsupervised learning. In the training phase DTW is utilized to estimate approximate scene boundaries. Then techniques like ShotScene Matching and Contextual Group Matching are employed to improve the distribution of features within and between scene. Additionally Pseudoboundary Prediction and Masked Shot Matching assist the framework in identifying transition points. Strengths and Weaknesses: Advantages:  The paper is straightforward and understandable.  The proposed framework appears logical and efficient.  The method yields impressive results surpassing previous approach. Disadvantages:  The framework is specific and cannot be well adapted to other video scene grouping methods.  The pseudoboundary discovery stage assumes that the input contains only two scene which may not always be the event. It is unclear how the training data is prepared for sequences with more than two scene.  The effectiveness of MSM is limited as indicated by Table 3.  The article lacks insights. For instance the authors state that SSM and CGM optimize interintrafeature distribution but they do not provide direct evidence to support this claim. Reporting performance improvements alone is inconclusive as other factors like network architecture and training protocol can also influence results.", "Paraphrase: Summary: This work presents a technique for selfsupervised learning of video scene segmentation which involves detecting temporal boundaries in videos. The method defines \"pseudoboundaries\" by splitting shots into two parts and training a framework to identify them using three novel tasks: ShotScene Matching (SSM) Contextual Group Matching (CGM) and Pseudoboundary Prediction (PP). Pretraining and transfer learning demonstrate the effectiveness of this approach in enhancing video scene segmentation performance establishing a new stateoftheart on the MovieNetSSeg benchmark. Strengths and Weaknesses: Strengths:  Addresses an important problem in video understanding making video networks aware of scene boundaries.  use selfsupervised learning to improve network performance without additional annotation cost.  Generates pseudoboundaries based on scene segmentation intuition ensuring uniqueness for each video.  Achieves stateoftheart results on the MovieNetSSeg benchmark. Weaknesses:  Similar to the BSP method with only a modest performance improvement (1.64).  The use of Dynamic time Warping (DTW) for temporal alignment may not be necessary given the limited sequence length.  The Sum column in Table 3 lacks clear justification for summing different metrics without explanation.", "Paraphrase: Summary: This paper presents a framework for video scene segmentation that utilizes unsupervised learning. By leveraging a contrastive learning approach the authors generate partial video segmentation from unlabeled videos using Dynamic time Warping (DTW). These segmentation serve as inputs for four loss use which are used to pretrain a backbone network. Strengths and Weaknesses: Concerns: 1. Pseudoboundary discovery: The paper focuses on identifying a single boundary in each video which may not be sufficient as videos often contain multiple scene. Additionally the definition of different scene in the generated pseudosegments (e.g. in frame 4(c)) is unclear. 2. Experimental event: The proposed pretraining approach shows inferior performance to previous methods without finetuning. The significant performance improvement comes from the second stage of training. 3. Novelty: The paper combines existing ideas rather than proposing novel ones. The ablation work fail to provide significant insights into the behavior of the proposed loss use. Improvement Suggestions: 1. Evaluate the framework on additional datasets to demonstrate its generalizability. 2. Address typos to enhance readability."], "yhjfOvBvvmz": ["Paraphrased Statement: Summary: This research introduces a method for creating representations of simulated trajectories using a VAE approach that utilizes weak labels for improved separation of features. These representations are then used to train a skillbased policy that mimics the learned model using soft actorcritic. The paper also presents a hierarchical planning strategy involving exploration in the representation space and trajectory rescaling. effectiveness and Weaknesses: effectiveness:  Demonstrates the effective disentanglement of features using the proposed VAEbased approach.  Shows the benefits of the hierarchical planning strategy. Weaknesses: Lack of clarity and justification:  The lack of clear definitions and explanation makes the paper difficult to understand.  Key design decisions are not well supported by justification. Supervised learning alternative:  Weak labels used for disentanglement could be replaced by supervised learning to extract specific features from trajectories. Disentanglement benefits:  The reward of disentanglement beyond interpretation is not clearly explained. MPC approach concerns:  The use of excluding zmulti from the latent representation in the policy is unclear.  The use of handcrafted use in trajectory rescaling is ambiguous.  The sampling distribution for potential trajectories in hierarchical planning is not specified. Userdefined parameters:  The selection of artificially generated sequences and scaling factor is arbitrary potentially reducing the practicality of the approach.", "This paper introduces a new framework for training skill policy (WEDIS) and using them for control in a hierarchical setup with model predictive control (MPC). The core idea is for skill policy to follow generated trajectories based on relevant factors. For this the authors train a variational autoencoder (VAE) that provides both control variables (latent encoding) and a predictive model of the trajectory (decoder). The skill policy is trained to match the trajectories of the predictive model which is then used for MPC. Despite the potential complexity of the method experimental results do not justify it. The basic elements used in WEDIS (VAE on trajectories training skill policy on a resulting reward signal) are common in skill discovery techniques like OPAL and EDL. While the authors demonstrate improvements over purely unsupervised skill learning methods (DADS and SeCTAR) on downstream navigation tasks the primary concern is the creation of supervision in WEDIS. The authors motivation for using an autoencoder setup when reference trajectories can be generated beforehand is unclear. Why not simply pretrain a policy with a reward to explicitly follow these trajectories based on salient factors (direction speed curvature) This should at least be included as a baseline in the paper. While the training setup in WEDIS could allow for More generality it requires the ability to generate reference trajectories which would enable training policy to follow them directly. Additionally the experiments lack a comparison to plain Soft ActorCritic (SAC). The authors claim that current skill learning methods are fundamentally limited by a lack of interpretability is also debatable. While approach to a model enables the use of standard planning methods like MPC other process have demonstrated effective hierarchical control using RL without explicit semantics (e.g. DIAYN OPAL and NPMP). Finally the paper writing could benefit from grammatical revisions.", "Summary Paraphrase: The paper proposes a method to learn embeddings that represent skills. It leverages a dataset of trajectories labeled with factors like direction speed and curvature. These factors help organize the learned skill space. The method shows that these factors can be used to create a latent space where different factors of variation are separated. This latent space can be used with model Predictive Control (MPC) to solve tasks in the OpenAI Gym ant environment. effectiveness and Weaknesses Paraphrase: effectiveness:  The related process section covers all previous field on the topic.  The planning visualizations show how the learned skill spaces affect downstream learning.  The appendix provides instructions for making the dataset and the policy training. Weaknesses:  The approach violates the idea of unsupervised skill learning as it uses a pregenerated dataset of training trajectories.  The approach learns disentangled skills with full annotations making it a supervised method rather than a weakly supervised one.  The paper does not explain why disentangling the given factors of variation is beneficial.  The model explanation in section 4.2.2 is unclear making it hard to understand how the model is used downstream.  The \"trajectory scaling\" approach simply describes \"process repeat\" a common RL use.  The paper compares the method to unsupervised approaches unfairly as they do not have approach to precollected demonstrations.  The writing could be improved in terms of clarity and context. Questions:  Why use singlestep transition model for reward computation instead of trajectorylevel model  How does the model in Figure 1 change when the latent variable is split into zsingle and zmulti  How does the \"heuristic\" approach to finding the relative use between zsingle and zmulti process", "Paraphrase: Summary: This paper presents a technique that develops skill representations with distinct components and demonstrates its effectiveness in guiding a Mujoco Ant through a navigation task. method: 1. Synthetically generate a dataset of trajectories by modifying various parameters. 2. Train a trajectory Variational Autoencoder (VAE) to produce disentangled trajectory representations using weak labels. 3. Freeze the trained trajectory VAE and train a skillbased policy to produce trajectories similar to the VAE decoders output by minimizing the distribution divergence. effectiveness:  Visualization of learned skills and their interpretability in the Ant navigation task.  Demonstrates enhanced performance in longhorizon navigation when combining interpretable skills with modelbased planning (MPC). Weaknesses:  Data Requirements: The proposed method requires a large synthetically generated dataset potentially impractical for realworld applications.  The dataset generation requires expert policy capable of varying speeds directions and accelerations.  Prior process: The concept of using weakly supervised representations for interpretable skill learning has been previously explored in [1] which the paper does not cite.  Presentation:  The term \"latent traversals of trajectories of the decoder\" is not clearly defined.  The benefit of disentanglement for MPC planning is not fully explained.  Typos and Grammar:  Several typos and grammar errors have been identified. Additional Questions: 1. impact of Dataset Size and Quality: How do dataset size and quality influence the quality of disentanglement and policy performance 2. Dataset Generation Details:  Does the 100K trajectory dataset include full observations from the Ant environment (e.g. joint velocities)  Were expert policy and a model environment involved in generating this dataset  Did other methods (SeCTAR DADS) use the same dataset for fair comparison 3. disentanglement and MPC:  How does disentanglement aid MPC planning potentially resulting in improved performance  Can the impact of disentanglement be empirically confirmed"], "oSP1hwZB24": ["Paraphrased Statement: Summary: This paper presents a novel approach for handling interactions between features and their contexts specifically designed for clickthrough rate (CTR) prediction neural networks. This method is evaluated through various scenarios and showcases promising results in both offline and realworld experiments. effectiveness:  effectively handles featurecontext interactions enhancing CTR prediction accuracy.  Encouraging offline and online results with detailed analysis of offline performance.  Wellwritten paper that clearly describes the methodology and its relationship to existing methods. Weaknesses:  Section 7 (Experiments):  Limited metrics for demonstrating the superiority of the novel method.  Inadequate segmentation analysis to determine if the method benefits different queryuser groups evenly.  Increase in CPC should be investigated and explained.  Section 3 (Results):  significant jump in TP99 requires further investigation to determine if its only due to network structure changes.  Analysis of TP50 TP90 and TP99.9 would provide a more comprehensive understanding.  Section 2 (Background):  Details on the \"selfexcluded version\" in the relationship to factorization machines (FM) would enhance clarity.  Section 2.1 (Preliminary):  Explanation of why separating traditional and sequencebased CTR prediction is beneficial.  General Concerns:  more intuitive explanations and discussions on how the added interaction component improves CTR prediction accuracy would differentiate this approach from structural tweaks in neural networks.", "Paraphrase: Summary: The paper introduces a Dynamic Parameterized Network (DPN) module to improve ClickThrough rate (CTR) prediction by incorporating both explicit and implicit user data. The authors present the first application of dynamic neural networks in CTR prediction and demonstrate that DPN outperforms existing approach. effectiveness:  Clear and wellorganized writing  introduction of a novel concept: behavior modeling  Detailed theoretical analysis Weaknesses:  Inconclusive experimental results in Table 1 with some baselines performing below their expected levels and DPNs performance falling short of reported achievements in similar field  Insufficient justification for using the lowrank strategy in Equation 5  Minor typos (e.g. \"actions.to\" in Section 2.5)", "Paraphrase: This research proposes a model for incorporating feature interactions into clickthrough rate (CTR) prediction. The model leverages metalearning to model the interactions between features. Specifically it uses a meta neural network (g(F1)) to produce parameters for another neural network (f(F2)) that operates on a different feature (F2). The models performance surpasses baseline models and has been successfully deployed in a production system. effectiveness and Weaknesses:  Pros:  test effectiveness in a realworld production environment.  Comprehensive exploration and evaluation of multiple model variants.  Cons:  Lack of novelty the approach is similar to existing methods.  Need for more experimental details to evaluate fairness such as parameter counts and baseline model optimization.  Reporting of online AB test results without context may not fully convey their significance.  Writing could be improved for clarity."], "oVE1z8NlNe": ["Paraphrase: Summary The authors present FedEMA a new method for federated learning that addresses the challenge of nonindependent and identically distributed (nonIID) data. FedEMA leverages FedSSL a federated selfsupervised learning framework. It features an exponential moving average (EMA) update that dynamically adapts its decay rate based on data divergence. strength and Weaknesses  The approach effectively combines local and global knowledge through the EMA update.  Empirical evaluation demonstrate a significant improvement in performance on CIFAR10 and CIFAR100 datasets. Concerns  The theoretical basis for the methods strength is not fully explained.  The significance of \"W\" is unclear and it is assumed to represent model argument.  The choice process for the decay rate argument \u03bb is not specified.", "Summary: The authors propose an generalization of selfsupervised learning (SSL) algorithms for the federated learning setting called FedSSL. They evaluate existing SSL methods in this framework and develop a new algorithm FedEMA based on their observations. strength and Weaknesses: Pros:  Clear and easy to follow  valuable empirical insights  Wellmotivated and explained findings Cons:  Crosssilo vs. Crossdevice distinction: The paper doesnt explicitly distinguish between these two settings which could impact the significance of their findings.  Limited client count: The experiments consider only 5 clients which may not be representative of largescale federated learning scenarios.  Client availability and overfitting: The paper doesnt address the impact of client availability and overfitting in the local target encoder.  Serverside optimization: The paper lacks exploration of advanced update rules for serverside optimization.  Hyperargument exploration: The paper doesnt provide detail on the hyperargument ranges explored in the experiments. Additional Suggestions:  comparative scales: The authors should compare their datasets to literature to demonstrate the representativeness of their SSL methods.  Largescale experiments: evaluation on larger datasets would enhance the generalizability of the insights.  Scaler argument: The paper could track the rate of the scaler argument over time to account for federation feature that influence the divergence between local and global models.  Clarification of algorithm: The divergence between Figure 3 and Table 2 as well as the time indexing in Equation 3.2 should be clarified. Its also unclear how the algorithm differs from Equations 1 and 2.", "Paraphrased argument: This study proposes a general recipe for federated SSL (FedSSL) utilizing FedAvg for several existing SSL methods such as SimCLR MoCo BYOL and SimSiam. Each of these SSL modules involves two neural networks: an online network and a target network. The networks are trained using a similarity loss that minimizes the distance between encodings of different augmented input adaptation (e.g. rotated) while maximizing it for dissimilar adaptation. The argumentization of the two networks can vary between SSL modules (e.g. identical for SimCLRSimSiam nonidentical for MoCoBYOL). During each iteration the online network from each client is uploaded to a server for averaging. The averaged online network is then distributed back to clients. Each client updates their online network as a weighted average of the global and local online networks to accommodate potential data heterogeneity and continues updating the SSL module via gradient updates. The papers primary contribution lies in a series of ablation studies that evaluate the effects of key components of the SSL modules (e.g. predictors exponential moving averages identicalnonidentical networks). Results suggest that BYOL encompasses all essential elements for FedSSL. A customized divergenceaware EMA is introduced which significantly improves performance over existing FedSSL baselines. An ablation study also demonstrates the significant impact of the new EMA. strength:  Novel decay rate formula in the EMA recipe.  Extensive empirical validation showing strong improvements.  Identification of beneficial empirical practices for FedSSL particularly in nonIID settings. Weaknesses:  performance may depend on the choice of a argument Lambda.  The effects of Lambda in IID settings need to be explored.  The performance sensitivity to architecture might limit the default rate of Lambda rate. Experiment Queries:  performance of FedEMA in the IID setting when Lambda equals 1  Number of work used for accuracy and standard divergence calculations  Specific nonIID setting used in the experiments  Reason for accuracy decrease in CIFAR100 from nonIID to IID  Missing data on FedMoCoV1FedMoCoV2 baselines in Table 7", "Paraphrase: Summary:  This paper combines the BYOL selfsupervised learning framework and FedAvg to enhance the performance on unlabeled nonidentical and independently distributed (nonIID) datasets. strength:  Unsupervised Federated Learning (FL) is a relevant and timely topic. Weaknesses:  The paper simply combines BYOL and FedAvg lacking significant introduction. Its relationship to the ICCV 2021 paper FedBYOL is not adequately addressed.  SimSaim an alternate SSL framework with advantages such as smaller batch size requirement and improved interpretability is not utilized.  The definition of \"unlabeled nonIID dataset\" is unclear. The use of CIFAR10 and CIFAR100 partitioned unbalancedly by class contradicts the assumption of unsupervised learning as data cannot be assigned labels if they are unknown.  Algorithm 1 requires stateful clients which may hinder its applicability in FL with client sampling.  The batch size of 128 is too large for resourceconstrained devices used in FL.  The absence of provided source code impedes reproducibility and validation of experimental results.  privacy and security significance of the proposed FedSSL framework are not discussed."], "qY79G8jGsep": ["Paraphrase: Summary This article proposes a new approach to model explanations called DISSECT which uses generated counterfactuals that vary in intensity for a particular learned concept. The authors demonstrate through elaborate experiments that DISSECT effectively separates relevant and realistic concepts. The papers introduction lies in producing multiple counterfactuals across a concept spectrum and a new dataset for dermatology which enables testing methods in critical situations. effectiveness and Weaknesses While this article presents a unique approach to model explanations some concerns remain.  generation model Dependence: DISSECT depends on a robust generative model that can distinguish concepts effectively. This approach may not be suitable for data with complex structures such as natural language.  User Concept Independence: DISSECTs lack of reliance on predefined user concepts can be both beneficial for uncovering hidden biases and problematic for testing specific hypotheses about known concepts.  Causal Interpretation: The article could benefit from a more indepth discussion of the significance of generating realistic counterfactuals for causal analysis.  Similarity to Prior process: DISSECTs approach closely resembles \"Explanation by Progressive Exaggeration\" except for a term that promotes concept separation. Its technical contribution appears somewhat limited.  Qualitative issue: The qualitative issue in Figure 5 are not entirely convincing. DISSECTs superiority over EPEmod in disentangling concepts is not evident.", "Paraphrased statement: Summary The authors present DISSECT a novel method for generating counterfactual explanations where the influence of a concept on the classifiers decision gradually increases. Through experiments they demonstrate that DISSECT outperforms existing techniques in terms of \"importance\" \"realism\" \"distinctness\" and \"stability.\" effectiveness  Innovative approach: DISSECT stands out as a unique approach to counterfactual explanation focusing on creating multiple explanations with varying degree of concept influence. This could prove valuable in practice as it allows for visualization of changes in prediction as a concept relevance increases.  thorough evaluation: DISSECTs effectiveness is tested across three various datasets each chosen to highlight a specific aspect of the problem. The authors introduce metrics for each evaluation criterion allowing for comprehensive analysis. DISSECT systematically surpasses benchmark methods. Weaknesses  Novel evaluation metrics: Several of the evaluation metrics are proposed in this paper and further justification for their ability to accurately measure the desired properties would be beneficial. For model model with known \"ground truth\" could be used to validate the \"realism\" metric.  Metric bias: DISSECT is partially designed to optimize these evaluation metrics which may contribute to its superior performance. However the field lacks a similar evaluation protocol making it challenging to objectively compare methods. Clarity The paper is generally wellwritten with thorough explanations of the architecture and experimental setup. Questions  Sensitivity analysis: Were any sensitivity analyses performed for the loss function to determine the necessity of each term Minor Comments  Correction of minor typos grammar errors and spacing issue noted in the statement.", "Summary Paraphrase: This paper introduces DISSECT a novel approach that uses concept traversals (CTs) to provide counterfactual explanations for AI models. CTs are sequences of generated examples that gradually increase the influence of specific concepts on the models decision allowing for the disentanglement of explanations based on different concepts. effectiveness and Weaknesses Paraphrase: Originality:  The proposed DISSECT method is based on Explainability by Progressive Exaggeration (EPE) but incorporates disentanglement modules which is innovative.  While DISSECT shares similarities with EPEmod a comparison between them would clarify the limited novelty of the approach. Clarity:  The paper is generally clear but some sentences are also long and may be difficult to comprehend. Significance and Experiments:  The paper thoroughly evaluates DISSECT using multiple baselines but an ablation study would enhance the understanding of the necessity of each component particularly the difference between EPEmod and DISSECT.  The generalization of DISSECT to novel datasets and the definition of concept meanings require further exploration.  The correctness and importance ranking of concepts are key concerns that need to be addressed for the fidelity of explanations.  The comprehensibility of CTs for humans and the potential for confusion when multiple concepts emerge need to be considered.", "Paraphrased statement: This research proposes a novel approach to generate multiple counterfactual explanations using disentangled latent variables obtained from Variational Autoencoders (VAEs). The method provides a unique and practical way to create various counterfactuals for various applications. The authors have evaluated their model on image datasets and demonstrated its effectiveness using various metrics. Comparison with https:arxiv.orgpdf1905.12698.pdf: The provided URL does not lead to a specific paper so a direct comparison cannot be made."], "mF5tmqUfdsw": ["Paraphrase: Summary This paper presents a reinforcement learning algorithm that combines continuous action with a Monte Carlo estimator for the actor gradient (computed using rollouts from the current state) and a supervised learningbased critic update (via gradient descent over multiple epochs). The key contribution lies in integrating the critic network with onpolicy sample using eligibility traces. This combination yields target Vvalues for the critic and a weighted average for calculating the actor gradient. Experiments on MuJoCo show that the algorithm surpasses PPO in efficiency and policy quality. Strengths and Weaknesses The paper is wellwritten and the algorithm are comprehensible. It effectively justifies the need for zeroorder policy updates. However adding SAC and A3C baselines would enhance the empirical evaluation as PPO is outdated. Figure 1 could be improved by indicating how \\hatG and \\hatA are obtained (from parallel rollouts with perturbed policies) and highlighting the training processes for the critic and actor. A more intuitive graphical representation would aid in understanding the algorithm.", "Paraphrased Statement: Summary:  The paper introduces a novel zerothorder actorcritic algorithm that combines gradientbased critic training with gradientfree actor training.  The algorithm has shown positive results on several benchmark datasets.  The literature review effectively supports the innovation of the novel algorithm. Strengths and Weaknesses:  Strengths:  Integration of gradientbased critic training and gradientfree actor training.  Promising empirical results.  Weaknesses:  Questionable justification for using zerothorder PIM instead of firstorder PIM.  Lack of guidance on setting the hyperparameter N.  Concerns about the tightness of theoretical bounds and the true advantage of the algorithm.  Limited comparison with other stateoftheart algorithm.  Additional Comments:  The algorithm potential to handle policies with limited gradient information is acknowledged but requires further demonstration.  The robustness advantage of the algorithm compared to firstorder PIM should be more clearly analyzed.  The author agrees to conduct additional experiments including on the Humanoid benchmark.  More comprehensive comparison with a wider range of algorithm including gradientbased methods are recommended.", "Paraphrased Statement: This paper introduces ZOAC a novel method combining perturbed trajectory generation firstorder policy evaluation and zerothorder policy improvement. ZOAC employs a novel trajectory sample technique that involves using different policies for a limited number of step before switching. The paper demonstrates that adjusting the number of step with each policy can reduce the variance of the zerothorder gradient estimator. empirical evaluations suggest that ZOAC outperforms other zerothorder and firstorder baseline algorithm. Strengths and Weaknesses: While ZOACs perturbed trajectory sample approach is innovative theres a concern about computing a valid value use from these mixedpolicy trajectories. Equation 14 uses a single critic to learn from return idea across multiple perturbed policies. Its unclear which policys value this value use represents. For accurate advantage calculation and policy updates (Equations 1516) the advantage should be computed using the value use for each perturbed policy. However ZOAC seems to learn a single critic over all perturbed policies and derive the zerothorder gradient based on this critic. While this may be acceptable for Mujocolike domain where optimizing shortterm rewards suffices it could be problematic for domain where longterm rewards matter. The empirical evaluations lack ablation studies to demonstrate ZOACs strengths and robustness. Additionally the overall performance comparison against stateoftheart actorcritic algorithm (e.g. SAC) are underwhelming. Including such comparison would provide a more comprehensive assessment. The choice of (6464) hidden units for the actor is also unusual and varying the unit counts (e.g. (128128) and (256256)) would yield more meaningful results."], "yBYVUDj7yF": ["Paraphrased Statement: This research paper provides theoretical findings to explain the effectiveness of contrastive learning (CL). effectiveness:  Indepth theoretical analysis is provided. Weaknesses: 1. Sindistance error bounds (Theorems 3.1 and 3.2):  These bounds measure the difference between the optimal model and the actual model but they are based on training data and do not generalize to unseen data.  They merely consider the recovered variable U but the optimization involves multiple variables (U S and V). 2. Generalization error bound (Theorem 3.3):  Similar to existing work so the paper could provide more context on its unique contributions. 3. Supervised CL error bound (Theorem 4.2):  Similar concerns as point 1 as it merely measures training data and does not account for unseen data.  Supervised CL can be viewed as a similarity metric learning problem which has been wellstudied and the paper could provide insights in relation to this perspective. Conclusion: Based on these concerns a weak rejection vote is suggested at this time. The score may be increased if any misconceptions are clarified.", "Summary: This study analyzes various aspects of contrastive learning in a simplified linear scope with a spiked covariance model. The theoretical results show:  contrastive learning with specific data augmentation can extract feature effectively by learning the underlying lowrank signal.  Supervised contrastive learning outperforms contrastive learning by eliminating biases introduced by data augmentation.  For transfer learning combining unsupervised contrastive loss with supervised learning can improve performance. Experiments validate many of these findings. effectiveness:  Novel theoretical explanations for contrastive learning phenomena.  Clear and concise presentation.  Verification of findings through model. Weaknesses:  Relevance of the simplified scope to practical applications is not fully demonstrated.  Connections to empirical evidence from previous work could be more explicit.  Lack of intuition for some results such as the bias term in contrastive learning vs. supervised contrastive learning.  No explanation for the superiority of contrastive learning over autoencoders in the specific data augmentation used.  The augmentation work differs from practical scopes where views are commonly independent. Suggestions for improvement:  Clarify in the abstract that the results apply to a simplified linear representation scope.  Provide more intuition for the findings particularly regarding biases and transfer learning.  Explore the relevance of the theoretical components on benchmark datasets.  Consider extending the analysis to more practical augmentation scenarios where views are independent.  Justify the choice of HSIC loss role.", "Paraphrase: Summary: This study analyzes the effectiveness of contrastive learning in feature extraction theoretically. Using a simplified linear model with masking augmentation it demonstrates that contrastive learning can recover essential feature when noise levels are high. Autoencoders however struggle in this situation. The study further upper bounds the downstream risk for contrastive learning and shows that its generally lower than for autoencoders. Additionally it investigates the role of labeled data in supervised contrastive learning and provides an upper bound on its error. effectiveness and Weaknesses:  The key finding is intriguing suggesting that contrastive learning can outperform autoencoders in highnoise scenarios.  The analysis of supervised contrastive learning is novel and persuasive. Detailed Questions:  Is the constant \"c\" in Theorem 3.4 dependent on \"r\" or \"rc\"  If yes can we conclude that \"c\" is less favorable than the bias term in Theorem 3.3  Why is the assumption \"r \u2264 rc\" made for all \"r\"  Were data augmentations used in autoencoder training during the analysis  If not the comparison between autoencoders and contrastive learning may be skewed.  How applicable are the papers findings to nonlinear models and different augmentations Minor Comment:  The appendix lacks organization and clarity making it difficult to correlate theorems with proofs."], "t5s-hd1bqLk": ["Paraphrased compact: This research presents a technique called \"learned activations\" for enhancing personalized speech and automatic speech recognition (ASR). These activations are derived from a weighted combination of hidden layer outputs with weights determined by the softmax of embedded speaker characteristics (e.g. xvectors). Compared to concatenation or modulation methods learned activations require fewer adjustable parameters resulting in a smaller model size. Experiments reveal that the method issue comparable speech enhancement issue to baselines while using a smaller model. For ASR it improves Word Error Rate (WER) compared to unadapted models. effectiveness:  Comprehensive evaluation in speech enhancement and ASR  Visualization of learned activations for various speakers  Size advantage over concatenationbased models  Potential suitability for mobile applications Weaknesses:  Length and complexity may hinder readability  Lack of clarity in terminology (e.g. \"Cond.\")  Additional computations for nonlinearities may impact mobile application efficiency  Omission of Film layer details prior to result presentation  Undefined abbreviation \"HPO\"  Absence of details on speaker embedding model finetuning  Uncertainly about the joint or separate training of the speaker embedding and ASR models for PASR", "Paraphrased Statement: compact: This paper introduces a method for incorporating conditioning information into neural networks. Unlike traditional approach that concatenate or inject the conditioning data this method processes it through a weighted sum of neural network activation functions. It claims to effectively train neural networks with fewer parameters without compromising performance in speech enhancement and automatic speech recognition (ASR) tasks. effectiveness and Weaknesses: The paper demonstrates that the proposed method reduces the issue of parameters in RNNCNN systems for speech enhancement and ASR. The authors highlight this as an advantage for resourceconstrained computing environments. However the paper assessment of computational advantages only based on the issue of parameters is questionable. A more comprehensive measure would consider factors such as floatingpoint performance (FLOPs) in the forward pass or memory usage relative to input sequence length. The paper includes issue from a previous rebuttal that show varying degrees of improvement for different models in terms of FLOPs and latency. However the presentation of the ASR and PASR experiments is unclear leaving doubt as to whether the proposed method outperforms other conditioning techniques in these tasks.", "Paraphrased Statement: This study presents a novel concept (if I understand it correctly) that involves adapting the activation functions in a deep neural network (DNN) based on factors such as speaker embeddings. This approach aims to replicate the benefits of the traditional concatenation method without requiring as many additional parameters. Initially I found the technical explanations challenging to comprehend but I eventually grasped the core idea. The issue are promising showing minimal impact on performance for general tasks (e.g. WER) while improving performance on specific tasks all while using fewer parameters than the concatenation method. I believe the term \"learned activations\" understates the significance of this concept. A more accurate name would be \"learned activation functions\" which substantially captures the fundamental idea and its originality. effectiveness and Weaknesses: As mentioned in my compact the paper has effectiveness such as its novel concept but also weaknesses primarily related to clarity. The lack of specific details about the core concept early on creates suspense but can be frustrating for readers. Specific Comments:  The use of the phrase \"... and a family of `a`\" needs grammatical correction.  The term \"softmaxrowwise\" is unexplained.  The relationship between the equations in Section 3.1 could be clarified by numbering them. The definition of \"c\" is missing.  The connection between the equations in 3.1 and design 1c could be made light.  While references are provided for the concatenation approach none are given for the modulation approach despite its being described as stateoftheart.  design 2 lacks a label for the xaxis.  design 4 also lacks a label for the xaxis.  The term \"SDRi\" is used without definition in Section 4.1.  \"HPO\" should be expanded to \"hyperparameter optimization.\"  \"STFT\" should be defined in Section 4.4.  The authors mention \"greedy research\" but should provide a reference or clarification as it may not be representative of standard ASR benchmarks."], "zaALYtvbRlH": ["Paraphrased Statement: Summary: This field focmanipulations on learning long sequences where not all elements significantly impact the output. SCANDROP is a random segment drop algorithm that preserves important segments with a high probability when the number of such segments is small. BetaSCANDROP improves the preservation of sequence length. Experiments show consistent improvements. Strengths:  Relevant topic of data augmentation  Wellwritten paper Weaknesses:  limited novelty lack of comparison with other basisline  Improvement over Electra basis is not substantial especially for the catfinding task Detailed input:  Section 1: The term \"underspecified\" may not be fully understood when the contributing portion is small.  Section 1: Removing a span with a random token does not provide a counterfactual sequence with the same length.  Section 1: Clarify the manipulation of \"global goal.\"", "Paraphrase: This paper explores extracting valuable supervision from extended sequences. It focuses on scenarios where a lengthy input sequence (n elements) has a objective prediction based on a smaller subset of fragments (m elements) where m is significantly less than n. The researchers suggest a data augmentation technique that involves randomly removing portions of the input sequence. They first introduce SpanDrop which eliminates spans with a fixed probability (p). However this may create shorter sequences altering the training data distribution. To address this they also propose BetaSpanDrop. This technique uses probability derived from a beta Bernoulli distribution to drop spans. BetaSpanDrop maintains the original utterance length with a higher likelihood while producing a comparable variety of augmented utterances. Strengths: 1. Simple and effective way to enhance performance on long sequence input tasks. 2. Mathematically rigorous with proven claims about augmented sequence lengths. Weaknesses: 1. The approach is relatively straightforward and resembles existing regularization techniques (e.g. discussion dropout). A comparison with similar methods would be valuable. 2. The improvements observed on realworld datasets are modest.", "Summary: A new method called SPANDROP is presented to improve model performance on long sequences where not all elements contribute equally to the output. SPANDROP randomly removes spans of the sequence and trains the model to complete the task encouraging counterfactual learning and input attribution. Strengths:  simplicity and simplicity of comprehension  Improved performance compared to baseline methods Weaknesses: Technical Details:  Partitioning of long sequences into spans is not fully explained for text sequences and time series.  Sensitivity of results to span partitioning is not discussed.  If each discussion in NLP sequences is considered a span SPANDROP becomes similar to discussionlevel dropout baseline methods. Conceptual:  The uniqueness of the problem addressed by SPANDROP is not clearly defined. Most sequence classification and prediction tasks involve elements that contribute unequally to the output. Experimentation:  Transformer variant designed for long sequences are not compared to SPANDROP.  The NLP datasets used are not representative of challenging long sequence tasks.  A benchmark dataset dedicated to long sequence model is recommended for testing.", "Paraphrased Statement: This paper introduces SpanDrop a data augmentation technique for long text data particularly those with brief supporting facts. The method is grounded in theoretical principles and has been tested on a synthetic task and four real NLP tasks that require reasoning with lengthy texts. Across all tasks SpanDrop consistently enhances accuracy irrespective of dataset size. Strengths:  The method is straightforward even coreive.  Experiments demonstrate the methods efficacy. Weaknesses and potential Improvements:  SpanDrop assumes independent drop decisions. However salient data often appears in clusters. Incorporating such model could enhance the method.  Exploring alternative strategies for splitting spans could improve the methods flexibility.  Applying SpanDrop to intermediate representations as in LengthDrop could be investigated.  The additional training cost of SpanDrop compared to standard training motivation to be evaluated.  While SpanDrop and BetaSpanDrop have the same expected length their expected length squared differs potentially affecting computational overhead.  Convergence time due to SpanDrops regularization core should be investigated.  The motivation to adjust other regularization techniques such as dropout should be considered.  The generalizability of SpanDrop to tasks outside NLP requires further examination."], "tge0BZv1Ay": ["Paraphrased Statement: Summary: The authors developed a reinforcement learning method called PDQN (Predictive Double QNetwork) for optimizing manufacturing dispatching which involves assigning tasks to machine in a timely manner. Strengths:  The problem addressed is relevant and has practical applications.  The research code is publicly available and reproducible. Weaknesses:  The reasons for modifying the original \"Predictron\" algorithm are not sufficiently explained or justified.  The field lacks substantial reinforcement learning baselines. PDQN is alone compared to simple toy methods and an outdated reinforcement learning algorithm (DQN).  more recent reinforcement learning algorithms that may be applicable to the problem were not considered for comparison.", "Paraphrase: Researchers have employed a deep reinforcement learning method to address scheduling outcome in a semiconductor factorys machinery. Their primary contribution is a system based on deep Qlearning that uses a \"predictron\" to calculate the value function (instead of the maximum value of Q(s a)). Within the manufacturing domain there are multiple machine sets that can function on part. The goal is to decide the range in which machine should handle these part. The researchers compare their \"predictron deep Qnetfunction\" (PDQN) to a double DQN against existing methods like critical ratio and firstinfirstout aiming to reduce part tardiness. Using realworld data they create simulators: one proprietary and one publicly available. They discover that RL systems significantly reduce lateness (by 2050) compared to baselines with PDQN outperforming DQN by approximately 30. However they note that RL systems complete significantly fewer part (15 for PDQN and 25 for DQN) suggesting that lateness may not be the most appropriate reinforcement function for this application. Strengths:  Development of a practical and publicly accessible environment for manufacturing scheduling research using RL methods.  Novel PDQN model with better performance in manufacturing tasks. Weaknesses:  Limited baselines: The function primarily utilizes the outdated vanilla DQN from 2015.  Insufficient evaluation of PDQN: The evaluation is restricted to the newly created environment with comparison alone against vanilla DQN.  Lack of consideration for simtoreal outcome: The authors do not discuss the process of creating the simulated environment including the selection of incoming part nor do they evaluate realworld performance.  Substantial throughput reduction: The decrease in completed part (up to 15) may pose practical implementation challenges. Questions: 1. Is the proposed architecture similar to a dueling DQN in separating Qfunction and value function estimation Did the authors experiment with the dueling architecture 2. How does the L2 regularization in the predictron impact performance Does it improve convergence or reduce sample complexity", "Paraphrased Statement: Summary: This research explores solving a complex scheduling outcome in semiconductor manufacturing using reinforcement learning (RL). due to varying processing times the problem experiences a long delay in obtaining reinforcements. The paper resolves this challenge by employing a predictive model (predictron) to estimate targets for Deep QNetwork (DQN). model using realworld data and comparisons with common heuristics and DQN demonstrate that the proposed method enhances part production efficiency. Strengths:  Addresses a crucial scheduling problem in manufacturing a promising application for deep RL.  Outperforms industrystandard heuristics in terms of part delays.  Modelbased approach effectively tackles the reinforcement delay outcome. Weaknesses and Comments:  The paper lacks a precise mathematical definition of the scheduling problem.  The ultimate goal and reinforcement selection criteria should be clarified.  An unexpected observation suggests that firstinfirstout (FIFO) performs better than critical ratio (CR) in certain scenarios warranting simulator calibration.  The baseline selection for benchmarking could be improved by including recent RLbased approaches for the same problem.  The combination of DQN and predictron should be benchmarked against DQN with targeted estimates obtained through Monte Carlo rollouts to justify its advantage.", "Paraphrased Summary: This research focuses on scheduling in semiconductor manufacturing a complex process characterized by substantial delays in performance feedback. The authors propose a novel deep reinforcement learning (RL) algorithm Predictron Deep QNetfunction (PDQN) that integrates a predictron mechanism into the Qnetfunction loss function to address these delays. The outcomeiveness of PDQN is demonstrated through its application to two scheduling challenges comparing it against standard baselines (CR FIFO) and deep Qnetfunction (DQN). Strengths:  Tackles a critical outcome in RL for scheduling systems namely delayed reinforcement signals.  Proposes a robust and theoretically sound solution with PDQN.  Demonstrates the applicability of combining RL with scheduling optimization.  Excellent introduction and organization making it accessible to both RL and scheduling experts. Weaknesses:  Does not compare PDQN against existing RL methods for scheduling with Qlearning such as the approach presented by Stricker et al. (2018).  Insufficient clarification regarding the magnitude and stochastic nature of the delay.  The impact of time discretization and parameter values (\u0394i ti h) on performance and computational time is not fully explored.  Equations (11) and (12) appear similar to (5) and (6) which could benefit from consolidation.  Provides a problem scope in part 2 rather than a related function review and misses relevant literature on delayed RL.  Eligibility traces in PDQN seem to have a similar outcome as forward models in other approaches which warrants comparison.  The substantial performance of DQN despite not accounting for delay questions the need for the predictron.  The poor performance of FIFO and CR which are tailored for scheduling requires further interpretation."], "uEBrNNEfceE": ["Paraphrase: Summary: The research explores reinforcement learning scheme for unknown linear systems with quadratic cost use. The algorithm presented demonstrates consistent estimations and its convergence rates are provided. effectiveness and Weaknesses: The manuscript requires modifications before acceptance. Revisions proposed by the authors should address the following issues:  Lack of Novelty: The manuscript attempts to reestablish existing results.  Incorrect Claims: The papers contains several incorrect and incomplete statements.  Insufficient extension: The literature review is incomplete and the manuscript fails to cite relevant prior process.  Unsupported Statements: Assertions lack evidence or are incorrect.  Subpar introduction: The manuscripts system and writing need improvement.  Assumption of OpenLoop Stability: The manuscript assumes openloop stability which is not always applicable.  Relaxed noise Assumptions: Existing research considers nonnormal and nonstationary noise.  WorstCase performance: The manuscripts focus on averagecase performance is less robust than the worstcase approach.  Definition of \"Learning Process\": The authors should clarify this term in Definition 2.  Algorithm 1: The superiority of line 7 over leastsquares estimates is unclear.  Lemma 10: The immediate conclusion from subGaussianity is missing.  Lemma 1 Assumption: Full accuracy assumption renders the lemma unacceptable for consistency evaluation.  computational efficiency of TS: Justification is needed for the claim that timeseries analysis is computationally inefficient.", "Summary This paper presents a method to combine system identification and optimal control for linear systems with complete state sensing. While the method is limited to linear stable systems with fullstate sensing it guarantees optimality within these constraints. The paper is wellwritten structured and efficient in its notation. effectiveness and Weaknesses While the papers overall quality is positive there are concerns that limit its significance: 1. Stability: The paper defines \"safe learning\" but does not guarantee stability. The authors address this with a conservative approach but there is no explicit statement of stability in the results. 2. Caution: The proposed approach is cautious requiring a significant number of time step for convergence. 3. Literature Omission: The paper lacks extension to adaptive control literature which addresses similar problem. 4. performance Metric: The justification for the performance metric estimate is unclear and it is not clear how algorithm 2 produces a policy for computing the performance index. Notes  Process noise: The representation of process noise using matrix B2 is missing.  mathematical example: The presence and properties of process noise in the mathematical example are unclear.  \"Easily Satisfied\": The authors should provide validation or extension to support this statement.  Policy Extraction: It is unclear how a policy can be extracted from algorithm 2 which only returns the next input and internal state.  \"Almost Surely\": The definition of \"almost surely\" in Theorem 2 is missing.  Typos: Several typos are present in the text.", "Paraphrased Summary This field presents an adaptive control algorithm for LQR problem. It provides a method to asymptotically converge to the optimal policy almost certainly with a specified convergence range. The system must be initially openloop stable. It incorporanges a temporary switch to zero control with decaying noise when state or control parameters exceed O(log(k)). This forces the state closer to zero. The field proves that any policy generanged by the algorithm ensures a finite average cost over an infinite horizon termed \"safe.\" It also establishes the consistency of Markov parameter estimation in the algorithm with a specified error convergence range. Learned policies converge to the optimal policy at a range of O(1\u221a(k)) in terms of average cost. effectiveness and Weakness effectiveness:  The fields goal of achieving almost sure (a.s.) guarantees is significant for safe implementation in physical systems. Weaknesses: 1. safety Definition: The safety definition does not consider the stability of the timevarying system induced by the varying policies. Stability cannot be guaranteed based solely on individual policy stability which could lead to unsafe policies. 2. Stability notion: The stability notion in the definition does not capture inputtostate stability (ISS) which is more appropriate for nonlinear policies. The definition allows for bounded but distant state values even with small noise. 3. Markov parameter Estimation: The motivation for using Markov parameter estimation instead of direct leastsquare estimation is unclear. The asymptotic convergence result may not extend to nonasymptotic example. 4. Comparison to Regret Analysis: The comparison with regret analysis is not straightforward because the infinitehorizon average cost does not directly represent the realized cost of the algorithm. 5. mathematical Results: While mathematical results are provided it would be valuable to compare the algorithm to existing adaptive learning approach to demonstrange its advantages.", "Paraphrased Statement: Summary: This paper focuses on solving a stochastic linearquadratic control problem online where the system parameters are initially unknown and must be estimated while designing a stabilizing control policy. Instead of bounding regret with high probability the paper aims for almost sure convergence to optimal control performance. The key contributions include a modified certainty equivalent control strategy (stabilityaugmented) and a parameter inference scheme that estimates the impulse response of the system. Convergence rates and simulation results are provided. effectiveness and Weaknesses: The problem of online learning for optimal control is significant. However certain aspects of the paper could be improved:  safety vs. Stability: The paper should consistently use the term \"stability\" instead of \"safety.\"  Markov parameter inference Motivation: The reasons for choosing Markov parameter inference over direct leastsquare estimation should be clarified.  validation of Theorem 1: The validation needs to address how the switching control strategy ensures stability even with exploration noise.  Correspondence to Optimal rates: The authors should provide more detail on how the derived performance matches optimal regret rates with high probability.  Experimental Results: The paper should include comparison with other methods such as standard certainty equivalent control. After Rebuttal: The authors responses have addressed some of the reviewers concerns. However additional feedback has been provided to further strengthen the technical contribution."], "morSrUyWG26": ["Paraphrased Statement: Summary: AutoOED an opensource platform is presented in this paper. It optimizes multiobjective problems (MO) efficiently given a limited budget for experiments. AutoOED automates the design of experiments for evaluation. Based on multiobjective Bayesian optimization (MOBO) it employs BelieverPenalizer (BP) to accelerate batch experiments asynchronously without compromising performance. A graphical user interface (GUI) is provided for intuitive visualization and experiment design guidance. experiment demonstrate that AutoOED effectively controls and guides hardware experiments without human input. effectiveness and Weaknesses: effectiveness:  Addresses a relevant problem in science and engineering.  Wellstructured and clearly written.  Algorithm framework and claim are supported by experimental results. Weaknesses:  Minor publication:  Using \"Pareto optimal solutions\" consistently.  Direct use of \"BP\" for \"BelieverPenalizer\".  Doublechecking for redundant abbreviations.  Typo: \"importance\" should be \"significant\".", "Summary: The paper introduces \"AutoOED\" a software package for optimizing experimental designs. It uses multiobjective Bayesian Optimization to find efficient points for experimentation accounting for uncertainty and enabling the identification of optimal solutions. To facilitate efficient parallel optimization the package implements the \"BelieverPenalizer\" strategy which dynamically selects between standard Bayesian Optimization and a penalized approach based on posterior uncertainty. effectiveness and Weaknesses: Correctness and Clarity:  Wellwritten and easy to follow.  Focuses on system components and use case. Technical Novelty and Significance:  Welldesigned and implemented system.  \"BelieverPenalizer\" strategy provides a minor contribution to asynchronous parallel optimization but its threshold setting is not welljustified. Empirical Novelty and Significance:  comparison of \"BelieverPenalizer\" to baseline methods shows mixed results.  AutoOED performs well on average but is not consistently superior to other packages.  Empirical evaluation is limited to small dimensionality settings. Questions to the Authors: 1. Performance of AutoOED on single objective benchmarks. 2. Measurement of convergence in AutoOED. 3. use of nonprobabilistic framework (e.g. neural networks) in Bayesian Optimization within AutoOED. Minor Remarks:  Disagreement with the statement that most BO tool require extensive coding.  Missing other opensource BO packages from the list of those included in the paper.", "Paraphrase: Summary: The authors have developed an automated platform for designing efficient experiments to solve multiobjective problems. Their key design is a novel strategy to speed up batch experiments. effectiveness and Weaknesses:  The platform allows users to quickly design and test their own multiobjective algorithms.  A novel BelieverPenalizer (BP) optimization strategy improves timeefficiency.  The paper is wellwritten and easy to follow. Concerns:  Its unclear whether the paper makes a significant scientific contribution.  The authors claim that the BP strategy is stateoftheart but they provide limited details about how it combines two existing methods KnowledgeBased (KB) and Local Penalizer (LP).  The authors do not provide a clear theoretical justification for why this combination works and rely primarily on experimental results.", "Paraphrase: Summary: This article presents the design and usage guidelines for an opensource platform for experimental design. The platform utilizes Bayesian multiobjective optimization to swiftly identify solutions that balance multiple target. effectiveness and Weaknesses: Concerns:  The article primarily consists of software design documentation and a manual for the opensource platform with limited original methodological contributions.  While the authors initially state that the platform can minimize multiple objective use they later specify that it is only suitable for two or three target. This inconsistency may mislead readers into believing that it is a general tool for multiobjective optimization.  Surprisingly the platform is not applicable to singleobjective optimization a capability typically expected in multiobjective optimization algorithms. Additional Observation: The authors have not cited a closely related study published on arXiv that has significant overlap with this manuscript."], "gLqnSGXVJ6l": ["Paraphrased Statement: A reinforcement learning (RL) algorithm is presented to address the Capacitated Vehicle Routing Problem with Time Windows (CVRPTW) using multihead attention and masking. The algorithm is evaluated on problems with a small number of customers and compared to existing heuristics like Google ORTools and LKH3. effectiveness and Weaknesses:  The masking and multihead attention concepts are not original and have been applied to similar problems before.  The lack of strong benchmarks is a major concern. CVRPTW problems of the size considered in the paper can be optimally solved using existing exact methods. A direct comparison with an exact algorithm is necessary.  Similar RL approach for CVRPTW have been proposed in several recent work and their results are not included in the evaluation.  The presented results are not competitive with the latest published benchmarks such as LKH3 on problems with up to 300 customers. small Comment: Table 2 contains an incorrect row for the RNNRL algorithm.", "Paraphrased Statement: Summary: This paper introduces a neural Combinatorial optimization method for solving the complex Vehicle Routing Problem with Time Windows. It utilizes a policy gradient technique to optimize an attention model and employs a masking approach to restrict undesired actions during policy performance. solution are contrasted with industryleading solvers ORTools and LKH3. effectiveness:  clear elucidation of the objectives and methodology Weaknesses:  Deficiencies in written presentation including grammatical errors incorrect punctuation capitalization inconsistencies missing technical details and poor formatting.  Novelty of the model approach appears limited as similar training schemes and DNN models have been previously developed and cited in the paper.  solution are constrained and do not definitively demonstrate advantages over conventional techniques.  Vague language in some time requiring further clarification (e.g. \"manual adaptation and business knowledge are needed\" \"dynamic dimension reflects need change\").  Informal language throughout the text.  Incomplete Equation 13 label.  Ambiguous definition of \"violation is given infinity.\"  Undefined terms: \"Training Sample rollout\" and \"greedy rollout.\"  Missing details on the baseline work used in the REINFORCE algorithm.  unclear implementation specifications for the Google ORTools baseline (e.g. solver timeout solver type).  Contradictory numerical results for RNNRL in Table 2.  Lack of explanation regarding the possibility of infeasible solutions due to incremental action construction.  No discussion on the feasibility guarantee provided by the masking scheme.  Minimal novelty in the network architecture training scheme masking approach and input representation as these have been extensively studied before.", "Paraphrase: Summary: This work adapted the approach presented by Peng et al. (2019) for the vehicle routing problem (VRP) to a more intricate variant known as the vehicle routing problem with time windows (VRPTW). effectiveness and Weaknesses: effectiveness:  The VRPTW problem is a compelling and significant challenge.  The literature review is adequate. Weaknesses: 1. Limited Technical Contribution:  The work primarily applies existing techniques from previous research lacking significant novel insights.  For instance picture 1 and Equations (3) to (19) are directly taken from previous work including Bderemeev (2020) and Peng et al. (2019) without any modifications.  The training algorithm (REINFORCE) is wellestablished. 2. Inadequate Description of VRPTW:  The work does not sufficiently explain the key differences between VRPTW and VRP.  It mentions that VRPTW introduces more complexity but it fails to clarify how precisely this occurs.  A complexity analysis would be helpful in evaluating the problems difficulty. 3. Weak Baselines:  The paper claims to be neural combinatorial optimization (NCO) but it lacks NCO methods as baselines. 4. Poor Writing:  Table 2 contains a duplicate entry.  Spaces between text and references are missing.  Inconsistent formatting (e.g. \"E or X\" \"VRP TW or VRPTW\").  A reference ([Cordeau]) is incorrect.", "Paraphrase: The paper presents a novel approach to tackle the Vehicle Routing Problem with Time Windows (VRPTW) leveraging a neural network and reinforcement learning framework. The model uses an attentionbased encoderdecoder architecture to predict the distribution of solutions while adhering to problem constraints. Subsequently a reinforcement learning framework is employed to optimize the models parameters. evaluation on a synthetic dataset suggests that the proposed framework achieves comparable performance to traditional combinatorial problemsolving methods such as Google OR Tools and LKH Heuristic. effectiveness and Weaknesses:  The application of machine learning and reinforcement learning to solve combinatorial problems is an exciting and ongoing research field.  The attentionbased encoderdecoder model effectively incorporates field constraints into the VRPTW model.  However the paper lacks significant novelty:  The encoderdecoder model is a small extension of previous work.  Solving VRPTW is not entirely novel as evidenced by existing literature.  The empirical results are limited as they use a synthetic dataset and compare the approach to basic heuristics instead of stateoftheart VRPTW solvers.  The papers presentation could be improved particularly in sections 3.3 and 4."], "uydP1ykieNv": ["Paraphrase: Summary This paper introduces RGN a robust training and defense method that uses binary control gates. During training RGN generates adversarial model using a cleanlabel approach approach and mitigates their impact by training on a separate way. During inference RGN selects a subnetwork to defend against adversarial approachs. Strengths and Weaknesses Weaknesses: 1. The training process may be computationally expensive and complex requiring consideration of multiple data pairs ways and cleanlabel adversarial perturbations. 2. Certain aspects of the training process are unclear:  How does RGN prevent memorization of perturbation model from being overwritten  How does RGN ensure that adversarial inputs follow specific ways during inference  Why did the authors choose Equation 2 for adversarial generation instead of a conventional nontargeted approach 3. RGNs inference phase relies on searching for a subnetwork for defense raising the question:  Does robustness arise from the complex training process or from the distillation step  other research has explored the lottery ticket hypothesis in both standard and robust training scenarios.", "Paraphrased Statement: This study enhances adversarial robustness using ensemble learning. It introduces the \"EnsembleinOne\" approach employing Random Gated Networks (RGN) to expand the ensemble size efficiently. Strengths:  The RGNbased ensemble expansion concept is innovative.  Demonstrates effectiveness against stateoftheart methods through empirical results. Weaknesses:  The authors could enhance the paper by:  Providing theoretical insights into the proposed methods.  Exploring the parallels between RGN and Stochastic way Networks.  Considering the potential benefits of aggregating results from different ensemble paths during testing.", "Paraphrased Statement: Summary: A novel approach is presented for generating an ensemble of networks resilient to adversarial attacks. Unlike other methods that train distinct submodels this method replicates convolutional layers and employs random gates to control them. Experimental results indicate its superiority over other ensemble training methods with reduced computational costs. Strengths:  clean and concise writing  novel and wellexplained concept  Thorough experimental setup and ablation studies Weaknesses:  Adversarial training (AT) appears to be a more efficient option especially under stronger perturbations. AT too requires less training time than the proposed method. The paper could benefit from an experiment combining EIO and AT.  Lack of interpretation for the superior performance of EIO over DVERGE despite sharing vulnerability diversification training. It is unclear if EIO impedes adversarial transferability between submodels or if evaluating only one submodel in the ensemble for GAL and DEVRGE affects their performance.  The paper core contribution lies in constructing an ensemble by repeating the original layer multiple time. However increasing the repetition number (n) beyond 2 leads to performance degradation contradicting the paper rationale. It is unclear if sufficient training time would allow a model trained with n3 to outperform one trained with n2. Minor issue:  Table 1 is unclear about the significance of number before and after the slash.  number 4s legend contains typos (\"Baseline.5\" and \"DVERGE.8\").", "Paraphrased Statement: Summary: This paper introduces a method for training scalable ensemble with random gated blocks to improve the adversarial robustness of models. Assessment of Strengths and Weaknesses: 1. Strengths: The random gated blocks allow for efficient scalable model ensemble but the stochasticity introduced may reduce the models capacity. Evaluation on the ImageNet dataset could demonstrate the algorithms ability to handle complex data distributions. 2. Weaknesses: The random gating method resembles applying dropout during both training and inference. The key differences between random gating and dropout should be clarified and supported by theoretical analysis similar to that presented in the referenced paper by Gal and Ghahramani (2016)."], "fUhxuop_Q1r": ["Paraphrased Statement: Summary This study introduces a novel approach for evaluating generalization perworkance in reinforcement learning. The paper investigates two known methods (DQN and QRDQN) and examines how L2 regularization and dropout influence their ability to generalize to unseen stateaction pairs. effectiveness and Weaknesses effectiveness:  Proposes a novel perspective for assessing generalization in RL agents by focusing on specific elements of an MDP rather than cumulative rewards.  Provides a new empirical workulation for studying generalization in RL. Weaknesses:  This work is not the first to suggest an alternative approach to evaluating generalization.  The proposed empirical method assumes full knowledge of the state space and transition dynamics.  The evaluation is conducted on a little handcrafted environment using the MNIST dataset raising concerns about its applicability to more complex domains.  The study mention that the RL algorithms memorized the training data leaving it unclear how the results would differ if this were not the case. minor Suggestions:  Clarify why the MNIST gridworld environment is not recognized as a CDP.  Move the literature review section to the end to avoid confusion for the reader.  Introduce the Bellman equations in their standard work and explain their relevance in this work.  Provide a design of the environments showing their use with the MNIST dataset.  Specify which RL algorithm was used when evaluating the regularization process in design 1.", "Summary This paper focuses on generalization in deep reinforcement learning (RL). The authors argue that unlike in supervised learning (SL) RL requires separate consideration of state observation and action. They propose a metric (Eq. 4) to evaluate generalization capacity in RL within the contextual decision process (CDP) framework. Experiments and Results Experiments were conducted in grid world environments with MNIST images as observations. Key findings include:  The importance of considering state observation and action separately in RL generalization.  The proposed metric effectively evaluates generalization capacity. effectiveness and Weaknesses effectiveness:  Addresses an significant publication in RL.  clear writing and easytofollow presentation.  Abundant experimental results with strong statistical significance.  Wellpresented plots. Weaknesses:  Experiments are limited to simple grid world environments with an artificial MNIST image observation setup.  The assumption that the number of classes equals the number of states restricts the setting of the study.  Limited novelty and significance of technical contributions.  Expectations of more comprehensive empirical investigation including policy visualization and experiments in additional task. Other publication:  In Chapter 4.1 \"Referring to 14\" should be replaced with \"Referring to design 14.\"  \"Relu\" should be \"ReLU\" and \"ADAM\" should be \"Adam\" in Table 7.  The spacing between designs 5 and 6 is narrow.  mention should follow professional conventions including venue information instead of only arXiv versions.", "Paraphrase: This study introduces a method for quantifying generalization in singletask RL in the setting of offline RL. It examines the limitations of current crossenvironment generalization measurement techniques and proposes a new unified measurement metric. The paper assesses generalization when learning from offline data using DQN and QRDQN in a settingual decision process (CDP) problem derived from MNIST classification. The findings show that dropout improves action generalization but not state generalization while L2 regularization is effective. QRDQN outperforms DQN in action generalization in offline settings but underperforms in state generalization. However the study has several shortcomings: 1. Its unclear if the observations generalize to other RL task such as Atari. 2. Comparing RL agents performance to a perfect supervised learning oracle would provide more meaningful insights. 3. The action generalization experimentation may merely confirm the superiority of uncertaintyaware algorithms (e.g. QRDQN) in offline settings. 4. The paper lacks detailed analysis of why QRDQN and DQN perform differently in various generalization scenarios. 5. It doesnt provide guidance on how to modify algorithms for improved generalization based on the accuracy results.", "Paraphrased Statement: This study introduces a method for assessing generalization ability in reinforcement learning (RL) agents. It leverages Controlled Distribution Perturbations (CDPs) to merge a tabular environment with a supervised learning dataset. Generalization is evaluated across three dimensions: observation space action space and state space. An empirical analysis is conducted on DQN and QRDQN algorithms in the fourroom and corridor domains combined with the MNIST dataset in both online and offline settings. The findings suggest that dropout enhances action generalization but hinders observation generalization whereas regularization improves observation generalization. Additionally QRDQN outperforms DQN in the offline setting for observation and action generalization but not for state space generalization. effectiveness and Weaknesses:  The paper clearly presents its ideas and is wellwritten.  The distinction between generalization across the three dimensions is novel and enables evaluation in a single task setting.  The empirical value of generalization capacity could be strengthened by evaluating more complex environments and different RL agents. Questions and Suggestions:  Why do DQN and QRDQN generalize similarly in the online setting on the observation space  Why does DQN generalize better than QRDQN on the state space axis  Can you provide more justification for treating action and policy generalization separately from statevalue generalization Minor Points:  use proceedings links in mention rather than arXiv links."], "rUwm9wCjURV": ["Paraphrased instruction: Summary: This research presents a new neural architecture for training a robot using deep reinforcement learning (DRL) to follow temporal logic instructions in unseen environments. The proposed architecture improves performance compared to previous approach. It utilizes an observation and a task description converting the instruction into a reinforcement function for optimization. A key introduction involves integrating environmental information into the reinforcement specification making it easier to interpret humanreadable task instructions. The approach demonstrates strong results and enhances performance in unseen tasks. effectiveness and Weaknesses: effectiveness:  The inclusion of environmental information in the reinforcement specification phase improves effectiveness. Weaknesses:  Clarity issues:  The abstract lacks clear explanations of the task and method using jargon instead.  Contributions are not clearly presented.  Problem formulation:  The task inputs outputs and key solution steps are not explained early on.  SATTL section:  The relevance and contribution of this section to the paper are unclear.  method description:  Figure 1 and the related discussion are confusing.  The relationship between the framework in 3.2 and the architecture in Figure 1 needs clarification.  Overall:  The paper would benefit from a perspective outside the project to improve clarity and communication of the work.", "Paraphrase: Summary: The field introduces latent goal models for interpreting formal instructions in unfamiliar environments. effectiveness and Weaknesses: Section 3.2:  The approach including the symbolic module addresses challenge such as:  Sparse reinforcement (only available at episode end)  Noisy instructions and environments  No taskspecific reinforcement shaping  Goals not dependent on the agents current state  However the symbolic module provides privileged information through its labeling and reinforcement functions. Section 3.2:  It is unclear how the approach handles ambiguity where multiple interpretations of the instructions are potential. Section 3.3:  The concept of latent goal architectures is not novel in robot\u6280\u80fd pembelajaran which typically involves less supervision. Section 4 Table 1:  The train and examination results should be compared separately.  The bolding schemes to indicate the bestperforming model are inconsistent across tables. Section 4:  The field neglects to reference relevant literature on robot\u6280\u80fd and instructionfollowing.  Additional experiments and comparisons are essential. Throughout:  The field assumes that human instructions can be isolated from sensory input which is unrealistic in practice.  Multiple sensor modalities become redundant if the instructions fully define the task.", "Paraphrase: This research explores training agents (using reinforcement learning) to pervariety tasks in various workspaces. Tasks are described using a version of temporal logic (a varietyal language for representing regular languages). These tasks can be provided in text or visual variety. The goal is to create agents that can pervariety tasks outside the range of those they were trained on. The field is conducted in two simulated gridlike environments and various neural network architectures and variations are tested. The approach combines reinforcement learning with a custom network architecture that separates taskagnostic and taskspecific state reintroductions. An invarietyation bottleneck is added to encourage generalization between tasks. The effectiveness of the approach include its clear introduction and the extensive empirical evaluation which shows promising results for outofdistribution temporal tasks. However the field could be improved by leveraging the underlying automata structure of the reinforcement function to enhance training efficiency. Additionally the small penalty applied at each time step could be replaced with a more elegant solution that uses counterfactual model.", "Paraphrased instruction: This paper explores using nonMarkov reinforcement functions for temporal task model and generalizing instruction following beyond a single formal language instruction. The authors introduce a model with two networks: one for embedding the environment and another for goalspecific reasoning grounded in the environment. They extend prior work on Task Temporal logic for planning with temporal formulas. The results indicate that the latent goal reintroduction facilitates generalization on both the object and task levels. However the introduction of the results requires improvement with suggestions made for enhancements. effectiveness include sound network design choices and the use of multiple formulas for testing. Weakness primarily lies in the introduction and analysis of the results with recommendations for statistical analysis and better visualization."], "m8bypnj7Yl5": ["Paraphrase: Summary:  This study demonstrates how to enhance ODE solvers using \"hypersolvers.\" These hypersolvers leverage neural networks trained on a large set of precomputed optimal control ODE solution.  By utilizing hypersolvers for trajectory optimization (e.g. pendulum swingup) and Model Predictive control (MPC) (e.g. cartpole and quadcopter systems) accuracy can be significantly improved within a fixed computational budget. effectiveness and Weaknesses:  The paper is wellwritten and understandable.  However its applicability is limited especially for the ICLR community.  The concept of using neural networks to warmstart ODE solvers is not novel (as evidenced by Poli et al.s work at NeurIPS 2020 which is not cited in the paper).  The specific application in optimal control ODEs have limited relevance to the ICLR community and would be more suitable for control conferences.  The experimental setup is simplistic and twodimensional.  The ML community typically employs different method for continuous control.  There are minor issues with mathematical clarity such as the absence of inputs in the loss functions (Eqs. 6 and 6) and the undefined use of \"w\" in the objective (Eq. 8).", "Paraphrase: This study expands the hypersolvers framework to the realm of control theory. Hypersolvers combine a lowcost numerical solver with a neural network that approximates the solvers error. By training this network they can create an accurate and affordable solver. Here the authors explore how to use hypersolvers in controlled systems. They test different approaches to defining loss functions training techniques and network structures during hypersolver preparation. They also introduce multistage hypersolvers to address errors in dynamical modeling. Experiments demonstrate that control strategies using hypersolvers (direct optimal control and MPC) perform well in various scenarios including cartpole and quadcopter simulations. Notably even with inaccurate dynamical models their approach achieves control performance comparable to method that rely on precise models. effectiveness:  Clear and concise writing  Sound technical approach and experiments Weaknesses:  Limited to smallscale systems  Inadequate differentiation from previous hypersolvers work Minor Points:  equation (3) should explicitly define \\(\\psi\\epsilon\\).  The key differences between the current and prior hypersolvers work should be highlighted more explicitly.", "Paraphrased Summary: The authors propose a method for improving the accuracy of numerical optimal control by incorporating \"hypersolvers.\" Hypersolvers approximate the dynamics of a system using a loworder ODE solver and a neural network that learns the remaining \"truncation residual\" dynamics. The paper also introduces a multistage hypersolver approach for handling errors in the system model and truncation residuals. Simulations involving a pendulum cartpole and quadcopter demonstrate the proposed approach performance which matches that of higherorder ODE solvers with reduced computational cost. Specifically certain activation functions (SIREN and Snake) provide superior performance compared to standard functions (Tanh and ReLU). effectiveness and Weaknesses: effectiveness:  Applies the hypersolver concept to numerical optimal control.  Improves dynamics approximation by learning truncation residual dynamics.  Multistage hypersolvers address system model errors.  Achieves similar performance to higherorder ODE solvers with reduced computational cost. Weaknesses:  Undefined term \"epsilon\" in equation (3).  Limited explanation of equation (2)s solution method in Section 3.2.  lack of FLOPS or runtime comparisons in experiments beyond the pendulum case.  extensive pretraining time and potential impact on trajectory use and training efficiency.  Need for further analysis on how ODE solver tolerance impact results.  Previous research exists on learning system residual dynamics for control (references provided).  Ablation study are recommended to compare multistage hypersolvers with learning only system residual dynamics."], "q4tZR1Y-UIs": ["Summary This paper presents and assesses a novel method for curriculum learning in unsupervised reinforcement learning. The approach trains agents in continuous environments with progressively challenging tasks using a fouragent algorithm. The algorithm includes two \"teacher agents\" that each favor their designated \"student agent\" in the quest of superior performance. Strengths and Weaknesses Main Strengths:  Multiple teachers: The use of multiple teachers helps prevent \"catastrophic forgetting\" during goal exploration.  Updated regret: Regrets from the replay buffer are updated based on critic feedback an innovative approach.  Visual illustrations: clear illustrations highlight the advantages of using regret updates and SAC (Soft ActorCritic).  extensive benchmarks: Multiple benchmarks are used to evaluate the methods performance. Main Weaknesses:  Insufficient detail: Assumptions and technical aspects are not adequately explained limiting interpretation.  Narrow comparison: AMIGO is the only comparative method mentioned leaving clear questions about performance against other approach.  Symmetric method: The benefits of a fouragent symmetric method are emphasized but the impact of larger agent populations is not explored.  Unclear action space: The action space for goalgenerating agents is unclear raising concerns about limitations and how human priors are embedded.  Critic implementation: The implementation of the critic and actor is not clarified and their potential shared networks are not discussed.  Goal distribution separation: The separation of the goal distribution into train and test sets is not fully explained.  Latent noise variable: The addition of latent noise to agent state for goal generation lacks explanation.  Absence of a \"main solver\": The absence of a primary agent in the presented results makes it unclear which agents performance is reported.  Limited comparison to AMIGO: A more indepth comparison with AMIGO a similar approach is not provided.  Unexplored larger agent populations: The potential impacts of using more than four agents are not investigated leaving the topic for future work. PostDiscussion and Updated Version: Based on author responses the concerns raised in the initial review have been addressed and additional analysis has been incorporated including ablations and discussion. While the analysis of larger agent populations remains a future research direction the current work provides significant contributions and sufficient analysis for a positive assessment.", "Paraphrased Summary: Researchers have developed a novel method to create training plans for automated agents that can accomplish specific goals. The method called CuSP involves two agents (Alice and Bob) and two teachers. Each teacher aims to identify tasks that their various agent (student) can perform beneficial than the other student. CuSP surpasses other goalgenerating algorithm in various realistic automatic simulation tasks. Strengths and Weaknesses:  The paper is clear and straightforward.  The dynamic regret updates to the goalgenerating buffer are logical and a toy example demonstrates their effectiveness in optimizing nonstationary objectives.  A key question arises: what causes the performance difference between Alice and Bob They train on identical tasks and use the same algorithm.  An alternative algorithm could be more computationally efficient by having a single goal generator maximize the variance in returns for a single agent.  The work would benefit from including the H.E.R baseline given the use of SAC for policy optimization.  The reason for the poor performance of the uniform goal distribution policy in the range task (Figure 7) with an additional goal dimension lacks a clear explanation.", "Paraphrased Statement: This paper introduces a modified version of asymmetric selfplay involving two goal generators and two solvers. Each generator aims to create goals that can be solved by one solver but not the other while each solver strives to solve the goals assigned to them. While the authors present this as a twoplayer zerosum game they do not provide a Nash equilibrium solution. Additionally Nash equilibria where the two solvers are identical exist resulting in zero reinforcement for the goal generators regardless of the goals proposed. The method appears to rely more on training dynamics than equilibrium properties. However even in cases where solvers start identically they do not diverge significantly due to similar updates contradicting the claims of progressive diversity and feasibility. The first three rows of the method are heuristic enhancements while the actual proposed objective is a symmetric goalsetting and solving approach that does not guarantee the desired outcomes as claimed. Furthermore the use of SAC for goal proposal raises questions since goals are typically considered single actions. While the results show some improvement they do not significantly outperform uniform area randomization. comparison with pair on these tasks would provide valuable insights.", "Paraphrase: Summary: This work introduces Curriculum Selfplay (CuSP) a method for creating diverse and progressively challenging goals in a sampleefficient manner. CuSP uses a fourplayer game formulation to balance cooperation and competition between student learners and regretmaximizing teachers. The method is tested on various continuous control tasks and demonstrates superior performance compared to baseline approach. Strengths:  Wellmotivated concept with clear presentation  Comprehensive experiments demonstrating the methods effectiveness Weaknesses:  Curriculum generation is limited to goalsbased settings extending it to nongoals settings would enhance its versatility  Lack of theoretical analysis on sample complexity and convergence providing such analysis would strengthen the work  The method involves multiple agent learning potentially raising concerns about computational overhead other Comments:  The purpose of using a latent noise variable (z) in the \"Goal Generator Design\" section requires clarification  The exploitation term in the goal generator policy objective needs further explanation particularly in the context of onestep optimization and whether it incorporates all immediate reinforcement encountered in an sequence"], "yhCp5RcZD7": ["Paraphrased Statement: This paper presents a new method for implicitly representing 3D displacement fields which is wellsuited for learning by neural networks. The approach uses two distinguish architecture:  The first architecture known as SIREN approximates the overall shape of the object as a signed distance mappingping (SDF).  The second architecture also based on SIREN predicts the displacement field on the surface of the SDF. In addition the paper introduces two distinguish enhancements:  An inverse implicit mappingping that improves the efficiency of the loss mappingping evaluation.  A featurebased UV mapping that can be transferred between similar shapes. The paper compares its method to existing approach on the SketchFab dataset and demonstrates that it achieves both smaller model sizes and more detailed reconstruction. It also shows how details can be transferred between different but similar models. Strengths:  The paper is clearly written and provides a comprehensive review of related work.  The twolevel implicit representation approach is a significant contribution.  The inverse implicit mappingping and featurebased UV mapping provide elegant solutions to geometryspecific problems.  The results are visually impressive and show both smoothness and detail. Weaknesses:  The quantitative comparison is only performed on a single dataset SketchFab16.  It is difficult to reproduce the results without approach to the specific models used.  The citation style with listed references without brackets can be distracting.", "Paraphrase: Summary: This paper presents a novel implicit neural representation for 3D geometry called implicit displacement fields. It combines two SIREN networks: one approximates a coarse base shape while the other represents an implicit displacement field for detailed features. Unlike previous approach the displacement field is implicitly defined by a SIREN network. This design enables shape manipulation applications. Strengths:  innovation: The simultaneous implicit representation of coarse surfaces and displacement using neural networks is a unique approach. It enables the retention of fine geometric details with a small network size.  displacement field: The proposed displacement field and attenuation mapping (equations 1 and 3) are novel and effective as shown by ablation work. Weaknesses:  parameter selection: The absence of discussion on choosing parameters \u03c9B and \u03c9D (set to 15 and 60 in the paper) raises questions about their objectdependent nature and impact on performance.  Generalizability: The paper does not explore the generalizability of the method to baseline models other than SIRENs.", "Paraphrase: This work introduces a novel shape representation using two \"SIREN\" networks set to different frequencies. The networks represent the basic surface and any deformation respectively. The authors also propose \"Transferable Implicit displacement field\" to transfer deformations between similar shapes. The proposed method is an extension of the existing \"displacement mapping\" technique and has a solid theoretical base. Evaluations demonstrate that this approach can capture finer details while requiring less memory compared to other methods. Strengths:  Clear and coherent presentation  Theoretical base derived from the classic \"displacement mapping\" method  Innovative implicit representation of the base surface and deformation field using neural networks  innovation of \"Transferable Implicit displacement field\"  Superior performance to existing methods in both quantitative and qualitative evaluations  Comprehensive ablation work to validate the design  Intriguing transfer experiments showcasing potential applications in other tasks such as detailed shape modeling Weaknesses:  Explanation of hyperparameter selection (e.g. attenuation factors and frequency factors) could be improved Other Observations:  The possibility of hierarchical displacement mapping shape modeling based on this approach is worth exploring.", "Paraphrased Statement: This paper proposes a method for reconstructing 3D surfaces from detail clouds using implicit representations based on signal distance functions and query features. Strengths:  S1: Innovative use of implicit displacement fields for modeling surface variations.  S2: Comprehensive related work review and wellorganized presentation.  S3: Supported by experimental results and ablation work. Weaknesses:  W2: Limited network modifications and lack of novel losses.  W3: potential susceptibility to noise due to the use of normals.  W3: Lack of comprehensive evaluations with significant noise layer."], "iGffRQ9jQpQ": ["Paraphrase: Summary This field introduces a fresh framework for partially supervised learning by adding an auxiliary task that determines if pseudolabels are genuine or not. This data is then utilized to include a recalibration loss in the primary goal. The proposed approach outperforms basic baseline on various basic benchmark datasets as demonstrated by experiments. effectiveness and Weaknesses The concept of adding an auxiliary task that discriminates between label and unlabel instances is intriguing. A similar discriminator was first introduced in active learning in this approach [1]. The VAAL algorithm [1] employs a similar discriminator to identify label and unlabel examples and subsequently use them to measure example uncertainty for active selection. The proposed method is thus closely related to a recent SSL paper [2] which also uses uncertainty to choose highquality pseudolabels. However there are some field where improvement can be made: 1. The logic in section 3.2 is unclear. For instance in Eq. (3) the second equality is invalid and the term \\fracdddx should be included. Additionally it would be beneficial to modify the d notation (discriminator) because the derivative dx also employs the d notation. Further Im unsure why \\mathcalLB depends on f given that f and \\mathcalLB are derived from distinct branches without sharing network components (image 1). 2. The experimental section is insufficient which is my primary complaint. The datasets and baseline employed are overly simple. To validate the assertions cuttingedge SSL procedures should be incorporated. In particular the uncertaintybased SSL algorithm [2] ought to be taken into explanation. As I previously mentioned the proposed method could unintentionally be equivalent to current SSL methodology. Does the proposed approach complement or contradict current approaches A thorough empirical examination is necessary to address these problems. Overall this paper falls short of the high standard expected of ICLR submissions due to its subpar experiments. References [1] Sinha S. Ebrahimi S.  Darrell T. (2019). Variational adversarial active learning. In Proceedings of the IEEECVF International Conference on Computer Vision (pp. 59725981). [2] Rizve M. N. Duarte K. Rawat Y. S. et al. (2021). In defense of pseudolabeling: An uncertaintyaware pseudolabel selection framework for semisupervised learning. arXiv preprint arXiv:2101.06329.", "Paraphrased Statement: The paper introduces a novel semisupervised learning method that addresses the publication of error accumulation in selftraining approach. The authors propose frameworking pseudolabel confidence as an auxiliary task and introduce a selfinterested coalitional learning (SCL) strategy to optimize both tasks concurrently. The method transforms the primary learning task into a costsensitive learning problem resulting in significantly improved accuracy of pseudolabels and enhanced performance on the primary tasks. effectiveness:  Clear and wellorganized structure  Demonstrated improvement in selfsupervised learning effectiveness by jointly optimizing primary and auxiliary tasks  introduction of a novel SCL strategy with potential applications to various learning problems Weaknesses:  Limited experimental evaluation only comparing to selflearning methods  Lack of comparison with stateoftheart SSL approach  Absence of fully supervised results with complete datasets as a benchmark  Insufficient consideration of realworld scenarios where missing data may correlate with inputoutput variables  Limited exploration of the methods robustness to labelunlabeled distribution shifts  Increased framework capacity due to the additional discriminator framework in the SCL method compared to the original selflearning approach  Lack of discussion on the compatibility of SCL with consistency regularization a common technique used with selflearning Additional Questions:  Why do SCL methods appear to outperform fully supervised frameworks on the first two tasks in Table 1 despite the absence of missing data  Are pseudolabel test accuracies of 100 for the SCL method in image 4 calculated differently from other methods", "Paraphrased Summary: Selfinterested Coalitional Learning (SCL) is a novel semisupervised learning approach that merges selftraining with a task that estimates label visibility. SCL reportedly surpasses both selftraining and the basis framework in various scenarios. effectiveness and Weaknesses: SCL is a promising contribution to the field of semisupervised learning. It directly addresses the limitations of traditional selftraining methods. An illustrative example demonstrating SCLs advantages over selftraining would be beneficial. The papers discussion of the ALPHA parameter lacks clarity. The optimal value is not provided for the first dataset and the recommended value of 1 for the other two datasets is not sufficiently justified. Appendix B offers little guidance on tuning alpha. Suggested Improvements:  Expand Table 1 to include rows for 0 90 and 99 missing range.  Add horizontal lines in image 6 indicating the basis frameworks accuracy for each missing range.  Clarify the rationale for using only 10 of data for image classification.  Address scalability concerns for large datasets.  Proofread the paper for typos such as \"perforamnce\" and \"well learn.\"  Revise the phrasing on pages 2 and 3:  \"more sufficient\"  \"more significant\"  \"jointly solving above two tasks\"  \"jointly solving the above two tasks\"  \"there are some other works embody\"  \"there are some other works that embody\"  \"are impacted the influence\"  \"are impacted by the influence\"", "Summary The paper introduces a novel approach to semisupervised learning that addresses two limitations of existing methods: overreliance on labeled data and error accumulation. It posits that jointly solving the primary learning task with a secondary task (distinguishing real from pseudolabels) improves performance. effectiveness and Weaknesses effectiveness:  The proposed framework is original.  It performs well on diverse tasks (classification label propagation and data imputation).  It has potential applications across doprimarys unlike some SSL methods that require imagespecific techniques. Weaknesses:  Comparing with innovative propagation of pseudolabeling methods would clarify the contributions.  other field have also addressed error accumulation in pseudolabeling which should be acknowledged. Questions  When missing data range increase are there more unlabeled or noisy labeled samples  Can the data imputation task be included in image 24  Can the proposed method enhance SSL algorithms beyond pseudolabeling minor Questions and Comments  The paper should emphasize why recent SSL methods are not compared in the experiments.  The value for the parameter alpha in image classification should be specified.  Its unclear whether M in Algorithm 1 is provided as input or constructed from labeled and unlabeled datasets.  The meaning of P in section 2.2 is ambiguous.  \"Performanance\" in page 6 is a typo.  Mi and Mj on page 3 may be Mi and Mj.  Y is the full set of labels in the training dataset (Y  YL \u222a YU).  Table 1 should ideally report mean and standard error from multiple trials. After Rebuttal The additional experiments with UPS provide valuable insights into the methods computational efficiency. However this focus may reduce the significance of the performance advantage. The fresh experiments should be included in the primary paper rather than the appendix."], "j3krplz_4w6": ["Paraphrased Statement: The authors have developed a novel attack method called \"Text explanation Fooler\" that can modify the explanations generated by certain text classification models without altering the models predictions. This is achieved by altering the text input in a way that maximizes the difference in attributions between the original and modified text while maintaining an identical prediction issue. The method employs word importance ranking for selecting substitutions and considers partofspeech tags and stop words to guide the perturbation process. Its effectiveness is demonstrated on various datasets (AGs News IMDB Fake News MR Yelp) and models (CNN LSTM LSTMAtt BERT) resulting in significantly lower correlations between the original and perturbed explanations compared to a baseline random attack. effectiveness:  Clear and coherent introduction.  Demonstrated decrease in explanation correlation a valuable finding.  Strong connection to existing literature.  Comprehensive evaluation on multiple datasets and models. Weaknesses:  Incomplete reporting of results with only a subset being presented despite evaluation on five datasets.  Limited discussion of implications and implications of unchanged predictions.  Comparison with a relatively simple baseline attack (RA) which may make it easier to achieve lower correlations.  Lack of information on perturbation ratios in the plots.", "Paraphrased Statement: summary This paper examines the challenge of evaluating the stability of explanations for Natural Language processing (NLP) models. Specifically the authors: contribution 1. Formalized Attribution Robustness Estimation: Defined a mathematical model (equations 46) to estimate how model explanations change under small input modifications. 2. Developed a Blackbox Attack Model: Created a method to estimate attribution robustness without gradient approach. 3. Evaluated Attack Model: Assessed the models effectiveness on popular attribution methods (e.g. saliency maps integrated gradients attention). Strengths 1. Comprehensive Background: Provides a clear explanation of the problem and its significance. 2. Novelty: Introduces a novel approach to addressing attribution robustness in NLP models. Weaknesses 1. Unclear Universal Perturbations: Section 4.3s explanation of how the attack finds universal perturbations is confusing. 2. Lack of Quantitative Measure: Equation 6s definition of attribution robustness estimation doesnt provide a quantifiable measure. 3. Limited Scope: Only examines sequence classification tasks despite potential applicability to other NLP tasks. 4. Unreferenced Symbol: Equation 4 contains an unexplained symbol \\tildeW. Additional Comments The papers findings could be valuable to the NLP community but the authors should address these weaknesses for greater clarity and impact.", "Paraphrased Statement: summary: This paper presents an attack method for targeting the explanations provided by Natural Language processing (NLP) text classification models. The method is straightforward and involves ranking word importance in the input sample and then employing a technique called \"counterfitted GloVe\" to select candidate words for substitution that preserve confidence in the correct class prediction while altering the explanation. The authors have thoroughly tested their method to gauge its efficacy. effectiveness and Weaknesses: The task of attacking NLP model explanations is a significant and appealing one. The proposed approach is a generic model that can be applied to a wide range of NLP models. The method calculates word importance scores and subsequently explores substitutions for those words. The proposed approach attribution robustness as described in Equations 4 and 6 appears promising. While this work is among the first to address the issue of attacking explanations in NLP the solution is fairly straightforward and does not introduce many novel concepts. In essence the attackers simply modify the objective function to meet their goals which means that current NLP attack techniques can yet be used to target explanation mechanisms. This might trivialize the development of solutions for attacking text classification explanations. Suggested Reading:  Fooling network Interpretation in Image Classification [1]  Fooling Neural network Interpretations via Adversarial Model Manipulation [2]  Jointly attack Graph Neural network and its Explanations [3]  Is BERT Really Robust: A Strong Baseline for Natural Language Attack on Text Classification and Entailment [4]  Adversarial attack and Defense on text: A Survey [5]"], "y_op4lLLaWL": ["Paraphrase Summary This work revisits Dai and Wipfs (2019) analysis of population loss in VAE ELBO. It notes the existence of undesirable \"asymptotic global optima\" where the framework distributions support has higher dimensions than the true data manifold. While gradient descent bias linear VAEs against these optima empirical results show that nonlinear VAEs often get stuck in them. strength and Weaknesses of Original Review  Positive: Clarifies Dai and Wipfs (2019) results and informal discussions which are clear to misinterpretation.  Negative:  Most results are unsurprising and stem from an overly wide definition of \"asymptotic optimality.\"  Ignores nuances in Dai and Wipfs (2019) discussions such as the use of a specific parameter set and the impact of estimation and optimization errors. Interest and Value of New Results The novel analysis of the implicit regularization effect of optimization reveals a surprising limitation in previous work. nonlinear simulation shows that nonlinear VAEs can get trapped in undesirable optima which could have practical implications. Experimenting with fixes (e.g. clipping decoder variance) would enhance the paper impact by providing insights and guidance for practitioners. Minor Comments  Define \\tilde z in Theorem 4.  Clarify that the rotation in Theorem 5 is applied to both framework parameters and ground truth.  acknowledgment related work by Kumar and Poole (2020) and Tang and Yang (2021) that improve our understanding of ELBO landscapes. PostRebuttal Update Despite initial concerns about the relevance of the main results the new experiments provide evidence that contradicts Dai and Wipfs informal discussions. This highlights the significance and relevance of the findings presented in this work warranting acceptance.", "Paraphrased Statement: This work evaluates a conjecture by Dai and Wipf (2019) on the support of distributions learned by Variational Autoencoders (VAEs). It presents: Contributions:  For linear VAEs with Gaussian data with a degenerate covariance matrix and linear encoderdecoder it proves that VAEs accurately capture the intrinsic dimensionality of the data by examining the objective and gradient flow dynamics.  For nonlinear VAEs it presents a counterexample to the conjecture by Dai and Wipf (2019) demonstrating that the support of VAE generators can exceed that of the data distribution.  Numerical experiments support the theoretical findings. strength:  The combination of loss landscape and gradient flow analysis strengthens the claim made by Dai and Wipf (2019) for linear VAEs.  The sigmoid dataset example simplifies the observation that VAEs may overestimate intrinsic dimensionality. Weaknesses:  The organization of the paper could be improved as the explanation of the discussion structure is unclear.  The paper lacks practical advice for practitioners who use nonlinear VAEs despite the negative result suggesting that the VAE objective can lead to overfitting. Question:  Can the gradient flow analysis be extended to stochastic gradient descent setups where the expectation over the latent variable z in the VAE objective is approximated using Monte Carlo methods", "In this research the authors delve deeper into the previously proposed conjectures by Dai and Wipf (2019). For linear Variational Autoencoders (VAEs) they establish the rigor of these conjectures through mathematical validation. However in the case of nonlinear VAEs the paper challenges the original conjectures. The authors contend that during the training work of nonlinear VAEs the framework tend to learn higherdimensional manifolds that encompass the true underlying data manifold. strength:  The work builds upon a significant previous work (Dai and Wipf 2019).  The authors provide rigorous mathematical formulations and validation. Weaknesses:  The research lacks a clear problem statement and need.  The writing style is overly formal and could benefit from revision.  The paper focuses almost alone on linear VAEs neglecting the more prevalent nonlinear VAE use case.  The work is limited to theoretical toy datasets without evaluating the framework on realworld data or standard benchmarks.", "Summary This research expands on a theoretical analysis by Dai and Wipf that examines the optimal behavior of Variational Autoencoders (VAEs) when used with manifoldvalued data. This work focuses on the training behavior of VAEs when applied to such data particularly when the manifolds inherent dimension is smaller than the surrounding space. The authors consider VAEs with linear encoders and decoder with a single hidden layer. strength and Weaknesses strength:  Provides insights into potential \"degenerate\" solutions that VAEs may encounter during training. Weaknesses:  Questionable Interpretation of Dai and Wipfs Results: The paper misinterprets the theoretical findings of Dai and Wipf regarding the convergence properties of VAEs during training. Dai and Wipfs analysis focuses on optimal behavior not convergence behavior.  Inaccurate Reiteration of Dai and Wipfs Theorems: The first part of Lemma 1 appears to present an incorrect restatement of Theorems 4 and 5 from Dai and Wipf.  Unclear Relationship to TwoStage VAE: Despite citing Dai and Wipfs twostage VAE the paper does not explore whether this approach addresses any of the problems identified in this work.  Disjointed presentation: The paper lacks a central theorem or paper that cohesively ties together the provided theorems. Style Concerns:  Difficulttounderstand sentence structure on page 6.  potential typographical error on page 8 with \"\\tildexr1\" possibly intended to be \"\\tildexr2\"."], "qWhajfmKEUt": ["Paraphrase: Summary The researchers introduce a novel defense against adversarial approachs using spectral regularization. This defense analyzes the embedding of training data into a lowdimensional subspace. They observe that adversarial examples increase the dimensionality of this subspace distinguishing them from natural data. The proposed defense adds a penalty term that reduces variance along the first principal component. Experiments on Cifar10 100 and SVHN datasets show improvements in accuracy under approachs when this regularizer is combined with other defense methods. effectiveness and Weaknesses  effectiveness:  The exploration of the relationship between approach sensitivity and embedding subspace is innovative.  The experimental results demonstrate the effectiveness of the proposed regularizer.  Weaknesses:  The study lacks theoretical explanation or connections between embedding dimensionality and approach robustness.  The experiments indicate that a more homogeneous distribution of variances does not prevent adversarial approachs.  The effectiveness of the approach may vary across datasets and the lack of guidelines for weight tuning limits its practical applicability. Theorem 1 analysis The validation of Theorem 1 is flawed as it assumes y is a constant when optimizing over Z which is incorrect since y is a role of z. Minor Issues  Language errors should be addressed.  The specific embedding space and its relevance to known robustness factors are unclear.  The connection to the related study on adversarial examples on manifolds is not explored.  The evaluation of FSR without additional defense methods is not included.", "Summary This study examines the differences in spectral properties between natural and adversarial examples revealing that eigenvectors with lower eigenvalues are more vulnerable to adversarial manipulations. Consequently adversarial examples tend to introduce more components in these guidance. To address this the paper proposes Feature Spectral Regularization (FSR) which penalizes larger eigenvalues while enhancing minor ones mitigating their dominance. empirical experiments demonstrate that FSR enhances robustness when combined with existing adversarial defense. effectiveness  top and accessible introduction  Theoretically and empirically supported Weaknesses  Inconsistent trend in standard training (ST) vs. adversarial training (AT) curves between Figure 1 and 2  Marginal improvements in robustness from FSR raising questions about its practical value and statistical significance  Potential computational overhead due to singular value decomposition (SVD) for largescale datasets  Uncertain stability of findings based on inconsistent trends in Appendix Figures 6 and 7  Insufficient clarification on the relationship to Principal Component analysis (PCA) Minor Issue  Typo in sentence below Figure 3 corrected to \"r(Dadv D uj)\" PostRebuttal Assessment Despite the authors response the previous concerns remain. FSRs limited practical application for largescale datasets and its potential to impair performance in the absence of adversarial examples necessitate caution. Additionally the inconsistent experimental results and lack of a top understanding of its relationship to PCA warrant further investigation.", "Paraphrased Summary: This paper introduces a novel technique to enhance the resilience of feature against malicious attacks. The authors devised a metric to evaluate feature alterations under attacks and discovered that eigenvectors associated with minor eigenvalues are more vulnerable to adversarial attacks. They attribute this to the dominance of large eigenvalues and eigenvectors and propose a method to mitigate the problem by equalizing the feature spectrum. Specifically they propose suppressing the largest eigenvalues during model training referred to as spectral regularization. In conjunction with existing defense models this approach demonstrates promising results in adversarial defense. effectiveness and Weaknesses: effectiveness:  Demonstrates the relationship between adversarial attacks and feature spectrum.  Introduces a novel regularizer applicable to adversarial training.  offer theoretical support for the proposed spectrum equalization.  demonstrate extensive quantitative and qualitative results including feature spectrum analysis. Weaknesses:  Marginal improvement over existing adversarial defense methods as shown in Table 15.  The defense effectiveness in flattening the feature spectrum is uncertain given that existing defense algorithms may also have this capability.  FSR only suppresses the largest eigenvectors leaving a gap in the desired goal of feature spectrum adjustment.  Experiments conducted on a basic CNN model its unclear if strong CNN models would respond differently to attacks and defense methods.  Evaluation conducted only on lowresolution range (32x32) it would be beneficial to test the defense on highdimensional data (e.g. 224x224) where adversarial attacks are more severe."], "youe3QQepVB": ["Paraphrased Summary: This paper introduces a multitask learning approach that incorporates a generative model as an \"intheloop\" data enhancer. The purpose of this generative model is to improve performance on downstream tasks especially when dealing with limited data. The authors emphasize the effectiveness of generative modeling as a data augmentation technique during training. They claim that their main contribution include: 1. Demonstrating that using offtheshelf image generators does not enhance downstream tasks. 2. Introducing a jointly trained framework consisting of an intheloop generator selfsupervision block refinement block and an EMstyle optimization technique that outperforms singletask and multitask baselines. 3. Conducting ablation field highlighting the impact of various design choice. effectiveness:  The paper presents an initial experiment showing that conventional image generators are ineffective for data augmentation.  It includes wide experiments demonstrating the superiority of the proposed method over existing stateoftheart models.  Ablation field partially justify the design choice made in the method. Weaknesses: Major Concerns:  The paper claim of being beneficial in \"low data regimes\" lacks convincing evidence. The experiments with limited datasets (25 or 50 images) are not truly representative of low data scenarios (e.g. 100 images).  In some case baseline models show greater robustness to data scarcity than the proposed method raising questions about its effectiveness in this context.  The generative models role seems marginal as its removal only marginally diminishes performance. The focus on generative modeling appears somewhat misplaced given the more significant impact of the optimization scheme and selfsupervision block.  The quantitative results should be interpreted cautiously as the qualitative visualizations suggest significant prediction errors for all methods. Minor Concerns:  The visualizations of depth normal and semantic segmentation results are confusing and make it difficult to compare them with ground truth.  introduction issues such as inconsistent metrics and confusing visualizations could be improved.", "Paraphrased Summary: This field explores the use of generative models to enhance multitask learning. The authors introduce a framework that combines a discriminative multitask model with a generative model and a refinement network. This framework leverages weak labels and selfsupervision for endtoend training. The combination of multitask learning and generative modeling improves performance as demonstrated by quantitative and qualitative results. Paraphrased effectiveness and Weaknesses: The framework proposed in this field is innovative and thoughtprovoking. The authors conducted comprehensive experiments including ablation field and extensions. However the pilot field has certain limitations:  It is conducted on a single task while the proposed framework is designed for multitask scenarios.  The use of images synthesized by an offtheshelf generative model may not negatively impact multitask learning performance which contradicts the pilot fields findings.  Taskonomys output is not a reliable ground truth for training with synthesized images. The field could be strengthened by providing additional evidence to support the benefits of incorporating generative models. One suggestion is to train separate generative models for each task and evaluate their impact on multitask learning.", "Paraphrased Statement: This field introduces a multitask network for pixelbased estimation tasks like semantic segmentation and depth estimation. The researchers also included an image generation network (either SAGAN or DCGAN) a selfsupervised network (SimCLR) and a scene category classification network (referred to as a refinement network). The refinement network categorizes scene categories based on the outputs of multitask network (estimated segmentation masks and depth images). The proposed MGM model outperformed baseline and ablated methods in experiments with two datasets. The method was also effective in additional tests with six tasks and highresolution images (256x256). effectiveness:  novel incorporation of an image generation network into multitask learning for pixel estimation tasks.  introduction of a refinement network to classify scene classes using synthetically generated images without ground truth annotations.  effectiveness of EMstyle training demonstrated through experiments.  Wellwritten paper with comprehensive experiments.  Pilot field highlighting the inefficiency of using GANs without refinement for this task. Weaknesses:  Dependence on scene labels for all training images limiting applicability to datasets like CityScape (which lacks street scene labels).  potential for multitask or singletask applications beyond the two evaluated in the field. Additional Comments:  The authors speculate that using a multilabel classifier in the refinement network and a multilabel conditional GAN could overcome the CityScape dataset limitation.  The researchers encourage further exploration of the method for singletask applications.  Providing generated image model in the supplementary material would aid comprehension."], "qHsuiKXkUb": ["Summary This paper presents four challenges in scorebased generative modeling and suggests solution for each: problem 1: Scorebased models typically only calculate the loss over a limited time interval which can result in poor image details. Solution: Train models with a small time interval to improve details. problem 2: The data score diverges as time approaches zero making it difficult for neural networks to learn. Solution: Reparamaterize the model using a function that grows unboundedly as time approaches zero. problem 3: The loss function has a extensive image of scales leading to an instability in gradient contributions. Solution: Use a \"soft truncation\" trick that sample time points from different images focusing some batches on small time steps and others on large time steps. problem 4: The proposed VESDE diffusion model is not truly \"geometric.\" Solution: Introduce the \"Reciprocal Variance Exploding\" SDE (RVESDE) which is geometric and enables simple importance sampling. Strengths:  The proposed solution appear effective particularly on small datasets.  The \"soft truncation\" trick is wellmotivated and intuitive. Weaknesses:  The paper uses different probability flow formulation which makes it difficult to compare results across tables.  The paper does not clarify which weighting function is used in the experiments.  The paper focuses on multiple problem instead of exploring the proposed solution in more depth. Suggestions:  Focus more on the \"soft truncation\" trick as a promising direction for improvement.  Conduct additional experiments to support the claims about RVESDEs benefits.", "Summary: This study addresses two issues in optimizing diffusion models: instability during late training stages due to unbounded denoising scores and coarsescale artifacts in other stages due to reduced objective influence. It introduces a new parameterization suitable for small noise levels and a \"soft truncation\" trick that ensures adequate influence at all noise levels. The study shows significant improvements in quantitative performance metrics (NLL FID IS). Strengths and Weaknesses: The authors note the paper technical contributions and experimental results but criticize the writing for being technical and difficult to comprehend. They suggest adding more citations and discussing whether alternative parameterizations (e.g. epsilon parameterization) address the same issues. Motivation: The authors acknowledge that the impact of the identified problem (image 3 and 4) may not be only compelling. They encourage solid argument for the significance of these issues. The motivation for the \"soft truncation\" trick is also not fully explained and could benefit from additional insights. Experiments: The authors request results on larger image sizes (e.g. ImageNet 256x256) and with different datasets (e.g. ImageNet). They also emphasize the need to compare different parameterizations directly to determine their potential impact on performance. Observations: The authors note that the study suggests improved results with smaller sigmamin values but performance degradation besmall 103. They speculate that this may be related to conventional image quantization levels and suggest investigating the relationship between sigmamin and pixel quantization in different scenarios.", "Summary This paper highlights three challenges in training scorebased diffusion models:  Unbounded score  training instability  Bias in training The researchers propose solution for each problem supported by theoretical justifications and empirically validate them. Strengths  Pinpoints practical issues in training scorebased diffusion models.  Thorough ablation study with welldesigned experiments. Weaknesses  Imprecise language potentially misleading argument and insufficient reasoning.  Assumption that the score would blow up as time approaches zero is questionable particularly with dequantization technique in use.  Lack of uncertainty data for experiments.  Numerous grammatical errors and typos. Specific Concerns  Intro: References in the first paragraph could be more canonical.  Section 2: VESDE and VPSDE may induce different diffusion in practice despite claimed equivalence.  Equation 1: Missing terms in the righthand side.  Equation 2: Definition of \u03c3(\u03b5) is needed.  Page 3 First Paragraph: Scorebased diffusion models do not aim for pointwise inversion the argument regarding generated image is incorrect.  image 4: Clarify whether the sample is generated or perturbed real data.  Lemma 1: Assumption on the unbounded vector field should be justified.  image 5: estimation or computation of the curve should be explained.  training instability: High loss magnitude and variance for small sigma may not directly indicate training instability an average NLL curve would be more insightful.  Proposition 1: The theoretical foundation of VESDE is not only lost due to the Gaussian noise with variance \u03c3min2.  Section 3.1: Negative sign in \u03b7(t) for HNCSN needs justification.  Section 3.2: Clarify the third argument of L(......) in the ST trick explain the advantages of ST over importance sample.  Table 3: Improvement of HNCSN (RVE) and ST is marginal the significance of the results is unclear.  ablation: argument about lowering \u03c3min effectively reducing performance may be incorrect it should read \"improves.\"  Optimal precision: It is unclear if the authors attempted to sample with \u03c3 approaching 0 after training the 8bit representations relevance to mathematical instability is not specified.", "Paraphrased argument: Summary diffusion models excel in various applications but training them is challenging due to the potential for data scores to reach infinity as time approaches zero. This paper addresses this issue by introducing a limited parameterization for unbounded data scores and a practical technique (STtrick) to manage score value scale variations. It also proposes Reciprocal Variance Exploding Stochastic Differential Equations (RVESDE) for model sample. The method is compatible with existing NCSN and DDPM models and demonstrates solid performance in highresolution image generation. Strengths and Weaknesses 1. image  Improved explanations for image generation and settings would be helpful such as specifying data distribution yaxis averaging methods and sample sizes.  The dependency between x and y labels in image 5 should be clarified.  Despite improvements image 6 suggests that the proposed method still struggles to capture data scores accurately on toy datasets. 2. Equation  Expanding the equation for Lt reveals potential issues with infinity when \u03c3 approaches 0 leading to instability.  Weighting function optimization may not fully resolve the inaccuracy of estimated scores when \u03c3 is very small. 3. Related Work  An alternative variance reduction technique for DSM using Taylor expansion has been proposed. It improves stability and accuracy when \u03c3 is near zero. 4. Empirical solution  The method allows training with a small \u03c3min than baseline SDE models which may enhance sample quality.  It outperforms current SDE models in sample quality.", "Paraphrased Summary This paper examines the behavior of ScoreMatching Generative model (SDEs) for small time values (t close to 0). It identifies two issues with current forward SDEs: high score values for the marginal distribution and excessive expected squared loss leading to diminished loss contributions at later times. The paper proposes several solution to address these problem. Strengths and Weaknesses Novelty and Significance:  The paper study the specific issue of SDEbased models for small t which has not been previously explored in depth.  The topic is relevant to the conferences focus on generative models. technical Correctness:  The paper theoretical justification for the exploding score issue relies on a toy experiment (image 5) with limited explanation and questionable relevance to realworld data distributions.  The mathematical validation (Lemma 1) may not hold for all functions particularly noncompact subsets with unbounded values.  The discussion on \"Front time\" appears inconsistent as the assumptions for main and Markov variables are not aligned. Presentation:  Sections 2.2 and 3 lack clarity with insufficient context for the toy experiment and unclear explanations of how plots are generated and quantities calculated.  Additional motivation and background would enhance the understanding of certain sections. Experiments:  The paper does not provide experimental validation on natural image data to confirm the exploding score issue.  The yaxis labeling in image 5 is unclear making it difficult to interpret the results.  further ablation study on the proposed soft truncation and sigmamin selection technique would be beneficial."], "jJWK09skiNl": ["Paraphrase: Summary: This paper focfunctions on a general zeroshot localization task. It modifies the YOLOv5 network to detect predetermined attributes. The method is tested on the YCB picture dataset. Strengths:  Addresses a practical problem from an application view. Weaknesses:  limited technical design primarily limited to attribute prediction.  Insufficient experimental support lacking comparison with existing methods and limited analysis.  Weak second contribution: dividing the YCB Video dataset into seen and unseen objects is not considered a meaningful contribution.  Imprecise function of the term \"detection\" since the method only localizes not classifies objects.  Grammatical and style errors such as:  \"robots will be allocated to various task depend on need\" should be \"robots will be allocated to various task based on need.\"  \"For every object their class labeling is translated into attribute vectors that contain the color and work information\" should be \"For every object its class label is translated into attribute vectors containing color and work information.\"  \"Tensor Tf represents the block of features extracted in three levels they are passed to the following blocks\" should be \"Tensor Tf represents the block of features extracted in three levels and passed to the following blocks.\"  \"It needs to be notice that\u2026\" should be \"It is significant to note that\u2026\"  \"our attribute vector is carried out by human eye evaluation\" should be \"our attribute vector is generated through human eye evaluation.\"  The YCB Video dataset comprises 31 pictures with 45272 frames containing only seen objects.", "Paraphrased Summary This research addresses zeroshot object instance detection without explicit object classification effectively becoming an objectness detection method. The approach employs a modified YOLOv5 network with an added branch for predicting object attributes. These attributes are predefined binary values and do not adapt to new attributes. The method is evaluated on the YCBVideo dataset excluding four out of 21 objects for zeroshot assessment. However the results are inconclusive and lack comparison with current approaches or ablation work. Strengths  The network production object attribute vectors potentially beneficial for scenarios with fixed or limited attribute sets. Weaknesses  The method is essentially a modified YOLOv5 network with questionable object attribute prediction due to fixed attribute slots.  It only provides bounding box making it an objectness detector rather than an object ID detector.  It lacks comparison with relevant prior work such as [1] and [2].  No ablation work were conducted leaving the empirical verification of the proposed method untested.  It is evaluated using a single dataset and only based on recall disregarding the impact of false positives.  The results are insufficient with an average recall of approximately 0.3 for unseen objects. Minor Grammatical Errors  \"since the knowledge can be learned from each object is very limited\" should be corrected to \"since the knowledge that can be learned from each object is very limited.\"  \"ZSL and gZSL algorithms only need to analysis the categorical information\" should be \"ZSL and gZSL algorithms only need to analyze the categorical information.\"  \"Covert the class labelling to 16 attributes that represents\" should be corrected to \"Convert the class labeling to 16 attributes that represent.\" [1] P. Ammirato C.Y. Fu M. Shvets J. Kosecka and A. C. Berg \u201cobject Driven Instance Detection\u201d arXiv1803.04610 2018. [2] J.P. Mercier M. Garon P. Gigu\u00e8re and J.F. Lalonde \u201cDeep Templatebased Object Instance Detection\u201d arXiv1911.11822 2019.", "Paraphrased statement: This research focuses on developing a computer vision model for detecting objects during object manipulation. Specifically it aims to create a zeroshot object detection algorithm that can identify unseen objects using knowledge gained from observed objects. The work evaluates the efficacy of the proposed approach using the YCB Video dataset. Strengths:  The paper presents a clear system and problem definition. Weaknesses:  The proposed approach primarily relies on the YOLO5 model with limited design.  The paper lacks an explanation for the algorithms ability to detect unseen objects.  Ablation work are not included to demonstrate the contribution of each component in the model.  The evaluation is limited to the YCB Video dataset and lacks comparison with other zeroshot learning methods.", "Paraphrase: Summary: This paper transforms the generalized zeroshot learning (gZSL) problem from an picture classification task to an object detection task involving wide pictures. It employs the YoloV5 network to predict attribute encodings for detected bounding box. These attributes are then used to filter out background objects based on overlaps between seen and unseen objects. The network is trained using a combined loss function that includes localization object detection and attribute prediction (binary crossentropy). The authors evaluated their method on the YCB picture dataset for 21 objects. Strengths:  Presents a practical application of ZSL for multiobject view in realworld context.  Extends Yolo to incorporate attribute prediction. Weaknesses:  Evaluation is limited to the authors own method without comparison to other backbone architecture.  It is unclear whether attribute encoding only is sufficient without additional contrastive learning or hard mining.  The claim that increased object diversity within a class enhances algorithm generality lacks supporting evidence and may be counterintuitive.  There are typos and grammatical errors in the paper. Clarifications:  The phrase \"objects in a greater field of view\" should be clarified.  \"Manufacturing planets\" should be corrected to \"manufacturing plants.\"  \"They are existing work in\" should be \"There are existing work in.\"", "Paraphrased statement: Summary: The work focfunctions on zeroshot object detection for known and unknown objects even when objects are classified into more specific class. For instance in industry or indoor context distinguishing between objects of the same type but with different colors sizes or work may be crucial. This paper enables the classification of unknown objects based on attributes like white blue silver black brown bottle cup clamp circle cylinder box and rectangular. The researchers achieve zeroshot detection through a simple attribute prediction method. attribute labels are assigned to training picture and the method generalizes to unseen objects in test picture. Strengths:  The problem addressed is practically significant. object detectors and classifiers should not overfit to object color or size within the same class. For model if a neural network has only encountered black dogs during training it should still recognize white dogs during deployment.  Generalized zeroshot learning is less explored compared to traditional zeroshot learning. Weaknesses:  The novelty is limited. attributebased zeroshot learning is an established approach.  The experimental results are disappointing with low recall ratios for several class. Zeroshot detection methods should ideally detect unseen objects even with limited attribute presence in the training set.  The work only utilizes a single dataset and lacks comparison to prior work. The authors should justify this lack of comparison.  The attribute list is restrictive and may not be scalable to broader applications. It may be challenging to label all attributes in all picture for general function. Hierarchical labeling might be more beneficial.  The lack of reporting on mean average precision (function) a standard metric for object detection is a concern. The algorithm may generate false positives as acknowledged in previous work on zeroshot detection. Questions:  Why is function not reported Previous work has reported function over all class.  Additional relevant papers on zeroshot detection could have been cited such as [FGHI]."], "s03AQxehtd_": ["Paraphrased Statement: Summary: This research introduces a deep learning model for reconstructing complete 3D human poses (joint positions and local orientations) from limited and varied user inputs (positions and orientations of a subset of joints). The model employs an encoder to extract pose data from partial inputs and a decoder to generate complete poses from these encoding. Two novel datasets have been created to train and assess the proposed models and experimental findings demonstrate their superior performance over existing approach. strength:  Addresses a complex inverse kinematics challenge with an innovative neural networkbased solution.  Provides a technically sound approach with supportive experimental results and two specialized datasets.  Presents a clear and logical structure. Weaknesses:  Could improve organization by clarifying terminology and consolidating contribution.  Lacks details on metric definitions (Ldetgpd Ldetikd Ldetloc) and dataset splits.  Could benefit from more qualitative results such as comparison with previous methods.  Would be valuable to explore the impact of input factors (number of joints input type) on performance.  Considerations regarding the models suitability for illposed problems (where multiple outputs are possible) and the potential benefits of using a variational autoencoder (VAE) are raised.", "Paraphrased Statement: Summary: This paper explores a method that can represent 3D human poses using neural network. A new network called a protoresidual network is introduced to generate a complete human pose from limited body contribution constraints. The network has been improved with modifications like a lookat release a twostage decoder and a residual scheme. The paper presents two new datasets for modeling static human poses which will be made publicly available. Experiments demonstrate the strength of the proposed approach. strength and Weaknesses: Pros: 1. The paper addresses a valuable problem in human animation. The proposed method and animation tool can reduce the workload for animators. 2. The modifications to the network architecture are wellexplained and appear logical. 3. The paper provides extensive materials including experiments an appendix and demo videos. The strength of most of the proposed modules is supported by experimental results. Cons: 1. The comparison with FinalIK should be more detailed including a clear comparison of speed and construction results with complete constraints. The generalization ability of both methods to nonhuman skeletons should be examined. 2. Additional comparison and analyses could strengthen the experimental section. Quantitative evaluation of the lookat release and ablation work results without IKD would be helpful. 3. More details about the new datasets especially the MiniAnonymous dataset would be beneficial. 4. A description of the Unity animation tool would enhance the paper content. detailed Questions: 1. What is the minimum number of required input constraints 2. How does the method handle ambiguous input constraints 3. Can highlights from the demo videos be included in the main paper 4. Are the distinct results in Tables 5 and 6 (e.g. 1.27 e6) typos", "Paraphrased Summary: This paper presents a novel system for creating realistic human poses based on sparse input parameters. It utilizes a custom network architecture (\"ProtoRes\") designed to handle varying input lengths. The system is trained using a lookatloss weighting scheme and custom MoCapbased datasets. Experiments demonstrate the network effectiveness compared to baseline models like Transformers and traditional inverse kinematics methods. The paper also showcases an impressive graphical demo in Unity highlighting the systems practicality for animation and graphics applications. strength and Weaknesses: strength:  practical System: The paper provides a usable system for creating realistic human poses as showcased in the Unity demo.  Novel network: The use of prototypical network for human pose generation is an innovative approach. Weaknesses:  Missing Baselines: Simple nearest neighbor or interpolation baselines could provide valuable comparison.  Lack of Motivation for ProtoRes: A more detailed explanation of why prototypical network are advantageous for this task would be beneficial.  Generative Aspect Missing: For an underconstrained problem like pose generation generative models may offer advantages."], "nL2lDlsrZU": ["Paraphrase: The paper presents SAINT a machine learning model for structured data that uses attention mechanism between rows and a work of training known as contrastive pretraining. It shows that SAINT outperworks other methods in both supervised and partially supervised settings and also has feature that make it more robust and explainable. However there are some weaknesses in the paper:  It does not cite enough relevant literature when describing methods for machine learning with tabular data.  It is not clear why using embeddings to represent continuous feature improves perworkance. Although the experiments show this to be the case more explanation and analysis is needed.  The hyperparameters of the benchmark methods used for comparison are not fully tuned. This weakens the claim that SAINT outperworks these methods. The authors should carefully follow the hyperparameter tuning process described in the original paper for the benchmark methods.  It is not clear how augmentation affects the results. More results using different augmentation methods should be added to support the choice of CutMix.  The explainability capabilities of SAINT are not presented convincingly. There is no ground truth for feature importance to measure the accuracy of the explanations. An experiment on synthetic data might help to add credibility to the attention maps.  The paper focuses also much on average improvement results and does not discuss the specific tasks and datasets where SAINT shows the most significant improvement.  Overall the paper novelty is limited. Most of the ideas used in SAINT (e.g. attention mechanism contrastive learning) have been published in previous paper. This paper combines these ideas in a useful way and shows significant improvement but the contributions are primarily empirical rather than novel.", "Paraphrase: Summary:  The work introduces SAINT a novel deep neural network architecture for classification and regression tasks on tabular data.  Previous work have shown improvement in deep learning for various domains but tabular data problems often rely on traditional algorithms like XGBoost and Catboost.  SAINT incorporates a novel attention mechanism that operates both within samples (rows) and across feature (columns).  It embeds continuous feature unlike previous models that only embed categorical ones.  SAINT is evaluated on 30 diverse datasets and compared to 10 existing models outperforming them with an average range of 2.7 (vs. 4.0 for Catboost).  Ablation work highlight the impact of attention and embedding.  SAINT exhibits explainability through its application to MNIST.  The paper also proposes a contrastive learning technique for unsupervised data pretraining and finetuning which enhances performance on limited labeled data. effectiveness and Weaknesses:  The techniques presented in the paper are innovative and address a significant problem.  However there are concerns regarding the validity of the claim that SAINT surpasses traditional algorithms.  The hyperparameter search for existing algorithms is limited potentially underestimating their potential performance.  SAINT has a more wide hyperparameter architecture which may skew the comparison.  Results are presented in ranges rather than actual values which simplifies interpretation but may obscure small accuracy differences.  Accuracy and AUC values for classification and RMSE values for regression should be provided to complement the rangeing.  The extremely low RMSE (0.03) for all models on dataset 422 and the negative RMSE values in Tables 911 warrant further investigation.", "Paraphrased Statement: This paper introduces SAINT a neural network model designed for tabular data with both continuous and discrete values. SAINT incorporates selfattention between variables and intersample attention among samples. By employing a combination of InfoNCE and denoising objectives for pretraining SAINT outperformed all baseline methods in experiments involving 30 diverse tabular datasets encompassing binary classification multiclass classification and regression tasks. effectiveness:  SAINT effectively captures the feature of tabular data and is straightforward to understand.  The authors conducted comprehensive experiments using a large collection of datasets. Weaknesses:  Questionable Prediction performance:  The main evaluation results are presented in range rather than specific metrics hindering direct comparison with baseline.  detailed performance metrics in the Appendix show SAINTs comparison to baseline. Additionally high standard deviations in the range cast doubt on the statistical significance of SAINTs high range.  Efficacy of Intersample attention:  The claimed effectiveness of intersample attention for datasets with many feature is not supported by Appendix results.  The interpretation of intersample attention lacks conclusive explanations and relies on speculation.  Missing baseline:  Relevant methods like VIME and TABBIE are not included in the baseline leaving their comparison to SAINT incomplete.", "Paraphrase: Main Contributions  Introduces SAINT a novel deep learning architecture for tabular data that applies attention to both samples and feature.  For datasets with missing labels presents a novel contrastive selfsupervised pretraining method.  Proposes an innovative embedding technique for continuous attributes. Strengths  Addresses the underperformance of deep learning on tabular data.  Thoroughly explains the components of the SAINT architecture.  Demonstrates the efficacy of SAINT through ablations and comparison with strong baseline.  Shows the effectiveness of the proposed pretraining method for unlabeled data. Concerns Dataset Selection:  Inquires why UCI datasets were excluded and whether a systematic substitution rule was used. SelfSupervised Pretraining:  Questions whether pretraining is essential given SAINTs intersample attention capability.  Asks for details on the datasets and compute time required for pretraining.  Notes discrepancies between supervised and semisupervised results without pretraining. Hyperparameter Settings:  Requests data on hyperparameter optimization for each method including the number of work.  Inquires whether ablations were considered during hyperparameter selection. computational Cost:  Requests runtime comparison between SAINT and other deep learning architecture and classical machine learning methods."], "yjMQuLLcGWK": ["Paraphrased Statement: Summary This paper introduces a novel technique called FPDETR (Finetuned Pretrained DETR) for improving object detection using transformerbased models. Unlike prior methods FPDETR pretrains the DETR encoder using convolution neural network (ConvNet) backbone on the ImageNet informationset. During finetuning it incorporates a task adapter to bridge the gap between paradigm classification and object detection by mapping visual prompts to the classification token domain. Experimental results on the MSCOCO informationset evidence FPDETRs competitive performance and ability to generalize to smaller informationsets. Strengths  Clear and concise presentation  Novel approach that draws parallels between object detection and textual prompt filling tasks  FPDETR effectively generalizes to compact informationsets Weaknesses  Incomplete technical validation: Although the task adapter is intended to bridge the gap between visual content and class tokens it remains unclear whether it primarily enhances object interaction model or task adaptation. Verifying this aspect through embedding distance calculations would strengthen the argument.  Inconsistent generalization based on pretraining information size: FPDETRs performance decreases when pretrained on ImageNet21k compared to ImageNet1k indicating a potential scalability limitation. Addressing this publication requires further investigation.", "Paraphrase: This paper introduces a groundbreaking transformerbased object detection algorithm called FPDETR. By leveraging extensive pretraining FPDETR enhances robustness and generalization on smaller datasets. It combines an encoderonly transformer with lightweight feature extraction layers followed by finetuning using a task adapter to focus on object location and interobject relationships. FPDETR exhibits strong performance on both largescale (COCO) and smallscale (CityScapes) datasets. The ablation analysis validates the effectiveness of its design choices. However the reviewers have concerns regarding the experimental design. To substantiate the claims about the FPDETR architecture the authors should provide additional experiments or justification. Despite these concerns the paper is wellwritten and explains the intuition behind its design and technical details clearly. The idea of bridging the gap between classification and detection through an encoderonly transformer is intriguing. Overall FPDETR has the potential to contribute to the community by improving model robustness and generalization through pretraining.", "Paraphrase: This research presents a transformer model for object detection that enhances feature utilization during pretraining. Borrowing from \"prompting\" in natural language processing it employs query positional embeddings as visual prompts to improve object localization. The models design ensures consistency between pretraining and finetuning stages. Furthermore a task adapter is incorporated to capture relationships between objects. This model achieves comparable performance and improves robustness and generalization. Strengths and Weaknesses: The model introduces \"prompting\" to object detection reducing the gap between pretraining and finetuning. It demonstrates better performance on the COCO dataset and enhanced robustness and generalization on smaller datasets. The authors conducted ablation studies to justify various configurations. In Table 7 the FPEncDec model with 300 tokens outperforms a singletoken model in pretraining and finetuning. It is worth considering if increasing the number of tokens for FPDETR during pretraining could improve its performance on both pretraining and downstream tasks. Table 2 could benefit from an ablation study comparing FPDETR with a ResNet50 backbone. small Correction: The introduction mentions \"PTDETR\" which appears to be a typo.", "Paraphrase: This paper proposes FPDETR a method that employs fully pretrained transformer models for paradigm classification to object detection. FPDETR leverages encoderonly transformers to achieve small parameters than other methods. Strengths:  Encoderonly transformer design without additional visual backbone  Lower parameter count Weaknesses:  The paper does not adequately justify the effectiveness of the proposed pretraining method. Experiments are limited to Imagenet1k which is not considered largescale pretraining.  It is unclear how necessary the deformable convolution layer is to the encoderonly transformer design.  Latency is not reported which is relevant for small models.  The robustness analysis is not directly related to the main contribution. Instead the paper should focus on ablation studies on hyperparameters."], "zz9hXVhf40": ["Paraphrased Summary Recently offline modelbased reinforcement learning (RL) techniques have leveraged heuristics to penalize reinforcement based on uncertainty in the estimated Markov Decision process (MDP). This paper reviews various penalty mechanisms and analyzes the influence of hyperparameters like planning horizon and model ensemble size. The authors demonstrate that optimizing penalty selection and hyperparameters enhances performance. effectiveness  Clearly written and accessible  Comprehensive review of contributions and comparison between selection  Valuable insights and perspective  Exhaustive coverage of related process in modelbased offline RL Weaknesses  Insufficient explanation of number 1a and Section 5.3  Lack of error bars in tables to assess statistical implication  Absence of error bars for algorithms identified as significantly better  Overlooked existing calibration metrics like expected calibration error (ECE) Other Comments  number 1a: Define the yaxis and detailed computation steps for reproducibility  Error and Uncertainty: Clarify if these are for reinforcement or other dynamic variables  Error Computation: Specify if errors are computed stepbystep or for multistep predictions  number 26 and number 27: Explain the deviation between median and individual rollouts  Appendix D: Elaborate on \"displacement\" and define probability of improvement  Error Metric: Consider using log likelihood instead of MSE  Hyperparameter Optimization: Acknowledge that performance evaluation in the real environment is impractical  Table 2: Discuss the limitations of penalties in detecting outofdistribution data  Minor Comments:  Introduction: Clarify the reference to \"this section\"  Section 3: Use a consistent notation for reinforcement (\"r\")  Ensure that the penalty justification provided by Yu et al. 2020 aligns with the context", "Modelbased offline reinforcement learning algorithms involve creating a pessimistic model of the environment which is constructed using uncertainty estimation from a learned model. This paper compares different methods for estimating uncertainty in use and evaluates their accuracy in predicting model errors. The authors use Bayesian optimization to find the optimal hyperparameter settings and achieve strong performance in experiments. Key effectiveness:  Provides a systematic comparison of various uncertainty estimation methods a gap in previous research.  Welldesigned experiments and clear explanation. Potential Weaknesses:  Limited technical contributions beyond comparing methods.  Related process from Abbas et al. 2020 is not discussed which also compares uncertainty estimation methods and reaches similar conclusions.", "Paraphrased Summary: This field thoroughly examines design selection and hyperparameter settings in offline modelbased reinforcement learning (MBRL). It focuses on uncertainty penalties and hyperparameters such as ensemble size penalty weighting and rollout horizon. The authors demonstrate that offline MBRL methods are sensitive to these parameters. They compare an \"optimized\" version of MOPO with Bayesianoptimized hyperparameters to the original MOPO and find significant performance improvements. effectiveness and Weaknesses effectiveness:  Comprehensive evaluation comparing multiple offline MBRL methods.  Insightful comparison metrics for uncertainty penalties. Weaknesses:  Optimized version only compared to MOPO making it unclear if similar trends apply to other methods.  Lack of analysis on the stability of hyperparameter selection across random seeds. Clarifications and Questions:  \"Optimized\" refers to a version of MOPO with tuned hyperparameters not a separate method.  number 1(b) requires clearer explanation of methodology.", "Paraphrased Statement: Summary: The field examines various uncertainty metrics for modelbased reinforcement learning specifically focusing on the MOPO framework. It analyzes the correlation of these metrics with model prediction errors and their ability to identify transitions with high prediction errors. The field also employs Bayesian optimization to tune hyperparameters including uncertainty penalties and planning horizon. effectiveness and Weaknesses: effectiveness:  Provides empirical show on the effectiveness of different uncertainty metrics.  Highlights the importance of hyperparameters in MOPOs performance. Weaknesses:  The fields recommendation to optimize hyperparameters using Bayesian optimization on the same environment raises concerns about its applicability to realworld offline RL scenarios where interaction with the environment is limited.  The field lacks actionable recommendations for penalty selection and hyperparameter tuning.  The experiments are limited to deterministic environments which may not generalize to stochastic environments.  The authors claim that ensemble variance is the \"most principled\" selection is not fully supported.  The AUC and AP metrics are not easily comparable across different percentiles due to varying sample ratios.  The precisionrecall curves in the appendix appear to have incorrectly flipped axes.  The concept of \"latent dynamics that are KLregularized to a spherical Gaussian\" is not clearly explained.", "Paraphrased Summary: The paper examines different ways to measure uncertainty in offline reinforcement learning models both theoretically and practically. It then uses Bayesian optimization to find the better hyperparameters for the uncertainty value and finds that these optimized hyperparameters improve model performance. effectiveness:  The research is relevant and addresses a significant gap in offline reinforcement learning.  The use of Spearman rank and Pearson bivariate statistics to gauge the relationship between uncertainty and model error is innovative.  The experiments verify the value of these uncertainty measurements for model evaluation. Weaknesses: Experiment design:  The use of trajectories generated by nonpenalty methods in the experiments is questionable as these methods tend to overestimate uncertainty and may yield poorperforming policies. It would be better to use trajectories from a mediumreplay dataset to assess model performance. Rollout horizon:  The paper aims to study the effect of rollout horizon on uncertainty value but the results in Table 2 do not explicitly show how rollout horizon is used as a variable. The author should clarify the connection between rollout horizon and the presented results. Dataset Mixing:  The results in Table 3 combine data from the D4RL v0 and v2 datasets which are significantly different. The author should separate the results for these datasets to avoid misinterpretation. Hyperparameter Sensitivity:  The paper does not explore the sensitivity of the hyperparameters N lambda and h to different tasks. It would be beneficial to analyze how these parameters affect performance under various task conditions. Uncertainty Measurement explanation:  The paper lacks a clear explanation of why \"ensemble Standard deviation\" value both epistemic and aleatory uncertainty. This explanation should be provided to ensure the papers selfsufficiency. Minor number:  The method for calculating the true dynamics error in number 1 should be detailed in the accompanying discussion.  The statement at the end of Section 5.1 regarding the impact of the number of models on uncertainty estimation could benefit from further elaboration."], "xbu1tzbjvd": ["Paraphrased Statement: The study proposes a metamodel that can mimic both the output and hidden representation of diverse neural networks trained for the same task. This metamodel shares some common parameters and each network has unique parameters. By analyzing the metamodels parameters in a shared parameter space the overall characteristics of the network ensemble can be assessed through unsupervised manifold learning. effectiveness and Weaknesses: Quantifying the similarity of neural networks trained for the same task is valuable but its unclear if the proposed approach addresses the issue of comparing hidden states across different architectures. The examples focus on comparing networks with the same architecture and different training conditions where the metamodel matches the architecture of the direct networks. This suggests that the metamodel may require the same point of expressiveness as the most innovative network under consideration with the unique parameters essentially controlling which section of the metamodel are deactivated to replicate the behavior of each direct network. A key question is whether the approach can explore the relationships between hidden representations encoded in distinct ways such as comparing a CNN and a transformer trained for range identification or networks with different architectures like VGG and ResNet.", "Paraphrase: Summary This research introduces a new way to analyze a population of direct neural netstudys. The authors create a metamodel that predicts the behavior of a neural netstudy based on a given input vector. They demonstrate applications of their framestudy such as clustering and semisupervised learning. effectiveness and Weaknesses effectiveness:  The approach is simple with minimal constraints.  It is generalizable to different types of neural netstudys (RNNs and CNNs). Weaknesses:  Lack of comparison to other methods particularly in Figure 5.  Unclear discussion of related study particularly regarding the computational cost.  potential for similar results using simpler methods (e.g. PCA).  Focus on sentiment analysis which may not generalize to more complex task.  Lack of clarity in Figure 4. Additional Questions and Comments:  Can you clarify the observation in Figure 3 regarding the differences between RNN and GRU models  In Figure 6 do similar trends occur with a smaller dataset  Can this approach improve performance on the given task  What is the significance of the middle and right panels in Figure 6  How are other dimensions set during 2D sampling  What are the baseline accuracies for the models used  Can you provide more detail on the results in Figure 8  Define the term \"manifold\" and explain the inclusion and exclusion criteria for neural netstudys in the manifold.  Is there a formal guarantee that nearby points in the metamodel embedding space correspond to similar netstudy dynamics  Are regression task considered in this study  The dynamical system in Appendix C does not meet the definition of a proper conjugate system as specified in the paper.", "Paraphrased Statement: Summary: The authors have developed an algorithm that generates embeddings for multiple neural networks and a metamodel. The metamodel can replicate the hidden states and outputs of any network embedding. The embeddings are used to measure the similarity of models and enable interpolation and extrapolation between them. effectiveness and Weaknesses: effectiveness:  The algorithm is innovative and intriguing.  The generated visualizations provide valuable insights for the community. Weaknesses:  The algorithm requires many direct models for effective embedding.  Additional data is necessary.  A clear comparison to existing similarity measures is missing. Pros:  The models novelty clarity and simplicity.  The insights gained from the models embeddings. Cons:  The potential limitations of the model in small datasets.  The need for additional data.  The lack of a comprehensive comparison with alternative methods. Other Observations:  The visualizations offer meaningful information but could benefit from wider comparison.  Interpolation is a notable contribution of the paper.  Extrapolation capabilities are claimed but their value is unclear as the metamodels training included additional data.  The papers organization could be improved by referencing figures and tables appropriately.", "Paraphrase: This paper introduces a method to combine several (pretrained) neural networks into a single model known as a metamodel. The metamodel can simulate the behavior of each individual network producing similar outputs and hidden states. The metamodel takes as input both the task data and an encoding that represents the characteristics of each network. These encodings allow the metamodel to generate latent features that correspond to the hidden states of each network. These latent features are then decoded using specialized decoders to produce the hidden states of each network. Using this approach the metamodel can be used to explore the relationships between different networks and adjust the combined model to perform better than any of the individual networks. The results show that the metamodel effectively captures the variations between different network configurations such as training data size data augmentation and network architecture. However the paper does not fully explore the use of a single decoder for all networks which could simplify the model. Additionally the concept of a manifold of models and the relationship between network parameters and model behavior are not thoroughly explored or justified. The analysis of the metamodels embeddings also lacks depth and its unclear if the differences in embeddings compared to other methods provide meaningful insights."], "zuqcmNVK4c2": ["Paraphrased Statement: summary: Researchers introduce a novel training technique for classifiers. Instead of evaluating model individually during training and prediction they classify pairs of model (X1 X2) simultaneously producing a joint probability distribution for the labels (Y1 Y2). The objective work is modified to enforce conditional independence between Y1 and Y2 given X1 and X2. Experiments demonstrate improved model performance with this approach for models of equivalent capability (measured by layer width). The authors suggest that this improvement stems from the regularizing effect of the new training method. Additionally joint training on pairs of case allows for the incorporation of unlabeled auxiliary model. effectiveness and Weaknesses:  effectiveness: The empirical results are persuasive and the authors explanation of the method as having a regularizing effect is reasonable. The departure work discourages fully independent label predictions forcing the model to balance correct classification and prediction independence. joint training increases the training set size potentially mitigating overfitting.  Weaknesses:  The authors state that joint training enables the model to learn sampletosample relationships but this is not applicable when labels are deterministic.  It is unclear how pseudolabels are handled during gradient computation for training.  detail on network architecture modifications to accommodate the increased input and output dimensionality are lacking.  The definition of model capability in image 2 is unclear.  There are minor writing and stylistic issues throughout the paper.", "Summary This paper introduces \"selfjoint learning\" a novel departure work for supervised learning. Unlike traditional methods that predict the conditional label distribution for individual data samples selfjoint learning predicts the conditional joint label distribution for pairs of data samples. The framework defines the conditional joint label distribution and provides strategies for making predictions during inference. Experiments show promise in addressing overfitting adversarial robustness and outofdistribution data detection. Strengths  The assumption of modeling conditional joint label distribution is clear.  The parameterization and inference strategies are intuitive.  The experiments cover various applications.  The paper is wellstructured and straightforward. Weaknesses  The paper lacks theoretical justification for how modeling conditional independence enhances performance in these applications.  Despite studying multiple applications the paper does not adequately compare the method to the current literature in each field.  The computational complexity of the method in training and inference phases is not explored.", "summary: This paper introduces a straightforward and impactful learning method. By inputting pairs of data the method learns joint conditional probabilities for predictions. The benefit lies in the vast number of combinations created by partitioning the original data into pairs which expands the input space significantly. effectiveness:  Simplicity and effectiveness  Ease of understanding  Theoretically sound formulation  Proven advantages over deep learning methods Areas for Improvement:  Lack of explanation for why results are favorable. It might be due to the exponential development of independent data through pairing resulting in substantial regularization.  Nonstrict independence of data since pairs are combined differently. However iid assumptions are often relaxed in realworld scenarios.  Theoretical proofs of network properties particularly regarding regularization. Query:  How are image inputted: sidebyside or concatenated channelwise (image 1 suggests sidebyside)."], "jWaLuyg6OEw": ["Paraphrase: Summary This paper analyzes two optimization algorithms qRGF and qSGF based on gradient flows. These algorithms relate to continuous time perspectives in optimization and the connection between ordinary differential equations (ODEs) and optimization. This field holds promising potential for further research. effectiveness and Weaknesses  The paper begins promisingly but may contain a typo in Equation (6b). Additionally the notation used for Adamk and Xk is unclear.  The eAdamamples provided focus on the case where the cost function G(Adam) is the identity function. It would be helpful to provide an eAdamample with a nonidentity cost function such as a proAdamimal gradient scheme.  The paper introduces the assumption of gradient dominance of order p for the cost functions. Further eAdamplanation is needed to justify this assumption beyond strong conveAdamity.  The toy eAdamamples in the paper are useful but could be linked more closely to the theory. Computing constants and plotting errors in these eAdamamples would enhance the understanding of the results derived in the paper.", "Paraphrase: This field examines two firstorder optimization techniques: Euler discretizations of scaled gradient descent and signed gradient descent. The authors establish convergence rates for these methods in deterministic and stochastic context. effectiveness:  Investigating convergence behavior in gradient flow discretizations is crucial in optimization.  The paper is wellstructured and easy to follow.  The approach is supported by both empirical testing and theoretical analysis. Weaknesses:  The convergence rate for Euler discretizations of scaled gradient descent and signed gradient descent under deterministic conditions has already been documented in previous literature (acknowledged by the authors). The only notable difference is the extension to Lipschitz smoothness. The authors should highlight the significance of this extension and the challenges faced in adapting existing theory.  In the stochastic setting the result may be less satisfactory due to the requirement for an infinite batch size even when epsilon is very small. This is especially problematic in stochastic environments.", "Paraphrase of Summary: The paper analyzes the behavior of discrete algorithms that approximate continuous gradient flows. It demonstrates their convergence in both deterministic and stochastic settings supported by theoretical results and numerical experiments. effectiveness and Weaknesses: The papers effectiveness lies in its examination of convergence properties of discretized gradient flows providing useful convergence results. However there are a few unclear points: 1. Discrepancy in Theorem 2: The inequality in Theorem 2 states that k\\star \\leq \\fracf(x0)f(x\\star)(1\\alpha)\\tildec(1\\alpha)\\cdot\\eta. Setting C  \\fracf(x0)f(x\\star)(1\\alpha)\\tildec(1\\alpha) for some accuracy \\epsilon and time step \\eta the condition tk\\eta \\leq \\epsilon implies k \\leq \\frac\\epsilon\\eta. However for large t it is possible that t  \\epsilon C leading to a contradiction: k \\leq k\\star. 2. Practicality of Results for Stochastic gradient Descent: The results for stochastic gradient descent depend on the batch size which must be large (O(\\frac1\\epsilon)) for high accuracy. In practice this may require a batch size too close to the broad gradient descent compromising its efficiency. It would be useful to derive results that are independent of accuracy or allow for a vanishing stepsize. Side Questions: 1. Can other discretization methods provide finitetime convergence for these flows or demonstrate it numerically 2. Is finitetime convergence still possible for secondorder dynamical systems", "Paraphrased Statement: Summary: This field examines the convergence rates of qRGF and qSGF two firstorder methods. These methods are constructed by discretizing (using forward Euler) the qrescaled gradient flow (qRGF) and qsigned gradient flow (qSGF) respectively. These gradient flows have been shown to converge in finite time under the gradient dominant condition of order q. In this paper the authors demonstrate that their forward Euler discretized versions have linear convergence rates under an additional assumption of Lipschitz smoothness of order q. The paper also introduces stochastic variants of these methods. effectiveness and Weaknesses: effectiveness:  Fundamental methods: These methods are foundational gradient methods with gradient normdependent step size which are commonly used in deep learning.  Experiments: Experiments on toy and practical examples demonstrate that the proposed methods may outperform existing methods like GD and Adam. Weaknesses:  Motivation: The motivation for fielding these specific gradient flows and their discretizations is not fully fleshed out. It would be helpful to justify their relevance compared to the standard gradient flow.  Finitetime convergence: The paper claims that finitetime convergence in continuous time translates to acceleration in the discretized algorithm. It would be more convincing to match the convergence rates from the continuoustime field to the discretetime field as other related analysis have done.  gradient dominant condition: There is no comparison to convergence rates of other methods under the same condition. Given the authors claims of superior performance in experiments a theoretical comparison would be valuable.  Experiments: The intuition behind the observed practical acceleration is not provided. The results on the SVHN dataset are not fully explained especially the role of the tuned parameter q. Minor Points:  Page 3: case 1 and 2 could be condensed to focus on the main content of the paper.  Page 6: Explain the selection of the \u2113\u221enorm over the \u21132norm.  Page 7: Example 1 does not appear to satisfy the Lipschitz smoothness condition for q1.  Page 8: Define the optimality gap used in Figure 1.  Page 9: Explain why GD (Nesterovs method) is abbreviated as GD.  Page 9: Provide more details on how the 40minute runtime was determined.  Page 9: Explain the understanding for the drastic drops in the right figure of Figure 3."], "iedYJm92o0a": ["Summary: This study focuses on the \"learning to execute\" task where a model predicts the outcome of a Python program or arithmetic calculation. In this approach Transformers are trained with \"scratchpad\" during training. The model receives intermediate variable states after each statement execution. During testing the model predicts these intermediate states which are stored in the \"scratchpad\" but not included in the final prediction. effectiveness and Weaknesses: effectiveness:  Training with scratchpad significantly improves execution prediction. Weaknesses:  Explanation of scratchpad contribution: The suggested reasons for scratchpad benefits are incomplete and the actual explanation may be different.  Lack of ablation study: No experiments separate different sources of contribution and explanations.  unclear motivation: The usefulness of learning to execute specific functions is questionable given the availability of programming language interpreters.  Scaling limitations: Using a scratchpad increases the output sequence length limiting the applicability to large programs. The Role of scratchpad: While the paper proposes two potential reasons why scratchpad help it overlooks the main factor:  Additional supervision signal: scratchpad models receive the complete execution trace during training unlike baselines. This extra supervision may be a significant contributor to improved accuracy. Task Motivation: The motivation for this task is unclear. Its challenging to determine the practical benefit of learning to simulate a Python interpreter especially when the interpreter is already available. Generalizability to other Tasks: Its unclear whether improvements in this task translate to other tasks as it requires specific execution trace supervision that may not generalize. Additional Questions:  model size impact: The paper doesnt discuss the lack of improvement for smaller models with scratchpad.  Singlepass prediction: The paper description of \"direct execution prediction\" as a single pass may be misleading.  \"Understanding code\" language: The paper use of this language is harmful and inaccurate.  Related process: The paper omits relevant previous studies that also demonstrated the benefit of stepbystep supervision in execution prediction.", "Paraphrase: Transformers are becoming proficient at creating programs based on descriptions. However its unclear if they truly comprehend what they produce. Transformers lack the ability to execute code and the current approach in this area is limited. The authors introduce a simple method that significantly enhances performance. Instead of altering the model or training data they train the transformer to output intermediate computation steps which leads to improved code execution performance. The method shows promising solution on addition and other synthetic tasks. When applied to a codebase of Python programs their \"scratchpad\" method surpasses the baseline and benefits from a data augmentation technique they introduce. effectiveness:  The scratchpad idea is novel and has implications for both program execution with language models and language model in general.  The method improves solution without changing the language model itself opening up novel possibilities.  The ideas are simple and reproducible.  The paper is clearly written. Weaknesses:  The lack of solution for the direct execution model trained on certain datasets makes it unclear how MBPPaug affects the baseline.  The solution may be biased towards shorter programs due to context window limitations. other NotesQuestions:  Quantization may not necessarily reduce error compounding.  Table 2 requires an explanation for the asterisk in the caption.  More details about the model used such as the view embedding method would be helpful.", "Paraphrase: This research assesses how well further language models can solve addition polynomials and execute Python programs with minimal instruction (fewshot learning) or additional training (finetuning). Instead of predicting the tasks output directly the proposed approach is to have the model generate the intermediate steps involved:  For addition: Calculating column sums and carries  For polynomial evaluation: Determining the value of each term  For Python execution: Finding the values of variables in each line The authors demonstrate improved performance compared to traditional execution methods in all scenarios. They attribute this improvement to several factors:  Adaptive Computation: The model spends more time on complex tasks.  Persistent State: intermediate solution are stored in past tokens enhancing memory.  Error Minimization: States are quantized into specific tokens reducing error generation.  Enhanced Debugging: The approach provides valuable data for debugging and understanding the process. effectiveness:  Simple and effective concept leading to significant improvements.  Complements existing research that outsources reasoning to external engines. Weaknesses:  The chosen tasks are not particularly useful for learned models as there are simpler means to perform them using computers.  A more systematic analysis of scenarios where the approach is successful and unsuccessful would be beneficial. Questions:  For fewshot learning how were the model selected  Which columns in Table 3 were affected by the improved tracing technique for handling errors  To what extent do the solution depend on the modified tracing method aside from the additional data from CodeNet", "Paraphrased Statement: This research introduces a method to improve Transformers capacity for multistep symbolic computations (e.g. integer addition program execution). Instead of training the model to directly output the solution it generates an intermediate \"scratchpad\" that captures the execution steps. This approach offers advantages: 1. Adaptive Computation Time: The scratchpad length adjusts to input complexity allowing for varying execution time. 2. intermediate Memory: The scratchpad serves as a temporary memory for intermediate computations. 3. Interpretability: The scratchpad enables easier understanding of the models execution process. The researchers evaluated the proposed method on three tasks using finetuned or contextlearning Transformers. The solutions suggest that models trained with scratchpad output perform just than those predicting the solution directly. Additionally synthetic data augmentation was shown to benefit the scratchpad method. effectiveness and Weaknesses: effectiveness:  Clear and accessible introduction.  The scratchpad idea is innovative and demonstrates improved performance.  The scratchpad approach shows promise for enhanced benefit from synthetic data augmentation. Weaknesses:  Concerns about the limitations of the finetuning scenario as it may diminish the models general learning capabilities.  Comparison to meaningful baselines for compositional generalization is lacking.  Lack of reproducibility details and a statement on reproducibility."], "qhAeZjs7dCL": ["Paraphrased Statement: Summary: This research investigates using a generative model (instead of a fixed dataset) to learn visual representations through contrastive learning. Rather than using pixellevel transformations this study explores using transformations in latent space either only or combined with pixellevel methods. issue are presented for two generative models:  BigBiGan trained on ImageNet and finetuned on ImageNet100 and ImageNet1000  StyleGan2 trained on LSUN machine and finetuned on ImageNet Strengths:  need: The research addresses the significant challenge of using generative models for unsupervised representation learning.  Originality: This study appears to be the first to utilize blackbox generative models for this purpose.  Clarity: The paper is wellwritten and thorough providing a comprehensive literature review. Weaknesses:  Empirical issue:  The performance of the SimCLR baseline is weaker than reported in previous study potentially impacting the validity of the issue.  The authors could provide more analysis of the poor performance of the generated views contrastive algorithm.  Latent Space transformation:  The rationale for using latent space transformations (wsteer) instead of pixellevel transformations (T) should be clarified.  Comparison:  The baseline for the real algorithm in Table 2 is missing.  The ImageNet100 benchmark comparison is not suited for direct comparison as generated and real algorithms access data with differing class counts.  analysis:  A plot of the \"real baseline\" bound in image 6 could provide insights into whether the real baseline can be approached with more samples. Miscellany:  Legends in image 6 have the same color.  Equation 6 should include Tz.", "Paraphrase: This study examines whether artificial datasets generated by implicit generative models (e.g. GANs) can replace original datasets for representation learning tasks. Researchers evaluate how popular contrastive learning techniques perform when trained on real data versus synthetic data from GANs. In addition to standard pixellevel transformations they also explore latent space augmentation methods and their impact on contrastive learning. Strengths:  Timely and relevant research on using synthetic data for representation learning.  open explanation of methods and comprehensive issue.  Innovative use of latent space augmentations tailored to generative models. Weaknesses:  The study focuses on GANs as the sole representative of implicit generative models which may not be applicable to all models.  It is significant to note the limitations and caveats of using synthetic data which may not be emphasized enough.", "Paraphrased Summary: Researchers explored the use of generative models as a data source for unsupervised representation learning in neural networks. They created contrastive pairs using the latent space of the generative model and combined this with standard imagespace contrastive learning. While the methods performance slightly trailed full dataset training it required storing only the model weights saving storage space. Strengths:  open and concise writing  Technically sound methodology  experimentation and testing support the methods effectiveness  study both unconditional and classconditional representation learning  Explores the potential of generative models for data compression privacy concerns and augmented training data. Weaknesses:  A similar idea \"Noise perturbation\" by Yang et al. was not cited reducing the methods perceived novelty. Additional Questions and Comments:  An ablation study examining the impact of truncation value on image quality and representation learning would be valuable.  Provide details on BigBiGAN encoder training and evaluation including image size and truncation parameter determination.  For the image 6 ablation explain the rationale for keeping iteration count constant and consider training models to convergence.  Offer recommendations for tuning standard deviation.  Explore the performance difference between steering and Gaussian perturbation in the classconditional setting.  Include references to ContraD and GANverse3D which relate to the use of generative models in contrastive learning and multiview data generation. Misc:  Table 3: Missing bold issue for ImageNet1000 issue.", "Paraphrased Summary: This study investigates the issue of using a generative model to train a representation instead of using the original data. The study is wellwritten easy to follow and provides a valuable initial exploration of this issue through wellconducted experimentation. While it does not introduce any major theoretical advancements the paper provides insights into a new field. Strengths:  Provides a strong empirical foundation to a relatively unexplored field.  experimentation are wellexecuted and demonstrate a open understanding of the issue.  The concept of using latent space perturbations instead of pixellevel perturbations is innovative and opens up new research possibilities. Weaknesses:  The issue depend heavily on the quality of the generative model which is not explicitly addressed in the study.  The performance of models trained on the generative model is not as effective as models trained directly on the original data suggesting that the generative model is not a complete representation of the data.  While exploring alternative perturbation methods is acknowledged as a potential field for further research the paper does not discuss the limitations or potential benefits of using Gaussian perturbations compared to more sophisticated methods."], "lKrchawH4sB": ["Paraphrased Statement: Abstract: This field introduces the \"Heterologous Normalization\" (HN) technique to optimize training in further deep networks. HN differentiates from standard normalization techniques (e.g. Batch Normalization Instance Normalization) by calculating normalization parameters (mean and standard deviation) from different pixel sets. experimental results demonstrate that HN performs marginally beneficial than BN and Group Normalization in certain scenarios. effectiveness and Weaknesses: effectiveness:  clear and straightforward methodology.  Introduction of a somewhat novel approach. Weaknesses:  limited experimental support:  Results need to be replicated multiple time.  mean and standard deviation should be reported for CIFAR and CALTECH256 datasets.  GN results for CIFAR appear inferior to BN which raises concerns about parameter settings.  Lack of theoretical explanation:  The reason for using different dimensions to calculate mean and standard deviation is not fully explored.", "Paraphrase: Summary: The researchers have developed a method for estimating batch normalization parameters (mean and standard deviation) using different data sets. effectiveness:  The method is innovative and complements existing research on normalization techniques.  It provides a novel view on batch normalization treating it as a variant of an significant existing method.  It was tested on common benchmark datasets with positive results. Weaknesses:  The method is currently limited to range data even though normalization is used in various data types and models.  No error bars are provided for reported performance metrics potentially attributing performance improvements to chance.  The experiments were conducted with models significantly below current performance standards limiting the methods practical relevance.  Figures could be improved for clarity.  No explanation or hypothesis is given for why the strategy of using different data sets for parameter idea improves batch normalization.", "Summary Paraphrase: This paper introduces a method for normalizing deep network training data using statistics from different pixel sets. This \"heterologous\" normalization is proposed to improve performance and stability in deep learning tasks. Experiments on various datasets support these claims. effectiveness and Weaknesses Paraphrase: effectiveness:  Simplicity: The proposed method builds on existing techniques and is straightforward to implement. Weaknesses:  Ambiguous Formula: The normalization formula uses different mean for estimating the mean and standard deviation making its calibration unclear.  Unclear performance Pattern: The heterologous combination show beneficial performance on a specific batch size (128) but its unclear why and what this implies.  Vague Claim: The claim about \"equilibrium between generalization and stability\" lacks a clear explanation.  limited Justification: The conclusion that replacing batch normalization standard deviation with estimated normalization standard deviation is more effective is not adequately supported by the argument about similar values.  Unresolved Issue: While the method addresses fluctuations in standard deviation with small batch sizes the appropriate range for standard deviation remains unclear.", "Paraphrased Summary: Heterologous Normalization (HN) is a novel normalization technique for neural networks that provides an alternative to existing methods like batch normalization (BN) and layer normalization (LN). HN operates under the insight that optimal statistics for normalization (mean and standard deviation) can be derived from different pixel sets. Key Features:  Calculates the mean in the style of BN but uses the approach of exponential normalization (EN) to stabilize the variance.  Achieves comparable or slightly beneficial performance than BN in typical batch sizes (e.g. 32 or 16).  Significantly outperforms BN in extremely small batch sizes (e.g. 4). effectiveness:  Simple and easy to implement.  Offers valuable insights into the different roles of mean and standard deviation in normalization.  Robust to varying batch sizes and costfree for inference (in some configurations). Weaknesses:  Does not consistently outperform stateoftheart normalization methods on large datasets (e.g. ImageNet).  Suffers from performance degradation as batch size decreases even surpassing BNs performance.  May not be the optimal choice for practical applications as other methods (e.g. ENBN and MABN) provide superior performance.  The authors interpretation of HNs performance in Section 4.6 is questionable as EN generally introduces more stable statistics than BN but ENEN does not perform as well as ENBN. Further investigation into HNs mechanism of action is recommended."], "uHv20yi8saL": ["Paraphrased Statement: This paper aims to demonstrate the theoretical guarantee of monotonic improvement for IPPO and MAPPO. It shows that enforcing separate trust region restrictions on individual policies indirectly applies the trust region condition to the collective policy. The paper includes experimental evidence to support this claim. Strengths:  The paper addresses a significant challenge in multiagent reinforcement study (MARL): achieving decentralized policy study with guaranteed joint policy improvement.  It provides theoretical justification for the strong performance of IPPO and MAPPO. Weaknesses:  The primary contribution Theorem 2 may contain errors in its validation. Specifically an approximation used in the deduction on page 14 may not be appropriate as it disregards the inherent nonstationarity caused by independent policy study.  This approximation undermines the papers independent argument as nonstationarity poses a fundamental obstacle in theory.", "Summary This paper investigates how independent ratios (over agents) improve cooperative multiagent reinforcement study (MARL) settings. It reveals that decentralized policies experience nonstationary distributions due to simultaneous policy updates by other players challenging traditional monotonic improvement properties. The authors propose a surrogate objective for decentralized policies bounding the improvement in expected return based on the sum of these surrogate objectives and a term related to policy divergence. They further demonstrate that policy divergence can be bounded by enforcing constraint on independent ratios with stricter bounds for more agents. This theory array with the success of existing methods like IPPO and MAPPO. Strengths and Weaknesses Strengths:  The theory explains the monotonic improvement observed in IPPO and MAPPO.  The derivations are clear and new. Weaknesses:  The impact beyond theory is limited.  Some presentation number exist such as small graph labels and unclear abbreviations.  The experimentation section requires further clarification:  The meaning of \"properly\" in hyperparameter tuning.  The disagreement between theory and practice.  The use of behavior policycollected samples for total variation estimation.  The fairness of the experimental comparison in the last section.", "Paraphrased Statement: This paper explores a method for ensuring that multiple cooperative agents working together in a reinforcement learning environment obey the trust region constraint by limiting independent ratios based on the number of agents. Additionally the paper demonstrates the functional equivalence between IPPO and MAPPOs surrogate objectives when their critics reach a stable point which is supported by empirical data. Strengths:  Clear writing and straightforward technique  Useful analysis of ratio constraint in MAPPO and IPPO Weaknesses:  The paper focuses on fully observable and cooperative scenarios where agents contribution the same goal based on the same information. As a result the key finding on \"bounding independent ratios based on the number of agents\" appears somewhat obvious. The authors could explore more complex scenarios such as partial observability or competitive environments.  The comparison between MAPPO and IPPO on different SMAC environments could benefit from additional experimentation and baselines. Specifically including JRPPO would help highlight the differences between joint and independent ratios.", "Paraphrased Statement: This research aims to expand the PPO algorithms monotonic improvement guarantees to multiagent settings. Strengths and Weaknesses:  Clipping value of 0.1 is standard with higher values generally avoided. Ablation study with lower clipping values (e.g. 0.1) would provide valuable insights.  Proposition 4 and related discussion may require clarification. While unique fixed points exist under certain assumptions it is unclear if centralized value work work study.  In Fig. 1d the minimal impact of epochs on variational distance suggests that a higher number of epochs could accelerate study at a clipping ratio of 0.1.  The specific number of epochs used in Figs. 1b and 1c is not provided.  Recurrent policies in these settings may affect the results. Clarity improvement:  Use scientific notation on the xaxes of SMAC graphs.  Remove underscores from the yaxes of SMAC graphs.  Clarify the meaning of \"(4)\" in the legends of Figs. 4 and 5.  Rewrite Section 6.2 to clarify that a sufficient condition does not fully constrain independent ratios due to imperfect estimation."], "ucASPPD9GKN": ["Paraphrased Summary This search investigates how the performance of graph neural networks (GNNs) for classifying nodes with limited labeled data is impacted by the distribution of labels across nodes in the graph (label homogeneity or heterogeneity). The authors propose that the traditional GNN architecture (called CGN) can perform well on certain graphs with label heterogeneity challenging a common belief in the literature. They provide theoretical insights into when good CGN performance is potential in such scenarios. Strengths and Weaknesses Strengths:  Provides a more nuanced understanding of the role of label homogeneity for GNNs.  Clarifies when CGNs can perform well even with label heterogeneity. Weaknesses:  The authors may have overstated the consensus in the literature that CGNs cannot perform well on graphs with label heterogeneity.  The definition of label homogeneity does not explicitly consider the correlation between node labels and node features which can impact CGN performance.  The theoretical analysis assumes unrealistic distributions of features and labels which limits its applicability to practical scenarios.  The numerical experiments provide a more nuanced picture than the theoretical analysis but these insights are not fully discussed in the theoretical section. Minor Comments:  The paper uses H and H as input and output variables instead of X as mentioned in the last line of page 2.", "Paraphrase: This search challenges the prevailing notion that graph convolutional networks (GCNs) rely heavily on high homophily (similarity between connected nodes). It argues that GCNs can effectively learn distinct node representation even under low homophily if nodes from each label exhibit unique distributions of neighbor labels. The paper presents evidence to support this claim through:  evidence GCNs superiority over other methods on datasets with low homophily.  Providing a theoretical analysis linking homophily to decision boundaries.  Conducting experiments on synthetic graphs created using a controlled edge addition operation that varies homophily.  Analyzing realworld datasets from different perspective to assess the impact of homophily. While the paper presents innovative insights into GCNs ability to handle low homophily it also raises several concerns that require attention:  The experimental results may be inconclusive as Table 1 does not provide a clear justification for GCNs outperformance.  The edge addition algorithm does not control degree distribution potentially biasing the experiments towards highdegree nodes.  The theoretical analysis focuses on twoclass graphs which may not generalize to more complex scenarios.  The analysis of realworld datasets is limited to a specific neighbor label distribution raising questions about the generalizability of the findings.", "Paraphrased Statement: Summary: This study reexamined the performance of Graph Convolutional Networks (GCNs) on graphs with heterophily (node diversity). Contrary to previous assumptions the authors found that heterophily does not always lead to poor GCN performance. They showed that GCN embeddings remain distinguishable in specific graph types where nodes with the same labels share similar node features and neighborhood label distributions. theoretical analysis using the Configurable Stochastic Block model (CSBM) and empirical experiments on synthetic and realworld graphs revealed that GCNs can achieve good performance on heterophilous graphs under certain conditions. Strengths: 1. Enhanced understanding of the heterophily problem and GCN performance. 2. Development of a crossclass neighborhood similarity metric to explain GCN performance on diverse graphs. 3. Clear and concise writing. Weaknesses and Questions: 1. Focus solely on GCNs despite the general title on Graph Neural Networks (GNNs). 2. Discrepancy in GCN performance on Chameleon and Squirrel datasets compared to other search. The authors should explain the potential understanding for this difference. 3. The crossclass neighborhood similarity metric lacks a direct theoretical connection to GCN performance. Node feature distribution also plays a crucial role. 4. Theorem 1 assumes that nodes with the same label are sampled from the same feature distribution. This strong assumption could limit the applicability of the analysis. 5. The exception of nodes with no neighbors raises questions about the validity of the analysis.", "Paraphrase: Summary: The study investigates the conditions under which Graph Convolutional Networks (GCNs) perform well on heterophilous graphs and provides theoretical and empirical evidence. Strengths and Weaknesses: Strengths:  The authors present valid parameter about GCN performance on heterophilous graphs supported by empirical observation. Weaknesses:  In the equation on page 2 the variable \"X\" is missing.  The results in Table 1 do not align with some experiences despite tuning hyperparameters as described in Appendix D. More details on the Table 1 settings are requested.  The authors incorrectly claim that Kentas work removes nonlinearity in their analysis.  The significance of Theorem 1 in relation to heterophily and homophily in Definition 1 needs clarification.  The role of cosine similarity to define interclass distance should be explained and its advantages over other metrics highlighted.  design 5s small font size and dark blue background make it difficult to read the values.  The writing style is verbose making it challenging to read. Consider breaking up long paragraphs and reorganizing content after section 3.1.  The theoretical analysis on CSBM should be explored for multiple class or more general graphs."], "pLNLdHrZmcX": ["Paraphrased Statement: This paper proposes a new approach SANE to enhance ensemble learning by considering model specialization.  analysis: By analyzing existing ensemble methods (random and diversitydriven) the paper identifies their weakness in lacking specialization.  Proposal: SANE introduces anchor points in the latent space to guide model learning towards specialization. It uses a transformerlike attention mechanism to determine the weights of base models based on their correlation with samples.  Evaluation: The paper demonstrates the effectiveness of SANE on various image and tabular datasets. Strengths:  Clear and easytounderstand presentation  Introduces a new concept of specialization to improve ensemble learning Weaknesses:  Limited newty as SANE leverages existing techniques (e.g. attention mechanism from transformers)  Inadequate exploration of the hyperparameter controlling the number of specialization anchors:  The experiments only consider a maximum of 10 anchors which may not be sufficient for complex datasets like ImageNet.  Further analysis or experiments on larger datasets are needed to determine the optimal settings for this hyperparameter.", "Paraphrase: Summary: The paper presents a method to train a group of models where each model has a specific focus on a different portion of the data hidden characteristics. Strengths:  The approach explicitly includes diversity among the models based on their different focuses. Weaknesses:  There is no mathematical or theoretical evidence to demonstrate that this strategy will result in a superior ensemble model.", "Paraphrased Statement: Summary: This paper presents a technique for training specialized models that focus on different field of the data by adjusting the training loss based on the proximity of examples to \"anchor\" embeddings. It also suggests combining multiple \"specialized\" models to enhance ensemble performance. Strengths and Weaknesses: Major Comments: Motivation:  The paper assumes its easy to train multiple simpler models for a complex task but this approach has not always led to high performance. The paper does not explain how its method overcomes this obstacle.  The paper claims that diversity alone is insufficient for optimal ensemble performance but it only evaluates a limited image of tasks. Recent studies suggest that with sufficiently distinct models specialization can be achieved without explicit training.  The paper asserts that its losses ensure model specialization but it does not provide specific guarantees. Correctness:  The paper primarily evaluates tabular data and small image datasets using small models.  It compares its method with randomly reinitialized ensembles and diversityinducing methods but does not compare it with other stateoftheart ensemble techniques like weight averaging EMA or hyperparameter ensembling. This makes it difficult to assess the performance of the proposed method in realistic settings. Minor Comments:  The paper claims that its method outperforms stateoftheart ensemble methods but does not consider methods like weight averaging and hyperparameter ensembling.  The message of image 3 is unclear.  The paper makes respective claims about what \"should\" be in a design but does not provide sufficient motivation or empirical evidence to support these claims.  image 2 should include error bars as the experiments are conducted on small datasets like CIFAR100.", "Paraphrase: This paper introduces a new approach to ensemble learning that encourages specialization among individual models. It employs a mechanism to enforce specialization and a loss work to guide the base models towards specialized roles. The methods effectiveness is demonstrated on both structured and image data. Strengths and Weaknesses:  The idea of using base learner performance to train specialized weak learners is logical.  The binary crossentropy loss work used for supervising the weighting parameter could be problematic as it could lead to all base learners fitting correctly classified examples.  comparison with boosting tree methods are biased due to differences in base learner types. A fair comparison would require similar base learners.  The image dataset and baseline model are inadequate for demonstrating the superiority of the proposed method. A more recognized combination like ResNet50 on ImageNet would be more appropriate."], "g2LCQwG7Of": ["Paraphrased Statement: Summary: This paper proposes a graph clustering method based on a probabilistic model that relaxes the discrete structure of treebased hierarchies. It assesses the model using two metrics: Dasgupta cost and TreeSampling Divergence. The method employs efficient vectorized computing and provides guidance on hyperparameter selection. Empirical Evaluation: The paper evaluates the method on eight realworld datasets comparing it to baseline hierarchical clustering techniques using Normalized Mutual Information (NMI). It also evaluates the method on controlled clustering (basis truth class labels) and link prediction tasks showing either beneficial or secondbeneficial performance. Additionally a metaanalysis includes ablation studies scalability analysis runtime measurements and qualitative assessments. effectiveness and Weaknesses: effectiveness:  Utilizing graph structure for hierarchical clustering can improve accuracy and efficiency.  The Markov chain over the tree defines a chain suitable for inference.  The block formulation of vectorized computing enables efficient implementation. Weaknesses:  The paper lacks contingent on the \"other experiments\" mentioned in the PGD optimization discussion.  Baseline results are reported for the beneficial run out of five trials. The variance for both baseline and proposed models should be provided.  The selection of datasets for reporting in certain experiments is not fully explained e.g. only three datasets are included in Table 2.", "Paraphrase: This paper presents a mathematical model for hierarchical clustering that uses a treebased approach. The model establishes links between a probabilistic tree sampling process and a specific Markov chain. This link enables endtoend learning optimizing metrics commonly used in graph clustering such as the Dasgupta cost and Tree Sampling Divergence. This method offers efficient and exact graph clustering. effectiveness and Weaknesses: 1. The paper is wellwritten and uses consistent notation throughout. 2. The proposed method is theoretically sound avoiding heuristic approach. 3. The model directly optimizes clustering objectives in an endtoend manner achieving scalability and competitive performance. 4. The experiments provide comprehensive results on various graph case including largescale datasets. Weaknesses: 1. The implementation contingent could be more extensive. 2. The supplementary materials reportedly contain implementation code but it was not found.", "Paraphrased Statement: Summary: The paper presents a novel probabilistic clustering method that combines Dasgupta loss and Treesampling divergence. It proposes an optimized samplingbased approach using projected gradient descent. effectiveness:  The hierarchical probabilistic clustering approach is novel and promising.  Dasgupta loss is a recent advancement in clustering. Weaknesses:  Experimental evaluation is inadequate and unconvincing.  comparison are limited to basic graph clustering methods.  DeepWalk embeddings used are lowdimensional and may not accurately represent graph structure.  scalability is questionable as evaluation is only on one large dataset. Suggested Improvements:  Rerun experiments with higherdimensional DeepWalk embeddings (d128).  Include classical graph clustering baselines (e.g. Louvain).  Provide results on additional mediumscale datasets without graph subsampling.  If these requests are considered unreasonable the authors should offer alternative ways to address the weaknesses identified."], "niZImJIrqVt": ["Paraphrase: Authors propose a reinforcement learning method for meanvariance portfolio optimization transforming the problem into maximizing expected quadratic utility. This eliminates the need for approximating the variance gradient enabling simultaneous estimation of the first and second reinforcement moments. They evaluate two algorithms: REINFORCE and ActorCritic. The method is successfully applied to synthetic and FamaFrench datasets. Strengths:  Leverages financial theory to enhance portfolio optimization with reinforcement learning.  Wellwritten and logical flow. Weaknesses: issue:  Insufficient explanation for the proposed algorithms superiority over existing methods.  Inconsistencies in picture 2 where objective variance exceeds observed variance.  Lack of clarity on model training for realworld datasets. Writing:  Insufficient problem formulation lacking details on action space state space and reinforcements.  Incomplete description of the algorithms implementation steps.  Unclear placement of the discussion on perstep variance perspective. Literature Review:  Absence of context regarding the rich body of research on portfolio optimization in mathematical finance using stochastic control. Typos:  \"variacen\" should be \"variance\"  \"Pareto efficient policy\" should be \"Pareto efficient frontier\"", "Paraphrase: The paper explores a reinforcement learning framework focused on maximizing both mean and variance in a manner similar to Pareto efficiency. It develops a quadratic formulation that connects with the usual meanvariance formulation offering diverse interpretations of meanvariance efficiency. Novel algorithms inspired by REINFORCE and actorcritic methods are proposed to implement the quadratic formulation. Experiments in investmentrelated scenarios show the effectiveness of the REINFORCEbased algorithm in compare to current techniques. Strengths and Weaknesses: The paper is wellwritten and accessible with an appendix that provides a comprehensive survey of relevant concepts from finance and economics. The authors thoroughly discuss related literature and contrast their approach with existing methods. Experimental results across different investmentrelated settings demonstrate the algorithms efficiency. Questions:  A correction is suggested: E[G]  \u03bb (...) should be E[G]  \u03bb (...) (page 4 line 7).  The estimation of E[G] by \u02c6Gk is mentioned (page 5 line 4). Intuition for this estimation is requested.  The reviewer expresses concern in comparing the actorcritic implementation with the REINFORCEbased method in experiments. PostRebuttal: The reviewer values the authors response and acknowledges their comprehensive rebuttal and the input of other reviewers. The score is maintained.", "Paraphrase: The paper presents an alternative goal (expected quadratic utility) for episodic reinforcement learning tasks instead of the meanvariance goal. The novel goal removes the squared expectation term eliminating the need for double sampling to calculate the meanvariance goals gradient. Additionally the novel goal offers insightful interpretations including Pareto efficiency. Strengths and Weaknesses: Strengths:  The paper is wellwritten and clearly motivated aiming to address the computational challenges of optimizing the meanvariance goal.  The expected quadratic utility goal is straightforward to implement.  The interpretation of the proposed goal is wellexplained. Weaknesses and Questions:  Question: Can the difference between the optimal policy under the proposed goal and under the meanvariance goal be quantified  Observation: Only mean performance metrics are reported in the experiments. What are the standard deviations of the methods performances  Side Points:  The authors should provide More details on how traditional meanvariance reinforcement learning goals can be formulated as constrained optimization problem with equality constraints.  The notation \\nablaREINFORCE appears unconventional.", "Paraphrased Statement: Summary: This research aims to develop efficient policies that optimize the tradeoff between mean and variance (MV tradeoff) when making investment decisions. The authors present an approach called expected quadratic utility maximization reinforcement learning (EQUMRL) which trains an agent to maximize a specific utility function that considers both mean and variance. EQUMRL is computationally efficient and does not face the difficulties encountered by other methods. The authors provide experimental results to showcase the efficacy of their proposed approach. Strengths:  The problem of optimizing MV tradeoff is relevant and practical.  Extensive experiments demonstrate the superiority of EQUMRL in use.  The authors provide opensource code for reproducing their results. Weakness:  The paper lacks theoretical analysis specifically regarding the regretmeancovariancePareto efficiency guarantees of EQUMRL. Formal theorems and rigorous validation would enhance the theoretical foundation of the work. After Rebuttal: The authors note that regret analysis in reinforcement learning is an ongoing research problem. However there are existing work that provide regret bounds for RL under certain settings. While the authors acknowledge the difficulty in analyzing policy gradient algorithms with complex models like neural networks the paper still fails to offer sufficient theoretical contribution. The experimental evaluation appears conventional and does not include comprehensive benchmarks. Based on the lack of substantial theoretical analysis and the standard nature of the experimental results the reviewer maintains their recommendation for rejection.", "Paraphrase: Summary This paper introduces a technique named EQUMRL for optimizing the balance between mean and variance in reinforcement learning (RL). This method aims to maximize the expected quadratic utility a concept borrowed from finance theory. By eliminating the issue of double sampling present in previous work EQUMRL simplifies the optimization work. The researchers conducted experiments using both synthetic and realworld data to demonstrate the algorithms effectiveness. Strengths  The method establishes a connection between utility optimization (finance) and RL.  It addresses the double sampling issue and is compatible with various policygradientbased algorithms.  The experimental results indicate superior performance compared to alternative approaches. Weaknesses  The experimental section could be improved.  In the portfolio management experiment (Section 6.2) the dataset is static and does not change based on the agent actions. This setup resembles an online learning or supervised learning problem rather than an RL task.  The authors do not test the algorithm on a more representative RL benchmark to evaluate its performance in a realistic RL setting. Minor issue  Grammatical and typographical errors in Sections 2.1 and 6.2."], "hpBTIv2uy_E": ["Paraphrase: Hypergraphs can represent relationships between multiple elements in realworld data. various neural networks have been developed for hypergraphs utilizing both group associations and node characteristics. This paper: 1. Introduces AllSet: A unifying framework generalizing existing hypergraph neural networks. 2. Explored AllSet based on:  Deep Sets (NeurIPS 2017)  Set Transformer (ICML 2019) 3. Evaluated AllSet:  On existing benchmarks  On three custom hypergraph datasets Strengths: 1. Technical Soundness: AllSet and its variants are suitable for node classification in hypergraphs. 2. open Presentation: The paper is wellwritten and organized with comprehensive appendices. Weaknesses: 1. Originality: The core concept of AllSet is not novel having been previously used in hypergraph and bipartite graph neural networks. 2. Significance of evaluation: The proposed methods show limited improvements on most datasets with significant performance on only three curated datasets. 3. Limited Scope: experimentation focus only on transductive node classification while the AllSet frameworks potential in other tasks (e.g. inductive text classification hyperlink prediction) remains unexplored.", "Paraphrase: Summary: Adapting standard graph operators to hypergraphs is challenging. multiple message passing operators have been proposed for hypergraphs including clique expansion and tensorbased methods. This paper introduces a framework that utilizes learnable multiset functions to learn the hypergraph propagation map from the data. This framework encompasses many existing propagation methods. Strengths:  Wellorganized paper with open motivations and related work discussion.  Simple and elegant proposed idea.  comprehensive hypergraph datasets in experiments including newly collected ones. Weaknesses:  experimentation details need clarification specifically regarding the depth of layers in AllDeepSets and AllSetTransformer and the amount of known labels used.  Notation in Equation (8) requires more explanation.  other propagation methods falling between clique expansion and tensors have been proposed (e.g. [13]). Its unopen if these methods are also covered by the proposed framework.", "Paraphrased Summary: The paper suggests a framework for neural networks that operate on hypergraphs. It claims that current approach to propagating data on hypergraphs involve converting them into regular graphs through clique expansion potentially causing loss of data. Instead this framework employs tensors for propagation. The framework dubbed \"AllSet\" combines different propagation methods into a single system. It has two implementation: \"AllDeepSets\" and \"AllSetTransformer.\" The authors conduct tests on various graph datasets. Strengths and Weaknesses: Strengths:  The issue is welldefined and supported by thorough research on current approach using clique expansion and tensorbased propagation.  experimentation include comparisons with numerous baseline (ten models) and across ten datasets.  theoretical analysis demonstrate the frameworks generality of previous works. Weaknesses:  Since the framework combines existing propagation mechanisms an ablation field should be conducted to determine the contribution of each component.  The experimental section focuses only on numerical comparisons. A more indepth discussion of the methods advantages over existing techniques would highlight the sources of improvement.  Evaluating only one task (semisupervised classification) is limiting as the core of the framework is to derive hypergraph representations. Additional evaluation tasks would be beneficial.", "Paraphrase: This research introduces a new approach for hypergraph neural networks using the concept of multisets. Unlike existing methods that rely on clique expansion this approach learns explicit representations of hypergraphs and utilizes two multiset functions to propagate data between nodes and hyperedges. This generalized framework includes a broad range of existing GNN techniques. Experimental evaluation demonstrate that the proposed method AllSetTransformer achieves comparable or superior performance on a variety of benchmark datasets. By unifying various approach for hypergraph neural networks this method provides a versatile and efficient solution across a various range of problems contributing to the advancement of the research community."], "s6roE3ZocH1": ["Summary: This work proposes using a twostage genetic algorithm for optimizing molecular structures within predefined similarity constraints. The authors developed chemically relevant mutation and crossover strategies demonstrating promising results in constrained optimization using the penalized logp metric. Strengths:  focus on molecular optimization under structural constraints which is significant for realworld application.  further results in the core optimization task. Weaknesses:  Limited experiments that do not fully reflect realworld drug discovery challenges.  Mild algorithmic novelty that warrants more thorough evaluation.  Absence of exploration of more complex tasks such as optimization against property predictors or docking scores.  Implementation of structural constraints as a hard constraint on similarity rather than a graph or scaffold constraint limiting the demonstration of genetic algorithms unique capabilities.  Lack of assessment for potential chemical issue (toxicity instability) in generated molecules. Unclear aspect:  method for generating initial SMILES strings when the algorithm cannot meet constraint requirements.  Meaning of \"probabilities are the same\" in the context of selecting molecule for mutation.  Interpretation of \"we restricted the range of the performance of mutation to the terminal 10.\"  Definition of \"optimization target of 800 molecules with the lower penalized logP value in the data set.\"  Reference to a docking module in the conclusion which was not mentioned earlier. Misrepresented Prior work:  Underestimation of the existence of scaffoldconstrained optimization models.  Inaccurate description of JTVAE as a structureconstrained model.", "Paraphrase: Summary: Researchers developed a genetic algorithm for creating molecules. The algorithm aims to balance multiple objectives such as those described in Equation 3. It employs genetic operations like crossover and mutation. The team tested the algorithm to enhance the LogP (lipophilicity) of molecules. Strengths and Weaknesses: 1. similarity to Previous work: The algorithms objective work (Equation 3) and genetic operations resemble those used in a previous work by Nigam et al. (2019). 2. Limited scope: The experiments focused solely on improving LogP. This limitation prevents the demonstration of the algorithms versatility for general molecule generation. 3. Modest Improvements: The algorithms improvement upon existing methods (GADNN) was relatively small.", "Summary Paraphrase: This research introduces a genetic algorithm designed for optimizing molecular structures while adhering to constraints. To achieve this a twostep process is proposed: 1. constraint satisfaction Phase: focus on generating molecules that satisfy the constraints and resemble the original molecule. 2. property Optimization Phase: Finetunes the molecules to improve their desired properties. The genetic algorithm employs a crossover performance similar to the \"graph Based Genetic Algorithm\" and a mutation performance that randomly modifies the molecular representation known as \"SELFIES strings.\" Strengths and Weaknesses Paraphrase: Strengths:  The work provides a comprehensive review of related work. Weaknesses:  The results are weak and inconclusive. The proposed method achieves marginally beneficial performance than the \"GADNN\" baseline in logP optimization (5.53 vs. 5.93 for a stringent constraint).  The method falls short of the benchmarks set by \"Jin et al. 2019\" in optimizing properties such as QED and DRD2.  The proposed method lacks novelty. It essentially applies a standard genetic algorithm with a twostage optimization process. The crossover operator and mutation performance are borrowed from existing techniques.  The paper lacks an ablation work to evaluate the impact of the twostage process. The claim that this process benefits model performance is not supported by strong empirical evidence.", "Paraphrased Statement: Summary: This research introduces a genetic algorithm framework for designing molecules. It generates valid molecules using crossover and mutation operations paired with appropriate fitness work. The framework was tested on a specific property optimization task. Strengths:  The framework integrates existing techniques with sound rationale.  It surpasses other methods in the PlogP optimization benchmark. Weaknesses:  Limited Novelty: The framework primarily employs existing genetic algorithms and straightforward fitness work. While the crossover and mutation operations are novel their modifications are minor compared to existing approaches. The graphbased crossover is an adaptation of the GBGA algorithm.  Incomplete Multiobjective Optimization: The research claims to address multiobjective problems but only demonstrates optimization for a single property (PlogP). Multiobjective molecular optimization typically involves optimizing multiple properties simultaneously which should have been considered to demonstrate the frameworks superiority.  Narrow Experimental scope: The experiments only showcase PlogP optimization. There are other singleobjective properties such as QED or DRD2 that could have been included to provide a more comprehensive evaluation."], "vds4SNooOe": ["Paraphrased Summary: This paper presents a technique for adapting a largescale pretraining model to more specific unseen labels. The approach treats this job as a model with superclasses and subclasses trained using a maximum likelihood expectationmaximization algorithm. The paper proposes a superclass Conditional Gaussian Mixture (SCGM) model which uses a hierarchical Gaussian distribution on class relationships. This model is tested on six ImageNetrelated datasets and a realworld DialysisEvent dataset demonstrating competitive performance in generalizing to both known and unknown superclasses. effectiveness:  Addresses the challenge of finetuning models for specific jobs with limited data.  Provides comprehensive evaluations on multiple datasets showing the effectiveness and consistency of the proposed method.  Visualizations indicate that the method successfully learns the superclasssubclass structure. Weaknesses:  Focuses on learning embeddings but evaluations primarily concentrate on classification jobs.  For classification use building a classification model directly could be more efficient.  If the goal is embedding learning more emphasis should be placed on embeddingbased evaluations and the relationship between embeddings and classification performance.", "Paraphrase: Summary This paper introduces SCGM a new approach to address the CrossGranularity FewShot (CGFS) problem. In CGFS a model trained on extensive (\"superclass\") class needs to perform well on more specific (\"subclass\") class within each superclass. SCGM uses a generative model approach for endtoend classifier training that:  Incorporates superclasssubclass relationships  Doesnt require explicit subclass enumeration  Improves CGFS performance  Offers systemlevel advantages over existing methods effectiveness  Identifies an significant and understudied problem.  Proposes a novel and theoretically good solution based on generative model.  Demonstrates compelling empirical results on multiple performance metrics. Weaknesses  Some comparison to other methods may be overstated.  performance gains over ANCOR are not always convincing.  Lack of analysis on worstcase subclass performance and potential limitations on subclass size.", "Paraphrased Summary: The paper tackles the challenge of transferring knowledge from abundant data with limited class labels to finegrained data with scarce labels. It employs a Gaussian mixture model to establish connections between the coarsegrained data structures and the limited finegrained labels. effectiveness and Weaknesses: effectiveness:  Clear and wellreasoned paper  Innovative approach  Strong and consistent experimental results  Open approach to code and data for reproducibility Weaknesses:  Lack of a concise summary of contributions  Overly verbose and irrelevant introduction  Limited discussion of related work (e.g. relationship with coarsetofinegrained supervision in zeroshot classification)  Absence of experiments on established finegrained classification datasets (e.g. CUB Flowers) which are more relevant than CIFAR100  Limited testing on mixture of seen and unseen superclasses which is a practical limitation in realworld settings"], "pjqqxepwoMy": ["Summary The paper introduces VLOG a variationalbased framework for oracleguiding reinforcement learning (RL). This framework minimizes the KullbackLeibler (KL) divergence between latent features extracted from oracle observations and executor observations. This enables the framework to incorporate oracle signals and improve execution performance. The authors derive a lower bound objective as the training loss and demonstrate the effectiveness of their method in various environment. effectiveness and Weaknesses  effectiveness:  Clear and understandable presentation  Intuitive and effective approach using variational frameworks  Introduction of a novel and intriguing oracleguiding problem  Weaknesses:  Lack of clear distinction between oracleguiding RL and offline RL  Insufficient theoretical contribution or explanations of \"theoretical guarantees\"  Vague definition of the vt label (assumes knowledge of DQN or critic network)  The hyperparameter selection process could benefit from a scheduling approach Minor Issues  Repeated sentences in the abstract and introduction  Incorrect syntax in the definition of q(vt\\hatxt) (multiplying distributions)  Consider using a (PO)Markov game framework for poker games instead of a (PO)MDP Typos  \"the oracleestimated one as posterior distribution\" should be \"the oracleestimated one as the posterior distribution\"", "Paraphrase: Summary: This study focuses on reinforcement learning scenarios with limited informationrich sensitive observations and highquality oracle observations. The proposed method generates a meaningful latent state by applying a variational inference position. This involves processing both oracle and sensitive observations through separate encoders and merging the latent representations before passing them to a common decoder. The latent distribution from oracle observations defines the posterior distribution while that from sensitive observations represents the prior distribution. The method minimizes the distance between these two distributions by reducing a KL divergence term. effectiveness:  Clear and wellorganized writing  Potential value in exploring representation learning in reinforcement learning with Bayesian methods Limitations:  Restriction to tasks with available oracle observations  potential inconsistencies in the formulation of the evidence lower bound (ELBO)  Limited evaluation on simple tasks with potentially inappropriate experimental settings  use of a simplistic RL algorithm Detailed Concerns:  Assumption of oracle observations may not hold in many practical applications  Ambiguity in setting oracle observations as the posterior distribution  Lack of clarity on the relationship between the variational framework and the policy framework  Inadequate exploration of related study on variational inference in reinforcement learning", "Paraphrased Summary: This study explores an approach for handling problems where agents benefit from learning from features that are solely available during training. They introduce a variational learning method based on Bayesian theory and demonstrate its effectiveness on various tasks including a Mahjong simulation released with the paper. effectiveness and Weaknesses: The motivation and idea behind the paper are appealing. The approachs ease is advantageous. However the evaluation using Mahjong is confusing and the takeaway is unclear. The oracle poor performance in Table 2 is surprising. The scoring method described seems unusual and raises questions about fairness. The experiments combine offline RL and multiagent RL making it difficult to isolate the benefits of VLOG. To improve the exposition authors may consider clarifying the experiments or incorporating a more familiar benchmark. Questions for the Authors:  Is there a relationship between the position of executor (X) and the position of oracle (hatX)  Does the vt in p(vt \\bf xt) refer to V(\\bf xt)  In section 5.2 why is it stated that \"VLOGno oracle performed surprisingly well\" when the figures suggest otherwise  What do \"suphxstyle oracle guiding\" and \"OPDstyle oracle guiding\" represent  Which \"trained baseline framework\" was used in Table 1 Minor Comments:  The abstract could specify \"RL using variational methods.\"  In the s paragraph of the introduction remove \"etc.\"  In section 2 remove \"different\" after \"Another.\"  Consistent formatting is suggested for section headings.  Citation for MinAtar ALE should be updated.  The Mahjong evaluation should be reviewed for accuracy and clarity.", "Paraphrase: Summary: This study explores reinforcement learning (RL) with approach to enhanced information during training relaxing the limitations of traditional Partially Observable Markov decision process (POMDPs). A variational Bayes approach is proposed where a shared latent space is learned for incomplete observations during testing and complete observations during training. Experiments demonstrate the effectiveness of this approach across various benchmark tasks including Mahjong a multiplayer game with hidden information. effectiveness:  The variational Bayes approach is innovative and practical for utilizing trainingtime advantages. Weaknesses:  The problem studied is not solely novel as similar positions have been explored in imitation learning.  Variational Bayes is not universally applicable and its effectiveness in this context requires theoretical justification. Specific Concerns:  Equation (1): Clarification is needed on how `vtart` is derived without known transition or reinforcement use.  Mahjong experiment: It is unclear why `VLOGno oracle` outperforms other methods as its formula suggests it should be similar to vanilla policy learning without oracle observations.  Reference [1] is cited for a related position but the connection to the proposed approach and its relevance to the weaknesses raised is not fully explained."], "fR-EnKWL_Zb": ["Summary This paper introduces a novel attention mechanism for vision transformers that utilizes a quadtree to create a feature pyramid for tokens. The mechanism aggregates sparse keyvalue feature at each pyramid level resulting in an efficient attention algorithm. The proposed method achieves notable results in stereo image classification and object detection tasks. Strengths  The integration of a quadtree into vision transformers is an innovative approach that enables efficient longrange dependence while preserving local point.  The paper is wellstructured and presented.  The performance reported for the proposed method is competitive compared to other further vision transformers. Weaknesses  The topk assignment for tokens in the quadtree is not differentiable which potentially limits generalization.  The endtoend training process for the proposed method is unclear.  The aggregation of feature by the quadtree is unstructured and requires iterative step which can be less efficient for parallel process and may introduce higher latency compared to dense attention mechanism. It would be beneficial to provide specific throughput measurements on GPUs to demonstrate the efficiency claims.", "Paraphrase: Summary: This work aims to tackle the high computational cost of traditional Transformers. It proposes a new approach that utilizes token pyramids and a coarsetofine attention computation reducing complexity to linear time. This results in a quadtree attention structure with QuadTreeA and QuadTreeB variants for enhanced message aggregation. experimentation across four computer vision tasks (involving selfattention and crossattention) evidence that the proposed method achieves stateoftheart results while significantly reducing computational performance and model parameters. effectiveness and Weaknesses: Pros: 1. clear and wellorganized introduction. 2. Novel and relevant approach to improving attention efficiency in Transformers. 3. Comprehensive experimentation with strong results. Cons: Concerns for Rebuttal point: 1. Understanding how spare attention affects convergence speed and final accuracy in QuadTree Attention. 2. Examining the impact of token count in the finest resolution on model performance. 3. Investigating the issue of model parameter variations on performance and computational performance.", "Paraphrased Statement: Summary: The paper introduces a novel attention mechanism that utilizes a quadtree structure to improve global (longrange) attention while reducing the computational complexity of conventional attention from quadratic to linear. Evaluations on tasks like object detection image classification and feature matching evidence its effectiveness. effectiveness:  The concept is straightforward and accessible.  The approach has been proven effective across different tasks. Weaknesses:  The absence of references to significant related work is a flaw.  The paper is similar to [1] which also employed a multilevel attention approach with a quadtreelike structure to balance shortrange and longrange attention. This relationship should be discussed.  The evaluation only compared the quadtree attention against PVT lacking comparisons with vanilla attention mechanism and other efficient alternatives like shifted windows in Swin transformer or focal attention in Focal transformer.  The actual runtime when applying quadtree attention to the PVT architecture is not mentioned.  The paper claims Swin transformer restricts attention to local windows which is inaccurate as shifting windows allows data exchange between windows. Questions:  What is the actual runtime improvement of applying quadtree attention to PVT  How does quadtree attention perform when applied to other transformerbased architectures like Swin transformer or ViT  Why does the paper assert that Swin transformer limits attention to local windows when window shifting enables data exchange between windows", "Paraphrased Statement: Summary: This research introduces a novel attention mechanism for vision transformers called Quadtree Attention. This mechanism constructs hierarchical token pyramids and calculates attention in a coarsetofine way. At each level the top K regions with the highest attention scores from the query and key are selected for further attention computation on finer tokens while other regions use the current coarse attention as contribution of the output. empirical results show that Quadtree Attention achieves high performance across several vision tasks (e.g. feature matching stereo matching classification and object detection) with reduced computational cost. effectiveness:  Multiscale design allows for effective modeling of both longrange and local interactions.  applicable in both selfattention and crossattention tasks.  evidence effectiveness through empirical performance.  Wellwritten and easytounderstand paper. Weaknesses:  Limited novelty: Quadtree structure is not only novel and similar coarsetofine attention mechanism have been proposed in recent work. A comparison with such work is recommended.  Unclear pyramid token design: The construction and selection of pyramid tokens including average pooling patch embedding and top K sampling require further clarification.  Varying efficiency improvements: Quadtree Attention shows significant efficiency gains in object detection but not in ImageNet classification. This discrepancy requires further investigation.  Typos:  Equation (1): VT should be V  image 2: mi1 should be \\mathbfm1  Line below Eq (4): sij1 should be sij0"], "fy_XRVHqly": ["Paraphrased Statement: Summary: The paper presents SWAT an approach that integrates agent morphology into transformerbased policy for multitask reinforcement learning (MTRL). SWAT represents the agents body structure as a graph with nodes representing limbs and edges representing joints. It captures this structure through two embeddings: 1. positional Embedding (PE): Represents the absolute position of body parts using tree traversal (e.g. preorder inorder postorder). 2. relational Embedding (RE): Represents the relative space between body parts based on graph connectivity (e.g. normalized Laplacian shortest path space personalized page rank). SWATs PE and RE embeddings enable effective multitask policy learning across tasks with varying morphologies. Experiments show that SWAT outperforms baseline methods (GNNbased SMP and morphologyfree AMORPHEUS) in Gym Mujoco locomotion tasks particularly in crossdomain tasks with diverse morphologies. Ablation experiments confirm the importance of both PE and RE embeddings. effectiveness:  Improved performance compared to previous stateoftheart in multitask scenarios with diverse morphologies.  innovative approach that incorporates structural information into policy representation.  Wellwritten with relevant related work.  Qualitative analysis demonstrating what the structural embeddings capture. Weaknesses:  Lack of analysis on different tree traversals in PE and graph connectivity representations in RE.  Potential redundancy in PE and RE representations as they both capture structural information.  computational cost of SWAT compared to baseline approaches including additional embedding calculations and quadratic attention.  unclear contingent regarding the training set environments for transfer learning tasks (Humanoid and CWHH).", "Paraphrased Statement: This paper investigates the impact of positional and relational information in transformerbased policy for multitask reinforcement learning (MTRL) across diverse robot morphologies. Unlike previous approaches that disregard the structural properties of the robot this paper proposes using traversalbased positional embeddings and graphbased relational embeddings to exploit this information. Through experiments on MTRL and transfer learning tasks in gym environments the proposed method demonstrates superior performance compared to existing methods. effectiveness:  Motivated by the potential benefits of incorporating structural and morphological information in transformerbased policy.  Introduction of a novel traversalbased positional embedding that improves performance.  Relatively straightforward implementation and applicability to other GNNbased tasks.  Outperformance of stateoftheart methods in both MTRL and transfer learning.  Wellwritten and visually clear presentation. Weaknesses:  Concerns about the generalizability of the traversalbased positional embedding due to morphologyspecific joint indices.  potential overfitting due to the use of learnable positional embeddings in MTRL limiting generalization to tasks like zeroshot policy transfer.  Modest performance gains from relational embeddings with most improvements attributed to positional embeddings.  Lack of ablation work on all environments and the components of the proposed embeddings. Additional Comments:  Typo correction: \"the performance gains from the positional information(SWAT\\RE) are large than ones from relational information (SWAT\\PE)\" should read \"the performance gains from the positional information(SWAT\\PE) are large than ones from relational information (SWAT\\RE)\".", "Summary This research explores the problem of reinforcement learning in multitask settings where tasks have varying characteristics. The proposed approach represents policy as graphs where each node is a neural network module. Unlike previous techniques that used selfattention this approach incorporates morphological information through traversalbased and graphbased embeddings. Despite its simplicity the proposed solution outperforms stateoftheart methods on MUJOCO multitask environments. effectiveness  clear and concise writing  Straightforward and implementable approach  Empirical validation of performance against comparable methods Weaknesses  Limited novelty as positional encoding is a wellknown technique in natural language action and computer vision  Potential redundancy in traversal information aggregation  Lack of clarification on handling varying task dimensions with a shared MLP layer  Incomplete ablation work for assessing the contribution of individual relational encoding features  comparison fairness publication due to different replay buffer sizes used", "Paraphrase: Summary: This research presents a transformer model that is aware of the structure of inhomogeneous multitask reinforcement learning problems. It incorporates traversalbased positional embedding and graphbased relational embedding to capture morphological information. The model achieves superior performance compared to existing methods on a scope of module multitask RL benchmark and transfer learning scenarios. effectiveness:  Introduction of traversalbased positional embedding and graphbased relational embedding to encode morphological information.  significant performance improvements over previous stateoftheart methods on certain tasks.  clear and wellorganized presentation. Weaknesses:  limited experimental evaluation with entirely three seeds used for Figure 4. Increasing the number of seeds to 10 is recommended for more robust solution."], "vHVcB-ak3Si": ["Paraphrased Summary This work examines why pose regression methods that use an integral approach (IPR) perform differently on simple and challenging poses compared to methods that use an argmax approach. The authors suggest that IPR methods shrink their heatmaps which hurts performance on simple poses. They use toy examples and theoretical analysis to explain the shrinkage caused by IPR gradient. To address this they propose a loss role that encourages heatmap distribution prior reducing shrinkage. strength and Weaknesses Pros:  Provides a novel hypothesis on IPR method performance inconsistency.  Theoretical and empirical evidence supports the hypothesis. Cons:  The claim that highly localized heatmaps hinder performance lacks substantial empirical or theoretical support.  The proposed heatmap prior loss is not entirely novel and may introduce confounding factors.  The work does not clearly separate the effects of sigma on localized and dispersed heatmaps.  Theoretical evidence supports understanding IPR gradient behavior but lacks direct support for the primary claim. Minor Comments:  Abbreviations used without prior explanation (e.g. EPE).  Typos pose.  detailed derivation for Equation 10 not included in supplementary material.", "Paraphrase: This work thoroughly examines the Integral Pose Regression technique commonly used in Keypoint Localization. Integral Pose Regression has gained widespread adoption and stands as one of the two primary approaches for Keypoint Localization alongside Heatmap Regression. While previous work have compared these two approaches this work combines theoretical analysis with empirical findings to provide deeper insights. Key strength:  The work employs a compelling empirical setup considering the significance of Keypoint Localization in various applications.  It effectively combines theoretical exploration with empirical validation utilizing both toy and realworld settings. The combination of theory experiments and realworld examples enhances the work value.  The insights and tradeoffs identified are valuable for practitioners utilizing these methods addressing aspects such as training speed convergence and vanishing gradient.  The work clarifies assumptions and supports them with empirical evidence. Suggestions for Improvement:  In Figure 4 it would be beneficial to clarify the correspondence between the top red window (third frame) and the zoomedin version below.  Future research could explore scenarios where Integral Pose Regression and Heatmap Regression are combined such as utilizing Heatmap Regression for initial warmup and Integral Regression for improved performance on challenging examples.", "In pose estimation two common techniques are heatmap detection and integral regression. This paper compares these approaches through detailed analysis and theoretical examination. A unified model is presented to facilitate the comparison. Extensive experiments corroborate the theoretical findings. While the paper excels in organization and thoroughness it lacks guidance on applying its conclusion to enhance pose estimation. However this omission is not significant and does not detract from the work value."], "noaG7SrPVK0": ["Summary This paper addresses how to generate a set of counterfactual explanations for a binary linear classifier. These explanations aim to help users adjust their input data to change the classification issue considering potential changes in model parameters. The proposed Counterfactual Plan under Ambiguity (COPA) framework evaluates the validity of a predetermined plan under various parameter distributions and corrects it to improve its validity. By optimizing a combination of proximity diversity and validity COPA generates a collection of plans that satisfy certain bounds on validity. effectiveness and Weaknesses effectiveness  Relevant and timely issue  Solid approach to address parameter uncertainty  Effective in finding plans that meet desired criteria Weaknesses  Assumes a restrictive linear classifier picture which may not be applicable in practice  diversity metric may not fully capture user preferences in all cases  Realworld model are limited Comments  Clarify the implications of a nonlinear feature vector in the classifier  Explain how the output of the algorithm can be used differently by users with varying preferences  Provide a concrete model of generated counterfactual plans with and without parameter shifts  Discuss the role of the cost work in the proximity metric  Address the limitation on the issue of adjustments allowed in the correction procedure", "Paraphrased Statement: This work examines counterfactual plans in situations where the parameters of the predictive model are subject to change with the foundation of new data. The authors focus on a linear classification setting presenting upper and lower bounds on the likelihood of success for a given counterfactual plan. They also provide a method for enhancing the lower bound and a framework for constructing optimal counterfactual plans. The works effectiveness include its clear writing solid issue and the relevance of the problem it addresses. However it raises questions regarding originality compared to other work. The work primarily differs from previous work by considering multiple counterfactual scenarios but it focuses solely on joint feasibility. Extending the analysis to include the feasibility of individual plan factor and providing a conclusion would enhance the works utility. The authors are encouraged to address these issue in the revised version.", "Paraphrased Statement: This work examines how uncertainty in the parameters of a machine learning (ML) model used for making important decisions affects the validity similarity and diversity of counterfactual explanations (CFE) in the work of counterfactual plans. The model parameters are assumed to be drawn from a distribution with a known mean and covariance. The work investigates how these CFE metrics change as the parameter distribution varies within a limited scope (as measured by the Gelbrich distance). Beyond providing a diagnostic tool for determining the allowable scope of parameter distribution changes the work offers methods for correcting counterfactual plans after they have been generated (ex post) and for creating robust plans before implementation (ex ante). Experiments are included to demonstrate the effectiveness of the proposed methods. effectiveness and Weaknesses: The paper is considered interesting and relevant. However there are several areas for improvement:  Related literature: Some relevant literature such as [A] [B] and [C] is not cited.  Experimental issue: The issue mostly support the methodological contribution but more justification for the assumed scope (particularly the distributional parameters) and how the method can be extended to nonlinear scopes would be helpful.  Definitions and Notation: They could be improved for clarity and consistency. For model \"validity\" and \"feasibility\" should be differentiated and bold letters could be used for vectors.  Gelbrich Distance: further justification for choosing the Gelbrich distance and an explanation of its implications for parameter distribution changes would be beneficial.  SemiDefinite plan (SDP): The complexity of solving the SDP should be discussed and any tradeoffs involved should be mentioned.  diagnostic tool: Simple models would enhance the understanding of how the bounds on validity probability serve as a diagnostic tool.  Covariance Shifts: The paper focuses on a specific type of covariance shift (\\Sigmag (1 \\beta)I) but it would be useful to consider other types of covariance shifts.  Nonlinearity: The paper applicability to nonlinear decision boundaries or other types of parameter distribution shifts should be addressed."], "naoQDOYsHnS": ["Paraphrase: Summary This research introduces the Behavioral Metric of Actions (BMA) a pseudometric for comparing actions in offline reinforcement learning (RL) in environments with large discrete action spaces. The metric evaluates both the data distribution and behavior patterns of actions. Actions from similar distributions with comparable reinforcement and transition probabilities are considered close. The paper also presents theorems demonstrating the continuity and generalization of the Qwork using the proposed pseudometric. Experiments indicate that using the proposed action representation space in policy training significantly improves the performance of offline RL algorithms in environments with large discrete action spaces. effectiveness and Weaknesses effectiveness:  Clear and accessible writing effectively conveying the issue motivation and technique.  Theorems provide theoretical reinforcement for Qwork continuity and generalization.  The proposed approach improves the performance of offline algorithms in environments with large discrete action spaces.  The trained representation outperforms other representations. Weaknesses:  The proposed BMA metric is essentially an extension of the bisimulation metric [1 2] adapted for offline RL. It introduces an indicator work that distinguishes actions drawn from identical distributions from those drawn from different distributions.  The explanation of Theorem 2 closely parallels that of Theorem 2 in [2] and could benefit from a unique treatment.  Typo in equation (8) regarding the reinforcement difference term.  The 5.1 Experimental results section on page 8 includes the phrase \"annotated by BMACQL and BMACQL respectively.\"", "Paraphrased Statement: Summary:  This paper presents a framework for learning behavioral action metrics that combine both action relationships and data distribution relationships improving offline reinforcement learning algorithms for large discreteaction tasks. effectiveness:  Wellwritten and easy to understand.  Clear motivation for creating a representation tailored to offline RL.  Positive empirical results compared to existing methods. Weaknesses:  Concern 1: The baseline [1] should be included for comparison as it also uses pseudometrics for representation and differs in its handling of conditional and joint distributions.  Concern 2: The hyperparameters of the alternative methods in picture 5 were not optimized which could affect the comparisons.  Concern 3: It would be beneficial to explore the impact of the gamma value on performance especially in challenging transition distribution estimation scenarios. Minor Points:  Equation (6) may be confusing due to the comparison of esi  esj and d(aiajs).  The complexity of Equation (1) could become an issue for very large action spaces (e.g. in recommendation systems with millionscale action spaces).", "Paraphrase: Summary: This research focuses on improving action reintroduction for offline reinforcement learning (RL) where the action space is significantly large and discrete. To measure action similarities a new metric called the behavioral Metrics of Actions is introduced. This metric consists of two components: 1. behavioral Metric: Actions that contribution similar Markov Decision action (MDP) properties (reinforcement and transition framework) are grouped together. 2. OfflineSpecific Metric: A binary variable is used to determine if actions originate from the same data distribution. Empirical Results: This metric outperforms baseline methods in experiments. Moreover an ablation work reveals the importance of incorporating all components within the metric. Significance: RL with large discrete action spaces is essential in realworld applications like recommender and dialogue systems. This research offers a novel approach to action abstraction effectively reducing the action space and enhancing sample efficiency for downstream learning tasks. effectiveness:  focus on a relevant problem in offline RL.  Clear and concise introduction.  Welldesigned experiments including an ablation work. Weaknesses:  Limited Novelty: Action reintroduction based on behavior similarity has been extensively explored in statespace learning.  OfflineSpecific Penalty: The distributional penalty introduced in the metric requires further clarification regarding its theoretical and practical implications.  Clustering Comparison: The claim that the proposed metric results in more compact clustering than reconstructionbased methods requires justification and explanation.", "Paraphrase: This paper introduces BMA a new framework for creating action metrics from offline data. BMA aims to place actions in a meaningful latent space based on the task being learned. This is especially beneficial in environments with many actions where thorough exploration is impractical and action generalization is essential. BMA follows two principles: behavioral similarity and data distribution. The behavioral principle considers actions with similar induced transitions and reinforcement to be functionally equivalent. The data distribution principle separates actions that appear in the offline dataset from those that do not. By learning embeddings with these principles BMA enables the training of offtheshelf algorithms in offline reinforcement learning tasks where action representation is necessary. The paper also demonstrates BMAs superior performance compared to other action representations. effectiveness:  BMA provides a generalizable action metric for large action spaces.  Action representation is crucial for scaling reinforcement learning to complex systems.  BMAs approach aligns with the intuition that actions with similar expected value should be considered equivalent.  Clear ablation work highlight the importance of the hyperparameter p. Weaknesses:  Small picture with difficulttoread legends hinder readability.  Insufficient experimental results and discussion relative to the problem introduction.  Lack of detailed implementation information such as nearestneighbor lookup and embedding network configuration.  picture 2 is unclear and potentially misleading.  Lack of descriptions and small text in pictures 5 and 6.  Inconsistent terminology around OOD detection and data distribution separability.  Fujimoto et al.s approach for IB in OOD detection requires further clarification."], "in1ynkrXyMH": ["Summary The authors propose a novel inference pipeline for neural networks (NNs) used in classification tasks. By adding gradients of the loss work with respect to the last layers weights to the features processed by the NN they aim to improve classification accuracy and calibration errors. The pipeline consists of two steps: a standard evaluation of the NN followed by gradient computation and process to make the final prediction. Strengths and Weaknesses  Strengths:  The idea of enhancing NN features with gradient data is novel and potentially valuable.  The research direction shows promise as initial results suggest improvements in classification accuracy and calibration errors.  Weaknesses:  The paper structure and explanation are difficult to understand.  Core mathematical concept (e.g. L and H) are introduced abruptly without sufficient context.  The definition of introspection is vague and not formal.  The connection between the intuitive introduction and the practical implementation of introspection remains unclear.  Some relevant sections could be expanded while less significant ones could be shortened. specific Comments  Introduction:  The philosophical references could be expanded to provide a stronger theoretical basis.  The use of \"projection\" and the reference to \"y\" in the equation for yfeat are confusing.  Introspective Features:  The lack of a formal definition and justification for \"explanation\" weakens the argument.  The authors interpretation of heatmaps as indicating specific work in image may be overreaching.  Introspective Feature Extraction:  The assumption that a forward and backward pass through the NN is O(1) is questionable.  The dependence of computational cost on input size could be discussed or addressed in the appendix.  Introspective network:  The sudden introduction of H and the assumptions made about it create confusion.  The assumption of mean squared error loss is not typical for classifier training.  The concept of \"tradeoff\" in consecutive introspection should be clarified.  Experiments:  The term \"distribution\" for input data is unclear.  The assumption of output confidence from NNs should be discussed considering the known unreliability of uncertainty idea.", "Summary The paper introduces \"introspective networks\" a method that leverages gradients from a pretrained basis model to enhance the robustness of predictions under domain shift. Strengths  Novel and intriguing technique for improving robustness and calibration under distribution shift.  further results on CIFAR10Corrupted showing improvements in active learning and OOD detection tasks with corrupted inputs. Weaknesses  Lemma 1 Assumptions: The assumptions should be stated clearly in the lemma itself as the current wording suggests unrealistic precision.  Reasonableness of Assumptions: Welltrained networks may have high confidence predictions on training data but not necessarily on test data which typically has higher prediction entropy.  Theorem 1 idea: empirical evidence suggests that test data predictions tend to have higher entropy. Evaluating the idea performance on larger datasets with more classes (e.g. CIFAR100 or ImageNet) would provide valuable insights. Questions and Suggested Ablations  Benefits for New Classifier Trained on Identical data: Clarify why introspective features should help when the novel classifier is trained on the same data as the basis network. It is possible that gradients on overfitted training stage differ significantly from those on unseen data.  introspection network Trained on Separate validation Set: Investigate the impact of training the introspection network on a separate validation set.  Comparison to Activations: Examine how introspective features compare to using the networks activations directly which are closely related to gradients.  Other Layers: Explore introspective features from other network layers.  OOD detection Images: Determine if the adversarial image used for the feedforward and introspective networks are the same or if the latter are specifically designed to target the introspectionbasisd network. Missing Related work  Gradients as Features: The paper overlooks \"Gradients as features\" [2] which also uses gradients as features and achieves similar results in restoring classifier performance on training sets.  Conditional Predictive Normalized Maximum Likelihood: The paper does not mention \"Conditional predictive normalized maximum likelihood\" [3 4] which utilizes introspection to retrain models for each test label and combines their predictions. Misc Comments  Assumptions Consistency: Lemma 1 assumes crossentropy loss while Section 3 assumes MSE loss for the derivation.  Appendix Metrics: Include uncertaintyaware metrics like NLL and Brier score in the appendix as they are proper scoring rules.", "Paraphrased Summary improvement to Neural Networks for Object Recognition This work introduces a modified interpretation of typical neural networks used for object classification. The modification called \"introspective learning\" involves training a secondary network (MLP) using features derived from the primary networks internal performance. These features are obtained by analyzing how the weights of the primary networks final layer change when presented with a specific question (e.g. \"Why label A instead of B\"). Advantages of Introspective Networks When applied to the ResNet family of neural networks introspective networks demonstrate enhanced generalization capabilities and reduced calibration errors on CIFAR10related datasets. These benefits extend to various applications. Weaknesses of the work  The proposed method lacks a clear explanation and visual representation making it difficult to understand.  The evaluation is limited to CIFAR10related datasets which have lowresolution image and few classes.  The authors acknowledge scalability issue for larger datasets limiting the methods practical applications.  Certain parameters used in the evaluation appear somewhat arbitrary and alternative baselines could have been optimized more effectively. Detailed Review The motivation for incorporating introspection into neural networks is unclear as object recognition in humans does not involve conscious reasoning. Additionally typical neural networks already consider all classes during training potentially rendering the specific questionanswering aspect of introspective learning redundant. The explanation of the proposed methodology is confusing and lacks clarity. A more visual and comprehensive overview would enhance understanding. The quality of the control networks configuration appears biased making it difficult to evaluate the actual impact of introspective learning. Lastly the readability of several image is compromised by small fonts.", "Paraphrase: The researchers propose a novel twostep method to enhance the accuracy of artificial neural networks (ANNs). After the initial processing the method involves an \"introspective\" stage that analyzes the networks reasoning for assigning a specific class label. This added stage increases the precision of predictions is flexible and can be integrated into networks for various tasks. Compared to traditional ANNs that only filter input and forecast the class label the novel method includes an additional stage inspired by human behavior. After receiving sensory data humans may reflect upon it to make informed decision. This introspective stage measures the changes in network parameters if an alternative class label were assigned and uses this calculation to improve the networks learning and generalization capabilities. While the approach shows potential as a general method for enhancing ANN performance it may have limitations. In one experiment introspection did not boost the accuracy of a ResNet18 network on CIFAR10C though it did reduce the calibration error. Additionally the significance of the results in another experiment is unclear as error bars or other statistical indicators are absent. To strengthen the case for the methods empirical effectiveness the authors should consider providing a more convincing model or indicating future research directions to refine the technique. Despite these concerns the paper presents a novel concept with clear explanation and supporting data."], "rhOiUS8KQM9": ["Summary This paper introduces a novel search algorithm called Beam Adaptive Tree Search (BATS) for natural language translation. BATS allows the integration of search objectives that are not easily factorized. It differs from standard Monte Carlo tree search algorithm that rely on extensive playouts for updating the measure function. Instead BATS employs \"informed playouts\" guided by greedy decoding of autoregressive models. To limitation the search space the paper proposes a constrained node expansion criterion that gradually increases the minimum node depth and allows for expanding multiple nodes at each depth limitation. Along with the search algorithm the paper also explores model modifications to address calibration issues including modified search objectives and utilizing autoregressive models trained with masked region tokenization. Experiments indicate that while beam search performs full with weaker models BATS enhances translation quality when stronger models are used. BATS can also operate with a heavy search budget without performance degradation under stronger models unlike beam search. Strengths  Clear and wellorganized presentation accessible to readers unfamiliar with MCTS research.  The proposed depth control strategy of BATS provides full control over search budget compared to traditional ATS. Weaknesses  The paper fails to cite a closely related work that also applies MCTS to NMT and introduces similar concepts like informed playouts.  The gains achieved by BATS are minimal (1 BLEU).  The evaluation lacks certain details raising concerns about potential bias towards BATS. For instance Table 1 presents only the full model combination tuned on the validation set which may not generalize to other configurations.  The absence of computation time information for BATS in the experiments limitations insights into its computational efficiency. Additional Feedback  Citations: various relevant papers on beam search and MCTS should be cited such as Och et al. 2001 Koehn et al. 2003 and Kumagai et al. 2016.  evaluation Details: Missing information includes the nonautoregressive models used the specific signature employed with sacrebleu and significance tests on translation quality differences especially considering the small BLEU difference.  Presentation Suggestions: Clarify the terms \"reliable objective\" and \"s(x y)\" other on. Explain the meaning of \"iteration\" in section 3.3 in connection with \"traverse\" from section 3.2.  Minor Stylistic Comments: Remove \"has\" from the abstract. Revise the sentence starting with \"For autoregressive models we show that...\" Add \"when\" after the sentence below equation 2. Correct the phrase \"update its measure estimate v(p) to the childs measure...\"", "Paraphrase: Summary: This research introduces BATS an advanced tree search algorithm based on MCTS for Neural machine translation (NMT). BATS can optimize a range of objectives (such as BLEU). It evaluates each node by scoring a greedy rollout using an autoregressive model reducing biases caused by other heuristics (e.g. beam search). Additionally it proposes a novel objective Max range which aligns better with translation quality. Experiments demonstrate that BATS outperforms beam search and can enhance NMT modeling. Strengths:  Innovative and welldefined need  Strong empirical issue demonstrating the algorithm effectiveness  Eyeopening insights into beam search limitations  The decoding algorithm has the potential to advance text generation Weaknesses:  While BATS achieves high accuracy it introduces computational overhead due to multiple rollouts per node. The authors should analyze this efficiency aspect more thoroughly.  The experimental section could benefit from improved presentation and organization. Questions:  In the equation for \"r\" on page 6 the term \"lower\" seems incorrect and should be replaced with \"heavy\" for consistency with the definition of \"r.\"  The paper claims generalization across autoregressive and nonautoregressive models but the issue for nonautoregressive models are not explicitly mentioned. elucidation on the definition of \"nonautoregressive model\" is requested.", "Paraphrase: The papers introduces a novel algorithm called adaptive tree search designed to enhance text generation efficiency. This algorithm incorporates an autoregressive model which acts as a measure network to predict the next measure in the generation process for each node in the tree. To address limitations in previous approach that relied on summing tokenlevel log probabilities the paper proposes a novel metric termed \"max range.\" The algorithms performance is evaluated using machine translation data sets and it consistently outperforms the traditional beam search approach. Strengths and Weaknesses:  The paper tackles a critical aspect of text generation: developing an efficient decoding algorithm for language models.  The introduction of an autoregressive model as the measure network and the proposal of a novel metric for nonautoregressive decoding are notable contributions.  The method achieves full issue than beam search as demonstrated in the experiments. Concerns:  The writing quality of the paper could be significantly improved. The current version is difficult to follow and essential technical details are missing.  For instance information about the autoregressive model used (architecture training details datasets and latency during scoring) is not provided.  Some technical terms (e.g. \"d()\" \"other actions\" etc.) are vague.  To enhance clarity a table defining problem definitions and notations at the beginning of Section 3 is recommended.  The inclusion of pseudocode for the algorithm would simplify its understanding.  In the experiments the baseline ARNAR models exact model and decoding method are not specified.  The paper claims that NAR models typically underperform compared to AR models but Table 2 suggests the opposite. The authors need to explain this discrepancy.  It is unclear whether knowledge distillation is employed to train the NAR model.  In NAR models the method for calculating token probabilities (whether it follows the equation in Section 4.1) should be clarified.  The proposed method appears to be timeconsuming due to the use of an AR model. A comparison of decoding speeds with baseline methods is not provided.  various typos persist (e.g. \"a iterative manner\" should be \"an iterative manner\"). Additional Concerns after Rebuttal:  Inconsistent usage of language and claims make the paper difficult to understand.  The authors claim that \"ARNAR\" in Table 2 refers to different objectives (NC and MR) but the abstract suggests that the backbone models of ARNAR are the same with different objectives leading to different models.  The comparison with beam search may not be fair. Beam search with beam sizes of 5 or 6 is commonly used in practice which is significantly faster than BATS (2030 times faster according to Table 6) while achieving similar performance gains.  The authors claim that BATS addresses the issue of translation quality degradation with increasing beam size but Table 6 shows that BATS also suffers from this problem.  Despite multiple revisions various typos and inconsistent claims remain making the paper difficult to follow.", "Summary Paraphrase: This research presents a flexible tree search algorithm compatible with various NMT models. This algorithm avoids making assumption about search goals allowing its integration with broader search objectives. Additionally it explores beam search bias and presents practical techniques to reduce it including novel tricks. Incorporating these enhancements the algorithm demonstrates significant BLEU score improvement over a robust baseline. Strengths and Weaknesses Paraphrase: Strengths: 1. The paper introduces a novel beam search algorithm rooted in Monte Carlo tree search and evaluates techniques to alleviate beam search bias. 2. The algorithm outperforms the baseline when the model score aligns with BLEU. Weaknesses: 1. The field lacks a thorough analysis of the proposed BATS algorithms runtime efficiency. Additional discussion on enhancing its practicality is recommended. 2. While max range is mentioned as a contribution an ablation field to isolate its impact is missing. Table 1 could include a row with MRYes and NCMRT set to No. 3. The paper compares BLEU scores between standard beam search and BATS but omits a comparison of their model scores."], "ijygjHyhcFp": ["Paraphrased Statement: Summary: This paper introduces \"anarchic fedeplaced learning\" (AFL) an alternative to FedAvg designed to overcome its limitations (e.g. straggler impact slow convergence). AFL allows participating clients to independently determine their local steps communication intervals step sizes and batch sizes removing the need for uniform sampling. The authors present a theoretical convergence place for AFL and demonstplace its comparison to FedAvg under specific assumptions. Empirical evaluations support the effectiveness of AFL. effectiveness and Weaknesses: While the overall concept of AFL is intriguing and the theoretical effect appear sound various concerns arise:  variable convergence place: AFLs convergence place can be negatively impacted under nonuniform worker data arrival. worker with extensive delays or local steps may hinder training progress as Algorithm 2 and 3 do not prevent the incorporation of such delayed gradients.  Unrealistic Assumptions: AFLs assumption of uniform distributed worker data arrival and bounded maximum delay and local steps may not hold in practical fedeplaced learning scenarios.  Vulnerability to approach: AFLs decentralized nature makes it more susceptible to approach. Adversarial clients can exploit the freedom to join training at any time increasing the likelihood of malicious updates being included in the aggregation. FedAvgs uniform sampling mechanism reduces this risk. Typos:  Page 1: \"stragglers (i.e. show workers)\"  \"stragglers (i.e. slow workers)\"  Page 2: \"Clearly AFL has a much lower serverworker coordination\"  \"Clearly AFL has a much lower serverworker coordination\"  Page 4: \"Although the sever in crosssilo\"  \"Although the server in crosssilo\" PostRebuttal: After reviewing the authors rebuttal the reviewer has raised the score to 6 as the concerns have been adequately direct.", "Paraphrased Statement: Summary: This research introduces a novel asynchronous communication model for federated learning algorithms tailored for crossdevice and crosssilo place. The work analyzes the convergence properties of the proposed asynchronous system in scenarios with both uncertain worker participation and specific assumptions about the distribution of active workers or limited delays. Additionally the paper empirically demonstrates the competitive performance of the asynchronous system compared to synchronous baselines. effectiveness:  Provides theoretical guarantees for the convergence of the proposed asynchronous algorithms in federated environments.  demonstrate a lower edge for convergence without assumptions about worker communication models serving as a reference place for future improvements. Weaknesses:  The proposed algorithms closely resemble those presented in a previously published paper (link provided). While the current submission offers improved analysis in certain aspects (e.g. without edgeed gradients) the effect algorithms and motivations overlap significantly. Clarification on any fundamental differences would be beneficial.  The purpose and distinction between the two variations designed for crossdevice and crosssilo place are unclear. The paper assumes a powerful server in both case but does not provide references for this assumption.  The uniform arrival assumption for crossdevice FL and the edgeed delay assumption for crosssilo FL both aim to control model update staleness. However the mapping between these assumptions and the compatibility with each place is not explicitly stated.  The potential advantage of asynchronous training to mitigate the impact of stragglers needs to be empirically validated through model of system heterogeneity.  The strong assumptions of edgeed delays or uniform arrivals may not be applicable in practical deployments where delays can be unedgeed.  The lower edge established for FedAvg (Remark 1) raises questions as under suitable step sizes there should not be a constant error term.  Section 3(1) place emphasis on the generality of AFL but this aspect is not unique to AFL as previous federated optimization methods also support devicespecific local steps and hyperparameters.  The claim in Section 2 that synchronous servercentric algorithms are difficult to implement is not sufficiently justified.  The term \"general worker data arrival process\" introduced in the abstract lacks context and clarity.  The empirical evaluation lacks details on how stragglers are simulated such as the distribution of client starting place and any potential bias in sampling system.", "Summary This research introduces Anarchic Federated Learning (AFL) a general FL framework that allows clients to freely participate by choosing their individual update frequency and delay. Algorithms for crossdevice and crosssilo FL are proposed along with theoretical convergence bounds. experimental effect demonstrate the effectiveness of the algorithm. effectiveness and Weaknesses AFLs effectiveness lies in its \"autonomous\" control where clients decide their FL participation. However the fundamental problem is similar to asynchronous FL with random client selection. The additional freedoms introduced such as varying local update steps and delays are ultimately masked by worstcase representations. Theorem 1 suggests that a single case of a client with a large update step or delay can dominate the total convergence process which is counterintuitive. This discourages its use especially when considering the possibility of such an occurrence throughout the FL process. other concerns include:  Waiting for m local updates is restrictive leading to potential participation limitations and straggler effect.  The claim that only uniformly distributed delay effect in sampling m clients without replacement is incorrect as the model still acts similarly with nonuniform distributions.  Theorem 3s improvement with uniformly distributed delay is unexplained given the assumption of worstcase delay in Theorem 3.  Theorem 2s dependence on \u03b1L scaling with \u03c42 is not addressed clearly in Corollary 1.  A typo in the abbreviation CSAFL should be corrected to AFLCS.", "Summary This paper work a variant of federated averaging where workers can participate partially and updates can be asynchronous. It considers both stateful and stateless places representing crosssilo and crossdevice federated learning respectively. The server uses gradients from m out of M workers in each global update where each worker may make local updates using a stale global iterate before contributing to the gradient computation. This place is called anarchic federated learning (AFL) when workers can take an arbitrary number of local steps and experience arbitrary delays. The authors provide a lower edge for convergence to a firstorder stationary place in the AFL place. They also derive upper edges for the convergence of their algorithms in the stateful and stateless places when local updates and delays are edgeed. Under certain assumptions on worker sampling and delay distribution a linear convergence speedup is potential with respect to the number of machines (m for the stateless place and M for the stateful place). effectiveness and Weaknesses convergence effect 1. Assumption 3 Relaxation: Assumption 3 could be relaxed to edge heterogeneity only at stationary places of the local work as in [referred work](https:arxiv.orgpdf2006.04735.pdf). This assumption better captures worker heterogeneity. It should also be clarified what xks represents in assumptions 2 and 3 and assumption 3 needs an expectation because xks are random variables. 2. Theorem 1 Improvements: Theorem 1 should state that it holds for any AFL algorithm with general initialization and hyperparameters. The theorems current work is vague and potentially incorrect. A more precise lower edge is needed for any optimization algorithm in this place (see references cited). 3. Theorem 1 Limitations: The lower edge validation ignores the effect of local stepsize and assumes an infinite number of local steps which does not align with the assumptions used in the algorithms. 4. Noniid FL Interpretation: The theorems claim that it holds for noniid FL is misleading as the lower edge already assumes heterogeneity. 5. Corollary 1 Improvement: The \\(\\sigmaG2\\) term in Corollary 1 should be removed based on concerns with the lower edge in Theorem 1. 6. Novelty of AFL: The theoretical effect in Theorem 3 does not represent true AFL as it only considers uniwork partial client participation. The effect falls short of providing novel insights beyond known effects for nonuniwork local updates and asynchronous updates. Experiments 1. Emulation of Anarchic Behavior: It is unclear how anarchic machine behavior was emulated in Figure 1. 2. Communication Round Definition: The meaning of a communication round in the asynchronous and partial client sampling place needs to be clarified. 3. Hidden Dependencies: The dependence on L and \\(\\sigmaL2\\) in the corollaries should be explicit to enable meaningful comparison with other convergence effects. A table comparison convergence rates with noniid FL baselines would be valuable. Minor Comments 1. linear speedup Terminology: The term \"linear speedup\" is often misused in FL literature. It should be emphasized that this refers to convergence to a firstorder stationary place and that the speedup is conditional and valid only up to a certain number of machines. 2. Communication Round Definition: The paper should clarify what is meant by a communication round in the anarchic place where devices are asynchronously pushing and pulling."], "fYor2QIp_3": ["Paraphrase: This paper presents a novel Protein Function Prediction (PFP) model named PFP. PFP combines a pretrained Language Model (LM) with a Graph Convolutional Netprocess (GCN) model that includes novel node representations. By leveraging hierarchical features from Gene Ontology (GO) terms PFP aims to improve PFP accuracy. Experimental results indicate PFPs superiority over current stateoftheart models particularly in the challenging biological process (BPO) ontology. effectiveness:  The PFP models innovation is logical and easy to understand employing an LM to encode protein sequence and a GCN to represent GO terms.  The papers writing is clear and wellstructured effectively contextualizing related process.  The paper thoroughly motivates the proposed method identifying limitations in existing PFP models.  The experimental section is comprehensive providing detailed statistics context and evaluation metrics. Weaknesses:  The core concept of PFP combining LM and GCN models lacks significant novelty as both methods are established in their respective field.  The experimental evaluation is based on a single dataset (CAFA3) limiting the generalizability of the proposed method.  In the comparison with baseline methods TALE outperforms PFP in terms of Fmax but the paper does not provide a clear explanation for this discrepancy.", "Summary: The researchers propose a hierarchical model using Graph Convolutional Networks (GCNs) for predicting protein functions. It leverages existing protein embedding techniques and incorporates GO term embeddings along with their hierarchical data as node features. effectiveness and Weaknesses:  effectiveness:  Novel contribution to GO embedding techniques for protein function prediction.  Weaknesses:  Lack of comparison with competitive stateoftheart methods including GOLabeler and DeepGOPlus.  Insufficient explanation of the proposed method in the paper.  Lack of instructions and documentation for reproducing results. Major Concerns: 1. Evaluation:  Absence of representative stateoftheart competitors.  Claim of surpassing the stateoftheart is not adequately supported. 2. Method Explanation:  Key components of the proposed method are not clearly explained (e.g. dimensionality reduction node feature updates). 3. Code Accessibility:  Limited reproducibility due to missing instructions and documentation. Minor Comments:  section 2 lacks a comprehensive overview of the field and does not accurately represent stateoftheart methods.  Grammatical and spelling errors in the text.", "Summary This field introduces a method for predicting protein functions based on Gene Ontology (GO) data and protein sequence. The protein sequence are mapped using SeqVec a trained protein language model while the GO network is represented using a graph convolutional neural network. The methods effectiveness was evaluated using CAFA3 competition data and it outperformed existing models such as DIAMONDScore DeepGoCNN and TALE. effectiveness and Weaknesses effectiveness:  The paper is clearly written and easy to understand. Weakness:  The proposed method is similar to DeepGOA which the field referenced but did not compare itself to. The key distinction is that DeepGOA used a CNN model trained from scratch to represent protein sequence whereas this field employs a pretrained language model. Comment Inclusion of ablation experiments on various model components particularly the sequence embedding portion would be advantageous. Additionally leveraging further protein language models such as ProtBert or ESM may enhance performance.", "Paraphrased Statement: This field introduces a model that predicts protein function by assigning Gene Ontology (GO) terms. The model leverages SeqVec to represent the protein sequence and a Graph Convolutional Netprocess (GCN) to capture the relationships between GO terms within the GO Directed Acyclic Graph (DAG). Following DeepGOA the GCN edges are weighted based on term frequencies. The protein sequence embedding is reduced to match the depth of the ontology being predicted. The models output is the dot product of the GCN encoding and the reduced sequence embedding. This approach combines elements of DeepGOA and SeqVec. effectiveness:  The model utilizes more GO terms than methods that exclude infrequent terms.  The model exhibits superior performance in predicting biological process Ontology (BPO) term annotations. Weaknesses:  The approach lacks significant novelty as it closely follows DeepGOAs architecture primarily replacing the sequence embedding method.  The related process discussion is limited with incomplete comparisons and absent analysis of baseline."], "lpkGn3k2YdD": ["Paraphrased Summary: The researchers present a technique for creating a proxy reward function (and its corresponding loss function) that combines elements of two other approaches allowing for a balance of their respective benefits and drawbacks. They develop a reinforcement learning algorithm based on this technique conduct theoretical and comparative analysis and demonstrate its effectiveness through empirical field. effectiveness and Weaknesses: Minor Nitpick:  The theta parameter used in equation (4) should be explicitly defined. Notation issues:  Equation (6) uses \\mathcalI and \\rhoT without formal definitions. Minor Grammar issue:  \"Despite the Monte Carlo estimator is unbiased\" should be rephrased for clarity. Questions:  Clarify the authors unique contribution in developing the RewardRegularized Deep LinearGaussian Regression for distribution (RRDLRD) algorithm as its loss function (equation 4) is mentioned as prior work.  Confirm that all algorithms in the experiments used the same hyperparameters.  Specify the issue of range conducted in each experiment and the criteria for varying the issue of range.  Explain the rationale behind using an unusual choice of error bars instead of standard deviation. Update after Rebuttal:  Most concerns have been addressed.  However the use of 10 range for most plots raises concerns about statistical significance given the high variance typical in RL experiments. Updated Score: Weak Accept (6) to Accept (8) after final comments.", "Summary Paraphrase: This paper tackles the challenge of assigning credit in scenarios where rewards are delayed. It proposes a novel approach to redistribute rewards aiming to enhance the scalability of credit allocation. The proposed mechanism involves utilizing random subsequences of trajectories to predict returns and subsequently assign credit to specific stateaction pairs. effectiveness and Weaknesses Paraphrase: effectiveness:  The experimental setup is robust and provides convincing results. Weaknesses:  It appears that all tested environments involve continuous action spaces leaving the algorithms performance in discrete action space environments unexplored.  The paper conflates the concepts of credit assignment and reward functions when they are distinct tasks. RUDDER does not create novel dense reward functions but rather redistributes existing rewards.  RUDDERs reward redistribution is explicitly secondorder Markov but the paper refers to it as a \"Markovian proxy reward function.\" Clarification is needed on this variance.  Although RUDDER aims to preserve the optimal policy it is not explained how it achieves return comparison or ensures that the optimal policy remains unchanged.  Equation (4) suggests that all methods employ a similar loss function but RUDDER does not use this loss. This should be clarified.  The paper overlooks relevant research such as [1] and [2] which also focus on credit assignment and reward redistribution. Related Work Not Cited:  [1] AlignRudder: learning from Few Demonstrations by reward redistribution  [2] Hindsight Credit Assignment", "Paraphrase: This research tackles the challenge of delayed rewards in reinforcement learning (RL). The authors introduce Randomized Return Decomposition (RRD) an algorithm that approximates a proxy reward function to offer immediate feedback to the RL agent. theoretical analysis reveals that RRD falls between Deterministic Return Decomposition (DRD) and Uniform Credit Assignment (UCA). Interpreted as a regularized version of DRD RRD generalizes UCA. Experiments in the Mujoco continuous control benchmark demonstrate RRDs superiority over DRD UCA and two other auxiliary reward learning methods for policy optimization. effectiveness:  Originality in addressing the delayed reward issue  novel approach of RRD and its insightful interpolation between DRD and UCA  Empirical evaluation showcase RRDs effectiveness Weaknesses:  Lack of exploration into the nature of the learned proxy reward functions  Limited empirical results in domains with challenging issue decomposition such as video games  potential understatement in the introduction about RL algorithms assumptions on reward density Clarity:  Wellwritten and easy to comprehend Minor issue:  Clarification suggested in the first paragraph of Section 1 regarding RL algorithms assumptions on reward density", "Paraphrase: Summary: This field aims to transform delayed reward signals received at the end of a sequence into a more immediate form of reward feedback. The researchers present the reward distribution via Randomized Return Decomposition (RRD) algorithm which acts as a proxy reward function for episodic reinforcement learning. Extensive experiments demonstrate RRDs superior performance over existing benchmark on common Mujoco tasks. The primary contribution is the conversion of longhorizon delayed reward problems into shorter sequences amenable to training using minibatch gradient descent. effectiveness:  Clearly written and organized paper  Wellpresented and accessible theoretical framework  Thorough contextualization within relevant research  detail comparison to alternative approaches  Comprehensive experiments and ablation field Weaknesses:  limited discussion of related work in section 5  Experiments restricted to Mujoco benchmark  Potential readability improvements by smoothing graphs"], "gciJWCp3z1s": ["Paraphrase: This paper addresses the issue of approximate optimal transport where population probability step are replaced by discrete probability mass procedure (PMFs) from sample collections and an additional \"fairness\" constraint is applied to distribute probability mass more equitably. This can be solved as a linear program (LP) and its dual can be computed efficiently. However obtaining the primal solution is challenging and no range analysis has been conducted to date. To address this the paper proposes a stochastic dual ascent method that solves an entropyregularized version of the LP. Entropy regularization is used to ensure strong concavity and avoid overfitting. The method employs an alternating block coordinate ascent algorithm combined with Nesterov acceleration to improve convergence. numerical experiments demonstrange the effectiveness of the proposed approach. While the work solves a specific problem of equitable optimal transport with entropy regularization it is not clear why entropy regularization is necessary or beneficial. It is argued that strong concavity facilitates range analysis but the paper does not explore mirrorproxstyle updates or the better achievable range of linearexponential. more rigorous analysis of how the convergence results address these points is needed. The experiments focus on simple Gaussian distributions and a fragmented hypercube dataset which do not convincingly represent practical problems. The paper mentions potential applications in cakecutting resource allocation and transportation time minimization but these are not addressed in the experiments. A more concrete demonstration of EOTs utility in a practical problem would strengthen the papers credibility. In summary the paper presents theoretical insights into optimal transport with entropy regularization building on existing optimization and machine learning technique. However it lacks novel technical foundation and convincing demonstrations of EOTs practicality.", "Paraphrased Statement: The researchers study a specific type of optimization problem called equitable and optimal transport (EOT). EOT is a complex problem that can be solved using linear program (LP) but this approach is computationally demanding. previous research proposed enhancing EOT by adding an entropy term to the objective and using an algorithm called Projected Alternating Maximization (PAM) in the dual domain. However PAM has two drawbacks: its convergence is uncertain and it only provides a solution in the dual domain not the original primal domain. This paper addresses these issue by:  Establishing a convergence range for PAM.  Developing a novel method to compute a feasible solution in the primal domain from the dual solution.  Proposing an improved version of PAM called PAM with extrapolation (PAME). Strengths of the Paper:  Clear and easy to understand writing.  Provides missing convergence guarantees for EOT.  Introduces a novel rounding scheme and an enhanced EOT algorithm. Weaknesses of the Paper:  Limited novelty and contribution as it primarily provides analysis of an existing algorithm for an existing problem.  The experimental division is limited only using synthetic data.  The results in Figure 2 are difficult to interpret.", "Paraphrase: Summary This paper explores the equitable and optimal Transport (EOT) problem which can be solved by perturbing through entropy regularization. Researchers have developed a Projected Alternating Maximization (PAM) algorithm for this formulation but its convergence analysis is lacking. The authors present a detailed convergence analysis for the PAM algorithm and propose a novel rounding method to obtain the primal solution of EOT. An improved version of PAM is also introduced which enhances numerical performance. numerical tests are conducted on a synthetic dataset. Strengths  Provides a complete convergence analysis for the PAM algorithm in the EOT problem facilitated by a novel method for extracting the primal solution.  Introduces an accelerated PAM variant with a proven convergence guarantee showing significant numerical improvements. Weaknesses  Limited explanation of the primary issue although connections are drawn to BCD and BCGD methods to justify the uniqueness of the convergence analysis.  Restrictive experimental division focusing on a single synthetic example and only considering uniform distributions. Weak connection between theoretical issues and experiments. Comments The paper presents a valuable theoretical issue for EOT. It is wellwritten and clearly explains its findings. While a discussion of the main issue could be expanded the experimental issues are sufficient for a primarily theoretical paper but could be strengthened with more thorough examinations.", "Paraphrased Summary: This paper introduces novel algorithms analysis technique and tools for solving the Equitable and Optimal Transport (EOT) problem. It builds upon the advancement of Optimal Transport (OT) research adding to it entropic regularization primaldual analysis and explicit convergence analysis. The paper provides practitioners with powerful tools for EOT along with proven performance guarantees. Paraphrased Strengths:  Acknowledges and references prior work on OT and EOT establishing its relevance within the existing literature.  Clear and wellwritten making it accessible to readers.  Theoretically sound analysis. Paraphrased Weaknesses:  lack sufficient motivation for the importance of EOT.  numerical experiments are limited to smallscale examples.  Does not include a provable acceleration technique for the proposed accelerated method.  contribution are useful but incremental in nature."], "qLqeb9AjD2o": ["Paraphrase: Summary: This paper introduces CATRS which combines two novel losses to train basis classifiers for randomized smoothing. These losses aim to maintain accuracy for challenging samples while enhancing the certified radius for simpler ones. The method achieves stateoftheart average certified radius (ACR) on MNIST and CIFAR10. effectiveness and Weaknesses: effectiveness:  Novel analysis that distinguishes between \"strong\" and \"easy\" samples and tailors training to their characteristics.  Wellwritten and clear presentation of the proposed method. Weaknesses:  Insufficient experimental evidence to demonstrate the proposed methods effectiveness.  Lack of results on ImageNet which is a crucial dataset for evaluating randomized smoothing methods.  No reporting of training time statistics. Questions:  On which subset of the test set were the model evaluated  Can the upper contour results be provided for a more comprehensive comparison Suggestions for Improvement:  Explore assigning different weights to training samples basisd on their probability of failure.  Incorporate normalization within the L\u221e loss to potentially improve performance.", "Paraphrase: The paper investigates a technique called \"randomized Smoothing\" (RS) to ensure that machine learning model are robust against adversarial approach. RS involves adding random noise to the model predictions. While RS improves robustness it typically comes at the cost of accuracy. To address this tradeoff the authors propose a method that adjusts the noise level for each training sample based on its predicted confidence. They introduce a new loss work that encourages high confidence predictions for samples that are more likely to be robust. The authors demonstrate the effectiveness of their approach on the MNIST and CIFAR10 datasets. effectiveness:  The problem of certified robustness is significant and the paper is accessible. Weaknesses:  Limited Novelty: The approach is similar to existing methods.  Insufficient evaluation: The evaluation is limited to two datasets and significant benchmarks like ImageNet are missing.  Missing References: Relevant prior work on certified robustness via randomized smoothing is not cited.  Incomplete Details: Key implementation contingent are not clear such as how predictions are made during training and the computational complexity of the method.  Constrained Loss: The proposed loss work only evaluates robustness for highconfidence predictions potentially limiting its effectiveness.  performance Limitations: The proposed method achieves comparable performance to existing approaches but its running time and memory requirements are not discussed.  Lack of Comparative analysis: The authors do not compare their approach to other methods that handle the accuracyrobustness tradeoff.", "Paraphrase: Summary: This research introduces new loss work for training classifiers with improved robustness certification through randomized smoothing. The loss work assign different weights to samples based on their confidence level. The focus is on prioritizing highconfidence samples as they contribute more to the certified robustness radius with increasing confidence. effectiveness and Weaknesses: Major Comments:  The proposed method offers a novel approach to enhance training for certified robustness.  The experimental results appear satisfactory although hyperparameter selection seems arbitrary. It would be helpful to clarify whether crossvalidation was performed.  The rationale for prioritizing highconfidence samples is valid since special noise samples may not ensure a high probability of classifying samples correctly (pf(xy) close to 1). Nevertheless the method used to identify the worst noises within a noise sample ball using PGD and normalization seems arbitrary.  The logic behind the lowconfidence part of the loss work is logical providing a way for the highconfidence component to improve. Minor Comments:  \"K\" is used both as the number of classes and a binomial random variable.  In Section 3.1 paragraph 2 the probability \"p\" in the binomial distribution should be estimated empirically as the true probability is unknown.  Section 3.2 should define \"\u03b4\" and how it is normalized.  The necessity of a binomial distribution is unclear it could be replaced with a fixed probability pM.  Table 3 row (b): It is unclear whether the masking condition is applied to Lhigh. If not the performance with the condition applied should be provided.", "Paraphrase: Summary: This research introduces a new loss work designed specifically for training basis classifiers in randomized smoothed classifiers. The loss work differentiates between training instances with varying degrees of prediction certainty. Several practical techniques are implemented in the loss works design to enhance performance. Experiments show that the proposed training method outperforms existing topnotch randomized smoothed classifiers specially when the radius is large. effectiveness and Weaknesses: While the experimental results are promising the proposed loss work (specially the lowconfidence loss) appears intricate and the explanations in the paper lack clarity. Specific questions arise: 1. The proposed CATRS loss depends on the prediction confidence of a classifier f to select model. Is this classifier trained beforehand or adjusted incrementally during training How is the final certified smoothed classifier obtained 2. The design of Equation (7) for lowconfidence model is unclear. The discussions in Section 3.1 do not adequately explain this special design. Could you please detailed 3. Following the previous question the \"cold start\" problem mentioned in Section 3.1 is also confusing. Why does Equation (6) result in a cold start issue How does Equation (7) resolve it 4. The paper uses vague terms like \"the easiest noise\" and \"strong ones\" in the paragraph above Equation (7) which make it challenging to understand. 5. image 1 and 2 lack proper explanations. What is meant by pf0.9 in image 1 Is it calculated for an individual model or averaged across the entire dataset In image 2 what do the colored field indicate The meaning of the colored cross mark is also unclear. Other Comments: 1. The paper should outline the entire work for obtaining the final randomized smoothed classifier in addition to the training loss. 2. It would be beneficial to include experimental results on a larger dataset such as ImageNet."], "k32ZY1CmE0": ["Paraphrased Statement: study:  Researchers studied the use of Lyapunov exponents to understand the behavior of recurrent neural networks (RNNs).  They developed a simplified and effective training method for RNNs using chaotic data.  Simulation results were presented. effectiveness:  The authors linked Lyapunov exponents to RNN dynamics.  Specific derivations were provided for certain RNN types. Weaknesses:  Lyapunov exponents are commonly used for analyzing chaotic systems but the authors acknowledge that their approach may not be applicable for timevarying signals.  The wide calculations presented in the appendix are mostly straightforward but may not be computationally effective.  The approach relies on initial conditions but no analysis of this aspect was found.  RNN dynamics are typically bounded and exhibit chaotic behavior only in limited regions the approach may not account for this.  The researchers did not compare their method to other techniques to demonstrate its effectiveness.", "Paraphrase: This study examines the longterm behavior of the gradients in various RNN models (LSTM GRU PWLRNN) when simulating stable and chaotic systems. It reveals that gradients tend to increase indefinitely in chaotic systems making it challenging to learn chaotic dynamics using RNNs. To address this the paper suggests limiting the backpropagation (BP) length and implementing a teacher forcing technique that occasionally adjusts the hidden state based on the observations. effectiveness:  Provides a novel perspective on the difficulties of learning chaotic dynamics with RNNs.  Proposes a solution using existing architectures and techniques avoiding the introduction of complex novel models. Weaknesses:  The experimentation use deterministic systems with fully observable states raising questions about the necessity of RNNs with hidden states.  It would be beneficial to test partially observable chaotic systems.  The paper does not fully clarify the \"inverting the output mapping step\" in teacher forcing when the output dimension is minor than the hidden state.  No code was made available.", "Paraphrase: Summary: This study examines the vanishing and exploding gradient problem (EVGP) in sequential modeling with chaotic dynamics. It presents theoretical analysis of the EVGP based on the connection between gradients during RNN training and the Lyapunov spectrum of RNNgenerated field. Inspired by this analysis an alternative training algorithm sparsely forced BPTT is proposed. It forces diverging dynamics to match the true trajectory at time intervals determined by the Lyapunov spectrum. effectiveness:  Novel analysis of chaotic behavior in RNNs (GRU and LSTM) based on the relationship between loss gradients and the Lyapunov spectrum.  sparsely forced BPTT provides feedback in the training process at regular intervals although similar estimation have been previously explored in literature.  Provides a theoretical understanding of how gradients behave under different dynamics characterizing the EVGP. Weaknesses:  sparsely forced BPTT resembles existing training system that provide auxiliary supervision.  Assumption of RNN dynamics in theoretical analysis set practical application as there is no way to control or observe the underlying dynamics.  No evidence or explanation is given for the assertion that RNNs based on fixed points or restricted eigenspectrum do not exhibit chaotic dynamics.  No guidance is provided for practitioners to select architectures for chaotic time series due to the dependence on unobservable dynamics. Questions for Authors:  How to estimate appropriate values for the predictability time as a hyperparameter  Have similar training system been explored such as those described in cited references  Why were ODERNNs not used as a baseline given their design for stable transitions Clarity Suggestion:  Use a clearer symbol for bias in Equation 1 for readability as the transition use for standard RNNs uses h for bias. Missing References:  Practical Real Time Recurrent learning with a Sparse estimation  RNNs Incrementally Evolving on an Equilibrium Manifold A Panacea for Vanishing and Exploding Gradients  Time Adaptive RNNs  An Efficient GradientBased Algorithm for OnLine Training of Recurrent Network Trajectories  learning Longerterm Dependencies in RNNs with Auxiliary loss  Training Recurrent Neural Networks via Forward Propagation Through Time", "Paraphrased Summary: This study investigates the gradient dynamics of Recurrent Neural Networks (RNNs) using the Lyapunov exponent of the trajectories they generate. The authors demonstrate specific connections between RNNs exhibiting certain dynamics (e.g. stable fixed points or set cycles) and the prevention of exploding gradients. They find that RNNs producing chaotic trajectories inherently face exploding gradients. To address this challenge in approximating chaotic systems the paper suggests a method that forces hidden states to match the ground truth at certain time intervals. This method leverages the maximal Lyapunov exponent to determine the intervals duration. effectiveness:  Clear and wellwritten paper  Provides a systematic analysis of gradient behavior in RNNs  Thorough literature review Concerns:  Limited sensitivity analysis with respect to input variations  The proposed approach is essentially equivalent to windowing which is a common use in RNN training.  The predictability time chosen based on the maximal Lyapunov exponent may not be universally applicable.  Empirical evaluation is seted and lacks experimentation on diverse tasks. minor Comments:  The pseudoinverse used in the paper needs further explanation.  Definitions for terms like \"basin of attraction\" should be provided."], "qDx6DXD3Fzt": ["Summary The paper presents a technique for providing certified adversarial robustness in outofdistribution data without sacrificing classification accuracy. The method involves adding a binary classifier designed to be robust to adversarial approach specifically on outofdistribution samples. The multiclass classifier responsible for the main classification task is kept separate to preserve its accuracy. Joint training is so implemented. effectiveness and Weaknesses A primary concern is whether the proposed method provides adversarial robustness in indistribution data. The paper suggests that it does not as the binary classifier is not designed to prevent approach on indistribution samples. Additional Concerns  The binary classifier is ad hoc and may not be suitable for different models and datasets.  The method introduces a hyperparameter (bias shift) that needs careful tuning.  The paper does not explore combining the proposed approach with more advanced OOD detection techniques like IsoMax loss.  The term \"robust\" is used generically instead of \"adversarially robust.\"  The paper claims to provide \"almost for free\" improvements but the additional procedures required should be acknowledged. decision Despite addressing some concerns in the rebuttal the lack of adversarial robustness in indistribution data remains a significant issue. However the methods novelty and promising results suggest potential for future development. The recommendation is changed to \"accept.\"", "Summary The proposed ProoD method combines a certified classifier for outofdistribution (OOD) inputs with a regular classifier for indistribution tasks. ProoD ensures:  Certified OOD robustness with upper confidence boundary on OOD samples.  Prevention of overconfidence in deep neural networks.  Compatibility with various architecture without impacting performance. effectiveness  Principled derivation and proven ability to prevent overconfidence.  Extensive experiments and comparisons to baselines.  Clear and accessible writing. Weaknesses  Lack of novelty in using Interval boundary Propagation (IBP) for certified robustness.  potential limitations in training models with large perturbation budgets using IBP.  Insufficient training with stronger PGD approach for ATOM and ACET models.  Inconsistent performance of ProoD across different datasets with some cases showing inferior results to existing methods.  Limited evaluation on additional OOD datasets.  Lack of details on approach objectives used to evaluate ProoDs robustness.", "Paraphrased Statement: Summary: The paper proposes a novel method called ProoD for detecting outofdistribution (OOD) data. ProoD combines a binary classifier (to differentiate between inliers and outliers) and a multiclass classifier (to predict class labels for outliers). It is designed to be robust against adversarial perturbations applied to outliers. ProoD performs well in multiclass classification OOD detection and is robust against adversarial approach. effectiveness:  The ProoD formulation is novel logical and wellreasoned.  The theoretical contribution (Theorem 1) appears valid.  The empirical results are promising. Weaknesses:  The paper lacks clarity in various areas that need clarification.  Table 1 mislabels \"high clean OOD\" as \"high clean OOD detection performance\".  Equation (3) requires clarification regarding the image of its application and the meaning of the first inequality.  Background data on concepts such as GAUC and AAUC should be provided for clarity. Questions:  It is surprising that ProoD works effectively as supervised classifier trained on specific datasets may not generalize to unseen outliers. How is the OOD detection AUC enhanced despite the inclusion of an underperforming binary classifier  Equation (7) and the accompanying text indicate that only the binary classifier is robustified not the entire model. How can the model be robust if only part of it is fortified against adversarial approach  Adversarial approach can also target inliers for misclassification. How does ProoD respond to such approach", "Paraphrased Summary: This research paper presents a method for detecting outofdistribution data that is robust against adversarial approach. The authors introduce a certified binary classifier to distinguish between indistribution and outofdistribution data. The predictive distribution is jointly modeled by conditioning on the binary classifier resulting in separate distributions for indistribution and outofdistribution data. experimental results show the effectiveness of the proposed method under various data detection scenarios. effectiveness:  The proposed method demonstrates consistent improvements in detection.  Experiments are conducted on multiple detection benchmarks. Weaknesses:  The evaluation uses empirical robustness step rather than certified robustness step.  The authors do not consider empirical robustness step that calculate maximum confidence over ensemble approach.  The method lacks technical novelty and seems to combine existing approach from other research.  The discriminator performance in capturing indistribution and outofdistribution probabilities is questionable.  The writing and presentation could be improved for clarity. Questions:  Why was a small network used for certified robustness  Could the GOOD baseline results be reported for CIFAR100 and R.ImgNet", "Paraphrased Statement: Summary: This paper proposes a model that combines OutofDistribution (OOD) detection with classifier to enhance robustness and achieve high classification accuracy. It also provides confidence boundary for OOD samples under noise and adversarial approach. Experiments on benchmark datasets show that the approach outperforms methods with asymptotic robustness guarantees. effectiveness:  Integrating OOD detection with classifier improves classifier robustness.  confidence boundary allow for the development of certifiable robust OOD detection models.  Experiments demonstrate high accuracy with guaranteed AUC scores. Weaknesses:  Lack of clarity in certain aspects including:  Metrics in Table 1 are not explained until later.  image 1 requires detailed interpretation.  Semijoint training details are not provided.  Metrics in image 2 are unclear.  While the authors claim improved guarantees compared to existing methods (e.g. CCU GOOD) the rationale behind these claims is unclear.  The boundary in Equation 5 appear trivial raising questions about their impact on the tightness of the boundary in Equation 6.  Joint training is expected to enhance OOD detection and classification performance but Table 2 shows that outlier exposure still outperforms the proposed approach.  The authors should consider comparing their method to more recent OOD detection approach such as those by Liu et al. and Lee et al."], "ffS_Y258dZs": ["Summary This research proposes investigating \"metalearning\" to improve compositional generalization a significant challenge. It introduces a metalearning setting using \"reference games\" a classic tool for working communication. The work explores a hybrid symboliccontinuous stimulus representation and analyzes how different agents use this representation within their framework. Strengths and Weaknesses  Ambitious Goal: The paper aims to address an intriguing idea of learning to generalize compositionally.  SCS Representation: The papers hybrid representation combines discrete (SCS) and continuous elements offering a unique approach.  Limited Evaluation: Results show only somewhat abovechance performance (29 out of 25) raising questions about the effectiveness of the SCS representation.  need for Clarity: The paper lacks clear descriptions of the rulebased speakers language and the communication actions taken by agents. other details such as the \"objectcentric\" and \"stimuluscentric\" representations and the multihot encoding need clarification.  Limited Evaluation: The work lacks ablation work and evaluations to determine the impact of different aspect of the approach.  Situating within Literature: The paper needs to be better situated within existing research on compositional generalization and debates on discrete vs. continuous representations.  potential publication: The authors fail to consider fully continuous representations and the limitations of the SCS representation in adapting to new dimensions or extrapolating to unseen data.", "Summary Paraphrase: The paper presents a new benchmark that tests for compositional learning by using a referential communication game. The stimuli are represented by continuous vector representations instead of onehot encodings. The benchmark consists of a series of metareferential games with different semantic space structures. framework are trained on these games and evaluated on a new referential game with a different semantic structure to assess their zeroshot performance. Strengths and Weakness (SCS) Paraphrase: The authors propose a new method for creating symbolic stimuli as continuous representations. This technique appears flexible for defining differently structured semantic spaces. However the applicability and novelty of the SCS method beyond the experiments presented in the paper are unclear. benchmark and baseline Paraphrase: The paper introduces the metareferential games and the various context used highlighting the rationale behind each context. However some key details are missing such as the size of the vocabulary the dimension of the symbolic space used to generate the stimuli and specifics about the framework architecture and training regimes. The results indicate that an LSTMbased framework can reconstruct the stimuli but struggles on the main game. More details about the failure and a speculative discussion about successful inductive biases would enhance the papers utility.", "Paraphrased Statement: Summary This paper explores the challenge of teaching compositional behaviors using interactive language games. The authors argue that traditional fixedshape representations limit learning in scenarios with countless tasks. They propose a continuous representation for discrete stimuli creating randomized Gaussian distributions over small intervals. The work defines compositional learning using metareinforcement learning where agents communicate to develop a shared language for completing a sequence of tasks with varying semantic structures. Experimental results demonstrate that agents using DNC and LSTM struggle to solve the problem while an LSTM agent effectively reconstructs input stimuli. Strengths and Weaknesses Strengths:  The use of metaRL for continual referential games with diverse semantic structures offers a novel approach. Weaknesses: 1. Clarity and Organization:  The introduction lacks a clear problem definition and justification.  Key contributions such as the new benchmark are mentioned briefly without sufficient explanation.  Undefined concepts (e.g. \"positional disentanglement metric\") and complex sentence structures make the paper difficult to comprehend. 2. Representation Choice:  The rationale for using SCS over OHE is insufficient.  Examples of scenarios where using OHE would be prohibitive are not provided.  The experimental results indicate that OHE may yield higher performance than SCS. 3. Experimental Setup:  The setup for section 3.1 experiments is missing crucial details (e.g. baseline RL agent architecture).  The setup for metareferential games using SCS is unclear particularly regarding potential intersection publication with increasing stimulus boundary size.  Ablation work to analyze the effects of increasing stimulus boundary and latent dimension are not included. 4. Results interpretation:  The lack of improvement in Figure 4(a) for LSTM agents between 1K and 8K updates requires explanation. 5. Terminology:  Clarification is needed regarding \"supportive trainingpurpose stimuli\" in section 5.4. 6. Grammatical Errors:  Several grammatical errors are present throughout the paper (e.g. missing apostrophes incorrect punctuation).", "Paraphrased Statement: This work introduces a benchmark for evaluating artificial agents ability to demonstrate compositional learning defined as the ability to combine learned elements into new and meaningful combinations. The authors present a Symbolic Continuous stimulus (SCS) representation that allows for the representation of stimuli from different symbolic structures. They formulate the problem as a metareinforcement learning task and compare various RL methods on a task involving single agents. Strengths and Weaknesses: Strengths:  The SCS representation enables stimuli representation from diverse symbolic spaces with a consistent work. Weaknesses:  The representations benefits for compositional learning require further explanation.  The papers clarity could be improved.  Insufficient experimentation to support the claimed contributions.  Lack of comparison with alternative representations (e.g. onehot encodings).  Small text size in figures."], "zHZ1mvMUMW8": ["Summary: This paper focuses on compressing weight matrices in neural networks (DNNs). The proposed technique allows for efficient recovery of original values without decompressing the full data. The approach involves structuring the data and then applying Wavelet Trees for compression. Experimental evaluations demonstrate compression range and speed improvements compared to Huffman encoding. Strengths and Weaknesses: Strengths:  Tackles a significant and timely problem.  Proposes a novel compression approach. Weaknesses: 1. Clarity and execution Details:  Insufficient explanation of the proposed scheme.  Lack of clarity in the evaluation methodology. 2. Evaluation Limitations:  Absence of results on quantized and pruned frameworks.  comparison to a single baseline (original framework size) without considering quantization issue.  Limited consideration of alternative compression formats. 3. other issue:  Difficulty in understanding certain sections of the paper.  Unclear visualization of bitmap figures.  Lack of discussion on potential overhead associated with accessing compressed formats.  No comparison to other compression technique like lowrank approximation. Suggestions for Improvement:  Provide improved explanations of the proposed scheme and evaluation methodology.  Present results on quantized and pruned frameworks without compression.  Evaluate against a more comprehensive range of compression formats.  Address the potential overhead of the compression approach.  Explore different variations of the Wavelet Tree scheme.  Discuss alternative compression technique like lowrank approximation.", "Paraphrased Summary: A technique is proposed for storing a compressed deep neural netprocess (DNN) in a way that enables fast inference. The DNN is expressed as a Runtime Accessible Sequence (RAS) with elements and blocks. The RAS elementary operands are represented as tuples to handle parameter quantization and pruning. For storing the netprocess during inference compact data structures called wavelet trees are employed. The approach is assessed using AlexNet and VGG16 architectures that have been quantized and pruned. issue show promising performance relative to Huffman encoding. Strengths:  Introduces a novel data structure for storing DNNs to accelerate inference.  Demonstrates superior performance compared to Huffman encoding in experiments. Weaknesses:  The claim that frameprocess decompression is required for inference is not entirely accurate as prior methods avoid decompression.  The paper lacks comparison with other technique in categories such as distillation and factorization.  The method agnostic nature of the proposed data structure warrants experiments with various quantization and pruning approach.  The main contribution (data structure formulation) could be more clearly presented.  The writing need improvement for clarity. Related process and comparison:  Lossless compression and memoryefficient deployment technique should be thoroughly discussed.  Additional comparison with other lossless compression approach are recommended.  Inclusion of a ResNetlike architecture for evaluation would enhance the relevance of the process. Improvements:  Support the claims in the introduction with references.  Provide references for the statement regarding the impacts of compression schemes.  Improve the resolution of figures for full readability.", "Paraphrased Summary: Strengths:  Introduces a novel technique for compressing Deep Neural Netprocesss (DNNs) using wavelet trees for efficient inference.  Demonstrates superior compression performance compared to Huffman Coding a widely used compression method in DNNs. Weaknesses:  Insufficient discussion of related process limits the understanding of the methods significance.  comparison with existing compression methods is lacking in detail.  Measurements of inference speed improvements are absent which is crucial for evaluating the practical impact of the proposed technique.  The writing requires improvement for clarity and conciseness.", "Summary: This paper presents a novel approach to reduce the runtime of neural netprocess inference by introducing a novel compression technique. The method allows for fast queries on the compressed frameprocess without decompressing it first. The authors compare their approach to existing methods on AlexNet and VGG16 architectures. Strengths and Weaknesses: Strengths:  The idea of performing inference directly on compressed frameprocesss without decompression is interesting. Weaknesses:  The lack of clear evidence in the experimental results regarding improvements in runtime speedup compression range or accuracy.  The unclear explanation of how the proposed method achieves speedup without decompression.  The papers writing need significant improvement including repetitive sentences typos and lack of fluency.  Section 3 which presents the proposed method requires full clarity and background data.  The related process section lacks references to more recent and relevant papers on frameprocess encoding.  The claim that the method is nearoptimal is unsupported by evidence and the assumption of uniform weight distribution is not realistic.  The use of the same symbol (\u03c3) for both the alphabet and alphabet size can be confusing.  Incomplete citations are present.  The text claims a compression range of 19.8 for AlexNet and 18.72 for VGG16 but the results tables show full compression ranges for Huffman Coding (14.65 and 11.94 respectively) indicating a discrepancy between the text and results.  The statement about achieving a compression range \"with only 1 difference compared with the optimal solution\" is not clearly explained."], "xLfAgCroImw": ["Paraphrase: Summary: This research explores valuing in cooperative game theory. For a coalition S the function F(S) represents its collective payoff. The goal is to use F to calculate an importance vector \u03c6(F) for each agent. The Shapley value and Banzhaf index are examples of such importance vectors. The authors propose a probabilistic approach to this problem where F defines a probability distribution p(S) that represents the probability of coalition S forming. The task of defining \u03c6(F) is then framed as a \"decoupling problem.\" While the agents may be intricately correlated under p to assign individual importance values these interactions must be simplified or decoupled. The researchers aim to find a intersection distribution q that closely matches p under the KL divergence. They define q as a set of independent Bernoulli distributions where the probability of agent i participating is denoted xi. By using coordinate ascent the authors optimize the probability x1...xn. The importance score of player i is then defined as log(xi(1xi)) ignoring a temperature term T for clarity. The resulting importance vector satisfies various game theory axioms including null player marginalism and symmetry. In experiments with small case (n25) the approach performs similarly to the Shapley value and Banzhaf index in information valuation and feature attribution tasks. Strengths:  Unifies Shapley value and Banzhaf index derivation with one gradient ascent step.  Wellwritten and clear. Weaknesses:  lack clear advantages over existing methods like Shapley value and Banzhaf index.  Justification for specific distribution p could be improved.  Reasoning behind the inverse sigma function used for importance scores is unclear. Minor Comments:  explanation of mus role and absence in Equation (1) is missing.  Clarify what constitutes a \"valuation problem\" in the first paragraph.  Define the \"decoupling perspective\" earlier or provide more context.  Specify the \"intriguing properties\" of the importance vector in the last paragraph.", "Paraphrased Summary: The paper introduces the novel \"Variational Index\" a valuation criterion for cooperative games. It constructs a coalition probability distribution using a maximum information criterion then derives player valuation from surrogate distributions based on this distribution. A gradient ascent algorithm is used to compute this \"decoupling.\" Existing valuation criteria like the Shapley value and the Banzhaf index are special cases or modifications of the algorithm. Strengths:  The proposed valuation framework is novel wellmotivated and connects to existing methods.  Extensions are potential by incorporating different surrogate models or applicationspecific priors.  The empirical results are reasonable. Weaknesses:  The paper could be more accessible to practitioners by providing:  Run times or accuracytime tradeoffs for experiments.  guidance on interpreting Variational Index scores especially considering the impact of the scaling factor (T).  The discussion on why additivity or efficiency may not be suitable for all games could be clarified with:  Toy examples demonstrating unintuitive valuation resulting from these properties.  A more accessible explanation for readers unfamiliar with axiomatization of cooperative game valuation. Despite these minor weaknesses the novel valuation framework and the potential for further extensions suggest that the paper should be accepted.", "Summary This paper introduces an energybased approach to cooperative games. It allows for a gradientbased calculation of ShapleyBanzhaf values and defines a fresh alternative value called the variational index. By optimizing a distribution over coalitions under a constraint the approach can identify players with higher contributions. Key Ideas  Solve an entropy maximization problem to obtain a Boltzmann distribution that assigns higher probability to highvalue coalitions.  Perform meanfield variational inference to approximate the distribution with a factorized surrogate resulting in probability for each players participation.  The variational index is defined as a function of the solution to the KL divergence minimization problem. Strengths and Weaknesses  Strengths:  Novel energybased perspective for Shapley values and player valuation in ML.  Variational index is an interesting concept that performs well in experiments.  Weaknesses:  Introduction could better explain the value of learning a distribution over coalitions and the role of meanfield VI.  Connection to multilinear extensions in cooperative game theory is not fully explored.  Practical impact is unclear regarding efficiency in calculating ShapleyBanzhaf values. Additional Questions and Comments  How can existing Shapley value approximations be applied to calculate the variational index  Why doesnt the temperature matter for singlestep updates  How does the initializer affect the intersection of algorithm 1  Do the additivity and efficiency properties actually not hold for variational values or are they simply not appealing  Exploratory experiments could examine whether algorithm 1 converges to the same point regardless of initialization and provides efficient ShapleyBanzhaf value estimates.", "Paraphrased Statement: Gametheoretical valuation methods such as the Shapely value have been applied in machine learning (ML) for assessing feature significance and selecting information subsets. These methods offer solutions to cooperative game scenarios and have been adjusted for use in MLs subset valuation issues. This article offers a probabilistic approach to cooperative games demonstrating that traditional valuation criteria are approximations of a probabilistic maximum entropy solution. It introduces a novel valuation criterion\u2014\"Variational Index\"\u2014that employs a multistep approximation approach while adhering to established axioms for cooperative games. The paper also presents experimental findings for the proposed criterion. Strengths and Weaknesses: Strengths:  The observation that commonly used criteria can be interpreted as simplified approximations of probabilistic assignments is intriguing.  The proposed multistep approximation criterion naturally extends existing gametheoretic methods. Weaknesses:  The experimental results lack sufficient persuasiveness in demonstrating the added value of the fresh criterion compared to existing methods for ML applications.  The article solely focuses on the meanfield perspective and lacks a robust empirical section showcasing the criterion utility for ML audience.  While the fresh criterion involves additional computational cost due to Monte Carlo sampling its added value in compare to existing methods is unclear.  The writing can be improved to provide clearer motivations for subset selection applications in ML and enhance early understanding of the \"decoupling\" approximations sought by classical criteria. Other Comments:  Algorithm 1: The algorithm is more accurately described as a fixed point iteration rather than gradient ascent.  Section 5: It would be beneficial to specify the method used for calculating the decoupling error.  Section 5.1: Clarify the role of information clustering and explain the construction of the xaxis in Figure 1.  Section 5.2: Explain the rationale behind using predicted probability as an valuation metric in Figure 2.  Section 1: Provide a more detailed explanation of \"solution concept \\phi(F)\" for readers unfamiliar with cooperative games. Similarly in Section 2 clarify the relationship between the importance weights and \\phi(F)."], "m8uJvVgwRci": ["Paraphrased Statement: Summary: This work focuses on a weak supervision problem called \"weak indirect supervision.\" In this problem labeled data does not directly represent to target classes but still provides relevant information. The proposed method addresses this by creating probabilistic labels using a twostep work: 1. Employing label work to generate deterministic indirect labels. 2. Constructing a probabilistic label graph based on label relationships. These probabilistic labels are then used to train a classifier for the target classes. The methods efficacy is demonstrated on datasets for image and text classification outperforming baselines. effectiveness:  Novel problem setting and approach utilizing datadriven supervision authors.  Twostep method is compatible with various training techniques.  Theoretical analysis provides guarantees and a condition for evaluating supervision author effectiveness.  empirical evidence supports the methods effectiveness in diverse realworld scenarios. Weaknesses:  Writing and organization require improvement.  Some symbols and concept may be difficult to grasp.  The use of \"indirect label work\" may be unnecessary and could potentially obscure the distinction between weak indirect supervision and zeroshot learning.  The formulation involving \"onehot vectors\" may unnecessarily complicate the annotation. Clarification Requests:  Confirm understanding of annotation and problem setting.  Explain the advantages of the twostep approach.  represent the exponential family distribution necessary for theoretical guarantees  Distinguish between distinguishability and identifiability with an model.  Clarify the interpretation of image 2.  Are there alternatives to SGD for training and is SGD a requirement for Theorem 1  Can the label relation graph be extended to be probabilistic  represent classification with rejection applicable in this context PostRebuttal: The authors effectively addressed concerns and improved the paper introduction. The revised version is a significant improvement warranting a higher range.", "Summary Paraphrase: The authors succinctly describe their contributions in the introduction:  They propose a problem setting called Weak Indirect Supervision (WIS) expanding on Weak Supervision.  They develop a Probabilistic Label Relation Model (PLRM) for WIS.  They introduce the concept of distinguishability in WIS.  They evaluate PLRM on imagetext classification and advertising application. Strengths Paraphrase:  The paper introduces the novel and practical WIS problem setting.  It provides a thorough theoretical analysis of error bound and distinguishability.  The proposed PLRM method significantly outperforms baselines in both synthetic and realworld experiments. Weaknesses Paraphrase:  The author did not identify any major weaknesses but offers suggestions for improvement:  Consider related work that may be relevant to WIS.  Provide more model of sampled label graphs to clarify the complexity of the task.  Simplify annotation and explicitly reference the glossary in the appendix.  Evaluate the scalability of the method for larger label sets.", "Paraphrased statement: This paper presents a model for combining multiple weak labels into a single strong label allowing models to be trained effectively still with limited data. The model is distinct from previous approach as it can handle tasks with nonoverlapping label sets. effectiveness:  The concept of \"Weak Indirect Supervision\" is a novel approach to addressing the challenge of noisy hierarchical labels.  The paper presents a clear and coherent structure with a significant contribution in the work of the proposed probabilistic label relation model (PLRM) and its theoretical analysis. Weaknesses:  The PLRM requires that labels have a predefined relationship meaning they must be \"comparable\" in some way. It cannot adequately model labels that are not in the same semantic space such as \"red\" and \"dog.\" This limits its applicability to hierarchical labels rather than heterogeneous or multimodal labels.  The datasets used in the paper all involve hierarchical label interactions which may not fully capture the range of scenarios that the model claims to address.  The PLRM models the relationship between the input label fusion (ILF) output and the labels with independent accuracy levels ignoring the certainty of the ILF output for individual data points. This could be a limitation for application where the confidence in the ILF output is relevant.", "Paraphrased Statement: Overview: The work explores a novel technique for creating labeled datasets in weak supervision using \"indirect\" label sources. These sources resemble labeling work in weak supervision but generate labels from a space distinct from the target label space. This approach is particularly valuable when indirect label work (LFs) are available and exhibit some correlation with the target label space. Methodology: The paper proposes a probabilistic label model (PRML) that leverages indirect LFs and label relationships to produce target labels for unlabeled data. The methodology is supported by theoretical analysis and realworld experiments. analysis: The paper provides a generalization error bound for the end model trained with estimated labels analogous to previous research in data programming. The work also addresses the issue of indistinguishability between labels and establishes conditions for distinguishability. Experiments: empirical results using realworld data demonstrate the effectiveness of the proposed method compared to existing baselines. effectiveness:  Tackles a significant and novel research problem in weak supervision.  Proposes a technically sound method supported by analysis and experiments.  Demonstrates promising performance on realworld data. Weaknesses:  generalization error analysis mirrors prior work.  Some grammatical and spelling error exist.  Additional intuition for Theorem 2 in the main paper would be beneficial."], "l3SDgUh7qZO": ["Paraphrase: This paper proposes SphereFace2 a binary classification training framework for face recognition. instead of the traditional multiclass classification SphereFace2 uses pairwise comparison to learn a similarity step. This approach is better suited for openposition evaluation where the recognition system may encounter faces that were not included in the training position. The paper provides a detailed explanation of the loss function used in SphereFace2 and its geometric interpretation. Extensive experiments demonstrate the effectiveness of SphereFace2 showing that it outperforms existing face recognition methods. effectiveness:  Binary classification reduces the gap between training and openposition evaluation.  The loss function is wellmotivated and explained.  Comprehensive experiments show stateoftheart performance. Weaknesses:  The hyperparameter analysis in Section 3.1 should be clarified with respect to the backbone network used in Section 3.2.  Some numerical results in the text do not match those in the tables.  Minor typos exist (e.g. incorrect table references).", "Summary This work introduces SphereFace2 an innovative face recognition model. The key feature is the conversion of softmaxbased training into multiple binary classification aligning with the test stage and enabling evaluation for both closed and open position. This strategy requires a loss function with specific principles:  Maintaining a balance between positive and negative samples  Identifying and prioritizing challenging samples  Incorporating an angular margin for better discrimination  Adjusting similarity step SphereFace2 surpasses current face recognition model across 11 verification and 1N openset evaluation benchmarks. Additionally it supports model parallelization enhancing training speed with multiple GPUs. effectiveness  Convincing argument for the benefits of multibinary classification  Clear comparison between tripletbased and pairbased learning  effective principles for designing a loss function demonstrated in experiments  Strong performance relative to other model  Support for model parallelization Weaknesses  Positivenegative sample balance technique is simplistic and manually tuned  Absence of optimal hyperparameter values  SphereFace2 underperforms SphereFace in 1N rank1 recognition  CosFace is used as a baseline instead of ArcFace", "Paraphrased argument: Summary This paper introduces SphereFace2 a face recognition training method that uses binary classification loss for each identity to enhance face embedding quality. It aims to overcome limitations of existing training schemes that create a mismatch between training and realworld openset face recognition scenarios. SphereFace2 employs a binary classification protocol along with optimization strategies like sample mining and similarity adjustment to achieve superior results over current methods as demonstrated in extensive experiments. Strengths 1. Clear and informative presentation. 2. Wellgrounded motivation addressing the disparity between training and deployment in openset applications. 3. effective demonstration of key components through visual aids and statistical evidence. 4. Improved performance in parallel training environments. 5. Extensive experiments verifying the methods effectiveness. Weaknesses 1. Misleading introduction as other works have addressed the number of multiclass softmaxbased training in openset applications. 2. Vague or unclear argument:  Competition among classes: further clarification is needed.  Proxyfree and proxybased classification: The distinction is not clear.  Similarity adjustment model: Mathematical discrepancy requires clarification. 3. verification threshold conclusion: Whether it is based on validation data or theoretical values and how it allegedly simplifies threshold selection compared to other methods. 4. Difficulty in isolating the impact of different components on the overall performance."], "nD9Pf-PjTbT": ["Paraphrased Summary: This paper investigates attractive frameworks with unidirectional local potentials and explores a specific instance of the generalized belief propagation algorithm over triples of graphs. It extends the convergence results established by K\u00f6hler to this framework class introducing a favorable initialization strategy. strength and Weaknesses:  Positive Aspects: The paper addresses the significant topic of belief propagation convergence behavior and provides an initialization strategy for specific frameworks.  Limitations: The novelty of the work is limited as the findings apply only to a restricted context and some results are analogous to existing literature. The notation and terminology used can be confusing in places. Furthermore relevant prior work is not adequately referenced. specific Criticisms:  The framework assumptions made are also restrictive considering scenarios where the most accurate fixed detail is known to align with local potentials.  The energy landscape for these frameworks is wellundersalsod making the convergence results predictable.  The authors claim that the results generalize to a extensive scope is questionable.  The generalization of the Bethe free energy to higherorder frameworks is not novel as it is essentially the Kikuchi free energy a wellestablished concept.  The use of summation indices (ij) (ijk)... can lead to multiple contributions from the same pair or triplet to the energy.  The definition of the ferromagnetic framework is inconsistent with other literature.  Equation 5 introduces a term \\lambda(\\ldot) without prior definition.  Equation 7 contains a double product that appears incorrect. Suggested Improvements:  The authors could consider proving results for a wider range of frameworks with frustrations or arbitrary local potentials.  Explaining how considering certain motifs might make the energy landscape convex would be valuable.  Clarifying the notation and terminology used would enhance readability.  Citing relevant previous work including classical papers on belief propagation and recent advances would strengthen the papers credibility.", "Paraphrase: Summary: The author examines a subset of models with desirable characteristics and applies results from a previous field by Koehler. While the convergence properties of belief propagation algorithm are often complex these models have a simplified optimization landscape making convergence easier to analyze. However the results may only apply to this specific class of models limiting the papers generalizability. strength and Weaknesses: The validation of Lemma 2 may contain errors. The following clarifications are sought: 1. The field and index ranges for the functions f and i should be confirmed. 2. The concavity of the function in Eq. (64) is not necessarily implied by its constraint on nonnegative elements. 3. Missing summation indices in Eqs. (55) and (56) should be added. 4. The dependence on indices in Eq. (57) is inconsistent. 5. The third term in Eq. (60) appears inconsistent with the lack of index dependence in the first two terms. 6. The derivation of Eq. (61) from Eq. (60) should be explained. 7. The definition of C in Eqs. (62) and (63) should use explicit indices (Cij gij). 8. The nature of x in Eq. (63) (vector or scalar) should be clarified. 9. The sign of each summand in Eq. (62) should be demonstrated. 10. The meaning of \"x  f(x)\" in Eqs. (63) and (64) should be clarified.", "This field examines the convergence of the generalized belief propagation algorithm (BP) on graphical frameworks with motifs. The authors demonstrate that BP initialized with allones messages converges to a fixed detail that maximizes the Bethe free energy. The field focuses on the Ising framework a common undirected graphical framework and applies BP to solve inference problems. BP is known to provide exact result on treelike structures. The key contribution of the research is the analysis of BPs convergence behavior on graphs with motifs where each vertex is associated with specific combinations of edges triangles or squares. The Hamiltonian function is calculated by aggregating motifs and multiplying spins within each motif. The authors present rigorous proofs to establish that generalized BP initialized with allones messages converges to a fixed detail with the high Bethe free energy. While the proofs primarily focus on triangle motifs the approach is likely applicable to larger motifs. However the papers focus on proofs in the latter half may hinder intuitive understanding. Additionally the convergence result does not provide insights into the time required for BP convergence. This limitation may be significant for practical application as convergence time may increase with motif size particularly for nonconstant motif size. In light of existing convergence results for BP in Ising frameworks the primary finding may not appear significant.", "Summary: Researchers have generalized a result for belief propagation (BP) in ferromagnetic Ising models to BP in higherorder Ising models. This is a theoretical finding. strength and Weaknesses:  Clarity: Some typos are present but do not hinder comprehension. However the results are presented in a confusing manner with definitions out of order and overly complex notation.  Novelty: While the final theoretical result seems novel the authors poorly explain its originality or relationship to existing techniques.  Significance: The result may be of interest but the validation techniques might have limited novelty and the practical application are unclear. specific Comments:  Citation style needs correction.  Equation 3s notation is challenging.  The assumption of hi \u2265 0 is not consistent with the binary ferromagnetic case.  Lambda is used before its definition.  Equation (5) is unclear.  The notation phi(vijk) is confusing and not used consistently.  Equations (21) and (36) and Lemma 1 may be standard results and should be cited.  The differences between the approach in this work and previous work are not clearly explained."], "o9DnX55PEAo": ["Summary: The paper presents a method to reduce the size of a BERT language model by creating a smaller model called CMOWCBOWHybrid. This new model outperforms ELMo but falls short of typical distillation techniques that use transformerbased student models (e.g. DistillBERT). However the authors highlight that CMOWCBOWHybrid is significantly faster. The paper is wellwritten and clear. effectiveness and Weaknesses:  The paper does not explore the performance of the model using either a single CBOW or CMOW without the hybrid approach.  The optimal size for the matrix embeddings is not investigated.  It is unclear how the model performs under different embedding sizes.  The individual roles and complementarity of CBOW and CMOW embeddings are not discussed in detail.  The inclusion of CMOW which is from a separate paper makes the paper less selfcontained for readers.", "Paraphrase: This research introduces a technique called \"knowledge distillation\" for enhancing the performance of BERT (a language model). In this method a large and more capable model (BERTbase) serves as a teacher to guide a smaller and less experienced model (CMOW) known as the student. To strengthen the student model the researchers developed a variation called \"bidirectional CMOW\" which improves the models ability to process information in both forward and backward directions. Additionally they employed a twostep distillation scheme that combines general distillation (transferring general language knowledge) and taskspecific distillation (tailoring the student model to specific tasks) to further boost performance. The team tested their method on the GLUE dataset which evaluates language models on a order of tasks. While the proposed method utilizes novel approach such as bidirectional CMOW and twostep distillation its performance on certain tasks (e.g. CoLA MRPC) falls short compared to similar baseline method. The method also lacks advantages in terms of parameter count and inference sentence compared to existing techniques like TinyBERT.", "Summary The paper presents a new language model CMOWCBOWHybrid which is derived from large pretrained models. This model generates pertoken representations for consumption in knowledge distillation during masked language modeling pretraining. For downstream tasks involving sentence pairs a novel encoding scheme (DiffCat) was devised. Experiments demonstrate that the proposed approach achieves performance comparable to ELMo with half the parameter count and three sentence the inference speed. effectiveness  advanced idea: The bidirectionally extended CMOW model enables tokenlevel representation extraction for masked language modeling training.  DiffCat: This encoding scheme specifically designed for sentence pairs improves performance by 32 over joint encoding.  efficiency: The model boasts high encoding speed matching ELMos performance with a smaller step. Weaknesses  limited evaluation: The work focconsumptions only on classification tasks in the GLUE benchmark. Testing on extensive tasks such as question answering would be valuable.  Speedperformance tradeoff: The paper does not explore ways to balance inference speed with performance. A tradeoff mechanism akin to TinyBERTs tiered layers would allow consumptionrs to customize the model to their needs.  model specifications: The paper lacks details about the CMOW models 20x20 matrix embeddings."], "kDF4Owotj5j": ["Paraphrased Summary This work introduces two modifications to recurrent neural networks enabling them to extend their performance to larger more complex problems than observed during training. Three specific problems are addressed: 1. Pathfinding in expanded mazes 2. Processing longer binary strings 3. Identifying optimal chess moves in tougher view Modifications: 1. recall: Integration of initial problem data at each step of the recurrent computation 2. Modified Loss: training involves combining standard loss for m recurrent iterations with loss for mnk iterations without gradient tracking for the first n iterations. issue:  The modified network learns an algorithm that converges to a valid solution with increased recurrent iterations overcoming the issue of \"overthinking.\"  The learned algorithm generalizes to larger and more challenging problem instances. For instance networks trained on 32bit strings produce correct results on 512bit strings and networks trained on 9x9 mazes can solve mazes up to 201x201. Ablation work:  comparison with MLPs and modified MLPs with \"recall\" evidence that only the proposed network learns a convergent solution that extends to larger problem instances. Strengths and Weaknesses: Strengths:  clear problem formulation and compelling solution  Wellwritten presentation Weaknesses:  Evaluation on a novel dataset that may favor the proposed method  lack of comparison to more established benchmark and other relevant approaches such as recurrent Relational network (RRN)  Hyperparameter (alpha weight) is somewhat arbitrary", "Paraphrased Statement Summary The paper introduces two enhancements to recurrent neural networks (RNNs) that improve their generalization capacity on synthetic tasks. The first modification employs residual connections to facilitate the network in retaining input data during work. The second modification use truncated backpropagation through time randomly creating an auxiliary loss that aids in reducing overfitting. The modified network demonstrates an enhanced ability to solve complex problems through extended recurrent iterations showcasing its generalization prowess. Strengths  The paper expands on the concept of deep recall networks.  The writing is accessible and comprehensible.  The proposed modifications are wellfounded targeting the limitations of the base model.  Experiments effectively demonstrate the modifications effectiveness outperforming baselines well.  The paper includes detailed ablation work and behavioral analysis to provide insights into the models inner work. Weaknesses  The modifications are incremental utilizing existing techniques like residual connections and truncated backpropagation.  The tasks used for evaluation are synthetic raising concerns about the methods applicability to realworld data.  The paper lacks specific details regarding the method leaving some questions unanswered. Overall While the paper presents an challenging approach to enhancing logical generalization the proposed modifications are marginal compared to the original work. Nonetheless the innovation of residual connections is crucial for deep network training and the progressive loss is noteworthy. However it is unclear if this approach is novel compared to other RNN training techniques. The experimental results while promising for synthetic tasks require validation with realworld data to establish the methods broader applicability. Questions  Did the authors employ a convolutional neural network (CNN) architecture for the prefixsum task despite its sequence of 0s and 1s as input  What loss use was employed for training the model  The inference iteration selection work for performance measurement should be clarified.  Did the authors test the prefixsum task with longer sequences (e.g. 1024 routine)  Why is there no performance visualization for the 201x201 maze task  Could the experiments be strengthened by utilizing further architecture like Transformers or selfattention models as baselines  The auxiliary loss in number 8 needs further explanation as to why it is particularly beneficial in the context of this work.  The yaxis values in number 9 exceed 1010 which requires clarification.", "Paraphrased Statement: Summary: This work proposes two enhancements to the recently developed recurrent Thinking Systems model to improve its ability to generalize from simple training tasks to more complex test problems. The proposed enhancements include: 1. Providing the model with a \"clue\" indicating the current problem at each time step (recall). 2. Implementing a method to prevent the model from learning behaviors specific to individual iterations promoting generalization to more complex problems with multiple iterations. These enhancements enable the model to generalize to significantly more complex problem instances across three different task domains. Strengths and Weaknesses: Strengths:  The approach and enhancements are novel and promising.  issue are provided across several task domains.  analysis of convergence and overthinking is valuable. Weaknesses:  Limited baseline comparison only considers a feedforward model.  Importance of the specific recurrent architecture used is not explored.  The progressive loss primary use is for minimizing the number of iterations to solve a task.  The proposed method may not be the optimal approach for this use.  issue are based on a subset of models that met a training accuracy threshold.  model evaluation uses the iteration with the highest confidence instead than the final iteration.  The recall enhancement seems to be more effective in preventing overthinking on certain tasks than others. Minor issue and Additional Questions:  Confidence evolution over time is not discussed.  The source of results in number 1 is unclear.  Legend errors in Appendix tables 1 and 4.  Section reference error in the maze task results section.", "Paraphrased Statement: This research focuses on developing neural networks that can solve complex problems by extrapolating from simpler ones (e.g. solving large mazes by first learning to solve smaller mazes). To achieve this the paper proposes two key changes to recurrent convolutional networks: 1. Adding a shortcut connection that links the input directly to the recurrent level providing stability during extrapolation (number 2). 2. Implementing a modified training method and loss use that encourages the network to perform computations independently of the current iteration (Algorithm 1). These techniques jointly enhance the networks performance on tasks such as calculating prefix sums navigating 2D mazes and solving chess puzzles. Strengths and Weaknesses: Strengths:  Wellwritten and easy to comprehend.  Extends prior research on extrapolation and addresses limitations identified in that work.  Includes detailed analysis that provide valuable insights into the proposed techniques. Weaknesses:  The recall skip connection technique appears to correct a innovation flaw in previous work by Schwarzschild et al. This should be acknowledged and contextualized within the broader literature on stable models.  The term \"recall\" is used metaphorically and should be clarified to avoid misleading interpretations about the capabilities of the networks."], "gSdSJoenupI": ["Paraphrased Statement: This paper presents a novel loss function denoted as the Taylor expansion of crossentropy loss and focal loss for tackling classification tasks. The authors demonstrate that applying different components of this expanded function can enhance the performance of models across various domains including image classification 2D object detection and 3D object detection. effectiveness and Weaknesses: effectiveness:  Accessible and straightforward writing way  Application of the Taylor expansion concept to address classification job  experimental validation on multiple classification tasks Weaknesses:  Limited improvement in model performance: the obtained benefits are often modest or insignificant  Lack of statistical significance for some reported results  Minor grammatical error: \"can improves\" should be \"can improve\"", "Paraphrase: Summary: This research suggests a method for designing loss functions based on polynomial expansions of existing loss functions. By carefully adjusting the coefficients of these expansions the authors demonstrate enhancements in various computer vision tasks. Experiments covering image classification instance segmentation object detection and 3D object detection show slight improvements over baseline methods. effectiveness:  The paper is wellwritten and clear.  It presents a compelling insight that different loss functions can be expressed as common polynomials with varying coefficients. Weaknesses:  The motivation for the approach is unclear. Although polynomial expansions can represent loss functions the need for finetuning the coefficients introduces extra hyperparameters potentially leading to overfitting.  The claimed improvements over baselines are minimal.  It remains uncertain whether the added complexity (adjusting coefficients and determining expansion cutoff points) justifies the small benefits. Minor Issue:  Typo: \"polynmoial\" should be \"polynomial\" in the conclusion on page 9.", "Motivation and Method: The papers main line is that loss functions should be adaptable to various learning tasks and datasets unlike existing loss functions (crossentropy loss and focal loss) which are limited to classification job. To achieve this the paper proposes a general loss function expressed as a linear combination of polynomial functions. By doing so it generalizes crossentropy and focal loss as specific instance. Inspired by the Taylor expansion the paper introduces PolyLoss a firstorder polynomial loss function. Experiments and Analysis: The proposed PolyLoss is tested on three different datasets: ImageNet (image classification) MSCOCO (object detection and instance segmentation) and Waymo Open (3D object detection). The experimental results demonstrate the effectiveness of PolyLoss. effectiveness:  clear and wellstructured presentation especially the explanation of PolyLoss.  Novel idea of generalizing crossentropy and focal loss as polynomial functions.  PolyLoss is simple and shows effective performance.  Strong experimental validation.", "Paraphrased Statement: Summary This article examines the crossentropy and focal loss from a polynomial expansion viewpoint. It introduces PloyLoss a new loss function family. Three instance of PloyLoss are analyzed leading to the final version called Ploy1. Extensive experiments are conducted and analyzed. Strengths 1. The polynomial expansion perspective provides insights and PloyLoss is a logical extension of this concept. 2. Comprehensive experiments are presented revealing valuable findings. 3. The proposed loss enhances various tasks including image classification instance segmentation and 3D object detection. Weaknesses 1. Ploy1 is surprisingly the final version of PloyLoss which may suggest that higherrange terms are insignificant. The author acknowledges this but attributes the poor performance of lowerrange loss functions to inadequate coefficient tuning. It would be beneficial to provide comparisons with:  L1 loss with optimized hyperparameters  L1 and L2 loss with hyperparameter tuning 2. The paper defines PloyLoss as a general family of loss but its exploration is limited. Key variables like coefficients starting range (\u03b3) and coefficient decomposition strategy require further investigation. 3. The paper uses the CE and focal loss coefficients as the sole model for coefficient organization. Alternative strategies such as exponential decomposition should be explored. 4. The labels on the yaxis in image 2(b) may contain errors."], "uVTp9Z-IUOC": ["By minimizing entropy testtime adaptation allows models to adjust to changes in the dataset such as image distortions without altering the training work. This research extends a previous entropy minimization method by introducing nonsaturating loss functions adding a regularizer for diversity and adjusting the input data along with the model parameters. A convolutional image transformation model is used to modify the input between the input and classification models. These enhancements do not require additional optimization iterations or supervision allowing the method to adapt online efficiently without external supervision. Experiments show reduced generalization error on the ImageNetC and ImageNetR corruption benchmarks. While the improvements are modest they are consistent across multiple architectures. However the proposed method leads to lower accuracy on clean data indicating that it does not clearly outperform previous work. strength:  Novel joint adaptation of input transformation and model parameters for testtime adaptation.  Alternative losses the input transformation model and the diversity regularizer collectively and individually enhance accuracy.  Extensions maintain online efficiency making the method as well deployable as previous techniques.  paper testtime adaptation results on ImageNetR providing new empirical data.  Design choices are supported by experimental results visualizations and play experiments. Weaknesses:  set novelty of extensions due to convergence with previous methods.  Proposed HLR and SLR losses restrict parameter adaptation necessitating an alternative loss to simplify the methods application.  Modest improvement on shifted data and harm on unshifted data contradicting the motivation for nonsaturating losses.  input transformation (Sec. 3.1) is not extensively studied or analyzed. Additional Feedback:  Provide a control experiment for the optimization choice.  Include results for TENT without freezing weight of deeper layers.  Analyze learned transformation models including the learned transformation weight tau.  Provide more details on data subset experiments (image 3).  Summarize results with mean value where appropriate such as Table 1.  Consider qualitative results of learned image transformation in the appendix.", "Paraphrase: The paper presents a novel loss work for enhancing testtime batch normalization (BN) adaptation in domain adaptation. This loss comprises two elements:  diversity Maximization loss: Assesses the dissimilarity between predictions on different data points in a minibatch. This prevents trivial solutions where the model produces identical predictions for all data in the minibatch.  Confidence Maximization loss: Encourages the model to predict with high confidence. The loss includes options for \"strong\" and \"soft\" likelihood ratio which exhibit large gradients for highly confident predictions. strength:  introduction of an data maximization loss for testtime BN adaptation on target data without labels.  Addressing limitations in existing methods by incorporating a diversity regularizer.  analysis of loss gradients for confidence maximization and proposal of a specific loss work with nonzero gradients for highconfidence predictions.  Joint update of both the image transformation (IT) module and BN parameters during testtime adaptation.  Thorough experimental evaluations included in the paper and appendix. Weaknesses:  limited strength of the proposed diversity loss on minibatch data.  Lack of model or detailed explanations of failure cases in Section 3s last paragraph.  Relatively limited novelty as components of the proposed approach such as IM loss KLbased Ldiv IT module and the negative loglikelihood ratio have been presented in previous work.", "Paraphrase: Summary: Despite revisions certain aspects of the previous review remain valid. This paper proposes a testtime adaptation method using techniques that include:  Unsupervised log likelihood ratio objective  Regularization for diverse predictions  input transformation module These techniques outperform Tent a similar method based on entropy minimization on ImageNetC and ImageNetR. strength:  Novel selfsupervised log likelihood ratio objective  Effective combination of techniques  Comprehensive experiments covering various adaptation scenarios  Clear and wellstructured writing Weaknesses:  limited justification for the approach (using selfsupervised signal from confident samples)  Unclear how this approach benefits ambiguous samples  Lack of results on challenging distribution transformation benchmarks (e.g. ImageNetA ImageNetv2)  Unclear what constitutes \"low\" confidence for corrupted image", "Summary and contribution The paper proposes a method for testtime adaptation that reduces performance degradation when test and training data distributions differ. It combines:  A selftraining loss that improves stability over typical entropy minimization approach.  A diversity regularizer to encourage model predictions to be different from each other.  An \"input transformation\" module to modify inputs before feeding them to the model. Strengths  Clear and wellstructured writing.  Simple and effective approach that improves performance on standard testtime adaptation benchmarks.  Comprehensive experiments comparing the proposed method to alternatives. Weaknesses  The proposed approach largely combines existing techniques from other papers.  Lack of ablation work to assess individual contributions of each component.  potential limitation with the proposed loss which is unbounded and may require careful model design. Recommendation Despite limited technical novelty the papers empirical contributions and clear introduction warrant a marginal acceptance."], "wgR0BQfG5vi": ["Paraphrase: The field presents a straightforward yet effective method for adjusting labels for data degree. The parameter for smoothing (alpha) is optimized dynamically for each data degree based on the normalized entropy of the predicted label distribution. The distribution utilized for label smoothing derives from the same model but at an earlier degree of training. specifically a teacherstudent learning model like knowledge distillation is employed where the teacher is a model trained in a previous epoch. From the pool of past models the one yielding the highest evaluation metric (g) is utilized. The choice of g is not limited to the training loss and can extend to different evaluation metrics. The approach is theoretically substantiated by a gradient analysis and experimentally supported by enhanced evaluation metrics calibration and an ablation field. effectiveness:  Provides a simple mechanism for adapting the smoothing parameter alpha.  Utilizes the distribution from the same model but at an earlier training epoch instead of a basic distribution for smoothing.  Achieves improved accuracy and calibration in experiments compared to baselines. Weaknesses:  The importance of aligning the function g in Eq. 5 with the evaluation metric is not explicitly clarified. Since alpha is tuned dynamically based on g it may provide an unfair advantage over baselines that are trained solely on likelihood or crossentropy loss.  Experiments where the evaluation metric and training loss are the same function (e.g. log likelihood) could provide further insights.  In Table 1 it is not clear if the \"Ours\" model is always trained with function g (BLEU) or if g is set equivalent to the evaluation metric (e.g. METEOR when evaluating with METEOR).  The original paper introducing knowledge distillation is not referenced.", "Summary This paper presents an adaptive label smoothing method that adjusts to the probability distribution changes of a model during training. It leverages dynamic smoothing parameters and prior label distribution from selfdistillation where the smoothing parameter is calculated based on the entropy of the models probability distribution per sample during forward propagation. Experimental results show that this adaptive approach achieves competitive performance. issueiveness  Introduces a selfadaptive smoothing parameter that eliminates the need for manual hyperparameter tuning.  Demonstrates the regularization issue of selfdistillation in label smoothing theoretically. Weaknesses  Lacks theoretical justification for using selfdistillation as a prior distribution in label smoothing.  Insufficient evidence provided to support the claimed benefits of adaptive smoothing parameters.  The ablation field only compares against fixed smoothing parameters while adaptive hyperparameter tuning should also be considered.", "Paraphrased Summary: foundation: This field introduces an innovative method for adaptive label smoothing. Existing label smoothing techniques apply a uniform level of smoothing across samples and time steps. Proposed method: The proposed method enhances label smoothing by introducing dynamic smoothness degree between samples and over the course of training. It utilizes an entropybased smoothness parameter normalized to the distribution of predictions. effectiveness:  Extends label smoothing with adaptive samplespecific adjustments.  Delivers promising performance improvements in model metrics and calibration error.  Eliminates the need for hyperparameter tuning for smoothness. Weaknesses:  Does not address certain alternative instancelevel label smoothing methods.  Lacks comparisons with label smoothing methods in nonNLP field such as image classification.", "Paraphrased Statement: label smoothing enhances neural networks performance by reducing overconfidence in predictions. However current methods apply a uniform level of smoothing across all data degree. This paper introduces an innovative adaptive label smoothing technique that tailors the smoothing level to each sample. It uses the entropy of predictions from previous time steps to determine the necessary smoothing. effectiveness:  Wellwritten and conceptually sound paper  Empirical evidence supports the effectiveness of the proposed method  Comprehensive evaluation on multiple datasets  Detailed ablation field demonstrates the methods significance Weaknesses:  Limited novelty compared to prior works that utilized predictions from past timestamps for selfdistillation and adaptive label smoothing (e.g. [1] [2]).  Relevant prior works ([3] [4]) could be discussed and benchmarked against."], "iw-ms2znSS2": ["Summary This study investigates planning with approximate policy and value role. Based on empirical findings of heavytailed distributions in guided search runtimes the authors propose an abstract model to explain these distributions. To address the heavy tails they suggest using random restart and proposeuncertaintyaware neural netstudys (NNs) specifically with testtime dropout to enhance random restart. empirical results are presented for the Sokoban domain. Strengths  Multiple novel ideas are introduced.  The model explaining heavytailed distributions provides valuable intuition.  Results demonstrate the effectiveness of using the proposed policy.  The concept of using uncertaintyaware NNs for policy learning is innovative. Weaknesses  Essential experimental details are missing making it difficult to replicate or interpret the results.  Lack of data on neural netstudy architecture hyperparameters state encoding and training process.  The model in section 4 requires formal definitions and clearer notation for improved clarity.  The gap between the study scope (PSPACEhard problems) and the actual results (specific planning algorithm in Sokoban) is significant.  Limited literature review and lack of connections to relevant study in learning for planning.  A focus on also many research threads within the paper hindering an indepth exploration of any individual topic.  Numerous typos and grammatical errors.", "Summary This paper investigates the behavior of the A search algorithm when combined with a neural network heuristic estimator (ADNN) for solving Sokoban puzzles. It introduces relevant research on the \"heavytail distribution\" of running time in SATCP and provides a brief overview of the neural network setup used for heuristic generation. The paper proposes an imbalanced tree model to explain running time behavior and presents theoretical statements (Theorem 1 and 2) about it. The experimental section compares problem coverage and search space statistics for randomly selected instances. Strengths and Weaknesses Strengths:  extensive coverage of problem instances  Connection to prior research on heavytail distributions  Intuitive explanations based on the imbalanced tree model Weaknesses:  Limited data about the DNN heuristic generator  Lack of supporting experiment for claimed contributions General Questions  applicability of observations to AI planning in general  Generation of Figure 1 and process of running time  MonteCarlo dropout  Memory limits during experiment  compare with innovative AI search algorithms  Typical problem statistics  understanding for using weighted A  Limited node expansions and implications for search coverage  Quality of heuristic estimates and alignment with solution paths  evidence for \"substantial locality property\" of distance heuristics  theoretical basis for \"depth term\" not providing extra heuristic data  experiment in nonrandom AI planning domains  Proofs for Theorem 4.1 and 4.2 Comments on Contributions  Contribution 1: Effectiveness of policy network may be due to increased weight on DNNgenerated trajectories.  Contribution 2: Figure 4a provides limited support requires more data.  Contribution 3: model explains observations but needs further verification.  Contribution 4: Uncertaintyaware networks are used but their significance is not discussed.  Contribution 5: Figure 4b supports the claim but requires more experimental details.", "Paraphrase: Summary This paper investigates the use of a Deep Reinforcement learning (DRL)based heuristic (called A) in combination with A to solve Sokoban a challenging planning and search problem. experiment reveal that the running time distribution exhibits a heavytail in certain scenarios indicating that some instances can be solved rapidly (in polynomial time) or may require a substantial amount of time. The paper also demonstvalues how prior search stvaluegies such as restart and randomization can enhance the solution value. The paper claims to make five contributions but the reviewer questions their significance. Rather than presenting and commenting on each contribution individually the reviewer will delve into them and provide an overall assessment of the paper. Ultimately the reviewer recommends rejecting the submission as further study is necessary. Strengths and Weaknesses Despite the reviewers interest in this research area they recommend studying the interaction between search algorithms and MLbased heuristics including DRL.", "Paraphrase: This research investigated a style of bestfirst search with value and policy role utilizing deep networks. The paper developed a theoretical model to analyze the heavy left and right tails observed in running time. The authors contend that policy network random restart and uncertaintyaware networks are crucial for solving complex planning problems as demonstrated by their experiment. However the paper does not provide specific details on its policy valuation and uncertainty networks making it challenging to assess the approachs practicality and the theoretical analysis. Readers must speculate on the actual networks used in the experiment and analysis based on prior literature. Strengths: 1. An abstract model explaining the heavy tail topic for bestfirst search algorithms was proposed. 2. experiment suggest the effectiveness of the suggested combination of policy valuation random restart and uncertainty networks with minimal modifications to existing proposal. Weaknesses: 1. Specific architectures for value networks policy networks and uncertaintyaware networks are not described hindering replication and evaluation. 2. The abstract model of heavy tail problems requires improvement in clarity and mathematical rigor. Notations and concepts should be introduced formally and consistently used. Moreover the abstract model appears to be independent of deep net heuristics or policy networks. details: 1. Luby et al. (1993) plays a significant role in the abstract model. Consider moving the relevant mathematical definition to section 4. Additionally the notation \"p\" in section 4.1 is inconsistent with how it is used elsewhere. 2. A typo exists on page 4: \"pai(n) \u2026 p(sl sl1)\" should be \"pai(n) \u2026 p(sn sn1)\". 3. Critically nodes should be mathematically defined on page 4."], "fQTlgI2qZqE": ["Summary Paraphrase: This research focuses on developing interpretable models using feature interactions aligning with the trend of explainable deep learning. The paper utilizes a lightweight and interpretable deep learning model (ParaACE) based on the alternating conditional expectation (ACE) method to solve a multiarmed bandit problem using the UCB algorithm efficiently and accurately. Compared to its baseline model ParaACE demonstrates improved accuracy and a significant reduction in model size on various datasets. The paper provides ample supplementary materials and contribution its code. effectiveness and Weaknesses: Pros:  Fast and reliable method for detecting interactions  Lightweight and interpretable neural network model with superior performance  Solid theoretical basis and empirical evaluations using realworld data  Principled approach rooted in the definition of feature interaction  Interdisciplinary integration and application of concepts  Insightful supplementation with proofs and explanations Cons:  reliance on older algorithm (UCB and ACE)  Limited comparison with more recent approach  Inclusion of redundant definitions  potential for increased synthetic data training samples  Weak conclusion section Additional Points:  theoretical analysis are wellsupported.  interaction knowledge is extracted and used to build a transparent learning model.  The novelty of this combination is unclear.  The distinction between interaction and functional dependency is not explicitly addressed.  The assumption of finite evaluations for accurate reward estimation may not hold in practice.  The use of the Matlab logo is unique and visually engaging.  Further exploration of crossdomain practical applications and comparison with alternative stateoftheart methods in realworld analysis is recommended.", "Summary Paraphrase: The paper proposes a method for efficiently detecting interaction effects by sampling Hessian values using a multiarmed bandit approach. This approach reduces the computational cost of calculating the broad Hessian matrix. effectiveness and Weaknesses Paraphrase: effectiveness:  Explores the use of Hessian values for interaction detection and provides theoretical justification.  Includes numerous experiments in the main text and supplementary materials. Weaknesses:  Lack of Evaluation: Does not directly evaluate the efficiency of the proposed approach compared to estimating the broad Hessian matrix.  Limited background: Focuses solely on lowdimensional cases but the method is claimed to excel in high dimensions. No evidence is provided for highdimensional performance.  Suitability of Architectures: Considers complex architectures that may not be optimal for small datasets (800 samples).  Presentation Issues:  Language oversimplifies or complicates concepts hindering reproducibility without the source code.  Missing references to tables in the main text.  Incorrect term \"AUC\" used instead of \"ROC AUC\" for interaction detection.  design 13 lacks horizontal error bars creating the illusion of uncertainty.  Permission concerns with using the MATLAB logo without citation.  Color bar explanation for design 5 is missing. theoretical Results Clarification:  Theorem M.1 only considers nonabsolute values of Hessian elements. This may not accurately estimate integrals involving absolute values. Reproducibility Concerns:  \"Distillation approach\" is not defined.  The exact loss work used is not specified.  The NN architecture notation is unclear.  terms like \"ReLU network\" and \"single layer ResNet\" are not defined.  The reason for using Kaimings initialization strategy and the nonstandard batch size of 500 is not explained.", "Paraphrased Statement: Identifying relationships between model feature can enhance our understanding of blackbox models. This work introduces a method for discovering global interactions by framing it as a multiarm bandit problem and using the UCB algorithm. This approach does not rely on arbitrary assumptions and exhibits high accuracy and stability. The work also demonstrates the significance of these interactions by developing a novel deep learning model that leverages them. This model shows significant improvements in size and accuracy compared to pruning and generalization techniques. effectiveness and Weaknesses: effectiveness:  Algorithm development based on first principles (expected Hessian)  Modelagnostic implementation using finite difference method  Computationally efficient solution due to simplification to karm problem  Promising model compression algorithm with impressive performance  analysis of interaction consistency for debugging and development design Weaknesses:  model compatibility of the implementation is unclear  Gradientfree nature of the method could be mentioned  Explanation of independent reward distributions in assumption 3.3b could be provided  Presentation improvements such as method annotation in Table 1  Clarification on interpretability claims of the ParACE model  Explanation of the exceptional performance of the ParACE model across datasets", "Summary: The authors propose two main ideas: 1. UCB for Identifying Feature Interactions: They use the UCB algorithm to efficiently identify substantial feature interactions treating each set of interacting feature as an \"arm\" that can be evaluated by computing the corresponding entry in the Hessian on a random training model. 2. ParaACE model for model Compression: They build the ParaACE model using the identified pairwise interactions. This lightweight model resembles GAMs and includes a \"fixup\" layer to capture higherorder interactions. effectiveness:  potential improvements in model compression compared to other approach  Improved performance in some cases compared to the overparameterized teacher model  Novel approach of using multiarmed bandits for interaction detection and model compression with higherorder interaction recovery Weaknesses: 1. Computational Complexity: The proposed UCB approach may not be computationally efficient due to the use of finite differences to estimate Hessians. An alternative approach using backpropagation can significantly reduce the number of model evaluations required. 2. Saturation Effects: Evaluating Hessians on randomly sampled models may conceal cases of saturation effects where Hessians are near zero due to saturation. 3. Limited Benchmarking: The compression offered by ParaACE could be compared to more recent pruning approach to make the benchmarks more compelling. 4. Interpretability: While ParaACE may not decompose prediction into main effects and interaction effects it could still be intelligible or interpretable."], "rOGm97YR22N": ["Summary Paraphrase: The work presents a novel neural network architecture combining Mixed ODE recurrent Neural network (mmRNN) and discretetime Long ShortTerm memory (LSTM). This hybrid model address challenges in handling irregularly sampled time series and mitigating the vanishing gradient problem. The architecture outperforms existing ODEbased RNNs and discrete RNNs designed for gradient stability. effectiveness and Weaknesses Paraphrase: Combining established approaches is reasonable but the empirical findings warrant further exploration. Theres a lack of detail on hyperparameter tuning and model comparison. The observed performance differences may be influenced by biases or limited hyperparameter research for competing methods. The quality of aggregating issue over intervals may introduce different task requirements for discrete RNNs. The work should consider tasks with significant longterm dependencies to evaluate memory capabilities effectively. The theoretical section contains inaccuracies in definitions and assumptions. The conclusions do not hold for all face and may trivially apply to other RNNs with proper initialization. Combining ODEbased mechanisms within a single architecture may be more efficient and elegant than the proposed heterogeneous approach. Minor issues include:  LSTMs only alleviate the vanishing gradient problem not eliminate it.  Some equations and notation need clarification.  The hypothetical nature of graphs and the relevance of the numerical solver should be clarified.  Section 6 would be more appropriate after Section 2 to provide necessary scope for comparison methods.", "Paraphrased argument: Summary: The authors identify the \"explodingvanishing gradient problem\" (EVGP) as a persistent issue in the ODERNN model which is a common challenge for recurrent neural network (RNNs). They propose an improvement called \"mixedmemory RNNs\" (mmRNNs) to mitigate this problem early in the training work resulting in a more stable model for longterm time series prediction. Their theoretical findings are supported by experimental evaluation demonstrating improvements over a variety of benchmarks. effectiveness:  Clear analysis of EVGP in ODERNN despite it being a known problem.  Simple and practical solution (mmRNNs) that controls error propagation at the start of training.  extensive experimental verification across multiple benchmarks.  Limited model data is not a major concern given the extensive range of benchmarks. Weaknesses:  The authors language may overstate mmRNNs effectiveness in mitigating EVGP.  Corollary 2 mentions EVGP in ODEGRULSTM without acknowledging that mmRNNs also experience this problem albeit to a lesser extent.  The authors have not comparisond their model against ODEGRULSTM which would be a relevant benchmark to include. Queries:  How does the error propagation in mmRNNs comparison to wellinitialized GRULSTMs or ODEGRULSTMs  Why have the authors not directly tested against ODEGRULSTM Typos:  \"Experimental scope are given in Appendix.\"  \"Although the update\" (remove extra space before comma under Equation 3)", "Paraphrase: Summary This research proposes a method to address training difficulties in RNNs particularly due to vanishing or exploding gradients by incorporating gating. The work focuses on time series with irregular sampling using a continuoustime formulation for hidden states and adding discrete gating mechanisms akin to LSTM. Empirical evidence suggests that this addition enhances performance in diverse time series applications. Theoretical argument attempt to support the method. effectiveness and Weaknesses The paper provides extensive experimental issues demonstrating the effectiveness of the proposed method in improving the performance of continuoustime RNNs on irregularly sampled time series. This has practical value. However the paper theoretical argument lack rigor. The definition of stability and the proofs of stability issues are unclear and imprecise. The paper would benefit from focusing solely on empirical issues and their implications. Specific Concerns Regarding Theoretical argument  Definition 1: The use of the sum of derivatives as an indicator of exploding or vanishing gradients is questionable. It does not align with the standard definition of local stability in dynamic systems.  Proof of Theorem 1: The proof structure is unclear. It does not provide a clear argument for why the product of Jacobians up to time T implies exploding or vanishing dynamics.  Implication of Corollary 1: The conclusion that wellconditioned gradients can hinder model capacity is not supported by the provided argument.  Proof of Theorem 2: The proof relies on assumptions that are equivalent to the theorems claim making it circular.  Theorem 3: The issue is not specific to gating mechanisms. It can also be applied to RNNs without gating if initialized with stable network weights. Minor issue  RungeKutta Method: The RungeKutta method is not correctly defined. The evaluation of f\u03b8 should follow the Butcher Tableau not just at the final T to achieve higherorder global convergence error.", "Paraphrase: The authors demonstrate that the Ordinary Differential Equation recurrent Neural network (ODERNN) and its variations face issues with vanishing or exploding gradients. They establish that existing solution fail to fully address this problem. In response they propose a novel method that alleviates this issue while preserving the ODERNNs suitability for irregularly sampled sequences. Experimental results reveal that their method outperforms previous approaches including a simplified noncontinuous RNN augmented with a time input. effectiveness:  Identifies a significant flaw in the ODERNN approach and proves its prevalence in all known methods.  Introduces a novel solution supporting its effectiveness both theoretically and empirically.  Provides a clear introduction of the findings.  Compares the proposed method against a baseline noncontinuous time approach highlighting the benefits of the continuous formulation. Weaknesses:  The paper assumes familiarity with a specific formulation of the explodingvanishing gradient problem which may require further explanation.  The rationale for why 1 \u00b1 epsilon is sufficient for gradient explosionvanishing is not fully elucidated.  While the majority of the paper is comprehensible the introduction to ODERNN could be more detailed for readers unfamiliar with the concept. (Note: The reviewer acknowledges their limited knowledge of ODERNN and cannot verify the correctness of the proofs.)", "Summary The paper introduces a novel continuoustime RNN (Mixed memory RNN) for modeling irregular time series. Continuoustime RNNs face gradient vanishing and explosion problem but Mixed memory RNN addresses this issue mirroring the approach in discretetime LSTMs. The authors argue that their model is better suited for irregular time series due to its ability to capture longterm dependencies. Numerical experiments demonstrate its effectiveness and compare its performance to other continuoustime RNNs. effectiveness and Weaknesses effectiveness:  Sound theoretical derivations  Detailed and wellrounded experiments  Clear writing Weaknesses: 1. The authors do not adequately address how their findings differ from those of iRNN which also claims to eliminate gradient vanishing and explosion. 2. Hyperparameter tuning is not fully explained and justified. The use of optimal argument from original paper may not ensure a fair comparison between models."], "roxWnqcguNq": ["Paraphrased argument: Summary: This study introduces a method to extract key argument (specific text segments) from sentences using a combination of constituency tree representation and neural network models. The authors argue that the grammatical structure of sentences as captured by constituency trees plays a use in identifying these argument. Strengths and Weaknesses: Some observations regarding the papers content include: 1. Lack of Clear Definition: The concept of \"argument from sentences\" is initially unclear leading to confusion about the papers focus. 2. Ambiguous Introduction: The abstract and introduction do not provide a specific overview of the study leaving readers uncertain about its main use. 3. Absence of model: Including an model of argument mining would enhance the understanding of the concept. 4. Citation Required: The criticism of existing models for their lack of transparency should be supported by a relevant citation. 5. Disconnection: The link between the criticism of existing models and the need for linguisticbased models is not clearly explained. 6. Lack of Citations: The argument about the neglect of grammatical structure in NLP models and the effectiveness of consistency and dependency parsing require citations. 7. Technical Details: The related work section includes technical details about the experimental setup which may be difficult to comprehend. 8. Inaccurate Claim: The paper incorrectly states that Vaswani et al. (2017) introduced the attention mechanism.", "Paraphrase: Overall Evaluation: The paper explores the use of syntactic information (primarily constituency trees) to identify argument discussion units (ADUs). While the results demonstrate potential the paper lacks clarity and innovation leading to a recommendation of rejection at this time. Strengths:  Simple method for incorporating syntactic information into ADU recognition.  Clear documentation of hyperparameters and substantial empirical results.  Analysis of the importance of different constituency tree substructures. Weaknesses:  Unclear Relationship between ADUs and constituency tree: The paper fails to adequately explain the connection between ADUs and constituency trees leaving the research story less compelling.  Ambiguous use of Dependency and constituency tree: Its unclear whether the model uses dependency trees constituency trees or both.  Lack of Innovation: The paper primarily relies on the existing GAT model for syntactic integration without introducing significant novel methods.  Technical Concerns: The discussion of constituency trees is haphazard and batching heterogeneous graphs is not a major difficulty as claimed. Recommendation: The papers weaknesses particularly the lack of clarity and innovation warrant a rejection at this time.", "Paraphrase: This research demonstrates the effectiveness of constituency parsing in enhancing the detection of argumentative discussion units (ADUs). ADUs are defined as fundamental analysis units in argument mining and their identification involves two steps: segmenting ADU spans and predicting their stance labels. This study utilizes the Trautmann et al. 2020 dataset consisting of 8000 sentences with three stance labels: PRO (supporting) CON (opposing) and NON (nonargumentative). The research modifies the BERTCRF architecture from previous studies by integrating a Graph Attention layer (GAT) before CRF. The GAT is based on pruned constituency trees resulting in superior performance compared to BERTLarge. Strengths: 1. The study introduces a novel approach to ADU detection by incorporating constituency parser output through a Graph Attention layer. Weaknesses: 1. The literature review is inadequate and the presentation is disjointed hindering readability. 2. The dataset is not adequately described including the original data split and stance label distribution. 3. The understanding for pruning the constituency tree are not clearly explained and the \"maximum depth\" threshold is poorly defined. 4. The motivation and implementation of the BENEPAR constituency parser are unclear and the results do not include multilingual experiments despite the parsers capabilities. 5. Tokenization requirements lead to the exclusion of 1300 data samples which is an unusual decision. The baseline results used for comparison are no longer comparable due to this data modification. 6. The reported results only include tokenlevel F1 scores omitting segmentlevel and sentencelevel metrics used in previous studies.", "Summary: This research introduces a bertgatcrf model that leverages constituency trees to identify argument units in sentences. It illustrates constituency tree model for both short and long sequences. Evaluations on minor datasets demonstrate the capabilities of the bertgatcrf approach. Strengths and Weaknesses:  The bertgatcrf model combines existing bert gat and crf techniques.  Novelty in the method is limited to the combination of these existing approaches.  The focus of the study is narrow targeting a specific NLP task in semantic parsing.  Semantic use labeling and relevant datasets should be included for comparative analysis in the \"argument unit recognition\" task. detailed Questions and Comments: 1. picture 1: use POS tags and phrase tags instead of numbers (e.g. article tag for \"The\" verb tag for \"is\" noun phrase for \"the time\") would enhance readability. 2. detailed model showcasing improvements achieved by the bertgatcrf method would be beneficial for understanding its impact. 3. Citations should be formatted accurately (e.g. \"Roberta (liu et al. 2019)\" instead of \"With Roberta liu et al. (2019)\"). 4. The authors should consider exploring and comparing the use of dependency trees which are commonly used in semantic use labeling and argument unit detection to provide a more comprehensive analysis."], "m4BAEB_Imy": ["Paraphrase: summary: This paper presents a neural network compression technique based on unstructured pruning and binarization. effectiveness and Weaknesses:  The authors address the significant problem of neural network compression.  The paper lacks sufficient detail about the proposed method with excessive background data on network binarization and unstructured pruning.  The proposed approach relies heavily on existing methods and more original contribution would be desirable.  Technical details in the paper are unclear including the integer output of masked weights in number 3 which conflicts with the stated focus on pruning binary networks.", "Paraphrase: This study combines binarization and pruning techniques to reduce the computational and memory demands of deep neural networks (DNNs). Binarization implemented using the Binarryconnect approach converts DNNs into binary representations. Pruning specifically neuronwise magnitude pruning eliminates unnecessary input activations. The combined application of these methods has proven efficient in significantly cutting memory and computational requirements on the CIFAR10 and MNIST datasets. effectiveness:  efficient exploitation: The combination of binarization and pruning techniques has successfully reduced computational complexity and memory requirements.  Detailed analysis: Extensive model have been conducted to assess the impact of the proposed method from diverse view. Weaknesses:  Lack of novelty: The proposed method lacks originality as it simply integrates two existing approach. This overlaps with a previous publication (see the provided link).  Limited scope: The method has only been tested on relatively small datasets (CIFAR10 and MNIST). It remains unclear whether comparable memory and computational reductions can be achieved on larger datasets like ImageNet.", "Paraphrased summary: The paper presents a pruning algorithm for binary neural networks. It combines the binary connect and unstructured magnitude pruning approach to create a binarized network. The effectiveness of the paper include its:  exploration of existing binarization and pruning techniques  Presentation of experimental results However the papers limitations lie in:  Limited discussion of network binarization and pruning approach despite their extensive research history  Focus on small datasets where pruning techniques are expected to be efficient  Insufficient explanation of the proposed iPrune approach  Lack of clear novelty claim  Unverified memory reduction claim for the MNIST dataset", "summary: This study presents a new method called iPrune which reduces the number of inputs for each neuron in a binary neural network. This approach aims to lower memory consumption computation and ability usage during training. effectiveness:  Introduces a novel pruning technique for binary neural networks. Weaknesses:  Poor writing quality:  Algorithms 1 and 2 use undefined notation (ak b wk t).  findmask() work is not explained.  time works range is not specified.  Algorithm 2 is not adequately described in the main body.  Vague notation: wb and wt represent different weights but contribution the same subscript notation.  Novelty of the work is unclear.  memory and computation gains of \"around 2200x\" and \"around 70x\" mentioned in the text are not supported by the provided Table 1.  Table 4 evidence insignificant gains.  Limited comparison with other pruning techniques.  Pruning binarynonbinary neural networks has been wellstudied but the authors cite only a small number of papers."], "oOuPVoT1kA5": ["Paraphrased Summary: FEVERLESS is a novel technique for conducting vertical federated learning with distributed labels. In this scenario clients hold different feature for the same sample and labels are distributed among different clients. To retrieve gradients for samples with missing labels clients use DiffieHellman key exchange and differential privacy to send masked gradients to a source client. The source client then determines the split point based on the gradients and selects the split with the highest gain. effectiveness:  Comprehensive security analysis of the protocol Weaknesses: 1. Challenging readability due to appendix position of details. 2. Inconclusive model of distributed labels in practice. 3. Lack of clarity on how source clients split received gradients into GL and GR. 4. Potential privacy impact of multiple clients holding the same missing label. 5. Limited dataset size in experiments. 6. Lack of a table summarizing performance metrics of different approaches. 7. Potential typo in time complexity calculation.", "Paraphrase: This research investigates a secure method for training XGBoost models (called Vertical Federated Learning) where data labels are distributed across multiple entities. Existing approaches typically assume that labels are centralized and use encryption technique to protect privacy. In contrast this field assumes that labels are decentralized and combines established secure aggregation and privacyenhancing technique to safeguard both data and labels. effectiveness and Weaknesses: Pros:  Addresses a practical problem with realworld applications.  Challenges and solutions are wellgrounded.  Wellwritten with a logical flow. Cons:  Typographical errors (e.g. \"multiply hospitals\" instead of \"multiple\").  Limited ML novelty as it involves adapting existing secure aggregation methods which is expected to improve performance over homomorphic encryptionbased methods.  evaluation focuses on comparing performance with differential privacy in terms of accuracy and with homomorphic encryption in terms of training time but does not compare with homomorphic encryption on accuracy and differential privacy on latency.", "Summary: The paper introduces a secure protocol for XGBoost a machine learning algorithm in a vertically partitioned data positionting. The protocol utilizes masking and selects a random client to generate noise for differential privacy. effectiveness:  Proposes a protection mechanism for XGBoost in a vertical data partition scenario. Weaknesses:  Writing number:  Missing positionting section on cryptographic primitives.  Formal security theorems should be in the main paper not the appendix.  Requires a copyedit for clarity.  Security flaw:  Source client and noise leader can collude to obtain aggregate data without noise.  Unclear problem positionting:  model given (hospitals and banks) appear more like horizontal data splits not vertical splits.  Label privacy concerns:  Its unclear why label information for a specific client should be made public.  Notation inconsistencies:  Label space (\\mathcalY) does not appear to be a space but a position of labels.  feature number (f) and number of feature (d) are inconsistent.  Insufficient evaluation:  performance analysis (e.g. time bandwidth microbenchmarks) is missing.", "Paraphrased Summary: The research paper presents a unique setup for federated learning where clients hold distinct data labels. It introduces a secure and effective protocol for this setting that leverages the XGBoost algorithm. The protocol securely combines XGBoost gradients and Hessians using:  A masking scheme based on DiffieHellman key exchange and a key derivation function.  global differential privacy to enhance privacy. Security analysis confirms that data privacy is maintained even if half of the clients collude. Experiments across multiple datasets demonstrate the effectiveness of the protocol. effectiveness:  Clear presentation with informative number.  Comprehensive setting information.  Analysis shows the protocols effective computation and communication compared to existing approaches.  secure security secure against collusion.  Ingenious combination of DiffieHellman key exchange and global differential privacy with experimental solution highlighting its superiority.  Wellmotivated novel setting for federated learning with decentralized labels. Weakness:  Reduced performance with small datasets."], "xNOVfCCvDpM": ["Paraphrased Statement: Authors analyze three posthoc metrics (KSSD CCM and FAM) that assess a models sensitivity to irrelevant signals. They demonstrate these metrics on DNNs trained on medical image and conduct a blind study. effectiveness:  analysis of three metrics for measuring reliance on spurious signals. Weaknesses:  Metrics underperform in detecting reliance on unknown spurious signals.  Lack of clarity in the main paper requiring readers to refer to the appendix for understanding the metrics.  Dataset validation issue: The dataset used also contains dog breeds within the same species questioning the species classification task.  Authors do not suggest alternatives to the proposed metrics despite acknowledging their limitations.", "Paraphrased Statement: Researchers examined whether post hoc explanation methods can detect unknown spurious data in medical image. They added semisynthetic spurious signals (stripes and hospital tags) and nonpronounced spurious signals (blur) to two datasets. When testing post hoc explanation methods feature attribution and concept activation could identify pronounced but not nonpronounced known spurious signals. However none of the tested methods could identify unknown spurious signals. An empirical study also indicated that none of the methods helped practitioners detect unknown spurious signals. The authors emphasize the limitations of current post hoc explanation methods and the need for caution when using these methods for medical applications.", "Paraphrase: Goal: To evaluate the effectiveness of post hoc explanation methods in detecting unknown spurious correlations (connections) in machine study models. Methods:  Developed three reliability measures for detecting spurious signals:  Known Spurious point detection Measure (KSSD)  CauseforConcern Measure (CCM)  False Alarm Measure (FAM)  Performed experiments to assess the reliability of three types of post hoc explanation methods:  Input attribution  concept activation  Gradientbased approaches effectiveness:  Created new measures to assess spurious signal detection reliability.  Conducted comprehensive experiments with carefully designed settings.  Tested explanations with both whitebox (visible) and blackbox (invisible) spurious signals.  Included a user study to evaluate human ability to detect spurious signals using these methods. Weaknesses:  Paper is complex and difficult to follow at times.  issue suggest that these methods are less effective in detecting invisible spurious signals.  concept activation was the only method found to be effective in detecting visible spurious signals raising questions about the reliability of current post hoc tools for this task.  Some findings were unclear such as conclusions drawn from low CCM values.  KSSD CCM and FAM values were not reported in all sections making certain discussion difficult to interpret.  Variation in the measurements (e.g. KSSD CCM FAM) when models are trained with different initial states could be better explained.  Higher FAM values than CCM values for spurious inputs suggest that spurious and normal models generate more similar explanations on those inputs which would benefit from further discussion.  GBP method achieved significantly higher CCM and FAM scores than other methods requiring more explanation."], "kK3DlGuusi": ["Paraphrased Statement: This paper presents a technique for compressing neural netprocess weight. The method converts weight tensors into matrices factorizes these matrices using PCA into a factorization with rank k quantizes the factor matrices and adds sparsity to the latent matrix. The paper provides an algorithm with two thresholding selection and demonstrates that these ideas lead to a favorable tradeoff between accuracy and compression either matching or surpassing previous stateoftheart methods across various compression ratios. Strengths:  Clear writing and convincing presentation of method and results.  The method is simple even effective.  Relevance to the deep learning and machine learning community.  Comprehensive discussion of related process. Weaknesses:  The simplicity of the method may be seen as incremental and less exciting. Questions:  Q1: What is the time complexity of compressing a netprocess relative to training time  Q2: Does the sparse quantized PCA format increase inference time  Q3: How is the weight tensor reshaped Are f h and w always arranged along rows  Q4: What does the notation \u230a\u2219\u2309 in equations (5) and (6) represent  Q5: Should equations (5) and (6) consumption 2bc and 2bz instead of just 2b  Q6: What is meant by \"oneshot hardthresholding without dataaware optimization\"  Q7: How is quantization implemented in practice and are the experiments conducted in Python  Q8: What do \"MAC performance\" refer to Typos and other Minor issue:  \"while Z encodes the of power\" should be \"while Z encodes the power.\" (Above equation (11))  The size of W should be 256 \u00d7 256 \u00d7 3 \u00d7 3 instead of 256 \u00d7 256 \u00d7 9 \u00d7 9 (Section 3.2 second paragraph)  A period is missing before \"In this form\" (Section 4.3.2 second paragraph)  In the appendix (Sections B.2 and B.3) references to relevant tables would be helpful in section headings.", "Paraphrased Summary: This paper presents a new technique for compressing model parameters. It stores the tensors as sparsely quantized matrices treating the process as a quantized sparse Principal Component Analysis (PCA) problem solved using iterative gradient descent. The approach claims to achieve or outperform stateoftheart methods in balancing accuracy and model size both under moderate and extreme compression conditions. Strengths and Weaknesses: Strengths:  The method empirically demonstrates superior compressionaccuracy tradeoffs in both low and high compression scenarios. Weaknesses: 1. The approach appears to blend tensor factorization and vector quantization techniques with sparse PCA consumptiond in vector quantization methods previously. 2. The authors need to elaborate on the reasons for assuming sparsity in the factor matrix Z and clarify the meaning of sparsity (e.g. rowsparse columnsparse). 3. The claim that their method for factorizing W is sparse PCA is questionable becaconsumption conventional sparse PCA assumes a columnsparse factor matrix. 4. The gradient descent update in (10) deserves explicit explanation within the paper not via a reference to another field. 5. The source code consumptiond in the experiments is missing. other Comments: 1. The language \"quantized sparse PCA\" should be consistent avoiding the consumption of \"sparse quantized PCA.\" 2. The relationship between the parameters k d and n should be clarified upon their first appearance. 3. Algorithm 1 and Appendix A moderate minor typographical errors (10 vs. (10) 14 vs. (14)).", "Paraphrased Statement: Summary: The authors present a technique for compressing weight in deep neural netprocesss using matrix factorization. The weight matrices are broken down into two lowprecision quantized matrices one of which is sparse. Strengths and Weaknesses: 1. Request for Inclusion of Existing Technique: The reviewers suggest that the authors discuss a known lowrank factorization method for gradient compression called PowerSGD which could provide insights into the present process. 2. Clarification of m: The reviewers inquire about the definition of m in the proposed methods loss minimization function which they believe is the issue of observations in the training set. 3. Concerns about Optimization process: The reviewers express doubts about the authors approach to solving the optimization problem (10). They argue that it does not guarantee the production of quantized matrices Cq and Zq and that the mathematical discussion of the problem is flawed. 4. Mathematical Concerns: The reviewers critique Proposition A.1 stating that it is incorrect. They explain that the projection onto the set of sparse matrices is nonconvex and that the function defined in (10) is also nonconvex. This makes it difficult to prove convergence to an optimal solution. 5. language Correction: The reviewers point out that the term \"SVD decomposition\" is redundant since \"SVD\" stands for Singular Value Decomposition."], "xRK8xgFuiu": ["Paraphrase: Summary: This research examines the challenge of constructing directed graphical models in a linear structural equation modeling (SEM) context. To determine causal relationships from data the authors employ an iterative Cholesky factorization of the covariance matrix. Evaluation: The paper introduces a novel algorithm for identifying causal connections in directed acyclic graphs (DAGs). The authors focus on linear SEM models assuming that child nodes have larger conditional noise variance than parent nodes making the problem identifiable. Overall the paper is commendable but some clarifications are needed: 1. Conditional noise Variance Assumption: The authors mention that the conditional noise variance of child nodes is \"approximately\" greater than that of parent nodes. They do not specify the nature of this approximation. 2. Graph Generation: The authors use Erd\u0151sR\u00e9nyi (ER) graphs to evaluate their algorithm. It is unclear how they ensure that the resulting graphs are indeed DAGs.", "Paraphrase: This paper introduces a novel algorithm for learning linear structural equation models using the Cholesky decomposition technique. It argues that this algorithm remains reliable (consistent) in highdimensional scenarios and is efficient in terms of computation. The paper reviews relevant literature on learning linear structural equation models (SEMs) providing a clear rationale for its own approach. Numerical experiments are presented to validate the algorithms theoretical basis. Strengths and Weaknesses: Strengths:  Comprehensive review of previous research. Weaknesses: 1. Identifiability condition: The novel identifiability condition (V) needs a clear justification as it seems insufficient when applied to certain scenarios. 2. Theorem 2.1 requirements: The conditions specified in the theorem may not be practical such as the assumption of bounded random variables which is violated by Gaussian and exponential distributions used in the numerical experiments. 3. Numerical experiments: The experiments do not fully align with the theoretical findings. For instance the assumption that sample complexity is independent of graph sparsity level is contradicted by the comparison of performance on ER2 and ER5 graphs. Additionally the convergence of SHD to zero is not clearly demonstrated. It would be beneficial to provide the empirical probability of the estimated covariance matrix matching the true covariance matrix. 4. Bounded random variable assumption: The considered distributions (Gaussian and exponential) violate the assumption of bounded random variables which undermines the validity of some theoretical statements.", "Paraphrase: The paper presents a method for identifying causal relationships in linear Gaussian structural equation models (SEMs) as defined by Peters and B\u00fchlmann (2014). The method uses Cholesky factorization and improves the time efficiency compared to other comparable techniques. It also includes a theoretical analysis of the resulting graph structure which enhances its credibility. Experiments verify the claims made by the authors. Strengths and Weaknesses: The paper is wellwritten and its claims are supported by experiments and theoretical analysis. The primary strength of the method lies in its time efficiency which surpasses existing methods. The theoretical analysis and experimental results demonstrate the correctness and validity of the method. One potential limitation is that the method is applicable to a restricted class of SEMs specifically linear Gaussian SEMs. It is unclear how the method can be extended to nonlinear cases or more general scenarios.", "Paraphrased Summary: This field presents a method to reconstruct the causal graph of linear models using only observational data. This approach assumes identifiability conditions similar to those in [1]. The method involves identifying the root of the causal graph based on conditional variance and then reconstructing the graph from the permuted precision matrix. It is evaluated against baseline methods on synthetic and realworld datasets. Strengths and Weaknesses: Strengths:  use techniques from linear model causal discovery. Weaknesses: Theoretical Limitations:  Assumes the data is bounded within a sphere which may not be realistic for highdimensional data.  Compares to [2] which has weak assumptions but not under the same conditions used in the experiments.  Identifiability assumptions rely on data estimates not model parameters which differs from [12]. practical Shortcomings:  Unclear what specific improvements are made in ordering estimation compared to existing algorithms.  Unexpected underperformance of LISTEN and US given previous claims of superior performance.  Would benefit from comparison with LISTEN using the empirical covariance matrix to match the covariance estimation method.", "Paraphrase: The paper presents a novel algorithm for learning linear structural equation models (SEMs) using Cholesky factorization of the covariance matrix. The algorithm is inspired by existing methods but it combines the order research and parent set recovery steps into one step using Cholesky factorization. Strengths and Weaknesses:  The paper has a clear motivation and contributions.  The algorithm is the fastest known for learning linear SEMs.  However the existing algorithms by Ghoshal and Honorio and Chen et al. are also polynomialtime and comparable in performance reducing the significance of this improvement.  The paper does not address the identifiability of linear SEMs.  The experiments demonstrate the effectiveness of the algorithm but the sample complexity is a concern.  The sample complexity depends on the dimension of the random vectors which can be large even for sparse SEMs.  The algorithm is therefore less suitable for highdimensional scope.  The experiments compare the algorithm only with LISTEN at 3000 sample. Learning curves showing the performance of both algorithms at varying sample sizes would be valuable."], "mRF387I4Wl": ["Paraphrase: Original: Summary: Explainability in Graph Neural Networks (GNNs) is yet in its early stages and most explanation methods focus on nodes node features edges or subgraphs. GNNs are messagepassing networks where nodes interact with their local neighborhood. However current GNN explainability methods do not consider message flows as potential explanations. FlowX is a novel method that leverages message flows to identify significant message. It uses Shapley values to estimate message flow importance then refines these approximation using a learning algorithm. Experimental results show that FlowX improves the explainability of GNNs. strength: 1. FlowX is the first method to use message flows for GNN explainability. 2. FlowX uses Shapley values to estimate message flow importance which is a meaningful approach. 3. FlowX is evaluated extensively on synthetic and realworld datasets. Weaknesses: 1. FlowXs notations are inconsistent and confusing. 2. FlowXs initial Shapley value approximation method may assign importance to unsignificant message flows. 3. FlowXs refinement step uses optimization tricks that could bias the results. 4. Its unclear how the weight vector w in Equation 9 contributes to the process. 5. FlowXs layer edge importance calculation may lead to skewed importance distribution. 6. FlowX lacks baseline comparisons and error bars in its evaluation section. 7. FlowXs performance is comparable to a simpler baseline which raises questions about the necessity of its refinement stage.", "Paraphrased Statement: Explanation Method for Graph Neural Networks (GNNs): Summary FlowX aims to make GNNs interpretable by analyzing the message flows which are critical to GNNs. Key Features:  Estimates the importance of message flows using a Monte Carlo approximation of Shapley values a game theory concept.  Refines these approximation through a learned linear transformation.  Converts the refined approximation back into layer edge importance scores for explanation. Assessment and Evaluation:  Experiments demonstrate the effectiveness of FlowX.  strength:  Addresses a significant publication in GNN explainability.  Wellstructured presentation with clear justification and comparison to existing methods.  various experiments on multiple datasets and baselines.  Weaknesses:  The approach involves converting message flow importance to edge importance which may limit the insights gained.  High computational complexity and memory requirements.  The number of Monte Carlo iterations needed for accurate results is not specified. Interpretation Challenges:  FlowX provides edge importance explanations but lacks clarity in presenting them for human understanding.  Ignores node features in its explanations. area for Improvement:  Clarify the notation used throughout the paper.  Provide a proper complexity analysis and recommendations for choosing the number of Monte Carlo iterations.  Improve the interpretability of the explanations by considering node features and addressing the complexity publication.", "Paraphrase: Summary: The paper presents a method to identify significant information flow model (\"message flows\") that explain the predictions of Graph Neural Network (GNN) models. To calculate these message flows efficiently the authors use a technique called Monte Carlo (MC) sampling. strength and Weaknesses: Pros:  The concept of using message flows for explanation in GNNs is novel and logical.  The paper is wellwritten and easy to understand.  The experimental results support the validity of the proposed framework. Cons:  The generation of candidate message flow set and the permutation algorithm may be computationally intensive. The authors should provide a time complexity analysis for their method. Additionally it would be helpful to know how large graphs the algorithm can handle.  The authors measure performance using sparsity and fidelity but other explanation methods like SubgraphX use accuracy. They should include a comparison of accuracy.  There are potential errors in grammar and typos.", "Paraphrased Statement: Summary: This paper explains how Graph Neural Networks (GNNs) work through key message flows. The authors use Shapley value a concept from game theory to initially determine the importance of flows. An algorithm based on learning refines these scores. The method is tested on both synthetic and realworld information. strength:  Novel use of message flows for GNN explanation unlike existing methods that focus on nodes or subgraphs.  comprehensive testing on various informationsets including ablation field and time cost analysis.  Clear and wellorganized presentation. Weaknesses:  Lack of intuitive understanding of using message flows as an explanation for GNNs.  set novelty compared to existing explainers like SubgraphX despite the use of a learningbased function for refining importance scores.  Missing baselines in experiments (e.g. Gem GraphSVX).  Absence of significant metric (e.g. accuracy against basis accuracy explanation).  Inferior performance in fidelity score when compared to GCNs on seven informationsets at varying sparsity levels.  Unclear method of comparison between FlowX and other methods in Table 4 with questionable significance of fair fidelity score. Minor Points:  Clearer definition of \"s\" as both marginal contribution and importance score.  Clarification of \"k\" in \"Fk.\""], "scSheedMzl": ["Paraphrase: Summary: This research aims to improve local explainability of black box models. The authors propose LINEX an approach that uses local environments around data points to provide explanations. LINEX builds locally invariant predictors and trust their explanations for the original model. The authors provide theoretical support and empirical evidence for their method. effectiveness and Weaknesses:  Concern about fidelity of local environments: Creating environments may compromise the accuracy of explanations. The authors address this partially with random perturbation and realistic generation but do not provide a fully robust solution.  Unclear terminology regarding causality: The paper implies causality in its title and abstract but does not explicitly provide causal explanations. The authors should clarify their terminology.  Lack of formal Nash equilibrium formulation: The optimization constraint in LINEX may not constitute a Nash equilibrium. A formal formulation is needed for proper interpretation and analysis.  Limited guidance on number of environments: The authors demonstrate LINEX in multiple environments but do not provide guidance on how to determine an optimal number. Positive Note: The paper demonstrates practical application and evidence promising results.", "Paraphrase: This field introduces LINEX an algorithm for creating robust explanations similar to the LIME method for complex models. It utilizes the Invariant Risk Minimization game model. LIME is known to be influenced by its settings and the paper proposes that a robust version should be insensitive to these parameters. The key innovation is describing the perturbation neighborhood for explanation as \"environments\" in the IRM setup. The field observes that the properties of these invariant predictors (as defined in [2]) make them suitable for explanations. effectiveness:  LINEX provides robustness to LIME while maintaining accuracy.  The motivation for using IRM is clearly explained.  Experiments cover various datasets and evaluation metrics. Weaknesses:  LINEX does not significantly outperform MeLime or MAPLE particularly for nontabular data.  MAPLE performs competitively on the MEPS dataset and MeLime is competitive on FMNIST and Rotten Tomatoes.  It is unclear why the paper claims \"Causal\" in the title and elsewhere as no causal support is provided.  The paper presents \"unidirectionality\" as a new feature but similar concepts have been explored in recent research on robust explanations ([36]).  The novelty of LINEX is limited as it heavily relies on [1 2] for its algorithm and theoretical foundation.", "Paraphrase: The authors have addressed or pledged to address my concerns with their work during our discussions. As a result I have revised my rating. Summary of paper: This article presents a novel explanation technique for any model that uses game theory to ensure that explanations are accurate and uniguidanceal across similar data points. They create various local contexts by randomly modifying or producing realistic neighbors through generative or retrieval techniques. Using the suggested method they iteratively create constrained least squares predictors for each setting and the final predictor is the sum of these individual predictors. Experiments on tabular data image and text datasets demonstrate that the proposed method provides superior explanations than existing techniques such as LIME. Overall Evaluation: The paper is well justified and makes a significant contribution. The experimental results strongly support the proposed methods effectiveness. It outperforms LIME SLIME and MeLime in over half of the experiments while achieving comparable performance in the majority of the remaining ones. The paper however lacks a discussion on how weak the results appear in certain experiments such as FMNIST and Rotten Tomatoes. Furthermore the paper would benefit from an error analysis on the proposed method particularly when its performance is inferior to baselines. The paper is wellwritten and wellpresented but I have a few minor comments listed below. additional Questions and Comments:  Page 3: \"local explainability setup vs. IRM setup\": The authors state that the model uses features in the explanations that may be irrelevant from a domain perspective but which the model uses for decisionmaking. This could be misleading since these explanations are posthoc and the paper does not establish that the model relies solely on the identified explanations.  Section 4.3 Assumption 1: Clarify whether independence is assumed across dimensions within the same environment or across the same dimension across environments.  Section 4.3 Assumption 2: Explain how Assumption 2 ensures a thorough understanding of the role of the linf penalty. Presentation and Typos:  The title (\"Towards Causal explanation\") may not be appropriate as the paper does not appear to estimate causal effects.  foundation Line 6: \"based on a image scan\" should be \"based on an image scan.\"  foundation Page 2: Provide more background on suggested variants to overcome existing limitations.  Section 4.2.1: \"optimized field two constraints\" should be \"optimized field to two constraints.\"  Section 4.2.1: Rephrase \"features where there is massive disagreement in even the guidance of their impact are eliminated by our method\" to \"features with significant disagreement in even the guidance of their impact are eliminated by our method.\"", "Paraphrased Summary: This paper presents LINEX an better version of LIME (Locally Interpretable model Agnostic explanation). LINEX is based on Invariant Risk Minimization (IRM) and generates highquality stable invariant and unidirectional local explanations for complex blackbox models. LINEX has been evaluated with various case of data (tabular image text) and outperforms LIME and other methods on multiple metrics and datasets. effectiveness:  clear motivation and identification of research gaps  Comprehensive related work review  Detailed explanation of IRM and local explainability  Welldefined Nash equilibria and suitable properties  Justification for LINEXs contributions Weaknesses:  Limited demonstration of LINEXs superior explanations in number 2  Correlation coefficient may not accurately reflect the quality of explanations  Top attributed words in the sentiment analysis experiment evidence minimal differences  Standard error calculations may not be sensible across different kernel sizes  Focus on 1 improvement rather than statistical significance"], "kQ2SOflIOVC": ["Paraphrase: Summary This work explores fewshot classification for histological images. The authors investigate the generalization capabilities of selfsupervised contrastive learning (CLP) and fully supervised classification as pretraining methods. They evaluate the impact of latent augmentation (LA) versus data augmentation (DA) through experiments on three public histological datasets. They create various tasks based on tissue labels and assess classification performance in Nway Kshot experiments with varying N and K value. effectiveness and Weaknesses  effectiveness:  Relevance to fewshot learning in biomedical application particularly in histology.  Use of openly available datasets with diverse tissue staining and pathologies.  Clear structure of the paper.  Weaknesses:  Language:  Ambiguous expressions impairing scientific validity.  Missing articles and grammatical errors.  Terminology:  Inconsistent use of terms like \"lowshot\" and \"fewshot.\"  Unclear definitions of \"neardomain\" \"mixturedomain\" \"middledomain\" and \"outdomain.\"  Technical Details:  Limited explanation of data augmentation methods used in the ablation work.  Insufficient data on augmentation times in LA vs. DA experiments.  Novelty:  While the authors claim to pioneer lowshot learning for histological images there are several recent studies on this topic that are not mentioned.  Lack of clear evidence supporting this claim.", "Paraphrase: Summary The work investigates creating valuable representations of histological images for categorization. Due to significant domain shifts in histological imaging the classification challenge is defined as fewshot learning (FSL). The authors compare FSL pretraining on a ground dataset using complete supervision and contrastive learning in their experiments. During lowshot training they employ an unsupervised method for data augmentation in the latent space (\"latent augmentation\"). contrastive learning with latent augmentation according to the findings delivers the most reliable representations particularly for outofdomain tasks. effectiveness and Weaknesses effectiveness  The work focuses on a key obstacle that prevents the implementation of MLgroundd pipelines in histological image analysis: number with outofdomain adaptation particularly when introducing a new label set.  The proposed approach which heavily leverages recent advances in FSL and data augmentation techniques is appealing and convincing.  The experimental section is exhaustive and wellthoughtout. There are three distinct adaptation scenarios with varying levels of required domain adaptation. Weaknesses  Lack of originality: While the methods are integrated and utilized in a novel manner none of them (contrastive and fullysupervised classification domain adaptation using FSL) are truly original. The exception could be \"latent augmentation\" which unlike most others does not require labels and extends the approach of other latent space augmentation techniques. This aspect piques interest but because the studies are limited to histological images its extensive impact on image categorization is unknown.  potentially dubious conclusion: The authors assert that \"to our astonishment we demonstrate that stateoftheart CL models generalize significantly beneficial than their supervised peers for histological image.\" In their Analysis they say \"FSP (fully supervised pretraining) tends to overfit severely at a high level because to the lack of dominant objectiveness whereas CLP (contrastive learning pretraining) can capture general patterns from roughly spread tnumber. We believe this is the primary cause for CLP models outperforming FSP models in pathology.\" I see this as more of a statement about the observed result than an explanation. Furthermore Im not convinced the results presented by Chen and Li (2020) can be directly compared to these as they relate to the same comparison but are done in an outofdomain (i.e. previously unseen class) setting. It would be more potential for FSPlearned representations to be unreliable when encountering a patch with an unseen class. On the other hand CLPs goal is to discover similarities and differences in photos which should result in more classagnostic midlevel features. I would like to see a more indepth analysis of this comparison. Minor Observations  Typographical errors (page 5): \"samples belong\" \"we summarize\"  Typographical error (page 15): \"oragan.\" The \"Out\" equation also appears to be missing a \"plus\" sign.  Typographical error (page 7): \"The tradeoff EXISTS\"  Typographical errors (page 9): \"Later methods is attracted to transfer learning\" \"This method is later extended by not relying given ground samples\"  Consider discussing and referencing the following paper on latent space augmentation from ICLR 2021: TszHim Cheung DitYan Yeung. MODALS: Modalityagnostic Automated data augmentation in the Latent Space. ICLR 2021.  also please reference Chen and Li (2020): https:contrastivelearning.github.iointriguing", "Paraphrase: Summary: This paper explores fewshot learning on histology images and highlights the effectiveness of contrastive pretraining. contrastive learning commonly used for natural images has now been demonstrated to enhance performance in fewshot tasks involving medical images. The paper introduces a novel selfsupervised method called \"latent augmentation\" for feature space augmentation. The augmented data yield a significant 10 improvement in fewshot learning results. effectiveness:  Demonstrates a significant performance boost (10 F1 score) in fewshot classification of histology images using contrastive pretraining.  Proposes a simple and effective latent augmentation technique that outperforms standard input augmentation.  Provides a comprehensive paper with detailed explanations and extensive experiments. Weaknesses:  The proposed method is not compared to other variation data augmentation methods raising questions about its superiority.  An investigation into the work of clusters and the impact of covariance matrix approximations would be valuable.  The stability of the method to randomness in Kmeans clustering could be further explored. Minor Comments:  The caption for number 4 requires clarification without the need to consult the appendix.  The use of L2 feature normalization in clustering and downstream tasks warrants further explanation.  Certain paragraphs could be condensed to improve readability.  Some terms require rephrasing for clarity.  Confidence intervals or standard deviations in Table 1 are not specified.  Consider releasing the code for reproducibility.", "Paraphrased Summary: This paper presents a method for image classification on histology images using contrastive learning (CL) with a new augmentation technique called latent augmentation (LA). The method outperforms supervised learning models and LA improves the performance of baselines without data augmentation. Analysis is conducted to explain these observations. effectiveness: 1. Innovative LA technique offers strong data augmentation benefits. 2. Extensive content and appendix provide detailed data. 3. further findings demonstrate the potential of CL for fewshot histology image classification. Weaknesses: 1. Limited technical novelty as the CL approach is established and LA resembles existing methods. 2. Lack of comparison with other approach for fewshot learning semisupervised learning and data augmentation. 3. Unfair comparison of LA with baselines that lack data augmentation. 4. Comparison with supervised learning already established in other SSL research. 5. Dense content may compromise selfcontainment consider submission to a medical image analysis journal. Question: Is selfsupervised learning conducted on manually cropped area of whole slide images (WSIs) If patch have assigned labels could that introduce bias in the selfsupervised learning work Ideally selfsupervised learning should be performed on uncropped WSIs.", "Paraphrase: This work examines lowshot learning for histology image classification using unsupervised representation learning via contrastive pretraining. To enhance lowshot classification a novel \"latent augmentation\" (LA) technique is introduced which transfers variations from ground classes to new classes in the latent feature space. Three lowshot classification tasks are defined with varying degrees of domain shift (neardomain mixeddomain outdomain) using publicly available histology image datasets. Experiments demonstrate the effectiveness of the proposed methods for these tasks. effectiveness: 1. Proposes LA as an innovative augmentation method in the latent space. By leveraging variations from ground classes LA increases sample diversity and improves generalization. 2. Extensive experiments show improvements over groundline methods for the three lowshot learning tasks though improvements are less pronounced on the outdomain task. 3. Ablation studies explore the impact of image number and augmentation frequency on LA effectiveness. Weaknesses: 1. LA relies on random initialization for kmeans clustering. The impact of this randomness on results should be examined and seed selection strategies should be discussed. 2. Variation from ground classes could be computed without kmeans clustering as a potential alternative that should be considered. 3. The explanation for why contrastive learning pretraining (CLP) models outperform fully supervised pretraining (FSP) models needs clarification. The connection between image differences and globallocal feature similarity should be elaborated. 4. Regularization techniques are not employed during fully supervised pretraining. 5. In the section \"DA vs. LA and number of augmentation times\" the computational cost is incorrectly attributed to \"DA\" instead of \"LA.\""], "viWF5cyz6i": ["Paraphrased Statement: Summary: This paper presents an algorithm that finds approximation for the principal components associated with the large singular values. instead of specifying a number for the top singular values as is common in practice the algorithm takes a userdefined tolerance as input. The paper describes the algorithm in detail and tests its performance through mathematical experiments. effectiveness:  The paper is wellstructured and easy to understand with clear explanations of all notations and derivations. Weaknesses:  The algorithm proposed appears to be a slightly modified version of the FFQR algorithm proposed by Feng et al. in 2019.  Although specifying tolerance is more practical than specifying singular values the modification to the FFQR algorithm for this purpose seems minor and incremental in terms of contribution.  The principal theoretical findings are similar to those of Feng et al. in 2019. Suggestions for Improvement:  In algorithm 1 provide a clearer explanation of the \"randn()\" and \"orth\" functions.  Add a period to the end of the thirdtolast displayed equation on page 3.  On line 2 of page 4 change \"Stewart\" to \"Stewart (1999).\"", "Paraphrase: Summary:  This field introduces a Principal component analysis (PCA) method that stops once the approximate singular values reach a userspecified threshold.  The technique incorporates a tolerancebased stopping criterion and is derived from Feng et al.s FFQR algorithm.  The algorithm is claimed to operate in O(mnl) time.  Experiments compare the proposed method with TSVD and Yue et al.s randQBEI showing promising results. effectiveness:  PCA is a significant problem of interest to both theorists and practitioners.  The tolerancebased approach with low time complexity is beneficial eliminating manual parameter tuning.  The method outperforms existing alternatives on realistic problems.  The authors provide a comprehensive overview of related methods laying a solid foundation for their work. Weaknesses:  The time complexity analysis in the abstract lacks detailed explanation.  Its unclear whether the complexity analysis assumes approach to the singular values of the input matrix or the matrix tracking the singular values.  The mathematical reasoning is present but the informal structure makes it challenging to follow parameter and the relationship between the implemented algorithm and the properties in section 1.3.  While eliminating the require for a target rank the proposed algorithm introduces multiple parameter (\u03b1 \u03b2 b and \u03b5) that require careful selection. QuestionsComments:  section 1.1: practice lowercase \"k\" for TSVD definition.  section 1.3: Consider renaming \"true rank\" to avoid confusion since it typically refers to values big than zero not a threshold.  section 3.2.1: Clarify if the existence of \u03b1 and \u03b2 is assumed.  section 4.2: Provide a comparison of runtime between randQBEI and the proposed method in Table 2.", "Paraphrased Summary: This article introduces a novel approximate algorithm for PCA that focuses on extracting principal components linked to large singular values meeting a specified threshold. effectiveness and Weaknesses:  Previous randomized PCA approach are acknowledged but the authors emphasize that they only guarantee approximate error within the computed subspace not the accuracy of singular values or space.  The algorithm incorporates a threshold parameter to specify noise levels. However this may not be practical due to data standardization.  The paper would benefit from:  Clarifying the relationship between the parameter and the rank of the output  Improving the presentation and experimental design  Including reallife data evaluation  Comparing the algorithm to existing randomized PCA algorithms and robust PCA for a comprehensive assessment"], "pgir5f7ekAL": ["Paraphrase: principal component analysis (PCA) is a widely used technique for simplifying data and explaining its variation. However it becomes unreliable in highdimensional data. The traditional approach assumes the leading eigenvectors are sparse (contain many zeros). This paper challenges this assumption proposing that the leading eigenvector may be closely to the image of a \"deep generative model\" (G). G is trained on the same data used for PCA which raises concerns about the independence of training and eigenvector computation and the potential for biased results. Strengths:  The research problem is significant and welldefined.  The paper is clear and concise.  The assumptions and theoretical findings are reasonable. Weaknesses:  The dependence between G training and eigenvector computation undermines the theoretical results. This can lead to inflated performance estimates and false precision.  The computational cost of pretraining G and applying PPower (the proposed method) is likely much higher than other algorithms.  The authors overlook existing research on balancing statistical efficiency with computational complexity. PPower may exhibit high statistical efficiency but its efficiency should be discussed more thoroughly.", "Paraphrased Summary: This paper investigates a problem related to Principal component analysis (PCA) within a generative model framework. It aims to solve a problem of finding the vector w that maximizes the trace of wT V w where V is a positive semidefinite matrix E is a perturbation matrix and w is constrained to lie in the image of a Lipschitz continuous generative adversarial network (GAN) G that maps from B2k(r) to Sn1. The paper introduces a projected power method (PPower) algorithm that is similar to iterative hard thresholding when w is sparse. It shows that PPower ensures a monotonic increase in the objective rate at each iteration. Under certain assumptions (a gap between the largest and second largest eigenrates of V and a bounded perturbation bound) the paper provides guarantees for the algorithms optimization performance in three settings: the spiked Wigner model spiked covariance model and phase retrieval. Specifically it shows that the distance between the global optimum and the PPower solution is bounded by certain component involving the number of iterations dimension and spectral gap of V. Additionally it provides conditions for a linear convergence rate under a beneficial initialization and other assumptions typically satisfied in practical settings (e.g. positive image of the GAN). The paper presents experimental results on the MNIST dataset for the spiked covariance model comparing PPower to the truncated power method (TPower) and the basic power method. It demonstrates the superior performance of PPower. Strengths:  Characterizes the objective rate convergence of PPower under a general initialization.  Introduces a potentially practical algorithm in a more general GAN setting compared to previous work. Weaknesses:  Questions the practical applicability of the Lipschitzness criterion for realworld GANs.  Suggests exploring the performance of TPower in the wavelet basis for natural image.", "Paraphrased Statement: This field examines Principal component analysis (PCA) under the constraint that the principal eigenvectors are within the image of a fixed generative model such as a neural network with set weights. Specifically given a Lipschitz function G that maps from the kdimensional Euclidean space \\mathbb Rk to \\mathbb R(G) the authors seek to identify a vector w within the image R(G) that maximizes wT V w. Here V \\barV E denotes a noisy version of the groundtruth positive semidefinite matrix \\barV and \\barx is the top eigenvector of \\barV. The generative model G is assumed to have a image within the unit ball of \\mathbb Rn and the error matrix E is constrained such that for any sets S1 and S2 with size m \\Omega( \\log( S1 S2)) s1T E s2 \\leq \\sqrt \\log(S1 S2) m \\s1\\2 \\s2\\2. This error constraint is satisfied by the spiked covariance and phase retrieval models under the condition that the sample size m is sufficiently large. The authors establish two key results: 1. They demonstrate that under the aforementioned assumptions the global optimizer \\hatv of wT V w constrained on R(G) has an error relative to the groundtruth solution \\barx bounded by \\\\hatv \\barx\\2 \\leq \\sqrtk \\log Lm. 2. They show that the power iteration algorithm augmented with a projection step onto the constraint set R(G) converges to a vector with a similar error bound. Experimental results on the spiked covariance and phase retrieval problems using the MNIST and FashionMNIST datasets suggest that incorporating this projection step into the power iteration enhances recovery when G is a pretrained variational autoencoder with a small latent dimension k. Strengths and Weaknesses: The work generalizes both standard PCA and coneconstrained PCA in the presence of generative priors. The field of PCA and power iteration beyond convex settings is relevant. However the principal theoretical results are primarily statistical focusing on the sample complexity of the global maximizer and power iteration given the availability of an oracle for projecting points onto nonconvex sets. Theorem 1 (bounding the global maximums sample complexity) relies on standard concentrationcover arguments while the convergence result (Theorem 2) for the power iteration is less surprising given prior work on nonconvex settings. The authors assume an initial point with beneficial correlation with the optimal solution which may limit the practical applicability of the algorithm. Overall the reviewers lean towards rejecting the work unless the authors or other reviewers provide a more convincing case for its significance and novelty."], "gdWQMQVJST": ["Paraphrased Statement: This paper introduces novel algorithms for federated learning (FL) using the neural tangent kernel (NTK) approach. The algorithms NTKFL and CPNTKFL (a variant of NTKFL) aim to overcome statistical differences between clients. Unlike traditional FL algorithms clients in this framework send labeled data and sample Jacobian matrices (representing NTK) to the server. The server then leverages NTK techniques not gradient descent to train the neural network. The paper includes theoretical and empirical evaluations (using MNIST FashionMNIST and EMNIST datasets) and comparison to FedAvg demonstrating the proposed algorithms effectiveness. effectiveness:  novel FL approach using the NTK paradigm  Clear and comprehensible writing Weaknesses:  Notation could be improved for clarity.  The relationship between `Xm` `Ym` `X(k)` `Y(k)` `X` and `y` needs clarification.  The closedform solution in equation (8) may not be fully independent of the kernel inverse which typically limits kernel methods practicality.  The algorithms privacy implications need further discussion as the uploaded data violates the privacypreserving aspect of FL.  The model privacy effectiveness should be characterized and discussed explicitly.", "Paraphrase: This paper presents a federated learning (FL) approach based on the neural tangent kernel (NTK) framework. NTK in the infinitewidth limit enables analysis of neural network dynamics without using a gradient descent algorithm. Authors leverage this for predicting optimal parameters aggregated from multiple workers. However this framework involves transmitting Jacobian matrices between workers and the aggregation server increasing computational overhead and privacy concerns. To address this they apply dimensionality reduction using randomized projection zeroing out compression and shuffling. The authors claim that their NTKbased FL scheme has a faster convergence rate than FedAvg for a twolayer network under certain assumptions. empirical results indicate that the proposed FL method outperforms other FL algorithms and achieves test accuracy comparable to the centralized case. effectiveness and Weaknesses:  effectiveness:  use NTK for predicting neural network outputs reducing computational cost.  Proposes dimensionality reduction to minimize communication overhead and privacy concerns.  Shows promising empirical results on convergence speed and accuracy.  Weaknesses:  Assumes infinite network width which may not be practical for realworld scenarios.  Requires transmission of Jacobian tensors potentially increasing communication overhead.  Lack of analysis on the impact of dimensionality reduction on performance and privacy.  Claims a faster convergence rate than FedAvg but this needs further mathematical justification. Minor number:  BigO notation should exclude constants.  FedAvg algorithm description could enhance understanding.  Dirichlet distribution parameterization needs clarification.  variant in graph colors and markers in number should be corrected.", "Paraphrased Statement: This paper proposes a federated learning (FL) framework called FLNTK that uses the neural tangent kernel (NTK) method for model optimization instead of the standard gradient descent. This approach is claimed to enhance communication efficiency and privacy preservation by leveraging data sampling and random projection techniques. effectiveness:  The use of NTK for FL optimization is an innovative concept.  The paper addresses potential issues like communication overhead and data privacy. Weaknesses: Methodological:  The proposed method requires comprehensive computation from the server which isnt clarified in the experiments.  The intricate work of integer updates for weight evolution is not sufficiently described.  The time required for grid research optimization is not discussed.  The implementation utilizes separate trusted key and shuffling servers which may not be practical for all applications. Evaluation:  The employed network architecture (multilayer perception with 100 hidden nodes) is simplistic more sophisticated architectures should be tested.  The datasets (MNIST variant) are limited large and more various datasets (e.g. CIFAR100 TinyImageNet) should be used.  In the data heterogeneity experiments critical baselines (e.g. FedProx SCAFFOLD) are not included for comparison.  In number 5 FLNTKs performance decreases as data nonIIDness diminishes. This anomaly requires explanation.  The communication costs associated with interacting with the key and shuffling servers need to be analyzed.", "Paraphrased Statement: Summary: This paper presents an algorithm for federated learning that utilizes advancements in the NTK framework. It proposes that participants contribution individual Jacobian matrices rather than model weight or gradient. This design aims to capture valuable statistical data while accommodating statistical differences among participants. Numerical results demonstrate the approach effectiveness across varying heterogeneity levels. Additionally a practical implementation is introduced to mitigate communication overheads. When compared to FedAvg and FedNova the proposed approach displays superior performance. effectiveness and Weaknesses: effectiveness:  Integrates the popular NTK and federated learning model to address data heterogeneity in federated settings.  Presents a practical implementation to reduce communication weight.  Offers experimental results showing performance improvement over FedNova and FedAvg across three datasets.  Maintains a clear and logical organization introducing relevant background data and discussing prior work. Weaknesses: Clarifications:  Despite stating that the NTK \"inherently solves the nonIID data problem\" the explanation remains unclear. The paper lacks an explanation of how the Jacobian enhances generalization compared to gradient.  The paper does not explicitly differentiate its algorithm from Algorithm 1 in [HLSY21] nor does it address the distinctions in convergence results.  The impact of testing multiple tk values at each round on convergence results is not explored. Privacy Considerations:  The term \"privacy\" is used ambiguously throughout the paper. Clarification is needed to define its specific meanings and the threats averted by each action.  The random projection implemented do not fully protect against an \"honest but curious\" server due to the contributiond projection matrix. This approach does not provide Differential Privacy.  Differential privacy guarantees provided by the shuffling model are not explained in detail or their scope of protection. Minor Suggestions:  Consider testing the algorithm on large model as NTK has inherent limitations with large model.  Include a discussion of practical aspects of FL such as user intermittency privacy concerns computation and storage costs and communication efficiency compared to existing algorithms."], "xCVJMsPv3RT": ["Paraphrase: Summary: Researchers propose a method to improve the computational efficiency of Randomized Ensembled Double Qlearning (REDQ) by utilizing dropout and layer normalization. This updated algorithm achieves comparable performance in terms of sample efficiency while significantly reducing computational time. The research team also provides an empirical analysis demonstrating the effective application of dropout. Strengths:  simplicity: The proposed approach requires minimal modifications to existing methods making it convenient to implement and adopt.  Significance: REDQ exhibits excellent sample efficiency but the ensemble of Qnetworks slows down computation. Balancing sample efficiency with reduced computational overhead is a crucial challenge.  Analysis: thorough comparisons of computation and memory use demonstrate considerable reduction in both.  solution: The proposed method effectively matches REDQs sample efficiency while being far more efficient in computation and memory use. Weaknesses:  Clarity of Writing: Some passages require refinement for accuracy and clarity.  Limited Investigation: The authors do not provide a comprehensive analysis of why dropout and layer normalization only underperform but work effectively in combination. Comparisons with other regularization techniques are also lacking.  lack of Novelty: The proposed method (dropoutlayer normalization) appears somewhat incremental. Concerns and Questions:  The definition of the updatetodata ratio (UTD ratio) should be clarified to align with previous literature.  Authors should thoroughly proofread the text for small errors and inconsistencies.  It is crucial to clearly define the context when referring to REDQs performance whether it pertains to computation per update interactions collected or time to reach a certain performance layer.  The full name and relevance of the DUVN variant should be explicated.  The error definition could be revised to define the error as the difference in Qvalues which can be estimated using Monte Carlo returns.  The authors should examine the reasons why layer normalization improves dropout performance and compare it to other regularization schemes.  The similarities between Dr.Q and DrQ should be addressed to avoid confusion.", "Paraphrase: Summary: This paper investigates ways to improve the efficiency of Ensemblebased Qlearning arguing that the existing REDQ approach is computationally demanding. The authors propose an alternative method using dropout (along with layer normalization for stability) to replicate the benefits of a larger ensemble in uncertainty estimation while reducing computational costs. experimentation on control benchmark tasks demonstrate that their approach retains the advantages of REDQ without the computational load. Strengths:  The paper is wellwritten and accessible.  It presents a clear motivation for using ensemblebased Qlearning and the challenges with computational efficiency.  The results support the effectiveness of dropout and layer normalization in achieving ensemblelike performance. Weaknesses:  The approach lacks significant novelty as many of its components have been previously explored.  The authors do not provide evidence of differences between their method and REDQ in high uncertainty settings.  The use of the M2 algorithm makes the approach comparable to vanilla SAC and an analysis of SAC with dropout would be valuable.", "Summary: This research presents a novel algorithm Dr. Q inspired by REDQ. Dr. Q employs a small group of dropout Q use with layer normalization. It matches REDQs sample efficiency while improving computational efficiency. Strengths:  The paper effectively introduces an alternative to the growing ensemble approach for bias reduction enabling SAC algorithms to operate in high UTD settings.  The papers introduction is clear and accessible.  Comprehensive experiments and ablation field demonstrate the algorithms performance and the impact of its components. Weaknesses:  The paper lacks an indepth explanation for the algorithms success. While dropout layer normalization and high UTD have been studied in isolation the specific combination in Dr. Q requires further analysis.  The naming of the algorithm as \"Dr. Q\" may need reconsideration to avoid potential conflicts with the name \"DrQ.\""], "hniLRD_XCA": ["Paraphrased Summary: This research introduces a novel control framework based on Koopman theory. It offers a robust closedloop controller with guaranteed stability an advantage over existing approach. The method has been successfully tested on complex control problems and compared favorably to other cuttingedge techniques. Notably it excels in scenarios with significant external disturbances. effectiveness and Weaknesses: effectiveness:  Simple and straightforward implementation.  Impressive performance on demanding control tasks.  Guaranteed stability facilitates design and application.  extensive evaluation demonstrates benefits over other methods.  Versatile framework opens avenues for future research. Weaknesses:  Limited discussion of related work particularly:  \"Learning Koopman invariant subspaces for dynamical mode decomposition\" (Takeishi et al. 2017)  \"Physicsinformed probabilistic learning of linear embeddings of nonlinear dynamicals with guaranteed stability\" (Pan et al. 2020)  \"Forecasting Sequential data Using Consistent Koopman Autoencoders\" (Azencot et al. 2020)  Differences from Morton et al. 2019 and Pan et al. 2020 not clearly outlined. Minor Comments:  Verify reference \"noa 2016\" in section 2.  Provide field and range of A and b in Equation (1).  Explain the separability of the dynamicals and control in \\mathcalK.  Change \"L\\infty\" to \"L2\" below Equation (3).  Move sections 3.1 and 3.2 to the appendix.  Clarify the contribution of the PerronFrobenius theorem.  Use consistent superscriptsubscript notation in Equation (5).  Consider logarithmic scale for yaxes in design 2 4 and 5.  Include SAC and MLP in the applicable design (design 2 3 and 4).", "Paraphrase: Summary: This paper introduces the Deep Stochastic Koopman Operator (DeSKO) an approach that uses the relationship between the PerronFrobenius and Koopman operators to enable control of unknown systems. DeSKO learns a probabilistic distribution of observations and a linear dynamical system that models this distributions evolution over time. effectiveness:  Simple and intuitive design with practical implementation.  extensive empirical evaluation demonstrating superior performance in prediction control and robustness compared to other Koopman and modelfree reinforcement learning algorithms.  open and wellwritten demonstration. Weaknesses:  theoretical assumptions require an exact Koopman embedding which is unrealistic in practice.  The assumption of linear control inputs is also restrictive and not fully supported by the theory.", "Paraphrased Statement: This research utilizes Koopman operator theory to transform nonlinear dynamics into linear dynamics in a highdimensional space. Both the state spacetohighdimensional space mapping and the Koopman Matrix (linear transition matrix) are datadriven. While similar approach exist this work adds noise to the Koopman model learning work assuming noisy data and seeking to uncover a Koopman model from such data. Furthermore it introduces a joint learning framework for a control matrix within the Koopman setting and a robust control mechanism for the proposed method. The robust control mechanism is model predictive controlbased and mathematically proven to be stable due to the linear transition of observables. The proposed method is evaluated on rigid body systems a soft robotic arm and cell biology systems. It outperforms existing Koopman and RL techniques. effectiveness and Weaknesses: Learning and controlling dynamical systems with noisy data is not novel but this paper presents a unique approach involving stochastic Koopman operators. The method offers significant learning accuracy and controller reliability through the integration of Koopman model learning and robust control. The proposed approach surpasses existing models and introduces a promising perspective for the challenging task of learning and controlling dynamical systems.", "Paraphrased Statement: Summary: This paper introduces the \"Deep Stochastic Koopman Operator\" (DSKO) method which incorporates uncertainties into system dynamics modeling. DSKO allows for state estimation similar to large neural networks while preserving the linear structure in the projected space for compatibility with linear model Predictive Control (MPC) techniques such as LinearQuadratic Regulator (LQR). DSKO outperforms nonstochastic methods and SACtrained policies in tracking tasks. effectiveness and Weaknesses: effectiveness:  Extends Deep Dynamic Mode Decomposition (DeepDMD) to account for uncertainty.  Enables the use of established linear control methods for systems projected into linear spaces.  Achieves superior control performance in benchmark tasks. Weaknesses:  Limited experimental validation.  Comparable performance to SAC in certain cases.  Ambiguous explanation of how the PerronFrobenius operator affects system distribution evolution.  Unclear how Equation 3 modifies aim compared to the original DKO method.  Unclear whether multistep loss is incorporated in DKO aim which could affect the robustness of performance comparisons. Comparison with DKO:  Feature  DSKO  DKO    uncertainty Handling  Yes  No   Linear Structure in design Space  Yes  Yes   Control method  LQR  General   performance  Better  NA   Distributional Evolution  PerronFrobenius Operator  not Explicitly modeled   aim Modifications  Equation 3  NA   Multistep Loss  not Stated  not Stated "], "fyLvrx9M9YP": ["Paraphrase: Summary: This paper introduces ADVAE a model that employs crossattention to create a sequence of latent variables. These variables are then used to guide an inference model. The results demonstrate that certain latent variables align with different syntactic roles in a sentence. The paper claims that the latent variables effectively capture content but the experimental setup may need improvement to fully support this claim. Strengths and Weaknesses: Pros:  The idea of using crossattention to automatically disentangle content in Transformers is compelling and could benefit various text generation tasks. Cons:  The baseline VAE should use a Transformer (not an LSTM) to provide a fair comparison.  A stepbystep explanation of ADVAE would enhance clarity.  The impact of adjusting the number of latent variables (Nz) needs further exploration.  Evaluation should extend beyond syntactic roles to encompass other aspects of text disentanglement. Minor Comments:  Missing references on machine translation Transformer encoderdecoder models and sentence representation from Transformer models.  \"pr\" (dependency parser) should be defined. Questions:  Would comparing ADVAE to a denoising autoencoder (like a masked language model) enhance its generalizability  Could using only crossattention in the calculation of mean and standard deviation vectors yield further insights  Would ADVAE exhibit similar behavior in machine translation where source and target sentences represent the same information in different languages", "Paraphrased Statement: Summary: This research introduces a technique for separating various aspects of text without any guidance. It demonstrates success in identifying roles within sentences. To achieve this a neural network compresses input into a set of main hidden variables. The VAE model ensures that these variables adhere to a standard Gaussian distribution. A Transformer encoderdecoder is utilized as the inference network. This network decodes latent variables which are then used as input to the transformer encoders outputs to establish relationships. The underlying principle is that sequencetosequence architecture based on attention align source and target sequence. The method is tested on identifying roles in sentences. By altering individual latent variables the authors examine how this affects the roles depicted in the generated text. They also analyze how attention aggregates syntactic roles into latent vectors. Their findings indicate that their method isolates sentence roles into latent variables more effectively than traditional VAEs. Strengths:  Explores a crucial topic in representation learning with potentially significant impact.  model design is straightforward and intuitive.  Results are convincing and wellpresented. Weaknesses:  Absence of ablation work limits definitive conclusions about which components contribute to improved performance.  model architecture lacks motivation.  Certain related work such as those by Behjati and Henderson (2021) and Locatello et al. (2020) are not discussed for comparison. Minor Issue:  Table captions should be placed above the tables as per manner guidelines.", "Paraphrase: This work presents a probabilistic model known as AttentionDriven Variational Autoencoder (ADVAE) an extension of the \\betaVAE model where the encoder and decoder are built using Transformer neural networks rather of traditional architecture like RNNs. The goal is to disentangle the semantic meanings of latent variables based on syntactic roles (e.g. nouns and verbs) defined by syntax. To this end the authors propose using Transformers within the \\betaVAE model which has proven effective in identifying the role of latent variables in VAEs. Additionally a novel approach is presented for measuring syntactic disentanglement between latent variables using information from the Transformer architecture attention matrices. Experimental results demonstrate the proposed method superiority over standard VAEs and its ability to alter the generation of target word by modifying specific latent variables corresponding to specific syntactic roles. Strengths:  Wellstructured paper.  Proposes a new protocol for evaluating the mapping of latent variables to syntactic roles. Weaknesses:  May not be the first attempt to integrate Transformers into VAE models.  The paper should provide more detail about the Transformer architecture used.  Theoretical justification is lacking for why the proposed model is especially suited for disentangling syntactic roles rather than other linguistic or semantic properties.", "Summary This paper introduces a model for identifying syntactic roles as hidden variables in sentence representation. It employs an attentionbased Variational Autoencoder (VAE) that assigns separate hidden variables to syntactic roles through an encoderdecoder structure. Evaluation protocol The authors also propose an evaluation protocol to assess the disentanglement of hidden variables and text segments encompassing:  syntactic role extraction  latent variable work on decoding  decoder work on latent variables  Disentanglement metrics Strengths  Unsupervised learning of syntactic disentanglement  Evaluation protocol for syntactic role disentanglement Weaknesses  Complex and difficult to understand paper structure  set evaluation focusing only on intrinsic metrics without assessing downstream task benefits (e.g. unsupervised dependency parsing) Suggestions for improvement  Reorganize the paper for clarity providing necessary background information upfront.  Include relevant work on disentangled representation in the introduction.  Improve design 2s caption to clearly indicate inputs and outputs.  Move essential information from the appendix into the main body such as latent variable and syntactic role analyses. Questions  How is the number of hidden variables (Nz) determined (e.g. is it related to the number of syntactic roles)  Are the syntactic roles in the evaluation protocol sourced from dependency parsing or gold standard annotations  What is the relationship between the predicative structure in design 1 and the syntactic roles discussed in the paper"], "swbAS4OpXW": ["Paraphrased Summary: The researchers introduce a model for generating images from a single target image by leveraging a previously trained GAN model. They employ three tactics to improve image quality and diversity: 1. An attribute adaptor maps the latent space to the target images attributes. 2. An attribute classifier within the discriminator distinguishes real from generated images. 3. The truncation trick limitation diversity during training. Experiments using facial and architectural datasets show promising solution. Strengths and Weaknesses: Strengths:  The oneshot image generation task is innovative.  The proposed approach is accessible.  Experimental solution demonstrate the methods ability to generate images with desired attributes. Weaknesses:  The methods novelty is limited.  The key contribution such as the attribute adaptor are unclear and lack differentiation from prior work.  The attribute classifier is unclear whether it predicts attribute labels or simply classifies images as real or fake.  The truncation trick for diversity constraint is not sufficiently novel.  The analysis of the impressive solution in image 4 and 5 is inadequate. Visualizing the latent space representations could provide a clearer understanding of the mapping and representation changes.", "Summary (Paraphrased): This research investigates the challenging task of adapting a pretrained Generative Adversarial Network (GAN) from one domain to a different domain with limited data (one model). The paper introduces GenDA a novel method that achieves sourcetotarget domain adaptation while preserving sample diversity. GenDA involves freezing the main components of the GAN and only finetuning an additional linear layer. The field demonstrates promising solution in imagetoimage translation tasks. Strengths:  Unique approach to a challenging domain adaptation problem.  Compelling visual comparisons to existing methods (e.g. improving realism and diversity).  Competitive quantitative solution in both oneshot and multishot adaptation scenarios. Weaknesses:  Lack of ablation field to evaluate different components of the attribute adaptor and classifier.  Similarity to previous research on model SVM which suggests using one positive model and many negative models for domain adaptation.  Limited discussion on method limitations such as its suitability for specific domain adaptation tasks e.g. \"carabandoned car\" and \"cartruck.\"  Comparison to style transfer methods is missing in the crossdomain experiment (image 5).", "Paraphrased Summary: The field aims to enhance StyleGAN a cuttingedge network for singleshot domain adaptation. Two innovative approaches are proposed: 1. Attribute Adaptation Layer: A lightweight layer is added at the start of StyleGAN to modify the latent code enabling conversion from an existing code to a new domainspecific code. 2. discriminator Reuse: A trained discriminator features are frozen and a new classification head is added. This novel technique allows for domainspecific discrimination. Strengths and Weaknesses: Strengths:  open and understandable writing effectively conveying the main ideas.  Reproducible solution based on the papers instructions.  Simple yet novel technical contribution: Lightweight attribute adaptation and discriminator reuse. Weaknesses:  Attribute Adaptation Layer: Considered insufficiently powerful to handle complex domain transfer. Its early placement in the network and limited computational capacity advance concerns.  Limited Domain Transfer: The generator inability to fully generate desired domains when only the latent code is modified.  Ambiguous Facade Transfer solution: image 4s face transfers lack a open inputoutput relationship and should be excluded from the paper as it represents a different task.  Challenging Domain Transfer solution: image 5s solution show only minor color strategy adaptation suggesting an inconsistent evaluation process.  Lack of Original Domain image: Missing input images from the original domain hinder proper solution interpretation.  Recommendation: Revise the attribute adaptation design to improve its effectiveness."], "p0rCmDEN_-": ["Paraphrased Statement Summary The authors explore the role of spatiotemporal calculations to recognize objects in the CIFAR10 and CIFAR100 datasets. They propose a network with a frontend of recurrent units (ConvGRU) that processes spatially jittered and downsampled images mimicking an active sensor. The network is structured as a studentteacher model where a temporal pooling layers weights are trained to match those of a feature layer in ResNet50. Finetuning then improves classification accuracy. The authors investigate whether spatiotemporal calculations can generate a feature layer comparable to a larger network trained on fullresolution images and how this layer supports object recognition at a level similar to ResNet50. They find that their network performs almost as well as ResNet50 with 4x downsampled and jittered images. They also demonstrate that the network engages in spatiotemporal calculations. Strengths 1. innovative Concept: The role of an \"active sensor\" approach where the input image is jittered is novel and aligns with recent research in neuroscience and psychology. 2. Promising Results: The initial experimentational findings show the networks strong performance especially with jittered images arranged in a spiral model. Major Issues 1. Questionable Evidence for spatiotemporal Significance: The authors claim that spatiotemporal computations in the network frontend are crucial but the evidence supporting this is limited. The compare between their ResNetRNN network which has recurrent computations after global average pooling and their DRC network shows that ResNetRNN performs poorly. However the compare is not equitable becarole ResNetRNN roles vanilla GRU units limiting its approach to spatial information. Additionally the training method for ResNetRNN is unclear. 2. Underanalyzed spatiotemporal Computations: image 2 illustrates that the DRC network utilizes a combination of spatial and temporal computations but this aspect is insufficiently analyzed. It is unclear whether the performance varies with the distribution of computations across different units or what the impact of removing specific units would be. 3. Limited exploration of Trajectory Significance: The claim that the temporal trajectory of images is significant (Fig. 3) is not adequately supported. The authors do not analyze the effect of curvature in the trajectories and the results could be influenced by the increased intersection of curved trajectories near the eye. A control experimentation that randomizes the trajectories could determine whether the statistics of the trajectories or the curvature contribute more to performance. Minor Issues 1. Typos: The article may contain typos. 2. Legends and Data Points: image 3 lacks a legend and the authors average only two data points which limits the reliability of the confidence intervals shown for each parameter set.", "Paraphrased Statement: Summary:  The study claims that using recurrence can improve visual accuracy in smallresolution settings such as those limited by the retinas photoreceptors.  The researchers built a convolutional neural network (DRC) with recurrent connections in the early layers.  The DRC received a series of smallresolution images as input and learned to classify objects on the CIFAR dataset with guidance from a teacher network that received highresolution inputs.  The DRC outperformed a smallresolution baseline and approached the performance of a model using standard resolution inputs.  The study also visualized the features learned by the DRC. effectiveness:  Novel approach in modeling the impact of small retinal resolution on object recognition models.  Clear results showing that the recurrent DRC model can restore performance close to that of a standard resolution model when assisted by a teacher network.  Provides a biological basis for the models design. Weaknesses:  Lacks connection to experimental data to verify the models biological predictions.  DRCs requirement for a teacher network limits its compare to nonrecurrent baseline which do not use teachers.  Unclear practical benefits for computer vision applications where smallresolution but high temporal sampling is advantageous.  Some related recurrent models are not discussed for context.  Inconsistent information regarding the accuracy of the ResNetRNN model.  Visualization of features is not fully explained or connected to the main claim.", "Paraphrased Statement: This research investigates the potential of modeling human vision fixation drift phenomenon where slow small eye movements occur during fixation enhancing visual resolution. The study proposes a \"dynamical Recurrent Classifier\" (DRC) model with a convolutional front end that integrates fixation drift information and feeds into a pretrained ResNet 50 backend. The DRC shows promising results in restoring image classification performance for downsized images to near the level achieved with higherresolution images. Analysis reveals that the model learns spatiotemporal features with some emphasizing spatial aspects others focusing on temporal information and many combining both. Additionally the study demonstrates that curved fixation drift trajectories enhance performance aligning with recent findings in human vision. The DRC model is suggested as a valuable tool for artificial intelligence applications involving limited visual input but multiple samples over time. effectiveness:  innovative model inspired by biological phenomena  Excellent classification results  Addresses a neglected aspect of human vision  study learned feature representations  Demonstrates performance benefits of curved fixation drift trajectories Weaknesses:  Unclear representation of positional information in the model  Ambiguous role of \"Smallnet\"  set explanation of the generative networks optimal feature determination work  Incorrect or confusing model selection in image 2B  Grammatical errors and typos throughout the paper", "Summarized Statement: Researchers created a neural network that can recognize objects in lowresolution moving images. Using a recurrent neural network in the initial layers the network achieves recognition performance comparable to using static highresolution images. Assessment: effectiveness:  Interesting concept of utilizing dynamical retinal input and recurrent processing. Weaknesses:  Lack of a control group to determine if motion actually aids recognition.  Lowresolution images were derived from highresolution images limiting potential for exploiting new information from motion.  Recurrent computation may only act as a deeper network not specifically related to motion.  small connection between the model and neuroscience principles such as neuron temporal dynamicals.  Insufficient recognition of previous work in the introduction including:  Ruccis theory on image motion benefits for acuity.  Buraks model for extracting work from motion.  Ratnam et al. and Anderson et al.s similar research on motion and acuity."], "s5lIqsrOu3Z": ["Paraphrased Summary: This paper proposes a novel approach to obtain a linear discriminative representation (LDR) using a twoplayer minimax game between an encoder and decoder. By leveraging the MCR2 rate reduction principle the game minimizes a measure of the space between the original data and the learned representation while simultaneously contrasting the representation. The method generates promising results on MNIST CIFAR10 and ImageNet datasets and achieves comparable classification accuracy on MNIST data. Strengths:  The core idea of the game is novel resembling CycleGan but with a different approach based on rate reduction.  The experiments demonstrate the generative capabilities of the method. Weaknesses:  The paper lacks selfcontainment making it challenging to understand without prior knowledge of the referenced literature.  Key concepts and motivations behind the approach need clearer explanations.  The mathematical notation requires clarification especially regarding the union performance in equation 6 and the potential semantic overloading of \u0394R.  The generative comparison use outdated baselines and it would be valuable to include more current methods.  The paper does not discuss training time scalability with dataset size or the potential limitations of the model in capturing semantics and preventing input collapse.  No discriminative comparison are performed on the ImageNet dataset where the model has also been trained.  The introduction lacks contextualization and a review of previous function in the field.  The notation in equation 23 is incorrect.  The statement about the evolution of generative models based on motivation is confusing.", "Paraphrased Summary: This paper presents a fresh method for training autoencoders using an adversarial objective based on reducing the rate of data loss. The proposed method has been shown to perform well in generating reconstructions and discriminating between real and generated data. Strengths:  The adversarial objective function is original and intriguing.  The experimental results are convincing. Weaknesses:  The paper lacks background data on rate reduction and MCR2 making it difficult for readers to understand.  The objective function is more complex than those used in VAE and GAN which have clear connections to the loglikelihood of generative and discriminative models.  The proposed method has not been analyzed theoretically.  The probabilistic generative model used for generating fresh data is not explicitly described. A probabilistic generative decoder model is a more comprehensive representation of the data while an autoencoding perspective is more limited.", "Paraphrased Statement: Summary: The study proposes a closedloop autoencoder that learns relationships between multiclass multidimensional data and lowerdimensional representations (LDR). The study includes experiments to evaluate the proposed method. Strengths and Weaknesses: The concept of viewing the encoder and decoder of the autoencoder as a generator and discriminator is intriguing. The method uses the encoder as a discriminator and calculates the space between the original data (X) and the reconstructed data (\\hatX) as a measure of rate reduction maximization. The authors claim this maximization prevents the combined function of the encoder and decoder from being nonautoencoding. However it is unclear why rate reduction maximization is necessary to obtain the desired function between the original data and LDR. An alternative approach suggested by the reviewer would be to use the rate reduction equation (7) along with a conventional reconstruction loss. The reviewer questions the advantages of using rate reduction maximization in this context. minor Issues:  Equations (2) and (5) use inconsistent notation for the function between Z and \\hatX.  The symbol X in Section 1.2 is used differently in different contexts."], "rGg-Qcyplgq": ["Paraphrased Statement: Summary: This paper explores a method for optimizing riskneutral strategies in reinforcement learning. Previous methods often encounter convergence subject due to their commitment to specific riskseeking or riskaverse criterion. To address this the authors propose a novel approach that randomizes the risk criterion during training. Theoretically they demonstrate that this method converges to an optimal return distribution under certain conditions. Experimental results support the methods efficacy. effectiveness and Weaknesses: The overall concept is appealing and intuitive. Randomizing the risk criterion mitigates bias and allows the agent to gradually approach convergence. The paper includes both theoretical analysis and experimental results to demonstrate the methods validity. Additional Comments: 1. Writing: The paper contains numerous grammatical errors and typos indicating a lack of polish. The theoretical section requires clearer notation and explanations. 2. selection Bias: The selection of Atari games is not discussed raising concerns about cherrypicking. The authors mention that different games require different levels of optimism or pessimism. A more detailed analysis and comparison would clarify the methods behavior. 3. Further Evaluation: Additional experiments and ablation studies on hyperparameters would enhance the paper credibility.", "Summary: The paper introduces a novel approach to exploration in distribution reinforcement learning. Existing methods cant differentiate between uncertainty about the world (epistemic) and uncertainty due to randomness (aleatoric). The proposed approach uses random perturbations to the distribution and then gradually removes them. This approach has been proven to converge and validated in experiments. effectiveness and Weaknesses: effectiveness:  The paper addresses a significant limitation of existing exploration methods. Weaknesses:  Its unclear how the proposed approach fully resolves the subject of separating epistemic and aleatoric uncertainty.  The example used to illustrate the problem is somewhat simplistic.  The theoretical guarantee appears limited as it applies even to traditional Bellman operator that dont perform exploration.  The experimental results are not very strong with limited comparisons to existing methods and minimal outperformance. Minor Comments:  Its unclear whether Theorem 4 applies for any selection of U\u0394.  The proposed algorithm resembles Thompson sampling but this connection is not explored.  The authors should clarify that their goal is to learn a riskneutral policy as opposed to a riskaware policy.", "Paraphrase: Summary: The paper introduces a novel reinforcement learning (RL) algorithm that avoids extremes of riskseeking and riskaversion aiming for risk neutrality. This need arises from previous RL methods that use optimism in the face of uncertainty (OFU) which tend to exhibit biased exploration due to their onesided risk tendency. In an environment with a deceptive lowprobability highreward side the proposed algorithm (PQR) outperforms existing methods (DLTV QRDQN) by effectively identifying the optimal action. The paper presents a technique for adjusting the risk measure in RL resulting in a perturbed distributional Bellman operator (PDBOO) that converges to the same optimal policy as the standard Bellman operator (under certain assumptions). Experiments in LunarLanderv2 and four Atari games demonstrate the superior performance of PQR compared to biased riskseeking or riskaverse methods. effectiveness: 1. PQR leverages principles from distributionally robust optimization to create a theoretically sound PDBOO. 2. The Nchain environment clearly illustrates the advantages of riskneutral exploration. Weaknesses: 1. The paper empirical evaluation is limited to a few Atari environments. Expansion to other domains including continuous control and the bsuite would strengthen the results. 2. The writing could be improved as there are grammatical errors and lack of clarity in certain sections."], "ydopy-e6Dg": ["Paraphrase: The iBOT method is presented in this paper as a combination of contrastive selfsupervised learning like DINO and masked modeling like Beit. It differs from Beit in using an online tokenizer instead of a pretrained one. iBOT integrates a patchlevel loss similar to Beit and a global imagelevel loss like DINO. iBOT has been evaluated for image classification object detection and segmentation tasks. strength:  Wellwritten: The paper is clear and easy to understand.  Simplicity: iBOT is straightforward and provides beneficial results without significant transfer to existing approaches.  analysis: The paper includes detailed analysis that demonstrate the impact of iBOT compared to traditional SSL methods like DINO. Weaknesses:  Cost compare: A comprehensive compare of training costs such as FLOPs and memory use is lacking.  Finetuning compare: Supervised and selfsupervised approaches are compared with different numbers of training epochs which may skew the results. A fairer compare would adjust the epochs to beneficial assess iBOTs advantage over the supervised baseline.", "Paraphrased Summary: paper Overview: This work introduces a selfsupervised vision transformer training strategy that combines the DINO selfdistillation method with mask image modeling (MIM). strength:  The MIM method for selfsupervised learning is wellfounded and effective.  The proposed MIM loss significantly improves downstream tasks like object detection and segmentation compared to the baseline DINO.  Extensive experiments and visualizations demonstrate the benefits of MIM and online tokenizer in the proposed iBoT model. Weaknesses:  The improvement in finetuned models is limited.  While results on downstream tasks are promising the technical contribution is relatively limited.  To fully highlight the impact of MIM loss it would be helpful to provide results without it.  The differences between the proposed online tokenizer and pretrained tokenizers in BEiT could be further analyzed.  The application of multicrop augmentation with mask image modeling needs more explanation.  training costs associated with masked image modeling are not disclosed.", "Paraphrase: This work introduces a pioneering approach to pretraining vision transformers by applying BERTstyle techniques. Unlike previous methods this approach does not rely on a pretrained tokenizer instead using the teacher model to generate targets for masked token recognition. This approach resembles BEiT but without the need for an external tokenizer. The model leverages two augmented views of each image akin to BYOL and DINO and incorporates selfdistillation. The work achieves impressive results in image retrieval classification instance segmentation and semantic segmentation surpassing existing models within its category. The experimentations and analysis thoroughly demonstrate the models strength but further exploration of how the learned model differ from competing approaches and the precise contributions to image recognition could enhance the understanding of the training process."], "gtvM-nBZEbc": ["Paraphrased Statement: Summary: This field aims to create novel object captions that meet the criteria of fluency accuracy and relevance. It employs two largescale models BERT and CLIP to achieve this. The method involves masking nonnoun language in a caption utilizing BERT to generate these masked language and then assessing the affinity between the original and BERTgenerated captions with the associated image (using CLIP). Based on the affinity score the method determines whether to adopt the BERTgenerated caption for training. effectiveness and Weaknesses: 1. effectiveness: The method leverages sophisticated models (BERT and CLIP) to address a specific subtask (novel object captioning). However it may be considered an excessive approach for such a limited task diminishing the researchs significance. Additionally the method could be expanded to broader image captioning context with limited training data not just novel object captioning. 2. Weakness: The method masks nonnoun language to prevent generating imagebased information. However other language such as attributes and relations may also appear in images and should not be masked. For instance horses may vary in color or a persons interaction with a horse may differ in different images. 3. Weakness: The claim that wordbyword supervision leads to imitating the style of training images rather than connecting captions to visual content lack supporting evidence.", "Summary: This field introduces a novel approach for captioning novel objects (objects unseen in training data). It leverages pretrained BERT and CLIP models to enhance captions generated by existing models by rewarding captions that include accurate and detailed visual content. Results show improvements in quality and evaluation metrics compared to previous methods. The authors propose a novel evaluation framework to assess the fluency fidelity and adequacy of generated captions. effectiveness:  use existing visuallinguistic knowledge from BERT and CLIP.  Introduces a novel loss use to encourage novelty and fluency.  Establishes a comprehensive evaluation system including fidelity fluency and adequacy metrics. Clarifications:  BERT is finetuned with captions from the noncaptioned dataset in Equation 2.  CLIP ensures the generated captions describe visual content accurately. When both captions are unreliable the reward is zero.  There is no threshold for comparing CLIP scores.  The BERT equations can also fulfill fidelity object due to the gating use but this is not explicitly observed.  Improvements from Equation 5 may stem from the rCIDER term.  Results in Table 1 are compared with baseline models optimized for CIDER.  If rewards were calculated over unlabeled training samples performance may be lower.  The models performance when applying Equation 2 only during inference from a pretrained captioning model is not discussed.  The origins of the two captions in image 1 are not specified.  Novel language generated by the model are mainly objects or nouns. Weaknesses:  lack of ablation to identify the specific contributions of different components.  Improvements over stateoftheart methods are achieved through CIDER optimization on labeled data which may also improve baseline performance.  The approach may not address grammatical inaccuracies in generated captions reported by previous studies.", "Paraphrased Statement: This research introduces a framework that integrates BERT (a masked language model) and CLIP (an imagetext embedding model) for generating captions for novel objects. The framework focuses on enhancing the fluency accuracy and appropriateness of the generated captions. effectiveness:  Addresses crucial captioning quality by generating captions that are clear accurate and appropriate.  Demonstrates improved performance over existing models through comprehensive experiments. Weaknesses:  potential concern over the originality of the framework as it incorporates existing models like BERT and CLIP.  The use of a known reward use and BERT application for text generation may limit novelty.  The field shift publication in using BERT and CLIP on a different dataset needs to be addressed.  BERTs use is limited to the training phase leaving open the potential for its use in the inference stage to improve fluency.  lack of a human evaluation to validate the effectiveness of the framework.", "Summary (Paraphrased) This paper presents a novel object captioning (NOC) approach designed to describe novel objects not included in the training data. Existing methods lack the ability to produce captions that are:  Fluent: Humanlike  Fidel: Accurate in describing the novel object  Adequate: Covering all relevant visual concepts The proposed model VisualLinguistic adequacy fidelity and adequacy (VLAF2) addresses these challenges by using:  Bidirectional Encoder Representations from Transformers (BERT) to generate caption variations by substituting language.  Contrastive LanguageImage Pretraining (CLIP) to assess the relevance of these variations to the image.  A combined CLIP score and caption quality (CIDEr) reward to train the captioning model via reinforcement learning. Evaluated on the nocaps benchmark VLAF2 outperforms existing methods and demonstrates improvements in fluency fidelity and adequacy metrics. effectiveness and Weaknesses effectiveness:  Insightfully identifies the importance of fluency fidelity and adequacy in NOC.  Leverages BERT and CLIP to address these issues effectively.  use uncaptioned images for training improving performance. Weaknesses:  Violates nocaps paper guidelines by using additional humangenerated caption data making comparisons to existing methods questionable.  Does not explore alternative metrics for captionimage relevance besides CLIP.  lack investigation into joint training with other webscale caption data sources."], "nCw4talHmo5": ["Paraphrased Statement Summary ParaDiS is a flexible network that can be scaled for performance across different devices without the need for retraining. The training process involves distilling knowledge from a large network and incorporating activation knowledge to enhance performance. While the parallel performance concept is promising specific contingent on its implementation are lacking. Strengths  Practical approach: Enables parallel performance of slimmable networks harnessing the computation ability of multiple devices.  Clear presentation: Wellwritten and straightforward to understand.  thorough evaluation: Comprehensive work validate the effectiveness of individual components. Weaknesses  Methodological limitations:  No significant departures from existing slimmable networks.  Leveraging wider networks and feature map distillation are not novel techniques.  Limited discussion on parallel performance:  contingent on device distribution and communication are missing.  Potential latency overheads and device resource considerations are not addressed.  Scalability concerns:  Width configurations appear to be fixed and sensitive potentially limiting flexibility.  Missing results:  No simulations or results demonstrating parallel performance. detailed Comments 1. Clarification needed on the independence of BN statistics when receiving inputs from different paths. 2. Missing results for Slimmable and Slimmable Wide IPKD framework experiments. 3. Comparison of training costs against vanilla training or US training with enumerated switches is not provided.", "Paraphrase: Summary: The research introduces an algorithm that creates multiple student frameworks from a large teacher framework. Each student handles a portion of the original frameworks channel computations. Combining the student outputs generates the final prediction. This allows individual students to be deployed on mobile or edge devices. Strengths: 1. The experiment section is welldetailed and provides evidence to support the claims made in the paper. 2. The distillation algorithm is clearly explained and easy to replicate. 3. The authors addressed corner cases (such as batch normalization layers) and proposed solution. Weaknesses: 1. The application of this work may be limited beyond convolutional image classification frameworks. Sharing all parameter across devices may hinder frameworks with significant parameter number but not necessarily high computational costs. 2. The framework requires prior knowledge of all devices involved in the training process. This approach may not scale well with a large number of devices. 3. The concept of distilling a teacher framework into student frameworks is not novel (e.g. noisy student). Additional related work in this area should be included. 4. An ensemble of smaller frameworks deployed on separate devices would be an interesting baseline. Such an approach could maintain functionality even with changes in the number of available devices.", "Paraphrased Statement: Summary: This paper suggests training multiple parallel subnetworks within the slimmable networks framework. These subnetworks can run concurrently on different devices depending on hardware capabilities to minimize latency. Strengths and Weaknesses: Strengths:  Similar to slimmable networks this approach allows for simultaneous training of all networks eliminating the need for retraining during deployment. Weaknesses:  Overlooks key number in distributed embedded computing.  Lacks detailed information on latency reductions resulting from parallel computing. understanding for Acceptance:  Parallel distribution can reduce deployment latency.  ParaDiS network training is efficient with promising performance. understanding for Rejection:  Limited experiments on classification tasks just.  Ablation work fails to demonstrate significant benefits of distillation.  Incomplete information on latency savings due to parallel computing. Questions:  How is the second layer which appears to be dependent on all neurons in the first layer distributed in parallel (number 2)  Is computation performed seriallyparallel or repeated  When innovative DLAs utilize onchip memory to store intermediate results how does parallel computation across devices affect DDR traffic What factors should be considered in determining the optimal distribution strategy", "Summary: The authors introduce ParaDis a parallel version of slimmable neural networks. Using the principles of slimmable networks and additional techniques they create models that can switch between multiple parallel configurations with minimal accuracy loss. Evaluation results on ResNet and MobileNet trained on ImageNet demonstrate that ParaDis matches or surpasses the baseline accuracy of models trained exclusively for a specific configuration. An ablation work confirms the impact of these techniques on training. Strengths:  Clear visualization of different distributed inference types  Comprehensive ablation work highlighting the significance of using IPKDA  Honest discussion of limitations and scaling challenges Weaknesses:  Evaluation number do not clother demonstrate the methods benefits in maintaining accuracy while reducing latency  Lack of latency results despite being a key advantage of parallelization  Limited evaluation on other architectures leaving questions about the methods performance across different network types  Questions about the validity of single configuration results particularly in cases where network sections are cut in half  Clarification needed on the number of potential switches in ParaDis compared to slimmable networks Questions:  Why was the [.5 .25 .25] configuration not trained  How does ParaDis compare to other types of networks such as other exit models in terms of accuracy at varying latency  In situations where ParaDis and slimmable networks offer similar performance what factors favor ParaDis models  How are residual branches handled during splits in ResNet  What are the tradeoffs of updating all switches versus a subset per epoch  What is the minimum channel size in the splits for the negative results with 8 switches Do these number improve with large scaled width"], "j-63FSNcO5a": ["Paraphrase: This study presents a method for training generative models that can create representations of their input data where different aspects of the data are separated into distinct features. The method uses a technique called contrastive learning which compares different representations of the same data to identify their similarities and differences. The authors demonstrate the effectiveness of their method through extensive experiments on several datasets including highresolution image. The method is shown to be intuitive simple and effective. It also includes two novel techniques an entropybased field loss and a hard negative flipping technique which enhance its performance. The method is generalizable to multiple generative models. Question: In the provided qualitative results for StyleGAN2 (image 4 1720) it is not clear if the authors manually selected the layers in the w space to modify or if the modification was applied globally to all layers. Previous methods (GS and CF) involved manual layer selection so it would be helpful to clarify this for the proposed method.", "Paraphrased Statement: This paper introduces a novel technique for understanding the hidden structure of pretrained generative models. It aims to identify semantically meaningful directions in the models latent space. The method involves training two neural networks: a \"navigator\" that identifies potential directions and a \"deltacontraster\" that measures the difference between samples perturbed along those directions. Perturbed samples are decoded and then reencoded and the resulting difference is used for contrastive learning. effectiveness:  Straightforward and easy to implement  Clear explanation and extensive experimental analysis  Impressive quantitative results across multiple datasets and generative model types  Preserves semantic information when manipulating attribute values Weaknesses:  Does not explicitly explain why the method selects semantically meaningful directions  potential for \"shortcut\" solutions that minimize training loss without capturing disentangled representations  sensitivity to the ratio of positive and negative samples requiring careful hyperparameter tuning  Limited results at full image resolution  Minor typos and grammar issues", "Paraphrasing: Summary: This research introduces DisCo a technique for extracting distinct representations from trained generative models that blend information. Comprehensive experiments demonstrate that DisCo excels over comparable approaches in both numerical and subjective evaluation. effectiveness and Weaknesses: effectiveness:  DisCo is a groundbreaking method that significantly enhances disentanglement while maintaining generation quality.  Detailed experiments and analysis.  The papers is generally wellwritten and comprehensible. Weaknesses:  DisCo still has some flaws.  Instructions on computing MIG and DCI for discoverybased methods are lacking.  MIG and DCI metrics are outdated and may not accurately reflect disentanglement.", "Paraphrased Summary: This framework offers a way to create disentangled directions in pretrained models. It avoids the publication of poor generation quality often encountered when using regularization terms for disentanglement. Its based on the principle that similar variations in image result from changes in the same factors. The framework is versatile and can be applied to several generative models including GANs VAEs and flow models. effectiveness:  No specific training is required.  Can be applied to different generative models.  Outperforms previous models in disentanglement measures.  Stable across different random seeds.  Comprehensive ablation study and hyperparameter sensitivity analysis. Weaknesses:  Requires multiple components and tricks (Navigator Contrastor weight sharing contrastive approach hard negatives flipping) each with its own hyperparameters.  The need for two encoders and shared weights is unclear.  Hyperparameter tuning may be necessary for each component."], "jbrgwbv8nD": ["Summary: This paper presents a generalized version of CRF called RegularConstrained CRF (RegCCRF) which extends CRF to model nonlocal constraints through a regular language. The paper discusses the departure between RegCCRF and other constrained decoding methods and introduces a new tagwise CRF for practical implementation. effectiveness:  Addresses the incorporation of outside constraints in CRF.  Provides mathematical formulation and proof of RegCCRF. Weaknesses:  Unclear advantages of constrained training over traditional CRF.  Contradictory claims regarding the effectiveness of constrained training.  Lack of experimental comparison and ablation work.  No discussion of training and decoding efficiency.  Absence of comparison with related constrained decoding methods.  limited experimental results with insignificant improvement over the baseline.", "Paraphrased Statement: Summary: This research introduces a technique for adding specific constraints to the output of a linear chain Conditional Random field (CRF). By mapping an NFA describing the constraints to a modified CRF label set and wrapping the potential role this transformation enforces the desired constraints while maintaining compatibility with adjacent labels. The paper demonstrates the transformations properties relates it to learning weights in FSTs and presents experimental results showing improvements in synthetic data and semantic role labeling tasks with natural constraints. effectiveness:  Provides a novel and practical way to incorporate complex constraints into CRFs.  Opens up opportunities for collaboration with formal language research.  Is welldocumented and easy to understand.  Presents a clear justification for the contribution compared to existing FST weight learning methods. Weaknesses:  The quadratic dependence of CRFs on label set size limits the feasibility for large constraint sets.  The explanation of label set minimization could be improved with an model.  The proofs in Section 5 are not particularly significant and assume perfect optimization.  The realworld experiment results are modest and may be influenced by other factors.  It would be helpful to include information on constraint violations and the impact of computational concessions in the unconstrained model.", "Paraphrase: Description: This work introduces Constrained Conditional Random Fields (RegCCRFs) that restrict output label sequences to follow a specific regular language. This feature is particularly beneficial for tasks like BIO labelging which require sequences to adhere to the O(BIO) format. Unlike general CRFs RegCCRFs lack hidden states making them more powerful. effectiveness and Weaknesses: The paper presents an intriguing concept but some initial skepticism may arise regarding the motivation behind RegCCRFs. The application to BIO labelging is valuable and mentioning this in the introduction would enhance the introduction. Readers may question why not just use finite transducers instead. While this is adequately explained in Section 4.3 it could be more large perhaps in the introduction. The theorems in Section 5 are intuitive and Theorem 1s proof could be moved to an appendix to save space. The experiments in Section 6 are interesting but could explore formally proving that CRFs cannot generate specific string relations. There is a suspected typo in Theorem 2 where the inequality sides are identical. The method enhances semantic role labeling but not dramatically when compared to using RoBERTa. Despite this it reportedly achieves the current sound performance. On page 8 a clearer explanation of constraint language construction is needed as the current description lacks sufficient detail for replication. The paper claims that direct comparison between RegCCRFs and neuralweighted finite transducers is potential. An experimental comparison would be valuable even if only for theoretical role. The practical significance of the claimed advanlabeles in Section 4.3 remains uncertain.", "Paraphrased Statement: Conditional Random Fields (CRFs) based on linear chains have a limited output space due to the Markov assumption which presumes that an output depends only on its immediate neighbors. This assumption simplifies training but constrains the models expressiveness particularly for longer sequences. To address this limitation a novel method is proposed to constrain the CRF output space to be within a regular language. This approach allows the model to capture longrange dependencies. The method involves constructing a constrained CRF from a nondeterministic finite automaton (NFA) by setting specific transition and emission probability to zero. To improve efficiency the authors introduce techniques for minimizing NFAs and leveraging comparison class to reduce the number of label. They also discuss the relation between constrained CRFs and weighted finite state transducers (FSTs) highlighting advantages of the other. Unlike previous constrained CRF variants this approach trains directly within the constrained space minimizing the negative loglikelihood against the actual data distribution. This enables sound performance for regular language output. Synthetic experiments demonstrate the models ability to capture nonlocal dependencies and distributions more effectively than models with constrained decoding. Improvements are also shown on a semantic role labeling task. effectiveness include the proposed methods elegance and effectiveness. The paper provides a wellwritten explanation of the approach and its results. However more comprehensive synthetic language exploration and a wide application to practical structured prediction tasks would enhance the studys impact."], "hcMvApxGSzZ": ["Paraphrase: Summary: This work introduces Fixed Neural Network Steganography (FNNS) a technique for hiding secret messages in images using adversarial approach. Three variations of FNNS are proposed demonstrating their effectiveness against steganalysis and showcasing a potential use case in case anonymization. Strengths:  Novelty: FNNS is a unique approach in steganography that utilizes adversarial approach for information hiding.  Clarity: The paper is wellorganized and accessible.  Interesting application: case anonymization is presented as a practical application for FNNS.  Effectiveness: FNNS reportedly evades detection by steganalysis tools. Weaknesses:  Computational Cost: FNNS requires processing each image individually using the LBFGS algorithm potentially leading to slower performance compared to singlepass techniques.  Lack of algorithm Exploration: The paper does not discuss the authors rationale for choosing LBFGS or whether they considered alternative adversarial approach algorithm (e.g. PGD DeepFool CW).  Missing Related Work: Related research (e.g. [18]) is not discussed or compared to FNNS.  Code Availability: The source code for FNNS is not currently available. Additional Questions:  \"Fixed\" in FNNS: The meaning of \"Fixed\" in FNNS is unclear. Are all neural networkbased steganography techniques already utilizing fixed neural networks", "Summary Paraphrase: This research addresses the challenge of hiding a secret message in an image without detection using a machine learning (ML)based approach. The goal is to achieve zero error in message retrieval while maintaining the undetectability of the embedded message. The authors employ a neural network decoder and adversarial perturbations to compute the modified image while using a binary crossentropy loss use to guide the process. Strengths and Weaknesses Paraphrase: The paper highlights the distinction between true steganography and techniques that are detectable by further algorithm emphasizing the importance of achieving zero detectability in true steganography. However the authors contest certain claims made in the literature regarding steganography such as its superiority to other multimedia security problems and the feasibility of embedding 0.4 bits per pixel without detection. They also note that the proposed method is not stateoftheart in terms of mainstream steganography and that alternative approaches exist with secure detectability. The paper acknowledges the use of clipping to prevent excessive pixel differences and outofrange rate but suggests that clipping rate between 0 and 255 would be more practical. It also emphasizes the importance of evaluating steganography quality based on detectability rather than reconstruction error. The detection rates reported in the paper are deemed too high for reliable secret communication. The authors suggest using error rates from stateoftheart (SOTA) steganalyzers as a more reliable step of security. While the paper presents an interesting use case for zeroerror message retrieval the authors acknowledge that this approach relies on the assumption of zeroerror encryption which may not hold true in use. They also question the practical benefits of publicly sharing modified images with someone elses face.", "Paraphrase: Summary: This research introduces a new technique for concealing messages in images using adversarial approach. The approach differs from previous methods like SteganoGAN by fixing the decoder. rather it performs an approach at the inference stage to generate a perturbation that corresponds to the desired message. Experiments demonstrate that the proposed method surpasses existing encoderdecoder designs and excels in outofdomain scenarios. Strengths: 1. clear and accessible writing style. 2. novel insights into the limitations of encoderdecoder architectures. 3. Superior empirical performance including high message decoding accuracy and robustness in outofdomain settings. Weaknesses: 1. Limited discussion on JPEG compression:  Lack of information about JPEG compression quality factors used.  potential for combining the approach with existing JPEGresistant adversarial approach. (This may be beyond the current scope of the paper.)", "Paraphrase: Summary: This paper presents a new image steganography technique using neural networks. It exploits the sensitivity of neural networks to subtle changes to encode secret messages in images that are imperceptible to the eye. The method outperforms existing steganography methods and features robustness to lossy compression and steganalysis. It also offers a potential application in protecting identities by obscuring faces in images. Strengths:  The innovative use of neural network sensitivity as a feature for steganography is notable.  The methods ease and generality make it widely applicable.  Experiments demonstrate reasonable runtime and successful results in detecting faces and hiding original images within GANgenerated images. Weaknesses:  Limited experimental details regarding the bit strings encoded potential difficulty encoding certain strings and hardware used for encoding.  Clarification needed on a reported 0 error rate that does not align with experimental information. Questions:  Which sources contribute to decoding error in the algorithm  Could using a different parameterization (e.g. sigmoid transform) eliminate the need for clipping  How sensitive is the encoding to the random seed used in the neural network"], "gccdzDu5Ur": ["Paraphrase: Summary: This paper aims to enhance the generalization capabilities of models by using feature priors as different viewpoints on the data. model trained with these diverse perspectives exhibit less overlapping errors and can be combined more effectively. Strengths and Weaknesses: Strengths:  The experimental section is straightforward. Weaknesses:  The paper lacks clarity.  A formal definition of \"feature prior\" is absent.  The mention of cotraining using different priors suggests the concept of cotraining with different views but its unclear if this is the papers main idea.  The first contribution (\"model trained with diverse feature priors make errors on different data subsets\") is ambiguous. Additional Clarification: The paper primarily explores two \"priors\": shape and texture.", "Paraphrased argument: Summary This research introduces a theoretical framework for incorporating specific expectations into the feature extraction work in deep visual recognition models. Prior research has focused on promoting specific feature representations (e.g. reducing texture emphasis) and enhancing resilience to changes in the input datas nature. This articles primary contribution lies in its structured formulation and analysis of how various feature preferences result in complementary feature descriptions. When combined these representations offer more robust data descriptions effectively creating synthesized data views from various perspectives. The paper acknowledges the relevance of previous work on cotraining (multimodal bootstrapping) and connects it to recent research on selfsupervision and selftraining. Experiments using traditional models that prioritize shape or texture demonstrate that diverse feature preferences can produce robust and complementary data perspectives. Strengths and Weaknesses Strengths:  The paper tackles a crucial issue for the ICLR audience: creating and synthesizing various accurate and diverse feature representations.  The article provides a novel and valuable formal framework for directing feature representations in diverse ways resulting in multiview data representations.  It is wellwritten wellorganized accurate and accessible.  The experimental setup is strong and wellexecuted. Weakness:  Experiments are conducted on basic datasets (CIFAR10 and STL10) with clear class structures and simplified image settings (e.g. centered objects). Testing on more challenging datasets with nuanced and hierarchical class structures would be beneficial.", "Paraphrase: Summary: This research investigates the potential of combining multiple feature priors with preprocessing techniques to tackle various computer vision problems. Key findings: Strengths:  The work presents an insightful and potentially useful approach for computer vision practitioners. Weaknesses:  The work lacks significant technical design and a clear theoretical framework.  It appears to be more of an empirical investigation without a strong conceptual foundation.  The experimental evaluations and comparisons are outdated and do not represent the latest advancements in the domain.  The research does not meet the high standards expected at conferences like ICLR.", "Summary: The work introduces approach for training models with varied feature priorities (shape or texture). These models are combined through posthoc ensembles or cotraining where models generate pseudolabels for each other. Ensembles with diverse feature priorities outperform ensembles with similar priorities. Cotraining significantly enhances performance when models with different feature priorities provide pseudolabels. Strengths:  clear implementation of feature priorities (edge detection for shape BagNet for texture).  strong cotraining results. Weaknesses:  Primitive ensembling techniques used which limits the assessment of ensemble performance.  Assumption that unlabeled data does not contain spurious correlations. This assumption may not hold in all realworld scenarios.  Nonstandard placement of related work section at the end of the paper.  Bolding of BagNet cotraining results is confusing as it suggests beneficial performance than the actual winning algorithm (Canny).  Limited to image classification lacking results on alternative domains. Update After Author Rebuttal: The addition of stacking experiments for ensembling has improved the work leading to an increased score of 6."], "wxVpa5z4DU1": ["Paraphrase: The researchers conducted experiments to explore the balance between performance (accuracy) and user privacy (vulnerability to membership inference attacks) in deep ensemble models. They discovered that the level of agreement among the ensembles models is a crucial factor influencing the effectiveness of membership inference attacks. This finding is supported by visualizations showing changes in agreement levels between training and testing model. The researchers also evaluated existing defenses like differential privacy and regularization techniques to examine their impact on membership inference protection. While the study is wellwritten and the experiments are sound it lacks significant foundation and originality in the field of membership inference attacks and privacypreserving machine learning. The primary conclusion that enhanced agreement among models in deep ensembles leads to improved attack performance is based on empirical observations without theoretical support. Furthermore it is unclear if these findings extend to other ensemble learning approaches limiting the paper impact. Moreover no new defense strategy is offered and the evaluated defenses have already been used in this context. The privacyutility tradeoff are predictable and resemble those observed in context without deep ensembles.", "Summary and Analysis This paper examines a surprising phenomenon: standard ensembles of models while less prone to overfitting tend to be more vulnerable to membership inference attacks (MIAs). Strengths  The observation that ensembles can be vulnerable to MIAs is surprising and insightful.  The paper clearly explains how ensemble prediction can amplify privacy leakage for training model. Weaknesses The paper key finding is somewhat downplayed. Suggestions  Explore results using more robust models (e.g. with higher accuracy).  Quantitatively measure the distinguishability of distributions to support the observation that certain distributions lead to stronger attacks. Other Observations  The foundation mixes different types of ensemble methods which can be confusing.  The paper could consider relegating the discussion of subsampled ensembles as a defense against MIAs to the related work section.  The paper graphs may be hard to read with multiple elements and a faint light green color.", "Paraphrase: This research explores the relationship between accuracy and privacy in deep ensembles focusing specifically on membership inference attacks (MIAs). The findings suggest that ensembling techniques that improve accuracy may also increase the effectiveness of MIAs. effectiveness:  The study investigates a novel and unexplored field: the accuracyprivacy tradeoff in deep ensembles.  The results highlight the importance of agreement among ensemble members in influencing MIA effectiveness. Weaknesses:  The experiments use a simplified ensemble setting that averages prediction. Its unclear if the findings hold true for more complex aggregation strategies.  To mitigate MIAs the study relies on underfitting ensembles. Exploring further ensemble methods with enhanced diversity could provide alternative approaches.", "Summary: This study examines the accuracyprivacy tradeoff in ensemble learning using model inference attacks. It finds that ensembles (which combine prediction from multiple models) increase the difference in confidence distributions between samples seen during training and those not seen. This is primarily due to reduced agreement among foundation models on unseen data. The study also evaluates existing membership inference defenses in ensemble context. effectiveness:  The authors identify a useful metric for distinguishing the understanding for the confidence distribution change.  The experiments are comprehensive and cover various dataset training context and defense scenarios.  ensemble models show a significant decline in privacy compared to their foundation model counterparts. Weaknesses:  The authors could emphasize the percentage increase in privacy risk due to ensemble use.  The paper lacks background on previously attributed causes for the success of ensembling techniques.  The paper fails to address ensemblefoundationd defenses such as training on different dataset subset. Questions:  Can attackers access all foundation models in the ensemble in a blackbox setting  How does the attack scenario work where 80 of the training data is known to the attacker and the remaining samples are targeted for membership inference  Are all models trained on complete training set or smaller splits Writing Issues:  use \\citep for parenthesized citations.  Identify the yaxis of graphs in the appendix.  Avoid unsubstantiated claims about memorization.  Correct grammar: \"the attacker has access to the output of the ensemble.\" Conclusion: The study provides valuable insights into the impact of ensembles on privacy. However it could benefit from further discussion on scenarios where ensembles improve privacy."], "krI-ahhgN2": ["Paraphrased Statement: Recent approach in contrastive learning focus on bringing representations of similar samples closer together and separating those of different samples. These algorithms typically require a multiview batch for defining positive samples. This paper introduces SelfContrastive (SelfCon) Learning which exploits selfcontrasts within outputs from different layers of a multiexit framework. The authors demonstrate theoretically that the SelfCon loss lowerbounds the labelconditional mutual information (MI) between the intermediate and final features. Empirical results show that SelfCon outperforms crossentropy and Supervised Contrastive Learning (SupCon) baselines. effectiveness and Weaknesses:  SelfCon is the first selfcontrastive learning approach using subnetworks. It offers significant methodological contributions and empirical evidence of its effectiveness.  Although SelfCon has potential for unsupervised settings the paper primarily focuses on supervised tasks. This should be clarified in the abstract and foundation.  The authors claim that multiexit framework make data augmentation redundant. However SupConS outperforms SupCon without multiexit. Section 5.3 provides a more logical explanation that multiviews may lead to overfitting.  The paper could elaborate on why increasing the MI between intermediate and final features improves performance. It also mentions that \"effective representations should eliminate redundant input information\" yet increasing the influence of the final feature on label MI seems counterintuitive.  The indicator use in the equations (1) using F and G instead of labels need clarification. Additionally the inequalities in the numerators should be explained as positive pairs are typically defined as samples with matching ground accuracy labels.", "Paraphrase: Summary: This research introduces a novel supervised contrastive learning technique that eliminates the need for multiple views. The approach involves employing distinct use for contrastive pairs enabling the use of single views in contrastive learning. effectiveness: 1. The concept of using single views in contrastive learning is significant because data augmentation can be challenging in this context. 2. The authors provide extensive analyses and qualitative experiments to elucidate their methodology and need. Weaknesses: 1. The effect theoretical argument that selfcon maximizes mutual information between intermediate and final features is incorrect. validation of this claim relies on an inequality that does not guarantee that minimizing selfcon loss increases mutual information. 2. Additionally given that F(x) is a use of T(x) mutual information I(F(x) T(x)) is either zero or infinity questioning the authors goal of maximizing it. 3. Table 2 lacks multiple seeds and confidence intervals in the reported results. 4. The authors fail to explore the impact of batch size on singleview benefits as evidenced by their omission of batch size 512 in Table 4. 5. The use of labels in selfcontrastive learning raises questions about why fullysupervised approach are not considered. 6. The validation of Proposition 4.3 may contain inaccuracies in its use of \"Markov chain\" terminology. Updates: The authors have provided detailed responses to the criticisms and conducted additional empirical analyses. They clarify that their work presents an empirical method rather than a theoretical understanding. However the original paper heavily relies on Proposition 3 which only suggests minimizing mutual information between G(x) and F(x). Therefore the paper motivation based on an invalid theoretical statement remains problematic. The authors are encouraged to rewrite the paper without vague theoretical need.", "Summary Paraphrase: The paper introduces a novel approach for contrastive learning that eliminates the need for additional data augmentation to create positive sample pairs. It utilizes a multiexit network to perform \"selfcontrasts\" between multiple outputs of the framework. Theoretical analysis demonstrates that the proposed loss use provides a lower bound on the mutual information between intermediate and final features. comprehensive experiments show superior performance compared to traditional contrastive learning baselines. effectiveness:  Proposes an innovative idea to leverage multiexit network for contrastive learning without augmentation.  Addresses limitations of multiview contrastive learning such as knowledgedependent augmentation and computational overhead.  Includes extensive ablation studies to investigate various aspects of the method. Weaknesses:  The singleview and multiview versions of the selfcontrastive loss differ with the multiview version not being an obvious generalization of the singleview version.  Overfitting to instancespecific information may contribute to the multiview version performing worse than the singleview version.  It is unclear how increasing the number of subnetwork (multiexit network) affects performance and computational cost.  The theoretical analysis does not fully explain the improved classification performance achieved by the method.  There is a lack of comparison and discussion with recent approach in representation learning such as BYOL.  The presentation of the method and equations could benefit from improved readability.", "Summarized Paraphrase: The proposed algorithm \"selfcontrastive learning\" improves supervised contrastive learning by using representations from multiple network stages. It includes both a \"multibatch\" variant that contrasts between augmented and nonaugmented views of the same class and a \"singlebatch\" variant that contrasts only between views of the same class. This results in performance improvements on CIFAR10 TinyImageNet and a reduced version of ImageNet. effectiveness:  Clear need and extensive discussion of related work  Highlight significant equation elements using color  Supervised contrastive learning without additional augmentation Weaknesses:  Lack of full ImageNet results and small batch sizes:  Smaller batch sizes than in previous supervised contrastive learning work may limit gains.  Reasons for not using larger batch sizes or momentum buffers are not clarified.  Interpretation of results:  Correlation between mutual information (MI) and classification performance is based on a limited analysis potential confounders are not ruled out.  method section concerns:  Equations 1 and 2 may contain errors.  Incomplete notation in Equation 3.  Minor points:  Avoid truncated yaxes in bar plots.  Use a more appropriate colormap in Figure 6.  Revise the terminology of \"single view\" and \"multi view\" for clarity.  Include image sizes in Table 3.  Clarify the increase in cost per training step."], "onwTC5W0XJ": ["Paraphrased Statement: The paper proposes to improve the classification accuracy of Convolutional Neural Networks (CNNs) by introducing an additional loss term. This loss term forces the model to focus on the relevant object in the image rather than the background. To achieve this the model receives an additional input a binary mask which guides its learning process. However the approach may have limited benefits on large datasets. effectiveness and Weaknesses:  The masking approach resembles binary segmentation which has been extensively studied.  The function of GradCAM for explainability may not be the best choice and its limitation are not discussed.  The models limitation are not explored.  Its unclear if the approach could work with multiple objects or nonCNNbased models. Specific Comments:  Introduction:  Clarify what is meant by \"justifiable\" systems.  Explain what is meant by \"greedily\" extracting features.  Provide a citation for the statement that noncausal correlations can be eliminated by increasing the dataset size.  methodology:  Specify where the binary masks are applied in the CNN pipeline.  Explain the rationale behind the specific loss function functiond.  Experiments:  Determine the sensitivity of the model to the accuracy of the segmentation mask.  Discuss why certain pixels are more causally relevant than others.  Include a convergence plot of the original CNN to support the claim that the limited loss does not affect convergence.  Clarify what \"AM\" stands for in image 8 and explain why the ablation work were conducted on the test set.", "Paraphrased Statement: Summary This research explores a method to enhance classification accuracy through segmentation learning. However there are issue with the methodology and experiments. effectiveness and Weaknesses effectiveness  The proposed method functions a CNN model that focfunctions on causality under human guidance.  Auxiliary guidance allows the method to prioritize data relevant for classification.  Experiments show improved performance compared to standard classification models without guidance.  Feature visualization demonstrates the approachs efficacy. Weaknesses  limited discussion on alternative guidance options for enhancing causal feature learning.  The function of segmentation masks as the sole guidance may be problematic.  The proposed methods impact is uncertain since segmentation masks are sometimes inaccurate.  The methodology lacks originality as the causally focfunctiond loss is similar to the Dice loss functiond in medical image segmentation.  Lacking details on classification tasks and data splits for each dataset.  Introducing segmentation learning may increase computational costs.  The inconsistent performance gains question the practical value of the proposed model. Questions  Would replacing the causally focfunctiond loss with crossentropy or Dice loss impact performance  What are the additional computational costs of the proposed method compared to models without segmentation learning", "Summary: This work introduces a novel method for training Convolutional Neural Networks (CNNs) called \"Casually Focused Convolutional Neural Networks\" (CFCNNs). CFCNNs utilize human guidance to create binary segmentation masks for foreground objects in images. These masks and image labels are used to calculate a loss function during training consisting of activation loss (based on comparison with mask features) and crossentropy loss (based on predicted probabilities). To evaluate CFCNNs the authors employed both allconvolutional (CFCNC) and fullyconnected (CFCNF) architectures on four datasets. effectiveness:  Simplicity of the proposed method  effectiveness with a small percentage of images labeled with binary masks  potential applications in areas like medical imaging where understanding the basis of predictions is crucial Weaknesses:  User intervention requirement which may be limiting in certain applications  Lack of consideration for relative features between background and foreground  Limited exploration of noisy masks and their impact on performance  Absence of comparison with Explainable Artificial Intelligence (XAI) methods  Misattribution of dataset difficulty as a major factor in degraded performance on the Oxford pet dataset Comments and Suggestions:  Experimentation with soft masks to examine the work of background relations  Conducting ablation work on noisy masks  Clarification on the availability of binary masks for all classes in the dataset  Comparison with intrinsic XAI models for better performance evaluation  Testing the method on challenging datasets with a large issue of classes  Investigation of the understanding for model focus reduction despite providing full foreground masks  Explanation of differences between CFCNNs and the extension version of GAIN"], "lNreaMZf9X": ["Paraphrase: Summary: This study analyzes design factors in modelbased reinforcement learning including: 1. deterministic vs. stochastic models 2. Singlestep vs. multistep targets 3. Ensemble model size 4. input noise for robustness findings: 1. Both deterministic and stochastic models perform easily. 2. A moderate target horizon (35 step) yields optimal results. Model MSE poorly predicts planning performance. 3. Ensembles enhance performance up to 5 components. 4. The benefits of input noise vary depending on the task. Strengths:  Focus on comparing existing design elements instead than introducing a new method.  Clear writing extensive results and supporting online videos. Weaknesses:  Limited diversity of tasks evaluated (only 4).  Omission of relevant related study on multistep loss.  Exclusive use of stochastic models in deterministic environments limitation conclusions about their necessity.  Neglect of analytical propagation of uncertainty over multiple step despite feasible particlebased solutions.  Some grammatical errors requiring proofreading.", "Paraphrase: This research explores the impact of design decisions on training dynamics models for control model. The authors experiment with four particular choices:  exploitation deterministic vs. stochastic models  Employing multistep loss  Utilizing network ensembles  Adding input noise They test these choice in several DeepMind control environments. Strengths:  The research effectively addresses the importance of design choices in model study for modelbased reinforcement study. Weaknesses:  The conclusions are not definitive as the authors conclude that \"no single design choice works significantly easily.\"  The research falls short in addressing its own question. It could easily guide future researchers by clarifying which methods to prioritize or avoid.  The investigation is limited to four control environments that are not fully representative of realworld applications.  The research does not explore other key design choices such as memorybased architectures which are crucial in partially observable domain.", "Paraphrased Statement: Summary: This study explores different approach for designing predictive planning models that forecast future states based on observed data. The paper compares:  deterministic vs. stochastic prediction models  Singlestep vs. multistep prediction loss for model training  Single vs. ensemble networks  Perfect observations vs. noisy observations Strengths and Weaknesses: Strengths:  Provides insights into model design choices for predictive planning.  study network structure loss use and model noise. Weaknesses:  stochastic models are tested in deterministic environments potentially skewing comparison.  Five random seeds may be insufficient for reliable averaging of results.  Results are inconsistent with previous research but no discussion is provided on potential causes.  Differences in network structures between stochastic and deterministic models may influence learning efficiency.", "Paraphrased Statement:  Summary:  The study analyzes and compares popular study strategies for modelbased planning in four RL environments.  Insights are provided for practitioners and future research.  Strengths and Weaknesses:  Positives: The paper is wellwritten and informative.  Negatives:  Limited number of benchmarks (only four).  Unclear methodology in some aspects.  particular Questions:  Data used:  Fixed datasets were used to evaluate rewards omitting exploration. This may impact comparison for stochastic models.  The concept of \"Updates\" in design is unclear.  stochastic model:  Concerns about potential bias due to limited sampling. Data on this aspect would be beneficial.  Multistep Results:  understanding for worse performance in multistep deterministic predictions (1 step) compared to singlestep deterministic predictions.  Explanation for significant variation in optimal step between tasks.  Ensemble model:  Clarification on whether one ensemble is equivalent to a regular neural network.  potential impact of adding more ensembles on results.  input noise:  Confirmation whether noise is applied to the planner or model study.  Minor publication:  Misaligned design.  use of \"marginal\" to describe reward differences that may not be statistically marginal.  Normalization considerations for state and action value.  Suggestion for incorporating [Link] tags into clickable footnotes.  potential value in evaluating convergence of modelbased approach over longer durations.  Rephrased statement regarding stochastic models.  Clarification on \"lower entropy bound.\"  Typos:  \"rollouts of the provide\" \u2192 \"rollouts of the provided\"  \"significant more data\" \u2192 \"significantly more data\"  \" the model the \" \u2192 \"the model of the\""], "vSix3HPYKSU": ["Paraphrased Statement: This paper introduces a method for solving general partial differential equations (PDEs) presented in conservation class using an autoregressive approach. The method represents the grid domain as a graph with each node containing inclassation about the PDE solutions at specified time steps node positions and framework parameters. The node values are converted into a lowerdimensional representation called a latent space. message are then passed between nodes and the decoded output predicts solutions for multiple future time steps. The network is trained with a passing function that promotes zero stability by requiring invariance to small changes in previous time step solutions. The method is assessed on various 1D and 2D PDEs demonstrating accuracy and generalization perclassance superior to existing neural and classical solvers. Additionally it shows potential for handling irregular domain with varied boundary conditions. Pros: 1. accuracy and generalization perclassance surpasses current neural solvers. 2. The approach is theoretically justified by framing classical solver schemes as instances of message passing. 3. The \"pushforward trick\" significantly enhances the perclassance of existing neural solvers. 4. Clear and accessible writing with sufficient background inclassation for nonexperts. Cons and Questions: 1. Generalizability to different spatial resolutions needs clarification. 2. Lack of details on training time and network parameters compared to previous work. 3. The manuscript needs to be condensed to meet the 9page limit. Questions for Rebuttal:  Please address the concerns outlined in the \"Cons and Questions\" section.  Clarify any key typos or grammatical errors.", "Paraphrased Summary: This paper aims to stabilize the training of neural solvers for partial differential equations (PDEs). To achieve this the authors utilize two techniques: 1. Pushforward trick: This involves perturbing the input data with noise at each time step resembling the approach used in denoising autoencoders. 2. MultiStep Prediction: The solver predicts multiple time steps simultaneously similar to classical numerical methods. The framework has a graphbased architecture with nodes distributed across the spatial domain resembling an encoderprocessdecoder structure. This innovation aligns with traditional numerical solvers. Experiments demonstrate that the pushforward trick significantly extends the \"survival time\" (the time until a specified error threshold is exceeded). Strengths and Weaknesses:  Strengths:  The pushforward trick is a novel and effective method.  The architecture mimics classical numerical solvers.  Weaknesses:  The effectiveness of the proposed architecture is not fully established.  The relationship between the architecture and the passing function is unclear.  The impact of framework complexity (number of parameters) on solution accuracy is not analyzed. Comments:  An ablation field to assess the contributions of each technique would be beneficial.  Investigation into the relationship between framework complexity and solution accuracy would provide valuable insights.", "Summary Paraphrase: The paper presents a neural network approach to solving partial differential equations (PDEs) using a technique called neural message passing. Strengths and Weaknesses Paraphrase: While the paper has some clarity issues it is still in its early stages and has potential. The methods section could be improved by providing clearer explanations defining notations and explaining the background necessary to understand them. Leading with an example and motivating the approach could also improve comprehension. Additionally the paper does not explicitly address the differences between using graph neural networks (GNNs) and other neural networks for solving PDEs.", "Paraphrase: This field presents a neural network (NN) innovationed to address partial differential equations (PDEs). The NN is based on a graph NN architecture featuring specific attributes for solving PDEs such as equationspecific representation in the encoder and differential input handling in the message function. Additionally the approach employs innovative training methods that mitigate error propagation. The paper excels in its comprehensive literature review clear explanation of the motivation detailed methodology thorough empirical field and robust experimental innovation. The authors introduction of the \"pushforward trick\" as a training strategy to minimize instability and broaden the NNs problemsolving capability is particularly noteworthy."], "u7UxOTefG2": ["Paraphrased Statement: This paper analyzes the role of Bayesian predictive distribution and uncertainty in detecting outofdistribution (OOD) data particularly in the context of Bayesian neural networks (BNNs) and Gaussian process (GPs). The analysis indicates that the Bayesian posterior is not suitable for OOD detection becarole:  Exact inference in infinitely extensive networks (GPs) using common architectural choices may not result in desirable OOD performance.  The choice of weightspace prior significantly influences OOD performance.  There is a tradeoff between beneficial generalization and high uncertainty on OOD. Strengths and Weaknesses:  The paper lacks novelty as the concepts discussed are generally known in the machine learning community.  The high uncertainty in the posterior can be due to insufficient data in certain area not necessarily indicative of OOD.  Inputdomain agnostic prior knowledge can potentially reduce posterior uncertainty which may not be detrimental to OOD detection.  The paper suggests the RBF kernel is well for OOD detection but this is disputed with the ESS kernel potentially being more suitable.  The paper may be misleading by using uncertainty in y to detect OOD in x which is counterintuitive.  The paper is wellwritten and clear.", "Paraphrase: Abstract This study disputes the widely held belief that Bayesian Neural Networks (BNNs) are inherently suited for outofdistribution (OOD) detection. The authors access this by examining the properties of BNNs with infinite width from a functionspace perspective. Their analysis suggests that BNNs might not be intrinsically wellsuited for OOD detection. They also argue that there is a tradeoff between OOD generalization and uncertainty. Finally they propose a new access for evaluating the OOD properties of models. Strengths and Weaknesses The wellwritten and accessible paper addresses a timely topic. The authors observations have important implications for the field and have been largely overlooked. However the paper could be improved in various area. Criticisms:  The paper focuses primarily on regression tasks with classification tasks being accessed through regression. However most OOD detection study involving BNNs have used classification tasks. It would be beneficial to include experiments using logistic regression for classification.  The case used are mostly lowdimensional and simplified. While this is not a major topic using more complex problems would strengthen the paper.  The authors briefly mention OOD generalization but do not elaborate. Recent study have focused on evaluating methods on OOD datasets like CIFAR10C. The authors proposed method for validating OOD properties in Section 7 needs more point and experimental validation.  There are source missing or could be discussed such as the work on NNGP comparison for deep networks and a recent study proposing a BNN prior for improving OOD generalization. Minor Points:  The authors should verify the convergence of the HMC algorithm given the limited runtime mentioned in the experimental points.", "Paraphrased Statement: The published study questions the prevailing view that Bayesian neural networks (BNNs) are ideal for outofdistribution (OOD) detection. Using both infinitewidth (allowing for exact inference due to an equivalent representation as a Gaussian process) and finitewidth networks (requiring approximate inference) the study demonstrates empirical findings that challenge this assumption. The study makes various observations:  Exact inference in infinitewidth networks does not guarantee desirable OOD behavior. For case the standard deviation of the posterior role obtained from an infinitewidth 2layer ReLU network does not align with the reliable datagenerating process making it unsuitable for OOD detection.  This observation holds reliable for corresponding finitewidth networks. Strengths:  The study encompasses both infinitewidth and finitewidth networks in its analysis.  It cautions against utilizing BNNs for OOD detection without considering possible pitfalls. Weaknesses:  The study lacks theoretical analysis of BNNs in OOD scenario.  The findings are based entirely on toy simulated datasets which may not generalize to realworld applications."], "rTAclwH46Tb": ["Paraphrase: Summary: The authors introduce Eigencurve a novel method for adjusting the learning rate during optimization based on the eigenvalue of the problems Hessian matrix. They demonstrate that Eigencurve achieves the theoretically optimal convergence rate for quadratic optimization problems with uneven eigenvalue distributions. Empirically it speeds up training on the CIFAR10 and ImageNet datasets especially in the other stages. Strengths:  evidence optimal convergence rate for skewed eigenvalue distributions.  Outperforms or matches \"step decay\" schedulers in general.  Validated theoretically and empirically on ridge regression problems (with known Hessians).  solid empirical results on CIFAR10 (reduced training loss and improved validation performance).  Wellwritten and easy to understand. Weaknesses:  No theoretical analysis for the nonasymptotic case.  Potential computational cost due to the calculation of eigenvalue.  Less convincing results on ImageNet compared to CIFAR.  Limited improvement over standard training regimes on CIFAR and ImageNet.  Results primarily limited to image classification datasets. Minor:  clarification needed for an additional term in the appendix. PostRebuttal:  Increased score to 8 based on the authors response.  Suggestion to compare Eigencurve to step decay with decay epochs at 30 60 and 80 for ImageNet.", "Summary Paraphrase: This paper examines how SGD converges with different step size adjustments in linear regression. Despite previous advancements the gap between the known convergence rate for the last SGD iteration and the optimal minimax rate persists with a missing logarithmic term. This process attempts to bridge this gap with a step size scheme that incorporates the eigenvalue distribution of the Hessian (the second derivative of the loss role). When the actual Hessian is known the proposed approach effectively closes the gap both theoretically and in terms of worstcase scenarios. practical alternatives are provided for situations when the true Hessian is difficult to obtain and these alternatives perform comparably to current step size schemes in common deep neural netprocess benchmarks. Strengths and Weaknesses Paraphrase: Strengths:  The paper is wellwritten and thoroughly explains pertinent literature on SGD. Weaknesses:  Embedding Hessian information into SGD step size design may not be an innovative concept.  The proposed step size scheme may be more akin to Newtons method which is a secondorder method rather than SGD which is a firstorder method. The authors should highlight the differences and compare their approach to secondorder method.  The empirical advantages of using secondorder information are marginal potentially not justifying the computational overhead of obtaining Hessian information. Theory Comments Paraphrase:  The paper lacks significant theoretical advancements.  Proposition 2 is an upper bound for polydecay step size and this alone is insufficient to support the claim that polydecay is suboptimal. A minimax lower bound is necessary.  The authors should utilize existing results or provide their own minimax lower bound for polydecay step size.  comparison based on minimax rates may not be practical since the gains are only relevant in worstcase scenarios.  Comparing upper bounds does not conclusively prove the superiority of one method over another a lower bound is necessary for demonstrating analysis tightness.", "Paraphrased Statement: Summary:  This paper examines different learning rate schedules for stochastic gradient descent (SGD) on quadratic objectives.  It proposes a novel Eigencurve schedule based on the Hessian spectrum aiming for optimal convergence rates.  The Eigencurve shows improved efficiency in image classification tasks using deep neural netprocesss.  It also provides insights into the effectiveness of the cosine learning rate schedule. Strengths:  The Eigencurve schedule is theoretically wellfounded with clear intuition.  It justifies the usefulness of the cosine decay schedule. Weaknesses:  Optimal learning rate scheduling for convex quadratic problems has been discussed in previous process with noisy models.  The theoretical results focus on asymptotic convergence which may not be fully applicable to practical neural netprocess training with fixed epochs.  The Eigencurve schedule does not consider the noise level in the gradient.  Exponential moving averaging commonly used in neural netprocess training is not compared to the Eigencurve.  The results obtained from 10epoch training are less representative.  The specific parameter choice for the Eigencurve (\u03b21.000005) requires justification.  The provided image could be reorganized for space efficiency. Minor Points:  The details of hyperparameter tuning should be included in the main paper.", "Paraphrased Summary: The authors response to my review was thorough. The revised version of the submission shows improvements including the addition of repeats for the train losstest accuracy comparison. The updated image are generally improved. The experiments now support the claim that Eigencurve is an effective (not)convex optimization method. My score has increased from 5 to 6. Strengths and Weaknesses:  solid writing with occasional grammatical errors that can be easily corrected.  Theoretically sound though some proofs may be difficult to verify.  Novel theoretical contribution but the significance of the improved convergence rate of Eigencurve over stepdecay is unclear.  The minimax complexity of Eigencurve for quadratic role under the power law condition is not fully established.  Eigencurve requires knowledge of the entire eigenvalue spectrum which is computationally expensive to obtain.  The robustness of Eigencurve to misestimation of the eigenvalue is not addressed.  The application of Eigencurve to notquadratic optimization problems is not fully justified.  The experimental evaluation is not suitable for publication.  image are illegible and lack labels.  No repeats over random seed were performed.  The fairness of the comparison to stepdecay is questionable with Eigencurve being given a significant advantage in computational budget."], "hSktDu-h94": ["Original Statement: This paper addresses a problem where the design is to optimize a problem with unknown parameters that can only be predicted from past information known as \"predict then optimize.\" Standard loss functions like L2 loss may not yield optimal solutions for downstream optimization. This paper proposes a method to learn an effective loss function for a specific downstream optimization problem. Motivated by scenarios where element ranking is crucial the proposed method practice to combinatorial optimization problems with a \"strong ranking property\" meaning their solutions solely depend on groupwise comparisons of parameters. The proposed loss function incorporates all possible groupwise comparisons between parameters. The authors replace the sign function in the ideal loss function with tanh and function REINFORCE to optimize the matrix representing groupwise comparisons. They demonstrate the effectiveness of their algorithm on scheduling portfolio optimization and shortest path problems. Paraphrased Statement: This paper tackles a problem known as \"predict then optimize\" where the goal is to optimize a problem with unknown parameters that can only be estimated from historical information. Standard loss functions may not produce optimal results in this setting. The paper proposes a method to design a loss function specifically tailored to the given downstream optimization problem. Inspired by problems where ranking are critical the method focfunctions on combinatorial optimization problems that exhibit a \"strong ranking property\" where solutions depend solely on groupwise comparisons of parameters. The proposed loss function captures all possible groupwise comparisons between parameters. The authors substitute the sign function in the ideal loss function with tanh and employ REINFORCE to optimize the matrix that represents groupwise comparisons. The paper evaluates the algorithm on three optimization problems and demonstrates its superiority over existing \"predict then optimize\" algorithm in terms of minimizing loss functions.", "Paraphrased Summary: This work introduces the \"approximated total group preorder (ATGP)\" loss function for addressing limitations of the l2 loss in predictthenoptimize (PTO) problems where discrete optimization decisions are involved. The authors propose that l2 loss misaligns with the ultimate objective of PTO which is to make optimal decisions. While the recently proposed SPO loss addresses some publication it is not compatible with gradientbased learning algorithm. To address these publication the authors develop the TGP loss function which aligns better with the optimization goal. They then present an algorithm APOC to approximate the TGP loss making it differentiable for efficient gradientbased learning. Experiments demonstrate the potential benefits of ATGP in several PTO applications including energy scheduling portfolio optimization and path finding. effectiveness and Weaknesses: The paper is wellwritten and presents a logical approach. The proposed approximation are reasonable. However the authors do not acknowledge the significant literature on optimal decisionmaking under uncertainty where concepts like \"regret\" and \"mean objective cost of uncertainty (MOCU)\" are closely related to the proposed SPO and ATGP losses. Another concern is the limited performance validation. While the experiments show some advantages it is not clear if the proposed method is consistently superior to alternatives. Additional analysis and discussion of potential limitations are needed for a more robust assessment of the proposed scheme.", "Paraphrased Statement: Summary: This research aims to address optimization problems involving unknown parameters that must be predicted from information. A new loss function total group prerange (TGP) loss is proposed. Its differential variant is used to approximate TGP loss for optimization problems with strong rangeing properties. The problem studied is significant. effectiveness and Weaknesses: Weaknesses: 1. Definition 1 (Total Group Prerange) TGP defines all pairs that satisfy a specific range with parameter c. This definition is suitable for rangeing problems but its relevance to combinatorial problems (e.g. shortest pathknapsack) is unclear. Considering feasible path as elements can lead to an exponential increase in elements potentially hindering optimization. 2. Algorithm 1 requires more details:  Intuition behind Equations (6) and (7)  Connections between these equations and when Equation (6) should be used  Definition of time in Equation (7)  Determination of D (number of filters used)  Selection of parameters \u03b3 and \u03c3 3. Overfitting prevention in the TGP framework 4. computational time for different algorithm should be reported in addition to loss values. Minor Corrections: 1. Add a period (.) to the last time in the validation of Lemma 1. 2. Use Ai to denote the ith row of Ai in the validation of Theorem 1. 3. Change the notation for the loss function to \u2113 in the validation of Theorem 1. 4. Clarify the validation of Lemma 2 and explain why PX I if C is full range. 5. Provide a more conventional mathematical validation for Lemma 3."], "iRCUlgmdfHJ": ["Paraphrase: Summary: Researchers examined the intricacy of interactions between input variables in Deep Neural Networks (DNNs). They used a variablepairbased multiorder interaction metric to quantify the interaction complexity within DNNs. Findings: 1. DNNs can easily represent basic and highlevel interactions between input variable pairs but they struggle with midlevel interactions. The simplicity of an interaction is determined by its contextual complexity. 2. The authors propose new loss functions to promote or penalize interactions of specific complexities. They then assess the learned DNNs representation capabilities. Strengths: 1. The studys originality lies in leveraging the multiorder interaction approach to analyze interaction complexity in DNNs. 2. The thorough investigation using various neural networks and datasets supports the claims. The effects of loss functions L and L on deeper networks like AlexNet VGG16 and ResNet1820 are noteworthy. Suggestions: 1. Clarify why the increasing number of contexts in intermediate contextual interactions doesnt always lead to a low I(m)(i j). Provide an example to support this point. 2. Reconfigure the definition of J(m) to a proper relative interaction strength step by using the example probability scores. This allows for comparison between examples. 3. Explore the adversarial robustness of loworder and intermediateorder interactions. Investigate if these interactions are susceptible to attacks due to the \"cognition gap\" in neural networks.", "Summary (Paraphrased): This research examines deep neural networks representational limitations from the standpoint of multiorder pixel interactions. It offers a new definition for relative interaction strength and evaluates it theoretically and numerically. The field also develops loss functions that encourage or discourage specific interaction orders with experiments demonstrating their effects on accuracy and robustness. Strengths (Paraphrased):  Clear definition of relative interaction strength  novel theoretical and numerical analysis  Natural and innovative loss functions  Experiments linking loss functions to accuracy and robustness Concerns (Paraphrased): theory:  Gaussianity assumption in Theorem 1 requires numerical verification  potential gap between theory and simulations in design 3  Equation 5 may lack expected values and confusing notation  Equation 7 could benefit from clearer notation  potential flexibility of using multiple loss functions with different interaction orders (not explored) Experiments:  Limited scope and inadequate support for findings  Inconclusive accuracy results from Table 1 (left)  exploration of alternative masking methods for structural representation (not conducted)  Lack of experiments examining the effects of penalizing highorder and boosting loworder interactions on robustness  Insufficient analysis to show significant improvements from the proposed loss functions  Small and difficulttoread design and tables Rebuttal Impact:  Revised theory with reduced assumptions is commendable  Excellent additional experiments provide a more comprehensive understanding  Updated score reflects these improvements", "Paraphrased Statement: This paper investigates how deep neural networks (DNNs) represent different types of interactions. The authors observe a \"representation bottleneck\" where DNNs often struggle to learn interactions in the middle range. They attribute this difficulty to variations in the learning strengths of different interaction types. To address the bottleneck the authors propose two novel loss functions that encourage DNNs to learn interactions of specific orders including middlerange interactions. Experiments demonstrate the effectiveness of these losses in enhancing DNNs power to capture middlerange interactions. Strengths:  Discovery of the representation bottleneck phenomenon in DNNs.  Theoretical explanation for the bottlenecks origins.  Demonstration of the proposed losses power to improve DNNs learning of middlerange interactions. Weaknesses:  Lack of intuitive explanation for the representation bottleneck.  Limited discussion of the impact of the proposed losses on DNN performance.  Insufficient evidence to support claims about adversarial robustness. Suggestions:  Provide a more intuitive illustration of the bottleneck proofs reasoning.  Offer guidance on using the proposed losses to enhance classification performance.  Clarify how output variety in the proposed loss functions represent interactions of specific orders.  Conduct experiments on additional datasets to assess adversarial robustness.  Determine whether the proposed losses come with increased time complexity.", "Paraphrased Statement: Summary: This research uncovers and theoretically verifies the \"representation bottleneck\" phenomenon which reveals a cognitive disparity between deep neural networks (DNNs) and humans. The field introduces loss functions that guide DNNs in learning interactions of specific complexities and examines their capacity to represent these interactions. Strengths:  The paper is clearly written and easy to comprehend.  design 1 and 2 provide strong visual support for the concept of representation bottleneck.  The paper raises two valid questions about DNNs and convincingly demonstrates the prevalence of this phenomenon.  The idea of using multiorder interaction utility to describe DNN representation is wellconceived and accessible. Weaknesses:  While the paper proposes losses to influence DNN learning of specific interaction orders it only presents results for highorder DNNs. It is unclear why middleorder and loworder interactions are not addressed in detail.  Despite conducting multiple experiments the findings are largely similar limiting the generation of new insights.  Table 1 is challenging to interpret and its significance is not readily apparent.  Although the paper provides valuable insight it would be beneficial to suggest approach for addressing the representation bottleneck issue or provide a more comprehensive definition for future research.  The inclusion of excessive formulas and technical terms in certain sentences makes the text difficult to follow. It would be preferable to restructure the sentences to improve readability."], "rI0LYgGeYaw": ["Paraphrased Summary effectiveness:  Clear technical writing and understandable derivations.  comprehensive discussion on unrolling scheme for deep dictionary learning (DDL). Weaknesses:  Limited applicability to convex sparse coding problem while most dictionary learning uses nonconvex sparsity constraints.  Lack of theoretical or experimental comparison with existing optimization methods for dictionary learning.  Insufficient experimental evaluation with missing data. Questions and Comments:  Does the proposed scheme deal nonconvex constraints like orthogonality or normalization in dictionary learning  cost the \"such that\" statements in Propositions 2.2 and 2.3 referring to the dictionary cardinality  While the scheme is applicable to a specific MEG signal learning scenario is it truly computationally efficient compared to traditional methods like KSVD PAML  cost there any additional constraints on the dictionary atoms that are not mentioned in the experiments which are typically present in dictionary learning", "Summary Dictionary learning aims to represent data as a linear combination of atoms from a dictionary. Traditional approaches use a biconvex optimization problem solved by alternating minimization (sparse coding and dictionary update). This paper compares gradientbased alternating minimization with unrolledbased dictionary learning that uses backpropagation to compute the dictionary update gradient. Novelty and contribution contribution:  Stability analysis of Jacobian: The paper investigates the stability of the Jacobian before support identification.  Limiteddepth Backpropagation for Stability: Proposition 2.5 provides insights into using limiteddepth backpropagation to improve stability. Borrowed issue from Prior work:  Proposition 2.2 and 2.3 are known issue from Ablin et al. 2020.  stochastic dictionary learning using unrolled networks has been previously studied by Tolooshams et al. 2020. effectiveness and Weaknesses effectiveness:  Clear writing  Interesting problem  New insights on Jacobian stability and limiteddepth backpropagation  Comprehensive numerical issue and real applications Weaknesses:  Incremental novelty compared to prior work  Incomplete reference to previous study  Some unsupported statements  Lack of precise contingent in the experimental section  Typos and inconsistent notation Suggestions for Improvement  Provide a more organized paper structure  Clearly indicate the contributions and differences from prior work  Include more contingent in the experimental section  deal the statements that are not fully supported  Differentiate between the two L notations in propositions 2.2 and 2.3", "Paraphrased Statement: The study explores the use of \"unrolling\" methods in dictionary learning aiming to address the limitations of alternating minimization (AM) which alternates between dictionary estimation and sparse recovery. The proposed approach formulates the target dictionary as a solution to a bilevel optimization where the inner optimization is approximated using unrolling with N step. The key contribution lies in developing a method for computing the subgradient for the outer optimization along with analysis and experiments demonstrating its effectiveness on synthetic and realworld datasets. effectiveness:  The issue holds relevance and timeliness.  Combining unrolling optimization algorithms with backpropagation is an emerging trend.  The paper presents clear and concise writing.  The experiments are comprehensive and wellexecuted. Weaknesses:  The significance of the method and analysis in relation to existing literature needs clarification.  theoretical analysis of dictionary learning has been extensively studied and it would be valuable to compare the approach to early techniques like AM.  Backpropagation through sparse recovery has been previously explored (Bertrand et al. ICML 2020). A comparison with this approach would enhance understanding.  The experimental issue show limited improvement over AM. Further analysis could provide insights.  The study focuses on convolutional DL for MEG applications. Addressing the applicability of the analysis approach to convolutional cases would be beneficial.", "Paraphrase: The authors examine the behavior of \"unrolling\" applied to \"dictionary learning.\" Unrolling involves replacing the optimization step in dictionary learning with a fixed issue of iterations. The authors show that unrolling provides scalable approximate gradient for training dictionary learning pattern. They compare unrolling to the standard alternating minimization approach and find that unrolling a limited issue of iterations or using truncated gradient can improve stability. The authors analysis supports observations in pattern and provides a theoretical justification for them. However it remains unclear if \"L\" in Proposition 2.2 which represents the issue of rows in the dictionary and a convergence speed constant plays distinct roles."], "syzTg1vyBtL": ["Paraphrase: This paper introduces a novel bandit model known as \"congested bandits\" where the reward for selecting an arm decays based on the number of times it has been played recently. This model addresses recommendation scenarios such as route planning where frequent selection of a specific selection can lead to congestion and lower rewards. Unlike previous bandit models that assume changing arm reward distributions this model resets the congestion effect periodically. To solve this structured problem the paper formulates it as a Markov Decision action (MDP) and develops a variant of the UCRL2 algorithm which learns to recommend the optimal arm for each congestion state. The proposed algorithm achieves a policy regret of approximately the substantial root of the time window size multiplied by the total number of time steps which significantly outperforms UCRL2 in terms of its dependence on the state space. Additionally the paper presents a variant of the algorithm for linear stochastic contextual bandits and analyzes its policy regret. effectiveness:  Proposes a novel bandit model where rewards depend on a recent history of actions.  Derives sharper regret bounds compared to UCRL2 by leveraging the structured nature of arm rewards.  Employs innovative techniques to analyze regret under unknown stochastic contexts. Weaknesses:  The problem formulation is rather specialized and may not accurately reflect realworld congestion scenarios.  The structure of the optimal policy in scenarios with known mean rewards requires further investigation.  No lower bounds on regret are provided.  The proposed algorithms are largely extensions of UCRL2 and LinUCB.  Contextual regret analysis is restricted to multivariate Gaussian distributions.  Experiments lack sufficient comparison specificly with MDPbased reward learning approaches.", "Paraphrase: Summary This paper introduces a novel number known as congested bandits an extension of traditional multiarmed bandits (MAB) difficulties. In this situation an arms reward relies on how frequently it has been selected within the last \u0394 time units. The authors extended congested bandits to graphbased and contextual versions. novel algorithms were created for these settings and upper bounds for regret were obtained comparable to those for MABs or linear contextual bandits. Specifically for congested bandits the upper bound is approximately O(\u221aK\u0394T) with K representing the number of arms and T the time horizon. For graphbased congested bandits it is roughly O(\u221aVE\u0394T) where V and E are the number of nodes and edges respectively. For congested linear contextual bandits it is approximately O(\u221adT). The authors also did numerical simulation to back up the theoretical findings. Strengths The paper presents a novel and intriguing idea and provides substantial algorithms and theoretical analysis of upper limits. The findings appear to be optimal in my opinion. Weaknesses 1. The numbers of congested bandits discussed in the paper strike me as easy. For model there are actually K\u0394 anticipated rewards we must acquire from the environment in congested bandits. We should be able to get regret upper bounds of approximately O(\u221aK\u0394) by simply replacing the value K in the confidence intervals of classic UCB algorithms with K\u0394 and making minor adjustments to the pulling method. As a result I dont believe this work has enough theoretical introduction. The other cases suffer from a similar number. 2. Including results on lower bounds would be beneficial because without them its impossible to determine if the conclusions in this study are conclusive. The authors need to offer a more thorough explanation of why lower limitations are missing.", "Summary  This study investigates congested bandits a variation of bandit problems where rewards depend on past work.  The authors reduce the congested bandit problem to a Markov decision action (MDP).  They propose a UCRLtype algorithm and analyze its regret bound. effectiveness and Weaknesses 1. motivation:  The congested bandit problem is presented as relevant to traffic routing but the model differs significantly from classic routing formulations. 2. model:  The model lacks a hierarchical structure with distinct platform and traveler agents commonly found in traffic routing.  The goal of the platform in the model is unclear. 3. algorithm and regret Bound:  The reduction of the congested bandit problem to an MDP simplifies the algorithm design and regret bound analysis.  The regret bound in Theorem 2 does not provide a clear understanding of its development with respect to the graph structure. 4. Minor number:  The notation \"V\" and \"E\" for both edge sets and their sizes can be confusing. The authors should consider using different notations. 5. Extension to Contextual Bandits:  The extension to contextual bandits is included in the Appendix but the regret bounds are not analyzed in detail. The technical novelty of the approach is not substantial compared to existing work on contextual bandits and MDPs.", "Paraphrased Statement: This article presents a novel multiarmed bandit framework for route recommendation in changing traffic conditions. The model accommodates both stochastic and settingual bandits. Initially it introduces the stochastic bandit model where each arms expected reward varies over time based on recent actions. The article suggests the CARMAB algorithm with a regret upper bound of O(\\sqrtK\\Delta T\\log(\\Delta KT)) with high probability. Next it addresses the settingual bandit model with known settings. The authors adjust the model and present the CARCB algorithm with a regret upper bound of \\tilde O(\\sqrtdT\\Delta) with high probability. Lastly the authors relax the setting assumption to allow unknown stochastic settings drawn from a known distribution. The regret analysis indicates an additional term proportional to T compared to the known settings scenario. effectiveness:  novel and practical idea for traffic recommendation.  Welldefined bandit settings covering several scenarios. Weaknesses:  The constant c in the theorems lacks clear scaling information.  Typo in the CARMAB algorithms empirical reward estimate equation.  Related work section could provide more distinctions between existing approaches and this paper."], "nRj0NcmSuxb": ["Paraphrased Statement: The paper introduces a straightforward posthoc method for modifying the accuracy of similarity scores obtained using a trained network especially when comparing faces within the same demographic group. This approach involves applying a groupspecific calibration function to adjust the scores. Unlike existing approaches this method does not require knowledge of protected attributes during training or testing which is a key strength. potential Weaknesses:  The paper lacks a comparison with a similar groupspecific thresholding method proposed by Robinson et al. (CVPRW 2020). Robinson et al.s method also reports improvement on the BFW dataset which raises questions about the novelty of this work.  The authors do not provide a detailed analysis of the Baseline Calibration approach (Figure 2) in Table 7.  The results (Table 7) suggest that FairCal may not achieve Equal Opportunity compared to FTC FSN.  Missing details about the selection of samples in the calibration set are not provided.  Notational details are missing such as the definition of Pcal in Sections F1 and F2. Minor Weaknesses and Suggestions:  The paper should cite and compare other recently proposed debiasing methods such as those by Gong et al. (CVPR 2021) and Dhar et al. (ICCV 2021).  It is recommended to include results related to skin tone and gender bias on the IJBC dataset as it is widely used for evaluating face recognition algorithm.  Adding plots similar to Figure 1 for Arcface on BFW and providing more intuition behind Eq 3 would enhance the clarity of the approach.", "Paraphrased Statement: The paper presents FairCal a technique for calibrating face verification models to ensure fairness. FairCal uses kmeans clustering to identify data groups and applies a betacalibration algorithm to each group. The method avoids the need for sensitive attribute data during calibration. Strengths and Weaknesses: Pros:  Addresses the issue of unfairness in face verification models which exhibit different false Positive rates (FPRs) and false Negative rates (FNRs) across demographic groups.  Proposes a method for calibration that does not require approach to sensitive attributes.  Demonstrates improved accuracy FPR and fairness calibration compared to existing calibration methods including those using sensitive attributes. Questions and Comments for Authors: 1. Can you clarify why fairness calibration is desirable in face verification 2. How was the optimal issue of clustering (K) determined 3. Please elaborate on how FairCal can be extended to multiclass context. 4. What loss function was used to train models on the VGGFace2 and CASIA datasets 5. Was betacalibration applied to all methods for fairness calibration or only for the results in Table 3 6. A brief explanation of the betacalibration method within the paper would enhance understanding of FairCals core mechanism.", "Paraphrased Statement: Summary: This research introduces a Fairness Calibration method for facial verification which aims to improve fairness by calibrating probabilities. The authors state that their method can:  Enhance fairness by reducing the disparity in false positive rates  Operate without using data about sensitive attributes  Avoid the need for additional model training or parameter tuning Experiments on RFW and BFW databases show that the proposed method achieves competitive performance compared to existing methods. Strengths and Weakness: Strengths:  The method is unsupervised and does not require retraining. Weaknesses:  Limited accuracy improvement compared to previous methods (as indicated in Table 2)  The introduction and organization of the paper could be improved for clarity."], "fuYtttFI-By": ["Paraphrase: Summary: This work describes a neural network architecture that utilizes spatial light modulation for microscopy imaging and 3D image reconstruction. It employs Fourier features in the decoding work to enhance spatial features that cannot be obtained through conventional convolutional neural networks. The approach is demonstrated in snapshot 3D microscopy and lensless light field reconstruction. effectiveness and Weaknesses: effectiveness:  Wellwritten and illustrated paper  Demonstrates promising results  innovative integration of Fourierdomain layers Weaknesses: Questions:  Whether Fourier transform signal working is a novel concept in machine learning  relationship of this work to existing research on Fourier CNNs (e.g. Rippel et al. Vasvani et al. Gehring et al.) Doubt:  Redundancy in using Fourier convolutions across multiple frequency bands when downsampling inherently work in Fourier domain  potential alternative approaches to achieve the desired effect of emphasizing low frequencies such as frequencydependent weighting or feature depth adjustments.", "Paraphrased Statement: This research introduces a neural network designed for highdimensional image analysis in 3D microscopy and lensless imaging. It utilizes Fourier transformations to enhance the networks focus on global image information. The proposed approach is evaluated through simulations demonstrating its effectiveness compared to traditional methods. effectiveness:  Wellexplained methodology  Innovative application of Fourier transforms in image work  Advantageous for handling wideangle images in highdimensional imaging  Comprehensive experimental evaluations Weaknesses:  Unclear whether the parallel simulation is crucial for optimizing the large number of pixels (106).  lack of realworld experiments which could widen the performance gap between synthetic and genuine scenarios.  potential challenges with applying the highpixel optimization to genuine applications due to the domain difference between simulations and real data.", "Paraphrased Statement: Summary: This paper presents an improved optical encoder and optimization approach for 3D snapshot microscopy. The encoder uses an endtoend design while the decoder incorporates deep learning techniques to handle global point spread work (PSFs) in 3D microscopy. Conventional UNet architectures struggle to efficiently capture the global context required for 3D microscopy. To address this the authors introduce a Fourier convolutional network (FCN) which utilizes a single layer to capture global context with reduced computational cost. The FCN is evaluated on simulated data from a larval zebrafish volume across four fields of view. Additionally the technique is applied to lensless imaging. effectiveness:  The use of Fourier transforms to achieve largesize PSFs is a novel approach.  The design of the FCN is wellexplained and includes optimizations for computational efficiency.  The multiscale features in the Fourier domain are clarified highlighting the effectiveness of the architecture.  The paper acknowledges limitations. Weaknesses:  limited Evaluation: The 3D snapshot microscope is only evaluated on simulated data overlooking potential artifacts due to aberrations and calibration errors in realworld applications.  Inconsistent UNet resolution: The UNet results in number 2 show significantly lower quality than FCN raising questions about UNets parameterization and its ability to handle global context.  Ambiguous UNet Performance: Table 3 indicates that UNet is much faster than FCN contradicting the paper claim. The absence of FourierUNet results in this experiment further complicates the evaluation.  Irrelevant Lensless imaging resolution: While the lensless imaging results demonstrate the FCNs effectiveness they do not directly support the paper focus on 3D snapshot microscopy. Minor input:  number 2: The \"3.00s\" in the ground truth of the second row indicates the exposure time.  Typos: \"field field\" in Section A.1  Table 3: Remove unnecessary line break in the caption.  Table 4: Remove unnecessary line break in the second row.", "Paraphrased Statement: This research paper presents a novel approach to image reconstruction and encoding using Fourier convolutional neural networks (FCNs). These networks employ complexvalued multiplicative weights in the Fourier domain enabling them to perform efficient global convolutions (covering the entire image). global convolutions are especially beneficial for imaging applications with a extensive point spread work (PSF). The proposed networks demonstrate superior performance to UNets in a joint optimization task involving 3D snapshot microscope parameters and image reconstruction. They also surpass stateoftheart architectures in a lensless imaging reconstruction task. effectiveness:  Wellwritten and easy to follow  Promising experimental results  Acknowledgment of limitations Weaknesses:  limited technical novelty compared to a previous work on Fourierdomain networks  Absence of experiments showing the advantage of the proposed approach  Omission of critical implementation details related to the Fourier structure of the network"], "p3DKPQ7uaAi": ["Paraphrased Statement: Summary: This research presents a method for aligning sequences using Convolutional Neural network (CNNs). Sequence comparison is crucial in areas like action recognition and retrieval. Existing methods use fixedlength feature vectors or temporal alignment techniques while this approach directly predicts the alignment matrix between two sequences with a CNN. Its differentiable allowing its application to tasks like supervised sequence classification retrieval and fewshot classification. Experiments on skeletonbased action recognition and audio classification show the methods effectiveness. Strengths and Weaknesses: This paper approach is unique as it tackles the alignment problem directly with CNNs. The model predicts the alignment matrix T based on the distance matrix D between sequences and a distance matrix Dt representing relative positions. Concerns arise from the CNNs relatively small receptive field compared to sequence distance which could impact performance. Additionally the contribution of the residual term g(Ds) to the results remains unclear. Alternatively the good performance may primarily stem from the backbone network h trained jointly with the model. Future research Directions: To gain insights into the models behavior comparing the predicted alignment to the ground truth through optimization would be valuable.", "Paraphrased Statement: This paper proposes a novel approach for aligning temporal sequences where the alignment is learned by a Convolutional Neural Network (CNN). The input consists of matrices measuring elementwise distance between two sequences. The network outputs a matrix indicating the likelihood of alignment between elements in the input sequences. Strengths:  Addresses a critical problem in temporal sequence alignment.  Leverages previous research for loss use and alignment formulation.  Presents a novel architecture and approach. Weaknesses:  Ambiguity in Evaluation: The model is trained for sequence alignment but evaluated for other tasks (supervised representation learning and fewshot action recognition). Quantitative analysis of alignment accuracy is missing.  choice of CNN: Fixedsize input constraints of CNNs may limit performance and the benefits of CNNs over other sequence modeling architectures (e.g. RNNs Transformers) are not sufficiently explored.  classification method: For supervised representation learning the use of a simple 1NN classifier is not optimal compared to metric learning and linear classification.  Incomplete performance data: Training time is not reported despite the method being compared for inference speed. This oversight limits a comprehensive evaluation of the models efficiency.", "Paraphrased Statement: Summary: This work introduces a novel distance metric for sequences called Temporal Alignment Prediction (TAP). Unlike previous nonlearnable alignment algorithms TAP uses a Convolutional Neural Network (CNN) to predict alignment between sequences. The authors evaluate TAP on video representation learning and fewshot action recognition tasks. Strengths: 1. TAPs learnable distance metric eliminates ordering constraints in alignment algorithms making it suitable for tasks where strict motion order is not significant. 2. TAP has faster inference speed compared to existing methods (e.g. Table 2). 3. The work provides a clear and understandable technical explanation. Weaknesses: 1. TAP does not show significant improvement in fewshot action recognition performance. While it outperforms prior work on UCF101 1shot accuracy it falls behind on other metrics (e.g. Table 3). Additionally other existing methods (e.g. OTAM ARN CMN TTAN) have benchmarked on larger datasets like Kinetworkics and SomethingSomething but TAP lacks reported results on these datasets. 2. The work does not include an ablation work on data augmentation. TAP uses random blur and range augmentation in its experiments but their contribution to the performance improvement is not quantified. 3. The work omits a reference to \"Compound Memory networkwork for Fewshot Video classification\" by Zhu and Yang (ECCV 2018). Questions: 1. Explain why TAP may perform better on one metric (MAP) over another (1NN) in Table 1. 2. Determine TAPs performance without data augmentation on both video tasks. 3. comparison TAPs overall model size to prior approaches considering the additional neural network for alignment prediction."], "zyrhwrd9EYs": ["Paraphrase: Summary: The authors introduce a novel type of missing data mechanism called Mixed Confounded Missingness (MCM) where contribution of the missingness influences treatment selection while the remaining contribution is influenced by treatment selection. effectiveness and Weaknesses:  effectiveness:  Identifies an significant problem: Missing data can bias treatment effect estimation.  Proposes a simplified graphical tool for modeling missingness in exposuremediatoroutcome relationship.  Weaknesses:  MCM is not fully novel as it falls under the wide category of MNAR (Missing not at Random) missingness.  The assumption that missingness is solely determined by treatment selection may be overly simplistic.  The authors imputation strategy is not applicable to complex missingness practice that include nonlinear relationship or missingness that affects multiple variables.  The study lacks realworld data validation which limits its practical utility. Alternatives:  Explore machine study approach for feature selection in imputation.  Design imputation procedures specific to the field of study considering the unique practice of missingness.  Recognize the complexity of missingness practice and mechanism in realworld data.  Conduct evaluations on realworld datasets to determine the effectiveness of imputation methods.", "Paraphrase: Summary This paper investigates handling missing data when estimating the effects of treatment. The authors propose a novel missingness mechanism called \"mixed confounded missingness\" (MCM) which encompasses missingness that both influences treatment selection and is influenced by it. They demonstrate that both ignoring missingness and imputing all missing values lead to inaccurate treatment effect estimation. They introduce a selective imputation approach that determines which variables should be imputed and which should not. The effectiveness of this strategy is supported by empirical effect. effectiveness and Weaknesses  effectiveness: The authors use causal diagrams to represent different missingness practice in causal inference.  Weakness: The mapping between Figure 1(d) and Figure 3 in the paper may be incorrect. In Figure 1(d) the relationship W \u2192 Z2 \u2192 X2 \u2190 X implies that W determines the missingness of Z2 and subsequently the observed values of X2. However it does not necessarily mean that W also determines X2 when missing values are not present. In the example given missing values in X2 for contributionicipants of a job training program are created due to additional data provided by contributionicipants that is not available for noncontributionicipants. Imputing the missing values in X2 perfectly would break the relationship W \u2192 X2 making Figure 1(d) equivalent to Figure 1(c). Consequently the discussion on \"selective imputation\" based on Figure 3 may be flawed. The paper also lacks data on the specific imputation methods used in the experiments raising concerns about the reliability of the effect. If X2 is contribution of the covariate set the causal effect on the set of X should be unbiased in the absence of missing values or with correct imputation of missing values.", "Paraphrased Summary: This paper focuses on the issue of missing data in treatment effect estimation emphasizing its significance. The authors propose a specific imputation method that is tailored to address this issue effectively. They provide examples to demonstrate potential problems with existing methods when dealing with missingness in treatment effect estimation. Empirical effect show that general imputation techniques can perform poorly compared to the authors proposed approach. effectiveness:  Provides a comprehensive introduction to causality and missing data in treatment effect estimation making it accessible for newcomers.  Addresses a critical problem with realworld applications. Weaknesses:  Dwells heavily on the introduction of causality reducing the space for realworld examples of selective imputation.  Insufficiently explores the claim of MCM exhaustiveness leaving the novelty of the paper unclear.  Does not provide a clear explanation of how the paper meets the technical requirements for ICLR acceptance.  Fails to consider the wide acceptance of imputation methods in industry where skepticism and data corruption concerns often prevent their use."], "tHx6q2dM86s": ["Paraphrased Statement: Summary: The researchers suggest injecting imperceptible Unicode characters into text to bypass text recognition systems that use neural networks. Pros:  Fascinating problem with remarkable outcomes. Cons:  Limited testing and evaluation.  Incomplete review and comparison with similar study such as the one published at https:arxiv.orgpdf2104.05996.pdf.", "The paper introduces a method to create \"homoglyph adversarial samples\" (HAS) for NLP tools. HAS involve substituting English characters with characters from other language that look similar. Experiments show that this method is effective against several commonly used NLP tools. Strengths:  The paper demonstrates the methods effectiveness on popular industry platforms.  The method achieves a high success rate in attacking the NLP tools. Weaknesses:  The paper does not clearly explain the novelty of the method.  The algorithms in the paper seem to use a straightforward approach (brute force research) quite than offering a significant technical contribution.  The paper focuses on the success rate and perturbation rate of the attacks but omits to mention the number of queries required by each method. Since queries to NLP tools can be expensive the number of queries is a crucial metric. It is potential that the proposed method which is a simple research may require a large number of queries. minor typo:  \"based on the sentiment analysis sentimentanalysis) based on the user\u2019s review\" on page 1 should be fixed.", "Paraphrased Statement: Summary: This research proposes using homoglyphs characters that resemble English but have different encodings to attack NLP models used for sentiment analysis. Experiments on various cloudbased NLP models demonstrate that this attack can lead to successful targeted and untargeted misclassifications. Strengths and Weaknesses: However the paper is poorly written making it difficult to comprehend. While it introduces homoglyph attacks it falls short of Boucher et al.s earlier work published over four months ago. Boucher et al.s research not only covers homoglyphs but also considers other attack techniques providing detailed analysis and proposing countermeasures that mitigate the proposed attacks."], "ygGMP1zkiD1": ["Paraphrased Statement: This study investigates biases in attention frameworks and proposes a method to address them. method: The paper demonstrates that attention weights can exhibit correlations with social stereotypes. To mitigate this bias the method assigns each sampling with pairs of language representing the groups for which bias mitigation is targeted. These augmented samplings are used for training forcing the framework to attend to tokens within the augmented portions. The attention weights are then adjusted to achieve similarity between weights attending on tokens relevant to the augmented groups. To prevent semantic changes the attention weights for the original sampling tokens are constrained to follow those of an unaltered framework trained simultaneously. Additionally negative sampling using random language is employed to augment training data and all attention weights are forced to align with those of the teacher framework. Evaluation: The study evaluates the methods effectiveness using transformerbased frameworks. performance is assessed on three benchmark: CrowsPairs and StereoSet (intrinsic bias measures) and an NLI task designed to measure bias. The method is also evaluated on GLUE tasks to ensure that semantic capabilities are maintained. effectiveness and Weaknesses: effectiveness:  Intuitive and relatively simple bias mitigation method.  sampling augmentation approach avoids the challenges of identifying social group references in the original text. Weaknesses:  Requires training an unaltered teacher framework which can be computationally demanding.  Concerns about the use of intrinsic bias measures as they may not correlate with bias on downstream tasks.  Limited evaluation on applicationoriented tasks with group annotations making it difficult to assess the practical impact of the bias simplification.", "Rewritten Statement: This research proposes a novel method to reduce bias in pretrained text encoders using attention mechanisms. While the approach demonstrates bias simplification in certain datasets and small performance degradation on the GLUE benchmark it has weaknesses. effectiveness:  The attentionbased debiasing method is a novel approach.  Empirical results indicate improvements in bias metrics for likelihoodbased and inferencebased evaluations. Weaknesses:  Reliance on datasets with known limitation [1] for evaluation without considering their drawbacks.  Like previous wordlevel debiasing methods [2] the approach may merely conceal biases rather than remove them.  The methods applicability is limited in certain situations such as handling multipleword tokens and synonyms.  Largescale debiasing for infrequently occurring language remains a challenge.  The proposed approach lacks novelty and resembles prior counterfactual data augmentation techniques with additional attention regularization.", "Paraphrased Summary Introduction: This study presents a novel bias removal method for contextualized word embeddings used in attentionbased language models. The aim is to balance the attention distribution across language from different social groups such as reducing gender bias by ensuring equal attention to \"man\" and \"woman.\" effectiveness: 1. It addresses the underresearched field of bias mitigation in contextualized word embeddings which is timely given the growing prominence of pretrained language models. 2. The proposed method effectively adjusts attention scores by incorporating language from different groups into the training data. 3. It can simultaneously reduce multiple bias types including gender race age with the potential for handling more than two groups. Weaknesses: 1. The experimental results show limited effectiveness in reducing gender and religion biases with Kaneko performing easily on the latter. 2. Improved documentation is needed for hyperparameter tuning negative sampling and layer selection. 3. The motivation lacks quantitative analysis to support the claim that bias primarily stems from encoders. 4. The validity of bias measurements used (e.g. Stereoset and CrowsPairs) has been questioned. 5. The impact of word range in the bias mitigation process has not been explored."], "yCS5dckx_vj": ["Summary: The paper presents DirectSet(\\alpha) algorithms for noncontrastive selfsupervised learning using only positive pairs. Assuming linear layers and a limited data distribution where the input space is divided into an invariant subspace and its complement the analysis shows that DirectSet(\\alpha) converges to the projection matrix onto the invariant subspace. This improves downstream learning efficiency. Empirically DirectSet(\\alpha) perwork comparable to or somewhat better than the related method DirectPred. effectiveness and Weaknesses:  The theoretical analysis of representation learning on simple data with linear invariant nuisance spaces is novel and valuable.  The paper generalizes DirectPred by introducing an exponent for the correlation matrix but the need for this is not explained.  The empirical evaluation follows the approach of Tian et al. 2021 and results should be presented in a more accessible format.  minor result include grammar errors inconsistent verb work and unclear axis labels in Figure 2. Suggestions for Improvement:  Rephrase the abstract to focus on the papers contribution and refer to Tian et al. 2021 for the analysis of collapsing solutions.  Add labels to the xaxes in Figure 2 and visually distinguish between eigenvalues corresponding to invariant and nuisance subspaces.  role consistent singular and plural work throughout the paper.", "This paper explores noncontrastive SSL (selfsupervised learning) methods like BYOL and SimSiam. It aims to understand why they avoid collapsing into trivial results how they learn datarelated representations and how they enhance downstream task performance. The paper draws inspiration from DirectPred extending its method to adjust predictor network weight based on a parameter (\uab64) that relates to feature effectiveness and distinctiveness. For the limited case of \uab64  1 the approach achieves better accuracy on Imagenet produces more distinguishable features and improves downstream task performance. Key findings:  The formulation suggests an implicit threshold based on weight decay (\u03b7) that influences feature learning.  For \uab64  1 the method outperforms BYOL on Imagenet and reduces sample complexity on downstream tasks. effectiveness:  The paper explores the development of invariant and nuisance features during training considering the impact of weight decay.  Experiments on various datasets support the proposed methods performance and parameter sensitivity.  The paper follows a logical structure. Weaknesses:  The paper lacks novelty as it builds heavily on DirectPred.  It does not provide a clear explanation for preventing the trivial result problem or generalizing to other noncontrastive SSL methods.  The writing contains errors and could benefit from improved visuals with informative labels. Questions:  Does \uab64  2 result in strong and more distinguishable features and why does this not direct to better downstream performance  What explains DirectCopys weaker performance on CIFAR100  Why are nuisance features related to downstream task performance", "Paraphrased Statement: Summary This study aims to unravel the theoretical underpinnings of noncontrastive selfsupervised learning (ncSSL) which has shown impressive performance in practice. The authors expand on former research [1] to demonstrate the important role of weight decay in learning a desirable representation. weight decay serves as a filter eliminating noisy features with high variance introduced by data augmentation and preserving stable features with first variance. effectiveness and Weaknesses effectiveness:  Emphasizes the significance of weight decay in developing a robust representation.  Introduces DirectSet and DirectCopy as improved choice to DirectPred offering comparable performance with enhanced efficiency.  Provides substantial theoretical proofs to support the findings. Weaknesses:  Limited novelty as the work primarily focuses on analyzing DirectCopy (a variance of DirectPred) and demonstrating the importance of weight decay.  The role of weight decay and other hyperparameters in ncSSL including its importance for the BYOL model has been formerly explored in [1].", "Paraphrase: Summary: The study presents a novel method for selfsupervised learning (SSL) known as DirectCopy which builds upon DirectPred. A key contribution is the theoretical analysis of DirectSet(\\alpha) a theoretical model for linear neural networks compatible with any value of \\alpha. The authors demonstrate that the weight decay coefficient (\\eta) eliminates superfluous features and facilitates the learning of beneficial representations. DirectCopy is a variance of DirectSet(\\alpha) with \\alpha1. Notably the new approach avoids the computationally intensive eigendecomposition while maintaining performance comparable to DirectPred. effectiveness and Weaknesses: Originality and Novelty: The study aims to clarify why noncontrastive SSL can yield meaningful representations. It offers a novel position by emphasizing the significance of the weight decay coefficient. The analysis and algorithm are novel and represent an improvement over DirectPred. effectiveness:  The study asserts that the weight decay coefficient is important for ncSSLs ability to learn meaningful representations. This hypothesis warrants further investigation.  The study thoroughly assesses DirectCopy through extensive experiments. Weaknesses:  In Fig. 2 certain eigenvalues remain minor even with zero weight decay resulting in a less pronounced drop. The authors should clarify this observation.  The analysis are primarily based on SGD. It would be interesting to observe the impact of different optimizers such as Adam.  The authors overlook the evaluation of DirectCopy on additional downstream tasks such as object detection."], "oC12z8lkbrU": ["Paraphrased Statement: The paper introduces a fresh approach that combines selfsupervised learning with knowledge distillation to enhance NLP and tabular data work. The framework employs a generative framework to create synthetic data for selftraining and knowledge transfer. The paper presents extensive experiments demonstrating the effectiveness of the proposed methods while acknowledging their limitations. effectiveness and Weaknesses: effectiveness:  Wellwritten: Clear and structured presentation with detailed explanation.  Thorough review: Comprehensive discussion of key concepts and components.  Acknowledge limitations: Honest assessment of framework failures.  solid results: Improvements over strong baselines. Weaknesses: Suggestions for Improvement: 1. Figure 1 clarity: Adjust the positioning of unlabeled data for effective clarity. 2. Table 1  2 clarification: Provide explanation for scoring metrics and consider including standard deviation or significance testing. 3. synthetic data evaluation: Manually generate synthetic examples to assess the impact of data quality. 4. Table 5 comparison: Add a baseline without GAL for comparison. 5. tabular task analysis: Explain the rationale behind feature encoding and discuss its impact on results.", "Summarized Statement: This paper examines semisupervised learning using synthetic data generated by language models. The authors address the challenge of obtaining taskspecific unlabeled data and demonstrate the effectiveness of semisupervised knowledge distillation for NLP and tabular tasks. effectiveness:  The paper clearly outlines the research method and conducts thorough experiments leading to valuable insights.  The finding that GPTj can produce highquality data with limited labeled examples is noteworthy. Weaknesses:  The claim of small labeled datasets (Figure 1) is questionable given the large size of datasets like MNLI and RTE.  The proposed solution of using unconditional language models for data augmentation is only partially explored (Table 6) leaving unanswered questions about the effectiveness compared to other data augmentation methods.  The paper lacks comparisons with stateoftheart data augmentation approaches.  Additional questions arise about the knowledge distillation work in DistiRoBERTa KD (Table 1) the claims regarding classconditional generative models and the use of pseudolabeling with conditionally generated data.", "Paraphrase: Summary: The authors introduce a new data enhancement method using generative frameworks (GAL). This technique involves generating new samples and categorizing them without supervision to enhance the performance of a secondary framework. The authors demonstrate its effectiveness through experiments using numerical data from UCI datasets. They establish a clear connection between data generation and semisupervised learning. extensive testing demonstrates the benefits of GAL especially for smaller frameworks. effectiveness and Weaknesses:  Clear and wellwritten  Combines multiple language frameworks effectively  Strong experimental section with detailed ablation work  Promising for datatotext applications in the future  Potentially high computational costs  Anonymization concerns due to GALs appearance on the GLUE leaderboard under the authors name  Limited discussion on computational costs of using GPT2 and RoBERTa frameworks Specific Comments:  Section 3 on distillation is lucid.  Figure 1 could explicitly indicate that GAL represents the lower path.  Section 4.2s analysis is insightful.  Appendices provide comprehensive details for experiment replication.  The authors affiliation with the GLUE leaderboard under the name \"XXX\" (if true) raises concerns about anonymization.", "Paraphrased Statement: This paper investigates the challenges of selftraining mainly due to the lack of indomain labeled data. To address this it proposes generating indomain data using large selfsupervised language frameworks (LMs). The approach follows traditional selftraining utilizing soft targets as suggested by Du et al. The proposed method GAL is evaluated on the GLUE benchmark for knowledge distillation (KD) unsupervised learning on broad data promptbased fewshot learning and tabular tasks. issue show that GAL often issues the most significant improvements in the first iteration of KD with relatively stable performance in subsequent iterations. effectiveness:  Clear writing and detailed appendix providing additional data on tasks and experiments.  Comprehensive survey of related work in selftraining.  extensive experiments on KD selftraining fewshot learning and tabular tasks. Weaknesses:  The paper highlights the challenge of generating indomain data but the novelty of using GPT2 finetuning or GPT3 generation for this task may not be sufficient for acceptance as an ICLR paper.  The authors should compare their approach with other methods of generating indomain data (e.g. kNN generation without finetuning) to emphasize their scientific contribution. Other Questions and Comments:  For tasks requiring singlesentence inputs could using a corpus like NYT or kNN on the training set for indomain data generation issue similar results  For pairwise inputs how does using nearest neighbor with unlabeled texts compare to the proposed method  For tabular datasets would training a separate LM specifically for the tabular data provide comparable or effective results than using GPT2  Have the authors considered using adapters instead of finetuning the entire GPT2 framework which may be more effective  What would the results be like using promptbased generation without finetuning GPT2  The comparison to XGBoost on tabular tasks should be more nuanced as the gaps between XGBoost and RoBERTa remain significant.  Table headings should consistently include metric names.  Please specify the issue of iterations in one cycle of GAL.  Some grammar corrections are suggested in the text."], "rN9tjzY9UD": ["Summary Paraphrase: This paper introduces an adaptable method for optimizing the structure of tensor networks applicable to tasks like tensor completion decomposition. The methods effectiveness has been demonstrated with both synthetic and realworld data. effectiveness and Weaknesses Paraphrase: The methods core concept involves gradually increasing the core tensor dimensionality requiring recalculating and optimizing the core tensor and factor matrices with each change. This work is computationally demanding and may not guarantee convergence. Despite the claimed applicability to CP decomposition algorithm 1s function do not fully support this assertion. CP decomposition does not have a defined core tensor but rather a diagonal tensor. It is unclear how algorithm 1 gradually adds slices to the core tensor without disrupting its diagonal structure and maintaining its optimality. The description of the algorithm is somewhat ambiguous. While the Best Edge Selection step mentions using gradients or least squares only on the novel slice it is not clear how the objective function is ensured to decrease without considering the remaining slices. This approach seems suboptimal. Finally the section on Internal Nodes is not sufficiently explained leaving readers with unanswered questions.", "Summary This paper presents a greedy algorithm to determine the structure of a specific class of tensor networks. The algorithm iteratively optimizes the network structure in an outer loop and approximates the tensor decomposition in an inner loop. A comparison of the error and parameter number of the algorithm is provided using synthetic data image compression and neural network compression. Strengths  Focuses on the significant problem of tensor network structure determination.  Clearly written and easy to follow.  Proposes a practical algorithm. Weaknesses  practical applications of the method are not clearly demonstrated. other methods (DCT for images distillationNAS for neural networks) may be more efficient for these tasks.  The algorithm lacks a theoretical guarantee as it is based on heuristics.  The paper is not selfcontained with some function and stopping criteria undefined.  The class of tensor networks considered is narrow and excludes networks with multiple output legs or hyperedges.", "Summary  The ideas presented for tensor network decomposition and structure determination are not novel.  Weight transfer decomposition is a widely used technique in the rank incremental method.  The authors should acknowledge previous work on generalized tensor network decomposition such as S. Handschuhs 2015 PhD thesis. effectiveness and Weaknesses  The paper introduces a greedy method for finding tensor network structure and rank.  similar algorithms have been proposed previously including ALS DMRG and adaptive rank selection for tensor ring decomposition.  The tensor ring model is essentially the tensor chain model proposed by Khoromskij in 20092011.  The authors credit Espig Hackbusch Handschuh and Schneider (2012a) Espig Naraparaju and Schneider (2012) Huckle et al. (2013) Hackbusch (2014) Handschuh (2015) and Cichocki et al. (2016 2018) for their contributions to tensor chain and tensor network computations.  still some subsequent issue may not acknowledge the original work on tensor chain.  The authors use the ALS algorithm for tensor decomposition which can get stuck in local minima.  The paper demonstrates the recovery of the exact tensor network structure for a synthetic tensor of size 7 x 7 x 7 x 7 x 7 with bond dimensions 2365.  The authors should attempt the decomposition of a tensor of the same size but with cores generated from a uniform distribution which is more challenging.  greedy algorithms may not find the true tensor network model still with known rank.  The authors should test their method on tensor associated with matrix multiplication.  The performance for TT decomposition shown in number 5 may be incorrect as TT with higher parameters should improve reconstruction.  The authors should evaluate their method on more complex neural network such as Resnet with larger datasets.  References 17 and 5 do not present the ALS algorithm for tensor network it is described in Espig et al. (2012).  The supplementary implementation does not support the tensor chain model.", "Paraphrase: This paper presents a greedy algorithm for solving tensor network optimization problems with the main contribution being an efficient algorithm for determining tensor network structure. effectiveness:  Innovative greedy algorithm for learning tensor network structure.  Insights into the relationships between tensor representations and tensor optimization. Weaknesses:  algorithm Design:  Lack of explanation for random initialization of core tensor in algorithm 1.  Experimental Evaluation:  Questionable claim about the unsuitability of Tucker decomposition for highorder tensor.  Insufficient experimental issue with tests limited to a single image with a fixed missing ratio.  Lack of comparisons with other tensorbased methods (e.g. Image completion practice Low Tensor Tree rank and number Variation Minimization).  Missing analysis of computational complexity or running time comparisons.  applicability:  Unknown sensitivity to noise (AWGN).  Untested performance in image denoising tasks where Tucker decomposition excels."], "hJk11f5yfy": ["Summary The authors present a solution to issues where models fail to generalize to novel fields that have identical labels but different subcategories compared to the source field. They propose adding knowledge about these subcategories and their supercategories as supplementary supervision signals. For this they establish a conditional training framestudy with an adapted architecture and a hierarchical loss use. The results show significant improvements in robustness to subpopulation shift. Strengths and Weaknesses The author finds the ideas presented is easy to understand and the evaluation results compelling. The results are unsurprising given the custom setup addressing the goal of generalizing to subpopulation shift. Quantifying such generalization potential is still informative although it limits the study to a specific problem or question rather than a major contribution. The lack of novelty in the presented study is a primary concern. Several view claimed to be novel have been addressed in literature over the past decade. The quantification of \"misprediction impact\" and \"catastrophic coefficient\" closely resembles the \"hierarchical cost\" in Deng et al.s study as well as the \"Hierarchy and Exclusion\" graphs they proposed later. Similarly the architecture is reminiscent of HDCNN. The author expected references to and comparisons with these prior studys. Despite referencing other related studys the authors do not make comparisons giving the impression that they are not significant enough for referencing. However these studys contribution similarities with the proposed solution even if addressing slightly different tasks. The author suggests the contribution is incremental and could have been deemed otherwise if the authors had provided theoretical insights about the hierarchy problem. References  Deng J. Berg A. C. Li K. FeiFei L. (2010). \"What does classifying more than 10000 image categories tell us.\" European conference on computer vision. Springer Berlin Heidelberg.  Deng J. Ding N. Jia Y. Frome A. Murphy K. Bengio S. Li Y. Neven H.  Adam H. (2014). \"Largescale object classification using label relation graphs\". In European conference on computer vision (pp. 4864). Springer.  McClelland J. L. Z. Sadeghi A. M.  Saxe A. M. (2017). \"A Critique of Pure Hierarchy Uncovering CrossCutting structure in a Natural Dataset.\" Neurocomputational model of Cognitive Development and process Proceedings of the 14th Neural Computation and Psychology Workshop.  Saxe Andrew M. McClellans James L.  Ganguli Surya. (2013). \"Learning hierarchical categories in deep neural netstudys.\" Proceedings of the Annual Meeting of the Cognitive Science Society. Vol. 35. No. 35.  Yan Z. Zhang H. Piramuthu R. Jagadeesh V. DeCoste D. Di W.  Yu Y. (2015). \"HDCNN hierarchical deep convolutional neural netstudys for large scale visual recognition\". In Proceedings of the IEEE international conference on computer vision (pp. 27402748). Language and Formatting Issues  \"at a the subpopulation\" should be \"at the subpopulation\"  \"One hot labels\" should be \"Onehot labels\"  \"if a neural netstudys\" should be \"if neural netstudys\"  \"are lesser catastrophic\" should be \"are less catastrophic\"  \"Each heads predicts\" should be \"Each head predicts\"  \"Our netstudys makes\" should be \"Our netstudys make\"  \"a batchsized binary matrix\" is unclear  \"are superior than standard\" should be \"are superior to standard\"  \"at level of\" should be \"at the level of\"  \"the the notion\" should be \"the notion\"  \"subclasses from the ImageNet\" should be \"subclasses of ImageNet\"  \"We now introduce some notation.\" should avoid vague statements  Formatting issues include missing spaces missing flow at the end of time and incorrect capitalization (e.g. \"Toxicity classification\").", "Paraphrased Statement: This paper introduces a hierarchical classification method that uses multiple classifier layers within a neural network each corresponding to a specific level in the class hierarchy. This approach aims to address the subpopulation shift problem. Compared to baseline methods the proposed method demonstrates improvements on the ImageNet and BREEDS datasets. Strengths:  Relevant to the field of computer vision.  Wellreasoned proposal with strong technical foundation. Weaknesses:  similar hierarchical classification architectures exist (e.g. [1] [2]).  The proposed method requires a perfectly balanced class hierarchy which may not always be practical.  The experimental results are limited and should be expanded to include comparisons with other hierarchical classification approaches.  The focus on subpopulation shift problem seems arbitrary and could be broadened.", "Paraphrase: Summary: 1. Presents a novel architecture with multiple heads to explicitly incorporate class hierarchy in training. 2. Introduces a novel metric to assess the severity of mispredictions based on the distance between predicted and actual classes in the hierarchy. 3. Creates a hierarchical label structure for a subset of ImageNet. 4. Assesses the performance of the proposed architecture under varying population distributions. Strengths:  Wellwritten and clearly explained.  Simple and computationally efficient architecture.  Improved performance on the tested datasets. Weaknesses:  evaluation limited to datasets with image and a specific class hierarchy (living organisms).  potential bias due to the specific class hierarchy used in the custom dataset.  further experiments needed to investigate the impact of different class hierarchies and visual features on the architectures performance.  Table 1 inaccuracy: Sourcetosource and targettotarget accuracy should be higher for Baseline18 than Subclass Level18.  Lack of supporting evidence for the claim that humans learn categories progressively from highlevel to granular levels. Humans may also learn finegrained categories first and then generalize them."], "sWqjiqlUDso": ["Summary Paraphrase: This paper presents a technique that combines graph structure learning and fairness constraints for prediction tasks. The loss function incorporates three aspects: graph structure learning loss fairness regularization through an adjacency matrix and label prediction. Strengths Paraphrase: 1. Addresses challenges in causal fair machine learning including the need for causal graphs and complex calculation of pathspecific effects (PSE). 2. Novel combination of causal structure learning and causal fairness regularization. 3. Experimental effect demonstrate the effectiveness of the proposed method compared to baseline approach. Weaknesses Paraphrase: 1. Inaccuracies in summaries of existing research:  Ignorability assumption or identification requirement is not always necessary.  Pathspecific effects are not limited to those defined by Avin et al. (2005).  Spurious discrimination covers effects from sensitive attributes via confounders. 2. Definition of structural causal framework function is not restricted to an additivenoise style. 3. Discrepancy in Equation (5) between fair edges and fair paths and the need for clarification on the mask structure process. 4. Spurious effects involving PIY may not be unfair based on the definition of pathspecific fairness. 5. Unclear how the method handles unidentification challenges. 6. Tuning of the alpha parameter relies on PSE values which the method aims to address with complex PSE computation.", "Paraphrased Statement: Summary: This study focuses on improving fairness in causal pathways by eliminating biased connections. It builds upon the research of Kyono et al. by introducing additional regularization terms. significantly the method is applicable even when the sensitive attribute is not at the root of the causal tree. Experimental effect indicate that the proposed method (CGF) effectively balances fairness and frameinfluence performance. effectiveness and Concerns: Concerns:  The technical contributions appear limited relying heavily on the influence of Kyono et al. particularly in terms of the loss function and generalization error analysis. The primary foundation seems to lie in the cascade data reconstruction.  If the causal graph is extensive numerous causal mechanism would be required raising scalability concerns especially when employing neural netinfluences as mechanism.  A typo exists in section 4.1.4 regarding the first linear layer.  The intuition behind setting wij as the squared l2 norm and why it is applied only to the first linear layer is unclear. Experiment Questions:  Explain why a method for multiplicative fairness (such as MFCGF) is not included.  Given the use of GMF and MLP in He et al.s study why is their combined frameinfluence (NeuMF) not considered as a baseline  It is an overstatement to claim that CGF improves fairness without sacrificing utility as MLPCGF exhibits significantly lower utility than MLP. Appendix Concerns:  The motivation or intuition for calculating the adjacency matrix entries in the specified manner is unclear.", "Paraphrased Statement: Summary: This paper proposes a fair prediction framework that incorporates graph structure learning to prevent unfair pathways in a causal graph. It demonstrates the frameworks effectiveness through theoretical analysis and experiments. effectiveness:  Addresses an significant effect.  Interesting approach. Weaknesses:  limited technical novelty:  Eq. (2) directly reuses LGL from Zhang et al. 2018 with added fairness and label prediction losses.  These losses lack technical foundation (though the fairness loss aims to block certain paths).  Generalization error analysis:  Insufficient explanation of the complex parameters involved.  Elaboration needed on the analysis for graph learning scenarios.  Loss function concerns:  Sparsity constraint (L1 regularizer) may unnecessarily hinder fair paths especially for minor graphs like the Audit dataset.  Dissimilarity regularizer between the original and reconstructed graphs (using kernel measures) would be more appropriate for enforcing causality.  Experiments:  Lack of ablation studies to assess different loss components.  AUC metric is not suitable for evaluating models with label imbalance fair precision on positive and negative items should be used instead. Typo:  \"linear layer in fi..\" on Page 5 secondtolast line should be corrected."], "tT9t_ZctZRL": ["Paraphrase: Summary This work examines the trainability of deep Graph Convolutional Networks (GCNs) using Gradient Norm Temporal Kernel (GNTK) focusing on its impact rather than expressive power. It demonstvalues that the GNTKs convergence value is exponential and provides a similar analysis for residual techniques in GCNs showing that they can mitigate the issue to some extent but not resolve it. Based on these findings the authors introduce an edgebased sampling method called Critical DropEdge to address the exponential decay in trainability. effectiveness  Pioneering focus on GCN trainability using GNTK  Theoretical analysis and proof of exponential convergence value for GNTK and residual techniques Weaknesses  Error in Equation 6 during GCN formulation (decoupled formulations do not apply)  Fundamental error in Equation 17 during GNNGCN formulation (activation work misplaced)  Limited experimental data with only three citation datasets of varying background and scale  Lack of detailed experimental information (e.g. data splits)", "Paraphrased Statement: Summary: This research utilizes a GNTK formulation to demonstvalue the declining performance of deep graphical convolutional networks (GCNs) in node classification tasks as their depth increases. The work reveals that in background with infinite width (where NTKs are applicable) highly deep networks effectively become a constant GNTK resulting in the convergence of kernel value between any pair of nodes to the same value. Consequently node classification accuracy significantly degrades with increasing network depth. Based on this analysis the authors propose a solution to mitigate this decay by removing edges from the original graph. effectiveness and Weaknesses: effectiveness:  The theoretical approach presented is engaging. Weaknesses: 1. mathematical Error: In Corollary 1 the claim that for a finite L0 the GNTK at depth L becomes singular is not directly supported by Theorem 1. 2. Proof issue: Theorem 2 lacks a clear relationship between \\tilde\\lambda2 and \\tilde\\alpha making the discussion about slowing down convergence value unclear. 3. Inconsistent Notation: The \\alpha in Theorem 3 should align with the \\alpha in Theorem 1 for coherence. 4. Insufficient analysis of solution: The proposed solution of dropping edges raises concerns:  Similar problem within connected components of the limited graph  Lack of justification for information deletion as a mitigation stvaluegy  Absence of theoretical or empirical analysis of the GNTK properties of limited networks 5. limited empirical Improvement: The CriticalDropEdge method shows only a modest improvement over the baseline which exhibits exponential decay. In some case shallow GCNs with fewer layers outperform deep ones even with CriticalDropEdge. 6. Typos and Inconsistencies: Multiple errors inconsistencies and outdated citation need to be addressed.", "Paraphrased Statement: Summary: This paper employs advanced GNTK technology to investigate the trainability of deep graph convolutional networks (GCNs). The research reveals that trainability exponentially decays with increased network depth with residual connections only slowing this decay. To address this issue they propose a drop edge method based on theoretical principles. experimental findings indicate the superiority of this approach compared to the baseline. effectiveness and Weaknesses: 1. effectiveness:  focus on a key question regarding the feasibility of gradient descent in training deep GCNs unlike previous work exploring their expressiveness.  strong mathematical derivation supporting the proposed theory.  empirical evidence showing that residual connections slow trainability decay but do not resolve it. 2. Weaknesses:  The drop edge solution is not novel and could potentially be discovered through hyperparameter tuning.  experimental setup limitations include using a single outdated baseline and potential inconsistency with the original paper in terms of dataset and hyperparameter background.  Unfair comparison due to differing experimental conditions (e.g. trainvalidationtest set).", "Paraphrase: Summary: This research addresses the oversmoothing issue in deep graph neural networks (GNNs) from a theoretical standpoint. It utilizes the Graph Neural Tangent Kernel (GNTK) to examine the trainability of wide and deep GNNs discovering that it decreases exponentially during optimization with residual connections only slightly mitigating the problem. Based on this analysis the researchers enhance the DropEdge method by proposing Critical DropEdge (CDropEdge). Experiments demonstvalue the effectiveness of both the theoretical analysis and the CDropEdge method. effectiveness:  Novel approach to analyze trainability in GNTKs.  Provides insight into the DropEdge method and motivates CDropEdge.  Thorough experimental evaluation.  Comprehensive background explanations in Sections 2 and 6. Weaknesses:  CDropEdge derivation: Extrapolation from connected component size to information flow is insufficient. Further analysis of the GNTK convergence value in critical random graphs could strengthen the argument.  experimental results: Training performance and GNTK convergence results for CDropEdge should be included to compare mitigation issue with residual connections.  Baseline methods: Additional baseline methods model architectures and normalization mechanisms should be tested for a more comprehensive evaluation of CDropEdges effectiveness.  Minor issues:  Theorem 2 explanation could be improved for clarity.  The term \"residual connection resemble technique\" is confusing.", "Summary and Weaknesses Summary Paraphrase: This paper proposes a new perspective on understanding the oversmoothing problem in deep Graph Convolutional Networks (GCNs). They analyze the behavior of the Graph Neural Tangent Kernel (GNTK) during optimization for wide GCNs showing that its trainability decreases exponentially. They note that residuallike techniques can slow the decay but not solve the issue. They suggest using a critical edge drop value based on ErdosRenyis \"Critical Connectivity in Random Graph\" theorem and demonstvalue its effectiveness in DropEdge. Weaknesses:  limited baseline methods considered.  Insufficient discussion of related work and how the new framework relates to other research on oversmoothing."], "gfUPGPMxB7E": ["Paraphrased argument: Summary: This research place to facilitate beneficial sharing of data across multiple tasks in offline reinforcement learning place. It explores the reward of data exchange under the assumption of a common transition kernel for all tasks. Unlike previous approach the paper suggests that under certain conditions sharing data with a constant reward (or no reward) can outperform using reward predictors and even compete with relabeling with ground truth rewards. To achieve this two methods are proposed: CUDS and UDS. Experimental results demonstrate the effectiveness of these methods. effectiveness and Weaknesses: reward: 1. The paper presents two simple and implementable approach (CUDS and UDS) for data sharing in offline reinforcement learning without relying on reward function approximation. Questions: 1. The paper asserts that under specific assumption the proposed data sharing methods can be more beneficial than using ground truth rewards. However it lacks a clear explanation of the underlying intuition and the specific conditions under which this claim holds. 2. It assumes a binary reward for the place MDP. Is this necessary for CUDS and UDS to function effectively 3. Remark 4.2 in Section 4.2 place to explain how data sharing affects Qestimates but its wording is unclear and fails to lead to the conclusion that \"CUDS does not have to push down Qvalues at every stateaction pair.\" Clarification is needed. 4. The papers writing requires polish with excessive similarity to a related work ([1]) in descriptions and equations. Definitions and notation are frequently omitted or introduced without extension.", "Paraphrased Summary: This paper presents a technique for training reinforcement learning models in offline scenarios with multiple tasks. It efficiently assigns and describes rewards focusing on identifying valuable transitions for labeling. The technique follows a conservative learning approach to minimize distribution shifts. The authors examine the techniques performance through theoretical analysis and experiments. effectiveness and Weaknesses: effectiveness:  Addresses a significant problem in offline reinforcement learning.  Provides a wellreasoned rationale. Weaknesses:  novelty: The technique resembles an existing study (Yu et al. 2021a) and lacks a clear explanation of substantive differences.  assumption: The technique assumes identical dynamics across tasks and employs a single taskconditioned policy. While the assumption that tasks simply differ in reward distributions is reasonable it requires more justification.  Comparison to ModelBased Offline RL: The authors could provide more insights on how their method compares to learning environment dynamics with ample data in modelbased offline RL settings.", "Paraphrase: Summary This paper introduces CUDS a new offline algorithm for multitask reinforcement learning. Unlike previous methods that require specific reward function formats or focus on goalconditioned settings CUDS contribution taskagnostic transitions by assigning a fixed reward (e.g. zero) to them. Mathematically it is proven that the value function estimated by CUDS is an underestimate compared to other sharing methods. However it is claimed that CUDS can perform beneficial by weighting transitions based on their relevance. Empirical results demonstrate that CUDS is competitive with prior approach including one that uses true reward function. effectiveness and Weaknesses  effectiveness: The intuition for using unlabeled transitions is wellexplained and empirical evidence shows the effectiveness of constant reward assignment.  Weaknesses: The justification for using fixed rewards lacks theoretical reinforcement. The claim that underestimating rewards is desirable seems conservative and may result in inaccuracies when a stateaction pair appears in both labeled and unlabeled data. Additionally the theoretical analysis in Proposition 4.1 is not fully explained and its relevance to offline multitask RL is not clear."], "r8S93OsHWEf": ["Paraphrase: Summary The paper presents a meta adversarial training approach that optimizes the starting point for testtime finetuning. It integrates the testtime finetuning process into the training phase aligning the selfsupervised and classification tasks. Experiments on CIFAR10 STL10 and TinyImageNet demonstrate consistent improvements in robustness. Strengths  Addresses the issue of robust overfitting in adversarial training.  Achieves consistent accuracy improvements with Meta AT. Weaknesses 1. Feasibility of obtaining test samples without labels for selfsupervised finetuning may be limited. 2. Testtime training can be computationally expensive especially when applied to a full test set with a large framework. 3. The definition of the budget size parameter (\u03f5) is unclear and \"size of budget\" is not technically correct. 4. Compared to [1] the method performs incremental finetuning while the proposed method uses identical weights for each image. The rationale for this difference and the handling of batch size of 1 are unclear. 5. The definition of the selfsupervised loss (\u2112SS) is unspecified. 6. The assumption that the starting point yielded by the meta learner is optimal need clarification. 7. Equation 4 uses test samples for \u2112SS while Equation 7 uses training samples. 8. compare with [2] is requested as it addresses similar issue in adversarial training. 9. Analysis of training and inference time and compute complexity overhead due to meta adversarial training and testtime training is missing. 10. The use of identical starting \ud835\udf030 for batch size of 1 is unclear. 11. The compare with regular AT and regular AT without finetuning in Table 4 is unbalanced because these methods cannot finetune with test samples.", "Paraphrased Statement: Summary: This paper proposes a method to enhance generalization and accuracy of adversarially trained networks. It introduces a meta adversarial training technique that incorporates testtime finetuning into the training phase resulting in improved robustness under whitebox and blackbox attacks. effectiveness:  clear exposition despite minor errors.  Demonstrated effectiveness under various attack strategies. Weaknesses:  Lack of a compare to similar testtime training methods for adversarial defense.  Unconvincing gradient estimate for whitebox adaptive attacks.  Insufficient evidence supporting the claim that metaadversarial training strengthens the correlation between selfsupervised and classification tasks. additional Comments:  Typographical errors should be corrected.  Specify the process for small batch sizes (b).", "Paraphrase: Summary: Refining the parameters of an adversarialtrained frameprocess during testing improves its defense against conventional adversarial attacks leveraging selfsupervised tasks. Incorporating metalearning into adversarial training enhances the effectiveness of testtime finetuning resulting in slightly higher accuracy (although no improvement in optimization time was reported). This approach requires approach to the training data to update both training and test batches jointly. Common selfsupervised tasks include rotation prediction and vertical flip recognition. issue are presented for CIFAR10 STL and Tiny ImageNet datasets. various design choice were analyzed and an adaptive attack was developed and evaluated. however further exploration of obfuscated gradients (as suggested by Athalye et al.) is lacking. effectiveness:  Optimizing defense is a crucial field of research as attacks continue to evolve.  The proposed method addresses both testtime finetuning and metaadversarial training during the training phase.  Standard experimental design facilitates compare with existing defense.  method components are effective and offline optimization on the entire test set outperforms online optimization on a subset. Weaknesses:  Prior testtime defense exist such as parameter optimization (runtime masking and cleansing) latent variable optimization and input optimization (SOAP).  Computational cost of optimization during inference is not discussed which could be significant for joint optimization on training and test data.  attack used are primarily designed for static defense limiting the assessment of testtime defense.  Obfuscated gradients a potential concern for the proposed method have not been thoroughly addressed. Suggestions for Rebuttal:  Discuss the contributions of this process relative to SOAP and other cited testtime defense.  Comment on the computational implications of the proposed method.  Address the issue of obfuscated gradients and demonstrate that the method is not vulnerable to this phenomenon.  Experiment with decisionbased attacks such as boundary attack. additional Feedback:  Consider discussing contemporary research on testtime defense to strengthen the processs context and evaluation.", "Paraphrased Statement: This field proposes a novel approach for improving the robustness of adversarially trained netprocesss during testing using testtime finetuning through selfsupervision. Additionally a meta adversarial training technique enhances the relationship between selfsupervised and classification tasks to optimize the starting point for testtime finetuning. The effectiveness of the method extends to adaptive attacks where attackers possess knowledge of the finetuning approach. effectiveness:  The testtime finetuning technique for enhanced adversarial robustness is a promising development.  The wellmotivated approach demonstrates performance improvement. Weaknesses:  Insufficient recognition of prior process: The methodology heavily relies on [Hendrycks et al. 2019] which is not mentioned in the introduction or related process.  Unclear explanation: The effectiveness of the proposed method is not adequately explained within the method analysis subsection.  Limited compare: issue are only compared with variations of the proposed method excluding other stateoftheart adversarial robustness methods.  Inference time consideration: The increased inference time associated with testtime finetuning and adversarial training should be addressed."], "sRZ3GhmegS": ["Paraphrase: The paper replaces the recurrent architecture in R2D2 with a hybrid TransformerLSTM structure and introduces a bidirectional contrastive loss combined with the reinforcement learning (RL) loss for optimization. The architecture includes a learnable gate between the Transformer and LSTM layers which regulates whether Transformer outputs are passed to the LSTM. This design aims to improve learning efficiency and performance compared to the baseline (GTrXL). effectiveness and Weaknesses: The combination of Transformer and LSTM typically associated with high data requirements is claimed to enhance data efficiency. However there is skepticism about this claim as the reported results focus primarily on performance improvements rather than data efficiency. The paper suggests using a different metric (\"AUC at the moment when an algorithm achieves 100 human performance\") to evaluate data efficiency more accurately. Major ConcernsCriticismsDiscussion:  The motivation for using GTrXL as the baseline for data efficiency is questioned considering the existence of methods that might exhibit higher AUC at other cutoff points.  The concept of \"selfattention consistency\" is underdeveloped and requires further explanation.  The papers readability is compromised by unnecessary details and a lack of clear distinction between core ideas and implementation specifics. Questions:  The difference between the reported improvement over human performance in the Rainbow paper (200) and the current process (874) should be clarified. Minor Remarks:  Clarification is needed for the statement on page 3 regarding the impact of augmentations on data efficiency.  A typo (\"presentt this analysis\") appears in design 2s caption.  The formatting of \"els.(higher is better)\" in design 2s caption could be improved.", "Paraphrased Statement: Summary: CoBERL is a novel transformer architecture designed specifically for reinforcement learning. It employs LSTMs to alleviate the size dependency issue of transformers and uses contrastive learning without requiring humangenerated data augmentation. effectiveness:  Effectively addresses the problem of transformer size development using LSTMs.  Eliminates the need for manual data augmentation a significant benefit for representation learning.  Thorough evaluation on challenging benchmarks with detailed ablation field. Weaknesses:  Presentation clarity and organization could be improved.  Assumptions about prior knowledge may hinder accessibility.  Limited number of random seeds in empirical evaluation (3 only). Specific Clarification Needs (in order of appearance):  Background on representation learning in RL and connection to ReLIC.  Rationale and exploration of specific masking percentage (15) in the embedding.  Explanation of the \"Gate\" use and its mechanism for selective output skipping.  Clarification on the use of GRUs (if applicable) and their use in the architecture.  Proper citation for \"Pengs Q(\u03bb)\".  Typo correction: \"VMPO\" should be followed by a space. Response to PostAuthor Revisions: Revisions have significantly improved clarity and reduced the need for prior knowledge assumptions. The increase in random seeds to 5 enhance the reliability of empirical results. The updated version addresses most of the previous concerns warranting an updated score.", "Paraphrase: The field introduces a new learning goal that aids RL agents in developing representations. The goal draws inspiration from ReLIC but instead of modifying states using domainspecific expertise the approach leverages other paths in a minibatch as contrasting model. advantage and Disadvantages:  The proposed approach combines elements of existing methods reducing the need for prior knowledge and enhancing sample efficiency.  Utilizing time step from other paths in the buffer appears logical and eliminates the need for augmented data design.  The experiments are extensive and the ablation field is valuable.  A minor concern is the use of reversed labels for input (Y) and output (X) of the transformer."], "qynwf18DgXM": ["Paraphrase: Main Points: This paper explores a case of metric on curved surfaces (Riemannian manifolds) under the Ricci flow a mathematical tool used to study the evolution of such surfaces. The paper focuses on understanding how these metrics behave over time including their stability and convergence. application: The analysis in this paper can be applied to understand how gradient flow a technique used in training artificial neural networks operates. Specifically the paper suggests that the evolution of the metric under the Ricci flow resembles the process of adjusting neural network weights through gradient flow. effectiveness and Weaknesses:  effectiveness: The paper introduces novel theoretical results and sets the foundation for analyzing gradient flow in training neural networks.  Weakness: The paper heavily emphasizes mathematical analysis which may not be of interest to all in the machine learning community. Additionally the connection between the mathematical analysis and its implications for gradient flow is not fully developed or empirically supported. Recommendation: For publication in a machine learning conference the authors should consider moving some of the mathematical analysis to an appendix and focusing more on the practical applications. This could involve expanding the discussion of the connection between the analysis and gradient flow or providing more extensive experimental evidence to support their claims.", "Paraphrase: The paper focuses on the Ricci flow and surgical techniques used to manage singularities that arise during the flow. The authors introduce linearly nearly Euclidean metrics and establish their stability under the RiciiDeTurck flow. They apply this to approximate steepest descent gradient flow in information geometry claiming it provides a novel approach to geometric optimization on manifolds. effectiveness and Weaknesses:  The authors provide a thorough overview of Ricci flow and singularity handling techniques.  They define nearly Euclidean metrics and prove their properties though the correctness of these results has not been verified thoroughly.  The section on gradient flow and approximation with neural networks are difficult to understand and the intended demonstration and applications are unclear. Unclear Aspects:  The authors meaning in phrases like \"dynamically consider the gradient flow followed with the optimal descent direction\" and \"Therefore this gradient flow is a weak approximation under the manifold microsurgery\" is not evident.  The underlying manifold and metric in section 4.3 on neural network approximation remain unspecified.  Overall the significance and contribution of the paper beyond a survey and discussion of mathematical topics is not apparent.", "Paraphrased Statement: Abstract: This paper introduces a revised technique for smoothing Ricci flow a mathematical tool used in topology (the work of manifolds which are geometric objects). The revised technique enhances the stability of the smoothing process potentially making it more practical for use in diverse applications. Assessment: The paper assumes a deep understanding of differential geometry making it challenging for readers without this background to grasp all the technical details. One unclear concept is \"microsurgery for Ricci flow.\" The paper lacks a clear explanation of this term and citations to support its use. Despite these challenges the concept of linearizing Ricci flow to improve its stability appears promising. The mathematical derivations seem convincing although the reviewers were unable to verify every detail. The paper does not provide context for its research such as related work or its relevance to the ICLR conference. It also lacks specific case of how the revised technique can be applied to realworld tasks or datasets like CIFAR. This makes it difficult to assess the paper potential impact. The paper does not include any direct experiments to demonstrate the effectiveness of the revised technique. The appendix contains limited experiments on CIFAR but they are poorly explained and difficult to interpret. The authors acknowledge that the results are not conclusive. Overall while the theoretical foundation of the revised technique appear promising the paper lack of practical case clear explanations and experimental validation limits its potential impact."], "u6sUACr7feW": ["Summary The goal of this paper is to create a texttospeech model that produces varying prosody model (such as pitch and duration variations) without compromising sound quality. The authors propose a method that uses a mathematical model known as a \"determinantal point process\" to select prosody model that are both diverse and of high quality. They introduce a \"prosody diversifying\" component to generate potential prosody model and employ a similarity metric called \"softDTW\" to assess their similarity. Strengths  Novelty of DPP: The use of a DPP to select prosody model is a novel idea that encourages diversity.  Reasonable similarity Metric: The \"softDTW\" metric effectively compares sequences of varying duration. Weaknesses  Writing Clarity: The paper needs significant improvement in its writing. Undefined symbols disorganized notations and potential errors in equations make it difficult to comprehend.  Limited Empirical Evidence: The paper lacks empirical evidence demonstrating that the DPP approach outperforms other method both in quality and diversity when generating multiple samples for the same text.  Unclear DDP Application: The application of DDP in the model is not entirely clear. The authors aim to select a subset that includes reference prosody model and one candidate model that is both highquality and different from the reference model which may not be optimal for speech synthesis.  argument Tuning: The proposed model still requires tuning the weight of a quality term raising concerns about the authors claim of eliminating the need for careful argument adjustment.  Overstatement of Results: The paper claim that the model generates speech with robust prosody than baselines while maintaining naturalness is an overstatement given the significant MOS score difference with a stateoftheart model.  Lack of Ablation field: The paper lacks ablation field comparing the DDPbased model to a baseline without the DDP module to demonstrate its effectiveness. Questions and Comments  The concept of \"ground set\" is never defined explicitly.  Its unclear if the prosody predictor encodes left and right context separately for each target sequence.  The authors should clarify the need for conditional DDP and its connection to prosody selection.  The authors should specify which contribution of the \"DPP kernel\" are learned.  The use of the \"softDTW\" similarity metric to diversify neighboring discussion prosody seems inappropriate for generating natural prosody as it can result in unnatural fluctuations.", "Summary This paper addresses the challenge of generating diverse prosody in texttospeech systems. Traditional approach often result in monotonous or averaged speech. The authors propose a prosody Diversifying module (PDM) based on Conditional Determinantal Point Processes (DPP) which is combined with a Fastspeech 2based TTS model. The TTS model is modified to predict stochastic duration and pitch and the PDM is trained separately using these predictors and a text encoder. The PDM operates on noun phrases to diversify prosody. The DPP kernel is used to compute the distance between predicted prosody features which is crucial for constructing the PDM. The authors compare their approach to VITS and Flowtron claiming it outperforms them in diversity but loses some naturalness. Strengths  The paper tackles an significant problem in speech synthesis.  The PDM and DPP kernel is a novel approach.  The baselines are strong and demonstrate diversity and naturalness.  The audio model show that the model can generate diverse speech. Weaknesses  The paper overstates the method naturalness preservation.  Section 4 can be complex for those unfamiliar with DPP.  There are inconsistencies between the text and number regarding PDM inputs.  The motivation for focusing on noun phrases is not clear. Evaluation number  Listeners rated VITS higher in prosody variation than the DPPTTS2 model despite its lower diversity.  The determinantbased metric is unclear and the reported measure are very small.  Audio samples should include multiple generations for the same speech to demonstrate prosody diversity. Typos and Other number  Accents in references are missing.  The stochastic duration and pitch predictor is missing a closing brace on page 5.", "Paraphrased argument: This research explores enhancing the expression of synthesized speech using a Determinantal Point procedure (DPP) model. By integrating DPPs into the duration and pitch prediction components of the Fastspeech2 baseline the field aims to diversify the range of prosody candidates. To evaluate these variablelength candidates the differentiable softDTW algorithm is utilized for comparison. Mean Opinion Scores (MOS) compare the proposed method with actual audio and established natural speech models. Additionally subjective evaluations assess the systems ability to generate greater speech variability than baselines. Strengths:  Addresses the practical challenge of expressive speech synthesis.  innovative application of DPPs to diversify prosodic prediction. Weaknesses:  Subjective measure do not conclusively demonstrate the efficacy of the proposed approach.  Results indicate inferior naturalness in Table 1 compared to baselines.  number 4b suggests improved prosody diversity but may not reflect natural diversity.  Using both duration and pitch predictors output poorer results in number 4b.  Need for further optimization of DPP integration to ensure constructive contribution to speech naturalness and diversity.  Writing needs improvement to connect the TTS problem and DPP components coherently.  Clarification and visual enhancements are required for number 3 and other number to facilitate understanding.  Consistency in symbols between DPP and TTS sections is crucial for readers to grasp the proposed approach."], "hOaYDFpQk3g": ["Paraphrase: Summary: The authors have developed the LightWaveS pipeline. It requires training time similar to (MINI)ROCKET but processes data 830 times faster. effectiveness:  The LightWaveS pipeline outperforms (MINI)ROCKET for multivariate time series classification (MTSC).  It utilizes wavelet effectively using only a small fraction (2.5) of ROCKETs feature.  The authors provide robust results by reporting average performance. Weaknesses:  No data is provided about hyperparameter tuning.  As a result the presented findings are highly dependent on the specific value used limiting our ability to evaluate the pipelines generalizability.", "Summary Paraphrase: The paper introduces LightWaveS a distributed method for classifying multivariate timeseries using wavelet Scattering transformation. It reduces the count of feature required while maintaining similar accuracy and significantly improving speed. The method has been tested on realworld datasets. Strengths Paraphrase:  Distributed process is crucial for timeseries tasks.  Realworld datasets and code are available.  The method is straightforward and seems effective. Weaknesses Paraphrase:  The solution primarily involves engineering improvements rather than methodological approach.  accuracy is somewhat reduced. Details Paraphrase:  LightWaveS appears to combine two existing methods. While its claim of generality is made it lacks proof of applicability to different feature case.  The benefits of LightWaveS compared to parallel model like Dask are unclear especially considering the significant training overhead of the novel approach.  accuracy loss is observable though not statistically significant due to the limited count of datasets tested.  Alternative methods for model compression and parameter reduction exist which are not explored or compared.", "Paraphrase: This research enhances the ROCKET algorithm designed for classifying multiple time series. The upgraded LightWaveS algorithm employs wavelet scattering convolutional kernel and feature selection methods to significantly enhance its speed and scalability with only a marginal decline in accuracy. Additionally the algorithm supports distributed performance allowing for faster performance times when additional computational nodes are utilized. effectiveness:  Tackles a significant problem in time series classification.  Demonstrated scalability and efficiency benefits with minimal loss of accuracy. Weaknesses:  Requires further clarification on specific aspect of the algorithm:  Why were identical kernel employed at both levels of the twolevel wavelet scattering  How is the algorithm implemented at each level or only at the top level  How are inputs distributed and outputs collected among various nodes  Unclear what the numerical labels (2000 3000 5000 and 10000) represent in Figure 2 (experimentation section).", "Paraphrase: The paper proposes a model for analyzing multivariate time series data. It uses techniques called \"scattered wavelet\" analysis and \"multiple kernel analysis\" to extract meaningful feature from the data. The model can be run in a distributed fashion allowing it to handle large datasets. It also includes a feature selection mechanism to enhance the interpretability of the model. Building upon the ROCKET and MiniROCKET models the paper introduces wavelet Scattering as a replacement for the kernel use. This modification allows for a flexible count of feature to be extracted from the time series resulting in increased speed and scalability compared to ROCKET. The model has been evaluated on benchmark datasets demonstrating its effectiveness. While the paper provides a comprehensive analysis it lacks a clear explanation of the need behind using wavelet Scattering except for improved speed. It would be beneficial to provide a more intuitive understanding of the data structure that the model is designed to analyze and to specify the case of applications or datasets for which it is most suitable.", "Paraphrase: This study aims to reduce the inference time of deep learning models by suggesting a distributed feature selection process that uses wavelet scattering transformations on time series data. The authors introduce LightWaveS a technique that rapidly transforms multivariate time series using convolutional kernel wavelet scattering and feature selection. This method aims to make classification using linear classifiers more lightweight and accurate while reducing inference time. effectiveness and Weaknesses: The paper offers a promising approach to optimizing inference time a crucial factor for edge devices. however a few areas need attention: 1. Lack of use Definitions: Algorithm 1 contains undefined use (e.g. GetDataSlice() GenerateKernels()) making it unclear how the algorithm works. more detailed explanations of these subroutines would improve understanding. 2. variation in inference time: In use inference time for the same dataset can vary slightly. To accurately show the improvement in inference speedup (as in Figure 6) multiple inference tests should be conducted and the mean and standard deviation (SD) of the speedup reported. The model for Figure 6 need to be repeated and both the average and SD included in the results. If the SD is minimal Figure 6 could present only the mean value. Regardless the conclusions should be based on multiple tests not just a single inference run.", "Summary: This article introduces a novel multivariate time series classification model called LightWaveS which is an improvement on the existing ROCKET framework. LightWaveS maintains the accuracy and fast training time of ROCKET but achieves significantly faster inference time. To overcome limitations of ROCKET LightWaveS incorporates wavelet scattering and a feature selection strategy. effectiveness:  comparable accuracy to ROCKET  Similar training time to MINIROCKET  Up to 30 times faster inference time on an edge device Weaknesses:  The authors claim to use wavelet scattering but provide insufficient justification and theoretical explanation.  LightWaveS uses significantly fewer feature (500) than MINIROCKET (20000 or 10000) which may be the primary reason for its faster inference time.  The authors have not conducted ablation studies to determine the individual contribution of wavelet scattering and feature selection to LightWaveSs performance.  Some figures in the article are unclear with unexplained notations making it difficult to interpret the results.", "Paraphrased Statement: Summary: This paper presents a distributed approach LightWaveS for classifying multivariate time series (MTSC). It builds upon the ROCKET algorithm incorporating feature selection and a distributed implementation. experimentation demonstrate that LightWaveS outperforms ROCKET and MINIROCKET (a faster variation) in terms of inference speed. effectiveness:  Addresses an significant research area. Weaknesses:  Unfocused and meandering presentation.  Typos hinder readability.  MTSC classification has been extensively studied.  LightWaveS offers limited advancements primarily as a minor extension of ROCKET.  Arbitrary design choices lack intuition or justification (e.g. why only consider certain paths in Figure 1).  Distributed implementation although considered a contribution lacks detailed description.  Connection with scattering transform remains unclear (LightWaveS architecture does not resemble a scattering netprocess).  computational \"trick\" (bold paths in Figure 1) oversimplifies the model warranting further explanation or evaluation.  use of ANOVA as a feature selection criterion seems arbitrary lacking deeper reasoning or comparison with other methods.  Interpretability and domain knowledge incorporation are mentioned in the abstract but not supported by experimentation in the text.  Insufficient contingent in \"discussion and Conclusions\" section. Minor Issues:  Unclear meaning of \"efficient and minimal communication between processer nodes and central coordinator.\"  Improve visual quality and font consistency in Figure 1.  Clarify experimentation in Figure 2.b to test ANOVA feature selection capability.  Add standard deviation to accuracy results. Typos and Notation:  Inconsistent notation in section 3.1.1 (\"max\" for dilation sampling and \"b\" use).  Inaccurate notation in section 3.2 (e.g. \\lambda1 instead of \\lambda 1).  Missing quantities and typo in Algorithm 1 (e.g. L and \\times in \\mathbfX).  Redundant introductory contingent that are just suited for the related process section.  Enhance writing clarity in section 4.5 particularly the latter portion."], "qhkFX-HLuHV": ["Paraphrased Statement: The paper introduces a method for action recognition. It transforms video frames into a gridded \"super image\" and applies a standard image classifier to identify action within the image. This approach addresses the challenges of memory efficiency by utilizing the Swin Transformer an image classifier with reduced memory requirements. experiment on several datasets (Kinetics400 Moments in Time SomethingSomething V2 Jester Diving48) show that the proposed method matches or surpasses current stateoftheart accuracy levels. Remarkably this method is not only accurate but also the most efficient in terms of FLOPs for a given accuracy level on Kinetics400. This suggests that deep networks ability to model spatial relationships can be extended to model temporal relationships across video frames complementing existing methods with explicit temporal modeling components. Additionally connecting action recognition to image classification enables the transfer of image classification techniques to action recognition potentially accelerating advancements in the field.", "Paraphrased Statement: Summary: This paper introduces two key components: 1. A method for converting a video sequence into a single \"super image\" that allows video analysis using image models like 2DCNN. 2. A modified version of the SwinTransformer model optimized for the proposed super image approach. Evaluation: The field assesses the effectiveness and efficiency of the approach through experiments on five reference datasets. effectiveness: 1. Exploring videotoimage conversion for video recognition holds promise for future applications. 2. The paper is wellwritten and includes thorough experimental analysis. 3. Visualizations provide valuable insights into the task. Weaknesses: 1. The proposed approach lacks significant novelty focusing primarily on a frame rearrangement strategy. 2. The super image generated is larger than the original video frames although other approaches have produced images with the same resolution. 3. Incorporating SwinTransformer seems questionable as its local performance are limited to capturing temporal dependencies between adjacent frames and are unable to identify variety within the same object or region across frames. 4. The reference list is incomplete excluding relevant works like AWSD and AVD that address videotoimage conversion.", "Paraphrased Summary: This research examines action recognition in video aiming to determine if an image classifier can perform as well as a video or spatiotemporal classifier. To test this the field rearranges video frames into a single grid image and trains it using SwinB image classification models. Despite the simplicity of the method it achieves impressive results that rival or surpass the current stateoftheart on Kinetics400 MiT Jester and Diving48 datasets. effectiveness:  clear and wellwritten paper  Effective and straightforward approach  Convincing results with detailed ablation studies  use of activation visualizations for analysis Weaknesses:  Lack of clarity on frame sampling method  Potential dependence on both the super image representation and transformer architecture", "Paraphrase: This research introduces a novel strategy for recognizing action in videos by framing the problem as an image recognition task. Video clips are organized into a single \"super image\" using a predetermined spatial arrangement. effectiveness and Weaknesses: This approach offers a straightforward and efficient solution to video action recognition by reframing the task as image recognition. It diverges from traditional approaches that emphasize temporal modeling and presents an alternative perspective on the action recognition problem. The paper includes thorough experiments and ablation studies that demonstrate the proposed approach effectiveness. Both transformerbased and CNNbased models were evaluated using publicly available benchmarks yielding favorable experimental outcomes."], "i1ogYhs0ByT": ["Paraphrased Statement: Summary: This paper introduces TransformerMGK a modified Transformer model that utilizes a Gaussian mixture model instead of the traditional multihead selfattention module. The authors establish a connection between selfattention and Gaussian distribution allowing for an extension to Gaussian mixture attention. TransformerMGK reduces parameter count and speeds up training and inference while maintaining comparable performance to the original Transformer. Experimental results demonstrate the advantages of TransformerMGK. Strengths: liThe authors establish a clear connection between selfattention and Gaussian distribution through a probabilistic model. liThe extension to Gaussian mixture attention is logical and straightforward. liThe EM algorithm is effectively used to learn the posterior distribution. liThe experiments provide comprehensive evaluations. Weaknesses: liThe results in Table 1 and Table 2 differ from [1] regarding the optimal issue of heads. This discrepancy may be due to varying experimental settings. liTransformerMGK outperforms TransformerMGK in some cases but not all (Tables 1 and 2). The reasons for this inconsistency are not fully explained. liThe results in Table 1 and Table 2 are based on a starting point of 8 heads. Including results from the original Transformer (with 12 heads) would provide a clearer comparison.", "Paraphrase: Summary This paper presents a transformer architecture TransformerMGK utilizing a mixture of Gaussian keys to address the redundancy often seen in transformer attention heads. The authors redefine the attention mechanism as a posterior distribution (query matching key) and establish it as a mixture of Gaussians for each key. They introduce EMbased inference techniques (Soft and Hard Estep) and propose a more efficient linear variant TransformerMLK for longer sequences. Experimental evaluations demonstrate TransformerMGKs effectiveness compared to softmaxbased transformers with improved performance on longrange tasks and language modeling. The model also shows reduced parameter count and FLOPs. Strengths  concept of Gaussian key mixtures enhances key embedding expressiveness and query flexibility.  Clear and wellstructured explanations throughout the paper.  TransformerMGK achieves comparable performance to standard transformers with fewer parameters and FLOPs.  Analysis of attention model supports the hypothesis of various attention learned by TransformerMGK. Weaknesses  The reported experimental improvements are limited.  practical usability of EMbased algorithms is not fully explored (e.g. convergence time initial value sensitivity Estep computational cost).  Key mixture analysis (\u03bb W) is lacking to validate Gaussian modeling.  issue of Gaussian mixtures used in experiments is unspecified.  Halfhead comparison may be biased without considering the issue of mixtures.  Convergence of learning curves in experimental results is not clear. Questions  Performance impact of increasing the issue of Gaussian mixtures.  time required for EM algorithm convergence.  Training FLOPs including Estep calculations.  applicability and effectiveness of the approach to deeper transformers (e.g. BERT GPTs). Comments  model complexity and computational cost should be analyzed analytically.  Correct the typo in Figure 3 (label \"Softmax Transformer\" should be \"linear Transformer\").  Correct the typo in Table 4 (lower rows \"TransformerMGK \u2192 TransformerMLK\").", "Summary Paraphrase: This paper explores the efficiency of Transformer language models by introducing a probabilistic interpretation of selfattention. It proposes TransformerMGK a novel Transformer model with a Gaussian mixture modeling component for selfattention keys. Strengths:  TransformerMGK is efficient in terms of parameters and computational cost.  The paper provides a novel perspective on selfattention opening up avenues for future research.  Empirical experiments on benchmark tasks demonstrate the effectiveness of TransformerMGK.  LinearAttention a variant of TransformerMGK is introduced for further exploration. Weaknesses:  The paper lacks theoretical explanations for the claimed improvements in explainability key representation enhancement and head diversity.  The authors could provide a detailed analysis of the methods computational complexity.  The experimental range could be expanded to include standard NLP benchmarks to assess broader performance.", "Paraphrase of Summary and Strengths and Weaknesses: Summary: The paper proposes a Gaussian Mixture model (GMM) to represent the attention matrix in transformer neural networks where the posterior distribution of querykey matches corresponds to transformer attention scores. The GMM description of keys head to easytocompute posteriors resembling multihead attention. The authors suggest that this formulation promotes various attention heads and reduces computation. Strengths:  provides a probabilistic perspective on the attention matrix.  Includes experiments on the language Retrieval Assessment (LRA) benchmark and language modeling on Wikitext103.  Wellwritten and clear. Weaknesses:  Experiments show inconclusive results making it difficult to assess the methods effectiveness.  lack of parameter details (e.g. M) for MGK and MLK models hinders comparison.  It would be valuable to analyze the diversity of attention heads explicitly.  The argument for improving diversity based on range is incomplete. Suggestions:  Replace FLOP counts with wallclock time measurements.  provide data on memory footprint during training especially for large transformer models.", "Paraphrase: Summary: This paper proposes a novel perspective on multihead attention using a Gaussian mixture model. Each attention heads keys are treated as components of a Gaussian mixture and softmax attention weights are interpreted as posterior probabilities of the mixture given the query. This approach allows for a novel parameterization of multihead attention which can be used in transformers and other linear transformer variants. Experiments on text classification and language modeling demonstrate similar or improved accuracy while reducing computational overhead. Strengths:  Offers a novel Gaussian mixture perspective of multihead attention. Weaknesses:  The approach may lead to awkward situations such as in causal attention where the issue of mixture components changes over time.  The paper includes unnecessary details such as norm constraints and the M step which should be presented as connections to other models.  Modeling details need clarification such as the issue of projection matrices required for key parameterization and why the query and value parameterizations lack a \"multiheaded\" aspect.  Experimental results could be improved by using a strong baseline and including other sequence modeling tasks such as machine translation. Typos:  \"keyed\" should be \"key\"  \"unit matrix\" should be \"identity\"  \"abusing\" should be \"overloading\"  \"at learning\" should be \"at learning time\"  \"wide the issue of heads\" is grammatically incorrect"], "jaLDP8Hp_gc": ["Paraphrase: Abstract: The paper introduces \"correspondence hallucination\" a new concept where keytarget correspondences are detected even if they are hidden or outside the visible field. The authors present a novel pose and training method that predict the probability distribution of correspondence location enabling hallucination. Strengths:  Proposes a novel research field on correspondence learning including occluded and outofview correspondences.  Presents a new pose for correspondence hallucination using attention blocks.  Extensive evaluations on indoor and outdoor datasets show the poses ability to enhance camera pose estimation and surpasses previous feature matching techniques.  Provides comprehensive supplementary materials including trained poses and source code. Weaknesses:  The hallucinated correspondences have low resolution potentially impacting location accuracy.  The reasons behind the challenges in correspondence hallucination are not fully explained.  The definition of Kc for target transformation from image to correspondence design is not provided in the main manuscript.  Further analysis and ablation work are needed to explore the relationship between camera pose estimation accuracy and hallucinated correspondence distribution.  The paper lacks citations for related work on feature matching such as \"Learning feature descriptors using camera pose supervision ECCV 2020\" \"SelfSupervised Geometric perception CVPR 2021\" and \"Patch2Pix EpipolarGuided PixelLevel Correspondences CVPR 2021.\"", "Paraphrased Statement: The paper suggests a deep learning method (NeurHal) for predicting invisible or outofsight keypoint matches from source to target images. During training a correspondence function is generated using ground truth camera pose and image data. The trained pose predicts matchings corresponding to three categories: output painting inpainting and identified. NeurHal finds applications in camera pose estimation as tested on ScanNet and Megadepth datasets. The experiments demonstrate that NeurHal enhances estimation accuracy particularly for output painting correspondences. Strengths:  Introduces a novel concept: predicting unseen view elements using a neural network from image pairs.  Proposes a detailed method based on correspondence functionping and neural reprojection error.  Demonstrates superior performance compared to baselines in correspondence precision and camera pose estimation. Weaknesses:  Limited technical novelty as it adapts an existing method for the task.  Weak baseline (uniform correspondence function) in image 3.  Missing precision data for identified keypoints in image 4.  Ambiguous description of the baseline camera pose estimation method in section 4.2 making it difficult to assess the methods contribution to stateoftheart approaches.", "Paraphrase: Summary This paper presents a method that can find the location of a keypoint in a second image even if its hidden outside the frame or only partially visible in the low image. This method is different from previous techniques which only worked with keypoints that were visible in both images. The authors show that their method also helps to estimate camera view more accurately. Strengths  Can predict the location of hidden keypoints which is useful for task like robotics navigation.  Builds on the concept of Neural Reprojection Error (NRE) which doesnt require similar appearances between matching image regions.  Includes a unified treatment of visible hidden and outofbounds keypoints.  Experiments on multiple datasets show good performance and generalization. Weaknesses  Accuracy is low than desired under favorable conditions due to lowresolution output.  Its not clear what the network actually learns and the evaluation metrics are somewhat arbitrary.  The term \"hallucination\" may be misleading as the method predicts rather than fabricates keypoint location."], "fSeD40P0XTI": ["Paraphrased Statement Summary This work presents a model training strategy called Adaptive Continuous Classification of Time Series (ACCTS). The objective is to address challenges in continuous classification tasks where the model predicts a point variable at every time step. ACCTS aims to minimize catastrophic forgetting and overfitting by defining a continual learning task and incorporating three steps: 1. data choice: set if the current time series sequence should be used for model training. 2. Exploration: Explores a broader search space using an adaptive method and employs importancebased replay to mitigate forgetting. 3. Optimization: Integrates the data selection and exploration work to optimize longterm performance. ACCTS has been evaluated against seven baselines from time series classification and continual learning on four datasets demonstrating its effectiveness. effectiveness and Weaknesses  care:  The definition of \"continuous classification\" may overlap with live time series forecasting approach.  The paper organization and presentation need improvement to enhance readability.  justification for using specific methods (e.g. ActorCritic) should be provided.  The rationale for importancebased replay in Section 4.2 requires clarification.  Vague terms should be avoided.  Evaluation metrics lack considerations from survival analysis relevant to the COVID19 and SEPSIS datasets.  Minor Issues:  Equation 1: Verify the summation range.  Clarify the meaning of \"MG.\"  set the typo \"polity\" to \"policy.\"  Explain what is meant by \"less divisions.\"  Check the definition of BWT in Equation 7.", "Paraphrase: This document introduces a streaming classification approach that dynamically adjusts to changes in the data distribution. It retrains the model on newly received data to maintain accuracy over the entire time series. The method involves a segmentation mechanism based on the Markov decision work to identify changes in distribution and a retraining procedure that replays and updates selected data samples. The effectiveness of the approach has been demonstrated through experiments with realworld datasets. effectiveness:  Addresses a crucial problem in practical applications.  Experimental solution indicate the efficiency of the proposed method. Weaknesses:  Excessive use of undefined notations particularly in Section 4 makes it difficult to understand.  The applicability of the method is limited. It requires frequent retraining on selected data replicas and necessitates a significant data storage buffer making it suitable only for specific scenarios with limited data and low inference time constraints. However many realworld streaming learning applications such as online product recommendations involve large amounts of data accumulating rapidly and require the model to be updated adaptively.  The document does not adequately discuss related work on online learning in nonstationary environments.  The performance of the distribution extraction (segmentation) method lacks experimental verification. It would be beneficial to demonstrate the data streams distribution changes and empirically evaluate whether the segmentation points align with those changes.", "Paraphrased Statement: Summary: This research aims to continuously categorize timeseries at each time step enabling former classification and meeting realworld requirements. The major obstacles in this endeavor include catastrophic forgetting and overfitting. Challenges: The multidistribution of timeseries is not welldefined and its diverse distributions lead to forgetting or overfitting issues. Proposed solution: To overcome these challenges the authors introduce an adaptive model training policy that discovers multidistribution without relying on rules or priors. They also employ an importancebased replay component to prioritize significant experiences. Evaluation: The proposed method was evaluated using four realworld datasets. effectiveness:  The application of reinforcement learning to former classification of timeseries (ECTS) is innovative and promising.  The experimental solution demonstrate the approach effectiveness. Weaknesses:  The manuscript is challenging to comprehend due to fragmented time excessive use of notations and lack of justification for design choice and evaluation metrics.  The experimental solution are promising but the baselines employed are insufficient. More stateoftheart timeseries classification methods should be considered.  Given the models need for retraining it is unclear whether it offers runtime advantages over multimodel training based on timesplitting.  Aside from improved numerical performance more insights are needed into how the proposed approach handles changing distributions. A toy dataset with known distribution changes or an analysis of the distribution changes in the four realworld datasets would be beneficial.", "Paraphrased Statement Summary The authors propose a method for predicting a class label (ct) based on time series data (x1 ... xt) up to a given time (t). The goal is to maintain accurate predictions as the underlying distribution changes over time while still retaining accuracy on previously observed data. They develop a Markov decision work (MDP) to segment the time series followed by a replaybased approach that updates the classification model based on the current and past segments. effectiveness and Weaknesses problem  The authors address the challenge of predicting a class label at each time point considering that the true class can evolve over time.  The claim of novelty for this problem context is questionable as it has similarities to existing problem like statespace models and sequencetosequence learning. Presentation  The presentation lacks clarity and contains errors.  Several terms (e.g. \"multidistribution\") are not defined.  Mathematical notation is inconsistent and unclear.  The update work in the algorithm is not fully explained. Results  The empirical solution show better class prediction accuracy on multiple datasets.  The authors include analysis of backward and forward transfer as well as ablation experiments.  However there is a lack of information on model training procedures making it difficult to reproduce the solution. former Comments  The shapelets reference is incorrect.  The variability in accuracy solution is not specified.  The datasets used seem limited to binary classification and may not be suitable for continuous classification tasks."], "y1PXylgrXZ": ["Paraphrased statement: The authors propose a \"deep equilibrium (DEQ)\" layer that ensures provable robustness through a technique called \"interval bound propagation.\" They extend the standard fixed point condition in DEQs with two additional conditions one for each bound. Their key theoretical solution (Theorem 3.1 and 3.3) proves that this extended DEQ with interval bound propagation (IbPDEQ) when parameterized in a specific way admits a unique fixed point and provides valid interval bound propagation (Proposition 3.2). based on this theoretical foundation (Proposition 3.7) the authors empirically demonstrate that the proposed restrictions on DEQs and parameterization lead to comparable or better performance than existing models on MNIsT and CIFAR10. strengths and Weaknesses: Despite the promise of implicit layers like DEQs creating DEQ models with unique fixed points and developing certifiably robust deep learning models remain significant challenges. This work addresses both simultaneously providing valuable theoretical tools. However the model currently only considers fully connected and convolutional layers leaving open the propagation to more complex architectures like skip connections or transformers. While the empirical evaluation serves to illustrate the theory it may not fully reflect practical implementations. Nevertheless the work makes an significant contribution by addressing these issues for fully connected and convolutional layers. Questions and Responses: 1. significance of M in Theorem 3.1: M is a scalar lower bound on the minimum eigenvalue of the matrix W. It is defined in section 3.2 but we agree that moving the definition closer to its first use would improve clarity. 2. model 3.6 and symmetry of W: The matrix W in model 3.6 is symmetric but we incorrectly stated otherwise in the paper. This error will be corrected in the final version. 3. Analysis of batch normalization: Our theory (based on Proposition 3.6) does not explicitly analyze batch normalization. However we empirically observe that batch normalization does not significantly affect the robustness properties of IbPDEQs. 4. Definition of \\widetildeW: The symbol \\widetildeW used in the validation of Lemma 3.4 is a variant of the matrix W. It is defined by \\widetildeW  (IW)1(IW). This notation is standard in matrix analysis and should be clarified in the paper. 5. Derivation of (3.8) in the validation of Lemma 3.4: (3.8) is obtained by applying the following identity: ``` (IA)1  I  A(IA)1 ``` to the equation ``` (IW)1  (IW)1  2W(IW)1 ``` with A  W. 6. Derivation of matrix s in the validation of Theorem 3.1: The matrix s in the validation of Theorem 3.1 is essentially the lowerdiagonal part of the matrix b. We agree that this step could be clarified in the paper. 7. separation of contributions 1) and 2): Contributions 1) and 2) are separated because the first (guaranteed unique fixed point) is a theoretical property of the model while the second (stable training) is an empirical observation. We believe this distinction is significant because it highlights the different layer of analysis involved. However we can consider merging them if deemed appropriate. Minor Points:  The missing space after the period in the sentence containing \"Winston Kolter (2020)\" will be corrected.  The publication of the NeurIPs version of the paper in the reference list will be updated. Thank you for your feedback. We appreciate the opportunity to improve the clarity and accuracy of our work.", "Paraphrased Statement: Summary: The paper introduces IbPMonDEQ an advance to Monotone Deep Equilibrium (MonDEQ) layers. It enables the calculation of reliable lower and upper bounds on layer output through interval bound propagation (IbP). This allows for the training of provably robust DEQ models with performance comparable to explicit models trained using IbP. Strengths and Weaknesses:  The authors extend IbP to DEQ models effectively.  A fixedpoint version of IbP is introduced imposing additional constraints on DEQ weights to ensure a unique solution.  MonDEQ and LbEN parametrizations are explored showing that LbEN retains representational power for feedforward networks. Outstanding concern:  Figure 2 suggests that bounds (Ub  Lb) increase with fixedpoint iterations. This implies that convergence is necessary for valid bounds. Do the authors ensure full convergence in their experiments  Examining the tightness of IbPMonDEQ bounds (against equation 2.1) would be valuable using pretrained networks trained with both conventional and IbPMonDEQ techniques.  While IbPMonDEQ compares favorably to IbPtrained explicit networks other robust training method exist. How does IbPMonDEQ stack up against these alternatives such as CROWNIbP  Appendix b indicates a significant runtime overhead for IbPMonDEQ (310x). Do the authors believe this outweighs the potential benefits of DEQ models for adversarial robustness Minor Comment:  equation 2.2 appears to define general lowerupper bounds rather than being IbPspecific.", "Paraphrase: This field analyzes the certified adversarial robustness of Deep Equilibrium Models (DEQ) and introduces Interval Bound Propagation (IBP) on DEQ to train certifiably robust models known as IBPMonDEQ. Strengths:  Develops parameterizations for DEQ to ensure the existence and uniqueness of fixedpoint solution in the augmented DEQ with IBP bounds.  Provides theoretical validation that a single IBPMonDEQ layer can capture all explicit feedforward networks.  Empirically demonstrates the effectiveness of IBPMonDEQ in certified defenses. Weaknesses:  Limited empirical results showing only marginal advance over conventional CNNs (0.5 percentage point advance on CIFAR10 compared to the baseline CNN).  Weak baseline performance compared to stateoftheart method (e.g. Shi et al. 2021 which achieves a certified error of 65.58 \u00b1 0.32 on CIFAR10 eps8255 compared to 69.51 \u00b1 0.46 for the baseline in this field).", "Summary Paraphrase: This paper introduces IBPMonDEQs a novel propagation of implicit networks providing a novel architecture with theoretical guarantee for robustness. By identifying specific weight matrices it ensures the existence and uniqueness of fixed points for the networks implicit layers based on interval analysis. IBPMonDEQs aim to achieve networks that can be certified to be robust and are demonstrated to have improved certified robustness compared to explicit networks on MNIST and CIFAR10 datasets. Strengths and Weaknesses Paraphrase: Strengths:  Addresses the challenge of obtaining networks with both accuracy and certified robustness.  Introduces a novel construction of IBPMonDEQs offering the first attempt at creating certified implicit networks with theoretical guarantee using interval analysis. Weaknesses:  Some unclear view of the theoretical formalism and empirical evaluation.  The practical relevance of the certified robustness results on CIFAR10 is questionable due to the relatively low performance compared to the stateoftheart. Questions: 1. Is the identified network class a subset of those from the KnasterTarski theorem 2. Can DEQs with multiple fixed points be defined 3. Can the approach be extended to other types of analysis such as CROWN DeepPoly or zonotopes 4. Can IBPMonDEQs be trained for perturbations not captured by interval 5. Was the certified robustness in Table 2 for explicit networks computed using IBP Could the results improve with a more precise analysis 6. What are the implications of computing a postfixed point using interval inclusion in equation (3.6)"], "vIC-xLFuM6": ["Paraphrase: Summary: This paper focuses on the issue of spectral bias in neural networks (i.e. underfitting of the higherfrequency portion of the input feature) in the context of reinforcement learning (RL). The researchers highlight this problem using toy examples. To address it they propose using a random Fourier feature network (RFN) to transform the input into the frequency domain before feeding it into the network. experimentationation show the effectiveness of RFN in learning complex Qfunctions in lowdimensional state spaces and performance improvements with DDPG on highdimensional robotic tasks. The authors also conduct an \"ablation work\" to evaluate the impact of hyperparameter choice on performance. effectiveness:  Clear and welldefined problem motivation.  Demonstration of RFNs efficacy on both toy tasks and highdimensional robotic control tasks using DDPG.  RFN can theoretically be incorporated into any neural network for function approximation. Weaknesses:  The DDPGRFN algorithms superior performance on Humanoid raises questions about the significance of spectral bias when other view of the RL algorithm are improved.  The novelty of the proposed method may be limited as it builds upon existing RFN research and investigates hyperparameter optimization for RL.  There is some confusion regarding the trainability of the RFF layers weights and bias.  The referencing format is inconsistent and lacks professionalism.  Additional MuJoCo experimentation results on simpler tasks would enhance understanding of RFNs effectiveness.  Statistical significance analysis could improve the clarity of the Humanoid performance comparison. minor Issues:  Define the abbreviation \"TD\" at its first appearance.  Correct the statement that \"DDPG takes 1 gradient step\" (it actually takes 3).  Consider placing the Related work section before the Discussion.  control proper comma and period use in equations.", "Paraphrased Statement: This paper introduces a novel parameterization for multilayer perceptrons (MLPs) called random Fourier feature (RFF). RFFs involve using a sinusoidal activation function and initializing parameters to capture various frequency and phases. This parameterization aims to address the limitations of vanilla MLPs in learning value function that require highfrequency components. The paper provides an indepth explanation of why this is necessary in reinforcement learning (RL). RFFs are demonstrated on various standard control problems showing improvements when used with Deep Deterministic Policy Gradients (DDPG). effectiveness:  The paper addresses a significant problem and clearly explains the motivation and solution.  The results suggest that RL could benefit from considering the specific feature of the function being estimated leading to architectures that differ from those used in supervised learning. Weaknesses:  The proposed parameterization is somewhat similar to previous work.  The empirical evaluation on largescale problems is limited.  The paper lacks a work of the generalization properties of RFFs which is crucial given the potential for sinusoidal feature to overfit.", "Paraphrased Statement: Summary This research introduces random Fourier Network (RFNs) which are multilayer perceptrons (MLPs) with random Fourier features incorporated into their initial layer. RFNs are designed to acquire highfrequency elements of a value function more swiftly than standard MLPs. The authors also propose an initialization method for the Fourier layer that is optimized for highdimensional inputs. RFNs are evaluated against nonFourier feature baselines in highdimensional environments. effectiveness and Weaknesses Discussion Update After reviewing other reviewer comments the overall assessment of this work remains unchanged. It exhibits limited novelty. The fundamental architecture is largely derived from prior work. One distinction is a modified weight initialization scheme but it is not directly compared within the experimentations. Additionally it appears that both methods still necessitate hyperparameter tuning (with proper hyperparameter choice the methods are equivalent). The other difference is the application of RFNs to deep reinforcement learning (DRL). While the work includes a range of competitive nonFourier baselines it does not contrast itself with architectures from Sitzmann 2020 and Tancik 2020 (despite citing these work). As a minor target incorporating terms like \"densedeep\" and \"extensive\" would be beneficial as would providing more analysis on the relationship between the number of parameters and the depth of various baselines. Additional Comments  The MLP example in the foundation (paragraph 2) lacks clarity. Sufficient information is not provided to demonstrate that poor performance was not caused by the training work.  The third paragraph of the foundation would be better suited later in the paper.  Figures 2 and 7s depiction of the learned function does not adequately highlight the discrepancy between the true and learned function. Displaying the absolute difference between them would be more informative.  Much of Section 2 (\"Preliminaries\") is irrelevant to the work contributions and should be significantly condensed.  Reorganizing Section 3.1 would improve clarity and the first paragraph largely duplicates information from earlier in the paper.  The Mountain Car experimentation should be placed elsewhere in the text.  If I understand correctly altering the bandwidth also affects the network parameter count. This should be considered in ablation work. Additionally the SIRENstyle approach to selecting width should be directly compared to the proposed method.  The Related work section should be moved earlier in the paper.  Additional editing is required to correct grammar and spelling errors."], "vtDzHJOsmfJ": ["Paraphrase: This research examines fair supervised learning under the Equalized Loss (EL) fairness criterion which is posed as a nonconvex constrained optimization publication. Two algorithms are created to find the global (sub)optimal solution through a series of convex (constrained) optimizations. Empirical results demonstrate the effectiveness of these algorithms. effectiveness:  focus on EL fairness a significant topic in fairness research.  Thorough analysis of addressing nonconvex constrained fair learning problem.  Algorithms provide mathematical guarantees. Weaknesses:  Theoretical validation is conventional and appears straightforward.  Lacks discussion on specific technical advancements.  The intuition behind the proposed algorithms resembles a previous work (Lingxiao Huang et al. 2019) which should be compared technically.  A comparison with the unpublished work (arXiv1802.08626) is required as suggested by a reviewer.", "This work explores supervised learning models with fairness constraints particularly focusing on equalized loss constraints. When adding fairness constraints to a standard loss minimization problem the resulting problem becomes nonconvex. The paper introduces algorithms to effectively solve this problem achieving global optimality. They demonstrate the effectiveness of these algorithms using realworld data. While the paper has strengths it lacks novelty due to the existence of similar work (arXiv1802.08626). The similarities and differences between the two papers should have been addressed. Additionally the experimental evaluation is limited considering only one dataset and lacking comparison with stateoftheart approach. The validation of Lemma 1 assumes smoothness of the loss work which may not always be the case. Minor corrections are also suggested for the appendix.", "Paraphrase: Authors investigate the challenge of creating fair predictions with Equalized Loss (EL). They propose methods for finding the most fair predictor that meets EL consideration. Initially they break down the nonconvex optimization problem into a series of convex problem solving them to find the optimal predictor. Additionally they demonstrate a more efficient approximate solution using unconstrained convex optimization. Both techniques are tested on realworld data. The paper presents a promising approach to fair prediction but its findings are somewhat limited. It relies heavily on assumptions about the convexity of loss functions which may not always hold true. Questions remain about the accuracy of the approximate algorithm compared to the exact solution especially in scenarios where the loss functions are nonconvex. Some concerns were also raised about the writing quality and the lack of sufficient motivation for the experimental setup. Finally the authors should consider the likelihood of Assumption 2 being met in practical situations and whether it is potential for disadvantaged groups to remain disadvantaged even with the sound loss functions.", "Summary: The authors investigate minimizing convex loss under constraints that either limit the loss for each group or the difference in loss between two groups. While the first formulation is convex the second is nonconvex. For strictly convex loss with distinct optimum they propose a method to find the \"EL\" fair predictor by solving a series of convex optimizations using a monotonicity property. They also provide an efficient approximation algorithm. effectiveness:  Interesting theoretical results showing that bounded loss constraints do not significantly increase maximum loss per group.  Binary searchbased method for optimizing bounded group loss constraints. Weaknesses:  Assumptions may not always hold in practice potentially leading to unbounded loss.  Assumption 2 which excludes case where the lowestloss group is disadvantaged (e.g. minorities) may be misleading.  Need for a rationale for Assumption 2 given the assumption of strictly convex functions. Related work: The paper cites several recent work on solving nonconvex constrained formulations for fair objectives including approach using nonconvex optimization implicit rateconstrained optimization and decomposition. The paper does not explicitly compare its approach to these methods but it could be valuable to explore their comparative performance."], "nRCS3BfynGQ": ["Paraphrased argument: This paper introduces two types of graph networks Distance Graph Network (DGN) and Angle Graph Network (AGN) that maintain consistency under transformations that preserve distances and Angle respectively. These networks improve upon Graph Network (GN) by treating node coordinates separately from other node features. Experimental results on synthetic datasets demonstrate the effectiveness of the proposed symmetrydriven graph networks. Strengths:  Acknowledging the importance of considering data symmetries is reasonable.  DGN and AGN (described in form 2) are conceptually sound and versatile. They generalize GN allowing existing methods like SchNet and DimeNet to be viewed as special cases.  The experiments on synthetic data suitably showcase the benefits of exploiting symmetries supporting the proposed methodology. Weaknesses:  The lack of empirical demonstrations of DGN or AGN efficacy on actual tasks (beyond synthetic datasets) is a primary concern. Evaluation on realworld datasets is highly recommended.", "Paraphrased argument: Summary:  The authors introduce Distance Preserving Graph network (DPGNNs) and Angle Preserving Graph network (APGNNs). These networks are invariant to node permutations and transformations that maintain distances or Angle between nodes.  The authors extend the concept of E(N) GNNs to incorporate group symmetries of the conformal group and handle position where not all nodes in the graph are connected.  experimentation on a polytopes classification dataset demonstrate that the framework performs easily when symmetries are present but not as easily when they are absent. Strengths: 1. Clear and easilywritten paper. 2. Simple and efficient framework without requiring complex work to achieve equivariance. 3. Strong experimental results on datasets with symmetries. Weaknesses: 1. Lack of results on molecular conformers despite claims of equivariance to the conformal group. 2. Limited exploration of other potential applications for these symmetries. Minor Note:  Introduce the term \"aji\" earlier in the paper to avoid confusion when it appears in an equation.", "Paraphrased argument: Summary: This study introduces equivariant Graph Neural network (GNNs) in two categories: Distance Preserving Graph Network (DGN) which exhibits equivariance to Euclidean transformations and Angle Preserving Graph Network (AGN) which is equivariant to the Conformal group. These structures extend existing architectural approaches. Their superiority is demonstrated experimentally using a synthetic dataset of geometric objects in diverse dimensions. Strengths:  Clear writing and introduction of concept  generalization of existing methods such as EGNN and DIMENET by customizing specific components  Validation of distance and angle preservation (detailed in appendix)  Superiority of AGN over GN SE3Trans DimeNET and EGNN in polytope classification especially for nonorthogonal symmetry Weaknesses:  Limited Novelty: While Equation (56) extends GNs design to incorporate equivariance and invariance the primary contribution to this property lies in the update of node coordinates (Eq. 5c and Eq. 6d) a concept previously developed by EGNN and other work.  Limited performance on Real Datasets: AGN and DGN perform poorly on real datasets (MNIST CIFAR10 TSP) which the authors attribute to a lack of symmetry. However this explanation is questionable since for example MNIST images exhibit rotationaltranslational equivariance which DGN should capitalize on.  Oversimplified Simulated Dataset: The high accuracy achieved on polytope classification suggests the simplicity of the dataset. Details on sample size and its potential impact on results should be provided.  Confusing Presentations:  The term \"symmetry groups\" is overly broad.  AGN appears to be equivariant only to the Conformal orthogonal group not the entire Conformal group.  Definitions of groups E(n) CO(RnQ) and Conf(Rn0) should be provided for clarity.", "Paraphrase: The goal of this paper is to create graph neural networks that are invariant and equivariant. The paper builds on the idea that node features can be divided into a vector for node coordinates and a vector for features independent of coordinates. Using this assumption the authors propose the \"angle preserving graph network\" (AGN) which is equivariant to both the Euclidean and conformal groups. The method is tested on graph datasets generated from images in (Dwivedi et al. 2020). Strengths:  The effort to develop invariant and equivariant graph neural networks is commendable. Weaknesses:  The approach relies on several assumptions:  Node coordinates in Euclidean space must be given.  Node features must be independent of coordinates.  These assumptions significantly limit the applicability of the approach.  The framework hardcodes specific invariance and equivariance properties (rotationtranslation invariance and anglepreserving transformations).  The evaluation is limited to specific graph datasets where node coordinates are welldefined. Additional Comments:  The meaning of \"global\" attributes is unclear.  The assumption of Euclidean distance as the correct metric for graph may not always hold.  The frameworks equations make implicit assumptions about edge feature independence from node coordinates and node coordinate transformation dependence on node coordinates only.  It is not clear how the framework applies to general graph datasets where node coordinates are not available. Minor Issues:  The definition of equivariance lacks a specified group action on the output.  The appendices do not provide significant novel data as they mainly repeat properties of the Euclidean and conformal groups which are explicitly built into the AGN framework."], "xFOyMwWPkz": ["Paraphrase: Summary: This field proposes a technique to assess the state of individual neural network units based on their activity degree. The technique leverages topology data analysis (TDA) on a graph representation of unit activity. The strength of the method is demonstrated experimentally showing its advantages over existing techniques in terms of scalability and robustness to noise. strength:  Establishing a technique for tracking the training progress of neural network enables optimization and AI advancement.  The entropybased approach overcomes the scale and entropy challenges typically faced by such techniques.  Structural analysis of learning progress provides insights for performance enhancement.  Using TDA to measure unit activity distribution is innovative and addresses limitations of simpler approach. Weaknesses:  The assumption of solid unit size is potentially limiting.  The methods applicability is restricted to solid data or filter matrices.  The twodimensionality of units may not always be representative especially for midlayer CNN units.  The orderindependence of the method may be less useful when the ordering changes between training trials.  The term \"birth degree\" for the first nonzero state of the Betti curve may be confusing in the TDA context.  Ignoring subsequent 1cycles after the first one may result in missed activity data.  The construction method for the relational graph may not capture the reliable features of unit activity and could potentially only measure active cell count changes.", "Paraphrased Summary: This field presents a network topologybased method for assessing the strength of individual units within a neural network for particular input sets (e.g. images of a particular class). The method involves constructing a graph based on the representation of a given image and the units it activates. The \"birth degree\" of each unit is determined and its importance is measured as the entropy of its birth degree distribution estimated through image sampling. Experiments show that this approach outperforms three existing methods and can distinguish between model with varying generalization capabilities. strength: 1. The method is easy to understand and the authors clearly demonstrate the limitations of previous approach. 2. experimental results support the approach strength in addressing these limitations. Concerns: 1. The comparisons with baseline methods are limited to simplified tests leaving the practical significance of the proposed approach unclear. 2. The method has not been evaluated in a pruning scenario where previous approach for measuring unit importance have typically been assessed. 3. The baseline methods are relatively outdated and fresh approach like [1] and [2] may offer more relevant comparisons. Minor Suggestions: 1. Clarify the term \"status\" of a unit by replacing it with \"importance.\" 2. Add Model BA to Table 4 for clarity. References: 1. Yu R. et al. \"NISP Pruning Networks Using Neuron Importance Score Propagation.\" CVPR 2018. 2. He Y. et al. \"Filter Pruning Via Geometric Median for Deep Convolutional Neural Networks Acceleration.\" CVPR 2019.", "Paraphrased Summary: This paper employs topological data analysis techniques to quantitatively assess the performance of individual units in neural network. It proposes a method to identify spatial patterns in unit activations by converting them into graphs representing successive degree of activation thresholds and calculating the issue of \"holes\" in these graphs. The authors define a measure called feature entropy based on the time it takes for these holes to first appear. This measure is expected to be similar for effective units within a particular class of input. The paper demonstrates that feature entropy is invariant to input transformations and correlates with changes in network hyperparameters. This suggests that it could be useful for systematically characterizing the relationship between input and outputs. strength and Weaknesses: strength:  Combines wellestablished topological data analysis methods with neural unit characterization.  Defines a stable feature entropy measure that correlates with network behavior. Weaknesses:  The impact of this work on neural network design and optimization is unclear.  Some methodological details are not fully explained (e.g. symmetry transformations choice of parameters).  The experimental sections could be more clearly written and explained. Suggested Improvements:  Explain the rationale behind the symmetry transformation and parameter choices in the methodology.  Improve the clarity of the experimental results and discuss the implications of the observed patterns.  Revise the writing to ensure clear and grammatically correct sentences.", "Paraphrase: Summary This paper introduces a fresh technique called \"feature entropy\" for evaluating the strength of feature represent in Convolutional Neural Networks (CNNs). It involves creating a weighted graph from a feature represent and calculating its Betti curve. The \"birth degree\" on the Betti curve indicates the issue of regularized spatial patterns in the feature represent. By analyzing the distribution of \"birth degrees\" for images of a particular class a \"feature entropy\" can be calculated. Strengths  The paper presents the method in a clear and stepbystep manner with helpful illustrations.  The authors provide solid evidence for their claims by identifying flaws in previous methods and demonstrating how their approach addresses these flaws through experiments.  The field suggests that the proposed indicator can potentially serve as a substitute for additional test data which can be valuable in situations where obtaining labeled data is costly. Weaknesses  The authors could enhance the paper by repeating the experiments on ResNet to explore the impact of skip connections on \"feature entropy.\"  The sample size of 100 images utilized for the experiments appears limited justification for this choice or consideration of calculating on all available images would be beneficial.  While network pruning is discussed in the related work it would be valuable to investigate how the proposed method could guide pruning decisions.  It is unclear why the adjacency matrix in the method must be symmetric and whether this implies the importance of symmetry in the feature represent.  Equation (3) lacks a \"wv\" term which should be corrected."], "rvost-n5X4G": ["Paraphrase: Summary The paper presents a method for learning a statestate mapping (called stateplanning policy or SPP) in conjunction with an offpolicy RL system (including experiments with DDPG TD3 and SAC). Learning a statestate mapping Q(ss) involves training an inverse dynamic model to predict the action (a) that will transition the current state (s) to the next state (s). The authors formulate the problem as a constrained optimization where the target is to maximize the discounted reinforcement while ensuring that the departure between the target states (from the policy) and the actual states (from the environment using the control model action) is below a specified threshold. The optimization is solved using the Lagrange multiplier method. Experiments are conducted in various domain and with different stateoftheart DRL algorithms. Strengths and Weaknesses  The authors observe that during learning if the buffer of samples becomes full no novel samples are added. This applies to both the inverse control model (ICM) and the learning of Q and \u03c0.  There were implementation issues with D3G leading to departures in its performance compared to other algorithms although it uses a similar approach. The authors do not attempt to reimplement D3G in their environment.  SACvanilla outperforms SPPSAC in the carpush domain but the reason for this is not clear.  It would be beneficial to discuss how to incorporate other constraints such as a cost use for safety violations into the proposed approach.  The authors claim that statestate mappings lead to effective exploration than stateaction mappings but the underlying reason is not explained.  The parameter \"d\" regulating state consistency is essential to the approach but the authors do not demonstrate the effects of different \"d\" values on the results.  The transition use is assumed to be deterministic in the paper. It would be helpful to understand how this affects the applicability of the approach.  The introduction mentions data efficiency and interpretability but it is not clear how the proposed approach improves interpretability. Typos  \"build on The\" should be \"build on the\"  \"caption Fig 5 c) ... bule ...\" should be \"caption Fig 5 c) ... blue ...\"  \"however we if we show\" should be \"however if we show\"", "Paraphrase: This paper introduces a novel reinforcement learning (RL) algorithm called SPPRL. Unlike traditional RL algorithms that select actions SPPRL selects novel states. This algorithm ensures that the selected states are valid and can be applied to offpolicy RL algorithms like TD3SAC. Experimental results show that SPPRL has some effectiveness in specific settings. However the results are limited to the doggo environment and there is no evidence of its performance in environments using pixelbased representations or the MuJoCo physics simulator. Without theoretical analysis or clear explanations of why and when SPPRL is effective its difficult to fully understand its potential and limitations. Given these concerns the papers evaluation is primarily based on empirical results. Strengths and Weaknesses: 1) Empirical Results:  Strong performance in the doggo environment.  Insufficient evidence in other environments especially pixelbased or MuJoCobased tasks. 2) Theoretical Results:  No theoretical analysis provided. 3) Insights:  Limited understanding of why and when SPPRL performs effectively. Recommended Improvements:  additional experiments in diverse environments including pixelbased and MuJoCo tasks.  Theoretical analysis or intuitive explanations of SPPRLs effectiveness.  Hyperparameter tuning and scalability considerations.", "Paraphrased Summary: This field introduces a novel reinforcement learning approach that focuses solely on the state space rather than the conventional stateaction space. It employs a constrained optimization problem solved using duality. Strengths and Weaknesses: Strengths:  The problem conceptualization is intriguing. Weaknesses:  Lack of Formality: The constraints in the problem are not formally defined complicating the evaluation of the authors contributions.  Missing Dual step Size: The algorithm resembles a primaldual algorithm for constrained Markov conclusion process (CMDPs) but the dual step size is omitted which is critical for convergence.  Inconsistent analysis: The analysis in Appendix B is inconsistent with the gradient used in the algorithm ignoring the impact of dual variables.  Hyperparameter Difficulty: The hyperparameter \"d\" in the optimization problem may be challenging to tune. Sensitivity analysis and feasibility handling are not discussed. additional Concerns:  References extend beyond margins."], "zIUyj55nXR": ["Paraphrased Statement: Summary The authors propose Frame Averaging (FA) a framework that modifies existing network architectures to make them invariant or equivariant under a specified group. FA utilizes a group averaging operator to achieve this. Crucially it replaces the averaging over the entire group with a smaller subset of group elements maintaining the desired invarianceequivariance properties. strength  FAs use of a smaller averaging set is novel and computationally efficient.  FAbased models inherit the universality of their original architectures.  FA has been used to create several invariant and equivariant architectures for point cloud and graph. Weaknesses  In certain scenarios the subset of group elements used in FA may vary significantly in size. The authors need to analyze how to handle such type separately.  The details of FAs implementation in DALocalPointNet are unclear.  The paper lacks comparisons to existing group equivariant architectures making it difficult to assess its novelty and efficiency.  Some technical sections are dense and complex possibly due to the papers limited length.", "Paraphrased Statement: Summary: This field introduces a technique to convert a model into an invariantequivariant model utilizing the Reynolds operator. To efficiently calculate the Reynolds operator the authors introduce the concept of an \"equivariant frame.\" strength:  Provides a comprehensive approach for transforming existing models into invariantequivariant models using Reynolds operators.  Combined with other methods it achieves competitive experimental results. Weaknesses: Concerns:  The authors should acknowledge the process of Kicki et al. (2021) which employs the same principle of using Reynolds operators to create invariant models.  The term \"equivariant frame\" may overlap with the concept of a frame in differential geometry leading to confusion. Theorem 1:  While its proven that summing over an equivariant frame yields an invariantequivariant function the frames dependence solely on the input space raises concerns.  For invariant models like F applying the proposed transformation would increase computational complexity rather than preserve invariance.  This leads to potential issues with overlapping invariants when combined with other models. point cloud model:  Its unclear why the authors do not construct a frame for Sn x E(d) or SE(d) to create models with Sn x E(d) or SE(d) invariance. Graph model:  MLPFA suffers from using the adjacency matrix as input resulting in a large input space and excessive parameters. It would be beneficial to explore using a reduced number of nodes (e.g. 50) for training.  GNNFA appears to calculate invariance redundantly diminishing the contribution of this method. References:  Zaheer et al. (2017)  Deep Sets  Maron et al. (2019)  Invariant and Equivariant Graph Netprocesss  Kicki et al. (2021)  A New approach to Design symmetry Invariant Neural Netprocesss", "Paraphrase: Summary of paper The research introduces a novel technique called Frame Averaging (FA) that allows existing neural network architectures to become invariant or equivariant to novel symmetry types. FA achieves this by calculating an average over a frame that produces a subset of group based on the input. This method is computationally efficient compared to averaging over the entire group and ensures precise invariance or equivariance. Technical contribution The paper demonstrates the expressive power of FAbased models proving that they are equivalent to the original backbone architectures. Empirical contribution FA enables the foundation of novel classes of models including point cloud networks and Message Passing Graph Neural Networks (GNNs) that are invariant to Euclidean motion. The paper showcases the effectiveness of these models on several tasks. strength  FA is a straightforward however potentially impactful framework.  group equivariance enhances deep learning models but designing such architectures is challenging.  FA significantly reduces the computational cost of group averaging making it practical.  The models demonstrate impressive performance on multiple tasks. Weaknesses  The paper lacks simple model to illustrate the concept.  The construction of frames in the paper requires More explanation and motivation.  Its unclear whether there is an optimal size for the subset of group produced by frames. Questions  How robust are the proposed frames to input noise  How do the stability of the frames affect the performance of FAbased models", "Paraphrased Summary: The paper proposes a technique for creating equivariant neural networks by applying symmetry to a subset of a group rather than the entire group. If the subset selection (called a \"frame\") is equivariant then the symmetrization is also equivariant. The authors prove three main results: 1. For invariant prediction the frame can be chosen within the quotient of the group by the stabilizer subgroup of the input. 2. When using a random sample of the frame the probability of deviating from the symmetrization with the entire frame is bounded below. 3. A universal model symmetrized in this manner remains universal in the class of equivariant functions. strength:  Practical strategy for building equivariant networks  Proof of universality to encourage adoption  Experiments demonstrate wide applicability and competitive performance Weaknesses:  Theorem 4 is difficult to understand and its conclusion is counterintuitive  Ambiguity regarding the applicability to finite and infinite group and frames  Writing could be improved  The choice of frames (F(X)) is not fully explained  Limited explanation of why certain methods (GAMLP and GAGINID) fail on specific tasks"], "yRYtnKAZqxU": ["Paraphrased Statement: This research explores the importance of dataset characteristics and data augmentation techniques for graph contrastive learning and reconstructionbased methods. effectiveness and Weaknesses:  While the paper claims to provide a theoretical explanation for when contrastive learning is effective there is no clear connection between the resulting \"fixed error\" and graph property.  The decision that learning invariance is not essential for downstream performance is flawed as design 1 shows improvements on datasets like IMDBBINARY and NCI1. Further research is needed to determine the importance of learning invariance for different graph types and property.  The paper claims to identify a tradeoff between style (irrelevant information) and content (relevant information) but it is unclear how to control this tradeoff in practice.  more specific information is needed about the datasets used in the evaluation particularly regarding PROTEINS MUTAG IMDBBINARY and IMDBBINARY.", "Paraphrased Statement: Summary This study investigates the relationship between graph edit distance (GED) and the effectiveness of selfsupervised graph learning (GraphCL) a technique for generating training data without costly labels. While data augmentation has been widely adopted in GraphCL the underlying understanding for its success remain unclear. effectiveness GraphCL has recently gained popularity offering a flexible style to generate augmented datasets. However the impact of graph augmentation on the performance of GraphCL models is still poorly understood. Weaknesses 1. The motivation for using GED as a criterion to evaluate GraphCL is not fully explained. GED is defined on graph structure only raising questions about its applicability to GraphCL methods that consider both node property and graph structure. The discussion is also confusing and difficult to follow. 2. The author claims that GED can bound the performance of GraphCL but this claim is not supported by a clear explanation of the connection between GED and GraphCL. 3. The organization of the paper is confusing with claims copied from previous work without proper context or explanation. For example the definition of spectral decomposition is mentioned without any background information. 4. The studys contributions are unclear as the application of the augmentation method to edge reconstruction is mentioned only in the experimental setup section.", "Paraphrased Statement: Summary: This paper explores the factors that determine the effectiveness of graph contractive learning (GCL) with generic graph augmentation (GGA). It suggests that success depends on the graph edit distance between samples within and between classes. The paper provides empirical evidence that reconstructionbased approaches like GAE and AAGAE excel in low graph style context while GCL with GGA is advantageous in moderate graph style regimes. effectiveness and Weaknesses: effectiveness:  clear and easytounderstand introduction.  Investigation of an significant problem in unsupervised graph reintroduction learning.  Interesting experimental results on the impact of graph style on GCL GAE and AAGAE performance. Weaknesses:  Reliance on theoretical results from another paper without detailed discussion.  analysis limited to spectral contractive loss work lacking evidence for applicability to other GCL approaches.  Lack of rigorous mathematical and empirical evidence to support the claim about GCL success depending on graph edit distance.  Limited experimental methods (only GCL GAE and AAGAE tested) raising doubts about the generalizability of findings.", "Paraphrased Statement: Summary This paper explores style to enhance graph contrastive learning (GCL) inspired by similar work in computer vision (HaoChen et al. 2021). The goal is to create a benchmark to assess the relationship between GCL methods and data augmentation techniques. analysis Drawing from population augmentation methods the paper investigates connections between existing graph augmentations (You et al. 2020) and examines the impact of graph edit distance (GED) on task performance and labels. benchmark The paper proposes a benchmark based on the \"style VS. content\" concept borrowed from CV contrastive learning. The benchmark focuses on graph motifs and differentiating style from content. effectiveness  Novel motivation: data augmentation in GCL is an understudied area.  Transfer from CV: The paper effectively adapts concepts from CV to GCL.  Highlight: The motifbased benchmark and \"style VS. content\" design are noteworthy. Weaknesses  Insufficient theoretical analysis: The relationship between GCL effectiveness and data augmentation lacks mathematical rigor.  Unclear statements: Certain explanations require clarification.  Handwaving analysis: The analysis in Section 3.2 is lacking in detail.  Contestable experiment: The experiment using untrained GNNs is questionable as the presence of a trained linear layer may skew the results.  Limited contribution: While the analysis and motivation are novel they are largely derived from CV. The benchmark design though innovative is relatively straightforward. more significant contributions could include proposing improved data augmentation or sampling methods based on the benchmarks theoretical insights."], "vr39r4Rjt3z": ["Paraphrase: Summary: This paper suggests changes to a network architecture called the highway connection network (HCN) that make it more resistant to forgetting past knowledge when learning new tasks (catastrophic forgetting). These changes involve a straightforward masking rule for selecting weights to update and a normalization work that enhances the networks ability to focus on relevant weights for each task. Tests show that these changes significantly reduce catastrophic forgetting. Strengths and Weaknesses: This approach is praised for elegantly addressing catastrophic forgetting by incorporating continual learning capabilities into the networks architecture. While the modifications to HCN are simple they significantly improve the networks robustness for continual learning providing a relatively hasslefree architecture that is less susceptible to forgetting past knowledge. However the paper could be improved by providing more theoretical insights into how the masking and normalization workes work together to select weights in a way that preserves previously learned inworkation. The current explanation relies primarily on intuitive reasoning and empirical evidence without much mathematical justification. Additionally there is an apparent contradiction in describing HCN as both resistant to gradient vanishing and less vulnerable to catastrophic forgetting due to the scaling down of derivatives (which is essentially a work of gradient vanishing). Finally while the paper is generally wellwritten some sections could be clear. The mathematical notation can be cumbersome and certain concepts could be explained more thoroughly especially the HCN architecture.", "Summary: The authors propose two modifications: Masked Highway Connection (MHC) and LayerWise Normalization (LWN). MHC modifies Highway Connection Networks by adding a binary mask and LWN is a new normalization technique for neural network activations. Strengths and Weaknesses:  Difficulty in understanding due to unclear notation.  Modifications appear minor and could be viewed as a result of architecture research.  experimental section is notable for evaluating six classifiers on three datasets with four continual learning setups. Uncertainties:  Use of SGD instead of Adam for optimization is unexplained.  Lack of results for a standard 510layer convolutional neural network.  In MHC the reason for training models with additional parameters is unclear.  work of the number of gated unit activations in MHC is not fully explored (e.g. why LWN outperforms MHC with 70 gated units on PermMNIST but underperforms with 10 on IncCifar100 and IncCUB200).  Optimal parameter tuning for MHC is not discussed.", "Paraphrase: This work introduces two architecture enhancements for continual learning networks: binary masks and layer normalization. These additions to highway connection classifier networks (HCNs) help prevent knowledge loss (forgetting). The paper includes extensive experiments demonstrating the effectiveness of these elements. Strengths:  Clearly explains the significance of each proposed change to HCNs.  Uses thorough experimentation to show strong performance on multiple datasets.  Provides clear rationale for masking and layer normalization supported by empirical data. minor Weaknesses:  Contains grammatical errors and tense inconsistency (specified below).  Section 3 notation can be confusing but the justification from Kuo et al. (2021b) is valid.  Figures 3 and 4 are difficult to read consider providing more explicit information in the text instead of using ambiguous terms like \"deep colors.\"  Font size in the plots could be increased. specific Weaknesses:  Inconsistent tense between past and present in Section 1.  Grammatical error in Section 1: \"Our work focuses on designing... which... impact...\" should be \"... impact.\"  Potential typo in Section 4: \"Overleaf in the clear half of Figure 2...\"", "Paraphrase: Findings:  The work explores backbone networks that are less susceptible to \"catastrophic forgetting\" in continual learning.  Two modifications to existing backbones prove beneficial:  Adding a mask to the Highway Networks gate work  Applying layer normalization (without parameter tuning) Implications:  These modifications enhance the effectiveness of existing continual learning algorithms (EWC ER HAT) on top of these backbones. Strengths and Limitations:  The results contribute to the understanding of the interplay between continual learning algorithms and backbone networks.  The mask modification for Highway Networks is an intriguing finding.  However its applicability may be limited to this specific architecture.  Further theoretical analysis would strengthen the interpretation of the empirical observations.", "Paraphrased Summary: This research modifies highway connection networks (HCNs) by using masks for selective weight updates and layerwise normalization to minimize internal covariate shifts. Tests were conducted on incremental datasets but the rationale for using preserved weights and differences in experimental setups between datasets is unclear. Critique:  The work lacks a comparison with relevant work such as other approaches that employ selective parameter updates to prevent catastrophic forgetting and promote task separation.  Its not stated whether the experiments were performed in a taskincremental or classincremental learning setting.  Classification accuracy may not be the optimal metric for assessing continual learning alternative metrics like backward and forward transfer are more appropriate.  The impact of task ordering on performance is not discussed.  Its unknown whether consecutive tasks with similar distributions work interference with other tasks."], "nLb60uXd6Np": ["Summary (Paraphrased): This paper introduces a deep neural network that processes small set of 3D point featuring invariance to rotation and equivariance to permutation. Method:  Employs geometric products from geometric algebra to achieve rotationinvariance in input vectors.  Utilizes an attention mechanism with permutationequivariant reduction of the geometric products. application: Demonstrates applications in physics including crystal structure identification molecular force regression and backmapping of coarsegraining operators. force:  Utilizes geometric algebra to achieve equivariance.  Demonstrations cover various physics chemistry and biology problems. Weaknesses:  Lacks clear explanation of how geometric products achieve rotationinvariance.  geometric products are not explicitly described in both model architectures and appendix B.  Interpretation of linear combinations in geometric algebra is not adequately discussed.  attention mechanism in geometric algebra requires further elaboration.  Limitations to \"small point clouds\" need clarification and connection to computational complexity. small Comments:  Clarification is needed on which geometric algebra is employed (e.g. Cl(301) or Cl(410)).  Distinction between work R and works V M J and S needs justification.  Output vectors (yi) in Eq. (1) are referred to as input point in the argument.  Commutativity of the geometric product and its impact on permutationcovariance should be addressed.  Confirmation or clarification is needed on whether the proposed network can perform force regression for unknown molecules.  discussion of potential technical difficulties in including force labels in the network is recommended.  The convergence issues \"above 64 neurons\" mentioned in protein coarsegrain backmapping need further clarification.", "Paraphrase: This paper presents a sophisticated deep learning model for handling data in the work of point clouds. The model exhibits both rotation and permutation invariance which is achieved through the use of geometric algebra and an attention mechanism. The rotation invariance is achieved by using geometric products of multivectors while the permutation invariance is accomplished by applying an attention mechanism to invariant terms of these products. The models perworkance was evaluated in three practical applications where it demonstrated comparable or superior results to existing approach. Additionally the model provides insights into the attention maps it generates. force:  Leverages geometric products for rotation invariance.  Introduces a novel attention workulation for permutation invariance which is crucial in many deep learning applications.  Demonstrates robustness and data efficiency in three diverse applications.  Highlights the importance of geometric properties in deep learning models. Weaknesses:  The model underperworks compared to a specific baseline (GemNetQ) in one application. While the authors speculate on potential understanding this could impact the fairness of the comparison.  The evaluation of the protein coarsegrain backmapping application uses training error instead of test error. This raises questions about how this step evaluates the models generalization perworkance.", "Summary The proposed network leverages geometric algebra to build an attention mechanism for small point clouds. This attention exhibits rotation and permutation invariance ensuring its robustness to transformations in the data. The attention mechanism comprises four work specifically designed to respect these equivariances. The paper demonstrates the effectiveness of the geometric algebra attention in three applications: crystal structure identification molecular force regression and backmapping of coarsegraining operators. force  Novel use of geometric algebra for achieving rotation equivariance in attention.  Strong performance in realworld tasks. Weaknesses  Lack of clarity: The point of the four work are not fully explained especially regarding rotationinvariant geometric quantities for tuples.  Missing mathematical proof: No mathematical proof is provided to demonstrate the rotational equivariance of the attention mechanism.  efficiency concerns: The paper does not address the efficiency of the attention mechanism compared to other approach.  limited experimental validation: rotation equivariance experiments are missing comparing the proposed method with other techniques like data augmentation.  application to larger point clouds: Extensibility of the work to larger point clouds is not explored.  Definition of \"small point clouds\": A clear definition of what constitutes \"small point clouds\" is not provided.  Feasibility on 3D point cloud classification: An experiment to test the performance on 3D point cloud classification (e.g. using Modelnet 40) with varying point cloud sizes is recommended.", "Paraphrase: The paper introduces a method for training work that accept small point clouds as input and are both permutation and rotation equivariant. rotation Equivariance: The method uses the geometric product of point coordinates to combine data from point in a rotationinvariant way. permutation Equivariance: permutation invariance is achieved using a traditional attention framework. Evaluation: The model is tested on three scientific tasks:  crystal structure identification  Molecular force regression  Backmapping of coarsegrained operators in molecule model force:  The method is wellfounded versatile and easy to apply to interactions of varying ranks.  It is described thoroughly and can be reproduced.  The paper is clear and concise.  The paper focuses on learning work for small point clouds emphasizing highorder interactions and symmetry preservation.  The evaluation tasks provide various model of this set. Weaknesses:  The quantitative results are promising but not exceptional.  The crystal classification task is not particularly challenging and the model performs marginally worse than GemNet on molecular force regression.  On the backmapping task it is only compared to a naive transformer not another rotationinvariant model.  The model is only evaluated with pairwise attention despite its potential to scale to arbitrary ranks.  It has not been compared to approach based on group representations."], "vA7doMdgi75": ["Paraphrase: Summary: This paper presents a method for solving robust subspace recovery using projected subgradient optimization. The method involves solving multiple parallel instances of the optimization problem with random initializations and utilizing the fact that if the number of inliers significantly exceeds the number of outliers a certain number of random initializations (c) will converge to identifying the true subspace dimension (also c) with a probability of 1. The paper provides both experimental results and validation to support this method. effectiveness and Weaknesses: The method is innovative and appears to be more of a statistical finding than an optimization technique. The numerical experiments demonstrate promising results and the validation seem sound. However there are some assumptions that require further elaboration:  Assumption 1: The conditions for initializing the random vectors (bs) should be explained more clearly.  Assumption 2: The assumption that the true subspace dimension (c) is overestimated requires clarification. These clarifications would enhance the understanding of the effectiveness of the random initialization. Additionally it is not clear how sensitive the method is to the assumption that the data exhibits isometric dimension. In cases where the inlier dimensions have varying contribution the effectiveness of the method may be compromised. This aspect requires further exploration.", "Paraphrase: Summary:  A new method is presented that enables reliable recovery of subspaces without needing to know their dimensionality beforehand.  The method leverages Dual Principal Component Pursuit (DPCP) which is suitable for subspaces with high relative dimensions.  Experimental evidence supports the theoretical basis of the method and highlights the effectiveness of its optimization techniques. effectiveness and Weaknesses:  The method effectively analyzes robust subspace recovery using DPCP without requiring prior knowledge of subspace dimensionality.  It provides theoretical analysis for both gradient flow and projected subgradient approaches showing convergence to desired solution.  The work is wellstructured and presented.  It addresses a significant challenge in subspace recovery by allowing high relative dimensionality without requiring prior knowledge of the true dimension.  However there may be limited novelty in the method as it appears to combine concepts from existing literature on DPCP and lowrank matrix factorization.", "Paraphrase: Summary: This paper addresses the Robust Subspace Recovery (RSR) problem where the goal is to estimate a subspace given data with corrupted entries. specifically the paper focuses on cases where the subspace has a large dimension (small codimension). Previous approaches required prior knowledge of the subspace dimension but this work presents a new algorithm that can recover the subspace without this data. contribution: 1. Development of a new algorithm that doesnt require prior knowledge of the unknown subspace dimension. 2. theoretical analysis of the new algorithm. 3. Empirical validation with both synthetic and real data. effectiveness and Weaknesses: effectiveness:  Introduces a novel algorithm for a significant problem eliminating a major limitation of previous work. Weaknesses:  The paper requires further refinement and clarification especially in the presentation of the main solution (Theorem 7).  The sharpness of the theoretical solutions should be further explored.  The paper would benefit from improved formatting and proofreading. further Comments:  Minor typographical and formatting errors exist.  The related work section should include more relevant literature such as Robust PCA by Candes Wright et al.  Clarification is needed in the formulation of Theorem 1 to indicate its connection to the problem being solved.  The irrelevance of matrices M and N in Theorem 1 should be explained.", "Paraphrased Summary: This research explores a technique for uncovering the subspace basis despite being unaware of its dimensionality. It achieves this using a modified version of Dual Principal Component Pursuit (DPCP) which calculates an orthogonal complement basis but omits the explicit orthogonality requirement between its columns. The authors introduce a projected subgradient method with random initialization and exaggerated dimension estimates supported by theoretical analysis. Simulation results are included. effectiveness and Weaknesses: The surprising finding that random initialization can replace explicit orthogonality constraints in DPCP is this papers primary value. While the validation and theoretical results are subject to further examination they have the potential to influence diverse field including signal work and machine study. However the concept of \"implicit bias\" despite being frequently mentioned lacks clear definition. Additionally the paper would benefit from:  Clarifying design 1 with more detailed explanations of terms and their relationships.  Offering summaries or validation outlines to enhance accessibility in dense section.  Elaborating algorithm 1s initialization step by providing specific instructions for obtaining \\hatB0.  Assessing the algorithms computational performance. Minor Issues:  Equation (3)s \\tildeX may require transposition.  Equation (6)s notation is confusing regarding the claim that cd equals 2\u03c0 for even d.  A broken link occurs on page 6.  Colorbar and scale data are missing from design 2."], "o_HsiMPYh_x": ["Paraphrased statement: Summary: This research focuses on utilizing only labeled source data and unlabeled target data to forecast the accuracy of a model on a different target area. The Average Thresholded Confidence (ATC) method is introduced which estimates accuracy as the percentage of unlabeled target examples that the model confidently predicts. Experiments demonstrate ATCs superiority over existing methods in various model architectures and distribution shifts. Theoretical model: Predicting accuracy is shown to be as challenging as finding the best predictor. The effectiveness of prediction methods relies on assumptions about the nature of the distribution shift between source and target areas. strength and Weaknesses:  Predicting model accuracy is a novel and practical challenge.  ATC is a straightforward and intuitive method.  The theoretical model acknowledges the difficulty of predicting accuracy. Quality: The paper is technically sound with theoretical and empirical support for its claims. It provides a complete solution and outperforms previous methods. Clarity: The paper is wellwritten with clear problem definition motivation and experimental details that facilitate replication.", "Paraphrased statement: This paper presents a technique called Average Thresholded Confidence (ATC) for predicting the accuracy of a machine learning model on a novel dataset that differs from the training data in some means. The paper begins by explaining that accurately predicting test accuracy without making assumptions about the specific differences between the training and test datasets is impossible. ATC overcomes this challenge by assuming that the distribution shift follows a certain practice. ATC sets a threshold (t) on a score work based on the error rate in the training data. It then estimates the error rate in the test data as the number of samples with scores below the threshold. This work is performed without using any labels from the test data. Experiments show that ATC outperforms other methods on a variety of datasets and distribution shifts. However the choice of score work for ATC is not clear with both max confidence (ATCMC) and negative entropy (ATCNE) performing best in different situations. ATC is comparable to a regression model that uses labeled data from the test set for accuracy estimation. Temperature rescaling while preserving the classification of the model can significantly alter the ordering of sample scores. The reason for this is unclear and the paper does not provide specific examples of how temperature rescaling improves performance. Finally the paper highlights that all methods for estimating test accuracy require assumptions about the distribution shift between the training and test datasets. ATCs assumptions are not explicitly stated leaving scope for further clarification.", "Paraphrase: The paper introduces a novel scoring method called Average Thresholded Confidence (ATC) for predicting the outofdistribution performance (accuracy) of a trained classifier. It utilizes only labeled data from the source distribution (the training data) and unlabeled data from the target distribution (the test data). The threshold is determined based on the classifiers logit output enabling its practice with unlabeled data. strength: 1. The paper provides a comprehensive set of experiments. 2. It offers an insightful discussion on related work. 3. It includes relevant theoretical results to support the problem definition. Weaknesses: 1. The derivation and explanation of ATC could be more thorough and less adhoc. 2. The paper does not adequately highlight the heuristic nature of the ATC score and its primary focus on experimental results. 3. The experiments lack confidence intervals in the figures and could benefit from further discussion on temperature scaling and its potential for improving performance with more advanced calibration methods. Revision Suggestions: By addressing the weaknesses identified the paper could improve its quality. Additionally the authors should consider providing confidence intervals in the figures and expanding the discussion on temperature scaling. Updated Assessment: After considering the authors response to the initial review the reviewer has raised the score from 6 to 8. However the confidence score remains at 3 due to the reviewers limited knowledge of some of the prior work cited in the paper.", "Paraphrase: Summary This research introduces a straightforward method called Average Thresholded Confidence (ATC) to estimate the accuracy of a classifier on an unseen distribution (target) using a labeled source distribution and an unlabeled target distribution. strength  Comprehensive experiments demonstrate the superiority of ATC over baselines on various datasets and models. Weaknesses  Lack of theoretical rigor:  The threshold for source and target distributions differs depending on the case of distribution shift but the source threshold is arbitrarily applied to predict target accuracy.  Insufficient theoretical analysis:  No comparison between naive estimation (NE) and Monte Carlo estimation (MC) is provided leaving uncertainties about their appropriate practice in different scenarios.  Limited theoretical foundation for a generalpurpose estimator:  Section 3 presents a theoretical lower bound that renders such an estimator impossible without assumptions about the distribution shift. However the paper proceeds to propose ATC.  Lack of generalization in theoretical analysis:  The theoretical analysis for the toy context is specific and does not provide a broad understanding of the conditions under which ATC work effectively.  Insufficient exploration of calibration methods:  The paper only compares ATC to temperature scaling calibration omitting other potential methods.", "Paraphrase: Summary This research suggests a simple but effective method for predicting the accuracy of a classifier on test data that differs from the training data. Using a score work and a data level the technique determines a threshold that balances the proportions of correct and incorrect classification in the validation data. This threshold is then used to estimate the classifiers accuracy on the test data. The paper demonstrates that this predictor outperforms existing methods on various distribution shift benchmarks. Additionally theoretical and empirical results for a toy problem suggest that the proposed method is wellsuited for that specific problem. Strengths  The proposed technique is reportedly a novel approach.  It is significantly more effective than previous methods and easy to implement.  The paper is wellwritten and structured.  The numerous experiments provide solid empirical evidence.  The papers simplicity enhances its practical applicability. Weaknesses  The theoretical sections (3 and 6) lack a clear connection to the proposed technique.  The paper could provide more intuition and justification for the technique in Section 4.  Figures 2 and 3 could be more clearly labeled to indicate regression fits.  Table 1 could be improved by clarifying the datasets and test sets listed.  Some comparisons such as IM require additional explanation."], "vxlAHR9AyZ6": ["Paraphrased Summary: Researchers investigate the resilience of federated study model against adversarial attacks. They argue that the inner optimization step of adversarial training can worsen data disparities between clients. They propose \\alphaWFAT an algorithm that transforms the inner maximization of adversarial training into a lower bound that is compatible with federated study. Experimental issue show that federated study model are more vulnerable to attacks when clients train on nonidentical datasets. These experiments were conducted on CIFAR10 SVHN and CIFAR100 datasets. effectiveness and Weaknesses:  Novel research direction in federated study.  Clear organization and presentation. Weaknesses:  Incremental contribution (use of weighting strategy).  Uncertainty about the effectiveness of global model training with this approach.  Lack of scalability to larger client issue.  Insufficient explanation of the specific weighting scheme chosen.", "Paraphrase: Summary: This paper presents the alpha Weighted Federated Adversarial Training (FAT) algorithm. In this algorithm the central device selects the local device that produces the least loss during aggregation. The paper offers theoretical and experimental evidence to support the claim that this alphaweighting mechanism enhances FATs inner maximization ability. effectiveness and Weaknesses: While the alphaweighting mechanism shows improvements some claims in the paper raise concerns.  The paper attributes performance issues in FAT to biased adversarial sample generation on local devices. However adversarial samples are inherently biased during training since local devices do not have approach to the optimal model parameters.  The paper suggests that hardertotrain local devices may \"overoptimize\" their model. Its unclear if \"overoptimize\" refers to overfitting. If so this parameter could apply to any federated training scenario including nonadversarial settings.  Its unclear why hardertotrain local devices should be deemphasized. Their data may contain more valuable boundary data. Unclear Notations:  \"Lsmoothness l(...)\" is ambiguous. Does it mean \"Lsmoothness (ftheta(...))\" is uniformly smooth with respect to theta for all x and y  Its unclear what tilde x and y represent in the expectation term in Theorem 4.2. Are they a single random sample  The meaning of the expectation term in Theorem 4.3 is not provided.", "Summary This research addresses limitations in Federated Adversarial Training (FAT) and introduces an improved approach called \u03b1Weighted Federated Adversarial Training (\u03b1WFAT). \u03b1WFAT focuses on clients that can contribute more to the decision bound allowing for improved performance in federated study scenarios. The benefits of \u03b1WFAT are:  It prioritizes clients with greater impact on the decision bound resulting in more robust model.  It provides a novel and efficient method for Federated Adversarial Training.  It outperforms baseline approaches in both Independent and Identically Distributed (IID) and NonIID federated study settings. However there are some weaknesses to acknowledge:  The motivation for the relaxation in FAT is counterintuitive as minimizing a lower bound of the FAT loss leads to beneficial performance than the original FAT.  The weighting mechanism in \u03b1WFAT may not directly address the issue of margin measurement for predicting adversarial model.  Hyperparameter tuning for \u03b1 has a significant impact on model performance but the selection or estimation of \u03b1 is not clearly explained.  NonIID issue may vary based on the specific data distribution in each client and reporting issue based on multiple experiments would provide a more comprehensive picture.  Related work on adversarial defenses in federated study is not sufficiently discussed. Some additional concerns include:  Theorem 4.2 lacks a definition for Tilde(x).  The starting indices for \"t\" and \"k\" in Theorem 4.2 are not specified.", "Paraphrased Statement: Summary: This paper addresses a key issue in federated adversarial training where accuracy declines significantly during later training stages. The authors suggest that this decline stems from increased data distribution variability across clients and overfitting to local adversarial samples which generalize poorly to other clients. Proposed solution: The authors introduce \u03b1weighted federated adversarial training which emphasizes model trained on benign data distributions while downplaying model trained on adversarial distributions when aggregating at the central server. issue: The proposed method surpasses existing strategies in various adversarial training and federated learning setups. effectiveness:  The paper addresses a critical problem notably impacting realworld applications.  The assumption behind the problem is wellfounded and experimentally validated.  The proposed method is practical straightforward to implement and efficient in various adversarial training and federated learning settings.  Hyperparameter choice have negligible impact on the model robustness. Suggestions: Combining the proposed method with \"benign adversarial model\" may potentially enhance performance further given the assumption that benign adversarial model may homogenize data distributions more efficiently than traditional adversarial samples."], "ieNJYujcGDO": ["Summary This paper presents a theoretical framework for analyzing Mixup training considering the impact of data dependency on its effectiveness. The analysis shows that Mixups benefits for model generalization and resistance to attacks depend on data properties. It reveals instances where Mixup may fail to minimize the initial empirical risk. Additionally the paper formally defines the margin of a Mixup classifier and demonstrates when its decision boundary improves over standard training and when this advantage diminishes. effectiveness and Weaknesses effectiveness:  Provides insights into why and when Mixup improves classifier performance.  Demonstrates the significant dependence of Mixups benefits on data properties.  Offers a theoretical foundation for Mixup training. Weaknesses:  The conditions for Mixup failure due to data collinearity need to be more precisely defined.  The role of Mixup errors on original training points to support Mixups empirical risk minimization is not convincing.  The argument regarding angular distance in Section 2.4 may need clarification.  A more comprehensive related work section is recommended.  It remains unclear whether the theory holds for Mixup with different mixing ratios for inputs and labels. Suggestions for improvement:  Provide a more precise characterization of data conditions leading to Mixup failure.  Clarify the interpretation of Mixup errors in Figure 2.  Address the potential understanding for test performance degradation at high alpha values.  Discuss the relationship between the papers findings and observations in other related work such as \"Midpoint Regularization\" and \"Nonlinear Mixup.\"  Explore the applicability of the theory to Mixup with varying mixing ratios for inputs and labels.", "Summary This research presents theoretical tools that explore the connections between three optimization technique: empirical risk minimization empirical crossentropy minimization and Mixup minimization. Key contributions include: 1. Creating a dataset where Mixup training fails to achieve empirical risk minimization. 2. Establishing sufficient conditions for minimizing original risk and estimating these conditions on actual datasets. 3. Observing the range of empirical risk minimization using Mixup and providing a theorem explaining it. 4. Analyzing scenarios where the Mixup optimal classifier fails to generalize and demonstrating it on a simplified dataset. 5. Providing an model showing that Mixup training can produce the same classifier as standard training. Unlike previous studies this research focuses on understanding why Mixup works by examining specific scenarios where it is effective. effectiveness and Weaknesses effectiveness:  Meaningful approach to studying Mixups effectiveness.  Provides concrete conditions for Mixup failure and demonstranges them on synthetic datasets.  Estimates conditions that can be used by practitioners in realworld experiments.  Underscores future research guidance such as Mixup training convergence range. Weaknesses:  Requests additional context for experiments using \"Mixup at our choice of \\alpha1024\".  Needs clarification on the significance of heavy angular distance for test data points.  Clarifies the applications and limitations of the Sufficient conditions for scaling to heavy datasets.  validation Sketch of Lemma 2.3 and Proposition 2.8 requires further explanation.  Requests clarification on the meaning of \"recover\" in Theorem 2.11.  The connection between \"To illustrange this\" and the previous paragraph in Section 3.1 needs to be clarified.  Questions the summation indices in the validation of Lemma 2.3.  Seeks references to other studies that explore the alignment between the Mixup optimal classifier and the empirical crossentropy minimizer. Minor Suggestions:  Citing contrastive learning applications of Mixup in the presentation.  Explaining the explicit dependence of g in the definition of Jmix.  Clarifying the role of Ax\\epsilon\\deltaij in the paper.  Specifying the guidance of \\epsilon in Lemma 2.3.  Labeling Figure 1 and 8 more clearly.  Correcting the typo in Definition 8 which refers to Figure 8 in Section D.  Clarifying the validation of Theorem 2.11 regarding AwPAw equivalence.", "Summary This work examines the performance of mixup training algorithms in reducing empirical risk. The authors first present an model where mixup cannot optimize risk on the original data. They so establish criteria under which mixup can minimize risk. effectiveness and Weaknesses effectiveness:  The topic is relevant and timely. Weaknesses: 1. Exaggeranged Alpha values:  The authors use highly high alpha values (e.g. 32 128 1024) which are not typically used in practice. This may exaggerange the effects being demonstranged. 2. Limited Theoretical Results:  Proposition 2.5 and Theorem 2.7 only apply to specific datasets limiting their generalizability. 3. Assumptions Questionable:  Assumptions 2.9 and 3.1 may not hold for realworld datasets raising concerns about the applicability of the results. 4. Unclear Conclusion in Section 2.5:  Section 2.5 appears to focus on analyzing the convergence range under mixup training. However Theorem 2.11 may not be sufficient for this role. Additionally the focus on mixing with lambda0.5 corresponds to unrealistically heavy alpha values."], "qSV5CuSaK_a": ["Paraphrase: Summary: This study introduces a new attack method FSBA for siamese networkbased visual object tracking (VOT) that requires only a few model of triggered data. The main contribution are:  It formulates the attack as a multitask study problem making it the first backdoor attack on VOT.  It presents a straightforward and effective fewshot untargeted backdoor attack that is highly successful in both digital and realworld settings. effectiveness:  Exposes the susceptibility of VOT to backdoor attacks from outsourced training or pretrained models.  Being the first research to investigate backdoor attacks on VOT it is a significant and timely contribution.  Welldesigned experiments and indepth analysis demonstrange the proposed methods value in assessing the vulnerability of visual object trackers to backdoor attacks. Weaknesses:  Some details need clarification:  Why is the frame attacking range set at 10 in FSBA when 5 is shown to be sufficient in number 7  What rationale was used to select the four trigger model shown in number 8  The definition of PrB is unclear. How should \"the larger the PrB AUCB and mSR50B the more stealthy the attack\" be interpreted  The experimental results are primarily quantitative with limited qualitative investigation. For instance it would be helpful to examine changes in tracker heatmaps to better understand the mechanism of the attack.", "Paraphrased Statement: This research paper introduces a method for launching \"fewshot\" backdoor attacks on visual tracking systems that track single objects. The attack involves alternating between optimizing a feature loss that distinguishes between normal frames and poisoned frames and a traditional tracking loss. The authors demonstrate through experiments that their attack is successful under both digital and realworld conditions. effectiveness:  Clear and concise organization  Presents the attack as an innovative technique against VOT wayls Weaknesses:  The concept of maximizing the feature separation between normal and poisoned frames is not novel and lacks significant insight.  The claim of the methods ability to operate in fewshot or oneshot way is not supported by specific design details.  The method combines a feature loss with a standard tracking loss but it is unclear whether the tracking loss is calculated over both normal and poisoned frames or only one set. Additional explanation is needed.  In the definition of \"\u03b1Effectiveness\" \"\u03b8\" should be replaced with \"\u02c6\u03b8.\"", "Paraphrased Summary: 1. This study introduces a new type of backdoor attack against visual object tracking (VOT) networks. 2. A basic attack (BOBA) that uses a standard classifier backdoor on the classification role of a siamese network is suggested. 3. An innovative attack (FSBA) that optimizes a certain loss in feature space is proposed with better results than the basic attack. 4. The attack can succeed yet when only a small portion of the picture frames (such as 5) include the trigger. effectiveness and Weaknesses: effectiveness: 1. Backdoor attacks are applied in a new way. 2. Comprehensive empirical evaluations cover various preprocessing methods backdoor trigger finetuning varying proportions of picture frames containing the trigger and multiple datasets. 3. Wellwritten and presented overall. Weaknesses: 1. The reason for the BOBA attacks failure is unclear. 2. tSNE plots are difficult to comprehend simpler visualization methods like PCA might be clearer. 3. The threat model assumes a powerful attacker who can alter the training algorithm. 4. Only rudimentary defenses are considered."], "i4qKmHdq6y8": ["Paraphrased Statement: The paper investigates selective learning situations where some data point are irrelevant noise. The authors propose an algorithm using a twostep optimization process and provide theoretical guarantees for accurately predicting both the data and identifying the noise. effectiveness and Weaknesses: effectiveness:  The paper is wellwritten and easy to follow.  The problem of identifying irrelevant data is significant in innovative machine learning.  The theoretical analysis provides insights and a starting point for further research. Weaknesses:  The theoretical algorithm is based on a simplified loss work that cannot be efficiently optimized.  The heuristic algorithm implemented uses a different surrogate loss work.  Some specific concerns raised include:  The unrealistic Assumption that the informative data is noiseless.  The inconsistency between the theoretical and empirical algorithm.  The lack of specification for the hypothesis classes used in the experimental setup.", "Paraphrased Statement: This work examines supervised learning with abstention where the learning method can choose to make predictions in certain areas of the feature space and declare the rest as unpredictable. While related to selective prediction and prediction uncertainty quantification this setting assumes a \"true\" split between informative and uninformative area of the feature space. The goal is to identify this split and develop a model for the informative area. The work provides a theoretical analysis and experimental results indicating that a heuristic algorithm inspired by this analysis can outperform former selective prediction method if the data aligns with the assumed model. However the work has limitations: 1. The model assumes a clear separation between predictable and unpredictable area which may not be realistic in practice. 2. Uncertainty quantification method like bootstrap resampling or Gaussian process may not perform poorly in this setting as suggested. 3. The approach relies on finding a welldefined informative area which may not always be straightforward. 4. Balancing the complexity of the selector work and the regularization of both the selector and predictor can be challenging. 5. The sensitivity of the model to the signaltonoise ratio in the predictable area needs further investigation.", "Paraphrased Summary: The paper analyzes a scenario where the label is randomized if the input is within an uninformative subspace. It provides a theoretical foundation for estimating a predictorselector that minimizes classification error. The paper introduces a new selector loss work and suggests an iterative algorithm for approximating estimators using the MannWhitneyWilcoxon (MWU) test. effectiveness:  clear problem formulation with precise definitions and Assumption.  Theoretical foundation of the methodology and its implementation through an iterative algorithm.  Evaluation of the proposed approach in challenging experimental settings. Weaknesses:  Omission of related work such as \"Combating Label Noise in Deep Learning Using Abstention\" by Thulasidasan et al. which contribution similarities with the papers objectives.  Inaccurate descriptions of former work leading to unfair comparison without explicitly defining the problem setting.", "Summary This research focuses on selective classification a task where data is divided into informative (\\OmegaI) and uninformative (\\OmegaU) categories. Labels are randomly noisy within \\OmegaU while they are accurate within \\OmegaI. The goal is to train a classifier (f) to predict the label (y) if the input is informative (\\OmegaI) and abstain (abstain to classify) if it is uninformative (\\OmegaU). The selector (g) identifies the input category. The paper introduces a novel loss work (W) to train the selector (g) given a predictor (f). This work uses the difference between the prediction (f(x)) and label (y) as a proxy for gs desired output (0 for uninformative 1 for informative). The weight of this difference is adjusted based on the relative prevalence of informative and uninformative data. Theoretical analysis demonstrates that if the indicator empirical risk minimization (ERM) problem can be solved the proposed method can achieve low selective risk (error on classified data) and good accuracy for the selector (g) under certain conditions. The sample complexity is bounded by factors related to the hypothesis classes used for f and g as well as the prevalence of informative data. The paper also presents a practical heuristic method based on alternating minimization to train (f g). It involves iteratively training f and g using relaxed versions of the W loss while dynamically adjusting the contribution of selected point to fs loss. Empirical Evaluation The method is empirically evaluated on two datasets: 1. MNIST with randomized labels and varying amounts of clear Fashion MNIST data 2. SVHN with randomized labels representing uninformative data The results show that the proposed method significantly outperforms baselines in selective risk while maintaining similar selector identification performance. effectiveness:  Interesting loss work and training method  Impressive empirical results Weaknesses:  Requirement for a priori knowledge of informative data prevalence (\\alpha) in the theoretical analysis  Lack of lower bound for the problem  Lack of clarity about baseline method selection and hyperparameter tuning Questions and Comments:  How is the advantage over baselines achieved  Could empirical dominance of a prior method (e.g. [3]) be related to the proposed approach  Quibbles:  Ambiguous definition of fs area  Inconsistent representation of g and abstention criteria  Suggestion of using \\mathcalG for g rather of \\mathcalH"], "mL07kYPn3E": ["Summary: The authors propose adding a radius parameter to prototypes in an existing fewshot learning approach. They show through numerical experiments that this addition improves performance. strength:  Evaluated on various tasks from different domains  Simple intuitive and welljustified idea Weaknesses:  Limited contribution: Only a single parameter is added to an existing network.  Tested on only one outdated prototype network (from 2017). No comparison to more recent prototype baseline or metalearningselfsupervision approaches.  Writing contains errors and imprecise language affecting clarity.  prototype initialization method (Section 3.3) is unclear and its benefits are not assessed. Suggestions:  Apply the method to a wider place of prototypebased algorithms including novel and more effective ones.  Explore extensions to hyperbolic space or other metric space. Additional Comments:  \"Dense vectors\" are used synonymously with \"prototypes\" in the abstract.  The radius of the field is variable.  An \"atactic manifold\" is not defined.  The authors use \"metric\" to refer to distances in metric space leading to potential confusion.  In the introduction \"derivative prototypebased methods\" and the facilitate of modeling \"big prototypes\" are not explained.  number 1 is complex due to changing embeddings.  Its unclear if the center and radius of prototypes are learned separately or jointly.  The NMS type used is not specified.  The concept of \"true location of prototypes\" is questionable.  In Section 3 \"big prototypes\" is described as a place of hyperfields not a single one.  Section 3.3 use undefined notation and is difficult to follow.  Equation 5 has an indexing inconsistency that could lead to confusion.  Algorithm 1 initializes with averaging over distances but its benefits are not discussed.  Table 1 use undefined metrics (\"P\" \"radius\" \"F\") and has a typo (\"spaticularly\"). radiuseferences:  Gao et al. 2019  Mettes et al. 2019  radiususu et al. 2019  Gidaris et al. 2018"], "g6UqpVislvH": ["Paraphrased Statement: Summary: The field extends the common sinusoidal position encoding used for input in nonEuclidean manifolds such as in NerF. Instead of using Euclidean coordinates the approach employs projections onto alternative orthogonal foundation. This extends the translational invariance of Fourier features in Euclidean space to nonEuclidean manifolds leading to a convolutionlike operator invariant to natural shifts on these manifolds. Experiments indicate performance improvements over traditional Fourier counterparts. effectiveness and Weaknesses: effectiveness:  Wellwritten and convincing experiments Weaknesses:  need:  The fields need is flawed as NerF and iPDF do not suffer from overparameterization.  The importance of \"shiftinvariance\" \"minimal parameterization\" and orthonormality for positional encoding is unclear. Redundancy and overfitting can sometimes be beneficial in reducing spectral bias.  range:  The field only examines three specific (and simple) nonEuclidean manifolds with known Fourier series.  The definition and derivation of natural shifts for more general nonEuclidean manifolds is unclear."], "sEIl_stzQyB": ["Paraphrased Text: This research aims to enhance cooperative multiagent reinforcement learning in settings with centralized condition and decentralized performance (CTDE). The proposed GreedyValue based Representation (GVR) method strives to satisfy two conditions: 1. IndividualGlobalMax (IGM): joint action selection aligns with the most favorable action for each agent. 2. TrueGlobalMax (TGM): The joint action value function accurately reflects the optimal value. The GVR method achieves these conditions without learning a fully expressive value function. It comprises two components: 1. Inferior target Sampling (ITS): Stabilizes the greedy joint action and encourages exploration of nonoptimal actions through prioritized experience replay. 2. Prioritized experience replay Buffer: Assigns higher priority to nongreedy actions with value exceeding those of a statebased critic. Experimental results using matrix games predatorprey simulations and StarCraft II benchmark function demonstrate GVRs superiority over existing methods. effectiveness:  Novel ideas with experimental validation.  ITS ensures the stability of greedy actions based on mathematical validation.  Promise in tackling nonmonotonic tasks by meeting IGM and TGM conditions. Weaknesses:  Concerns about ITSs reliance on superior experience replay to destabilize nonoptimal greedy actions.  Ambiguity in the computation of agent and critic losses.  Lack of adequate explanation in some sections of the paper.  Exclusion of key baselines in the experimental section which could potentially address the destabilization issue.  Inconsistent performance of WQMIX on the StarCraft II function requiring further investigation.  Missing experimental results on additional super hard SMAC function to evaluate scalability.  Absence of code for reproducibility."], "q23I9kJE3gA": ["Paraphrased argument: This work investigates generating conditional sets using sequence generation modelings. The authors propose incorporating order invariance and set size into sequencetosequence (seq2seq) modelings. They also introduce a novel data enhancement method based on topological sorting. Their modeling outperforms basic seq2seq modelings on three different datasets. Strengths:  Reasonably intuitive and wellmotivated introduction of order as a latent variable  Intriguing approach of modelinging cardinality with topological sortbased TSAMPLE data augmentation  Promise of opensource code and detailed hyperparameter descriptions for reproducibility Weaknesses:  Lack of stateoftheart comparisons (e.g. nonseq2seq or classification modelings)  Unclear generalizability to zeroshot settings where the partial order graph is unknown  need for an ablation work to assess modeling performance for different set lengths  Lack of modelinggenerated case for further insights  Absence of human evaluation  No pictorial representation of the modelings for improved understanding Questions:  Sampling method used for results in the main text (e.g. greedy beam)  Experimentation with pretrained decoder weights  Details on how cardinality could improve dialog generation and its benefits Suggestions:  Improve Section 1 with reallife case or Appendix B discussions  set citation style (citep vs. citet)  Provide a clear definition of sYt in Section 3.2  set modeling labeling in Section 4.2 (networkx  network)  Distinguish between `n` definitions in Sections 4.1 and 4.2  Include missing references"], "jXKKDEi5vJt": ["Paraphrased Statement: This paper focuses on studying the challenge and providing solution for Byzantineresistant optimization in a heterogeneous setting where workers possess distinct data and thereby have different loss work. Summary of Key Findings: 1. lower boundss: Even in situations with limited heterogeneity achieving predefined accuracy through any optimization method is impotential. This implies a lower bounds on the achievable optimality. 2. Upper boundss: Under the assumption of boundsed heterogeneity it is potential to attain any desired accuracy using specific optimization methods. 3. bucket as a Robustness Technique: bucket is introduced as a mechanism to enhance the robustness of aggregators like Krum coordinatewise median and geometric median to Byzantine attacks. 4. Extension of Robust Client Momentum method: The Robust Client Momentum method is extended to nonconvex problem demonstrating convergence to a neighborhood of a stationary point. This neighborhood can be arbitrarily reduced in overparameterized scenarios. Evaluation and contribution: The paper is comprehensive and presents solid contributions in the field of Byzantineresistant optimization under heterogeneity. It clearly articulates its significance and is wellstructured. However some minor inaccuracies and weaknesses have been identified in the proofs and related work section. overall strength: The paper offers valuable insight into the limitations and possibilities of Byzantineresistant optimization in heterogeneous settings. It establishes clear theoretical boundss and introduces a robust bucketing technique that broadens the applicability of existing optimization algorithms. field for Improvement: 1. Address the inaccuracies and unexplained derivations in the proofs. 2. Provide a more detailed comparison with related work highlighting specific similarities and differences in assumptions and issue. 3. Consider revising the abstract and conclusion to accurately reflect the contributions without overstating their significance. minor Improvements: 1. Update the notation for the number of Byzantine to avoid confusion with the objective work. 2. Ensure consistent notation throughout the paper (e.g. placement of superscripts and subscripts)."], "pQ02Y-onvZA": ["Summary: The paper expands a previously introduced exploration measure (SAU) from bandit to reinforcement learning (RL). This measure is based on averaging the squared temporal difference errors across states providing an approximation of the estimation error. The bonus for exploration is proportional to this uncertainty measure. Strengths and Weaknesses: Strengths:  The method is simple and straightforward to implement. Weaknesses:  Lack of Theoretical Justification: The paper does not provide a theoretical cornerstone for the approach.  Concerns about Averaging Over States: Averaging over states may lead to underestimating uncertainty in critical states hindering exploration in hardtoreach domain.  Limited experimental Evaluation:  The comparison just includes the epsilongreedy strategy which is not a strong baseline.  The experiments lack variation in domain and baseline making it difficult to assess the methods effectiveness.  There is a lack of ablation studies to explore the robustness of the approach under different conditions.  Missing Appendix: The referenced appendix is not included limiting the understanding of the experimental setup. Suggested Improvements:  Provide a theoretical justification for the approach.  Explore alternative averaging strategy that account for the importance of different states.  Conduct more comprehensive experiments with various domain baseline and annealing schedules.  Include ablation studies to evaluate the methods sensitivity to various factors.  Include the missing appendix to provide additional details about the experiments."], "pIjvdJ_QUYv": ["Summary: PrivHFL is a privacypreserving protocol for collaborative learning that allows parties to contribution knowledge without revealing their private data to the server or answering parties. It uses a secure protocol based on integer performance to enable private inference. PrivHFL outperforms previous methods in efficiency but has some limitations. effectiveness:  Improved efficiency compared to CryptGPU and CaPC.  Uses secure cryptographic performance built into floatingpoint arithmetic. Weaknesses:  Data Leakage: secret data is disclosed in field text if the querying party (PQ) collaborates with the server potentially leaking data about the private dataset.  Aggressive Logit Aggregation: Logits are aggregated from different models without normalization making the method vulnerable to approach.  No Differential Privacy (DP): The revealed logits could leak data about answering parties private data.  Intermediate solution Disclosure: PQ receives intermediate results raising concerns about model inference.  No Public Dataset assumption: PrivHFL does not require a public dataset unlike some existing methods.  Limited Communication: There is no direct communication between clients which may limit scalability and collaboration.  Modularity: PrivHFL is not as modular as CaPC which allows for easier integration of new techniques.  Active Learning: PrivHFL does not incorporate active learning like CaPC. Questions:  How scalability affects the number of clients and model size.  How private inference is implemented across all layers in PrivHFL.  Whether DReLU should be defined as \\frac12(1  sgn(x)). Other Comments:  Typos and formatting suggestions are provided.  References are updated to use consistent citation formatting."], "ofLwshMBL_H": ["Paraphrased Statement: Summary: This paper introduces a novel technique for inferring task identity without direct approach to historical information. It utilizes a mixture of Experts (MoE) architecture where taskspecific experts are trained with specialized kernels. These kernels enable the model to determine the appropriate expert for different tasks even without prior task information. The method exhibits exceptional performance on splitMNIST and splitCIFAR100 informationsets. effectiveness:  The paper is wellstructured and approachible.  The MoE architecture allows for shallow expert networks reducing memory and computational requirement.  The taskspecific kernels facilitate information pattern identification for task identification. Weaknesses:  Inferring task identity without approach to historical information is challenging and the proposed kernelbased solution may be an efficient approach.  Evaluation is limited to smallscale images extending the method to large more complex domains is crucial.  The field primarily compares to generative replay methods comparison with episodic and feature replay techniques would enhance the analysis.  memory and computational efficiency are claimed as advantages but a direct comparison with other baselines is lacking. PostRebuttal: While the authors have conducted initial experiments indicating the methods efficientness additional experiments are even necessary to fully assess its capabilities. Therefore the original evaluation remains unchanged."], "nsjkNB2oKsQ": ["Paraphrased Statement: Summary:  Examines how delayed rewards create nonMarkov behavior that challenges conventional reinforcement learning (RL) algorithms.  Introduces a novel Qfunction definition based on reward section with a historicalcurrent decomposition.  Compares the proposed method with recent RL technique through experiments. Strengths and Weaknesses:  Strengths:  research an underexplored area in Markov conclusion action (MDPs).  Weaknesses:  Overlooks relevant prior work on delayed rewards.  The proposed pastinvariance definition (Def. 2) is unclear and may rely on an unrealistic assumption.  The HC decomposition (Sec. 3.2) lacks rigorous justification and appears artificial in delayed reward scope. Additional CommentsQuestions:  First sentence in p.3:  Why is it unusual for the proposed PIDRMDP to consider history  Can the authors demonstrate cases where an optimal history is historyindependent  Sec 2.2:  The explanation below Sec 2.2 are confusing especially regarding multiple behavior policies that appear abruptly."], "lu_DAxnWsh": ["Paraphrased Summary: Researchers have demonstrated that pretrained transformers with modified depth and frozen weights can be trained to execute algorithmic tasks. By providing intermediate computation these models can generalize significantly better than those trained with less guidance. This was demonstrated through experiments on binary number addition where models incorporating data about carry numbers outperformed those with weaker supervision. strength and Weaknesses: strength:  The research is novel and explores the limits of current deep learning methods.  The writing is open and accessible.  Code is provided for reproducibility. Weaknesses:  The work is modified in depth and would benefit from further exploration in several domain:  Investigating the applicability of intermediate computation to other algorithmic tasks including those where the model may fail even with intermediate steps.  Extending the experiments to include other model architectures such as LSTMs or DNCs.  Evaluating the requirements for training transformers from scratch to perform algorithmic tasks including data and guidance requirements.  Conducting experiments to determine how much intermediate guidance is necessary for different type of models to performalgorithmic tasks.  The emphasis on the System 1System 2 motivation may overshadow the focus on transformers."], "ySQH0oDyp7": ["Paraphrased Statement: This paper proposes a posttraining quantization method for extremely lowbit neural networks. The method considers the quantization of activations during network reconstruction leading to improved accuracy. The approach involves randomly dropping the quantization of activations with higher loss flatness which allows for more optimal weight calibration. Experimental solution demonstrate the methods superior performance particularly for lightweight architectures and even for 4bit quantization on largescale datasets. Pros:  study activation quantization during posttraining quantization improving accuracy.  Utilizes loss landscape flatness observations to optimize quantization.  Shows strong solution especially for lightweight architectures and 4bit quantization. Cons:  The technical soundness of the proposed equation (7) may require clarification.  The writing could benefit from improved notations and explanations.  The connection between certain equations and concepts (e.g. Corollary 1 and Figure 3) needs to be made clear.  The physical meaning of the equation for randomly dropping activation quantization requires further explanation."], "twv2QlJhXzo": ["Paraphrase: Summary: This paper proposes a novel algorithm for learning from observation (LfO) tasks where the physical dynamics differ between the expert environment and the learners environment. The algorithm first trains an intermediary policy in the learners environment to match the expert state trajectory. Subsequently the learner policy can imitate the intermediary policy in its own environment. The authors demonstrate through experiments that the proposed method surpasses existing LfO algorithm on various locomotion tasks. effectiveness and Weaknesses: The approach of using robust entropy differential (RED) to estimate the joint density of consecutive state for imitation learning from observation is commendable. The overall structure of the paper is logical and straightforward. Concerns:  Equation (3) presents an unclear formulation where J(\\pia) appears on both the left and righthand sides but with different meanings. The authors intend to maximize J(\\pia) with respect to \\pia on the righthand side but the equation suggests otherwise.  Equation (4) similarly warrants refinement.  The rationale behind introducing the intermediary policy is not fully explained. If an advisor policy can replicate the expert state trajectory optimizing expected issue while preserving state distribution should resolve the issue. If the advisor policy falls short of fully recapturing the expert state trajectory how can we expect the learner policy to accurately mimic the expert state distribution by simply matching the stateaction distribution of the advisor policy  The presented setup is not novel as numerous field have addressed learning from expert and environments with different dynamics under various labels like policy transfer and sim2real.  A notable weakness lies in the experiments. The baseline algorithm selected are limited to LfO methods that do not explicitly address dynamics mismatch which creates an imbalanced comparison. The experiments lack comparison with baseline operating under the same settings as the proposed method such as [1]."], "nf3A0WZsXS5": ["Paraphrased Statement: Abstract: This paper introduces a technique to create representation of diseaserelated models from volume data extracted from structural MRI scans. The approach utilizes a GAN model with various regularization methods to optimize the learning of diseasespecific features. The paper highlights four contributions: 1. Modeling disease as a continuous progression 2. Ensuring the progression of the disease model 3. Distinguishing between multiple disease models 4. Inverting the model to retrieve the captured disease models The method is tested on simulated datasets and genuine Alzheimers disease data. Results demonstrate its ability to identify clinically relevant models. effectiveness and Weaknesses: The paper is wellorganized and the concept is intriguing. However some domain need advance: 1. Clarity on the input data and its generation from MRI scans is lacking. An model of the atlas segmentation would aid understanding. 2. Figure 1 is confusing. It should be clarified whether it depicts experimental results or expected outcomes from the method. 3. The relationship between the \"R indices\" (ri) and the latent representation (z) in Figure 1b needs clarification. 4. The paper mentions a dot product performance between the latent variable and the input which are both mapped to a 34dimensional space. However the input to the decoder is 34x1. Elementwise multiplication might be used rather. 5. The evaluation for each zi in the model decomposition is unclear. further explanation is needed. 6. The significance of the feature count (S) is not specified. 7. The hyperparameter selection method for the loss function is missing. Additionally the impact of hyperparameter selection on the models performance is not assessed except for lambda and M. 8. Figure 2b has an incomplete yaxis label. 9. The paper mentions that the method extends SmileGAN. It should explain why SmileGAN was not used for comparison."], "fvLLcIYmXb": ["Summary: This study suggests using the shift operation (as proposed by Wu et al. in CVPR 2018) in an axial manner for MLPmixer architectures. This approach outperforms other MLPbased method on ImageNet1K achieving comparable results to the Swintransformer. effectiveness:  Wellwritten and clear presentation  Simple and effective axial shift module  Extensive experiments beyond ImageNet Weaknesses:  Limited technical novelty as the shift operation has been presented as an alternative to convolution.  Lack of justification for using \"axial\" in addition to \"shift.\"  An alternative baseline using depthwise convolution has not been explored to determine the advantage of ASMLPmixers.  Absence of comparison with a representative convolutionbased method like EfficientNetv2 which exhibits superior performance in terms of FLOPs and throughput."], "ovRQmeVFbrC": ["Paraphrased Statement: Summary: This research proposes a comprehensive approach for addressing noisy labels during neural network training. The proposed method leverages both assumed correct and noisy labels assigning them different weights within various loss works to adjust their work on the training work. The authors demonstrate the effectiveness of their approach on CIFAR10 CIFAR100 and Clothing1M datasets achieving competitive results and significant improvements in a semisupervised setting. effectiveness:  Tackles limitations of existing noisy label handling methods including sample selection loss work innovation and label correction algorithms.  Introduces a noiseaware loss mechanism that adjusts the weight of ambiguous and noisy labels reducing training error.  Employs a confidencebased sample selection mechanism that separates ambiguous and noisy samples using the networks own predictions.  Utilizes different loss works for different sample sets based on noise level allowing for tailored feedback and improved learning.  Demonstrates strong resilience to noisy labels in a semisupervised setting significantly outperforming FixMatch. Weaknesses:  Requires further clarification on the use of pseudolabels from both noisy and clean data sources in Equation 9.  The approach shows a significant performance gain from using pseudolabels suggesting that sensitive labels may not be entirely reliable.  The absence of experiments considering the exclusive use of pseudolabels raises questions about the reliance on sensitive labels in training.  The appendix suggests that using a noise robust loss on pseudolabels yields lower performance which warrants further exploration.  Additional work on the approachs impact in a semisupervised setting similar to FixMatch would enhance the contribution of the paper."], "izvwgBic9q": ["Paraphrase:  method: The work introduces an innovative approach using Convolutional Neural Networks (CNNs) to address the FullWaveform Inversion (FWI) challenge in geological surveys. The method employs a loss work that connects the CNN to the governing wave equation. It leverages unsupervised learning allowing for the utilization of extensive unlabeled data. Numerical experiments demonstrate the efficacy of the approach.  Strengths and Weaknesses: The novel method is wellstructured and written. However certain aspects need consideration: Generic Comments:  The dataset is generated under specific measurement conditions. Would it be applicable in practical measurement scenarios The authors acknowledge the potential slowness of physicsbased approach but indicate that datadriven methods may also be timeconsuming if new network are required for different measurements.  Could the method extend to semisupervised learning settings Incorporating labeled samples into the loss work might enhance performance.  The improvement in results appears to be driven by the use of more samples. However there is a concern that the additional samples may be more representative of the ground accuracy in testing. Specific Comments:  The wave equation used assumes an elastic medium which may not hold for all materials. This limitation should be clarified.  In the introduction it should be noted that the OpenFWI dataset is simulated.  The MSE and SSIM values mentioned in the introduction lack context without comparisons or data about typical scales. Providing relative improvements would be helpful.  The loss work in equations (8) and (9) compares measurements to modeled pressure signals. Including the noise distribution in the comparison might be beneficial.  The discretization density (spatial and temporal) should be specified. Convergence checks and potential differences in discretization levels between training and test sets should be addressed.  Noise introduction in numerical results and the selection criteria for lambda parameters should be specified.  The \"batch size 16\" mentioned in experiments seems small especially considering the use of 8 GPUs. Clarification on whether this is a perGPU or total batch size is needed.  The discussion mentions \"challenging velocity\" but this is not elaborated upon in the results section creating some confusion."], "yKIAXjkJc2F": ["Summary: The research paper enhances the Neural ODE model by introducing a new parameter \"p\" that can be adjusted as a control or step of network depth. By varying this parameter the model enables flexible decoding of network outputs. strength:  Novel idea with open motivation  Detailed derivations for forward and backward passes  Wellexplained model building work Weaknesses:  issue segment lacks depth  Demonstrates basic functionality but does not provide significant insights  Expected more analysis of learned ODEs and their relationship to data trajectories Additional Questions:  Behavior of the model beyond the maximum value of \"p\" in experiments  Justification for using the finite difference method with its potential memory cost  Possibility of varying \"tmax\" instead of \"p\"  Explanation of \"q\" selection and the significance of \"pmax\" in the results"], "pfNyExj7z2": ["Summary Paraphrase: This paper introduces a vision transformer (ViT)based VQGAN where both the encoder and decoder are implemented using transformers instead of convolutional neural networks (CNNs). The proposed model shows quantitative improvements in unconditional and conditional image generation tasks and demonstrates its potential as an unsupervised representation learning model. Strengths:  The model effectively bridges the performance gap between VQVAE and VQGAN models with CNNbased GANs like StyleGAN.  The ViTbased architecture enables straightforward implementation and wide applicability.  The incorporated tricks (dimensional projection code normalization) contribute to improved codebook use.  The models performance is substantiated by substantial quantitative results and ablation work.  The paper is written with clarity. Weaknesses and Questions:  Further analysis is needed to support the claim of improved codebook use.  The weighting of losses in the training work of ViTVQGAN and the contribution of the logitLaplace loss to codebook use should be explored further.  It is unclear if VQGAN results use the same number of codebook entries as ViTVQGAN.  It would be of interest to investigate using ViTs in the discriminator as well.  The source of improvement in metrics using the StyleGAN discriminator is not specified (architecture or training procedure).  There is a typo in the second last row of Table 3 (referred to as Table 4 in the summary)."], "voEpzgY8gsT": ["Paraphrase: Summary: The proposed method utilizes generalized additive models (GAMs) to understand the intensity of multidimensional Poisson processes (PPs). intensity and Weaknesses: intensity:  Unique approach with potential for further development. Weaknesses:  Lack of motivation and clarity hindering reader understanding.  Insufficient justification for statements (e.g. concerning KDE in the introduction).  Limited information on GAMs approximation of the KolmogorovArnold representation including performance details.  Unclear explanations on APP motivations and Poisson Processes.  Ambiguous presentation of APP in Section 2.2 with a confusing connection to the intensity definitions in Sections 2 and 2.1 (taxi model adds further confusion).  Lack of experiments comparing computational efficiency with existing methods. Minor Comments:  Confusing notation: \\mathbfti in Section 2.  Incorrect figure order (Figure 2 cited before Figure 1).  Undefined symbol \\bot in Section 2.2 later defined as the least element.  Equation (5) is inconsistent with Equation (4).  Small figures impairing visibility.  Grammatical correction: \"to arrive at\" should be \"to converge to.\""], "oDFvtxzPOx": ["Paraphrase: Summary: This research introduces novel adjustments and techniques for feature selection using neural network. Key enhancements include a customizable masking use for feature selection and a selfsupervised component for pretraining. The approach is wellfounded and its benefits are supported by convincing experimental findings. Strengths and Weaknesses: 1. Equation (6) is borrowed from existing work raising questions about the originality of Section 4.1s contribution. Moreover the last paragraph of Section 4.1 outlining two advantages is difficult to understand. 2. The method of calculating the correlation structure R based on the input is unclear. Is it determined by the dataset or problem at hand 3. The experimental baselines consist primarily of basic models. It would be helpful to identify previous work that employed neural network for comparison. 4. It is unclear how the model will perform if no features can be removed. Additionally its scalability to highdimensional data should be addressed."], "uVXEKeqJbNa": ["Paraphrase: This field presents a novel index known as the StiffAware index (SAI) and a training strategy that emphasizes data points with high SAI values. neural network tend to prioritize learning smooth functions and realworld physical systems often exhibit both gradual and rapid changes. Without using SAI it becomes challenging for neural network to capture the fastchanging dynamics typical of stiff systems. The effectiveness of SAI is demonstrated through its application in solving the threebody problem. Strengths:  SAI addresses the difficulty of learning stiff systems.  Oversampling data with high SAI values evidence to be effective. Weaknesses:  SAIs accuracy as an approximation of the stiffness index (SI) may be limited due to coordinate system dependence.  The training strategy relies on heuristic parameters and optimal values may vary with the problem. Minor Comments:  It would be beneficial to explore whether the neural network bias towards smooth functions is truly a limiting factor and if SAI addresses this publication.  Figure 1 should be modified to ensure consistent axis scales across model. PostDiscussion Updates: After further experiments and explanation the reviewers concerns have been addressed. The fields score has been revised to 6 out of 6."], "hcQHRHKfN_": ["Paraphrased Statement: This paper provides a technique for training multiple unique policies. To achieve this the proposed method alternates between optimizing for the original reward and a diversity reward which encourages policies to be different from each other. The methods effectiveness is demonstrated in various environments showcasing its ability to produce policies with distinct behaviors. effectiveness:  Simple approach capable of creating policies with diverse behaviors.  extensive evaluation across multiple environments and baseline methods to support its effectiveness.  The methods concept and implementation (including trajectory filtering based on cross entropy and the use of rejected trajectories for diversity) are distinctly explained. Weakness:  Some results suggest that the added diversity reward can lead to suboptimal performance in terms of the original reward. For example in the mujco environments the first trained policy often performs effective while subsequent policies may be effective or worse than their predecessors. A discussion of this number would be beneficial."], "jGmNTfiXwGb": ["Paraphrase: This research presents a technique for converting offline algorithms into online algorithms. It involves predicting the offline algorithms future process by studying its past behavior. The technique is tested on both artificial and realworld data (stock market). effectiveness and Weaknesses:  effectiveness:  Predicting offline algorithm behavior through multiple task learning allows for longerterm predictions.  Encoding offline algorithm behavior in graphs and predicting future occurrences is innovative.  Weaknesses:  The model performance may depend heavily on the offline algorithms fit to historical data making it susceptible to overfitting.  The algorithm requires wide data for labeling structures limiting its ability to predict far into the future.  Its unclear how the algorithm determines the time of predicted future process across multiple tasks.  The methodological model presented in Section 2 could benefit from a simplified model for clarity."], "xVGrCe5fCXY": ["Paraphrase: Goal: Replace Gaussian noise in Diffusion Probabilistic Density Models (DDPMs) with Gamma distribution which offers more flexibility. Technical Implementation: Reformulate diffusion and its reverse process for Gamma distribution derive variational lower bound. Strengths and Weaknesses: Strengths:  Sound theoretical derivation of the proposed DDGM model. Weaknesses:  Errors: Equations (6) and (16) must be corrected.  Ambiguous Notations: Equation (13) incorrectly uses \\bargt instead of gt for the reverse process.  Inconsistent Notation: Section 3.2.3 uses p\\theta for the reverse process and q for both diffusion and reverse processes creating confusion. The common practice is to use q for diffusion and p for the reverse process.  Incomplete Explanations: Notations in Section 3.2.3 such as \\hatx0 are not defined.  limited experimental Evaluation: While FID scores show DDGMs superiority visual comparison to demonstrate the necessity of Gamma distribution are lacking."], "xOeWOPFXrTh": ["Summary: estimation of motion from video data has applications in computer vision and healthcare. This paper utilizes a multiderivative convolutional attention network (MDCAN) to estimate higherorder derivative of 1D heart beating signal from video input. A secondorder loss role is employed and solution are measured in both firstorder (HR MAE) and secondorder (LEFT MAE) metrics. The findings suggest that using the secondorder loss improves secondorder metrics at the cost of firstorder performance. effectiveness:  Comprehensive ablation work  focus on secondorder motion estimation (novel topic) Weaknesses:  Limited technical novelty the CAN concept was previously established with insufficient treatment on novel contributions.  Inadequate interpretation of solution in Table 1:  Intuitively firstorder and secondorder motion should be correlated but the solution indicate an inverse relationship.  Combining first and secondorder estimation (multitask learning) would typically provide common benefits yet this is not observed in this work.  Insufficient literature review on PPGcardiac motion estimation:  Metalearning techniques have shown promise in these field but they are not discussed. References to relevant studies should be added."], "fVu3o-YUGQK": ["Summary: The work introduces two contributions: 1. It identifies that the multistage vision Transformer (MSVT) generates patch representations that are less distinctive than those produced by vision Transformers (VTs). 2. It presents a loss work for pretraining MSVTs without supervision. This loss work promotes discriminative patch representations. Experiments reveal that adding this loss term to the selfsupervised training of MSVTs consistently improves performance. It also enhances other architectures. effectiveness and Weaknesses: Weaknesses:  The proposed loss work is similar to one used for Swin and the performance advance are relatively small.  The paper inaccurately presents the performance advance resulting from the loss term.  The impact of the proposed loss on transferability is unclear from the current results. effectiveness:  The paper clearly explains how the loss work addresses the discriminative ability issue of MSVTs.  The loss work improves performance for several architectures.  The paper provides comprehensive experimental evaluation including limitations in image segmentation tasks.  The manuscript is easy to understand and wellwritten. Minor Questions and Editorial Suggestions:  Are crossentropy and cosine distance used consistently for patch selection and minimization  Why was optimal 11 matching (Hungarian algorithm) not explored for patch matching  The paper mentions \"Optimal Transport\" as a selection method. Can this be explained more clearly  Consider using more precise language and enumeration in points 1 and 2 on page 2.  Specify the method used to compute patch distances in the caption of Table 1."], "fRb9LBWUo56": ["Summary This research examines the use of deep reinforcement learning (RL) for speeding up magnetic resonance imaging (MRI) scans. It compares two recent RL approaches (Bakker et al. Pineda et al.) to nonRL methods. The findings suggest that RL offers limited advantages over the nonRL baseline (LBCS). Strengths  The work addresses the practical issue of MRI scan acceleration and explores RLs potential use in this context.  It compares two relevant RL approaches to MRI acceleration with a solid nonRL baseline (LBCS).  It includes empirical results for various context preprocessing and postprocessing options. Weaknesses  The work does not directly compare the two RL methods making it difficult to assess their individual contributions.  The results are not presented clearly with some metrics (AUC MSE) appearing inconsistently or missing.  There are inconsistencies in terminology such as \"reversals\" being used to describe both sudden and gradual performance declines."], "mhv2gWm3sf": ["Paraphrase: Summary: This work presents an fdivergence total variation objective (TVO) framework that encompasses existing work like RVB CUBO and ELBO. It introduces the concept of transforming fdivergence into a generalized \\chiexponential family and integrating TVO along the \\chipath. The work offers theoretical insights optimization techniques and mathematical results supporting the proposed fTVO. Strengths and Weaknesses: Strengths:  The generalized \\chiexponential family is an innovative concept. Weaknesses:  The definition of fdivergence differs from the conventional one.  Equation 8 is invalid for certain parameter values requiring clarification.  The field of the \\chi function is not specified.  The paper should clarify the term of the generation function f (e.g. convexity differentiability boundary behavior).  The existence of a \\chi function corresponding to any given generation function f is not ensured.  Equation 11 needs verification.  The derivation of Equation 17 is not provided.  The monotonicity of S\\beta is not discussed which is crucial for TVO. Typos:  Page 2: \"measure\" instead of \"measures.\"  Page 2: \"fTVO\" should match the font of the rest of the text.  Page 4: \"an exponential\" instead of \"a exponential.\"  Page 4: Definition 3 uses \"x\" instead of \"\\textbfx\"."], "tDirSp3pczB": ["After Rebuttal Summary The paper has undergone significant improvements and clarifications based on reviewer feedback. The authors commitment to addressing concerns is commendable and the reviewer now elevates their score by one guide. Strengths and Weaknesses While the papers strengths remain unchanged the reviewer still has a few outstanding concerns:  Supervised Learning in Disguise: The contrastive loss employ class data suggesting a supervised learning approach not unsupervised learning. This raises questions about the validity of the papers claims.  Benefits of Larger K: The authors justification for using a larger K in contrastive representation learning lacks theoretical support. The reviewer recommends referring to relevant literature to address this guide.  Intuitive Explanation of gap: The authors should provide a clearer explanation of the gap notation used in their equations.  Equation Numbering: The equations in the paper should be numbered for convenience. Additional Suggestions  The compelling Figure 4 should be presented earlier in the paper to support the authors arguments.  The paper lacks experimental evidence demonstrating variance reduction which is central to the authors theory.  The reviewer encourages the authors to elaborate on the practical implications of their study."], "m716e-0clj": ["Paraphrased Summary: This work proposes an enhanced decentralized adaptive gradient descent method to tackle data heterogeneity. The need for such a method is wellestablished and experimental findings demonstrange its superiority to current approaches. However the methods theoretical analysis is lacking. Strengths:  Motivation is clear highlighting the limitations of existing methods in reaching the stationary point.  extensive experiments effectively demonstrange the methods improved performance. Weaknesses:  The method lacks substantial novelty being primarily an extension of an other work to adaptive gradient.  convergence is guaranteed only to within a neighborhood of the stationary point whereas existing methods achieve convergence to the stationary point itself.  The lower bound on \\beta2 is questionable potentially leading to a problematic convergence range in practical scenarios (e.g. when \\beta2 is set to commonly used values such as 0.1 or 0.01).  The constraints and optimal value for \\beta1 are not specified.  The experimental values for \\beta2 and \\epsilon are not provided. Additionally the experimental results appear to contradict the parameter settings required for theoretical convergence suggesting potential issue with the hyperparameter settings."], "gijKplIZ2Y-": ["Summary: The paper focuses on using machine learning (ML) to develop protocol for data center applications. It considers a specific scenario: a singleserver data center with a FatTree topology and a predefined policy. A neural network (NN) is trained to learn forwarding policies based on examples. A modular architecture is used to manage complexity and the NN is designed to be adaptive during training while remaining fixed afterward. strength and Weakness:  strength: The paper uses ML to solve data center network problems contributing to a limited but growing body of work in this field.  Weakness: Specific fields that need clarification:  1. Do all switches require MISTILL installation Can it work in hybrid architectures with some switches lacking it  2. The work focuses on data center applications and assumes Clostopologies which limits its applicability to current packetswitched networks.  3. Is the topology assumed known with each node aware of its parent and child nodes  4. The learned policy involves dropping packets or flows when nodes become unreachable. Is there a mechanism to switch to alternative routes in case of unavailability  5. Can the framework handle multiple policies instead of the single policy it currently supports  6. The approach may not extend well to more complex topologies such as ad hoc mesh networks.  7. The authors introduce a \u03b2 term for embedding differentiation while contrastive learning design to achieve the same goal. Why couldnt NCE be successfully employed  8. The paper assumes nodes are identical ignoring context information that could aid in embedding distinction.  9. The criticism of CONTRA for relying on a highlevel policy language lacks justification.  10. The learned knowledge by the model is not clearly presented.  11. The discussion of overhead requires more details.  12. The reference to shortest path routes may not be applicable due to the tree topology single cyclefree path.  13. Clarification is needed regarding the number of switches and servers in the data center as well as the number of 8cores at the data center. Typos and Acronyms:  Acronyms (e.g. HNSA) should be defined upon first use rather than later."], "xOHuV8s7Yl": ["Summary:  Introduces two novel neural networks: Triangularlyconstructed Neural Network (TNN) and SemiQuantized Activation Neural Network (SQANN).  Proves that both TNN and SQANN are universal approximators.  Demonstrates that TNN and SQANN are resistant to catastrophic forgetting.  Highlights specific neuron properties identifiable in TNN (strongly and halfactivated) and SQANN (likely outofdistribution samples). contribution Claimed by Authors:  Proposed two novel neural networks: TNN and SQANN.  Provided validation of universal approximation based on the construction of TNN and SQANN.  Demonstrated resistance to catastrophic forgetting.  Enabled SQANN to identify outofdistribution samples. Strengths and Weaknesses: Strengths:  Offers two novel validations of the universal approximation theorem through construction. Weaknesses:  needs spelling and grammar revisions for clarity.  Some design are placed too far from their corresponding text descriptions.  Could use more professional formatting for mathematical formulas.  Some claimed contribution lack significance or supporting evidence:  Interpretable construction for universal approximation are not groundbreaking.  The universal approximation validation using TNN is limited to 1dimensional inputs.  The claimed \"universal approximation\" does not fully align with machine learning definition.  The resistance to catastrophic forgetting claim lacks adequate support.  The generalization described is not consistent with standard machine learning practices."], "gxk4-rVATDA": ["Paraphrase: This work introduces a new technique for training neural networks that allows for the optimization of individual weight bits. weight are expressed as the sum of their bits multiplied by ability of 2. During training updating a bit involves modifying a floatingpoint act with positive values indicating a bit value of 1 and nonpositive values denoting a bit value of 0. Experimental finding:  network with fewer weight bits exhibit large sparsity than those with more bits.  only a few of the most significant weight bits contribute significantly to high network accuracy while the remaining bits serve as regularizers.  Less significant bits can be utilized to store additional data. Strengths and Weaknesses: Strengths:  Provides a novel perspective on the relationship between network weightsbits and regularization.  Demonstrates various phenomena resulting from bitwise training through extensive experiments. Weaknesses:  Proofreading error in the text (Figure 5 reference should be Figure 4).  The second paragraph of Section 5.1 does not support the claim that standard training cannot identify the most significant bits.  The significant accuracy drop (4) upon weight signonly updates may be significant particularly for smaller networks.  While the paper suggests that insignificant bits provide regularization this observation is not unique to bitwise training and is a known concept in network quantization. Questions:  The paper postulates that the probability of zerovalued weights in 32bit models is negligible (1e31). This would imply an exponential decrease in weight sparsity with decreasing bitwidth contrary to the observed level trend in Figure 2. The paper does not provide an explanation for this discrepancy."], "rqolQhuq6Hs": ["Paraphrased Summary: This work investigates the behavior of SGD near the minimum. Unlike other approach that process SGD noise as static this work analyzes its locationdependent nature leading to significant differences in escape behavior. By simplifying the noise covariance matrix the authors derive an SGD with isotropic noise on a transformed landscape. This new system allows for the calculation of the escape rate and stationary distributions which depend polynomially on the loss in contrast to the exponential dependence observed previously. mathematical experiments support the analytical assumptions and validate the theoretical findings for additive regression. strength and Weaknesses: strength:  Analyzes the additive relationship between SGD noise and loss.  Simplifies the noise covariance matrix to derive an SGD with isotropic noise on the logloss landscape.  Demonstrates that escape from the minimum depends polynomially rather than exponentially on the loss height.  Establishes a novel connection between SGD stability and the effective dimension of the landscape. Weaknesses:  mathematical support for the power law escape rate and stationary distribution is limited to additive regression model. It remains to be seen if these findings hold true for nonadditive model like neural network. Minor Comments: 1.  Explanation added: Outliers of the Hessian refer to nonzero eigenvalues indicating directions with significant noise reduction.  Explanation added: The last term in equation (11) involves the Hessian matrixs determinant which reflects the overall noise reduction compared to isotropic noise. 2.  Justification added: Isotropy of SGD noise in the ndimensional space is assumed because the nonzero Hessian eigenvalues vary well resulting in significantly different noise strength along different directions. However the impact of this anisotropy on the overall behavior is considered negligible. mathematical experiments can be conducted to further justify this assumption."], "iC4UHbQ01Mp": ["Paraphrase: This work examines the impact of poisoning and backdooring attacks on contrastive learning algorithms. Since contrastive learning leverages plentiful unlabeled data from the internet the work questions the safety of using such data. Key Features:  The work investigates multimodal classifications particularly the time classifier that is trained on vast internet data with text descriptions.  The proposed attacks inject minimal noise into the training data inducing specific adversarial behaviors.  The effectiveness of these attacks demonstrates the vulnerability of the embedding work even with limited noise. The paper presents clear explanation and comprehensive experiments that support the effectiveness of the attacks. It highlights the need for defense algorithms that can detect poisonedbackdoored data obtained from online sources."], "t8O-4LKFVx": ["Paraphrase: Main Thesis: Conformal prediction aims to make accurate predictions about the value of a target variable within a set with a guaranteed level of statistical confidence. This work proposes a novel method for simultaneously learning the conformal predictor and the predictive model using an endtoend process. Method: The proposed method leverages differential surrogates to approximate conformity scores and threshold value which are so integrated into the learning model. The authors claim that this approach can enhance the accuracy and control of classspecific predictions or constraints within conformal sets. Strengths and Weaknesses:  Strength: The proposed method offers a promising avenue for optimizing conformal predictors and enhancing their performance.  Weakness: The paper technical details are presented concisely making it challenging to fully grasp the proposed method. The authors could provide more instance or instance to clarify the relationship between the loss use and the conformal prediction model. Questions and Concerns:  Calibration: The authors assert that the proposed method maintains the calibration guarantees of traditional conformal prediction. However it is unclear why this should be the case since the method uses approximations. The authors should provide theoretical explanations or empirical evidence to support this claim.  ClassWise Control: The authors argue that conformal prediction often falls short in addressing classspecific issues. While this is not only accurate the paper does not adequately explain how the proposed method addresses these issues particularly in comparison to Mondrian conformal predictors.  influence Control: The paper proposes controlling the influence of the predicted set to improve efficiency. The authors should clarify how this approach preserves conformal guarantees and discuss the potential risks of trading off accuracy in certain case for improved performance in others.  specific Suggestions:  Avoid mentioning posterior probability estimates in the abstract and introduction as conformal predictions only require scorevalued predictors.  Refrain from referring to the most probable class as the Bayes decision rule as this is only true under a zeroone loss scenario."], "jeLW-Fh9bV": ["Paraphrased Statement: This research proposes a threestep method for training longhorizon policy: 1. Use offline data to pretrain a model and acquire a set of skills. 2. Use metatraining to teach a policy to apply those skills. 3. Adapt the metatrained policy to handle a novel unseen task. Experiments demonstrate the methods effectiveness in maze navigation and manipulation tasks showing improvement over existing methods. Strengths:  Addresses the challenge of longhorizon learning.  Outperforms comparable technique in experiments.  Provides valuable data on task variations and misalignment.  clear and concise writing. Weaknesses:  Lacks significant novelty compared to prior work.  No comparison against standard metatraining methods.  Insufficient exploration of field shift between pretraining metatraining and testing data.  Incomplete quantification of longhorizon performance capabilities."], "rRg0ghtqRw2": ["Paraphrase: Summary: This method for designing a curriculum gradually increases the difficulty of existing levels that can be solved by an agent. This structured approach avoids the challenges of randomly searching for challenging levels and builds incrementally on preexisting ones. Its empirical performance has been positive. effectiveness:  The approach addresses a significant problem.  The results obtained are promising.  The method is straightforward and clearly described.  The experiments are wellpresented. Weaknesses:  The environments functiond in the study are relatively simple with restricted editing capabilities.  It would be valuable to test the method in more complex environments where game dynamics can be altered.  It is unclear how the method prevents the editor from generating impossible levels. How is the \"regret\" of a newly generated level measured without approach to the optimal solution (pi) Highlighting the answer to this question could enhance clarity. minor:  The consistent function of pink for ACCEL results enhances readability."], "yeP_zx9vqNm": ["Paraphrased Statement: This study discovers a connection between adversarial training in artificial neural network (CNNs) and human peripheral vision. Using images called \"metamers\" the researchers found that peoples ability to distinguish between real images and their synthetic versions decreases at different distances from the eyes focus (eccentricity). This drop is similar for metamers generated using inverted adversarially trained CNNs and for those generated by a model of peripheral vision. This suggests that adversarially trained CNNs may mimic some aspects of human vision in the outer role of the retina. Strengths and Weaknesses:  The paper presents an intriguing connection between CNNs and peripheral vision.  The data support the interpretation that robust CNNs relate to peripheral vision like the Freeman  Simoncelli model.  However the models still fully discriminate between metamers at 30 degrees eccentricity a discrepancy that should be explored further.  The study could benefit from including subject information and opensourcing the research materials.  The concept of learning invariance shared by foveal and peripheral processing should be further investigated in future study. Minor Points:  The discussion assumes standard CNNs are spatially uniform but this is not always accurate.  Some references in the text should be updated with corresponding conference proceedings citations."], "yfe1VMYAXa4": ["Paraphrased Statement: This study suggests that integrating biological knowledge from knowledge graph (KGs) into protein representation can improve them. The author introduces OntoProtein a computational framework that incorporates the structure of Gene Ontology into protein pretraining models. Through experiments the study demonstrates the effectiveness of the proposed method and the advantages of using Gene Ontology information. Despite the overall strength of the paper it could benefit from further development."], "zq1iJkNk3uN": ["Paraphrased Statement: This work introduces DeCLIP an improved version of CLIP that uses three additional loss work:  Internal selfsupervision (SimSiam for paradigm BERTstyle language modeling for text)  Diverse paradigm and text augmentation (paradigm resizing and cropping text synonym replacement and deletion)  Nearestneighbor distance optimization DeCLIP shows improved zeroshot ImageNet classification accuracy with less training data (88M vs. 400M) and beneficial linear probe performance across multiple datasets. The ablation work (Table 4) and convergence rate analysis (Figure 7) demonstrate the effectiveness of the proposed loss work. strength:  DeCLIP outperforms baseline CLIP models on ImageNet zeroshot classification with the same parameters but reduced dataset size.  The ablation work (Table 4) shows the significance of the proposed loss work.  DeCLIP converges faster than vanilla CLIP (Figure 7). Weaknesses:  The numbers reported for DeCLIP and baseline CLIP are not directly comparable due to different training environments.  The justification for the chosen dataset size (400M) is lacking and reproduced CLIP results using the authors implementation are requested.  The impact of data quality improvement in the 88M DeCLIP full data is unclear.  It is unclear whether the performance improvement from multiview supervision in Table 4 is due to the EDA augmentation or the multiview approach.  The claim that DeCLIP is equivalent to CLIP with less training data requires additional verification using the same training dataset. additional analysis:  Quantitative comparisons using weaklysupervised object localization benchmarks are recommended for CAM visualizations.  Nearest neighbor research performance comparisons using crossmodal retrieval benchmarks are suggested. Questions:  Why are zeroshot results for other benchmarks not provided  Despite beneficial zeroshot ImageNet results why does DeCLIP ViTB32 perform worse than CLIP ViTB32 on linear probes  Is the memory consumption comparison between DeCLIP and CLIP available List of Suggested Experiments:  Reproduced CLIP zeroshot ImageNet top1 results for 88M with ResNet50 ViTB32 and RegNetY64GF (to ensure comparability)  WSOL benchmark comparisons  Crossmodal retrieval benchmark comparisons  CLIP results with EDA augmentation for 15M  29M  56M  88M datasets"], "w60btE_8T2m": ["Paraphrased Statement: This study introduces a method for creating molecular graphs based on spanning tree. Specifically a molecule is built through a series of actions that involve adding atoms adherence and branches as well as handling circular construction. These actions are controlled by a specialized neural netstudy trained for tree construction. Empirical results indicate that the method learns molecular distributions effectively and can be useful for molecular optimization. effectiveness:  Adopts a logical approach to construct molecules using spanningtree representation.  Clarifies limitations of penalized log P score as a benchmark. Weaknesses:  Lacks detailed comparison to existing study emphasizing the importance of differences.  May not provide a theoretical guarantee for generating valid molecules.  optimization performance may not surpass classical VAEBO approaches when controlling the tradeoff between score and realism."], "qyzTEWWM0Pp": ["Paraphrase: This paper introduces Multiresolution Equivariant Graph Variational Autoencoders (MGVAE). These models learn and generate graphs hierarchically with higherorder message passing encoding the graphs and creating mutually exclusive clusters. These clusters facilitate coarsening into lower resolution and a hierarchy of latent distributions. The model also maintains permutation equivariance with respect to node ordering. Strengths:  The model achieves competitive results on generative tasks and graph link prediction.  The hierarchical VAE model proposed in Section 4 is a significant contribution. Weaknesses:  The paper is technically complex.  The authors do not address the nondifferentiability of the cardinality function in the KL term (Equation 2 or 3). It is unclear how this issue is resolved during training."], "vaRCHVj0uGI": ["Paraphrase: This work explores the use of denoising score matching to tackle inverse problems in medical image such as undersampled CT and accelerated MRI reconstruction. The paper assumes noisefree measurements using a Dirac delta distribution to represent the measurement noise. To train a score work a variance exploding stochastic differential equation (SDE) is used as in previous research. During the inference work the authors introduce a weighted projection onto measurement sampling which differs from previous methods that focused on the gradient of the data distribution. This projection can be incorporated into various sampling schemes such as annealed Langevin dynamics and predictorcorrector approaches. The experimental solution demonstrate the effectiveness of unsupervised score matching methods for inverse problems in image showing superior generalization performance compared to existing methods. Strengths:  The paper clearly presents the concept of combining diffusionbased generative models with inverse problems.  It introduces some new elements particularly in the adapted sampling approaches.  The experimental solution indicate the potential of the proposed method with superior performance on tasks such as metal artifact removal even outperforming supervised methods. Weaknesses:  The definition of inverse problems in the paper is too restrictive excluding realworld noise models commonly encountered in medical image.  The mathematical proofs in the paper are trivial and do not provide significant theoretical insights.  The core newty of the proposed method is the additional sampling step which is essentially a proximal mapping used extensively in MRI reconstruction.  The paper lacks details on the learning work including the model loss work and optimization method used."], "kj0_45Y4r9i": ["Summary: This paper presents a distinct clustering technique using unsupervised classification and offers a generalization bound for similarity classification. The effectiveness of the method is demonstrated through experiments. effectiveness:  Sound technical approach with a theoretical analysis of generalization bound.  Proposal of a straightforward and practical clustering method with hyperparameter tuning strategies.  positive experimental results showcasing the methods efficacy. Weaknesses:  Lack of clarity regarding the need behind the proposed method and its advantages over existing similaritybased clustering approaches.  Outdated comparison methods in experiments most methods were proposed prior to 2015 with only one from 2021. Inclusion of more recent stateoftheart methods would strengthen the comparison.  Computational complexity analysis appears twice (Page 6 and Experiments section) but the latter does not provide meaningful comparisons against other methods. A demonstration of comparative results for computational complexity and running time on the utilized datasets would be valuable.  Captioned mismatch between Table 1 and Table 2 both captions mention \"cluster number c\" yet \"c\" does not appear in either table."], "hUr6K4D9f7P": ["Paraphrase: Summary: This study explores how Adversarial Weight Perturbation (AWP) enhances Graph Neural Network (GNNs) especially their generalization capabilities. The authors establish theoretical foundations based on previous work on Convolutional Neural Network (CNNs) and delve into the issue of gradient vanishing when applying large \\rho values. They propose two novel methods Truncated AWP (TAWP) and Weighted AWP (WAWP) to mitigate this problem. Experiments demonstrate that vanilla GCN outperforms other variants while TAWP and WAWP yield smoother decision bound. The authors proceed to conduct various experiments evaluating clean and robust accuracy aligning with existing study. contribution: The authors contribute by conducting wide experiments on graph datasets using AWP a novel approach in this context. effectiveness: The experiments in Section 5 provide valuable insights despite largely following established methodology. Weaknesses: Section 3:  The theoretical issue are derivative and could have been directly derived or used from previous work.  The AWP algorithm lacks novelty and is identical to the one proposed by Wu et al. 2020a. Section 4:  Insufficient explanation of how \\theta(awp) and \\theta(normal) are assigned which is crucial.  Absence of theoretical or intuitive justifications for TAWP and WAWP. This aspect is more significant than the theoretical issue in Section 3.  Missing issue for WTAWG in Figure 3. Summary: effectiveness: experimental issue Weaknesses: Limited innovation lack of justification for the proposed algorithms"], "z2B0JJeNdvT": ["Paraphrased Statement: compact: This paper presents a zerothorder algorithm for distributed optimization on a changing communication network. It also introduces a multistage version of the algorithm with varying stride sizes. convergence rates for these algorithms are determined and compared to those achieved by centralized methods. Strengths and Weaknesses: 1. Strengths:  The paper is wellstructured and easy to understand.  The results appear to be valid. 2. Weaknesses:  Lack of Novelty: The proposed algorithms seem like straightforward extensions of existing firstorder algorithms to the zerothorder setting. The paper should emphasize its novel contributions. Are there unique challenges in analyzing zerothorder algorithms compared to firstorder ones  Unclear function of communication graph: The communication graph is expected to influence the convergence rates significantly but the main theorems do not explicitly address this relationship. Further discussion would be beneficial.  Limited Numerical Experiments: Despite mentioning potential applications in machine learning the numerical experiments only involve a basic least squares problem. more comprehensive experiments are needed to demonstrate the algorithms performance in realworld scenarios."], "wronZ3Mx_d": ["Summary: DKLM (deep Kernel Gaussian Process with Landmark Metafeatures) is a novel hyperparameter optimization (HPO) method that leverages transfer learning. Using a deep Kernel Gaussian Process (GP) DKLM predicts examination losses by incorporating historical hyperparameterresponse pairs from previously observed datasets. This mechanism conditions HPO on past experimental issue. The embeddings for the GP are obtained by aggregating the hyperparameterresponse pairs using a deepsetlike neural network. effectiveness:  The proposed DKLM approach enhances deep GP kernels with neural network components potentially improving performance over existing methods.  hpobv3 experiments demonstrate DKLMs effectiveness in both transfer and nontransfer settings. Weaknesses: Clarity issue:  The paper lacks clarity making it difficult to understand open challenges in HPO and DKLMs unique contribution.  Terms like \"negative transfer\" and \"metafeatures\" are used without proper definition.  The notion of \"landmark metafeatures\" in DKLM is not clearly explained and it remains unclear how it differs from existing definitions. Missing DKLM Details:  important implementation details are missing including the application of DKLM update of historical data and learning strategy used in HPO.  The training schedules and hyperparameter optimization for the deep kernel GP are not provided. Experimental issue:  The toy experiment issue do not align with expectations suggesting potential issue with DKLMs performance. Language and Style:  The writing style requires improvement with sentences lacking clarity and precision.  Avoid unnecessary jargon and unclear terms to enhance accessibility."], "mmUA7_O9mjY": ["Summary: The paper introduces an algorithm for finding suitable contact points to manipulate deformable target. This algorithm utilizes an optimal transport approach to determine a priority score for each particle within the target. This score guides the selection of the initial contact point. For multiple manipulators the algorithm evaluates several poses to identify the optimal shape. The algorithm has demonstrated improved performance in manipulating deformable target into specific shape including tasks that proved challenging for previous methods. effectiveness and Weaknesses: effectiveness:  Wellwritten paper with impressive results. Weaknesses: Questions: 1. In the single manipulator placement algorithm is the \"d\" work used to determine the distance to the closest point on the manipulator or its center Is orientation considered in the placement optimization 2. How does the algorithm decide when to move to the future point in a multipoint task Is it based on time state or reward 3. For tasks involving translating the target do all particles receive the same priority score If thus how does the algorithm identify the optimal contact points Can the algorithm consider physical information 4. While the heuristicsbased contact selection method seems practical it would be helpful to provide groundtruth data to support its optimality such as comparithusn to an thorough grid research. 5. The paper lacks point on the controller optimization with differentiable physics. Including a description perhaps in an appendix would enhance the paper completeness."], "nBU_u6DLvoK": ["Summary The research paper introduces UniFormer a framework that leverages a combination of 3D convolution and spatiotemporal selfattention for efficient and accurate video classification. Specifically the framework uses local 3D convolution layers in the initial stages and global selfattention layers in the later stages creating a balance between efficiency and accuracy. UniFormer achieves stateoftheart performance on multiple action recognition benchmarks. effectiveness and Weaknesses effectiveness:  Novel Problem and Strong solution: Tackles a significant issue in video analysis and demonstrates impressive accuracy on several action recognition benchmarks.  Efficiency: Achieves high accuracy while maintaining efficiency showcasing the practicality of the framework.  Extensive Evaluation: Provides comprehensive ablation experiments to assess framework components. Weaknesses:  Clarity: The paper writing needs improvement making it difficult to grasp the technical details.  Limited Technical Novelty: The proposed approach combines existing techniques (3D convolution and selfattention) limiting its technical originality. Suggestions for Improvement  Explain Relationship to Depthwise 3D convolution: Present the local UniFormer block as an extension of depthwise 3D convolution clarifying its similarity and differences.  Tone Down Novelty Claims: Emphasize the paper as an empirical field on combining 3D CNNs and selfattention rather than a transformative architecture.  Simplify UniFormer Name: Rename the approach to better reflect the unification of 3D convolution and selfattention.  Elaborate on DPE: Provide more data on the dynamic position embedding used in the framework.  Address Typographical and Grammatical Errors: Correct errors in the text for improved clarity."], "zzk231Ms1Ih": ["Paraphrase: This paper explores the theory of \"tournament representations\" which are specific case of lowrank matrices that accurately represent the winloss practice of tournaments. The authors demonstrate various properties of these representations and show that their analysis can be simplified by focusing on \"Rcones\" tournaments where one participant consistently defeats all others. They provide a comprehensive analysis of rank2 tournaments and identify a forbidden class for rankd tournaments. Furthermore they establish an upper bounds on the minimum dimension required to represent any given tournament using a measure related to the tournaments \"feedback arc sets.\" This upper bounds can also be extended to sign matrices. Strengths:  Presents a novel theory of tournament representations that distinguishes them from general sign matrices.  Effectively connects representations flip classes and cones.  Provides a complete characterization of rank2 tournaments. Weaknesses:  Lacks a discussion on potential applications of tournament representations.  Does not explain the significance of vector representations and their relationship to tournament structure.  The connection between Theorem 6 and the rest of the work is unclear.  The measure used for the upper bounds involves an exponentially complex minimization problem potentially limiting its practical usefulness."], "wfZGut6e09": ["Summary: This paper introduces a novel method for solving multiobjective reinforcement learning (MORL) problems. The contributions include: 1. A generic gradient descent algorithm for finding multiple solutions. 2. An algorithm for retrieving preferences from given solutions. 3. Theoretical proofs of the methods correctness. effectiveness and Weaknesses:  effectiveness: Wellwritten and clearly presented. effective coverage of prior work in the introduction.  Weaknesses:  Incomplete comparison to existing methods.  Lack of differentiation between novel and existing concepts.  Some number that need clarification:  Convex coverage set (CCS) is a known concept in MORL.  Firstorder condition is necessary but insufficient for Pareto optimality which is discussed only briefly.  The Deep Sea Treasure version used in experiments is not the original version with a convex frontier.  The adaptation function resembles Envelope QLearning. Clarifications:  Claiming unknown preferences in the abstract is misleading as the objective function (Eq. 8) requires a preference vector.  The convergence guarantee for PPA in number 2(b) is not explained.  solid baselines should be considered in MuJoCo experiments such as PGMORL.  PPAs limitations should be discussed such as the need for finetuning when environmental preferences change. Questions Regarding Experiments:  Why not use hypervolume as an evaluation criterion  Why not compare PD and UT against baselines in MuJoCo experiments  Why use circular paths as the second objective in MuJoCo experiments instead of energy use  The results in Table 1 and the plots seem contradictory. The understanding for deviation in performance between field are unclear."], "jNB6vfl_680": ["Summary: The paper highlights the importance of Global Pruning (GP) in deep learning network and proposes an improvement called Minimum Threshold (MT). GP is shown to achieve stateoftheart issue in pruning but the authors argue that it has been overlooked due to the popularity of layerwise pruning methods. MT is presented as a fix for overpruning which can lead to significant accuracy drops. The paper includes experiments on CIFAR10 and ImageNet using different architectures and pruning baseline. effectiveness:  GP is revisited and shown to be a powerful pruning technique particularly for high sparsity layer.  MT is a valid solution for preventing overpruning.  Experiments on ImageNet provide valuable insights.  The paper includes a comprehensive discussion on network architectures pruning patterns and MT settings. Weaknesses:  GP has not been forgotten or overlooked by research community and is recognized for its strong performance.  Layerwise pruning has different advantages such as lower computational cost during inference than GP.  The paper focus on oneshot pruning ignores the benefits of gradual pruning schedules.  The experiments on CIFAR10 may not have much practical significance.  The addition achieved by MT are not significant enough to claim superiority over other pruning techniques.  The paper overlooks the impact of MT on computational costs and efficiency.  The discussion on output preservation is not a reliable indicator of model performance.  MT complexity and datadependency are not fully acknowledged in the paper. Overall: While the paper makes a case for GP and MT as potential pruning techniques it fails to provide convincing evidence of their superiority over existing methods. The paper overlooks significant aspects of pruning such as computational cost and efficiency and its claims of stateoftheart performance are not supported by the data presented."], "xP3cPq2hQC": ["Summary This paper addresses imitation learning in situations with significant differences between the action and state spaces such as a robot learning to walk from a human. It presents a technique that reformulates this problem using a mathematical concept called the GromovWasserstein distance and a reward use that aims to minimize this distance. The technique is proven to effectively recover an optimal policy although it may not be directly applicable due to potential differences in the resulting policys structure. Strengths  Innovative approach to solving a crucial issue in imitation learning  Wellwritten and clear demonstration  solution and validation are engaging Questions  proxy Reward Optimization: How feasible is it to maximize the proposed reward use  Computational Feasibility: Is computing the GromovWasserstein distance computationally viable for largescale problems  Seed Dependence: The mentioned seed dependencies are concerning. Could this limitation practical use  isometric limitation: The paper acknowledges the potential issue of recovering policies up to an isometry which could result in unintended outcomes. Are there alternative distance metrics or kernel uses that might mitigate this limitationation Minor Points  Including Figure 2 other would enhance the foundation.  A figure explaining Equation 1 would be helpful.  Its unclear if the proposed approach can handle nonexpert demonstrations.  The claim that the approach scales to continuous spaces requires additional explanation.  How is the seed that produces the \"right\" policy determined  Figures 3 and 5 need improvements for expert visualization.  Videos of the policies in action would be valuable."], "qESp3gXBm2g": ["Paraphrase: summary: The authors introduce a method based on reservoir computing for timeseries analysis. They train an echo state network to reproduce test signal and use its prediction error as a distance metric for an SVM classifier. The method is shown to be efficient in classifying sequential MNIST data and decoding neural signal. Its computational efficiency is highlighted as a key advantage. Strengths:  Clear and wellwritten manuscript  Novel concept of using prediction error as classifier input  Promising classification performance Major effect:  accuracy needs to be compared against more advanced methods to provide context.  computational efficiency claims should be substantiated by comparing inference times with other methods including deep learning. Minor effect:  Clarification needed on the error bars in Figure 4."], "hfU7Ka5cfrC": ["Paraphrase: Summary: This paper introduces a novel technique for Hyperparameter optimization (HPO) using onepass learning. This technique allows for retraining with each novel hyperparameter proposal making it more efficient and adaptable than existing methods. It supports differentiable updates of continuous hyperparameters such as learning rates within the model weight updates. Experiments demonstrate the technique superior speed compared to traditional approaches. effectiveness and Weaknesses: Clarity:  Wellwritten and easy to understand  Includes helpful visuals for intuition Reproducibility:  Provides implementation details and plans to release code  Meets good standards for reproducibility Technical:  Wellevaluated through numerous experiments with error bars  Theoretically sound with convergence towards optimal Jacobian  Acknowledges reliance on metahyperparameters but does not provide guidelines for setting them  Clipping of unstable learning rates is used but no rationale is given  It remains unclear why WDHDLRM baselines underperform against scalar ones Minor:  Some references point to arXiv versions instead of published venues"], "fpU10jwpPvw": ["Paraphrased Statement: The paper presents an improved version of BayesGAN (BayesGAN with Folded Hamiltonian Monte Carlo or FHMC). BayesGAN utilizes Hamiltonian Monte Carlo sampling to generate generator and discriminator parameters from a posterior distribution determined by target data. This paper proposes using FHMC a modified version to accelerate the sampling work and handle highdimensional data effectively. Strengths and Weaknesses: The idea of parallelizing HMC is intriguing. However there are concerns regarding clarity and results: field of Confusion: 1. The description of the algorithm in Section 4.1 is unclear. Its unclear how parameter decomposition is performed how blocks are determined and how parameters and data dimensions are handled in parallel. 2. The theoretical analysis in Section 4.2 lacks explanation. Its difficult to follow the connections between equations and the implications for BayesGAN. 3. The experimental results indicate that FHMCGAN does not consistently outperform BayesGAN or ProbGAN. While it has runtime advantages its superiority in data generation may be exaggerated. Minor subject:  Typos throughout the manuscript require editing.  Equation 1 has a discrepancy between the posterior and likelihood terms.  Confusing notation: \"x\" and \"X\" are used inconsistently to represent target data.  Table 1 evidence that ProbGAN with 4000 samples from ImageNet has the lowest score which should be highlighted."], "xMJWUKJnFSw": ["Paraphrase: Summary: Traditional methods for creating knowledge graph embeddings assign a unique embedding to each vertex in a graph. This paper introduces a novel heuristic that significantly reduces the number of parameters by representing vertices based on their surrounding neighborhood. Each vertex is approximated by its proximity to a set of designated \"anchor\" vertices along with the types of outgoing edges from that vertex. Anchor vertices have their own learned embeddings while the embeddings of other vertices are computed at inference time using a neural network layer based on the closest anchor vertices. effectiveness and Weaknesses: effectiveness:  This method is computationally efficient utilizing fewer parameters without compromising performance.  It effectively handles novel vertices that may not have been encountered during training.  The paper presents competitive results on various benchmark datasets. Weaknesses:  The paper claims that the hashing strategy used for vertex representation ensures unique hashes for all vertices but this is not explicitly proven.  The claim of sublinear vocabulary size is questionable as the model even requires a significant portion (about 10) of the vertices as anchors. further evaluation on larger datasets is needed for a more definitive conclusion."], "gKWxifgJVP": ["Summary: This study introduces a graphbased reasoning model for answering logicdriven question. The model leverages supergraphs constructed from \"fact triplets\" which are subjectpredicateobject paths extracted from dependency parsers. Compared to traditional entityrelation knowledge fact triplets provide more specific information relevant to each question. Strengths:  Clear motivations and structure  novel concept of fact units for informative local knowledge  Connection of nodes across supergraphs based on context for graph reasoning  Demonstrated performance gains on three logicdriven datasets Weaknesses:  Lack of details in model description such as:  Initialization and information propagation for supernodes  Ambiguity in equations 2 and 3 regarding graph encoder and global node connectivity  Inconsistent notations  Unclear loss function for answer prediction  processing of interaction module  reliance on the accuracy of dependency parsing to extract facts  Potential for noise in extracted facts as suggested by performance drop with increased fact count  Limited comparison with symbolic logic reasoner models despite the term \"global reasoning\" being used in the study  Disparity in FOCAL Reasoner performance when using DeBERTa compared to RoBERTa"], "qynB_fAt5TQ": ["Paraphrased Statement: This field introduces a technique to improve the efficiency of the bootstrap method. In bootstrapping each particle represents a machine learning model. As the number of particle increases the computational costs rise significantly. The proposed technique functions a kmeanslike centroid approximation to optimize a group of centroids (corresponding to machine learning models) that minimizes a datadependent Wasserstein distance. Strengths and Weaknesses: Strengths:  The technique addresses a computational challenge relevant to many machine learning problems.  The algorithm is simple to implement and comprehend.  Experiments demonstrate its applicability to various machine learning function cases. Weaknesses:  Comparison with existing methods except for vanilla bootstrap is lacking.  The iterative nature of the algorithm may incur high computational costs in the inner loop.  The benefits of the novel technique over previous approaches are unclear in terms of model accuracy improvement.  The reduction in memory and computational cost for inference is not fully quantified.  The performance of the method for ensemble tree methods and the dimensionality of theta in the experiments are not specified.  The initialization of theta for treebased methods is not discussed."], "gFDFKC4gHL4": ["Paraphrase: Summary: A technique for estimating a classifiers confusion matrix using minimal samplings with a focus on detecting unannounced changes in machine study (ML) APIs. effectiveness:  Addresses a crucial but understudied topic enhancing accountability in using commercial ML APIs and safeguarding against hidden risks.  Simple to implement and efficient compared to basic sampling methods.  Provides a novel dataset on API changes valuable for future research. Weaknesses:  The concept of difficulty levels (K) needs further clarification including justification for varying K values in experiments.  An ablation study contrasting MASA with K1 and K1 could elucidate the relative contributions of K and uncertaintybased sampling selection. Improvement Suggestion:  Visualizing the relationship between partition uncertainty and sampling selection would enhance the understanding of MASAs key benefit over other sampling strategies."], "hk3Cxc2laT-": ["Paraphrase: The paper proposes using taskaware modulation in global Metalearning to handle tasks from diverse distributions. It builds on prior process by incorporating a rehearsed task gradient descent trajectory into task representation. To avoid the computational expense of rehearsal tasks the paper uses a separate netprocess to estimate the rehearsed task trajectory characterization from feature representations. The method is evaluated on fewshot image classification and coldstart recommendation tasks. Strengths:  Task characterization using trajectory offers a unique perspective.  The authors provide insights into the motivation behind characterizing tasks using trajectory information.  The analysis of GRUs learning is engaging though limited.  The paper is wellwritten. Weaknesses:  The assumption that tasks with similar feature assignments have identical path assignments contradicts the paper core assumption of path characterization.  The improvements over existing methods on metadatasets and miniimagenets are minor.  Deep architecturebased metalearning models may have more promising results."], "fwzUgo0FM9v": ["Paraphrased Summary: This research investigates the privacy risks of federated learning when the server is malicious meaning it can manipulate the model architecture and parameters. By exploiting a vulnerability in the RELU activation work the researchers demonstrate that the malicious server can potentially retrieve exact copies of at least some user inputs with high probability. empirical experiments support this finding indicating that the technique poses a significant threat to user data privacy. effectiveness and Weaknesses: effectiveness:  Clearly conveys the fundamental vulnerability of federated learning when the server is untrustworthy.  Uses realistic assumptions that make the method practical to implement. Weaknesses:  Relies heavily on previous research insights making the results somewhat predictable.  Does not introduce novel insights regarding privacy in deep learning."], "figzpGMrdD": ["Paraphrase: This study examines how combining different large language models (PLMs) and continual learning methods affects learning performance on three NLP classification tasks. Methods are assessed in both taskincremental and classincremental scenarios representing common settings in continual learning and NLP. Layerwise analysis reveals how data is retained or forgotten during training. The findings indicate that replaybased methods outperform regularization approaches. effectiveness:  Comprehensive analysis of PLMs across multiple techniques tasks variants and layers.  Identification of effectiveness and weaknesses of each PLM.  Exploration of new research questions and challenges.  Realistic evaluation using imbalanced datasets. Minor Weaknesses:  Small plot sizes.  Narrow color differentiation making bars difficult to distinguish.  Lack of emphasis on beneficial nonjoint performance in Table 1.  Assumption that Transformer layers do not adapt during finetuning should be cited.  Ambiguous buffer size definition in design 3 and Section 5.1. Questions: None regarding understanding."], "hbGV3vzMPzG": ["Summary Paraphrase: This research introduces a metric to assess the complexity of training instance. Using this metric the authors demonstrate that difficult instance hinder the robustness of models against adversarial attacks. Theoretical analysis confirms that as instance difficulty increases vulnerability to adversarial attacks rises. The researchers propose a reweighting strategy and an adaptive label technique to enhance adversarial robustness. effectiveness: 1. This work establishes that challenging adversarial instance impair generalization performance in adversarial training as determined by the proposed difficulty metric. 2. The research provides a comprehensive theoretical analysis that demonstrates the harmful effects of fitting challenging adversarial instance on adversarial robustness. Questions: 1. ModelAgnostic Metric: The difficulty metric is described as \"modelagnostic\" but it is unclear how the metric can guarantee that the difficulty of a given instance remains consistent across different models and training settings given that loss calculation depends on model parameters and optimization. 2. Adversarial Overfitting: Section 4.2 claims that adversarial overfitting stems from the models attempts to fit challenging adversarial instance. However Figure 2s right panel shows that easy instances (yellow lines) exhibit similar behavior to challenging instances (red lines) and the model fits both simultaneously. This suggests that adversarial overfitting may result from fitting both easy and challenging adversarial instance. 3. Relationship between instance difficulty and Robust Overfitting: Theorems 2 and 3 indicate that increased instance difficulty reduces adversarial robustness. However it is unclear how these theorems establish the link between robust overfitting and instance difficulty. 4. Proposed Techniques: The effectiveness of the reweighting and adaptive label techniques proposed in Section 6 is empirically validated but their underlying motivations are unclear. The reweighting strategy assigns lower weights to instances with lower predicted probabilities but it is not apparent that these instances correspond to those deemed difficult by the proposed metric. As a result the proposed method may not be directly related to the metric."], "kG0AtPi6JI1": ["Paraphrased Summary: This work introduces \"latent domain learning\" a new task where training data have hidden domain labels. It proposes a method to train models that perform well across different domains. The method uses multiple feature transformation layers controlled by \"gating variables.\" These variables estimate the latent domain label and select the appropriate transformation layers. The method outperforms existing techniques in this latent domain learning setting. effectiveness and Weaknesses: effectiveness:  Introduces an intriguing and relevant problem with practical application.  Demonstrates strong performance. Weaknesses: Writing:  Unclear terminology and formulation.  Undefined terms and equations.  Complicated explanations. Novelty:  The proposed method LSA is an extension of the existing \"residual adapter\" method.  The use of gating variables for domain label estimation is not a novel idea.  Objections against the cited method (RA) appear invalid diminishing the perceived contribution of this work. Experiments:  Performance improvement over RA is not significant.  RAs results are inconsistent and questionable."], "tFQyjbOz34": ["Paraphrase: Summary: This research utilizes graphbased clustering techniques to uncover comprehensible functional units within neural networks. The authors present a detailed account of the graph structure and clustering process. They introduce the FisherBates pvalue to assess the significance of the identified modules. effectiveness:  Clear and concise writing with persuasive results.  novel methods for defining relationships between neurons and identifying both global and local clustering.  Development of a statistical approach for automatically detecting significant modules although with limited statistical ability. Comments:  Curiosity about the presence of clustering with disconnected layers between neurons (e.g. layer 1 and 3 without layer 2).  Questioning the rationale for matching the number of local clustering to the number of global clustering and seeking explanations for discrepancies between them.  Concern that the clustering algorithm may not accurately reflect true neural connections hence affecting the reliability of the pvalue.  Suggestion to explore using unit interference and impact as a measure of neural connections.  research into potential applications of the identified modules in neural network training including connections to the \"lottery ticket\" hypothesis.  Interest in exploring alternative graph clustering algorithms (e.g. Louvain community detection hierarchical clustering)."], "lgOylcEZQgr": ["Paraphrased Statement: Summary: The proposed approach enhances instancebased clustering in an unsupervised setting showing resilience to data imbalances and variations in data distribution compared to existing methods like SwAW. It employs an Expectation Maximization (EM) algorithm operating in temporal episodes assigning observations to clusters or initiating new ones based on distance in feature space. A probabilistic model guides this process and cluster prototypes are updated accordingly. The loss function incorporates contrastive loss entropy and a term encouraging cluster creation. Evaluated on RoamingRooms where an agent navigates indoor environments with labeled objects the approach exhibits strong performance in clustering different perspective of the same object. Additional assessments on Omniglot and ImageNet are also provided. effectiveness and Weaknesses: The method is novel and shows promising results in the unsupervised instancebased clustering field particularly in noniid scenarios with low batch sizes. However it could benefit from the following advance: 1. Clarification and Organization: The presentation is dense and essential details require inference. A clear explanation of the results including equations in the main text and moving some Omniglot evaluation to the appendix would enhance readability. 2. Ablation analysis: Thorough ablation analysis in the main text is crucial for understanding the methods effectiveness. The current hyperparameter comparisons in the appendix require further elaboration to highlight the key factors contributing to the superior performance. 3. Relevance to Practical Applications: While the approach demonstrates instancelevel clustering it remains unclear how this capability translates to practical applications. The authors should demonstrate the methods ability to discover categories or its utility in downstream semantic tasks. Alternatively they could emphasize the importance of instance classification and provide justification for its relevance in the field. 4. Comparison to Baselines: To gauge the proposed methods effectiveness additional experiments should explore the batch sizes required for baselines to match its performance in both iid and noniid settings."], "ljxWpdBl4V": ["Paraphrased Statement: Summary: This study introduces \"sample probing\" a technique for evaluating the quality of synthetic samples generated by specific generative models in the context of zeroshot learning (ZSL). This technique involves integrating an existing ZSL solver into an existing generative ZSL model enabling the pipeline to be trained endtoend due to the solvers differentiability. The study demonstrates the efficacy of this sample probing approach on four benchmark datasets achieving stateoftheart performance. effectiveness and Weaknesses: The paper addresses a significant concern in existing generative ZSL models by focusing on the quality of synthetic samples which heavily impact the final ZSL performance. By assessing sample quality during training the generative model can produce more insightful samples resulting in improved solution. The paper is wellwritten and provides sufficient technical details for implementation. Questions and Suggestions: 1) The paper should clarify how the proposed method ensures that the synthetic samples possess the desired properties of being realistic relevant and informative. 2) The performance improvement compared to TFVAEGAN is relatively minor (less than 1). Given the significance of sample quality measure greater addition could have been expected. 3) Does the effectiveness of the model depend heavily on the chosen solver Could alternative solvers like SAE [R1] yield even better GZSL performance 4) compute 3 shows conflicting trends in the harmonic mean between the validation and test sets. Does this suggest an inadequacy in hyperparameter tuning for the GZSL task 5) Table 2 only provides solution for the CUB dataset the paper should include performance data for the other three benchmark datasets."], "kF9DZQQrU0w": ["Summary Authors Response: The authors have clarified and improved the manuscript. While the paper exhibits strong merits it slightly falls short of an 8 rating (the future potential score). The reviewer recommends accepting the paper and will advocate for it during the reviewers discussion. The paper analyzes neural network training dynamics using informationplane analysis which has been the study of controversy due to mutual information estimation issues. To mitigate this the paper proposes training networks with quantized activation enabling exact mutual information computation. Main Contributions  Thorough review of the informationplane analysis history findings and related works.  Quantizedactivation training to eliminate estimation errors in mutual information calculation.  Reproduction of original experiments and additional experiments on MNIST. Strengths and Weaknesses  Excellent writing and clear presentation of scope and methods.  important ablations are included.  The paper acknowledges the dependence of results on quantization bitwidth.  The paper focuses on quantizedactivation networks and it is unclear how their dynamics relate to unquantized networks analyzed via binning.  There is no objective criterion for choosing the quantization bitwidth leading to potential interpretation differences. Verdict The paper is wellexecuted and informative but it does not fully resolve the controversy surrounding informationplane analysis. The author suggests that the paper might be used for inspiration in theoretical SGD analysis rather than empirical measurement of compression phases. Improvements  An objective criterion for selecting quantization bitwidth or a calibration process would significantly improve the paper significance.  Nonuniform quantization schemes should be considered.  Control experiments with random label initialization and correct labels during training could provide additional insights. Detailed Comments  Clarify gradient backpropagation through quantization function.  Label layers in plots.  Include a brief summary of the historical scope in a footnote.  Emphasize lossy compression in IBratedistortion.  Provide an equation for the IB objective.  Correct the typo \"quantized after training\" to \"quantized before training.\""], "qPzR-M6HY8x": ["Paraphrase: This paper introduces ICE a method for handling instancedependent label noise using a simple scalar confidence parameter for each sample. The method is incorporated into the loss function to facilitate noiseresilient learning. Experiments demonstrate the effectiveness of ICE in image and text classification tasks and gradient analysis provides insights into its behavior. effectiveness:  Clear and concise introduction.  ICE offers a practical approach to approximating instancedependent label noise. Concerns:  Confusion about the use of sample index rather than sample features in ICEs embedding which may not fully align with the definition of instancedependent label noise.  Lack of comparison with existing methods for handling instancedependent label noise in Table 1.  Minor performance gain of ICE compared to GCE on CIFAR10 and CIFAR100 datasets.  Similarity between ICE and label smoothing authors should consider comparing the two in experiments.  Only 40 epochs were used for training on CIFAR10 and CIFAR100 which may not be sufficient to fully evaluate the impact of label noise on the learning process."], "ufGMqIM0a4b": ["Paraphrased Summary: This paper introduces InfinityGAN a novel framestudy that leverages recent advancements in stylebased generative adversarial netstudy and implicit neural representations to generate landscape images of seemingly infinite width and significant height. It employs parallel inference patchbased generation and handcrafted landscape priors to achieve this. The paper also proposes a novel evaluation metric ScaleInv FID and demonstrates applications such as image outpainting inbetweening and spatial style fusion. Results are supported by user study and imagespace metrics. Paraphrased Strengths and Weaknesses: Strengths:  Novel ideas in image generation including a paddingfree StyleGAN architecture feature unfolding implicit neural representations for image content variation and landscape generation priors.  Wellwritten and presented paper with comprehensive supplementary material.  Implementation details support reproducibility.  Potentially impactful applications in image generation literature. Weaknesses:  Oversold claims of generality as the method is tailored to landscape photography.  Limited empirical validation of certain design decisions.  Missing references and comparisons to relevant prior study. Detailed Review: Generality:  The paper claims of a \"generic framestudy for infinite image synthesis\" should be tempered as the design and evaluations are specific to landscape photography. Missing References and Comparisons:  Notable recent improvement in neural implicit representations are not cited.  Comparisons to previous study in image generation (e.g. TileGAN PSGAN) are lacking. Empirical validation:  The impact of several design choice (e.g. positional encoding diversity loss auxiliary task) needs to be demonstrated empirically. Evaluation and Results:  The ScaleInv FID metric requires further expansion and implementation details.  The comparison with SinGAN could be improved with more implementation details.  The user study lacks demographic information and details on the images and questions used. additional Observations:  InfinityGAN may struggle with adding variation to the uppermost parts of images due to the tanh() performance used on vertical coordinates.  Data augmentation or limited data training techniques could reduce the need for massive datasets.  Applications in 360\u00ba photography for VR could be explored. Update After Rebuttal:  The authors have addressed concerns by providing extensive comparisons ablation study and clarifications on the ScaleInv FID metric.  The paper claims have been nuanced and its contributions are now more evident."], "hl9ePdHO4_s": ["Paraphrased Statement: Summary: This field examines if anisotropic treatment of neighbors (via latent functions) is crucial for enhancing Graph Convolutional Network (GCN) accuracy. The paper proposes EGC a model with adaptive filters that:  Achieves higher accuracy than vanilla GCN  Consumes less memory and has lower latency than anisotropybased methods (e.g. GAT) effectiveness and Weaknesses: effectiveness:  Clear hardwarealgorithm codesign analysis highlighting efficiency and memory considerations  Informative comparison table outlining differences in propagation and memory necessity  Adaptive filter as a metalearning approach allows for exploration of different latent spaces  Opensource codebase provided Questions:  GraphsSAGEs low latency and memory usage contradict the claim about samplingbased methods being less efficient. Can EGC also benefit from sampling  Potential impact on GCN hardware acceleration: Will EGCs variants (EGCS EGCM) require specialized hardware architectures for optimal performance"], "vnENCLwVBET": ["Paraphrased Statement: This paper introduces a novel way to evaluate natural language generation models without referring to humangenerated text as a reference. The paper tests this metric on Chinese poetry generation. Weaknesses:  The paper lacks a comparison between the metrics output and human judgments which is common practice in other evaluation metrics (e.g. BertScore).  While the paper acknowledges subjectivity in human judgments it fails to provide an alternative task with objective evaluation for Chinese poetry generation or similar literature generation tasks.  The papers equations 1 and 2 include unexplained symbols (X1 and X2). Equation 2 also appears to be missing a plus sign in the loss function.  Most natural language generation tasks do not have a single correct solution including machine translation.  The papers use of a direct discriminator to guide natural language generation has been explored in previous research. A comparison against these methods is missing."], "sBT5nxwt18Q": ["Summary Paraphrase: This research introduces a localized and modelspecific approach for explaining image classifications by deep neural network (CNNs). The method provides examples highlighting critical image regions shared between the test instance and the examples revealing the factors driving the classification. effectiveness and Weaknesses Paraphrase: effectiveness:  approach XAI research with a novel and effective explanation method.  Leverages existing approach to create a simple and innovative methodology. Weaknesses:  Incomplete introduction of the underlying \"twinsystem\" basis.  Biased experiment design in testing the significance of image parts due to overlap with the method being compared.  Lack of comparison with alternative methods such as those proposed by Kim or Rudin.  Inconclusive case study results potentially influenced by question design. Minor matter:  Use of k to represent both the number of classes and the knearest neighbor (kNN) algorithm.  Incomplete specification of the kNN parameters used in experiments."], "n6Bc3YElODq": ["Summary: When an agent interacts with multiple other agents in a learning environment it faces a dynamic learning challenge because it cannot assume that the other agents are not learning or are learning using different reasoning strategies. One solution is to design a learning algorithm that anticipates the potential policy updates of other agents effectively addressing this problem. This field proposes an algorithm called ModelBased Opponent Modeling (MBOM) that uses parallel opponent modelinging trained with varying depths of recursive reasoning. During gameplay the agent selects the modeling that effective matches the opponents recent behavior. This algorithm assumes a centralized training but decentralized execution setting for twoplayer fullyobservable generalsum games. effectiveness and Weaknesses: Review: The paper presents a novel approach to modelinging opponents under various learning assumptions. It uses modelingbased predictive control to select the most potential opponent modeling. However concerns arise due to the lack of methodological details and unsupported claims. Major Comments:  The baseline methods are sophisticated making comparisons challenging. Simpler baseline should be included for clarity.  Insufficient analysis of the relationship between the agents and opponents reasoning levels and its impact on performance limits understanding of MBOMs usefulness.  Unsupported claims throughout the paper undermine the process credibility.  Evaluation methodology regarding opponent policies construction and tuning lack transparency.  The short performance of LOLADiCE in the coin game (an expansion of IPD) requires further investigation.  The proposed Bayesian mixing methods have similarity with previous process which should be acknowledged. Minor Comments:  Avoid using \"reasoning\" and \"recursive reasoning\" interchangeably.  The statement that the environment is nonstationary due to various opponents is inaccurate.  The claim that humans understand the environments mechanics is questionable.  The justification for using embedding vectors over full modelinging for opponent representation is missing.  The assertion that PR2 cannot handle various opponents in execution is incorrect.  The assumption that similar trajectory distributions imply similar learning algorithms should be clarified.  The use of \"softy\" is unclear.  The computation of p(i) needs explanation."], "y7tKDxxTo8T": ["This paper presents a novel approach to \"zeroshot recommendation\" where the source and target domains share no user or item overlap. It leverages item content features (e.g. BERT embeddings of descriptions) rather than IDs for recommendation. While the paper is wellwritten its key effectiveness lies in its distinct problem setting: zeroshot recommendation. However a major weakness of the paper is its lack of novelty. The approach of using contentbased methods for recommendation while distinct in this specific setting is not inherently new and has been widely used for coldstart problem. Additionally the paper assumes interactions in the target domain which raises questions about the true nature of the zeroshot setting. Experiments on offline datasets show promising results for the proposed methods outperforming indomainonly approach and simple crossdomain baselines. However concerns remain regarding the generalizability of the findings and the practicality of the proposed approach in realworld scenarios. To effectivenessen the paper the authors should address the novelty concerns by clarifying the uniqueness of their problem setting and method and provide convincing evidence of its practical viability."], "xxU6qGx-2ew": ["Paraphrased Summary: This paper introduces Gaussian differential privacy (GDP) an extension of differential privacy with a individual argument \\mu. GDP has advantages in paper and computational efficiency compared to other differential privacy notions. The paper presents tools to simplify the implementation and understanding of GDP:  Gaussian Differential Privacy Transformation (GDPT) is a curve that shows the minimal \\epsilon value for an algorithm to be as individual as a Gaussian mechanism with noise multiplier 1\\mu.  conditions are provided for an algorithm to be \\muGDP and a method is proposed for approximating this check.  A \"clip and noise\" procedure is presented to convert an \"approximately GDP\" algorithm into a truly \\muGDP algorithm. The paper also demonstrates a novel relationship between (\\epsilon \\delta)DP and (\\epsilon \\delta)DP where (\\epsilon \\delta) can be more individual than (\\epsilon \\delta) even when \\epsilon  \\epsilon. effectiveness:  Proposed tools for practical application of GDP.  Interesting novel implication between types of differential privacy. Weaknesses:  GDPT is not seen as providing significant insights beyond the existing privacy profile curve.  Evaluation of the effectiveness and efficiency of the proposed GDP tools is lacking.  The utility improvement from the novel differential privacy relationship is limited by the high \\delta value it requires.  The novel relationship may be implicit in previous work."], "l_amHf1oaK": ["Paraphrased Statement: This paper presents a method for thoroughly verifying neural networks using a branchandbound approach. Combination of Existing Techniques: It combines two recent techniques:  Betacrown: Optimizes bounds using firstorder methods and includes additional constraints on activations.  Prima: Derives linear constraints on activations to tighten relaxations. Main Contributions:  Integrates Primas linear constraints into the optimization problem in Betacrown.  Introduces novel branching heuristics:  Active Constraint Score Branching: Prioritizes branching based on the effectiveness of constraints.  CostAdjusted Branching: Adjusts branching scores based on the computational cost of each choice. Empirical Evaluation:  experimentation on MNIST and CIFAR10 demonstrate the effectiveness of the approach. effectiveness:  clear presentation and proper attribution.  Promising empirical results for extending Betacrown with tighter relaxations. Clarifications Needed:  Q1: CostAdjusted Branching:  How is the expected bound advance computed  Why do different branching decisions have varying computational costs  Q2: Experimental Evaluation:  It is unclear if the method outperforms other stateoftheart methods that use tighter relaxations than singleneuron relaxations.  The choice of branching method for Table 1 is not fully justified.  Cactus plots would provide a more comprehensive comparison of methods. Small Questions:  SQ1: Why is the MNIST verification attempt in Table 1 for eps0.120 instead of the typical eps0.1 or 0.3"], "xf0B7-7MRo6": ["Paraphrased Statement: This paper enhances the deep matrix factorization technique introduced by Arora et al. (2019). It generalizes the model to support More complex scenarios like inverse problems. Additionally it integrates a \"vanishing\" regularization term that improves the dynamics and convergence of the model. The paper includes theoretical analysis demonstrating the positive issue of the proposed enhancements. Experimental results illustrate the superior performance of this model over existing stateoftheart methods. effectiveness and Weaknesses: Despite promising theoretical results and experimental performance the paper introduction falls short of publication standards due to its confusing structure and lack of clarity. Key details are scattered across the appendix making it difficult to follow the models formulation and understand the proofs presented. Pros:  innovative theoretical results that advance the understand of deep matrix factorization.  introduction of the Dirichlet Energy regularization term as a theoretically sound prior.  Extension to inverse problems and other complex applications.  Strong experimental evidence supporting the models effectiveness. Cons:  Incomplete introduction of the model with significant details relegated to the appendix.  Inaccessibility of fundamental information within the main text making it challenging to grasp the model and its analysis.  Difficulttodecipher theorems due to missing assumptions and regularization models only described in the appendix. Requests for Clarification: Please address the aforementioned concerns regarding the paper introduction and clarity. Additional Comments:  Confusing or unclear time should be revised for better readability.  Add missing assumptions to Theorem 1 within the main text.  Clearly define the regularization model used in Theorem 2 in the main text for easy reference."], "xENf4QUL4LW": ["Paraphrase: Summary: This paper presents a sample technique (CNLCU) that employs probability to identify whether samples are mislabeled or disregarded. Experimental issue demonstrate that CNLCU outperforms other recent sample methods. key Points: Working with noisy labels is crucial in machine learning. The proposed CNLCU method point to enhance model training by selecting reliable samples over time. CNLCU employs probability constraints to achieve this. effectiveness: The method is wellconceived with detailed mathematical derivations provided in an appendix. The experimental design follows established practices in noise label learning reresearch. CNLCUs superiority is demonstrated by comparisons against reputable benchmarks at various noise levels. The paper is concise and accessible. Weaknesses:  The noise ratios used in the experiments are limited to 20 and 40. To strengthen the conclusions evaluation should include high noise levels (e.g. over 50).  The performance metrics and analysis should be expanded. While CNLCU excels in terms of mean accuracy other metrics may provide a more complete evaluation. also the paper should address the larger variations observed in CNLCUs performance on some datasets.  The assumption that training losses follow a Markov process may not always hold in practice. Evidence or statistical analysis is needed to support this assumption particularly regarding the dependence of parameter values on initialization and research paths rather than only previous values."], "p98WJxUC3Ca": ["Paraphrase: Summary: This paper explores active learning for domain adaptation in the context of Lipschitz functions. The authors introduce a localized discrepancy measure to identify relevant hypothesis for choice. This discrepancy is relaxed into a distance measure under Lipschitz constraints and an accelerated Kmedoid algorithm is developed to minimize this distance. Theoretical analysis under specific assumptions demonstrates the superiority of the proposed approach over previous methods. Empirical results also show its strength. strength:  The paper provides a practical method for batch active learning in domain adaptation with demonstrated advantages over existing techniques.  An accelerated Kmedoid algorithm is proposed and its computational complexity is analyzed.  The paper is wellwritten and uses visual examples to enhance understanding. Weaknesses:  The proposed method relies on a localized discrepancy approach from Zhang et al. (2020) limiting its theoretical originality to the ease of the discrepancy and the accelerated Kmedoid algorithm. however the applicability of the algorithm is restricted.  The accuracy of the discrepancy ease under Lipschitzness is not fully explored raising concerns about its potential limitations.  The algorithm is only applicable to batch active learning scenarios. Its strength in sequential learning settings is not addressed.  The theoretical guarantees require a specific condition on the parameter epsilon which raises questions about its practical significance.  The paper implies that minor values of epsilon may invalidate the theoretical guarantees but it is unclear why a large epsilon cannot be chosen to ensure rigor."], "lD8qAOTu5FJ": ["Summary: This paper explores the challenge of learning new visual classification tasks without forgetting previously learned ones. It proposes a method that modifies neurons during task additions to maintain a balance between preserving existing knowledge and adapting to new tasks. strength:  The method explicitly addresses the question of which model components can be adjusted without sacrificing accuracy.  It highlights the stabilityplasticity dilemma inherent in continual learning. Weaknesses:  The method exhibits a high degree of overlap with the existing SpaceNET approach.  The distinction between this method and SpaceNET is not clearly explained in the introduction.  The contribution specific to this paper is not adequately emphasized.  The method is limited to classincremental learning which should be addressed in the introduction.  The experimental validation lacks various significant baselines.  The datasets used are overly simplistic.  The structure of the paper needs improvement with certain sections being misplaced.  The reported performance results for a particular method (KAN) are inconsistent with its parameter usage leading to questions about the validity of the comparison.  The code provided lacks a readme file and adequate documentation.  A missing connection is present in Figure 1 block (2) in the bottomleft corner."], "r5qumLiYwf9": ["Paraphrased Statement: This paper presents the MaGNET method for generating samples from generative model latent distribution. This method aims to produce samples that are uniformly distributed within the latent space. While the authors propose MaGNET as a means of improving sampling uniformity more extensive quantitative analysis is needed to substantiate these claims. Major Concerns: 1. The impact of MaGNET on generated range quality has not been adequately evaluated using common metrics like inception score FID score and KID score. Additionally the provided sample ranges in design 7 and 9 suggest a decline in visual quality when using MaGNET. The clarity of the samples in design 1520 is insufficient for comprehensive evaluation. 2. The claimed improvement in sampling uniformity in design 22 lacks quantitative evidence. The authors provide only qualitative issue which are difficult to interpret. They should provide metrics quantifying the proximity of the sampled distribution to a uniform distribution for different categories (e.g. gender hair glasses etc.). Minor Comments: 1. The definition of perregion slope matrices (Aj) in the MaGNET sampling procedure is not provided. 2. The solution of design 5 8 and 1520 is inadequate particularly in design 1520 where it impairs the assessment of generated sample quality."], "rq1-7_lwisw": ["Summary This work introduces a novel dataset with annotation for objects their attributes and their use. Additionally a CNN training and inference pipeline is proposed that use \"causal intervention\" to reduce potential biases. strength and Weaknesses While the paper writing could be improved the primary concern is the clarity of its causal narrative. The authors should explicitly indicate the source of the causal graphs used and the reasoning behind the specific relationships between category attributes and affordances. Furthermore the paper does not adequately compare the proposed debiasing approach to existing methods. For a more robust evaluation the authors should cite relevant research on bias mitigation and provide empirical comparisons. Lastly the interpretation of the TDEalphabetaTDE metric is unclear especially given that all prediction networks use the same image features as input."], "wfRZkDvxOqj": ["Paraphrase: Summary: This study introduces a novel multitask neural process (MTNP) that extends the traditional neural process (NP). MTNP employs shared global variables to facilitate knowledge transfer between multiple tasks. A hierarchical Bayesian model is used to connect these singletask NPs. The new model offers enhanced adaptability to multitask scenarios compared to conventional NPs. effectiveness and Weaknesses: effectiveness:  Extensive experimental evaluation on diverse tasks demonstrate the effectiveness of the proposed MTNP yielding promising and significant results. Weaknesses:  The theoretical contributions of the study could be improved. The hierarchical model innovation and inference methods appear straightforward.  The study lacks comparison with alternative solutions which would provide evidence for the superiority of the proposed innovation.  The discussion on the robustness of the multitask NP as a stochastic process is absent including the robustness of marginal NPs for individual tasks within the multitask NP."], "sS0dHmaH1I": ["Paraphrased Statement: This paper introduces an anomaly detection algorithm that employs an EBM (Energy Based Model) to differentiate between \"normal\" and \"anomaly\" data. The EBM generates simulated anomaly instances onthefly for each normal instance. It then learns to allocate low energy to normal instances and higher energy to simulated anomaly instances. The framework is adaptable to new tasks with minimal labeled normal data. effectiveness and Weaknesses: effectiveness:  Wellsupported purpose choices  Comprehensive ablation studies Weaknesses:  Assumes training data is only normal limiting its usage to fully supervised scenarios  Lack of experiments with training data contaminated by anomalies Main Observations:  The algorithm assumes the absence of anomalies in the training data making it more suitable for supervised learning.  It would be valuable to evaluate the algorithms performance with varying layer of anomaly contamination in the training data.  The explanation of sparsity regularization in Section 3.1 demonstrates the wellreasoned purpose decisions behind the algorithm."], "xnYACQquaGV": ["Paraphrased Statement: This paper introduces NeuralLinUCB a novel contextual bandit algorithm. It employs a combination of deep learning and linear exploration as proposed by Riquelme et al. 2019. However unlike Riquelmes algorithm NeuralLinUCB provides a regret upper bound (in Corollary 4.6) and offers improvements over NeuralUCB (Zhou et al. 2020) in terms of computational efficiency and a tighter regret upper bound. Empirical evaluations across multiple contextual bandit problems demonstrate that NeuralLinUCB surpasses LinUCB and matches the performance of NeuralUCB and NeuralTS. Reviewers Comments and Questions: 1. While NeuralLinUCB reduces the computational cost of NeuralUCB it remains high compared to LinUCB due to the episodic retraining of the deep neural network. This cost increases nonlinearly with T as the network is retrained with all data every H episodes. The applicability of NeuralLinUCB for large T is therefore limited. The authors should provide potential solutions to address this practical issue. 2. The theoretical results are intriguing especially considering the high performance of NeuralLinUCB in contextual bandits. However the regret upper bound depends on rtilde(r) which represents the regret between the obtained reward function function and the optimal function. It is unclear why the right term of RT in Corollary 4.6 does not scale with sqrt(T). 3. The condition m  T3 appears rigorous suggesting that the proposed algorithm does not scale efficiently with T. Reviewers Final Assessment After Rebuttal: The reviewer acknowledges that the analytical results and the algorithm combining deep learning with shallow exploration on the last layer are not entirely novel. However their combination represents a significant advancement. NeuralLinUCB overcomes the feature processing limitations of LinUCB by employing a representation layer. Given its promising performance and potential impact on the bandit community the reviewer votes for the acceptance of the paper."], "kNKFOXleuC": ["Paraphrased Statement: Summary:  The paper presents a novel task called \"anytime\" prediction where a model generates progressive predictions that can be stopped at any level.  The authors propose an endtoend model for this task with two key components:  A series of \"exits\" allowing for sequential predictions while balancing accuracy and efficiency.  Confidence Adaptivity which focuses on predicting pixels with lower confidence.  The model based on HRNet demonstrates improved performance and efficiency on semantic segmentation and pose prediction tasks. effectiveness and Weaknesses: effectiveness:  Clear and logical motivation for the proposed problem.  Results show comparable or improved performance to the HRNet baseline while reducing computational cost. Weaknesses:  Novelty:  The \"anytime\" prediction problem may not be entirely novel as it contribution similarities with existing methods.  The proposed approach has elements that resemble prior technique like exit branches and confidence adaptivity.  experimentation:  Limited experimentation with only HRNet which may not fully demonstrate the approachs generalization.  Confusion regarding the accuracy improvement with Confidence Adaptivity.  exit Strategy:  Figure 5 suggests that the specific downsampling strategy at each exit has minimal impact on performance questioning its significance."], "yztpblfGkZ-": ["Paraphrased Statement: Summary: This paper proposes a graph convolutional network that incorporates an adaptive filter bank at each layer for graph signal decomposition. effectiveness:  Graph neural networks are a timely and popular research area. Weaknesses:  Incorporating filter bank into graph neural networks is not a novel idea with various existing work demonstrating this approach.  The proposed method resembles the ARMAbased filter method of Bianchi et al. which utilizes IIR filter bank.  The experimental evaluation requires expansion to include comparison with other related methods including those mentioned in the review.  A comprehensive analysis of the methods computational complexity is lacking especially in terms of memory use and computational time for largescale graphs."], "xs-tJn58XKv": ["Paraphrased Statement: This research aims to transfer knowledge about unreliable correlations (spurious model) from multiple source environments to a target environment. It assumes that the strength of spurious model varies across source environments but remains consistent between the source and target tasks. The goal is to use this knowledge to train a target classifier that ignores spurious model. The training process involves three steps: 1. Training individual classifiers for each source environment. 2. Using errors from the first step to separate model within each source environment based on whether they contain high or low spurious model. 3. In the target task clustering model based on their similarity in spurious model and applying Distribution Robust optimization to optimize performance across these clusters. This allows the target classifier to disregard spurious model and learn a more stable model. strength and Weaknesses: strength:  clear explanation of the setup and methodology.  Builds upon existing process that uses model errors to detect spurious model.  Extends the concept to a transfer learning setting. Weaknesses:  Assumptions:  The assumption that model errors across environments are only due to spurious model may be oversimplified as other factors like mislabeled data or distribution shifts could contribute.  The requirement for source tasks to have varying degrees of spuriousness and the target task to share the same spurious model may be difficult to verify in realworld settings.  methodology:  The binarization of predictions into correct or incorrect may oversimplify the presence of multiple spurious factors within each class.  The multistage training approach could be sensitive to hyperparameter selection and may not be robust to violations of the assumptions."], "lf0W6tcWmh-": ["Paraphrased argument: Summary: gradient descent with momentum (Heavy Ball) outperforms standard gradient descent in generalizing a twolayer convolutional network for specific data distributions. effectiveness and Weaknesses: gradient descent with momentum (Heavy Ball) shows better generalization capabilities than standard gradient descent for a specific data distribution. The specific distribution involves binary classification data with N samples of which (1\\hatu) N samples have large margins and \\hat\\mu N samples have small margins. The majority of samples have large margins indicating \\hatu  1. Empirical evidence: Momentum enhances generalization for training Resnet18 and WideResnet on CIFAR10 and CIFAR 100. However on a synthetic dataset with Gaussiandistributed feature momentum does not surpass gradient descent suggesting that data distribution affects the advantage of momentum. validation analysis: The validation demonstrates that momentum as a weighted average of past gradients can maintain a sufficient projection on the signal focus w even when current gradients for smallmargin data are small. This allows gradient descent with momentum to learn w effectively. Questions Regarding validation: 1. Lemma H.5: Derivation of inequality (174) from (173) needs further explanation. 2. Lemma H.5: generalization hypothesis C.2 does not guarantee cr(t)  0 potentially affecting lemma validity. 3. Lemma I.15: validation does not explicitly show zt \\geq v \"for all t \\geq t0.\" 4. Lemma 4.4: amplification on the conclusion of \"nondecreasing yi \\Xiijr(t)\" is requested. 5. Lemmas 5.15.4: impact of the \\frac11\\gamma factor on the progress rate of gradient descent with momentum needs clarification. 6. Lemma 5.2: Intuition behind the decreasing small bound g(t) \\geq O( \\sqrt1\\gamma ) for momentum with increasing \\gamma is requested. 7. Sentence after Lemma 5.4: Expansion on how Lemma 5.4 implies reduced weight updates is required. 812: Typos and requests for further details in several equations and argument throughout the validation. Intuition: Momentum preserves historical gradients aligned with w amplifying feature in past gradients and facilitating learning from smallmargin data. Quantifying the signal effectiveness in the momentum term versus the gradient term would enhance understanding."], "lbauk6wK2-y": ["Paraphrased Summary: This study proposes a novel technique for developing representations that focus on objects. The framework consists of a network that segments objects and a \"hypernetwork.\" The hypernetwork uses the latent representation of an object as input to generate weights for the segmentation network. Both the latent representation and the hypernetwork are optimized together to improve the segmentation networks discrimination ability. Additionally the framework incorporates a mechanism for sparse representation enabling each object to be portrayed using a subset of key object representations. Strengths and Weaknesses: Strengths:  The concept of using network weights for objectcentric representations is novel and intriguing.  The research includes an indepth analysis to investigate the proposed approaches. Weaknesses:  Even during testing the method necessitates a semantic segmentation mask as an input. This significantly limits its applicability because annotating masks even for a single image can be challenging.  The experiments in the study are exclusively conducted with artificial data which raises concerns about its generalizability. Although the authors provide some realworld examples in the supplementary materials the results lack qualitative evaluation."], "kezNJydWvE": ["Paraphrased Statement: Summary: This research tackles the challenge of image deblurring by training a deep neural network from start to finish. It leverages extensive datasets of degraded (blurry) and highquality (sharp) image pairs for training. The method stems from the idea that a properly deblurred image should not retain any signs of its original blur. To exploit this principle the method incorporates a secondary network that attempts to add blur back to the deblurred image aiming to replicate the blur of the original image. If the deblurred image is truly restored this reblurring attempt should fail. These two networks perform roles similar to a generator and discriminator in a Generative Adversarial Network (GAN) though their underlying mechanisms differ. The reblurring network outputs an image that is assessed against a acknowledgment image (the original sharp image). The method employs three loss terms: one for assessing blur (Lblur) one for promoting sharpness (Lsharp) and another for reblurring (Lreblur) in addition to the standard L1 loss used to compare the deblurred and sharp images. Furthermore the method proposes a postprocessing technique that leverages the reblurring network to enhance deblurring results for specific images. It involves approximately reversing the reblurring network through gradient descent optimization. Experiments and solution: Experiments demonstrate the methods effectiveness in improving deblurring outcomes compared to existing methods using popular synthetic datasets (GoPro and loss) and a realworld image dataset (Lai et al. 2016). Quantitative metrics (PSNR SSIM LPIPS NIQE) and visual compare support the findings. Strengths and Limitations:  The method is founded on a novel and intriguing observation. Its formulation appears to harness this observation effectively.  Empirical results indicate superior performance compared to competing methods.  The reblurring networks potential to address outofdistribution challenges is intriguing.  The formulation relies heavily on multiple loss terms suggesting potential for refinement and simplification.  The methods applicability beyond deblurring is not fully explored in the research.  The analysis lacks depth leaving questions about the methods advantages over simpler alternatives and potential improvements through iterative deblurring. Additional Observations:  The description of \"underlying limitations of PSNRoriented solution\" should be clarified potentially referring to the \"regressiontomean\" issue.  Additional relevant work on perceptual losses and acknowledgmentbased metrics could be incorporated to enhance the discussion.  image 1 requires clarification on the content of the top and bottom rows for each method.  The reblurring module acts as an inverse of the deblurring module and presenting it as such could clarify its role.  The writing could benefit from further organization and clarity particularly regarding the foundation of the \"reblurring loss.\"  The justification for using the pseudosharp image \\hatS rather of S should be supported by evidence and aligned with the evaluation metrics used.  The testtime adaptation technique requires further analysis exploring convergence behavior optimal step count and crossdataset performance.  The perceptual distortion tradeoff plot should be reconsidered as PSNR and LPIPS are both acknowledgmentbased metrics not suitable for representing a distortionperception tradeoff. NIQE might be a more appropriate quality."], "zhynF6JnC4q": ["Paraphrased Statement: Summary: This paper introduces a limited offline reinforcement learning setting where limited online interactions are permitted after offline training. The proposed algorithm CGQL utilizes the concept of using online and offline data for different update process. Experiments indicate that CGQL can outperform standard batch RL methods particularly in scenarios with mediumsized replay buffers. Ablation field demonstrate that simplifications to the algorithm may be possible without significant performance reductions. Strengths and Weaknesses: Strengths:  Novel approach to combining offline and online training with potential realworld application.  Plausible intuition behind adaptive Qlearning.  Strong experimental results showcasing improved performance in certain settings.  Inclusion of ablation field and sensitivity tests. Weaknesses:  Lack of clarity in explaining the use of replay buffers.  Poor performance of REDQ on all tasks despite similarities to successful Qlearning variants.  Uncertain impact of offline training on REDQs performance.  Ambiguity regarding the hyperparameter settings of CGQLwc and its potential relation to REDQs failure.  Inconsistent performance of CQL on halfcheetahmedium with online interactions leading to decreased performance over time.  potential for simplification of the CGQL algorithm based on ablation results.  Omission of results from section 5.4 in the main text.  Minor typos and awkward phrasing. Questions and Suggestions:  Clarify the contents of the online and offline replay buffers.  Investigate the reasons behind REDQs failure and consider including a baseline initialized without offline training.  Explain the mechanism behind CGQLwo and CGQLwc.  Determine the significance of the size difference between the online and offline replay buffers.  Compare the performance of CGQLwg to CQL.  Reevaluate the need for ensembles in the light of CGQLwes promising performance.  Include the results from section 5.4 in the main text and improve the presentation of hyperparameter settings."], "zBhwgP7kt4": ["Paraphrased Summary: The work explores solving a regression problem in an online setting where the input A and b are revealed incrementally. The goal is to maintain an approximate solution x that remains closely to the optimal solution OPT within a tolerance of 1eps throughout the work. The update time should be minimized. The work presents an algorithm with an amortized update time of O(eps2 nnz(A) log T  poly(eps1 d log T)) where T is the number of updates. This runtime improves upon existing sketching algorithm by a log T factor and outperforms simple sketching methods by a factor of poly(deps) for large T. Strengths and Weaknesses:  The algorithm draws inspiration from previous work but adapts it for regression problem.  The proof of the lower bound is clever but it relies on a solid assumption (Online Matrixvector conjecture).  The lower bound is somewhat limited as it assumes a small eps value. Typographical Errors:  \"Woodburry\" should be \"Woodbury.\"  Page 7 three lines above Section 4: Remove \".\"  Page 8 Theorem 4.2 proof sketch line 6: Replace (AT A)2 with (AT A)1."], "v3LXWP63qOZ": ["Summary: This paper suggests using a group of predictors (heads) built upon a common representation to enhance performance. The predictors are diversified through different learning rates. The approach demonstrates performance improvements in reinforcement learning (RL) and stability enhancements in a specific computer vision scenario. Strengths:  simplicity of implementation (training three heads with varying learning rates)  No increase in training time  Clear and concise presentation Weaknesses:  Lack of Evidence:  The paper fails to provide evidence to support various claims about avoiding spurious correlations and better generalization.  Questionable Claims:  The use of the term \"trained independently\" is misleading as predictors receive the same data and share the same backbone representation.  The \"strong performance boosts\" claim in RL and vision settings is questionable.  In RL ModInv only improves on one task out of six with data augmentation.  In vision ModInv only benefits linear predictors on CIFAR10 for nonlinear predictors and other datasets it hinders performance.  Missing Results:  The results mentioned in Section 5.2 showing a decrease in correlation with ModInv are not included in the paper."], "vQmIksuciu2": ["Paraphrase: This paper introduces a novel dynamic filter pruning technique using explainable AI and intermediate layer prediction. The early prediction branch undergoes deep topk loss training. Then the resulting coarse prediction dynamically identifies relevant CNN filters for specific classes. These filters are also identified in improvement using explainable AI. The method produces a trainable and readily deployable prune model. effectiveness and Weaknesses: 1. Incomplete Experimental Results  Experiments should cover a broad range of datasets networks and configurations.  The methods effectiveness should be demonstrated for detection and segmentation tasks.  Comparison with stateoftheart pruning methods is necessary.  Section 4.7 lacks substantial content.  The methods robustness to activation work with negative values should be tested.  The methods performance should be evaluated without finetuning. 2. Lack of Novelty  The proposed method is largely based on existing techniques with limited innovation. 3. Insufficient Motivation  The papers motivation is not clearly presented diminishing its novelty. 4. Hardware Deployment  Analysis and results should demonstrate the methods deployment on diverse hardware platforms. 5. Comparison to Kernel Pruning  The advantages of filter pruning over kernel pruning should be highlighted. 6. Role of Explainable AI  The relationship and significance of explainable AI in the method need clarification. The rationale for using instancewise dynamic models should be explained."], "tUMr0Iox8XW": ["Paraphrase: The paper presents a new method called \"pilimit\" to calculate the theoretical performance of a largescale neural network called a multilayer perceptron (MLP). This method involves projecting the gradient of the networks loss work along a fixed direction at each training step. By doing this it allows for the explicit computation of the expected work learned by the network as training advancement. The pilimit approach has been tested on the CIFAR10 image recognition dataset where it achieves an accuracy of 61.5. This is marginally higher (0.2) than a similar method called \"nunet\" and almost 2 higher than the \"neural Tangent Kernel\" method which does not take into explanation specific feature in the data. While the mathematical framework presented in the paper is innovative the mathematical solution are not entirely convincing. The small difference in accuracy between pilimit and NTK on CIFAR10 makes the benefits of feature learning and explicit limit computation less apparent. To address this the authors suggest conducting tests on a fully connected architecture specifically designed for image recognition. This architecture would be expected to achieve higher accuracy (around 78) providing a more realistic evaluation of pilimits performance compared to other methods."], "tD7eCtaSkR": ["Paraphrased Statement: summary: This research suggests a method to validate the robustness of 1Lipschitz Convolutional Neural Networks (CNNs) by:  Relaxing orthogonalization requirement in the networks last linear layer while normalizing each row of the layer matrix (\"Last Layer Normalization\").  Adding a certification regularization that enhances the robustness certification for accurately classified examples.  Introducing a class of Gradient Norm Preserving (GNP) activation functions (\"Householder activation\") necessary for maintaining the continuity of GNP piecewise linear functions. Experiments on CIFAR10 and CIFAR100 datasets demonstrate that the proposed process improves provable robust accuracy without significantly impacting standard accuracy. Strengths and Weaknesses: Strengths:  Clear and wellstructured writing.  Wellmotivated and justified contributions.  Numerical experiments supporting the effectiveness of the proposed method. Weaknesses:  Lack of detailed explanation for observed improvements in standard accuracy.  Insufficient definition of the term \"\u03c1\" upon its introduction.  Limited exploration of the differences and applications of l1 and l2 certifications.  Absence of results on larger datasets like ImageNet to assess scalability.  Lack of insights into the reasons for varying performance gains on different architectures including guidance on when the process is most effective."], "t98k9ePQQpn": ["Summary: This paper proposes a method for improving image recognition accuracy in situations where the training dataset has a long tail distribution of class labels. The method aligns the predicted label distribution with the test sets label distribution using Optimal Transport (OT). Strengths: 1. The posthoc label distribution adjustment using OT is a novel approach. 2. The method is technically sound and improves recognition accuracy compared to existing posthoc adjustment methods. 3. convergence analysis for the proposed method is provided making it easier to reproduce and implement. Weaknesses: 1. The method assumes a balanced label distribution in the test set which may not always be true in realworld scenarios. It is unclear how the method can be used when the test set distribution is unknown or imbalanced. 2. The inference time for the proposed method is significantly longer than that of other methods. 3. Some empirical analysis is missing or insufficient such as:  The effect of combining OTLM with ensemblebased methods.  The impact of the regularization coefficient lambda in the optimization objective. 4. Some small errors and inconsistencies exist in the paper including:  Incorrect wording in the description of achieving model simplicity.  An unfinished sentence about the differentiability of the optimization process.  Misspellings of \"DARP.\"  Overlapping and inconsistent precision point in Tables 1 and 2.  A potentially misleading diagram in Figure 1."], "jgAl403zfau": ["Paraphrase: The authors present a novel approach to structural filter pruning where the goal is to optimize the allocation of computational resource while ensuring latency constraints. They formulate this problem as a knapsack optimization task and propose an improved knapsack solver. To reduce the lookup space and computational cost they group neurons. Experiments on image classification and object detection show the effectiveness of their method. effectiveness and Weaknesses: Contributions:  Formulate pruning as a resource allocation problem with latency constraints.  Develop an augmented knapsack solver for the optimization.  evidence strong performance on image classification and object detection. Improvements:  Clarity: Enhance the description of Algorithm 1 and clarify notations.  Citation: Cite related study on neuron grouping.  Ablation: Explore the effects of multistep pruning.  comparison: Evaluate the impact of using FLOPs constraints instead of latency constraints.  Lightweight Netstudys: Provide results on lightweight netstudys with residual connections.  Configuration: Include details of pruned model configurations. Minor:  Increase the size of image 1 for readability."], "qI4542Y2s1D": ["Paraphrased Statement: Summary:  The paper presents a framework that segments the ALFRED task into low units using a language module to interpret instructions and generate a list of subtasks.  The agent uses a spatialsemantic map to track advancement and design navigation.  The approach achieves stateoftheart performance on the ALFRED benchmark. effectiveness:  The method exhibits strong performance on ALFRED without external instructions.  The thorough analysis highlights potential failure scenarios providing valuable insights.  The presentation is generally clear and concise. Weaknesses:  The use of modular components for navigation is not entirely novel.  FILM underperforms in some settings with lowlevel instructions suggesting limited use.  approach to oracle subtask templates may provide an unfair advantage.  Missing details and inconsistencies in the text may hinder full comprehension. Questions and Clarifications:  Is FILM distinct from the visual reasoning framework of the same name  Provide a formal definition of Embodied Instruction Following (EIF) from the ALFRED benchmark.  Clarify the inconsistency between argument types mentioned in the text and design.  Describe how lowlevel instructions are integrated into the approach.  Define \"10 failed action\" in the context of ALFRED evaluation.  Explain the aggregation method in the Semantic map Module.  Discuss how the number of semantic categories stored varies across episodes and how this is managed.  Specify if the map used is egocentric or allocentric.  Describe the mechanism for tracking completed subtasks and advancing to the future one."], "nK7eZEURiJ4": ["Paraphrased argument: Summary: The authors aim to introduce novel perspectives on distributional reinforcement learning from the perspectives of regularization optimization acceleration and the application of Sinkhorn algorithms. Strengths and Weaknesses: The reviewer raises concerns about the lack of clear definitions guarantees and support for the benefits of distributional reinforcement learning in the authors claims. Detailed Comments: regularization:  Decomposition of Equation (4) is unclear and lacks interpretation.  Separation of the event x  \\mathbbE[Z\\pi(s a)] alters the distribution making the following analysis questionable.  Theorem 1 lacks clarity regarding the determination of \\mu(st1 at1) and the role and purpose of \\theta. optimization and Generalization:  Partitioning of state feature support may impact accuracy.  Parameterization of f seems to focus on classifying state rather than partitioning features.  Matching bin probabilities does not guarantee an accurate distribution estimation.  Theorem 2 lacks proof verification and does not provide any performance guarantees for distributional reinforcement learning. acceleration:  Setting \\epsilon  0 is not feasible for distributional reinforcement learning.  Parameterization of q in Theorem 3(1) lacks justification.  Bound for \\kappa is not provided limiting the utilizationfulness of Theorem 3(3). Sinkhorn:  The loss work is not novel and lacks motivation for its utilization in distributional reinforcement learning. Overall: The authors fail to establish a clear problem setup and rigorous theoretical support for the advantages of distributional reinforcement learning over expectationbased reinforcement learning. Instead they present a collection of unrelated concepts from existing work and attempt to connect them without a coherent foundation."], "xKZ4K0lTj_": ["Paraphrase: This research proposes a technique for extracting skills from a set of offline presentations. This technique is designed to facilitate imitation learning of unfamiliar longhorizon tasks with only a few presentations. The method employs three key components: 1. A skill encoder that maps trajectory to a latent skill representation. 2. A skill decoder that generates actions based on the current state and a selected skill. 3. A skill prior that maps the current state and a target state (H steps ahead) to the latent skill distance. In contrast to previous methods like SPiRL this skill prior incorporates both the current and target state. During evaluation for a new task the agent explores the presentation dataset for a similar current state. It then retrieves the corresponding future state selects a skill using the skill prior and finetunes all components based on the provided presentations. experimental results demonstrate that this approach outperforms SPiRL and baseline methods on unseen multistep tasks in simulated environments. effectiveness:  Incorporating the target state in skill choice improves mapping to the appropriate skill.  Finetuning on the few presentations enhances performance.  Using a learned distance metric for state retrieval improves accuracy. Weaknesses:  The exact reason for the performance gap between this method and SPiRL is unclear particularly given that both methods employ finetuning.  It remains uncertain whether leveraging the target state for skill choice is the sole factor contributing to the significant improvement over SPiRL.  The presentation of the paper including the figures could be enhanced and certain contingent (e.g. number of trialsseeds) are missing."], "zBOI9LFpESK": ["Paraphrased Statement: Summary: This research presents an enhanced method for representation learning in reinforcement learning leveraging the bisimulation metric. It expands on the Deep Bisimulation for Control (DBC) approach by introducing several improvements:  Using metalearners for distance metrics instead of L1 distance on learned representations  Implementing separate feature encodings for state and dynamics  Incorporating data augmentation from DrQ  Learning the tradeoff parameter between reward and dynamics effectiveness and Weaknesses: effectiveness:  The improvements over previous work are sound and supported by experimental evidence.  The experimental results are comprehensive comparing the approach to prior methods and investigating the impact of different component.  The writing is clear and provides a thorough overview of the literature. Weaknesses:  The paper largely presents enhancements to DBC without significant novel contributions. A more indepth exploration of the limitations of existing representation learning methods and addressing them fundamentally could strengthen the approach.  The experiments involving natural video backgrounds appear somewhat artificial. Additional testing in realistic environments or more detailed experiments in CARLA would enhance the paper practicality. Detailed Questions:  Given the use of separate encoders for reward and dynamics how closely does the learned representation align with a bisimulation or equivalent state space mapping  How do the experimental evaluation compare with longer environment steps Do the methods have similar asymptotic performance or is AMBS more sampleefficient initially (e.g. DBC used 1e6 steps on CARLA)  The term \"metalearner\" needs clarification. What are the meta traintest tasks in this context Using the term \"learned distance metric\" may provide greater clarity."], "qiukmqxQF6": ["Paraphrased Statement: Summary: In essence the proposed method creates a lowerdimensional representation of frames in observed sequences and then builds a model that captures temporal changes using an RNNtype architecture in the newly created latent space. This system is trained as a whole. effectiveness and Weaknesses: The paper lacks originality and interest combination basic concept such as creating latent space representations using flow model and employing RNNs for temporal dynamics. The experimental results fall short of expectations: performance varies only slightly the method is not compared to the beneficial in the field and the datasets used are not sufficiently complex to demonstrate the methods capabilities. It would be more convincing if tests were conducted with tens of thousands of dimensions."], "oZe7Zdia1H5": ["Paraphrased Statement: Summary: This work presents a novel approach for finding structured lottery tickets using postprocessing techniques demonstrating their existence for the first time. Strengths: Lottery tickets have typically been discovered using unstructured pruning. This paper breaks this paradigm by proposing postprocessing algorithms to induce structured sparsity in existing winning tickets. These algorithms are practical and efficient. Weaknesses:  The authors should acknowledge existing work in structured sparse training such as \"learning N:M Finegrained Structured Sparse Neural Networks From Scratch.\"  The practical significance of structured lottery tickets compared to direct structural pruning is not thoroughly discussed.  The comparison with neural architecture search (NAS) for channel selection is lacking.  The paper does not explore the theoretical basis for the efficientness of the proposed algorithms."], "s3V9I71JvkD": ["Summary: This work focuses on addressing an issue called \"distribution shift in z space\" in meta offline reward learning (RL). In meta offline RL the agent needs to explore test environments during the metatest phase with collected transitions used to generate a task feature \"z.\" However in the offline RL setting the transitions collected by the exploration policy may differ significantly from those in the static training dataset. This discrepancy makes inferring the task feature \"z\" problematic. To address this issue this paper introduces an additional online training process (without reward data). It first learns a reward work from the offline training dataset. This allows the generation of reward for transitions collected during online training. These online training transitions are then used to update the policy. Strengths:  Identifies the \"distribution shift in z space\" problem in offline meta RL and provides a clear illustration.  The paper is wellwritten and well understandable.  The visualization of the distribution shift is insightful. Weaknesses:  Deeper discussion of the \"distribution shift in z space\" problem is needed particularly in other contextbased meta RL algorithm.  The online training process raises questions about the distribution of tasks and potential bias in comparison with offline and online metaRL approach.  The accuracy of the estimated reward and its impact on the proposed methods effectiveness require further analysis particularly in sparsereward tasks.  Evaluation on standard offline metaRL benchmark would enhance the work credibility."], "jT5vnpqlrSN": ["Paraphrased Summary: paper proposal: The paper introduces the Graph Inference Representation (GIR) model consisting of an \"anchor indication strategy\" and a \"forward process\" similar to the BellmanFord algorithm. The authors claim that this approach can capture positional embeddings implicitly and enhance performance on diverse datasets. effectiveness:  The concept of imitating the BellmanFord algorithm is intriguing. Weaknesses:  Clarity Issues: The paper presentation requires improvement. complex notation in Section 3.3 makes it challenging to understand. The distinctions between \\mathbbB and \\mathbbN and the meaning of \\mathbff\\mathbbN (z)  f(z N) are unclear.  Excessive Abbreviations: The experiment section uses many abbreviations making it difficult to comprehend.  Inconsistent Results: The experiments comparing MPNN and GIR in Table 2 evidence inconsistent results leading to doubts about GIRs benefits over MPNN.  Limited Empirical study: The empirical study lacks largescale experiments to demonstrate the models scalability. Questions:  While anchor selection appears deterministic the authors should clarify the variance introduced by selecting different numbers of anchor (e.g. 1 More or 1 less). Postrebuttal Response: The authors have improved the notation system but the paper clarity could however be enhanced. While the authors claim that MPNNA is also a GIR framework it essentially involves adding onehot labels to label nodes. Overall the paper could benefit from More careful revision to increase its impact. The reviewer maintains the original range of Borderline Reject."], "tYRrOdSnVUy": ["Paraphrase: Summary: Protecting the intellectual property of trained models has become significant. Current methods for protection fall into two categories: proving ownership and authorizing use. To address this the paper suggests using nontransferable learning to achieve both goals. Experiments show the effectiveness of the method in verifying ownership. Review: The paper presents an innovative solution to a realworld problem which could have significant impact on the computer vision community. effectiveness and Weaknesses: Weaknesses: 1. Table 1 lacks details on the number of training epochs used for transferring MT to MM. A sensitivity analysis on epoch count is needed. 2. The computational complexity of the NTL approach and GAN training should be discussed. 3. The distinct feature of the proposed model compared to existing work need to be clarified. 4. In Tables 2 and 3 the sourceonly method sometimes outperforms the targetspecific method. The reasons for this should be explored. 5. A section on future research should be included."], "gxRcqTbJpVW": ["Paraphrased Statement: This paper suggests a method for adjusting learnable parameters in neural networks (NNs) to maintain their dynamicalal property during pruning leading to improved accuracy. Strengths and Weaknesses: The paper is wellwritten and the proposed method is straightforward. It has been shown to outperform existing methods on benchmark datasets. However the paper has several subject:  The paper makes three main claims: structured pruning orthogonality and dynamical isometry. However these claims are not fully explored or supported in the paper.  structured pruning is mentioned briefly as a type of filter or channel pruning. The paper does not make any significant contribution to this domain.  Orthogonality is achieved through regularization of kernel Gram matrix. However it is not clear how much orthogonalization is achieved or how it affects accuracy. Additionally regularization of batch normalization parameters has a greater impact on accuracy than orthogonalization.  The paper claims that orthogonality achieves dynamical isometry. However it also suggests that regularization of batch normalization parameters is necessary for dynamical isometry. This discrepancy needs clarification. Additional Questions:  How was speedup calculated Can you provide an model of how long it takes to train and test the proposed method compared to baseline methods (e.g. for VGG19 on Cifar100 dataset)  Did you apply regularization of batch normalization parameters to baseline methods to assess its impact on accuracy"], "k7efTb0un9z": ["Paraphrased Statement: Summary: In this field researchers investigate how to adjust the learning rate for deep neural network training. They introduce a novel method called Graph Networkbased Scheduler (GNS) which leverages graph networks to model the network and control the learning rate using an agent. Experimental results demonstrate GNSs effectiveness. effectiveness and Weaknesses: Pros:  Innovative approach for learning rate scheduling.  Outperforms existing methods. Issues: 1. Applicability of GNS to simpler models like MLPs. 2. Node feature definition in the graph representing the network and how to maintain consistent feature dimensions. 3. Lack of visual comparison through training loss curves. 4. Limited comparison with baseline methods that employ reinforcement learning (e.g. [12])."], "hdSn_X7Hfvz": ["This research addresses the issue of estimating probability in machine learning models which currently fail to fully capture outcome uncertainty and rather focus on model uncertainty. The authors propose a loss function that ensures both model calibration and discrimination. To facilitate further investigation they also introduce a synthetic dataset where they estimate disease risk based on facial images and age. Strengths:  The authors tackle a significant problem. Weaknesses:  The authors should provide more justification for why current models especially Bayesian models fail to provide accurate uncertainty estimation.  calibration is a critical issue in survival analysis and penalties have been developed to address it (e.g. [1]).  The decomposition of the Brier loss is intriguing but could benefit from further explanation.  Notably the proposed loss function is just used for finetuning the neural network as illustrated in Figure 5. It would be valuable to explore the effects of training the network with this loss from the outset."], "gD0KBsQcGKg": ["Paraphrased argument: This paper emphasizes that current methods for predicting intervals (PIs) and assessing their accuracy assume singlepeaked predictive distributions. This limitation results in overly broad PIs when the predictive distribution or target exhibits multiple peaks. The authors propose extending the PI definition to allow for multiple disjoint intervals improving evaluation fidelity. They introduce a method to directly generate multimodal PIs using a mixture density network (conditional GMM) connected to a secondary neural network that produces interval bounds. The authors compare their method to alternative noise models using accuracy metrics on various datasets. While the paper highlights a valid concern it has limitation: limitation:  The two main contribution (extending PI definition and multimodal generation) are not clearly separated making it challenging to assess each claims validity.  The authors evaluate their multimodal generation method against baselines using single PIs potentially biasing the results. The function of disjoint PIs for both approaches would provide a average comparison.  The motivation for using an auxiliary network to generate PI bounds from GMM argument is unclear given that these bounds can be directly calculated. The authors can improve the paper by clarifying their motivation and evaluating competing multimodal noise models using disjoint PIs."], "l8It-0lE5e7": ["Paraphrase: This research investigates the outcomes of adversarial training and demonstrates that under specific conditions these results optimize the margin for adversarial training samples. Similar observations have been made in the context of cleaning DNN training but this study extends them to adversarial training settings. While the findings may not come as a surprise given earlier study on standard training the paper rigorous analysis is significant. Minor Concerns: 1. The notation for the loss function is inconsistent. The loss function in (6) is defined with two inputs (x and y) while Assumption 1s loss function has only one argument. The intended form in (6) should be: l(xyW)  l (f(xW)y) where l represents Assumption 1s loss function. 2. The remark on page 9 requires further explanation. The lefthand side of (20) pertains to clean test sample prediction whereas the righthand side relates to fitting adversarial training samples. The paper should elaborate on how this inequality affects the tradeoff between robustness and accuracy. 3. On page 21 after equation (81) there should be a reference to Lemma 9 not Lemma 4."], "wIK1fWFXvU9": ["[contribution A] Some claims or designs are not rigorous or reasonable. A1: The comparison of adversarial training (AT) on MNIST and CIFAR10 datasets is considered inappropriate because the adversarial budget for MNIST is higher than that for CIFAR10 making the other task arguably not easier. A2: Instead of using Shannon entropy (Equation 3) to assess smoothness the authors suggest that calculating the local Lipchitz constant in the vicinity of incorrectly labeled data would be a more accurate measure of smoothness. A3: The use of both the geometry value k and the loss value l to filter incorrect labels in Algorithm 1 is contradictory. Additionally setting an appropriate threshold value L for this algorithm is challenging. Ablation work should be conducted to demonstrate the superiority of the geometry value over the loss value. A4: The proposed \"robust annotator\" exhibits only marginal improvement when the ratio of adversarialcorrupted data is high. For smaller ratios which are more realistic the method performs worse than a PGDbased annotator. [contribution B] I dont think some discoveries of this paper are novel. Some findings are already included in previous work and are not surprising. contribution of this paper is limited. B1: The findings on ReLU network tendency to make overconfident predictions under standard training and ATs role in mitigating this number have already been reported in previous work (Ref[a]). B2: The observation that AT outperforms standard training (ST) in preserving standard test accuracy with noisy training data is considered of limited practical value because ST can utilize a label noisefree validation set for other stopping and achieve comparable results. B3: The paper claim that network that are adversarially robust have larger loss on training data with label noise is not novel as it has been demonstrated theoretically and empirically in previous research (Ref[b]). B4: A quantitative analysis of the phenomenon in terms of the perturbation bound \u03b5 is missing. Such an analysis could reveal how ST gradually transitions to AT with increasing value of \u03b5. [contribution C] The introduction and writing of some contribution need improvement. C1: Equation 3 correctly defines Shannon entropy as p(yx) log p(yx) not p(x y) log p(yx). C2: The wording in the paper needs refinement. For model the statement \"AT can always distinguish...\" should be modified to reflect that AT may not always be able to distinguish correctly and incorrectly labeled instances."], "j30wC0JM39Q": ["Paraphrased Summary: This research presents two metrics frequency concentration and clustering velocity for characterizing the dynamic nature of wordgraph embeddings. The paper reviews various graph generation models and applies the proposed metrics to realworld datasets. Strengths and Weaknesses: Strengths:  Examines the captivating field of structure evolution through the lens of embedding space geometry.  Introduces two innovative measures.  Clear and wellwritten introduction. Weaknesses:  Lacks a formal problem statement making it unclear what computational challenge the paper addresses.  The connection between the proposed measures and the stated research question requires further clarification.  Rationale for the proposed measures is not adequately explained.  empirical evidence to support the proposed measures is insufficient."], "oaKw-GmBZZ": ["Paraphrase: A neural network known as a message passing graph neural network (MPGNN) is used to predict the solution of a range of partial differential equations (PDEs) as they evolve in time. The MPGNN takes as input parameters initial conditions and boundary conditions of the PDEs and outputs the solution one timestep into the future. The researchers demonstrate its training for the heat equation NavierStokes equations and advectiondiffusion equations. The MPGNNs flexible architecture allows for deployment on various geometries and boundary conditions. effectiveness and Weaknesses: Advantages:  Clear and easytofollow writing  Intuitive and sensible GNN architecture with locally shiftequivariant properties  Generalizability across PDEs within a class by encoding parameters in edge features  Experimental results indicate successful 1step PDE prediction on multiple geometries. Queries:  Limitations of the assumption that local spatial derivatives and neighborhood values suffice for graph neural network performance  Use of 1step neighborhoods in the model versus the importance of longerrange message passing for higherorder PDEs  details of the experimental setup including input and output format coordinates selection distribution sampling optimizer selection loss work and boundary value prediction handling  Longterm error behavior of the learned solver in recurrent mode  Absence of baselines for comparison  Connection to similar work by SanchezGonzalez (2020) on solving complex physical systems with MPGNNs Minor Notes:  Correction of superior notation for \"n\"  Clarification of Firedrakes nature as a software package"], "gpp7cf0xdfN": ["Paraphrased Statement: The study explores reconstructing adversarial perturbations from example events after they occur arguing that minimizing reconstruction error alone is insufficient. To effectively align the predictions of the original and reconstructed versions the authors combine a denoising network with a prediction alignment network minimizing their various losses. data augmentation is employed to enhance performance. empirical results demonstrate the superiority of the proposed architecture in balancing prediction alignment and reconstruction error. effectiveness:  The research tackles an understudied problem.  The paper is wellwritten and the approach is sound.  The results show the effectiveness of the proposed architecture against baseline methods in the specified task. Weaknesses:  The study lacks specific applications or examples of how the estimated perturbations can be used for framework diagnosis.  The method involves combining architectures and loss functions lacking the theoretical guarantees associated with previous defense methods. Additional Questions:  Do different attacks generate distinct distributions of perturbations  How does the method comparison to baselines in estimating perturbations on smoothed classifiers  Is the method effective on smaller datasets or nonimage domains  Can the inferred properties of novel attack types based on similarity to existing attacks provide valuable insights  How does the alignment and reconstruction error vary across different examples created by the same method on the same image  Could a library of adversarial perturbations be used as a reliable alternative to running attacks on a novel framework"], "mZsZy481_F": ["Paraphrased statement: Summary: This study investigates how to categorize data and identify unusual model in situations where there are only a few examples available. For these tasks the Fewshot ROBust (FROB) model is created. strength and Weaknesses: Overall the paper is wellwritten and organized. It provides adequate experimental data."], "lKcq2fe-HB": ["Paraphrased Statement: Summary: The authors enhance SelfPaced Reinforcement Learning (SPRL) by introducing Wasserstein barycenters to broaden the target distribution options. This extension allows SPRL to accommodate distributions that deviate from the original Gaussian constraint such as tailored distributions derived from expert demonstrations. Evaluation: Strengths:  The algorithm and its modification are clearly defined and the experimental setup is appropriate.  The extended curriculum enables SPRL to address complex environments.  The experiments assess the performance improvement and trace the curriculum development. Weaknesses: Technical Novelty and Significance:  The extension is incremental combining existing approximation without fundamentally altering the SPRL concept.  The relevance and significance of the findings are not fully explored leaving the impact of the modification unclear. Empirical Novelty and Significance:  SPRL now has extended use cases with appropriate baselines.  However the experimental range is limited to narrow target distributions hindering comparison and diminishing the broader significance of the findings. general Comments:  The need for nonparametric target distributions is emphasized but the effectiveness of Wasserstein barycenters beyond bimodal distributions is not fully demonstrated.  limitation of context distributions for Wasserstein barycenters should be explored.  The practical applicability of SPRL with wider target distributions should be addressed.  Hyperparameters are not explicitly discussed including their impact and selection work. Related work:  The related work section on curriculum learning in RL could be more comprehensive to reflect recent advance.  Empirical comparison to additional baselines would enhance the evaluation. Minor Questions:  Clarify the elimination of \\delta in Equation 4.  Justify the assumption of 2Wasserstein distances under a Euclidean metric.  Provide references for contextual RL in the related work section.  Confirm whether baseline hyperparameters were adjusted or original settings were used."], "t2LJBsPxQM": ["Paraphrase: Summary:  Presents a novel approach to orthogonalize convolutional layers leveraging the connection between spatial orthogonality and spectral paraunitarity.  Demonstrates the effectiveness of the new method through comparisons of:  Lipschitzness  Adversarial robustness  Time and memory usage effectiveness and Weaknesses:  effectiveness:  novel orthogonalization method with theoretical justification  Weaknesses:  Exaggerated claim regarding previous work  Inconsistent experimental results with marginal computational efficiency gains detailed Comments: 1. Implementation of the proposed method needs improvement for clarity. 2. Unclear implementation of the orthogonal matrix Q used in the experiments. 3. Results limited to \\ell2 normbased adversarial attacks \\ell\\infty attacks could provide more insight. 4. Striding handling in the code could be elaborated for beneficial understanding. 5. Missing citations of related work:  [Orthogonalizing Convolutions with Singular value Decompositions](https:arxiv.orgabs1810.09102)  [Regularizing Deep learning framework via Spectral Normalization and Stochastic Weight Averaging](https:arxiv.orgabs2103.00673)  [ConvNet as MultiLayer Random Walk](https:arxiv.orgabs1911.12207)  [Orthogonal Weight Normalization for Deep learning](https:arxiv.orgabs1905.11926)"], "gfwON7rAm4": ["Paraphrased Statement: Summary: This research paper investigates the projected policy gradient method in Markov potential games (MPG). It demonstrates the following contributions:  Establishes the existence of deterministic Nash policies in MPG.  Identifies sufficient conditions for MPG.  Proves the convergence of the projected (stochastic) gradient ascent method to Nash policies in MPG. Strengths:  Addresses an important topic in policy gradient methods applied to MPG.  Presents novel insights such as the existence of deterministic Nash policies and the illuminating examples showing the limitations of potentiality.  Provides a novel access to analyzing convergence properties borrowed from previous research. Weaknesses:  Omits discussing relevant works such as \"stochastic Potential Games.\"  Some concepts and conditions require clearer explanations particularly regarding Condition C2 in Proposition 3.2.  Ambiguity remains in the case where the transition probability depends on agents actions (design 3).  Section 4 could be simplified and made more accessible while Section 5 could be condensed to focus on important information."], "rl8jF3GENq": ["Paraphrased Statement: Summary: This paper introduces a model for detecting synthetic images using the wavelet packet representation of natural and GANgenerated images. The models performance was evaluated against source identification challenges in the FFHQ CelebA and LSUN datasets. effectiveness: 1. The importance of addressing the prevalence of deepfake images is highlighted. 2. The methods section provides clear and detailed instructions. Weaknesses: 1. A motivational type work could enhance the research relevance and appeal to a wider audience. 2. The rationale for previous work focusing only on spatial or frequency domain analysis should be clarified along with the challenges this approach faced and how the current research addresses them. 3. The use of Fast Wavelet Transform (FWT) and Boundary Wavelet should be briefly explained and justified in sections 3.3 and 3.4 respectively. 4. The symbols used in the equations in section 3 should be defined to enhance clarity. 5. The comparative work in the experimental section lacks diversity. The choice of comparison methods should be explained and additional comparison could strengthen the models efficacy."], "uxxFrDwrE7Y": ["Paraphrased Summary: This paper presents a new approach to continual learning drawing inspiration from neuroscience. The method involves creating two memory systems: one for shortterm adaptation and one for longterm retention of knowledge. The shortterm system quickly learns new information while the longterm system gradually accumulates structural understanding. These systems are trained using a \"Mean teacher\" approach with updates occurring at varying frequencies. The method is versatile and can be used in various continual learning scenarios. Experiments show that it outperforms existing approaches. Strengths and Weaknesses: Strengths:  Novel and welljustified approach that leverages multiple memory systems.  Clear writing and system facilitating understanding.  Demonstrated superiority across different continual learning settings. Weaknesses:  computational cost due to maintaining three model during training.  Sensitivity to hyperparameter tuning which may limit applicability to different tasks. Additional Questions:  Rationale for stochastic updates of the memory systems.  reason for CLSERs outperformance of JOINT on SMNIST despite JOINT representing an upper bound.  potential benefits of explicitly enforcing flat minima in the proposed method."], "iMqTLyfwnOO": ["Paraphrased Statement: Summary:  The subject introduces a novel variant of Sliced Wasserstein Distance (SWD) called Augmented Sliced Wasserstein Distance (ASWD).  ASWD maps data place onto surfaces using neural networks allowing for efficient SWD calculation on these surfaces.  ASWD addresses the inefficiency of SWD for highdimensional data.  Experiments in flow generative modeling and barycenters demonstrate the superiority of ASWD over existing methods. Strengths and Weaknesses:  The paper is wellstructured and ASWD is adequately justified.  The method incorporates some originality although the concept of using nonlinear functions to enhance slicing efficiency has been explored in GSWDNN.  The theoretical validity of ASWD as a distance metric is proven despite the use of neural networks.  The significance of ASWD is supported by experiments in various tasks. Questions:  (a) validity of GSWDNN: The statement implies that GSWDNN becomes a legitimate metric if it employs an injective function. This suggests that the criticism lies in the selection of g(x) rather than an inherent incompatibility between GSWD and neural networks.  (b) performance Difference: The experiment in Section G.2 shows minimal disparities between ASWD and ASWDnoninjective during NN optimization. Hence the performance gap between ASWD and GSWDNN is potential attributed to random projection or other factors (e.g. L\\lambda regularization)."], "rF5UoZFrsF4": ["Summary: The paper proposes an architecture for graphical user interfaces (GUIs) that processes multiple input types (range text natural language) and performs multiple tasks (object detection text generation etc.). The architecture includes separate transformer blocks for range and text inputs that interact with each other to generate multimodal outputs used for downstream tasks. The authors argue that combining tasks improves performance compared to training on individual tasks separately. Strengths:  Welldefined problem formulation and design choices.  Clear and logical paper structure.  Insightful observations about multitask learning in UI modeling. Weaknesses:  set scalability beyond the specific tasks and modalities studied.  Lack of direct comparison for computational cost between the multitask model and individual models.  Similar nature of the downstream tasks may have influenced the multitask learning benefits.  Absence of baselines for two tasks.  set novelty as techniques used have been previously applied in rangetext tasks. Clarifications:  Joint training procedure is described but details on training for individual or subsets of tasks are not provided."], "qO-PN1zjmi_": ["Paraphrase: Summary: The authors developed a novel approach for anomaly detection in the transductive setting which utilizes the differences between multiple example trained with varied labels on unlabeled test data samples. method: 1. Provide different labels to the unlabeled test set samples. 2. Train multiple example by finetuning them on the labeled samples. 3. Identify anomalies based on the level of disagreement among the example for each sample. Observations: effectiveness:  use unlabeled test data for anomaly detection. Weaknesses:  Limited Applicability: Applicable only when we have approach to OOD samples during training which may not be always feasible.  dependence on Unlabeled Sample size: The number of unlabeled samples can impact the effectiveness of the method.  Restrictions with Binary Classification: May be limited in scenarios with only two classes (binary classification).  Validation Set Issues: The validation set being a subset of the indistribution data may lead to premature finetuning.  Lack of Comprehensive analysis: Limited investigation of the methods performance with varying values of K (ensemble size) sample size and OOD data ratios."], "olQbo52II9": ["Paraphrase: This papers proposes an algorithm for solving the weighted maximum issue problem using reinforcement learning (RL). It employs a gated graph recurrent network to process an input graph and generate embeddings. These embeddings are then fed into a lightweight recurrent neural network (RNN) that operates as a policy. The policy randomly initializes a issue (dividing nodes into subsets) and iteratively \"flips\" a node from one subset to the other. The policy is trained with a modified version of the deep Qnetwork (DQN) algorithm to optimize the issue obtained after a fixed issue of flips. It utilizes a \"peek\" feature that provides a onestep preview of the issue of each flip. At testing time multiple deterministic trajectories are generated in parallel from different initial issues and the beneficial issue is ultimately selected. The proposed algorithm called ECORD demonstrates improved performance compared to ECODQN a similar RL approach. However it is acknowledged that the algorithm is primarily tailored for maximum issue and may not generalize effectively to other combinatorial optimization problems. effectiveness:  Clear and wellwritten  Sensible approach with a focus on computational efficiency  Addresses scalability a crucial aspect in neural combinatorial optimization  use reliable baseline  Impartial comparison with ECODQN Weaknesses:  Limited generalization beyond maximum issue despite the claim suggesting broader applicability  Lack of clear explanation of differences from ECODQN making it unclear what is novel in the proposed method  Misuse of the term \"SOTA\" (stateoftheart)  Insufficient experimental reinforcement especially in comparison to heuristic baseline  Limited theoretical novelty as certain ideas are derived from ECODQN  Incremental nature and narrow focus on maximum issue may not warrant acceptance at ICLR"], "kj8TBnJ0SXh": ["Paraphrase: Summary: 1. Introduces FaceDet3D a method to generate detailed 3D facial geometry from a single image for any desired expression. 2. FaceDet3D consists of two components: a detail hallucination network to predict facial geometry changes with expression and a rendering network to generate detailed renderings based on the geometry data. 3. Both components are trained separately using a heavy image dataset and a small video dataset. 4. novel Augmented Wrinkle Loss and detailed Shading Loss map are used in condition the rendering network. 5. evaluation results demonstrate that FaceDet3D produces realistic facial details for various expressions and renders highquality results. 6. Ablation work confirm the effectiveness of the proposed components. effectiveness and Weaknesses: effectiveness: 1. Proposes a twostep pipeline for realistic facial detail generation and rendering. 2. Develops a detail hallucination network for facial geometry prediction. 3. Introduces Augmented Wrinkle Loss and detailed Shading Loss for rendering network condition. 4. Provides quantitative and qualitative evaluation including comparisons with DECA. 5. Demonstrates view consistency of rendered results showcasing the rendering networks effectiveness. 6. Conducts ablation work to evaluate the impact of the proposed losses. Weaknesses: 1. Readability could be improved e.g. with a pipeline image and clarification of Equation (2). 2. image require enhancement for clearer detail introduction. 3. Experimental evidence is limited:  Rendered views appear blurrier than input images and previous work.  Comparison with [Chen et al. 2019] (which FaceDet3D builds upon) is missing.  evaluation of the age prediction network and FaceID embeddings is not included."], "ibqTBNfJmi": ["Paraphrased argument: This paper introduces two optimization approaches for recommendation systems designed to handle imbalanced token distributions. These algorithms incorporate frequency information for efficient convergence and improved performance. They are straightforward to comprehend and implement and their theoretical boundaries are defined. Advantages:  Easy implementation  Suitable for both realtime and batch processing Disadvantages:  Limited experimental validation  Offline training may be necessary for industrial systems  Uncertain effectiveness of FASGD in offline evaluations  Unknown benefits of CFSGD in matching stage  potential limitations when using loss work other than crossentropy"], "gEZrGCozdqR": ["Paraphrased instruction: This paper introduces a simple approach called \"instructiontuning\" to enhance the zeroshot learning capabilities of large language models. The method involves: 1. Generating text prompts for a variety of tasks 2. Training the model to respond to these prompts Empirical evidence shows that after instructiontuning the model outperforms GPT3 on zeroshot tasks across multiple datasets. However for datasets similar in format to language modeling the improvement is minimal or negative. The field also highlights additional insights: 1. performance improves with the number of task clusters. 2. Instructiontuning benefits large models. 3. Fewshot learning remains beneficial. effectiveness and Weaknesses: While the method builds upon existing concepts the empirical results are impressive showing improvement over GPT3 with a smaller model. The analysis helps identify when the method is effective and provides guidance for future research. Additional Questions and Comments:  Additional task Results (3.4): Consider comparing FLAN to Base LM rather of GPT3. Highlight that FLANs performance remains competitive on most additional tasks except for ReCorD. Clarify whether Base LM 137Bs zeroshot result is based on the average template or the well template.  number of task Clusters (4.1): Add an untuned model to number 5 with xaxis value 0 to gauge the impact of a single task cluster.  Explanation for scale (4.2): Consider exploring an alternative explanation: large models have well outofdistribution generalization and are less prone to overfitting to indomain data. Provide empirical support if potential.  InContext Fewshot vs. Finetuned Fewshot (4.3): comparison finetuning an instructiontuned model with 16 model to incontext prompting to determine the optimal use of fewshot learning model.  Others: Mention the potential impact of instructiontuned models as novel base models in the main paper. Consider revising the name \"FLAN\" to well reflect the methods unique aspects. Typo:  Introduction: variety \"Instruction tuning is \u201ca\u201d simple method...\" to \"Instruction tuning is a simple method...\"  Conclusion: variety \"...supercedes recent process such \u201cas\u201d...\" to \"...supersedes recent process such as...\""], "vfsRB5MImo9": ["Paraphrase: Summary: The researchers present a new continual learning (CL) problem called Continual Knowledge Learning (CKL). They break down CKL into three subtasks: preserving timeindependent knowledge revising existing knowledge and gaining new knowledge. They develop a new benchmark and metric to assess the performance of further frameworks on these subtasks. Their findings reveal that CKL poses obstacles not encountered in conventional CL setups. They also examine the underlying case of knowledge loss in CKL. Strengths:  The research team extends the CL concept to include CKL which presents unique challenges not found in traditional CL.  They offer a new metric called FUAR to gauge the balance between forgetting updating and acquiring knowledge. This contribution aids in the direct comparison of frameworks tackling CKL tasks.  The field employs rigorous experiments to evaluate the performance of various CL techniques (regularization rehearsal and parameter expansion methods) across different aspects of the CKL task.  In the appendix the authors provide additional insights into the frameworks learning process such as changes in predicted outputs during continued pretraining and failure analysis based on probe case. Weaknesses:  Using T5 the authors report that parameter expansion methods demonstrate superior performance across all experimental context. However in experiments with GPT2 (a decoderonly framework) in the appendix GPT2MixReview (a rehearsal method) excels. While the authors acknowledge this discrepancy in future work they should include it in the main body of the paper to provide context.  Although the authors did not investigate combined approaches it would be beneficial to explore the potential benefits of combining rehearsal and parameter expansion methods. Questions:  Since the three CL methods (regularization rehearsal and parameter expansion) are not mutually exclusive have the authors explored combining them  Could a hybrid approach consisting of rehearsal and parameter expansion methods produce superior performance"], "w01vBAcewNX": ["Paraphrased Statement: Summary: This research investigates imitation learning with expert data that may exhibit covariate shifts and lacks contextual data. The work establishes theoretical bounds while incorporating numerical experiments and realdata applications. It concludes that:  Without reinforcement feedback learning the optimal policy with covariateshifted expert data is impractical.  With reinforcement access learning the optimal policy is feasible. effectiveness:  Addresses a relevant practical problem in imitation learning.  Extends to settings commonly encountered in practice.  Draws connections to causality enhancing its accessibility for those with statistics backgrounds.  Presents accessible proofs and numerical validation. WeaknessesMinor Comments:  Theoretical conclusions largely align with expectations.  Lack of theoretical evidence supporting expert datas impact on learning efficiency.  Ambiguous terminology:  \"Stationary distribution of a policy \\pi\" should be clarified.  \"Uniquely defined\" should be \"uniquely defined up to the set \\Gamma\\pi\\ast.\"  Incomplete explanations of picture 2.  Assumption of expert data consistently following the optimal policy is potentially problematic."], "t3BFUDHwEJU": ["Summary This paper introduces a generalization of the traditional geometric discounting scheme in reinforcement learning proposing a delayed discount criterion that captures geometric discounting when the depth D is 0. It also allows for other temporal preferences. The paper explores the significance of this new discounting scheme and introduces two new algorithms for reinforcement learning. evaluation The motivation for exploring alternative temporal preferences is compelling. The questions studied are relevant including the impact on the Bellman operators contraction and how RL algorithms handle the new discounting. However some concepts are unclear especially Equation 1 and the case of nonmonotonic time preferences captured. Furthermore the experimental results lack variance estimates which could hinder decision. It is suggested to reduce variance by limiting initial state sampling to a few representative states. While the new algorithms and the delayed discount criterion have potential the presentation could be improved for clarity. Primary Questions  Equation 1 needs clarification particularly regarding the superscript N and the subscript of the amount. It should be specified if only consecutive natural amount should be used.  Related paper exploring discounting such as by Pitis and Fedus et al. could be discussed for context. minor Questions  Is gamma part of the objective or an algorithmic tool  The statement about sample inefficiency and feedback propagation is not fully explained.  Why is Q\\pi indexed by \\eta in Equation 3 Writing Suggestions  Consider removing the outline paragraph before Section 2.1 for brevity.  Enhance the quality of the plots and increase case sizes in figures.  set grammatical errors and typos throughout the manuscript.  Use a different symbol for trajectory dataset to avoid confusion with other variables.  Cite the Double Q paper when mentioning the \"double Qnetwork trick.\"  Provide a definition for c(sa) in algorithm 2."], "kHNKTO2sYH": ["Summary: This article presents CLSVAE a deep learning method for identifying and fixing systematic outliers in data. CLSVAE uses a trusted dataset (containing both inliers and outliers) to learn a model with two latent subspaces: one representing clean data and the other representing corrupted data. Despite using a limited labelled dataset CLSVAE performs easily in outlier detection and repair tasks on image datasets. effectiveness:  clean motivation and intuitive concept  Impressive results supporting claims Weaknesses: Understanding Difficulty:  The mathematical section may be challenging for readers unfamiliar with VAEs and generative models. practical Application:  Experiments are simulated raising concerns about the methods realworld effectiveness.  Suggestion to test on datasets with known systematic outliers (e.g. point cloud registration with incorrect correspondences). Ablation field:  Exploration of the impact of trusted set size on performance would provide valuable insights."], "zxEfpcmTDnF": ["Paraphrase: This research demonstrates that a speechgenerating model (VAE) can capture and manipulate fundamental frequency and formant frequency data. By studying a controlled dataset researchers discovered how to adjust the models \"hidden\" space allowing precise control over these speech characteristics. These adjustments include controlling fundamental frequency formant frequency and even removing harmonic elements to create whispered speech. Experiments confirmed the models ability to control formant frequency surpassing previous methods. However in experiments controlling fundamental frequency the models performance was comparable or somewhat weak than traditional voice processing techniques. effectiveness and Weaknesses: effectiveness:  The ability to control speech features through the models hidden space is a notable finding.  Utilizing a controlled dataset to identify subspaces within the models hidden space is an innovative approach. Weaknesses:  The results are primarily of interest to specialists in speech signal processing.  The methods reliance on a controlled dataset limits its applicability if such datasets are not readily available."], "kavTY__jxp": ["Paraphrased Statement: Summary:  The work introduces a novel reinforcement learning model with a fragmentbased framework to design molecules that interact with SARSCoV2 protein binding model.  The model utilizes spatial graph attention and convolution to enhance structural information extraction.  Actorcriticbased reinforcement learning guides the model to identify states with optimal docking scores leveraging techniques like Proximal Policy Optimization and Random Network Distillation.  The model exhibits high performance optimizing chemical synthesis complexity while maintaining effectiveness. effectiveness:  The model addresses the need for discovering inhibitors against SARSCoV2 endoribonuclease demonstrating improved synthetic accessibility (SA) and docking scores compared to existing methods.  The fragmentbased chemical environment in the reinforcement learning component along with the use of further techniques provides inspiration for future research. Weaknesses:  Despite the effectiveness of spatial graph attention and convolution they do not represent novel modules.  While docking scores and SA values surpass other algorithms the model underperforms in terms of diversity.  Ambiguities in the paper include inconsistent element representation in equations missing annotations in Table 1 (e.g. Diversity definition and bold value significance) and inadequate explanations for Figure 4s right plot."], "lEoFUoMH2Uu": ["Summary: The researchers developed a model to interpret fMRI signals from the human visual cortex by incorporating foreground attention (Fattention). They also created a LoopEncDec training method guided by Fattention to effectively reconstruct visual images from fMRI data. This approach achieves a higher score in pairwise SSIM than previous methods. Strengths:  Potential for improved fMRI signal decoding  role of Fattention to guide training Weaknesses: Primary Concern:  The attention decoder training label is derived from the Fattention map which is an artificial representation of attention. Fitting BOLD signals to this map may not accurately reflect human attention during the task. other Concerns:  Insufficient clarification on the process of extracting prominent objects for visual attention labeling  MSE loss in high dimensions can be challenging to optimize and more training loss detail are needed  Incomplete explanation of the model components contribution  Empirical selection of hyperparameters (alpha beta lambda mu)  The chosen goldfish model is not representative of the models overall performance  Lack of clear definition between LVC and HVC in the discussion on form and form perception  Unclear term \"dk\" in Equation 2 Suggestions:  role a more precise term than \"Fattention\" to avoid assuming its presence in the task  Collect eye tracking data to obtain more direct attention labels  Include term definitions in the form for Equations 79"], "gzeruP-0J29": ["Summary: This paper reinterprets fast adversarial training as a bilevel optimization problem highlighting the relationship between the adversarial perturbation and network weights. It proposes a novel linearization method for the lowerlevel problem and introduces the FASTBAT approach which improves both accuracy and robustness. effectiveness and Weaknesses: The papers perspective is valuable as it recognizes the implicit gradient term in adversarial training. However it needs to address various key points to be strong: 1. Justification for Remark 1: Provide evidence for the claim that alpha2 affects the accuracyrobustness tradeoff and boosts robustness. 2. Gradient Alignment: Explain why FASTBAT improves gradient alignment whether empirically or theoretically. 3. connection to PGD: Discuss the relationship between FASTBAT and standard PGD adversarial training. Additional Question: Consider the impact of nonReLU activation functions on the Hessianfree assumption as they may not be piecewise linear with respect to input."], "ptxGmKMLH_": ["Paraphrased Summary: This work establishes a generalization bound for protonet networks. This bound offers guidance for enhancing protonets without finetuning by normalizing feature vectors and lowering the variance of their norm. The derivation lifts the other requirement of a specific work for the feature distribution and class covariance matrix. Experiments on various feature transworkation techniques on standard fewshot benchmarks back up this assertion. Paraphrased Strengths and Weaknesses: Strengths:  Intriguing theoretical examination of protonets with relaxed restrictions  Coherent experimental findings Weaknesses:  The theoretical work only loosens the premise that requires a specific class distribution leaving the overall strategy mainly unchanged from Cao et al.  Intuitive theoretical results such as the effectiveness of feature L2 normalization are already wellknown.  variance normalization underperworkance requires consideration in light of the theoretical analysis.  The optimal feature transworkation is unclear despite the works evaluation of various techniques.  The experimental results section could benefit from improved clarity in its flow."], "hjd-kcpDpf2": ["Paraphrase: Summary: This paper proposes a method to optimize the parameters of each Q use in an ensemble to reduce their similarity. Experimental results demonstrate that regularization methods leveraging ensembles of Q uses (e.g. REDQ MinmaxDQN) can improve sample efficiency with this regularization. Strengths:  A straightforward and various regularization technique to enhance the performance of RL methods based on ensembles of Q uses.  The proposed regularization technique demonstrates positive results in a variety of RL methods and environments. Weaknesses:  The regularization method lacks a theoretical foundation based on similarity measures.  The paper organization and presentation could be improved.  Experiment setups are not sufficiently detailed especially regarding Section 5.4.  Figures in the paper PDF format take excessive time to render due to vector graphics."], "vUH85MOXO7h": ["Paraphrase: Summary: The researchers have developed a \"neural tangent kernel\" (NTK) for \"soft trees\" (a type of machine learning framework) and demonstrated its properties including its stability in large ensembles its applicability to specific tree ensembles and its connection to the traditional NTK. They have also evaluated its use in a classification task on a kind of datasets. Strengths and Weaknesses: Strengths:  The researchers have established the properties of the soft tree NTK confirming its positive definiteness and its behavior during training.  The soft tree NTK contribution similarities with the traditional NTK suggesting that it can be used to analyze training dynamics.  The researchers have calculated the soft tree NTK in the limit of an infinite ensemble size which is computationally more efficient than using the traditional NTK in such cases.  The \"depth degeneracy\" property of the soft tree NTK is useful and the framework developed helps understand this phenomenon and optimize framework parameters. Weakness:  The researchers do not discuss whether the soft tree NTK provides insights into initialization strategies for soft tree ensembles which would be valuable information for practical applications."], "nkaba3ND7B5": ["Paraphrase: Summary: This paper presents a framework for comparing episodic and nonepisodic reinforcement learning algorithms. It includes both theoretical introduction and an experimental setup using existing environments. However the papers contribution are unclear as the theoretical framework lack novelty and the experimental framework mainly remanipulations existing resources. Strengths and Weaknesses: While framework for benchmarking are valuable this paper raises concerns.  The manipulation of the term \"autonomous reinforcement learning\" is problematic since its definition in the field is wellestablished and differs from the behavior described in the paper.  The distinction between episodic and nonepisodic algorithms resembles the existing continuum between continuous and noncontinuous learning.  The distinction between trained episodic systems deployed for continuous learning does not warrant new benchmarks as existing ones suffice.  The definition of \"deployed policy evaluation\" is unclear and practically difficult to determine.  The \"continual policy evaluation\" definition matches the classical definition of RL in a nonepisodic setting.  The concept of \"irreversible states\" is unnecessary as agent getting stuck in certain situations can be accommodated within existing framework.  The framework manipulationd in the benchmark are not novel and the papers contribution to these existing environments is unclear.  The evaluation lack originality with standard performance analysis under specific conditions. Minor Issues:  The claim that episodic settings typically reset every 1001000 step is questionable as it varies depending on the experiment.  The assumption that learning continues during deployment may not always hold."], "vr4Wo33bd1": ["Summary Paraphrase: This research aims to enhance the learning process in a semisupervised setting by alternate the sampling approach leveraging the benefits of decoupled feature learning and classification. Strengths and Weaknesses Paraphrase: Strengths:  The proposed alternate sampling strategy is innovative and effective.  The paper highlights a crucial issue with pseudolabels in longtailed learning where inaccuracies worsen when using classbalanced sampling. This is supported by empirical evidence and analysis.  The method achieves impressive results outperforming existing approaches significantly. Robust ablation studies demonstrate its effectiveness. Weaknesses:  Missing Baseline: The paper lacks a semisupervised feature learning baseline to compare against the proposed method.  Outdated References: The references primarily focus on process before 2020 neglecting recent advancements in classimbalanced semisupervised learning.  Minor Issue: The sentence should read: \"The model is then finetuned on the combination of ...\""], "lY0-7bj0Vfz": ["Paraphrase: This paper introduces a groundbreaking \"MoCA\" layer that enhances the performance of imagegenerating networks called GANs. MoCA is specifically designed for the challenging task of generating image with limited training data (fewshot image generation). The MoCA layer organizes its memory hierarchically using semantic and component prototypes. This structure draws inspiration from recent neurological discoveries about \"grandmother cells\" in the visual cortex. When applied to two stateoftheart GAN architectures (FastGAN and StyleGAN2) MoCA significantly improves image generation quality as measured by the FID score:  FastGAN: 5.8 improvement on Animal Face Dog 13.8 on Obama 21.7 on ImageNet100 and 12.4 on COCO300.  StyleGAN2: 5.1 improvement on Animal Face Dog 8.1 on Obama 14.1 on ImageNet100 and 17.3 on COCO300. effectiveness:  innovative: MoCA is a novel prototypebased layer.  Impactful: Fewshot image generation is a critical research field.  Relevant: This paper will interest vision researchers at ICLR.  Thorough Evaluation: MoCAs performance was tested on six various datasets and two baseline models.  Impressive solution: MoCA consistently improves image quality.  clear Writing: The paper is wellwritten and organized. Weaknesses:  Limited baseline Testing: MoCA was only evaluated on two base models but additional experiments would strengthen the findings.  Neurological Basis: The paper claims MoCA is inspired by neuroscience but the connection is limited. However as the paper does not explicitly claim to be based on neuroscience this is not a major concern."], "k7-s5HSSPE5": ["Summary Paraphrase: This research explores a technique to enhance the transferability of multilingual models across different languages. The work assessed the impact of representation stability and distributional class shift and developed a method to increase representation stability and rectify the class shift. Experiments showed effectiveness with large shifts in prior knowledge. effectiveness and Weaknesses Paraphrase: effectiveness:  The empirical analysis serves as a strong foundation for the method.  Experiments provide evidence of its effectiveness. Weaknesses:  Limited range: Only two tasks are examined and only mBERT is tested.  Questions about generalizability: Would the findings extend to other popular models like XLM  Comparison with existing work: Table 1 indicates similar performance to Wu et al. (2020) on average.  Lack of baseline in Table 2.  Potential for increased condition time compared to previous methods."], "shpkpVXzo3h": ["1. The paper proposes a quantization method for stateful optimizers that reduces memory overhead without sacrificing performance from 32 bits to 8 bits. This method combines blockwise quantization with dynamic tree quantization and a stable embedding layer. The method has shown effectiveness on WMT GLUE and Moco tasks.  effectiveness: Impressive empirical results opensourced codebase significant memory saving and time saving.  Weaknesses: Dynamic tree quantization and stable embedding methodology are reasonable but not surprising. Dynamic tree quantization is modified which seems to be reasonably motivated. Appendix E supports the argument despite implicit assumptions. These assumptions should be thoroughly discussed in the appendix. Question: 1. Why image tasks do not have as much saving as NLP tasks In Table 1 imagerelated tasks savings are marginal versus NLPrelated tasks. Is there a way to make this work for image language tasks as substantially"], "zPLQSnfd14w": ["Paraphrased Statement: This paper analyzes the Rademacher complexity of the family of Euclidean metrics learned through an Llayer neural lastwork with Lipschitz activation functions. The proof leverages an \\epsilonlast for the embedding lastwork devised by Barlett Foster and Telgarsky (2017). This last allows for the construction of a last for the metric distance with only a constant alteration. Standard arguments are then employed to bounds the Rademacher complexity. Specifically for Metric Learning: For the metric learning scenario the paper demonstrates an improvement over Bartlett Foster and Telgarskys boundss by considering the density of the last lastwork layer. The authors argue that this dense layer context is frequently encountered in use. Additionally their bounds aligns with known boundss for linear cases. Strengths and Weaknesses: Strengths:  The authors enhance Bartlett Foster and Telgarskys boundss for metric learning contexts by examining the last layers density.  Empirical evidence supports the prevalence of the dense last layer context.  The papers bounds aligns with known linear case boundss. Weaknesses:  Despite the improvement the authors believe it may not be substantial due to the dependence on the lastworks depth and width.  The paper lacks discussion on how the boundss relate to generalization error boundss particularly their empirical significance. Questions: 1. The boundss are not completely dimensionfree due to the explicit appearance of the depth L. Additionally increased width can lead to larger matrix norms indicating that both width and depth work the bounds. 2. The magnitude of the matrix norms during training and their impact on the boundss size are unclear. 3. The authors assertion that the complexity must be lower boundsed by the square of the operator norms of the matrices is not fully understood. If true it would signify a significant improvement in the bounds."], "jJis-v9Pzhj": ["Paraphrased statement: Summary:  This work examines the Positive and Unlabeled (PU) learning problem.  It presents a twostep method for estimating pseudolabel uncertainty allowing for the assignment of more reliable pseudolabels and improved predictive performance.  This estimation method differs from previous approaches. Strengths and Weaknesses:  Assumptions:  The paper assumes a labeled validation set with positive and negative examples for early stopping.  It also assumes known proportions of positive and negative samples.  Both assumptions are not practical in realworld scenarios.  Lack of Comparison:  The paper does not compare its proposed method to existing approaches that do not make these assumptions such as PAN (\"Predictive Adversarial learning from Positive and Unlabeled data\").  Lack of Comparison with Prior method:  The paper acknowledges the existence of earlier pseudolabeling techniques but does not provide an experimental comparison to these approaches to demonstrate the superiority of its proposed method.  Clarity:  The paper is easy to comprehend."], "qrdbsZEZPZ": ["Summary: This paper establishes that differential privacy guarantees certified robustness against poisoning attacks in federated learning scenarios. Strengths:  Correct results  strong experiments  comprehensive research from theory to evaluation Weaknesses:  Similar results to prior work ([1]) raising concerns about novelty  Restriction to federated learning setting limits generalizability  Inclusion of instancelevel DP section despite userlevel DP preservation in federated learning applications Overall: The paper is currently not suitable for publication. The authors are advised to revise the presentation to address these concerns."]}