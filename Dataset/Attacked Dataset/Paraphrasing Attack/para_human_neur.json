{"pkzwYftNcqY": ["After the authors response and review of other reviews I upgraded my score from 4 to 6. This paper presents a unified framework for three types of kernelbased hypothesis tests (Twosample Independence and Goodnessoffit). It highlights the advantages of using incomplete Ustatistics for efficiency and statistical significance. The authors also aggregate tests across multiple kernels providing theoretical insights into their efficacy. While the experiments on toy data demonstrate the benefits of incomplete Ustatistics more applications with larger datasets could showcase the aggregation procedures effectiveness. Practitioners could benefit from practical guidance on kernel selection and aggregation parameters. Despite theoretical limitations regarding lineartime tests and handling imbalanced samples this paper contributes to the understanding of kernelbased testing.", "Paraphrased Statement: After rebutting critiques the papers score improved to 7. Original Statement: Summary POST REBUTTAL Score increased to 7. In this work the authors propose faster than quadratic tests for the twosample independence and goodnessoffit problems using the Maximum Mean Discrepancy (MMD) Hilbert Schmidt Independence Criterion (HSIC) and Kernel Stein Discrepancy (KSD) respectively. They are based on incomplete U statistics that can interpolate between linear time and quadratic time costs (the latter cost is incurred by typical tests which are complete Ustatistics). Strengths and Weakness The authors provide a tradeoff between the computational cost used and the power achieved in Theorem 1while achieving the minimax rates over Sobolev balls when using the quadratic runtime variant. The authors then use this result to also achieve appropriate power results for kernel selection (up to logarithmic inflation in the number of kernels). Notably this result is adaptive and does not require the knowledge of the smoothness parameter of the difference between the null and the alternative density. The authors also provide several experiments which demonstrate the advantages of the proposed methods. The writing of the paper is pretty clear and worth appreciating Questions 1. The work in Huggins and Mackey (e.g. the L1 IMQ and Cauchy RFF random feature Stein discrepancies) were all linear and not quadratic time as the authors mention in l 53. Given the focus on linear time tests in this work I believe that these tests should be treated as a useful baseline for goodnessoffit comparison experiments. In particular Huggins and Mackeys experiments showed that their tests typically outperformed the FSSD test (which is one of the key linear time baselines in the current work). 2. In the separation rate the dependence on alpha is logarithmic but that on beta is polynomial (1x)is the latter unavoidable I can see from the proof that it is because of the nature of concentration inequalities used in the two contexts (namely Rademacher chaos concentration and Markovs inequality)but are the arguments known to be tight Does there exist a setting where such dependence is necessarily needed 3. Is the tradeoff in Theorem 1 tight I can see its tight when L N2 but is it tight for smaller values of L Some discussion on this would be very useful. 4. Do you not need any requirements on p for Proposition 1 And does only the difference pq need to lie in the Sobolev ball for Thm 1(ii) 5. Does the choice of design D not matter Does it have to be an iid subsample Can it be adaptive (My guess is all the arguments go through relying on iidness of data points in D). Minor comment It would be easier to process the results if the assumptions on densities are stated in an assumption environment and then referenced in the theorem results.", "Paraphrase of Summary This study introduces a family of nonparametric tests (including twosample independence and goodnessoffit tests) based on incomplete kernelbased Ustatistics. These tests are shown to be valid (Theorem 1) and have guaranteed power under certain assumptions about the underlying densities. The power guarantees are initially proven for a statistic that depends on the true density smoothness but an adaptive estimate is later developed (Theorem 2). Compared to existing independence test results a tighter bound with a better dependence on the type 1 error probability is achieved (Theorem 3). Experiments demonstrate the performance of the tests under varying hyperparameters and compare them to other nonparametric tests. Strengths Provides rigorous theoretical results including minimax optimality for an adaptive estimator. Unifies discussions of twosample independence and goodnessoffit testing. Weaknesses Uses excessive notation that is not always clearly explained. Could provide more explicit explanations of notation and distinctions from prior work. Questions 1. Redundancy in Lemma 1 variance bound. 2. Motivation for choosing order 2 Ustatistics. 3. Novelty of Theorem 3s improved dependence on 1\u03b1. 4. Interpretation of green curve in Figure 1 and absence of other methods results. 5. Advantages of proposed tests over previous minimax optimal tests."], "r70ZpWKiCW": ["Paraphrased Statement: This research introduces a supervised segmentation method utilizing a teacherstudent framework. In conventional approaches the teacher provides pseudolabels for unlabeled data which are then used as training signals for the student model. However pseudolabels can be unreliable potentially hindering the students performance. To address this the paper proposes an assistant model that takes pseudolabels and transfers feature representations to the student model through an exponential moving average (EMA). This aims to reduce the noise associated with pseudolabels. Additionally a weighted pseudolabeling scheme is used to train the assistant model. Experiments on Pascal VOC and Cityscapes datasets showcase the effectiveness of the assistant model in improving segmentation performance. Strengths: Clear and concise writing Simple and efficient method for reducing pseudolabel noise Feature transmission technique has potential applications in other tasks Demonstrated performance improvements on multiple datasets Weaknesses: Lack of detailed explanation for certain hyperparameters and implementation details Unclear rationale for the threshold used in pseudolabeling Limited experimental results on smaller labeled datasets leaving uncertainty about performance with larger data sizes Superior performance of the teacher compared to the student model which warrants further discussion Missing References: Adversarial Learning for SemiSupervised Semantic Segmentation BMVC18 Semisupervised semantic segmentation with high and lowlevel consistency PAMI19 SemiSupervised Semantic Image Segmentation With SelfCorrecting Networks CVPR20 SemiSupervised Semantic Segmentation With PixelLevel Contrastive Learning From a ClassWise Memory Bank ICCV21 Questions: What is the value of \u03c4 in Equation (6) How is the threshold for pseudolabeling selected and is \u03b3 a constant or variable How would Table 47 results change with larger labeled datasets Why does the teacher model perform better than the student in all experimental settings", "Paraphrase: Summary: This paper focuses on improving semantic segmentation accuracy using a semisupervised approach. The authors introduce a \"teaching assistant\" model that supports the \"student\" model in their learning journey. This technique isolates the impact of incorrect pseudo labels (estimated labels) on the feature extraction and mask prediction components of the model. By protecting the student model from these unreliable labels the method enhances segmentation accuracy. Experiments demonstrate the methods competitiveness against existing approaches. Strengths and Weaknesses: The method is straightforward and impactful. The \"teaching assistant\" concept is original. While decomposing the segmentation task is not novel the analysis of pseudo label effects for these tasks is innovative. The paper is clear and concise. Questions: The authors have not provided statistical significance analysis for their experimental results. Did they average results across multiple runs with different random seeds They have not but they intend to release the code when the paper is accepted.", "Summary Paraphrase: Strengths: The proposed extension to the teacherstudent paradigm is innovative and effective. The method addresses a significant research topic in semantic segmentation. The method outperforms previous approaches on established benchmarks. Extensive experiments and ablation studies provide validation. Clear illustrations complement the text enhancing comprehension. Weaknesses: Certain design choices require further explanation. Some experiments are incomplete. Potential discrepancies in experimental setup and reported results raise questions. Questions: Why restrict GTA training to pseudo labels Could supervised training using labeled data improve feature representations Are labeled and unlabeled data splits consistent in the evaluation tables How are training samples distributed among the GTA Teacher and Student networks Did the authors explore training GTA with supervised labels and the Student with semisupervised labels Do all networks share the same feature extractor and mask predictor How does the reweighting mechanism affect performance beyond the initial pseudo label filtering What are the implications of not warming up the networks on labeled data What is the maximum achievable performance on a combination of labeled and unlabeled data Suggested Clarification: Consider renaming \"SupOnly\" to better reflect its hybrid nature. Maintain consistency in naming conventions for the TeacherStudent and SupOnly paradigms. Provide more details on experimental setup data splits and training procedures.", "Paraphrased Statement: Summary: The authors contend that in semantic segmentation adding pseudolabeled data is more advantageous for improving the feature extractor than using a mask predictor. Thus they introduce the Gentle Teaching Assistant (GTA) model which leverages pseudo labels and exclusively transfers feature representations to the student model. Strengths: Novel GTA model that uses pseudo labels to enhance feature representation learning. Clear and logical reasoning. Comprehensive ablation studies. Weaknesses: Despite using only pseudo labels its unclear how GTA effectively transfers feature representations to the student model. Potential for improvement in presentation (conciseness and clarity). Limited qualitative results (more examples would enhance the paper). Questions: Why is GTA limited to using only pseudo labels rather than leveraging both labeled and pseudolabeled data How is GTA initialized and trained (from random initialization or with pretraining) Is it crucial to maintain parameter similarity between GTA and the student model Additional qualitative results (e.g. in the supplementary material) would be beneficial. An ablation study exploring EMA hyperparameters is missing. In Table 5 does the difference between the proposed method and original EMA involve updating only the decoder parameters"], "zbuq101sCNV": ["Summary: The paper introduces a technique to create a reflectance map (and lighting environment) for a 3D shape from a textual description using a CLIPbased loss function. Strengths: Novel approach to geometrytexture generation using text input. Plausible results for various examples. Incremental contribution to the field of textguided geometrytexture generation. Weaknesses: Lack of justification for employing SVBRDF environment maps over direct texture fitting. Unclear whether the fitted lighting and material properties can generalize beyond a single viewpoint. Opaque and overly technical presentation of the method. Questions: Are there constraints on the computed surface normals such as consistency with the geometry or neighboring vertices What limitations exist on the deviation of surface normals from the original geometry or vertex normals", "Summary This research aims to refine 3D models based on written instructions. It combines three features: altering the shape of the model (geometry) simulating light conditions and modifying the material properties (Bidirectional Reflectance Distribution Function or BRDF). The model is trained using CLIP Loss under a spherical Gaussianbased renderer that can produce realistic images. The results show that the model can generate more realistic and visually appealing 3D models than existing methods and is also effective in refining lowquality meshes. Strengths Innovative approach: The research introduces a novel technique that uses a realistic shading model to improve the appearance and geometry of 3D models. Clear presentation: The paper is wellorganized and easy to understand. Convincing results: The experiments demonstrate the improved rendering quality and 3D geometry consistency of the stylized meshes. Acknowledge limitations: The researchers acknowledge potential weaknesses in the current approach. Weaknesses Disentanglement of light and reflectance: It is unclear how the model effectively separates light and reflectance information. Using only CLIP Loss may not provide sufficient incentive for the model to learn this distinction. Quality of stylization results: The quality of the stylized meshes in some examples (e.g. \"A shoe made of brick\") may not be as impressive as desired. Quantitative evaluation: The quantitative results in Table 1 lack clarity. A more detailed user study should be conducted separately for geometry and appearance aspects. Questions Data augmentation: Why were randomly cropped images chosen for data augmentation instead of multiview images like Text2Mesh Could the use of cropped images contribute to the models focus on detail Random background: What is the purpose of adding a random background to the rendered images during data augmentation Is it necessary Control over lighting: How can the intensity and direction of lighting be explicitly controlled or determined", "Paraphrase: This paper introduces a method for 3D mesh stylization that combines both visual and geometric elements. Unlike existing methods that focus on color and displacement adjustments this approach employs a physicsaware rendering model that simulates geometry (normal map) material properties (diffuse roughness spectrum) and lighting (shade graph). This model is claimed to produce more realistic and detailed results and it allows for editing capabilities such as relighting. Strengths: Promising concept of physicsaware modeling for mesh stylization Visually appealing results Technically sound and practically feasible rendering formulation Wellwritten paper Weaknesses: Lack of results showcasing individual components (normal maps BRDFs lighting) and their quality compared to Text2Mesh Limited demonstration of editing capabilities particularly relighting and material modification Approximation of geometry with normal maps limits actual geometric changes Text2Mesh provides joint geometry and appearance stylization with geometric detail potentially surpassing this method Technical contributions are based on existing works limiting innovation Questions: Are the concerns raised in the \"Weaknesses\" section adequately addressed Can convincing evidence be provided to support the methods advantages over Text2Mesh Does the method truly advance the stateoftheart given its limitations and the lack of substantial technical contributions", "Paraphrase: Summary: This research aims to modify a 3D mesh based on a text description. It describes the meshs appearance as spatially varying surface attributes (normal and lighting) and aligns the visual features of the rendered image with the given text prompt. By optimizing the parameters of these attributes the method achieves better results than the Text2Mesh approach for textbased mesh styling. Strengths: Clear and comprehensible presentation. Novel approach of representing appearance as spatially varying surface attributes. Weaknesses: The method focuses on surface shading and lighting which may limit its ability to handle geometric modifications. The paper does not address the inherent limitations of using a shadingbased approach. Questions: Method: How does the model determine accurate lighting parameters using only CLIP latent space supervision Implementation: What criteria are used to select the anchor view and find the first intersection point and face Novelty: Despite drawing inspiration from NeRD and Text2Mesh the papers contribution and novelty are potentially limited."], "rF6zwkyMABn": ["Summary (Paraphrased): This paper designs improved \"bestofbothworlds\" algorithms for online learning with feedback graphs a setting where predictions are made in rounds losses are incurred and partial feedback is received based on the selected actions. These algorithms aim to minimize regret the difference between the learners loss and that of the best fixed prediction in hindsight. The proposed algorithms extend previous work to handle general feedback structures addressing both strongly observable (with an independence number \u03b1) and weakly observable (with a domination number \u03b4) feedback graphs. For strongly observable graphs nearoptimal regret bounds are achieved: \u221a\u03b1T for adversarial losses and \u03b1ln(T)\u00b3\u0394 for stochastic losses (where \u0394 is the difference between the expected loss of the best and secondbest actions). For weakly observable graphs the bounds are \u03b4\u00b9\u00b3T\u00b2\u00b3 for adversarial losses and \u03b4ln(T)\u00b2\u0394\u00b2 for stochastic losses. Strengths: New and general results that improve upon previous work. Substantial improvements in regret bounds for stochastic losses. Novel analysis of FollowtheRegularizedLeader (FTRL) with Shannon entropy regularization. Weaknesses: Stochastic loss bounds may be suboptimal by a factor of ln(T)\u00b2. Optimal bounds for weakly observable graphs remain elusive.", "Summary: The authors propose algorithms for online learning with directed feedback graphs. These algorithms use a \"followtheregularizedleader\" (FTRL) approach achieving optimal regret bounds in both fully observable and weakly observable settings under adversarial conditions. The bounds are polylogarithmic in the number of time steps in stochastic settings. In the fully observable case the algorithm uses FTRL with negative entropy regularization and an adaptive learning rate scheme. In the weakly observable case the regularization depends on a weakly dominating set with negative entropy regularization for vertices without selfloops and Tsallis entropy for vertices with selfloops. Strengths: Improves on existing regret bounds for online learning with feedback graphs. Relevant for general directed feedback graphs and weakly observable graphs. Shows that FTRL with negative entropy regularization is sufficient for nearly optimal regret bounds in the fully observable setting. Provides the first bestofbothworlds regret bounds for weakly observable graphs. Weaknesses: Stochastic bounds may be suboptimal in certain cases. Algorithm requires knowledge of the independence number of the feedback graph in the fully observable setting which is computationally challenging. Questions: Authors define a weakly dominating set differently from previous works potentially leading to suboptimal regret bounds. Use of Tsallis entropy on vertices with selfloops in the weakly observable setting introduces an additional term in the stochastic bound. Exploring alternative approaches using negative entropy regularization alone could potentially improve this bound.", "Paraphrased Summary: This paper explores the \"bestofbothworlds\" problem in bandit learning with feedback graphs. It introduces an algorithm with nearoptimal regret bounds in the worstcase scenario and satisfactory regret guarantees in the probabilistic setting. While the probabilistic bound is not completely optimal it surpasses existing algorithms. Moreover the paper introduces algorithms for bestofbothworlds problems in partially observable environments. Strengths and Weaknesses: The papers mathematical content is sound and presented clearly. It offers technical advancements over traditional regret analysis due to the use of Shannon entropy for optimal regret in worstcase scenarios. A significant limitation is the existence of a very similar paper with nearly identical results and techniques. However this paper takes a simpler approach using selfbounding which allows for a straightforward probabilistic corruption bound. It also provides unique analysis for partially observable graphs. Questions: Are you aware of the highly similar paper \"A NearOptimal BestofBothWorlds Algorithm for Online Learning with Feedback Graphs\" Given the substantial overlap in results and techniques consider exploring the possibility of collaborating on a combined paper.", "Paraphrased Statement: Summary: The study explores the BestofBothWorlds model for online learning with feedback graphs. Here the rewards for different options can be random or deliberately harmful and feedback is represented through a directed graph of options. The researchers demonstrate that for highly observable graphs the FTRL algorithm with a negative entropy regularizer and adjustable learning rates yields nearoptimal regret bounds in both random and adversarial scenarios. For less observable graphs a more complex regularizer delivers similar outcomes. Strengths: The work addresses a significant and challenging problem. The proposed solutions are straightforward and wellpresented. The analysis is concise and appears sound. Weaknesses: The regret bounds include logarithmic terms that are not optimal. The choice of regularizer for less observable graphs is unclear. The researchers could provide more insight into its derivation. The gapdependent constants in the bounds are not optimal especially when there are many options with small gaps. Questions: The researchers are asked if it is possible to derive bounds proportional to \u2211i\u2208S 1\u0394i for highly observable graphs using their approach."], "zK6PjBczve": ["Paraphrased Statement: Summary: Haplotype assembly is essential for various tasks including identifying structural variations imputing genotypes from SNP panels and reconstructing viral \"quasispecies.\" The process becomes more challenging with deeper sequencing data higher ploidy increased sequencing error and the presence of multiple related strains. Assembly Problem: For diploid sequences (with N haplotypes) the assembly problem is defined as assigning reads to N haplotypes such that the minimum sum of Hamming distances between each read and its assigned haplotype is achieved. This problem is computationally challenging (NPhard). Proposed Solution: The authors relax the problem by assigning reads probabilities of belonging to each haplotype and solving the original problem by selecting the haplotype with the highest probability for each read. They then propose a heuristic loss function that penalizes inconsistent assignments and conflicting (overlapping and consistent) assignments. Optimization and Tuning: The loss function is minimized using a neural network (GNN) with optimized initial embeddings. The resulting assembly is further refined to minimize the minimum error correction (MEC) metric. Strengths: 1. Accessibility of data and code 2. Proposal of a new objective for assembly that leads to improved performance 3. Successful application of the method in benchmark tests Weaknesses: 1. Limited justification for the use of the GNN in the benchmarks 2. Benchmarks focus on small datasets and lowcomplexity assembly tasks 3. Ambiguity in some concepts presented in the main text 4. Lack of comparison to alternative phasing metrics Questions: 1. Could gradient descent be applied directly to the haplotype membership probabilities instead of using a GNN 2. Is the method scalable to assembling haplotypes over entire chromosomes 3. Why was the MEC metric used exclusively for performance evaluation", "Paraphrased Statement: This research examines the haplotype assembly and phasing problem as a form of graph coloring. It presents a solution using graph representation learning that is improved through local search. The problem has a unique graph coloring formulation that includes both consistent (similarity) and conflict (dissimilarity) edges unlike traditional graph coloring. The authors propose a loss function to address both types of edges leading to superior performance in experiments compared to existing methods. Strengths: Converts the haplotype assembly and phasing problem into a modified graph coloring problem enabling the use of techniques from that field. Develops a loss function that captures both similarity and dissimilarity relationships between vertices using consistent and conflict edges. This function has potential for broader application in graph neural networks. Validates the approach through experiments on synthetic and realworld datasets demonstrating its effectiveness over alternative methods. Weaknesses: Limited information is provided on the specific definitions of the COMBINE AGGREGATE and MESSAGE functions. Questions: 1. Could you elaborate on how the COMBINE AGGREGATE and MESSAGE functions are defined 2. How would you envision the generalization of concepts related to consistent and conflict relationships to graph neural networks more generally", "Paraphrase: Summary The paper introduces a Graph Neural Network (GNN)based approach to group together short DNA sequences (reads) that originate from a random process of obtaining shorter sections with errors from a few similar but distinct longer sequences (such as viral quasispecies genomes). The ultimate goal is to reconstruct these longer sequences from the collected short reads. Strengths and Weaknesses The problem and technique are generally welldefined although specific design choices are made without sufficient explanation and appear questionable. Specific Concerns: Using only Single Nucleotide Polymorphism (SNP) positions to determine if reads overlap may exclude reads that have no SNPs but overlap in nonSNP positions. Conflicting definitions: \"nonoverlapping\" and \"consistent\" are defined as having 0 SNP differences while \"conflicting\" reads have \u2265 q SNP differences. This creates a situation where overlapping reads with q1 SNP differences fall into neither category. Stringent requirement: \"any two conflicting reads have two different colors and any two consistent reads have the same color\" may not apply in all cases as illustrated by an example with three quasispecies and three overlapping reads. The GNN approach for assigning reads to quasispecies involves minimizing a loss function with two terms: consistent and conflicting interactions. This is an extension of existing work indicating limited novelty. Toy Example: The toy example in Figure 2 appears simplistic and suggests that the problem can be solved by identifying connected components in a graph considering only consistent edges. Questions: Please elaborate on the assumptions and design choices mentioned above particularly regarding the use of SNPs and the definitions of \"consistent\" and \"conflicting.\" Discuss the novelty of the proposed GNN approach in comparison to prior work.", "Summary Paraphrase: The authors have developed a technique that aims to determine the genetic makeup of individuals in species with multiple sets of chromosomes (polyploids) and viruses that exist as a collection of closely related variants (quasispecies). Their innovative approach utilizes neural networks to process data represented as graphs. Strengths: The authors present a comprehensive explanation of the challenge and offer a unique solution. Their method performs significantly better than earlier approaches indicating its potential for widespread adoption in biological research. Weaknesses: The lack of a generative model for the problem limits the understanding of the statistical properties of the estimation. The assumed objective (Hamming distance) while common in the field impedes connections to broader statistical literature and established methods for clustering data with missing values. Questions: How does the Hamming distance (objective) relate to the relaxation constraints in Equations 2 and 3 Could the authors provide a mathematical derivation of Equation 3 from Equation 2 There are typographical errors in the text: \"ployploid\" should be \"polyploid\" (line 94) and \"quasiqpecies\" should be \"quasispecies\" (line 287). The title of Table 3 states that there are 3 viral quasispecies datasets but 10 are listed. The color coding in Figure 3 is not explained. Please clarify which haplotype assignment is used as the reference."], "mxnxRw8jiru": ["Paraphrase: Summary: This research introduces a novel technique for privatizing highdimensional data generation that improves the quality of synthetic samples rather than the underlying generative models. Experiments on MNIST and FashionMNIST datasets illustrate its effectiveness. Strengths and Weaknesses: Originality: The proposed approach appears to be novel. Relevant literature is thoroughly discussed. Quality: The algorithm is generally reasonable and no technical errors were identified. However specific implementation details and experimental results are missing. Clarity: The paper is clear and wellstructured. Significance: The results show promise but their sufficiency is debatable. Only extreme privacy budgets (\u03b51 or 10) were tested leaving the performance at moderate budgets (e.g. \u03b55) unexplored. Benchmarking on colored image datasets (e.g. CIFAR10) would enhance the studys significance. Questions: 1. How is \u03c3 calculated in Algorithm 1 to ensure (\u03b5\u03b4)differential privacy 2. Provide numerical values for the computational cost of the algorithm particularly for different values of spc (e.g. 10 20 or 6000) for MNIST.", "Paraphrased Statement: Summary: This paper proposes a new approach for generating synthetic data to replace sensitive data for release. Unlike traditional methods that model the entire data distribution this approach directly updates data points to maximize their usefulness for a specific downstream classification task. The synthetic data is used to train a classifier while the original data trains a separate classifier of the same design. The synthetic data is updated to minimize the difference in network gradients between the two classifiers. Privacy is preserved by applying Gaussian noise to the gradients of the classifier trained on the original data before using them to optimize the synthetic data. Evaluation: The proposed method is compared to existing techniques in image classification tasks using convolutional neural networks on MNIST and FashionMNIST datasets. Classifiers trained on synthetic data generated by the proposed method achieved higher accuracy than those trained on data from competing methods. The proposed method allows for smaller synthetic datasets without significantly sacrificing accuracy reducing training complexity. The method is robust to different downstream task architectures as long as they use convolutional layers. Limitations: The synthetic data may deviate from the original data during unconstrained optimization. A proposed modification ensures the synthetic data remains on the original data manifold but its performance is not directly compared to the unconstrained method. Strengths and Weaknesses: Originality: The paper presents a novel approach to synthetic data generation under differential privacy (DP) guarantees. However similar methods may exist in nonprivate data distillation techniques. Significance: The proposed method demonstrates superior performance for downstream tasks making it valuable for releasing sensitive data when the downstream task is known or predictable. Soundness and Quality: The method uses established privacy techniques so privacy results from previous work apply. The experiments lack error margin information. Clarity: The paper is generally clear and comprehensible. Additional Questions: Line 141143: Clarify the statement regarding differences between original and synthetic data.", "Paraphrase: Summary This paper suggests using dataset condensation techniques as an alternative to generating differentially private data. It employs DPSGD to minimize the difference between the gradients of real data and synthetic data on a specific task. This approach produces data that outperforms the traditional method of training a discriminator and a generator in terms of downstream classification accuracy. However the paper acknowledges potential limitations in terms of scalability and applicability to more complex datasets. Strengths Clear and concise writing adequately discussing contributions and limitations. Significant improvement in data utility compared to previous differentially private data generation methods resulting in higher downstream classification accuracy. Weaknesses Limited technical contribution primarily involving existing techniques (DPSGD and gradient matching). Exploration of only one algorithm from data condensation (gradient matching). Lack of comparison with the visual quality of data directly generated from an external generator. Minor grammatical error (line 121: \"minimized minimize\"). Suggestion to consider alternative data condensation algorithms such as distribution matching for a more comprehensive understanding."], "oQIJsMlyaW_": ["Paraphrase: Summary: This paper introduces a new pruning technique that combines gradient and magnitude criteria and integrates them along the path of neuron removal. It also suggests a finetuning process to enhance network performance. This approach involves a layerbylayer strategy that sets specific pruning ratios for each block. The paper demonstrates that SInGE performs well on ResNet56 for Cifar10 and ResNet50 and MobileNetV2 for ImageNet. Strengths: The integration criterion which significantly contributes to SInGEs superior performance. The integration criterion is wellreasoned and the ablations support the authors design decisions. SInGE achieves stateoftheart results on CIFAR10 and ImageNet. Weaknesses: The paper does not explain how pruned network FLOPS and parameters were computed. This is crucial since direct comparisons to other methods are not made and reported figures may be significantly affected. The paper employs an iterative finetuning scheme but compares to singleshot pruning methods. This gives SInGE an advantage making it difficult to isolate the impact of the pruning criteria. The computational cost of evaluating the criterion is not mentioned. Due to the Riemannian Integral it should be notably higher than other gradient magnitude or firstorder methods. A direct comparison to a competing method within the same pruning pipeline would clarify the performance gains from the finetuning process and criterion. Questions: Why are not the same methods compared across all datasets Why was the product of output gradient and weight norms chosen Other gradients such as loss gradient could be considered.", "Paraphrased Statement: This paper introduces a novel neuron pruning method that leverages the Integrated Gradients (IG) attribution algorithm. The proposed method outperforms existing magnitude and local gradientbased approaches by considering the cumulative gradient effects along the weight path from zero to the assigned weight at each layer. Through extensive experiments the paper demonstrates the superiority of IGbased pruning over various stateoftheart structured and unstructured pruning techniques as measured by the number of removed parameters and floatingpoint operations. Additionally the paper proposes a combined pruning and finetuning procedure that enhances model accuracy during the pruning process. The experimental evaluations involve the ImageNet and CIFAR10 datasets using the MobileNet V2 backbone. Strengths: Clear exposition of the underlying concepts and related research Rigorous experimental validation demonstrating the superiority of IGbased pruning Novel entwined pruning and finetuning approach that improves accuracy Weaknesses: Readability could be enhanced especially in describing the notations and structured pruning techniques Figure 1 and its accompanying text require clarification Limited explication of the entwined finetuning process Minor Comments: Notation Nn is introduced without explanation Table 6 contains two SInGE rows with unclear distinctions The paper does not specify which Integrated Gradients variant (CSG2 or CIG2) is used in the reported results Questions: Can the authors elaborate on the differences between Integrated Gradients CSG2 and Integrated Magnitude x Grad CIG2 Would the Noisegrad paper which incorporates stochasticity into model weights have relevance to this work", "Paraphrase: The authors propose a new metric for model pruning inspired by the integrated gradient attribution method. This metric captures the influence of setting a parameter to zero by approximating the integral of the product between the parameter weight and its attribution along a path between its current value and zero. The metric provides a more comprehensive view compared to earlier magnitudebased and gradientbased pruning methods. The authors then introduce SinGE a method that combines pruning with network finetuning. Empirical evaluations show that SinGE performs well compared to both structured and unstructured pruning methods on Cifar 10 and ImageNet datasets. Strengths: 1. The motivation for the new metric is clear and wellfounded. 2. The paper is wellwritten and easy to follow. Weaknesses: 1. Figure 1 is not sufficiently clear or informative to support the arguments in Section 3.1. 2. The claim that SinGE outperforms existing pruning methods is not fully supported by the empirical results as some methods remove more parameters or FLOPS with similar or better accuracy. Questions: 1. The integration evaluation can be timeconsuming. Do the authors consider adding an experiment to assess the time efficiency 2. There are several typos such as \"range of \\mu\" at line 138 \"tradeoffs\" at line 167 and incomplete parentheses at line 181. 3. Some of the bolded numbers in the tables do not correctly indicate the best results potentially misleading readers. 4. The top1 accuracy remains unchanged (85.38) for the integrated magnitude method without finetuning and with 1000step entwined finetuning. Please explain why no accuracy improvement is observed. 5. What does \"1\" represent in Table 1 6. Provide error bars for the results in Tables 2 and 6 to demonstrate the statistical significance of the differences. 7. Does the performance in Table 3 include finetuning If so was the same entwined finetuning strategy used for all methods 8. Only the accuracy of SinGE is provided in Table 6 when 90 of parameters are removed. Include the performance of other methods at this setting to justify the superiority of SinGE."], "vy7B8z0-4D": ["Paraphrase: Problem Statement: Brain images from different individuals vary in shape and function which makes direct comparisons challenging and requires alignment. Proposed Solution: The paper introduces a new brain alignment method (FUGW) that considers geometric functional and fundamental regional differences to achieve a more accurate alignment. Strengths: Clearly defined problem statement and motivations Wellstructured and written paper Simple examples illustrate method differences Technically sound method with proper references Demonstrated superiority in numerical experiments Weaknesses: Impact of improved alignment on downstream experimentscomparisons not explored Computational comparison with existing methods (MSM) missing Scalability issues in Experiment 2 with other methods unable to process the data Questions: How does the improved alignment impact subsequent experiments or comparisons Does the numerical advantage translate to practical applications or enhanced understanding Can the method handle brain images with missing regions of interest", "Summary The authors present a mesh registration method based on \"optimal transport\" a technique used to find the best matches between two sets of points. This method is designed specifically for aligning cortical meshes in neuroimaging applications. They introduce a \"GromovWasserstein\" loss that penalizes large deformations while considering the geodesic distances between points on the mesh. Additionally a \"Wasserstein\" loss ensures that corresponding points are matched. The transport method allows for imbalances and restrictions in the matching such as requiring that certain regions match exactly. Results Experiments on synthetic and real data show promising results. The method outperforms the baseline Multisurface Matching (MSM) on synthetic data. On real data the method achieves similar functional correlation as MSM with default parameters. Strengths Wellwritten and clear paper. Strong technical foundation in optimal transport. Implementation of a standard solution method seems sound. Weaknesses Method requires a dense interaction matrix limiting it to meshes with around 10000 vertices. Baseline method is untuned potentially underestimating its performance. Evaluation primarily relies on functional correlation which may not fully assess the accuracy of the registrations. Questions How can we ensure the registrations are reasonable beyond functional correlation Could alternative evaluation metrics or data sources provide more insights What are the limitations of avoiding diffeomorphic constraints given the large displacements observed Do extreme topological or geometric changes occur due to the lack of diffeomorphic constraints Comments Reference to Robinson 2014 should be added to the text. More details on the solution method and mesh reduction method would be helpful.", "Paraphrase: Summary: This study presents a new unsupervised image registration method the Fused GromovWasserstein Optimal Transfer (FUGW) algorithm which optimizes an unbalanced fusion problem. It was tested on fMRI data to enhance signal alignment. Strengths and Weaknesses: Unbiased registration from individual images to a common space and between individuals is essential in medical imaging. The FUGW algorithm offers a novel and promising approach for this task. It incorporates both image and feature information a valuable feature for multimodal image integration. However the evaluation strategy needs improvement. Questions: 1. The registration evaluation should include comparisons based on anatomical features such as gyri and sulci to ensure alignment accuracy. 2. The parameter \u03b1 which balances anatomical validity and registration effectiveness should be estimated more intuitively to prevent unwanted voxel movement. 3. For clarity more details are needed on: Definition of ws and wt in the equation Mesh construction and update procedures for the barycenter The meaning of \"fixed mesh\" in the first experiment"], "x5ysKCMXR5s": ["Paraphrase: This research explores sparse principal component analysis (PCA) using incomplete and noisy data introducing a semidefinite programming (SDP) relaxation approach. While this formulation had been previously used for sparse PCA with complete data this study demonstrates its applicability to incomplete observations as well. The paper presents conditions for precise support recovery and includes experiments using both artificial and realworld data. Strengths: Clear and wellstructured paper Strong theoretical approach to sparse PCA with incomplete data through SDP relaxation Evaluation using gene expression data Weaknesses: The paper does not address how the SDP method compares to alternative approaches for incomplete data sparse PCA. The gene expression data experiment could benefit from additional visual representations in the appendix. Questions: How does the SDP method perform compared to other approaches for incomplete data sparse PCA Can the paper provide additional visualizations of the gene expression data experiment to enhance the results", "Paraphrase: Summary: The research explores the issue of recovering sparse principal components from incomplete and noisy data. It focuses on the semidefinite programming (SDP) reformulation of this problem and presents a novel algorithm with theoretical insights into the probability of a unique optimal solution. Numerical trials on simulated and realworld gene expression data demonstrate the methods efficiency. The study offers intriguing ideas supported by sound theories. Strengths: The research provides several conditions to ensure the effectiveness of the SDP algorithm. It makes theoretical contributions to the field. The application to gene expression analysis is both fascinating and valuable. Weaknesses: The suggested algorithm relies on the SDP reformulation of the original sparse PCA problem which may constrain the algorithms development. Section 4.1 does not contrast the approach with other relevant methods. Questions: 1. In Section 2.2 should the eigenvectors \\mathbfuis be normalized 2. Does the SDP formulation in page 2 guarantee a rankone solution and even X\\mathbfx\\mathbfxT 3. What is p in Theorem 1 How is it defined Also the existence of the optimal solution is not addressed.", "Paraphrased Statement Summary: This study presents a simplified version of the nonconvex L1regularized sparse PCA problem suitable for situations with limited noise and missing data. By transforming the problem into a convex programming form the method ensures the accurate identification of the sparse leading eigenvectors true support under specific conditions. Empirical evaluations validate the proposed methods effectiveness. Strengths: Provides theoretical assurances for accurately determining the sparse leading eigenvectors true support. Weaknesses: No comparisons with alternative methods. If the approach involves using 0 for missing entries to obtain matrix M existing sparse PCA techniques without missing data considerations could also be used on matrix M for comparison. The proposed method can only guarantee the recovery of the sparse leading eigenvectors support not its values. Questions: 1. Given a data matrix D with missing entries (e.g. uniformly random) how does one obtain matrix M 2. Matrix M is assumed to be symmetric. Why not explore imputing Mji using Mij This alternative should be evaluated in experiments as well.", "Paraphrased Statement: Summary: The researchers explore a mathematical method for analyzing incomplete data called sparse principal component analysis (PCA) which helps identify patterns and correlations within data. They show that under certain conditions the method can accurately estimate the primary pattern or \"principal component\" of the data despite missing information. Novelty: Previously researchers demonstrated that this method works well with complete data. This work extends the analysis to incomplete data. Concerns: While the work provides detailed technical information it lacks context and organization. It does not sufficiently highlight the key differences and challenges compared to previous research. The applicability of the model to realworld situations with significant missing data is unclear. Questions: 1. How does the model handle missing observations when some data columns have few available entries 2. Does the method require additional conditions on the original data matrix to avoid potential issues with decomposing the derived symmetric matrix 3. In contrast to previous work the proposed method does not impose sparsity on all principal components. What conditions are needed for support recovery even in the complete observation case 4. The paper should clarify the differences in analysis compared to previous work and explain the implications of the incoherence conditions in practical terms."], "tvwkeAIcRP8": ["Paraphrase: This research presents a method for reconstructing a scene from a single viewpoint under varying point light conditions utilizing a neural reflectance field. The method incorporates the inversesquare law and BRDF to simulate shading and employs rootfinding to model shadows. By explicitly incorporating both shading and shadow cues the model reconstructs the scene from a single viewpoint even for obscured regions. The study evaluated the model using diverse objects and demonstrated its ability to accurately reconstruct geometry from singleview images under various point light conditions. Strengths: Innovative NeRFbased technique that reconstructs scenes with multiple single point light conditions from a fixed viewpoint by modeling shading and shadow. Precise modeling of single point light shading using inversesquare law and BRDF and shadow using rootfinding leads to excellent results. Detailed description of the method. Quantitative and qualitative comparisons with other methods. Evaluation with occluded objects demonstrating reconstruction of unseen areas using shading and shadow cues. Weaknesses: Lack of analysis on the impact of different loss terms specifically the value of \u03b1 used for normal smoothness loss (Line 171). Questions: How does the normal smoothness loss affect the method Is the loss function similar to that used in UNISURF The results on LEGO show a significant loss of detail in studs and tracks. Is this due to the normal smoothness loss or more complex light reflections in those areas", "Paraphrased Statement: Summary: This study explores the use of neural radiance fields to reconstruct object geometry under varying directional light conditions using cast shadow cues. An optimized ray tracing approach aids in shadow computations. Strengths and Weaknesses: Strengths: Utilizes shadow cues for 3D geometry reconstruction when multiview capture is unavailable. Weaknesses: Limited practicality due to the specific setup requirements: Controlled lighting conditions with real directional light and known lighting direction. Simplification of the scene background and elimination of surface geometry that can cast shadows on the object. Potential ambiguity in realworld scenarios due to the influence of both object geometry and the surface receiving the shadow. Reconstructed geometry quality is inferior to multiview settings and decreases with fewer light sources. Questions: How does the method perform in less controlled lighting conditions with softer shadows (e.g. larger light sources) Has the proposed shadow generation method been evaluated against direct regression using an MLP Can the results include visualizations of the nearest neighbor training samples", "Paraphrased Statement: This paper presents a technique to solve nearfield photometric stereo with neural reflectance field representation. Core Concept: Leveraging both illumination shading and cast shadows to retrieve visible and concealed portions of a scene. Method: Uses a NeRFlike neural field representation to model the occupancy and reflectance fields of the scene. Employs physicallybased lighting and shadow modeling for image reconstruction. Optimizes the network jointly through volume rendering smoothness and color losses. Strengths: Estimates geometry in invisible regions from a single viewpoint. Presents novel use of shadows for estimating hidden areas without explicit shadow detection. Achieves stateoftheart geometry reconstruction outperforming existing nearfield photometric stereo approaches. Provides rigorous ablation studies validating each component of the method. Wellwritten and accessible. Weaknesses: Lack of significant novelty in individual components. Limited testing on only two datasets with simple synthetic and real data. Unclear performance with complex backgrounds or varying lighting conditions. Unexplored impact of lighting on accuracy in invisible regions. Questions: Performance assessment on more challenging datasets including those with textured backgrounds. Investigation into the influence of light count on invisible region geometric accuracy.", "Summary This research presents an innovative framework for capturing nearfield shapes using only light measurements employing both volume and surface rendering. The method is evaluated on both synthesized and real datasets demonstrating stateoftheart results. Strengths and Weaknesses Originality Combines surface and volume rendering in a unique manner. Integrates shadow constraints into 3D occupancy field modeling. Quality Lack of clarity in calculating the volumerendered image. Inaccurate depiction of MLP blocks in Figure 2. Clarity Ambiguity in sampling points along rays for surface and camera rays. Inadequate explanation of shadow clues for surface reconstruction. Absence of analysis on the impact of different shadow types on performance. Significance Explanation of shadow modelings role in shape estimation is lacking. Limited evaluation on synthetic data with unsatisfactory results on the LUCES dataset. Naive implementation of comparison methods (NeRF UNISURF) raises questions about validity. Environmental limitations due to sensitivity to background shadows. Insufficient discussion on performance in complex backgrounds. Questions How effective is the method in more complex background scenarios"], "xuw7R0hP7G": ["Paraphrased Summary: This research establishes conditions for the convergence of Gradient Descent with Stochastic Gradient Descent (GDSGD) methods. It achieves this by deriving a potential function that satisfies specific regularity conditions ensuring the desired convergence. This approach is applicable to both convex and nonconvex problems. The potential function should satisfy a geometric property to establish a onetoone correspondence between admissible convergence rates and potentials. The authors demonstrate that this strategy is applicable to problems such as phase retrieval and matrix square root. Strengths and Weaknesses: Strengths: Connects Gradient Flow (GF) and Gradient Descent (GD) analysis. Provides simple examples of admissible rates and potentials. Weaknesses: Typos and incorrect derivations. Lack of clarity in proof details especially regarding the exponential rate of GD. Inconsistent use of functions h(w) and \u03c3(h(w)). Missing explanation of how the result of Chatterjee is recovered. Questions: Mathematical typos in specific lines. Explanation of the derivation of the second inequality in line 947. Explanation of the exponential rate of GD in theorems 6 and 7. Clarification of the use of h(w) and \u03c3(h(w)) in the proofs. Proof of equation (51) and how the bound on the Hessian is calculated in line 1028. Suggestions: Provide a more rigorous statement of Chatterjees result in the Appendix. Include a conclusion section to summarize the findings.", "Paraphrased Statement: Summary: The study establishes an equivalence between meeting specific convergence rates and geometric constraints. This framework is applied to gradient descent and stochastic gradient descent with more assumptions required for the latter to derive convergence rates. The framework is applied to matrix square root and phase retrieval where convergence rates for gradient flow have been demonstrated. Strengths: The approach is original and leverages converse Lyapunov theorems from the ODE literature. The connection to gradient descent and stochastic gradient descent enables direct analysis from discrete convergence rates. The study presents a wide range of results for various applications. It demonstrates the practical application of the framework to complex problems. The assumption for stochastic gradient descent (Assumption 3) encompasses a wide range of scenarios. Weaknesses: The papers progression is somewhat challenging to follow with a focus on technical contributions without a clear direction. In the fifth section where the method is applied there is a lack of discussion on the tightness and novelty of the results. Lemma 4 in subsection 5.3 appears to improve upon existing conditions but its applications and significance are not explained. The absence of a conclusion and occasional haste in writing result in some minor flaws. The applicability of the results is limited by the requirement for preexisting knowledge of convergence rates. Minor Flaws: The mention of implicit regularization in the introduction is not entirely relevant. Some notations are nonstandard. The definition of the pth derivative is unclear. Admissible rate functions may not always exist. Questions: Theorem 4 and Theorem 6 7 show different convergence rates for gradient descent. A clarification on the underlying principles is needed. In the phase retrieval example the SGD noise scales with the function value suggesting exponential convergence. The authors perspective on this is sought. A comparison with known GD convergence rates for the two applications would be valuable.", "Paraphrase: This research introduces a new approach to studying SGD using Lyapunov analysis of gradient flow over time. The authors begin by proposing that a Lyapunov function is necessary for convergence of gradient flow. They then utilize the same Lyapunov function to demonstrate the convergence of SGD. Although the paper presents intriguing examples it lacks experimental results classifying it as theoretical. Strengths: The abstract approach involving Lyapunov functions is novel in the context of optimization and SGD. The initial finding regarding the necessity and sufficiency of Lyapunov functions for convergence is noteworthy. Weaknesses: The benefits of the abstract approach are unclear. The convergence rates for SGD are comparable to existing methods and many advanced rates rely on variations of SGD such as varying step sizes batch sizes or averaging schemes. While the controltheoretic framework is intriguing it lacks a key result showcasing the effectiveness of this perspective. The reliance on Lyapunov functions of gradient flow may be a valuable insight for future SGD research but this paper fails to fully demonstrate its utility beyond its novelty. Question: What is the primary result that demonstrates the effectiveness of the Lyapunov analysis framework for SGD", "Paraphrase: This paper proposes a novel approach to analyze the convergence of gradient descent (GD) and stochastic gradient descent (SGD) by utilizing the convergence properties of gradient flow (GF). The approach involves constructing a Lyapunov potential that establishes a convergence rate for GF formalized as a general converse Lyapunovlike theorem. When combined with additional regularity conditions the convergence of GF implies the convergence of GD and SGD. This framework offers a unified analysis for both GD and SGD achieving convergence rates comparable to those for convex functions and phase retrieval problems. Strengths and Weaknesses: Strengths: Clear presentation of the proposed contribution Comprehensive interpretations and examples Creative approach to demonstrating GD and SGD convergence Conceptual value for extending convergence analysis to more complex models Weaknesses: Lack of familiarity with GF optimization theories Suggestions for future application scenarios (e.g. SGD convergence analysis in neural networks) Potential extension to other optimization methods (e.g. secondorder methods and SGD variants) Questions: Typos in duplicate references: [21] [22] [41] [42]"], "mhp4wLwiAI-": ["Paraphrase: This research investigates the reasons for the performance limitations of Graph Convolutional Networks (GCNs) with increasing depth. It extends the concepts of \"oversmoothing\" and \"information bottlenecks\" observed in other neural networks and applies techniques to improve gradient flow in GCNs. Specifically it proposes topologyaware isometric initialization and Dirichlet Energy Guided Dynamic Rewiring to optimize the distribution of initial weights and establish skip connections for better gradient propagation. By implementing these enhancements to a basic vanilla GCN the authors demonstrate significant performance gains (typically ranking within the top two out of 14 models). Strengths: Wellmotivated and clearly presented work. Employs gradient flow analysis to identify issues with deep GCNs. Demonstrates the effectiveness of dynamic skip connections. Clear and accessible writing style. Weaknesses: Potential ambiguity regarding the primary cause of issues in deep GCNs (gradient flow vs. overfitting). Missing appendix referenced in the text. Minor typographical and wording errors. Table 4 could be improved by bolding only the best entries in each column. Questions: Under what conditions does the assumption of sparsity for the structure of aiaj no longer hold true", "Paraphrase: Summary: The research study attempts to enhance the ease of training for the vanilla Graph Convolutional Network (GCN) model. The authors analyzed the issue of gradient flow and devised two methods: topologyaware isometric initialization and rewiring guided by Dirichlet energy. These methods aim to enhance optimization. Experimental results on various benchmark datasets demonstrate that the proposed techniques indeed improve the trainability of vanilla GCN resulting in competitive model performance (although not stateoftheart). Strengths: The paper presents a wellstructured argument and provides compelling justifications for the proposed methods. The empirical evaluation is methodical and persuasive. Weaknesses: Trainability issues seem to arise primarily when using a large number of layers. However increasing the number of layers generally does not improve model quality even when the proposed methods are applied. This raises the question of whether the trainability issue may stem from the datasets being too small or noisy potentially limiting the benefits of increased model capacity. It is unclear whether the proposed techniques would also enhance the performance of more complex GNNs that incorporate elements such as attention or normalization. If they do the proposed techniques would have wider practical applications. While the adaptive rewiring mechanism based on Dirichlet energy is effective it consistently utilizes the output feature matrix of the first layer as the skip feature. This approach makes the network shallower compared to standard skip connections used in ResNet models. The authors could consider examining alternative ways of incorporating skip connections. Questions: Can the proposed techniques enhance the trainability of not only GCNs but also other advanced GNN models", "Paraphrased Statement: Summary: This study proposes enhancements to a deep graph neural network (GNN) model by optimizing the gradient flow during training. It introduces two techniques: topologyaware isometric initialization and dynamic rewiring through Dirichlet energy. Isometric initialization aims to provide an initialization that preserves the topology of the graph while dynamic rewiring dynamically creates skip connections based on the change in Dirichlet energy. Strengths: The premise of improving gradient flow is valuable. The paper is clear and wellwritten. Weaknesses: 1. Lack of Theoretical Justification: The construction of the isometric initialization in Equation 14 lacks formal justification. The use of Dirichlet energy as a metric for rewiring is artificial and its connection to gradient flow is unclear. 2. Practical Considerations: The impact of the proposed methods on models other than GNNs is not explored. The computational cost of dynamically updating Dirichlet energy may be significant for large graphs. Suggestions: Evaluate the proposed methods on other GNN backbones (e.g. GAT APPNP). Investigate the impact of dynamic rewiring on training time. Questions: Refer to the Strengths and Weaknesses sections above for specific questions related to the theoretical basis and practical aspects of the proposed methods.", "Paraphrased Statement: Summary: The authors analyze the gradient flow of deep Graph Convolutional Networks (GCNs) and introduce an isometric initialization approach for vanilla GCNs that incorporates knowledge of the network topology. They also propose using Dirichlet Energy to dynamically rewire vanilla GCNs with skip connections improving their expressive capacity. Strengths: Clear and accessible writing Thorough experimental evaluation Weaknesses: Limited novelty in the method and results Some plots are cluttered and certain claims lack sufficient evidence Missing literature on the gradient flow and trainability of GNNs Questions: 1. Line 76: Why is Dirichlet Energy used to measure the expressiveness of feature embedding 2. Equation (4): Why is the gradient flow defined as the average of the L2 norms of the gradients across all L layers 3. Table 2: What is the standard deviation of the reported results 4. Equation (6): What is the mathematical representation of \"Dirichlet Energy guided rewiring\" Writing: 1. Figure 1: Adjust the position for clarity. 2. Figure 3: Simplify the plot by displaying only one set of legends and moving it outside the plot area. 3. Equation (6): Use the following simplified notation: y[i] W[X[A[i]]e[i] where e[i] is the onehot vector with a 1 in the ith element and zeros elsewhere."], "u4dXcUEsN7B": ["Paraphrased Statement: Summary: The study examines how training data and replay buffer selection impact the stability of continual learning models. Instead of calculating the Hessian the authors assess the effect of data points on stability and plasticity through small data perturbations. They utilize this influence to optimize gradient updates and sample selection. The algorithm is tested on CIFAR10 CIFAR100 and MiniImagenet and outperforms several stateoftheart methods. Strengths: Clear and wellorganized presentation of the method Timely focus on the crucial problem of sample selection Demonstrated effectiveness and potential of the proposed method Weaknesses: Ambiguous definition of metrics (\"Finished Accuracy\" \"First Accuracy\") Limited clarity in assessing plasticity and stability beyond provided definitions Unclear explanation of how sample division is implemented The significance of example influence on plasticity or stability is not fully explored Questions: Comparison of proposed sampling selection with other approaches in the literature Highlevel interpretation of what constitutes increased plasticity or stability Elaboration on the process of dividing example influence into five groups Explanation of why the difference between examples is not leveraged to improve training Clarification of figure 3 and 4 legends Undefined term \"KKT\" Inclusion of standard deviation in results to improve reliability Additional Comments: Move taskincremental results to the appendix and provide standard deviation in the main body. Consider grouping figures 3 and 4 or modify figure 4 to avoid the legend overlapping with figure 3.", "Paraphrased Statement: This study explores the significance of individual examples in preserving plasticity (adaptability) and stability (retention) during continuous learning. The researchers propose a novel approach inspired by the Influence Function (IF) which measures the impact of removing an example from the training set. However as computing the exact IF requires a complex operation they develop MetaSP a twostep optimization technique that approximates the IF. MetaSP calculates a loss balancing factor (\u03b3) that optimizes both plasticity and stability. This balanced loss can then be used for training and selecting examples to retain. Extensive experiments on various datasets demonstrate the effectiveness of MetaSP. Strengths: The approach of utilizing IFlike information to improve performance is novel and intriguing. The empirical evaluation is thorough and demonstrates the efficacy of the proposed method including its computational complexity. Ablation studies provide insights into the methods components and the impact of example retention on plasticity and stability. Weaknesses: The connection between the IF motivation and the MetaSP algorithm needs clarification. A theoretical or empirical argument supporting this link would be beneficial. The application of the algorithm only to the final 5 epochs out of 50 should be emphasized and its impact on other methods explored. The papers presentation could be improved for clarity and selfcontainment. The effectiveness of KMeans clustering on the input space should be examined further including its impact on example clustering and the potential of using fewer or random clusters. The paper lacks a limitations section despite its inclusion in the checklist. Minor Comments: Line 22: Replace \"outstanding S\" with \"high S\" to avoid overstating the desired result. Line 96: Clarify that \"performance\" refers to metrics such as accuracy or loss. Line 120: Replace \"assumed as positive definite\" with \"usually assumed as positive definite.\" Line 121: Use precise wording to acknowledge the challenges of Hessian computation but avoid informal language like \"always outofmemory.\" Line 168: Consider including the KKT conditions for completeness. Line 225: Rephrase as \"reuse existing CL metrics when possible to facilitate comparisons across studies.\" Figure 3: Specify the timepoint at which the metrics are evaluated. Figure 4: Clarify the purpose and interpretation of the three bars per task and the two separate plots. Figure 5: Provide details on the method used to plot the Pareto front. Table 2: Replace \"implementation time\" with \"training time.\" Author Responses and Improvements: The authors revised responses and improvements to the paper have significantly improved its clarity and rigor. They now provide theoretical and empirical evidence supporting the connection between the IF and MetaSP clarify the limited application of the algorithm to the final epochs address the issue of example clustering and include a limitations section. These improvements justify an increased score of \"Accept.\" Additional Questions for Authors: Do you have any theoretical or empirical evidence to demonstrate how closely MetaSP approximates the results obtained from the Importance Function How would the performance of MetaSP change if it were applied to the entire training process rather than just the final epochs", "Paraphrase: The proposed method improves rehearsalbased approaches for continual learning by considering the influence of individual training examples. The method involves: 1. Estimating the influence of each example on the models stability and plasticity. 2. Using this influence to regularize parameter updates during learning. 3. Selecting past training examples to store in memory based on their influence. The method shows significant improvements over rehearsalbased baselines but its presentation could be improved for clarity. Some background information especially on influence functions could be explained more thoroughly. Additionally it would be beneficial to include baselines from other continual learning approaches such as regularizationbased methods. Question Paraphrase: Is it expected that the method would perform well when handling sequences of tasks with different input domains (e.g. classifying FMNIST and CIFAR10 images)"], "ldxUm0mmhl8": ["Paraphrased Summary: This paper presents a causal model that predicts the deletion of events in temporal point processes. It uses a superposition theorem to simulate potential missed events. Counterfactual events which are theoretical outcomes under different conditions can then be generated using this causal model. The GumbelMax causal model ensures the uniqueness of counterfactual distributions. Strengths and Weaknesses: Strengths: Wellwritten and easy to understand Combines point processes and causality making it novel Weaknesses: Section 3: The last part of Section 3 is unclear especially regarding the computation of the equation under line 157 and the use of docalculus. A causal graph would enhance understanding of the datagenerating mechanism and adjustment calculations. A pseudoalgorithm could clarify the sampling process after line 157. Line 155: The estimation of p(U) is crucial and should be explained in the preliminaries or appendix. Section 4: The impact of varying lambdamax on the counterfactual realization should be discussed. It is unclear how lambdamax is used in Alg 1 considering that Hmax is provided. General: The reason for restricting the SCM to GumbelMax should be clarified. It is not clear why the model is limited to onedimensional point processes. Experimental Section: The application is relevant but lacks synthetic experiment validation. It would be valuable to assess the accuracy of generated counterfactual realizations in a synthetic setting. Questions: Same as the \"Weaknesses\" section.", "Paraphrased Statement: This study introduces a novel approach for answering \"whatif\" questions in temporal point processes such as \"what would have happened if the event rate had been different at an earlier time\" This type of query has been difficult to address in temporal point processes due to technical challenges. The proposed approach combines the thinning algorithm with the GumbelMax structural causal model. It also includes a sampling algorithm that uses this model with the superposition theorem to simulate counterfactual scenarios in temporal point processes. An illustrative example is provided in the main text. Strengths: Introduces new ideas for handling counterfactual queries in temporal point processes. Combines existing concepts from temporal point processes and structural causal modeling in an innovative way. Handles the challenge of missing \"rejected\" events in the observational data. Weaknesses: The title is overly broad and does not fully reflect the assumptions and limitations of the approach. The scope and implications of the assumptions need to be clarified. The motivation for counterfactuals could be better justified as interventions alone may be sufficient in some cases. The positioning of related work is unclear and could be improved. The experimentation section is limited with synthetic experiments relegated to the Appendix. Clarifications are needed on the applicability of the approach to different types of temporal point processes and the role of counterfactuals in decisionmaking. The application example is complex and not as representative of realworld applications as it could be.", "Summary: This research introduces a straightforward and effective method for answering hypothetical questions about temporal point processes specifically whether an event would have occurred if its corresponding intensity had been altered. Method: For the inhomogeneous Poisson process the researchers suggest sampling events based on an adjusted intensity (lambdamax lambdaaccept) and then combining them with observed events. This approach based on a thinning process allows for simple estimation of the counterfactual question. The method extends to linear Hawkes processes because they can be modeled as a superposition of multiple inhomogeneous Poisson processes. By applying the thinning process to each branch of the Hawkes process the counterfactual question can be answered. Evaluation: An experiment using epidemiological data supports the validity of the proposed method. Strengths: Clear and accessible presentation Simple and theoretically sound approach Practical application in epidemiology Weaknesses: Limited to inhomogeneous Poisson processes and linear Hawkes processes Possible need for further discussions on extending the method to more complex TPPs", "Paraphrased Summary: Researchers have developed a method for predicting alternative outcomes (counterfactuals) in sequences of events over time. This method uses a specific mathematical model (GumbelMax structural causal model) to represent the sequence of events assuming that each event has a single cause and that events are independent of each other (no hidden influences). Based on these assumptions counterfactuals can always be determined. The researchers have designed a sampling algorithm to estimate counterfactuals based on known thinning and superposition techniques. They tested their algorithm using both simulated and realworld data. Strengths and Weaknesses: The method addresses a significant problem but some assumptions are not clearly stated. The assumption that random noise is independent across events implies that there are no unmeasured influences between events. Additionally the model assumes that noise has an additive effect. These assumptions are not part of the general definition of the model and should be explicitly described as additional requirements. While these assumptions may be reasonable in some situations they could be violated in practice potentially limiting the methods effectiveness. The research also lacks a detailed evaluation of the methods robustness against violations of its assumptions. Clarifying the assumptions and conducting a more comprehensive empirical study are crucial for assessing the methods validity and applicability."], "yoLGaLPEPo_": ["Paraphrased Statement: This paper addresses feature augmentation a specific type of data augmentation for low data scenarios like fewshot learning. The approach involves: Modeling each category using a wrapped normal distribution. Using differentiable sampling in hyperbolic space to generate augmented data. Estimating distribution parameters using a Neural Ordinary Differential Equation (ODE). Proposing an upper bound on classification loss for efficient tangent space optimization. Employing a bilevel optimization strategy utilizing validation data. Strengths and Weaknesses: Strengths: Clear and informative writing. Novel and significant approach in hyperbolic manifold learning. Effective techniques for generating highquality hyperbolic data. Weaknesses: Does not significantly outperform baselines. Does not explore the use of category hierarchy for prototype learning.", "Paraphrased Statement: Summary: This method introduces a technique to add features using hyperbolic transformations to mitigate overfitting in scenarios with limited data. The authors establish an upper limit for the loss function assuming unlimited data augmentation. This upper limit allows for training classifiers and distribution estimators without complicated hyperbolic operations through bilevel optimization. The study includes thorough experiments on fewshot and continual learning demonstrating the efficacy of the proposed method. Strengths: Employs Neural Ordinary Differential Equations (Neural ODEs) to estimate feature distribution and sample augmented features. Derives an upper bound for augmentation loss under infinite augmentation enabling efficient augmentation without complex calculations. Provides comprehensive comparisons ablation tests and visualizations evidencing the effectiveness of the feature augmentation. Weaknesses and Questions: Question 1: The estimation process employs three Gradient Flow Networks to learn various parameters. However the authors specify that some parameters are shared across classes. Its unclear how these conflicting statements are reconciled. Question 2: The update procedure for F2 and F3 using Eq. (16) remains ambiguous when classifiers are fixed. It appears that only F1 can be trained. Missing Experimental Details: Training data to validation data ratio (Dt:Dv) during training. Initial value of parameter c. Baseline Comparison: The FEAT baseline in Table 2 achieves comparable accuracy to the proposed method. Time consumption should be discussed to highlight the proposed methods superiority. Computation Complexity: Despite simplifying classifier training through Eq. (14) Eq. (16) remains computationally demanding. Typo: Table 2s description and content mismatch (e.g. \"Euclidean Metric\" vs. \"Hyperbolic Metric\")", "Paraphrased Statement: Summary: This paper introduces hyperbolic feature augmentation using a neuralODE distribution estimation scheme. This method optimizes a continuous process to approximate the real data distribution effectively in situations with limited data. The paper also establishes an upper bound for the augmentation loss which is used to train models without direct augmentation. Experiments on smallsample datasets indicate the methods efficiency. Strengths: 1. This paper is an early attempt at feature augmentation in hyperbolic space which is a novel approach. 2. Experimental results show that the method outperforms baselines under extreme data scarcity (fewshot settings). Weaknesses: 1. The paper focuses too heavily on fewshot learning neglecting the broader applicability of hyperbolic feature augmentation. The datasets used do not have inherent hierarchical structures which raises questions about the methods effectiveness in true hyperbolic contexts. 2. The method relies on Euclidean Neural ODEs to learn feature densities despite the focus on hyperbolic augmentation. Using Neural Manifold ODEs for hyperbolic space would enhance the method. 3. Results in certain tables are marginal suggesting that the evaluation may not be conducted on sufficiently hyperbolic datasets to highlight the methods unique advantages. Verdict: While this paper shows promise as a preliminary effort in hyperbolic feature augmentation its evaluation is inadequate. The lack of results on datasets with true hyperbolic structure and the focus on extreme data scarcity limit the significance of the proposed method. Therefore a borderline rejection is recommended until further evaluation is provided. Additional Questions: Why is the method primarily tested on datasets without hyperbolic structure and in the fewshot setting Can the method be applied more broadly to general data augmentation tasks beyond extreme data scarcity", "Paraphrase: Summary: This paper introduces a method to augment features with hyperbolic geometry to prevent overfitting when training with limited data. The authors expand this method by employing infinite augmentations supported by a rigorous analysis. They demonstrate promising results in fewshot and continual learning. Strengths: Clear motivation and presentation of the method. Novel methodology that is easy to comprehend. Rigorous approach to leveraging infinite data augmentation for classifier training. Positive experimental outcomes in fewshot learning. Weaknesses: Concerns regarding the gradient flow network: Coping with multiple factors in modeling Uncertainties about learning accuracy and generalization particularly with limited data Lack of empirical evaluations to validate network performance Difficulty in optimizing gradient flow networks due to: Multiple optimization steps (inner and outer loops) Numerous hyperparameters Need for detailed guidance on practical use Questions: Is a projection of sampled noise into the tangent space required Can the code and algorithm for reverse mapping visualization be shared Is it feasible to design the method as endtoend for specific tasks"], "upuYKQiyxa_": ["Paraphrased Statement: Summary: Models trained without guidance often focus on irrelevant features. This paper suggests using humanannotated or automatically generated foreground masks to guide the models attention during finetuning. By aligning attention with actual foreground areas the method enhances the models robustness. Strengths: Addresses a significant issue in a simple and intuitive manner. Human guidance like segmentation masks is necessary for addressing spurious correlation due to insufficient dataset guidance. Improves robustness across different datasets and setups. Internal mechanisms are wellsupported by ablative studies. Hyperparameter selection is consistent and described in detail. Sensitivity tests show improvements with limited additional annotations. Weaknesses: The use of \"relevance maps\" as the explanation method lacks intuitiveness. The objective metric for hyperparameter tuning is unclear which could lead to overfitting to the robustness measures. The applicability of the technique to CNNs or training from scratch is not explored. Conclusion: The paper effectively tackles a fundamental problem with a promising approach. Clarifying the use of relevance maps and addressing the potential limitations would strengthen the papers contributions.", "Paraphrased Summary: This study proposes a method to enhance the robustness of Transformerbased image classifiers across diverse image distributions by exploiting the connection between improved attention focus and generalization ability. The method drives the attention mechanism to concentrate on the primary objects in an image by using a relevance map (or a collection of attention maps) as a guide. TokenCut generates a relevance map using the idea from [8] and then uses a manual or unsupervised segmentation map as a reference relevance map. The experiments demonstrate the advantages of the approach. Strengths and Weaknesses: Strengths: Simplicity and effective performance improvement. Thorough comparisons and ablation studies. Weaknesses: Reliance on gradients for relevance map calculation. Further details on model training with these gradients are requested. Similarities to methods that use manual segmentation for debiasing models (e.g. [Burns et al. 2018]). A discussion on the differences and potential adaptation to classification tasks is recommended. Lack of exploration of the data distribution used for training TokenCut and its impact on generalizing to significantly different distributions. Questions: Discussion and clarification on the noted weaknesses. Details on the number of images used for training with unsupervised segmentation results. Performance comparison of TokenCut on the datasets employed in the study.", "Paraphrase: The paper aims to enhance the resilience of Vision Transformers by scrutinizing their relevancy maps. Introduced as a finetuning stage the method employs three loss functions to reduce relevance in background areas oblige the model to rely on foreground data for predictions and draw knowledge from its own projections. Empirical evaluations on multiple datasets demonstrate the methods efficacy. Strengths: Wellsupported motivations Straightforward concept backed by experimental findings and visualizations Weaknesses: 1. Lack of baseline comparisons with selfsupervised learning methods using Vision Transformers which are known to excel on outofdistribution data. Incorporating results from SSL ViTs would strengthen the papers credibility. Additionally the DINO method has shown potential for identifying foregrounds potentially mitigating the reliance on background information for classification. 2. By incorporating an Lclassification component during finetuning the method is confined to predefined classes. The paper should address the potential performance limitations for outofdistribution datasets with distinct classes (e.g. iNaturalist or natural datasets with varying classes). Would knearest neighbor testing on novel classes continue to outperform the baseline 3. Structural issues within the paper: Section 2 combines descriptions of validation datasets with related work. Questions: 1. Can the method provide baseline experimental results using pretrained DINO checkpoints 2. How would the method perform on outofdistribution datasets with semantically distinct classes Would knearest neighbor testing on such novel classes still surpass the baseline The paper suggests that only some classes utilized in finetuning benefit other classes therefore the method should be applicable to unseen classes as well.", "Summary: This work improves the robustness of Vision Transformer (ViT) models by leveraging interpretability methods during training. It uses a technique to calculate pixelwise relevance maps for ViT models and adds two regularization terms to the finetuning loss: one encouraging positive alignment of the relevance map with the foreground and the other discouraging negative alignment. Strengths: Simple and effective idea Improved robustness without significant effort Efficient requiring only limited annotated segmentation masks Weaknesses: Concerns about hyperparameter selection for competing methods Increasing finetuning data beyond a certain point can harm performance Unclear impact of different loss terms Unstated assumption that relying on background is always detrimental Questions Raised: Is there a baseline that finetunes the model with only the classification loss How were the hyperparameters for competing methods chosen Why does increasing finetuning data harm performance beyond a certain point Which layers are most affected by finetuning What are the differences between AugReg and vanilla ViT in terms of training"], "x3JsaghSj0v": ["Paraphrase: This paper proposes two novel techniques to improve the scalability of graph transformers to larger graphs. Methods: 1. Adaptive Sampling with Bandit Learning: Identifies that graph transformers are agnostic about graph structure during training. Proposes using bandit learning to dynamically adjust weights for multiple node sampling strategies. 2. Hierarchical Attention Mechanism: Addresses the challenge of capturing distant information in graph transformers. Introduces an attention mechanism that allows nodes to attend to a coarser graph capturing longrange dependencies. Strengths: Motivation: Wellmotivated based on empirical findings on node sampling strategies. Effeciveness: Achieves stateoftheart results with linear computational complexity. Presentation: Clear and illustrative with diagrams and ablation studies. Weaknesses: Necessity of Bandit Learning: Questions whether a simpler averaging approach could achieve similar performance. Computational Overhead: Coarsening the graph for hierarchical attention introduces additional computation. Clarity: Ambiguous use of boldface for \"Reward Scheme\" and lack of details on the coarsening algorithm. Questions: 1. Why not incorporate hierarchical attention into the bandit learning framework 2. How are supernodes in the coarsened graph represented and related to their member nodes", "Paraphrased Statement: Summary: This research introduces a transformer architecture tailored to graph data. It employs adaptive node sampling and hierarchical attention to overcome challenges in modeling longrange dependencies and capturing local neighborhood information. The authors leverage a multiarmed bandit approach to learn effective sampling strategies customized to graph properties. Strengths and Weaknesses: Strengths: Addresses the limitations of transformers for graph data particularly scalability issues. Proposes a method to combine multiple node sampling schemes based on bandit learning. Provides concise explanations of related methods and equations. Weaknesses: Lack of discussion on related transformermultiarmed bandit frameworks. Incomplete comparison with previous works on heterophily graph data as the dataset split differs. Questions: Question 1: Can the reward be defined using attention to guide direct prediction of nodes with high attention scores eliminating the need for predefined sampling methods Question 2: Do the node sampling methods provide relative importance of nodes relative to a center node and how is this importance updated for other nodes Question 3: Why is attention computed for both center nodes and all tokens in Figure 2 rather than just for the center nodes Question 4: Can Figure 2 be simplified to improve clarity", "Paraphrased Summary: This paper introduces an enhanced Transformer architecture (\"ANSGT\") for processing large graphstructured data. Traditional graph Transformers struggle with large graphs due to scalability challenges and difficulty in extracting meaningful node representations. To address this ANSGT integrates an adaptive node sampling strategy inspired by an adversarial bandit problem. Additionally it employs a hierarchical attention mechanism with graph coarsening algorithms to capture distant dependencies. Strengths: Clear and concise writing Focus on addressing scalability issues for graph Transformers on large graphs Innovative use of multiarmed bandits for informative node selection Wellsupported motivation and experimental validation Weaknesses: Lack of consideration for heterophilic graphs Insufficient baseline comparisons in Table 1 Limited experimental results on larger datasets (maximum 19717 nodes) Unclear contribution of individual ANSGT components (super nodes global nodes) Computational complexity considerations for augmentation Questions: 1. Are there any comparisons with heterophilic graph neural networks (e.g. H2GCN GPRGNN) to assess the effectiveness of ANSGT on such graphs 2. Can an ablation study be provided to separately evaluate the contributions of graph coarsening and global nodes 3. Can the experimental results be extended to larger OGB datasets (e.g. ogbnarxiv) to demonstrate ANSGTs scalability 4. What is the impact of varying the number of input sequences \\mathcalS on ANSGTs performance Would it improve with \\mathcalS 1 (i.e. without augmentation)", "Paraphrase: Many graph transformer node sampling methods dont consider graph properties or longrange node connections. To overcome this researchers have developed Adaptive Node Sampling for Graph Transformers (ANSGT) that incorporates graph properties into its node sampling strategy. Additionally ANSGT leverages complementary global information via graph coarsening and hierarchical attention schemes. It achieves stateoftheart results on six benchmark datasets for semisupervised node classification tasks. Strengths: ANSGT addresses the issue of inconsistent sampling performance for graphs with varying homophily ratios. It utilizes a multiarmed bandit problem formulation and attention scores as rewards to guide node selection. Weaknesses: The use of different letter cases for hidden states and matrices (bold uppercase bold lowercase uppercase lowercase) can be confusing. Table 2 should include a baseline with no coarsening (c 0) and explore a wider range of coarsening rates. Questions: How does ANSGT differ from models that predict adjacent nodes for attention using linear layers"], "kY1RbKE7DWE": ["Paraphrase: This paper introduces an algorithm framework called Anchorchanging Regularized Natural Policy Gradient (ARNPG) that incorporates effective ideas from other optimization methods for solving multiobjective Markov Decision Process (MDP) problems. The algorithms developed using the ARNPG framework are proven to achieve optimal convergence (O(\\frac1T)) with accurate gradients. While the paper provides experimental evidence to support the algorithms performance concerns remain about its effectiveness in more complex scenarios. Strengths include a strong theoretical basis experimental verification and clearly stated motivation while a weakness is the limited number of practical examples used to evaluate the algorithms effectiveness raising questions about its applicability to more intricate tasks like robotics or video game control.", "Paraphrased Statement: Summary: The authors introduce a new framework called Anchorchanging Regularized Natural Policy Gradient (ARNPG) for designing policy gradientbased methods for multiobjective Markov Decision Processes (MDPs). ARNPG is based on firstorder methods for convex optimization and offers significant advantages over previous approaches: Faster convergence (O(1T)) without the need for unrealistic assumptions. Theoretical analysis that supports its performance. Empirical evaluation across a variety of scenarios including gradientbased and samplingbased MDPs. Strengths and Weaknesses: Originality: Novel framework for policy gradient methods in multiobjective MDPs. Demonstrated applications in various scenarios. Achieves O(1T) convergence without additional assumptions. Quality: Clear presentation of problem setting ARNPG approach and theoretical foundations. Thorough analysis and assumptions for proofs. Experimental evidence of effectiveness. Clarity: Wellstructured paper with clear guidance for readers. Significance: Importance of policy gradient methods for multiobjective MDPs. Comprehensive theoretical and experimental analysis. Conclusion: The authors work is considered valuable and wellpresented. However the reviewers confidence in the recommendation is low due to limited familiarity with the research area.", "Paraphrase: Summary This study explores policy optimization in decisionmaking scenarios where multiple objectives are involved (multiobjective Markov decision processes). It focuses on three specific optimization scenarios: minimizing a smooth concave function satisfying objective constraints and optimizing tradeoffs between objectives. The primary technical contribution is a framework called ARNPG (anchorchanging regularized natural policy gradient). The study provides theoretical analyses for each scenario within the ARNPG framework supported by empirical experiments. Strengths and Weaknesses Originality This work stands out due to its novel approach. It establishes a link between mirror ascent and multiobjective policy optimization. The proof of Proposition 1 which establishes a connection to classical firstorder methods is technically innovative. The papers theoretical applications are insightful covering a range of multiobjective decisionmaking scenarios. Quality The proof of Proposition 1 is sound and employs novel techniques. However the empirical evaluation lacks strong baselines for comparison. Baseline methods could be included particularly in the smooth concave scalarization scenario to provide a more robust assessment. Clarity Overall the paper is clear. However there are a few areas that could benefit from further explanation: Equation (3) on line 122 should use v instead of vk in the inner product. The derivation of equation (20) on line 639 could be expanded to enhance readability. The meaning of \"exact gradients\" in Section 5.1 requires clarification.", "Paraphrased Statement: This research paper examines three types of multiobjective RL problems: concave scalarization CMDPs and maxmin tradeoff. The authors present a novel algorithm for these problems that updates a modified objective by limiting the KullbackLeibler (KL) divergence between the new policy and a fixed \"anchor\" policy. The algorithm updates the policy using natural gradients. This approach leads to a convergence rate of approximately O(1T) when using exact gradients. Strengths and Weaknesses: The paper is wellwritten and accessible despite the topic being slightly outside the authors area of expertise. Multiobjective RL is becoming increasingly popular and this paper offers valuable insights into theoretical aspects of the problem. Questions: Intuition behind natural gradients: Can you provide a more intuitive explanation of why natural gradients are crucial for achieving global convergence Regularized vs. unregularized objective: What would be the impact of using the unregularized objective in the update rule instead of the regularized objective Sample efficiency: Would removing the Fisher information term make the algorithm more efficient Also wouldnt collecting new samples from the new policy for each update be highly inefficient Typographical errors: The paper has typographical errors in the spelling of \"Lagrange\" and the use of a tilde on top of V."], "p6hArCtwLAU": ["Paraphrase: Summary The authors develop a method (CoNMoR) that represents individual neuron nodes as feature vectors and uses a Multilayer Perceptron (MLP) and Treebased Long ShortTerm Memory (TreeLSTM) network to embed these vectors in a shared representation space. Contrastive learning is employed to enhance the embeddings. Additionally specialized data augmentations tailored to neuron morphology are proposed. Strengths and Weaknesses Originality: The approach lacks significant novelty as it primarily adapts SimCLRs contrastive learning techniques to treestructured neuron data. Quality: The evaluation process is questionable due to the absence of a validation set. CoNMoRs training is performed on the entire training and test set potentially inflating test set performance when using a supervised classifier. Additionally the authors report the highest KNN evaluation accuracy obtained during pretraining which may not accurately reflect the models performance after 100 epochs of pretraining. The methods selfsupervised nature is questionable since epoch selection is based on labeled data. Furthermore the representations may not comprehensively capture all neuron features as the L5 class appears to be highly distinguishable from others suggesting a cluster effect around it. Clarity: The paper assumes a basic understanding of neuron morphology which may challenge readers without background knowledge. Key concepts such as 3D reconstructions swc format and neuron reconstruction pipeline biases require clearer explanations. Significance: While the application of contrastive learning to tree structures is intriguing the methods broader significance could be enhanced by generalizing it to a wider range of treestructured data. Assigning a separate name (CoNMoR) to an adaptation of SimCLR may be redundant. Questions: 1. What specific features comprise Vi in L108 2. Which child nodes are referred to as input states in L133 3. Define the second subscript in hvariables in Eq 14. 4. Clarify the definition of q and k in L170 and Eq 5. 5. Specify the perturbations mentioned in L193. 6. What is the significance of m in L203 7. Explain why batch normalization is considered particularly important for selfsupervised training in L301.", "Paraphrase: Summary: This paper explores selfsupervised learning for representing neuron morphology as treestructured data. It employs TreeLSTM to encode the tree structure and representations are learned via contrastive learning using novel tree data augmentation techniques. Strengths: The neuron morphology representation approach is logical. The paper is wellwritten and accessible. Experiments and ablation studies are wellanalyzed and discussed. Weaknesses: The framework lacks significant novelty as it is based on the MoCO visual contrastive learning method with modifications for treestructured data and tree data augmentation. Graphbased SSL should be included as a comparative model to evaluate the proposed approach adequately. Details about the baselines (morpheVAE and TRNN) used in neuron celltype classification are missing making it difficult to assess the proposed methods performance against them. While visualizing the learned representations using tSNE is useful it would be more informative to include representations from the baselines for comparison. Questions: In line 203 the value of m is set to 10. What is the significance of this setting Could altering this value impact performance", "Paraphrase: Summary This paper introduces a new approach for extracting features from neuron trees. The authors designed specific data augmentations tailored to tree structures. The extracted features were employed to categorize cell types with improved accuracy over existing techniques. Strengths The method is wellgrounded and innovative. The augmentations designed for neuron trees enhance selfsupervised feature learning. The approach demonstrates strong performance across various datasets. Weaknesses While effective for neuron morphology learning the approach may be limited in broader applications. The authors should consider its utility in other domains such as classification of molecular properties. The choice of InfoNCE loss among other contrastive settings should be motivated. Direct comparisons with previous graph contrastive learning methods are missing. The experimental setup for baseline experiments (2151) and the inclusion of axons in the tree structure should be clarified.", "This study aims to enhance representation learning for neuron morphology data using contrastive selfsupervision. The authors utilized Tree LSTM and MOCO introducing topology data augmentation for treelike data. Effectiveness was evaluated using three datasets (BIL JML ACT). While the method performed well it faced limitations: Strengths: Clear writing and presentation Significant problem of interest (representation learning for neuron morphology data) Weaknesses: Limited novelty: Contrastive selfsupervised learning on neuron morphology data has been attempted before. Tree topology data augmentation seems ineffective as ablation studies indicate that general graph augmentation techniques are more effective. Evaluation issues: ACT dataset contains incomplete neuron reconstructions potentially hindering representation learning. Arbitrary training and evaluation procedures (e.g. variable training epochs for pretraining and ablation studies) Unclear reproduction of baseline methods (MorphVAE and TRNN) potentially compromising result validity Questions: Why was the trainingtest set split only used for supervised finetuning but not pretraining How was the choice of neighbors for KNN determined What does \"the best KNN evaluation accuracy during pretraining included\" mean Why were different pretraining epochs used for the main experiment and ablation study"], "yipUuqxveCy": ["Paraphrased Statement: The authors propose a new approach to offline reinforcement learning for multiagent systems called Transformerbased Offline MultiAgent RL (TOMAL). To overcome the limitations of traditional TD learning methods in credit assignment TOMAL employs transformer networks to model sequential interactions among agents. The approach consists of two steps: 1. Training a \"teacher\" decision transformer on the complete dataset to capture crossagent dependencies. 2. Distilling knowledge from the teacher transformer to individual \"student agents\" through a \"structural relation loss\" constraint. By leveraging the attention mechanisms of transformers TOMAL aims to improve the learning of interactions between agents compared to existing offline RL methods. Strengths: Uses transformers to learn crossagent interactions and distills them into student policies using a structured relation loss. Achieves strong performance in multiagent environments. Includes extensive ablation studies to demonstrate the importance of individual components. Weaknesses: Does not include a comparison with other methods in Figure 6. May not be scalable to environments with numerous agents. Transformerbased methods like MADT exhibit comparable performance in simpler environments. Contains grammatical and typographical errors. Questions: 1. In the algorithm should the student agent gradient update be within the inner for loop 2. How does the FillIn environment operate Does an agent passing a block make it inaccessible to other agents 3. If crossagent credit assignment is necessary why is it unclear in the FillIn environment 4. Why is the return distribution for Highway in Table 11 reversed 5. Are obstacles and apples needed in the FillIn environment 6. What are the initial positions of agents in the Equal Space environment 7. How were trajectories classified into \"good\" \"normal\" and \"poor\" in Table 2 8. The citation number for MADT in the text and tables is incorrect.", "Paraphrase: Summary: This paper introduces a method for enhancing the quality of existing multiagent policies during online execution. The method uses a distillation process where a \"teacher\" policy with access to global information guides the learning of a \"student\" policy that can make localized decisions. Experiments show that this approach surpasses other competitive methods. Strengths: The use of global and local information in the teacher and student policies respectively improves cooperation. The framework aligns with existing approaches like Centralized Training and Decentralized Execution (CTDE). Clear writing and thorough experiments demonstrate the methods effectiveness. Weaknesses: Computational complexity may increase as the number of agents grows. Offline data limitations could affect performance compared to MADT (MADTbased sequential modeling). A detailed theoretical analysis of the superiority in poor offline data scenarios is needed. Questions: Compare the approach presented in this paper to JumpStart Reinforcement Learning which uses a guide policy to inform online policy. Provide a more detailed explanation of the distillation process used in this work. Explain how a suitable student policy is distilled.", "Paraphrase: Summary The research introduces an offline multiagent reinforcement learning (MARL) framework utilizing a decision transformer and policy distillation technique. A novel distillation loss is introduced to capture structural relationships between agents from a centralized teacher policy. Experimental results demonstrate the benefits of policy distillation. Strengths 1. The integration of knowledge policy distillation and decision transformer in a multiagent setting is intriguing. 2. The proposed relational policy distillation loss effectively addresses multiagent scenarios. 3. Extensive experiments in various environments show stateoftheart results with ablation studies highlighting the significance of the relational policy distillation loss. Weaknesses 1. While the framework shows substantial improvement in environments created by the authors the advancements in the SMAC benchmarks are limited in comparison to previous methods. In ablation studies conventional distillation methods perform poorly on several tasks. This may be due to the teacher policy itself not having a significant advantage on these tasks. 2. The authors do not specify settings for the initial target return which can significantly affect singleagent decision transformer performance. Credit assignment in multiagent settings complicates target return determination. 3. The number of parameters is not discussed which is relevant as decision transformers are more complex than previous offline MARL methods. Questions Why do conventional distillation methods perform poorly in ablations on specific tasks How do the authors determine the optimal settings for initial target return in multiagent scenarios What is the impact of varying the number of parameters in the proposed framework"], "w6fj2r62r_H": ["Paraphrase: This paper suggests focusing solely on torsion angles rather than bond lengths and angles simultaneously as torsion angles pose a greater challenge. The authors introduce a diffusion model that operates on the Riemannian manifold of torsion angles. They also create a Boltzmann generator based on diffusion model likelihoods. Tests on datasets of various sizes show improved performance in terms of rootmeansquare deviation (RMSD) and convergence time. Strengths: Clear motivation to work exclusively on torsion angles resulting in potential speed and performance enhancements under certain assumptions. Novel approach to using a diffusion model on a torsion angle manifold. Compatibility of the diffusion model with a Boltzmann generator leading to beneficial properties. Wellwritten explanations of concepts including symmetry intrinsic coordinates and computation. Weaknesses: Additional insights could be gained from ablation studies on diffusion depth constrained matching and other hyperparameters. Questions: Refer to the weaknesses section for questions related to ablation studies.", "Paraphrase: Summary: This research introduces a novel approach for predicting molecular conformations by exclusively focusing on torsional angles of rotatable bonds. Unlike earlier methods that targeted coordinate or distance prediction this study isolates torsional angles leaving other molecular attributes unchanged. The authors draw inspiration from diffusion models on curved surfaces to establish the mathematical framework for \"torsional diffusion\" on a hypertorus in accordance with diffusion models for denoising. They further demonstrate that this diffusion concept can also be applied to Cartesian coordinates and create a scoring network that is invariant to rotations and reflections. The proposed method outperforms existing models on the GEOMDRUGS dataset with considerably fewer denoising steps. It is also capable of calculating exact likelihoods enabling it to align with the Boltzmann distribution of torsional angles governed by the energy function. Strengths: Prioritizing torsional angle modeling for conformation prediction is innovative and aligns with their critical role in molecular conformations. The score function for torsional angle diffusion satisfies rotational and reflection invariance for individual rotatable bonds. The models ability to estimate exact likelihoods enhances its versatility. Weaknesses: The method depends on external algorithms (e.g. RDKit ETKDG) to generate initial local structures and limits degrees of freedom to torsional angles. The chirality of the conformation is determined solely by the initial structure. Questions: Does the studys definition of \"freely rotatable bond\" match the one provided by the RDKit API How does the method handle rotatable bonds in molecular cycles which are also crucial for determining conformation", "Paraphrased Statement: This study introduces a novel approach to generating molecular conformers called torsional diffusion. Unlike previous methods that predict atom positions in 3D space this method focuses on generating torsional angles providing a more efficient and physically intuitive representation of conformations. The model incorporates a diffusionbased algorithm that estimates the likelihood of generated conformers enabling enhanced training pipelines. Experimental results demonstrate exceptional performance on the GEOMDRUGS benchmark surpassing stateoftheart methods. Additionally the study introduces a torsional Boltzmann generator that can be customized for generating conformers of different molecular classes. Strengths and Weaknesses: Strengths: Clear and concise writing Novel and reasonable concept of using torsional angles Solid experimental results with extensive details Weaknesses: Significant performance gap between the reported baseline models and the authors reproduced results Lack of additional experiments to address this discrepancy Questions: Is it appropriate to use different data splits for comparisons between GeoDiff and GeoMol Can the authors provide another evaluation using their own data split for a more accurate comparison", "Paraphrase: Summary: This research introduces a new diffusion model for generating conformations of molecules. The model operates based on the torsional angles of the conformer. It uses a learned extrinsictointrinsic score model to predict torsional scores from the conformers 3D representation. Accurate likelihoods of generated conformers are calculated enabling energybased training with samples from the Boltzmann distribution. The model surpasses current machine learning and cheminformatic approaches on the GEOM benchmark. Strengths: The diffusion model significantly improves the accuracy of molecular conformer generation. It generates higherquality conformers faster than existing machine learning solutions. The model consistently outperforms commercial software (OMEGA) on the GEOM benchmark. Detailed evaluations provide strong quantitative evidence. Wellexplained design decisions and sound theoretical basis for the diffusion model and training algorithm. Questions: What are the practical limitations (efficiency training time inference time) for using the model in industrial settings Is it suitable for highthroughput applications What challenges arise when adapting the model to larger molecules (e.g. proteins)"], "kMiL9hWbD1z": ["Paraphrased Summary: This study introduces RTFormer an efficient transformer network for realtime semantic segmentation that outperforms CNNbased models in the balance between accuracy and speed. To maximize efficiency on GPU devices RTFormer employs a GPUfriendly attention mechanism with linear time complexity and omits the multihead technique. The crossresolution attention mechanism efficiently gathers global context for the highresolution branch by disseminating highlevel knowledge acquired from the lowresolution branch. Experiments on common benchmarks demonstrate the effectiveness of RTFormer achieving stateoftheart results on Cityscapes and CamVid with promising performance on ADE20K. Strengths: Novel RTFormer block with superior performance and efficiency tradeoff for GPUlike devices in semantic segmentation. RTFormer network architecture fully leverages global context to enhance semantic segmentation with deep attention without compromising efficiency. Stateoftheart results on Cityscapes and CamVid and promising performance on ADE20K. Weaknesses: Crossresolution attention is a variation of selfattention a widely used technique limiting the papers novelty. Modest performance improvement on Cityscape dataset as indicated by limited mIoU and FPS gains. Thorough experiments in semantic segmentation support the proposed method but the techniques simplicity and lack of innovation raise concerns. The paper does not explore the application of the method to other vision tasks such as classification or object detection. Questions: 1. Can the proposed network be generalized to other vision tasks like image classification or object detection 2. Since the attention used appears to be MHA or selfattention which is not novel is the main contribution limited", "Paraphrased Statement: Summary: This study introduces RTFormer a novel method for realtime semantic segmentation. RTFormer employs a GPUfriendly attention mechanism with linear complexity and eliminates the multihead system. Its effectiveness has been substantiated across multiple datasets. Strengths: 1. RTFormer delivers promising performance on benchmark datasets. 2. The paper is comprehensively organized and clearly presented. 3. Developing efficient segmentation methods is a significant research area. Weaknesses: 1. RTFormer combines aspects from preexisting methods such as linearcomplexity selfattention HRNet and CNNtransformer hybrids. This may limit its novelty. 2. The paper does not specify the use of TensorRT for model acceleration potentially affecting the fairness of comparisons. 3. RTFormers performance and model size do not offer substantial advantages over comparable approaches. Questions: Refer to the \"Weaknesses\" section for further insights into potential limitations.", "Summary: The presented manuscript introduces an efficient semantic segmentation model. Its main contribution is a GPUfriendly attention layer that optimizes efficiency by using keys and values as adjustable parameters. Key and value dimensions are hyperparameters much smaller than the image resolution. Additionally the model replaces the standard transformers MLP with convolutions resulting in a module resembling the classic selfattention layer from before the transformer era. Finally crossresolution attention further enhances performance. Experiments were conducted on the Cityscapes and ADE20k datasets. Strengths: Hybrid model combining convolutions on highresolution representations and selfattention on lowresolution representations Achieves a competitive performancecomplexity ratio Weaknesses: Incremental contribution: GPUfriendly attention is similar to previous work and LinformerNystromformer Hybrid convolutionaltransformer models have been proposed before Proposed improvements show only marginal gains over baselines (Figure 3) Missing configuration in Figure 3a: EA CA Large training footprint (only 3 crops 512x1024 fit on a V100) Incomplete related work on efficient semantic segmentation models Missing Related Work: Vision Transformers for Dense Prediction (Ranftl et al. ICCV 2021) Nystr\u00f6mformer (Xiong et al. AAAI 2021) Linformer (Wang et al. CoRR abs2006.04768) Efficient semantic segmentation with pyramidal fusion (Orsic et al. Pattern Recognit. 2021) HarDNet (Chao et al. ICCV 2019) Questions: The model may be more appropriately termed hybrid convolutionalattention rather than a transformer. Experiments on embedded platforms would be valuable. Clarification is needed on the V100 GPU variant used (16GB or 32GB).", "Paraphrased Statement: Summary: This paper explores realtime semantic segmentation using Transformer models. A novel RTFormer block is introduced incorporating two attention models to combine features across varying resolutions. Experiments on multiple datasets show the effectiveness of this approach. Strengths and Weaknesses: Strengths: Achieves impressive performance on various datasets. Consistently outperforms baseline methods. Weaknesses: Lacks key ablation studies. The architectural design choice (placing RTFormer block on last two stages) is not supported by results. Baseline without attention is missing from Table 3 (a). Comparisons with other lightweight attention mechanisms are absent. Questions: While the authors claim GPUFriendly attention is GPUefficient Table 3 indicates only a marginal improvement in speed (189.6 vs. 189.8 or 187.9 vs. 189.8). There is no direct comparison between EA or other lightweight attentions and the proposed attention mechanism."], "nE6vnoHz9--": ["Paraphrased Statement: Summary: This research aims to utilize existing NAS Benchmarks to forecast the performance of architectures outside of the original domain. The authors propose a technique to develop a performance predictor by addressing the disparity in domain features between source and target architectural spaces. Strengths and Weaknesses: 1. The use of preexisting NAS benchmarks for training performance predictors is a practical approach. 2. The proposed approach for training the predictor by minimizing the distance between network feature space and label distribution appears novel. 3. The authors provide thorough ablation studies. Questions: 1. Eq. 9: The scheduling for Subspace Partition appears subjective. The authors should consider reducing its significance or comparing Eq. 9 with alternative schedulers to highlight its impact. 2. While the utility of the Assistant Architecture Space is evident the authors should provide design principles for it to maintain the methods generalizability. 3. dH in Eq. 10 is undefined or undiscussed. It is likely a distance metric between domains. 4. Thm 2 is overly broad to be directly applicable to this (NAS) work. The conclusion that larger m and m lead to lower error bounds is a general observation not specifically utilized in this study making it appear less relevant.", "Paraphrased Statement: This study presents CDP an innovative crossdomain predictor for Neural Architecture Search (NAS). Unlike previous methods CDP eliminates the need for training separate neural architectures in the target search space. Instead it utilizes existing benchmark datasets from the source NAS search space and employs domain adaptation to transfer knowledge to the target space. This approach significantly reduces the computational cost of NAS. Strengths: 1. Clear motivation to harness existing NAS benchmarks for costeffective NAS. 2. Method is theoretically sound and justified. 3. Demonstrated improved performance on ImageNet and CIFAR10 tasks compared to alternative methods. Weaknesses: 1. Dependence on existing NAS benchmarks making it challenging to apply to domains where such data is unavailable. 2. The design of the assistant space may not be suitable for complex search spaces beyond the relatively simple NASBench and DARTS spaces. Questions: 1. How does CDP perform when the source and target domains involve different tasks such as image classification and natural language processing 2. Could you explain the design process for the kernel function and mapping function used in CDP", "Paraphrase: Recent innovations in Neural Architecture Search (NAS) focus on creating neural predictors that directly assess the performance of neural architectures reducing the expense of evaluating performance. However these neural predictors still require annotated architectures for training. This paper presents a crossdomain adaptation strategy (CDP CrossDomain Predictor) that lowers these annotation training costs by using existing labeled datasets. Despite borrowing concepts from domain adaptation this papers originality remains significant as it transfers key ideas between subfields (domain adaptation to NAS). The paper presents the existing challenges in training neural predictors and explains how the proposed method addresses them. Applying domain adaptation methods to utilize labeled architecture space from one domain for another domain seems logical and NASspecific improvements add to the papers uniqueness. Notably the ablation study in Section 4.3 quantifies the quality of neural predictors under different settings of the proposed crossdomain adaptation strategies. While the paper demonstrates the effectiveness of the CDP strategys components (cosine scheduling mediumsized assistant space and LMMD) using Kendalls Tau (KTau) there are no comparisons to existing methods for training neural predictors that use annotated datasets for the target space. It would be valuable to assess if the proposed method generates neural predictors of comparable quality without annotations. Caveats: GPU hours used in the study are not specified. No comparisons with existing methods that utilize annotated datasets for target space training which would provide a more comprehensive evaluation of the proposed approach.", "Paraphrased Summary: The goal of this study was to address the difference between the source and target domains in neural architecture predictors. To do this they developed a progressive subspace adaptation strategy that trains a CrossDomain Predictor (CDP). Strengths: The CDP approach is straightforward and innovative. Transfer learning shows strong performance in NAS although the search cost comparison could be improved. The paper is wellwritten and presented. Weaknesses: The KTau score of 0.5306 is not particularly impressive. The authors should provide the TopK correlation to better evaluate the predictors performance. The comparison methods in Table 1 are outdated. More recent methods should be included for a comprehensive analysis. Questions: How would the performance be if only NASBench101 or NASBench201 were used as the source domain In Table 12 is it fair that other methods do not have transferred knowledge while CDP does considering that it includes prior knowledge from NASBench and does not account for the training time"], "nN3aVRQsxGd": ["Paraphrased Statement: Summary: This study explores the expressive capabilities of Khop message passing Graph Neural Networks (GNNs). It establishes that Khop GNNs are more effective than 1hop GNNs and can differentiate between certain regular graphs that are indistinguishable for 1hop GNNs. Furthermore a new GNN called KPGNN is proposed which surpasses Khop GNNs in expressiveness. KPGNNs use subgraph (edge) data from multiple hops alongside node information to discern between various distanceregular graphs. Empirical experiments corroborate the expressiveness of these new architectures for modest values of K (greater than 1). The study also provides a runtime analysis to assess the computational complexity. Strengths: Theoretically sound analysis Experiments demonstrate efficacy in graphs encountered in practice Weaknesses: Lack of an algorithm to determine sufficient K value Omission of established GNN hierarchies Ambiguity regarding practical applications of the analysis Unclear comparison with other GNN methods utilizing graph substructures High variance in some experimental results casts doubt on significant improvements over existing methods", "Paraphrased Statement: Summary This work explores the expressive capabilities of Khop message passing graph neural networks (GNNs) and introduces KPGNN an improved version that leverages peripheral subgraph information. Experiments validate KPGNNs performance on various graph tasks. Strengths Presents a comprehensive view of Khop message passing GNNs (Figure 1). Clear and wellorganized presentation. Weaknesses Contribution lacks clarity. Insufficient distinction from prior works. Certain claims require further clarification. Specific Weaknesses 1. Definition 3 and 4 introduce the concept of Khop message passing GNNs and node configurations but the gap between them is not explained. An injective COMBINE function doesnt guarantee hop order which may affect the expressive power of Khop message passing. 2. The paper fails to clearly differentiate KPGNN from GNNAK and Nested GNN both of which consider encoding khop subgraphs for all nodes. The advantages and differences over these methods are not discussed. 3. Empirical performance or time complexity improvements compared to these prior works are not evident in the manuscript. Questions Can you clarify the gap between Definition 3 and 4 and the role of the COMBINE function in preserving hop order How does KPGNN differ from GNNAK and Nested GNN and what are its additional advantages Can you provide data on the performance improvements or time complexity reductions of KPGNN over these prior works", "Paraphrase Summary: This paper examines the expressive power of existing message passing Graph Neural Networks (KGNNs) and introduces the Khop Peripheral SubgraphEnhanced Graph Neural Network (KPGNN). KPGNN enhances KGNNs by aggregating peripheral subgraph information leading to improved expressive power. Experimental results show that KPGNN performs well on benchmark datasets. Strengths: Theoretical analysis of expressive power for KGNNs and KPGNN. Competitive performance of KPGNN on benchmark datasets. Clear derivation of KPGNN. Weaknesses: Assumption of equal node features in the theoretical analysis. Lack of comparison with additional KGNNs such as MixHop Khop MAGNA and GPRGNN. Missing comparison of time and space complexity with other GNNs. Questions Raised: 1. Misuse of shortest path distance kernel and graph diffusion kernel in existing works. 2. Reason for the low increase in computational costs despite large K values. 3. Suggestion to use total running time for efficiency evaluation instead of time per epoch. 4. Use of multiset instead of set in Definition 2 for Khop neighbors. 5. Missing experiment settings such as run times and error bar definitions.", "Paraphrased Summary: The paper examines the Khop message passing Graph Neural Network (GNN). It demonstrates that the initial Khop GNN surpasses 1WL in terms of performance. The authors then propose an enhanced version that incorporates induced subgraphs at each hop to augment the original Khop GNN. The paper establishes theoretical results that differentiate regular graphs. Strengths: Indepth analysis of the Khop message passing framework and establishment of theorems for regular graphs. The paper is organized and straightforward. Weaknesses: Neglects the similarities between Khop message passing and distance encoding techniques used in Nested GNN and GNNAK. This analysis is crucial given the close connection between these methods. The proof that Khop is stronger than 1WL is considered trivial. Theorem 1 is also straightforward. Theorems 2 and 3 have similarities to theorems in the Distance Encoding paper. Limiting the analysis of expressiveness to regular graphs is a limitation. The enhanced Khop message passing approach utilizes subgraph information which has been extensively explored in methods such as Nested GNN GNNAK and Equivariant subgraph aggregation networks. While there are variances the contribution is limited. The experimental results on the TU dataset are questionable. The authors employ two evaluation settings and in the prevalent setting no performance improvement is shown. If the authors intend to demonstrate enhancement in the alternative setting baselines such as GIN and GINAK should be included. Questions: Despite the papers extensive exploration of Khop message passing concerns remain about its originality and effectiveness compared to other subgraph approaches."], "zD65Zdh6ZhI": ["Paraphrase: Researchers studied the concept of \"sufficient reasons\" (SRs) for decisions made by classifiers. An SR is a set of features that explains why a classifier classifies an input as a particular class. However SRs can be very large and complex making them difficult to understand for users. To address this the authors introduced a new concept called \"deltaSRs.\" DeltaSRs allow a certain degree of error by requiring only some percentage of possible completions of the partial feature assignment to be classified the same as the original input. The authors showed that finding deltaSRs over decision trees is computationally hard even for minimalsized deltaSRs. However they also provided some positive results for decision trees with certain structural properties or monotone classifiers. The authors also provided a SAT encoding for finding the smallest deltaSRs. However they found that this encoding is only practical for small decision trees. Strengths and Weaknesses: The complexity results show the difficulty of finding deltaSRs even for simple models like decision trees. The positive results provide some guidance for finding deltaSRs in specific cases. The SAT encoding provides a theoretical method for finding deltaSRs but it is not practical for large decision trees. Minor Points: The abstract should be rephrased to clarify that \"formal XAI\" is not only concerned with explaining decisions of ML models. The paper should cite a reference for the definition of \"free BDDs.\" The output for a decision tree with bounded split number should be n \u2264 k instead of n \u2264 k. Several grammatical errors need to be corrected. Question 2 in \"Future Work\" seems to be answered by Corollary 1. The phrase \"a series\" should have \"be\" before it. The reference to Izza et al. is duplicated in the bibliography. Questions: Can the notion of split number be extended to more complex models Are bounded split number and bounded treewidth equivalent Has the loss in accuracy been evaluated for restricting decision trees to have bounded split number", "Paraphrased Statement: After the authors addressed my concerns I now understand that the problem is more complex than I initially thought. While I still have reservations about the practicality of decision trees for NeurIPS the relevance of the dataset and the efficiency of the SAT encodings I am willing to change my evaluation to \"weak accept\" with lower confidence. The paper investigates computing \"deltasufficient reasons\" for decision trees. It demonstrates that finding minimal deltasufficient reasons is computationally hard under PNP assumptions. The paper proposes a SAT encoding for this task but experiments show that it is slow even for trees with a small number of leaves. While the papers technical contribution is strong I question its significance for NeurIPS. The computational complexity seems to stem from considering all possible extensions of minimal sufficient reasons which may not be necessary for decision trees. Additionally the runtime of the SAT encoding is impractical for realworld scenarios. I suggest exploring alternative approaches that focus on specific decision nodes and their extensions within the tree. This could potentially improve efficiency and provide more intuitive explanations. The authors may also consider using a tabular dataset instead of MNIST which is not an ideal choice for decision trees.", "Paraphrased Summary This paper examines probabilistic explanations for decision tree predictions using the concept of \"sufficient reasons\" in the context of explainable artificial intelligence. Given a decision tree a data instance and a confidence parameter a \"sufficient reason\" is a set of conditions that are both necessary and sufficient for the data instance to make a specific prediction with a certain level of confidence. The paper proves that finding minimal or minimum sufficient reasons is computationally difficult (NPhard) but it also explores tractable cases by considering restrictions on the decision trees structure and semantics. Finally it proposes a propositional SAT (Boolean satisfiability) encoding for computing minimum sufficient reasons. Strengths and Weaknesses The paper has some strengths including: Proving the NPhardness of finding minimalminimum sufficient reasons Exploring tractable cases and proposing a SAT encoding However it also has weaknesses: Requires reorganization and more polishing to be selfcontained The sketch of proof in the main theorem could be improved The SAT encoding has not been thoroughly tested and compared with other methods Questions Are the problems of finding minimalminimum sufficient reasons still intractable for decision trees of bounded depth Are they fixedparameter tractable with respect to depth Can we obtain an FPRAS (Fully Polynomial Randomized Approximation Scheme) for computing minimalminimum sufficient reasons for monotone DNF (Disjunctive Normal Form) formulas"], "slKVqAflN5": ["Paraphrased Summary: Authors present a novel algorithm for minimizing Lipschitz functions. It builds upon the [ZLJ 20] algorithm and introduces a practical oracle. The algorithm is easy to implement and achieves optimal complexity. In low dimensions the authors show improved complexity using a cutting plane algorithm instead of a subgradient method. Strengths and Weaknesses: Strengths: Easy implementation Optimal complexity in general Improved complexity in low dimensions Weaknesses: Last section on low dimensions lacks clarity and context Minor typographical errors Questions: Typographical error in line 53 Inequality in paragraph after equation (4) may need correction Lemma 2 constant may be optimizable Existence of set Q in Lemma 3 requires additional justification Different fonts used for Q in Lemma 3 proof Boundedness of Q may be necessary for line 194 Proof of Lemma 3 could benefit from more explicit mention of firstorder optimality conditions Section 3.1 could provide more details for readers unfamiliar with cutting plane and ellipsoid algorithms Proof of Theorem 4 uses Lipschitzness twice without explicit mention Bound for \\ \\hat gk \\hat \\zeta\\2 in Theorem 4 may contain an extra factor of 2", "Paraphrased Statement: This research paper explores methods for minimizing nondifferentiable Lipschitz functions commonly found in machine learning applications like ReLUbased neural networks. The subgradient method has been used for such functions but the algorithm in [ZLJ20] requires solving a complex auxiliary problem. To address this issue the paper proposes two improved algorithms that replace the auxiliary problem with a simpler standard approach. These algorithms provide approximate solutions and have improved efficiency compared to [ZLJ20]. Strengths: Addresses a practical problem in optimization with Lipschitz functions. Novel algorithms with rigorous analysis and stateoftheart accuracy guarantees. Cleverly employs randomization to overcome nondifferentiability issues. Weaknesses: No experimental evaluation provided in the paper. Computational cost of calculating Lipschitz constants may be a limitation. Questions: 1. Could these algorithms be applied to training ReLUbased neural networks 2. Are there potential limitations due to the time required to calculate Lipschitz constants", "Paraphrased Statement: This paper investigates finding stationary points of functions that are Lipschitz continuous. It improves upon the algorithm proposed in [ZLJ20] which used Goldsteins subgradient method. The difficulty with [ZLJ20] was its requirement for solving a convex feasibility problem (Equation 2). Our paper introduces an algorithm that does not have this limitation and achieves the same result as [ZLJ20]s Theorem 6. We show that our algorithm is more efficient in lowdimensional scenarios. Additionally it can handle weakly convex functions (defined in line 19). Strengths: Significant improvement over [ZLJ20] Comprehensive literature review Weaknesses: Lack of clarity on technical differences from [ZLJ20] Absence of a concluding discussion section Limited explanation of the role of low dimensions and its impact Questions: 1. Clarification on the result in [ZLJ20] mentioned in lines 110111. 2. Definitions of \\mathbbR\\y\\ and Q in the proof of Lemma 3. 3. Computation of the center of gravity for the potentially complex set \\Omegak in Algorithm 3 line 4. 4. Definition of the covariance matrix of a convex set in Theorems 11 and 12.", "Paraphrased Statement: Summary: This paper addresses optimization in nonconvex and nonsmooth settings. Utilizing definitions from Golshtein (1977) and recent findings from Zhang Li and Jiang (2020) it presents an efficient algorithm (Algorithms 1 and 2) that provides guarantees (Corollary 5 and Theorem 6) under assumptions of Lipschitz continuity and almost everywhere differentiability (according to Lebesgue measure). The authors also propose a cuttingplane method to address the intermediate minNorm problem (previously handled by Algorithm 1). While this method improves complexity dependence on epsilon it introduces a dependence on dimension limiting its usefulness to lowdimensional problems. Strengths and Weaknesses: Strengths: Strong guarantees with minimal assumptions. Weaknesses: Zhang et al. (2020) have obtained similar results although their method assumes prior knowledge of the solution to a specific problem. The assumption that the solution to this problem is readily available appears overly optimistic. Additional Questions and Clarifications: Line 47: The cited paper implies that if the solution to the problem is known for constituent functions the chain rule can provide the solution for the composite function. Line 61: This assertion depends on the assumption that the previous claim is incorrect. Line 110: The authors may be referencing Golshtein (1977) rather than Zhang et al. (2020). Lines 113 and 123: It is recommended to use consistent notation throughout the paper including uniform labeling of iterates and a strict inequality in Line 123 to ensure the validity of the argument."], "wOI0AUAq9BR": ["Paraphrase: This paper introduces a straightforward regularization technique that enhances graph network generalization across graphs of varying sizes. It operates by employing coarser versions of a graph and promoting the alignment of node embeddings between these coarse and original graphs. Strengths and Weaknesses: Strengths: Simple and intuitive concept Clear and wellwritten Technically sound Potential for practical use in research and industry Weaknesses: Lack of clarity regarding the influence of the selected CKA metric on results Possible concerns about the experimental protocol used for evaluating the regularization technique (specifically potential reliance on test set performance rather than validation set performance for hyperparameter optimization) Minor Comments: Consider clarifying the relationship between coarsening ratio and percentage of retained nodes. Suggest exploring hyperparameter optimization on a more challenging dataset than PROTEINS. Questions: Provide details on the lambda values used in the experiments presented in Section 4. Analyze how the graph size distribution changes after preprocessing especially for datasets with significant size variations. Consider exploring the impact of higher coarsening ratios particularly on social datasets.", "Paraphrased Statement: Summary: The paper presents a new technique for enhancing size generalization in Graph Neural Networks (GNNs). It utilizes coarsening algorithms to create simplified versions of the training data. The technique forces node embeddings from the original graph to be close to embeddings from the coarsened graphs. The paper evaluates its effectiveness on four graph classification benchmarks. Strengths: The method is straightforward and applicable to various scenarios. The problem addressed is relevant. Weaknesses: The paper lacks a deeper explanation of certain design choices (e.g. graph coarsening and discrepancy metrics). The experimental setup is limited to four molecular datasets and follows a previous setup. The analysis section lacks a comprehensive assessment of the methods impact on node representations. Questions: Is there a connection between the assumption that labels are not sizesensitive and the selected benchmarks Why is the method only tested on four molecular datasets Would different discrepancy metrics such as distancebased ones yield similar or better results Can the method enhance pretraining by splitting molecules according to their scaffold Are training times reported to support the claim that the method incurs a 50 overhead", "Paraphrased Statement: Summary: This paper explores methods for making Graph Neural Networks (GNNs) more adaptable to graphs of varying sizes for graph classification tasks. Prior approaches either employed arbitrary strategies or demanded access to evaluation graphs. The authors present a regularization technique based on graph coarsening to enhance the sizegeneralization capabilities of GNNs. Experiments demonstrate that this regularization approach can increase performance by up to 30. Strengths: Introduces a regularization method that enhances the ability of GNNs to generalize across graphs of different sizes. Experimental findings demonstrate that GNNs using the proposed regularization method exhibit comparable or superior sizegeneralization performance to previous methods. Weaknesses: The method is relatively straightforward and its novelty is limited. It combines existing graph coarsening techniques and a known metric (CMD) for regularization. Questions: While the experiments used the SGC coarsening algorithm it is unclear if the authors evaluated other coarsening methods and their impact on the regularization performance."], "qwjrO7Rewqy": ["Paraphrased Summary: Extracting topological features (PDs) from data is useful for developing machine learning methods but the current algorithms are computationally complex. This article proposes a new approach using neural networks to approximate PD computation making it more efficient. Strengths and Weaknesses: Originality: The approach is novel approximating the inner workings of topological descriptor algorithms with neural networks. Clarity: The paper is wellwritten and easy to follow. Significance: The method has potential for future research in topological data analysis (TDA) and demonstrates the benefits of graph neural networks (GNNs) for extracting topological information from data. Quality: The paper is of high quality with a comprehensive experimental evaluation. Questions: 1. How computationally intensive is the W2 loss function which requires matching computation for gradient calculation 2. Please explain how the training loss is implemented and how edge decomposition improves learning efficiency. 3. Is using the persistence image evaluation metric redundant since PDs should be stable under the W2 metric Suggestions: Cite additional research on learning to produce topological features from point clouds such as \"Learning persistent homology of 3D point clouds\" and \"RipsNet.\"", "Paraphrased Statement: The authors present a mechanism for improving the efficiency of constructing extended persistence diagrams (EPDs) which are effective in graph analysis. They analyze the EPD calculation algorithm and optimize it by using a GNN to approximate the UnionFind algorithm thereby reducing computational complexity. The authors demonstrate the effectiveness of EPD in graph analysis highlighting its efficiency and accuracy particularly for large graphs. Strengths and Weaknesses: Strengths: The proposed algorithm efficiently optimizes EPD calculations based on structural analysis. The authors evaluate the impact of the optimized algorithm on EPD and show significant improvements in both speed and accuracy. Weaknesses: The paper lacks a comprehensive comparison with other EPD acceleration methods. The assumption that EPD outperforms other graphoriented TDA methods limits the broader applicability of the proposed algorithm. Questions: Provide evidence that EPD is more effective than other TDA methods. The paper does not address this question. Discussion on whether there is any other research on EPD acceleration. The paper does mention other methods for TDA acceleration but not specifically for EPD. Instead of approximating part of the algorithm with a GNN why not approximate the entire flow with a GNN or NN The paper does not discuss this alternative approach. Additional Considerations for Future Research: Investigating the applicability of the proposed algorithm to other TDA methods. Evaluating the performance of the proposed algorithm in comparison to alternative approximation techniques. Exploring the potential of approximating the entire EPD construction process using GNNs or NNs.", "Paraphrased Summary: This research introduces an efficient framework for estimating extended persistence diagrams (EPDs). By recasting EPD as an edgewise prediction task it breaks down into separate pairing issues. The authors employ a graph neural network (GNN) architecture to implement the unionfind algorithm which can address these pairing challenges. Strengths and Weaknesses: Strengths: The EPD estimation method delivers superior performance on various datasets. The approach demonstrates efficiency in handling large and dense graphs. The paper is wellwritten and straightforward. Weaknesses: A related work on employing persistence diagrams for graph representation learning could be included as a baseline. Realworld graphs tend to be sparse but the study focuses on dense graphs. The performance on sparse graphs which are more prevalent in complex systems should be examined. The proposed methods speed advantage diminishes for sparse graphs (citation networks). Questions: 1. Can this EPD decomposition be applied to simplicial complexes with dimensions beyond graphs (2simplices 3simplices etc.) 2. How is the khop value chosen when constructing vicinity graphs 3. Is there a threshold value for average neighborhood size or degree that determines the most efficient method for EPD estimation 4. Does the khop parameter choice influence the efficiency results reported in the study"], "s6ygs1UCOw1": ["Paraphrased Statement: Summary: This research presents an algorithm for metalearning that creates a set of predictions with PAC guarantees (Probably Approximately Correct). The algorithm aims to adjust the predictor to new unseen tasks using only a few labeled training samples. Experiments on various datasets show that the proposed method is effective in reducing the prediction set size while maintaining PAC guarantees. Strengths: The paper is wellorganized and written. The method is sound and supported by theoretical analysis. Experiments demonstrate the effectiveness of the method on different domains and datasets. Weaknesses: The method does not adapt the metaprediction set parameters after adaptation which could improve results. The method is currently limited to metricbased metalearning algorithms (e.g. prototypical networks) and may not be directly applicable to optimizationbased methods (e.g. ModelAgnostic MetaLearning). Experiments on the miniImageNet dataset only consider 5way classification. It would be valuable to compare the methods performance on more challenging multiway classification tasks. Questions: Could the method be extended to adapt metaprediction set parameters during adaptation Is it possible to apply the method to optimizationbased metalearning algorithms How does the method perform on more complex multiway classification tasks such as 10way or 20way classification on the miniImageNet dataset", "Summary The research paper presents a technique for creating prediction sets during the metatest phase based on prediction sets gathered during metatrain phases of adaptation and calibration. This method aims to meet PAC (probably approximately correct) requirements and ensures theoretical support for producing metatest prediction sets without calibration. The approach was evaluated across four datasets in three different domains delivering competitive results. Strengths: The method provides PACbound guarantees. Weaknesses: The papers presentation is confusing with notation that can be misleading. It lacks a clear overall picture omitting details such as the pretrained nature of the metamodel in Figure 1. Questions: 1. In the evaluations both prediction set error and size were considered. MetaPS exhibits low error but sometimes larger set sizes than others. An ideal result would have both low error and size. Could the authors clarify the comparison between MetaCP and MetaPS 2. The paper uses PSBinom to estimate \u03c4. Do alternative estimators impact performance 3. The score functions in lines 108 and 106 have different input spaces. Could the authors explain why My Understanding: f is a pretrained metamodel.", "Summary POST DISCUSSION The authors have improved the readability and addressed the attribution issue leading to a provisional acceptance. Final Assessment The proofs and related work are accurate. The paper presents a novel concept of twostage trainingconditional guarantees in metalearning. It introduces a distributionfree uncertainty quantification guarantee and an algorithm to implement it. The paper contributes by extending existing guarantees and using trainingconditional conformal for metalearning. Strengths Technically sound results. Comprehensive experiments. Weaknesses Complicated Guarantee: The guarantee is difficult to interpret and may not be widely used due to the cognitive burden associated with parameter setting. Writing Difficulty: The paper is challenging to comprehend due to dense writing and complex explanations (e.g. Figure 2). Limited Novelty: The technical result is not particularly groundbreaking and the domain area has been explored. Attribution Issue: The concept of trainingconditional prediction sets originally belongs to Vovk [21] and should be properly attributed. The algorithm is also based on Vovks work. Questions How can tasks be considered \"iid\" in practical settings outside of synthetic environments In what realistic scenarios can the authors guarantee be expected to hold", "Paraphrased Statement This research explores concepts and techniques for generating accurate predictions in meta learning settings where multiple distributions are involved. Unlike conventional machine learning the PAC notion in meta learning differs. Additionally it differs from scenarios with covariate shifts. To address this the paper introduces the metaPAC (mPAC) concept which utilizes a calibration set from multiple distributions (p\u03b81...p\u03b8N) to create a reliable prediction set for a test distribution (p\u03b8) with high probability (1\u03b4). The study proposes Algorithm 1 to determine the prediction sets parameter (\u03c4) employing a twostep approach. It first calculates the PAC estimator threshold on each distribution (N) and then combines these thresholds (with labels) to create a larger sample (N) to determine a threshold with (\u03b12 \u03b4)PAC. Theorem 1 demonstrates this algorithms guarantee for (\u03b5 \u03b1 \u03b4)mPAC. Strengths Extends PAC theory for conformal prediction to the meta learning scenario. Provides a practical algorithm for constructing mPAC predictions. Presents strong theoretical analysis. Weakness Algorithm 2 employs grid search for \u03c4 potentially affecting computational efficiency. Questions Could the authors provide more information on the computational costefficiency of Algorithm 2 including the granularity of the grid search Are there any potential optimizations to speed up the grid search process", "Paraphrased Summary: This paper expands on PAC prediction sets by applying them to fewshot metalearning scenarios where access to a sequence of independent and identically distributed (i.i.d.) tasks is assumed. In fewshot metalearning the goal is to generalize to a new task with limited training examples. To achieve this the learner is exposed to a set of related tasks before encountering the test task. Establishing reliable confidence sets for fewshot learning is challenging. This paper builds upon previous work utilizing the task distribution to generate meaningful conformal prediction sets with coverage guarantees over calibration and test tasks. However this paper employs PAC prediction sets which offer highprobability bounds on the expected miscoverage across new tasks given the observed calibration tasks. The paper presents a theoretical framework and empirical evidence to support its algorithms effectiveness on simulated fewshot metalearning datasets and a realistic heart attack prediction task. Highlighted Strengths: Novel extension of MetaCP (MetaConformal Prediction) to the PAC prediction set framework. Desirable performance guarantees including validity on average across all new tasks. Practical assumption of using a fixed calibration set for future task predictions. Highlighted Weaknesses: Fixed threshold for all downstream tasks regardless of adaptation set size. Impractical assumption of access to hundreds of i.i.d. calibration tasks. Relatively modest theoretical contribution due to its straightforward combination of existing techniques. Questions: In the experiments are the calibration and test tasks sampled from disjoint sets The explanation in lines 171174 is unclear regarding the estimation of a threshold t."], "zqQKGaNI4lp": ["Paraphrased Statement: Summary: Researchers have developed a method called \"manifoldHilbert kernel\" to combine multiple predictors for predicting values on a curved surface (a manifold). This kernel uses distances between points on the surface similar to distances in ordinary threedimensional space. The researchers have shown that this method is theoretically reliable. They have also created a specific version of the kernel that works on spherical surfaces. Strengths: The method can accurately predict values on curved surfaces. The kernel is based on welldefined mathematical principles. A specific version of the kernel has been developed for use on spheres. Weaknesses: The paper is highly technical and difficult to understand. The paper does not include experimental results to demonstrate the methods performance. Questions: None", "Paraphrased Summary: This study defines a mathematical kernel for estimating the relationship between two variables (X and Y) on a geometric space called a manifold. Specifically the \"manifold Hilbert kernel\" is defined as the inverse distance between two points on the manifold raised to the power of the manifolds dimension. The paper presents a key theorem (Theorem 3.2) proving that this manifold Hilbert kernel which interpolates the observed data provides consistent estimates of the relationship between X and Y both locally (at each point) and globally (in terms of the distance measure relative to the density of X). In a specific case discussed in the paper (Section 5) the manifold considered is a ddimensional sphere. It is shown (Theorem 5.2) that an ensemble method based on a random arrangement of hyperplanes coincides with the manifold Hilbert kernel. This implies that ensemble classifiers of this type are interpolating and consistent. Strengths and Weaknesses: Strengths: Addresses an important statistical problem. Clear mathematical presentation. Weaknesses: Limited to theoretical consistency analysis (i.e. no convergence rates). Requires specialized knowledge of Riemannian geometry. Questions: Does the specific ensemble method considered in the paper (weighted random partition) represent a general form that encompasses many ensemble methods Are there any limitations in the scope of the contribution due to the specific type of ensemble method used", "Paraphrase: The paper analyzes ensemble methods for classifying data on Riemannian manifolds (curved surfaces). It introduces the \"manifoldHilbert\" kernel which allows for both interpolation (exact prediction of training data) and consistency (accurate predictions for unseen data). The authors link ensemble methods to the manifoldHilbert kernel through weighted random partitions. When a specific random partition results in the manifoldHilbert kernel the corresponding ensemble method becomes interpolating and consistent. They demonstrate this phenomenon for the sphere manifold where a hyperplane arrangement random partition leads to the desired kernel. Strengths: The problem addresses a topic of interest in machine learning with a significant consistency result. The paper is wellwritten and accessible despite its technical complexity. The related work section provides a clear understanding of the papers goals. Weaknesses: The core technical contributions (lemmas) are difficult to evaluate for nonexperts in Riemannian geometry. The papers theoretical and technical nature may be more suitable for specialized venues such as COLT rather than NeurIPS. Additional Comments: The PDF links are not working. There is a typo in line 77 (\"when\" instead of \"where\"). Figure 1s placement could be improved for readability.", "Paraphrase: Summary: This paper introduces the manifoldHilbert kernel and establishes its use in kernel smoothing regression and classification. These methods achieve weak consistency ensuring generalization capabilities even when the data has an interpolating characteristic. For spherical data the kernel is represented as a weighted random partition similar to an ensemble of partitionbased classifiers. The authors suggest that this provides a theoretical framework for understanding the generalization of ensemble methods like decision trees. Strengths: Indepth theoretical analysis Clear and concise writing Explanations using natural language to facilitate comprehension Extends Devroyes work on kernel regression with the Hilbert kernel demonstrating interpolation and weak consistency for data with density and bounded labels Weaknesses: Heavy use of technical jargon making it challenging for those unfamiliar with the field Lack of background context to enhance understanding Questions: The authors propose a connection between their work and the generalization of decision trees. Can they provide more concrete evidence to support this claim"], "vbPsD-BhOZ": ["Paraphrase: Summary This study explores challenges in graph neural networks (heterophily and oversmoothing) using the framework of cellular sheaf theory. It suggests that these issues can be addressed by crafting a suitable sheaf model for the task. The paper introduces the NSD model based on this theory which shows promising performance on graphs with heterophily. Strengths and Weaknesses Strengths: The concept of linking sheaf theory and graph neural networks is innovative and beneficial offering a fresh perspective on heterophily and oversmoothing. The theoretical discoveries regarding selecting an appropriate sheaf structure to mitigate these problems could significantly advance research in the field. The mathematical analysis appears rigorous and the experimental findings support the theoretical predictions. Weaknesses: Readability: Readers unfamiliar with sheaf theory may struggle to understand the abstract theoretical concepts. Additional explanations would enhance comprehension. The theoretical results in Section 3 relating to harmonic space may be more relevant to mathematical analysis than to the main topic of the paper and could be placed in an appendix or elaborated on further. Implementation and Experiments Concerns: Computational complexity: The algorithms complexity scales with stalk dimension potentially hindering scalability for inference. Lack of baseline: The proposed NSD model should be compared to its predecessor SheafNN to quantify its empirical superiority. Performance implications: While higher stalk dimension and complex restriction maps may enhance separation power it remains unclear whether this directly improves the models ability to handle heterophily. Positional encoding: The table caption incorrectly states \"stalk dimension\" in reference to positional encoding results.", "Paraphrased Statement: This research utilizes cellular sheaf theory to investigate oversmoothing in Graph Convolutional Networks (GCNs). It introduces a novel model the Sheaf Convolutional Network (SCN) which extends GCNs by incorporating both sheaves and convolutional weights. Strengths: The paper is wellwritten and accessible despite its technical complexity. Its theoretical contributions identify limitations of GCNs in heterophilic graphs and multiclass tasks providing potential explanations for their performance issues. SCNs with their linear class separation capabilities offer advantages over GCNs. The Dirichlet energy in SCNs is not constrained to decrease unlike in GCNs. Weaknesses: The paper could benefit from improved connections between theoretical results and the problems they address specifically clarifying the relationship between Dirichlet energy and oversmoothing. The numerical experiments are compelling but somewhat limited. Questions: Would nonlinearities other than Leaky ReLU and ReLU alter the conclusions of Theorem 18 Can readability be enhanced by highlighting the implications of GCN limitations in lines 239242", "Paraphrase: Summary The paper presents a new concept of graph neural networks (GNNs) based on \"sheafs\" on graphs. Sheafs are vector spaces and linear operators associated with the nodes and edges of a graph defining a specific message passing mechanism where messages are transformed by linear maps. This leads to a generalized notion of Laplacian as a block matrix containing the linear operators. The authors analyze the harmonic properties of this Laplacian finding that its dominant eigenvector influences the diffusion process. They investigate different types of sheafs and their ability to separate data through this diffusion process. The paper also proposes a sheaf learning procedure analogous to a generalized attention process. Strengths The concept is original and grounded in algebraic geometry and topology. The study is comprehensive. Weaknesses The connection between the proposed model and existing ones could be clarified especially regarding the learning of sheafs. The claim that SCNs (sheaf convolution networks) are more expressive than GCNs (graph convolutional networks) may be misleading as MPNNs (messagepassing neural networks) can potentially simulate SCNs with enough layers. In practice learning different sheafs at each layer challenges the relevance of the asymptotic diffusion analysis. Questions Will the data separation capability of sheafs be affected by perturbations How well does learning a single sheaf for all layers simulate the expected diffusion process", "Paraphrased Statement: Summary: This paper presents a new topological approach to graph neural networks (GNNs). The authors introduce the concepts of \"cellular sheaf\" and \"sheaf Laplacian\" which enable the definition of a diffusive process on a graph. This process can classify nodes in a labeled input graph and its effectiveness is determined by the type of sheaves used. The continuous sheaf diffusion can be discretized using the Euler method to obtain the sheaf convolution. The properties of the sheaf convolution are analyzed by observing its impact on the processs Dirichlet energy. Since the sheaf convolution operator is typically unknown it must be learned from data. The paper proposes three different convolution models for this purpose: diagonal orthogonal and general sheaf operators. Experimental results show competitive performance compared to standard GNN models. Strengths and Weaknesses: Strengths: The paper applies topological concepts to graph learning an innovative approach. The connection between sheaf structure and diffusion process classification ability is a notable finding. The paper is wellwritten and easy to understand. Weaknesses: Section 5 requires clarification. The experimental setup could be described in more detail. The variance in experimental results makes it difficult to conclude that the proposed models outperform baseline models. Questions: 1. Why is the Dirichlet energy of the sheaf convolutional network important 2. Why is preference given to sheaf convolutional networks that do not converge rapidly to equilibrium 3. How is X(0) calculated Is an MLP used and are the same MLPs applied to all input features 4. How is the parametric matrixvalued function \u03c6 used Is it employed to explicitly construct the sheaf Laplacian Is the computational cost of evaluating \u03c6 considered (which should be O(d2) for the diagonal case) 5. Does the definition of \u03c6 in the paper allow for the construction of only symmetric sheaf operators Are there alternative approaches that balance complexity and expressiveness Minor Observations: In Proposition 1 W should have a subscript 1. Suggest using a different capital letter for the parameter of the MLP that learns the sheaf operator (W is already used)."], "pd6ipu3jDw": ["Paraphrased Statement: This research presents a novel Multiagent Reinforcement Learning (MARL) technique that employs the Agent Transformer Memory (ATM) module. ATM is comprised of four components: memory self allies and entities. It notably incorporates a selftoken duplication mechanism for use in the EntityBound Action (EBA) layer which significantly influences performance. ATM draws inspiration from working memory concepts and explores other memory types through an ablation study. MARL agents powered by ATM excel in the StarCraft MultiAgent Challenge (SMAC) and its architectural elements (EBA and working memory) outperform alternative approaches (e.g. relational memory or no EBA). While the authors acknowledge the potential benefits of appending style memory they did not investigate this option in their model. They also addressed numerous concerns raised during the authorreviewer discussion phase. Strengths: Novel ATM memory module that infers interactions between agents allies entities and memory. ATMbased agents outperform GRUbased agents on SMAC. Ablation studies validate the efficacy of EBA and different memory types and slot numbers. Weaknesses: Transformerbased agents generally require more computation which may compromise efficiency (although the study does not provide specific time comparisons with GRUbased models). ATMs working memory mechanism limits explicit attention to the past unlike other transformerbased agents. Questions: Can the authors provide wallclock time comparisons for the models in Figures 2 and 6 to assess the potential performance gap between ATM and GRU Have the authors explored the use of appending style memory or TrXL style memory as potential improvements to the models ability to attend to the past", "Paraphrase: This paper proposes a novel architecture Agent Transformer Memory (ATM) for multiagent reinforcement learning. Each agent has its own limitedcapacity working memory and action space. The ATM architecture utilizes a transformerbased model to process both the environments dynamic entities and the agents recent memory slots. The transformers output consists of an updated memory slot and representations of each entity. Each agent selects an action based on their latest memory and these entity representations. Experiments on SMAC and LBF environments show that agents with ATM perform better than those without demonstrating the benefits of memory and personalized action spaces. The study also explores different memory update strategies and finds that ATM performs optimally. Strengths: Unique concept of each agent having its own environmental perspective working memory and action space. Comprehensive ablation study demonstrates the importance of working memory. Potential Weaknesses: Lack of explicit coordination among agents. Assumption of predetermined entity count and attributes which may limit applicability in partially observable environments. Unclear performance against other learning agents. Originality and Significance: The paper leverages a transformerbased working memory in a multiagent RL context. The EntityBound Action Layers relation to previous work is unclear. Empirical results suggest ATMs potential value for the community. Minor Issues: Typographical errors (e.g. \"de if not what is do\") Ambiguous terminology (e.g. \"sight field\" vs. \"sight range\") Incomplete Agent 5 heat map in Figure 5 Questions: Are Tin and Tout indexed by the agents ID (i.e. Tini and Touti) Why is Agent 5s heat map incomplete in Figure 5 Does memory become less useful after the initial steps in the experiments Have the authors evaluated the performance of trained agents with different architectures against each other", "Paraphrase: Summary: This study introduces a method for modeling partiallyobservable multiagent Markov decision processes (POMDPs). It includes two enhancements to a standard transformer: 1. Memory Module: Stores observation data linked to all agents from prior observations. 2. Prediction Layer: Filters observation information for predicting actions by specific agents (e.g. attacking enemy i is based on observation data about enemy i). The model was tested in two MARL environments and outperformed existing memory modeling methods including those using recurrent neural networks (RNNs). Strengths and Weaknesses: Strengths: Novel approach to modeling POMDPs. Clear explanations and comprehensive experiments. Superior performance over peer methods in memory modeling. Weaknesses: Transformerbased working memory integration may seem like an adaptation of an existing method to a known problem. Unclear definition of \"observation\" (allys or enemys perspective). Potential impact of information sharing between agents on performance. Questions: Alternative approaches to the entitybound action (EBA) layer: Predicting actions based on selfobservation only. Combining all observations (with potential drawbacks). Clarification on EBA ablation experiments (whether all three settings were tested: EBA selfonly and all observations). Potential for interpreting the memory data such as visualizing it as an image."], "p62j5eqi_g2": ["Paraphrased Statement: This research introduces the first attack against deep clustering models that can operate without knowing the models internal details (blackbox). It bridges the gap between earlier adversarial attacks on traditional clustering and the unexplored vulnerabilities of deep learningbased clustering models that have only been targeted by attacks with known model details (whitebox). The attack leverages Generative Adversarial Networks (GANs) and only requires limited access to the clustering models output scores. It has been tested against stateoftheart deep clustering models and a realworld Machine Learning as a Service (MLaaS) system. Despite the attacks effectiveness existing defense mechanisms have proven ineffective in mitigating it. Strengths: Targets realworld MLaaS systems. Evaluates potential countermeasures. Achieves high attack success rates. Weaknesses: Limited originality in technical approach. Some aspects have been previously investigated though the combination of elements is novel. Unclear justification for epsilon bounds used in the evaluation. Questions: How is the surrogate model for Face trained Why are epsilon bounds inconsistent across datasets Are epsilon bounds equal in transferability studies Why do noise patterns appear consistent Why are most attacks clustered in the same class Can specific realworld threat vectors be provided to clarify societal impact and justify the need for robust deep clustering techniques Minor Comments: Clarify the specific threat model contribution in the introduction. Include specific performance degradation figures with the attack in the introduction. Distinguish between \"scorebased\" and \"decisionbased\" blackbox models.", "Summary This paper introduces a novel attack on deep clustering models where the adversarial perturbation is generated by a generative model. It exploits the weaknesses in existing defense methods for clustering and exhibits transferability making it practical. Experiments on the Face API and widely used datasets demonstrate its effectiveness. Strengths Focuses on runtime adversarial attacks against deep clustering models an underexplored area. The attack method is straightforward and effective as evidenced by experimental results. Weaknesses Minor issue (paper organization): The introduction could be improved by omitting traditional clustering methods and focusing on deep learningbased methods their attacks and the risks they pose in practical applications. Minor issue (unclear figure): Figure 1 does not clearly identify which picture corresponds to the SPICE RUC model. Minor issue (potentially misleading): The statement on line 35 that \"zeroth order optimization is intractable for deep models\" is ambiguous and may convey incorrect information. Zeroth order optimization is a commonly used approach for blackbox attacks against supervised deep classification models. Insufficient citations: The paper should cite and discuss relevant attackdefense works for image classification and explain why those techniques cannot be adapted for clustering. Misleading contents: \"Adversarial attacks against clustering\" is a misleading title as it refers to runtime attacks while trainingtime attacks are referred to as backdoor or poisoning attacks. Important relationship with classification attack: The paper claims that existing blackbox runtime attacks against classification models are incompatible with clustering models which is incorrect. Scorebased attacks like NES and SPSA can attack cluster assignment based on softmax scores. Minor issue (unconvincing claim): The claim that ALRDC restricts the use of a robust latent space for adversarial robustness is not wellsupported. Confusing figure: Figure 2 shows an irregular pattern after attack which contradicts the untargeted nature of the attack. Questions Why cannot existing classification attackdefense techniques be adapted for clustering Why is the title misleading What specific evidence contradicts the untargeted nature of the attack in Figure 2", "Paraphrased Statement Summary This paper presents a novel attack strategy against deep clustering models. The attack is formulated as an optimization problem where the authors define a threat model and attack objective. To craft adversarial perturbations they leverage a combination of generative adversarial network (GAN) training loss and the attack objective resulting in a GANbased blackbox attack. The attacks effectiveness query complexity and transferability are thoroughly evaluated through experiments. The paper also demonstrates the attacks success against two common defense mechanisms (robust deep clustering and deep learningbased anomaly detection) and a realworld face clustering service. Strengths and Weaknesses Originality: This paper introduces innovative ideas and is the first to explore adversarial attacks against deep clustering models. Strengths: 1. Extensive testing of the attack against various clustering models and performance metrics. 2. Exploration of query complexity and transferability to assess the attacks capabilities. 3. Demonstration of the attacks effectiveness against existing defense strategies. Weaknesses: 1. The visualization of Principal Component Analysis (PCA) patterns between adversarial examples and original samples may not provide a comprehensive analysis of the attacks impact. Clarity: The paper is wellwritten and easy to understand. Significance: 1. The attack exhibits remarkable effectiveness significantly reducing clustering performance across multiple models and metrics. 2. The attack poses a significant threat to existing defense mechanisms highlighting the need for enhanced robustness in deep clustering algorithms. 3. The successful transferability of the attack to a realworld face clustering service demonstrates its practical relevance. Questions: 1. Exploring the impact of the attack on earlier deep clustering models may provide insights into its behavior in lowerdimensional representations. 2. Investigating anomaly detection on lowdimensional representations of adversarial samples could further assess the attacks ability to evade detection.", "Paraphrased Summary: This study examines vulnerabilities in deep clustering models to \"blackbox\" adversarial attacks. It introduces a blackbox adversarial attack method using the vanilla GAN approach. The attack performance is evaluated against several topperforming deep clustering models and realworld images. Results indicate that the proposed attack significantly weakens model performance while requiring minimal data access (query complexity). To enhance robustness the study explores two unsupervised defense mechanisms but finds them ineffective against the attack. It further demonstrates successful attacks on a productiongrade API (Face). Strengths: Proposes the first blackbox adversarial attack against deep clustering models. Conducts extensive experiments to validate the attacks effectiveness across various models datasets query complexities transferability and resistance to defenses. Weaknesses: 1. The attack approach appears straightforward and there is limited innovation or novelty in the methods design. The approach relies on indirect attack objectives raising concerns about the validity of success rates. 2. The epsilon value used to quantify adversarial noise may not be an accurate representation of attack intensity as its threshold can vary significantly across datasets. A more appropriate metric to assess the impact of adversarial noise on data quality is needed. Questions: 1. The PCA plots may not provide conclusive evidence as they include all classes and adversarial samples may not be easily distinguishable when mixed with original data. 2. The definition of \"best\" in Figure 6 is unclear and it is not evident why the plot contains an error bar. 3. The necessity and effectiveness of anomaly detection as a defense mechanism are questionable given the close proximity of adversarial samples to original data. 4. The comparison of query complexity between different attack mechanisms may not be fair as the methods and attack stages vary."], "t4vTbQnhM8": ["Paraphrased Summary This paper presents an adjusted version of the kernelized Stein discrepancy (called nonparametric KSD or NPKSD) for evaluating implicit generative models. The author addresses a situation where: 1. Densities of the generative model are not available. 2. Data has high dimensionality. 3. The number of actual observations is limited. To address these challenges the author proposes a nonparametric Stein operator based on: Conditional distributions with summary statistics Resampling of conditional dimensions Score matching for gradient estimation Applying the kernel trick and a Monte Carlobased goodnessoffit test results in the NPKSD method. Strengths and Weaknesses: Strengths: Wellwritten and understandable paper Addresses a significant problem in evaluating implicit models Weaknesses: Motivation for using goodnessoffit test could be stronger Theoretical guarantees for equivalence between NPKSD 0 and p q are unclear Use of summary statistics and score matching introduces potential complications Novelty of the method may be limited as it builds on existing techniques Comparison to kernel Stein discrepancy (KSD) in synthetic distribution experiments is lacking Questions: 1. How does NPKSD compare to other Stein discrepancies and their applications in generative modeling 2. What are the theoretical implications of using summary statistics and score matching on the evaluation of implicit models 3. What is the impact of equivalence classes on the evaluation using NPKSD 4. Why is NPKSD theoretically better than KSD in synthetic distribution experiments", "Paraphrased Summary: This study proposes a novel test for evaluating black box generative models based on the kernel Stein discrepancy (KSD). However the standard KSD test cannot be directly applied because the generative models true score function is unknown. To address this the authors present a modified KSD variant that estimates the score function from samples generated by the model. The resulting hypothesis test aims to determine whether the dataset used to train the model aligns with the estimated score function. The score estimation method relies on estimating componentwise distributions conditional on others through summary statistics. The study provides theoretical guarantees for the tests consistency. Strengths and Weaknesses: Strengths: Addresses an important challenge Presents innovative score estimation techniques Thorough execution Weaknesses: Limited experimental evaluation with insufficient baselines and comparisons Lack of clarity on the added value compared to MMDbased tests Absence of empirical exploration of the score estimation approachs scalability Potential for alternative score learning methods (e.g. neural networks) Missing references to relevant prior work Questions: Is it meaningful to assess generative models using hypothesis tests What specific insights can be gained from test results How can the test be used to improve generative models", "Paraphrase: Summary: This study explores a method for evaluating implicit generative models using a modified version of a previously limited metric called Kernelised Stein Discrepancy (KSD). The new method Nonparametric Stein Operator KSD (NPKSD) removes a major limitation of KSD that prevented its use with implicit models. Experimental results show that NPKSD outperforms a benchmark method (MMD) in detecting discrepancies between generated and real data. Strengths: Novelty: NPKSD expands the capabilities of KSD to include implicit models. Clarity: The paper effectively conveys its technical content. Relevance: The evaluation of generative model quality is crucial in AI applications like privacyenhancing synthetic data generation. Promising Results: NPKSD exhibits superior performance compared to MMD. Further investigation with more advanced comparative techniques would be beneficial. Weaknesses: Limited Comparison with MMD: The discussion of NPKSDs advantages over MMD could be expanded to address specific criticisms or comparative strengths. Questions: Algorithm 1: Is NPKSD effectively learning the gradient of the loglikelihood function of the implicit model Line 232234: Does the criticism of MMD in these lines refer to its reliance on estimating function gradients Algorithm 2: Does NPKSD utilize more samples from the implicit model than MMD"], "sFapsu4hYo": ["Paraphrased Summary: This paper investigates the benefits of using sparse training for blocksparse models with fine granularity. The proposed approach uses the \u7ecf\u5178\u7684\u65b9\u6cd5 of \"pruning and regrowing\" weights but with a focus on blocks and a new growth criterion that\u4e0d\u9700\u8981 dense gradient calculations. Experiments on ImageNet and CIFAR10100 show that this blockwise sparse training method can achieve good results and significantly speed up training at moderate sparsity levels. Strengths and Weaknesses: Strengths: 1. The exploration of sparse training with hardware compatibility is valuable and potentially beneficial for the field. 2. The blockwise growth criterion is wellconceived and reduces the computational overhead of sparse training. 3. The paper is wellwritten and easy to understand. Weaknesses: 1. The claim that blockwise sparsity can match the performance of the unstructured one in most cases may not be accurate given the inferior performance of DSB compared to stateoftheart unstructured methods. 2. The advantages of DSB are primarily in terms of acceleration and computational savings on common hardware. However structured sparsity methods can also achieve these benefits so it is important to compare DSB with these methods to assess its tradeoffs. Minor: 1. SDB is not defined in the provided context. A caption should be added to Tab.2 to clarify its meaning. Questions: 1. The authors assert that DSB accelerates computation on widely available traditional hardware. However their codebase uses binary masks to represent pruned weights which is a simulated approach. It would be beneficial to provide a running demo or code that can be deployed on general hardware to demonstrate the feasibility and ease of implementation of their method.", "Summary Paraphrase: This research presents a sparsitybased training technique that utilizes grouped weight structures for improved hardware compatibility. This approach enhances training speed by optimizing the retrieval of weight indices during forward and backward propagation. This research is notable in the field of sparse training. Strengths and Weaknesses Paraphrase: Strengths: Proposes a structured sparse training method that groups sparse weights yielding competitive accuracy to dense models and up to a fivefold acceleration. Employs a criterion that eliminates dense gradient calculations. Includes a comprehensive review of related work. Weaknesses: The authors claim of pioneering group sparsity in dynamic sparse training may need to be revised due to prior research. The methodology description lacks clarity and could benefit from improvements. The distinction between results obtained using masks and the proposed method is not specified. The provided acceleration rates are not explicitly stated in terms of actual wallclock training time. The endtoend applicability of the proposed method is unclear. The speed advantage of the growth criterion over dense gradient calculations requires clarification. Questions: Provide details on experiments conducted with masks versus the proposed method. Specify whether the reported speedups in Figure 9 represent a single feedforward pass or endtoend training. Quantify the FLOP reductions achieved by the growth criterion compared to dense gradient calculations and its performance on GPUs.", "Paraphrase: This research investigates dynamic sparse training (DST) using finely tuned structured pruning. Unlike previous methods DST runs solely on randomly initialized unformatted models. It combines shuffled blocking with a blockbased growth criterion to adjust active weights during training. This paper provides experimental evidence on CIFAR and ImageNet datasets showing DST outperforms Structured Sparse Lottery Ticket (SSLT) and RigL. Strengths: Introduces DST with finely tuned structured models eliminating the need for specialized hardware or pretrained models. Proposes a novel growth algorithm that avoids dense gradient and weight calculations enabling purely sparse training. Clearly delineates DST from Transposable N M Sparsity. Written in a clear and easytounderstand manner. Weaknesses: Lacks specific details on hardware acceleration. Should report the overall training time and memory savings achieved. Should compare DSTs speedup with structured pruning approaches in Figure 9. Requires error bars in Table 2 Figure 6 and Figure 7. Questions: The work presented is solid but the absence of acceleration comparisons limits the evaluation. The authors are requested to address these weaknesses in their rebuttal to potentially improve the assessment.", "Paraphrased Statement: Summary: This paper introduces a training technique for sparse matrices using finegrained block sparsity. The technique utilizes shuffled block sparsity where matrix entries are rearranged to form blocks to enhance hardware efficiency and performance. The method involves alternating block pruning and growth at regular intervals resulting in dynamic block sparsity. The process of block pruning and growth is optimized without computing gradients for the entire dense matrix. Experiments conducted on ResNet and VGG architectures trained on CIFAR and ImageNet datasets demonstrate that at 50 sparsity the proposed method achieves comparable accuracy to fully dense models. Strengths: 1. The blockwise growth criterion is efficient eliminating the need for gradient computation for the entire dense matrix. A microexperiment could further support the accuracy of this heuristic. 2. The analysis of accuracy versus block size provides valuable insights. A block size of 16x16 has shown to maintain accuracy while also enhancing performance. Weaknesses: 1. The absence of wallclock time speedup validation and specific details regarding the benchmark configuration (e.g. channel order data type) limit the evaluation of the methods efficiency. The code implementation appears to use a simplified operation that may not fully optimize performance. 2. A comparison with existing blocksparse methods (e.g. Block Pruning for Faster Transformers) would help establish the superiority of shuffled block sparsity. 3. The explanation of shuffled block sparsity could benefit from further clarification. It should specify whether blocks are entirely zero or nonzero the significance of the shuffling step and how row and column shuffling is determined. Questions: 1. What is the overhead associated with block dropping The paper asserts that it improves accuracy but experimental evidence is lacking."], "s1yaWFDLxVG": ["Paraphrased Statement: Summary: This study explored using gradient descent to optimize functions in the intersection of the Restricted Strong Isoperimetry (RSI) and Enhanced Barrier (EB) sets. The intersection set is a broader category than strongly convex differentiable functions. The authors derived matching lower and upper bounds on the convergence rate. Strengths and Weaknesses: The study extended classical convex optimization results and established that gradient descent is optimal for functions in the RSI and EB sets. The authors believe the findings are relevant to optimization and machine learning. The presentation is clear and accessible. However they acknowledge concerns about the significance of the contribution. There are existing studies on lower and upper bounds some of which can also guarantee linear convergence of gradientbased algorithms. Questions: Major: 1. The authors question the validity of claiming linear convergence for gradient descent with SGD due to the possibility of escaping local minima. 2. They suggest comparing the optimality gap metrics used in Theorems 1 and 2 to assess the tightness of the bounds. 3. The RSI and EB intersection may satisfy the Polyak\u0141ojasiewicz (PL) condition which implies linear convergence under certain conditions. The authors are asked to compare their bounds with existing PL function results. 4. The experiments only demonstrate the superiority of gradient descent over the heavyball method for a specific function. The authors are urged to provide more details and explore if the trend holds for other RSI and EB functions. Minor: 1. Clarifying that alpha in Line 20 represents the step size. 2. Asking for an example of a nonconvex function that satisfies RSI. 3. Suggesting that the discussion in Remark 2.4 may be redundant when comparing RSI and EB to other conditions with the same parameters. 4. Revising the statement in Line 86 to accurately describe the nature of Kurdyka\u0141ojasiewicz and Polyak\u0141ojasiewicz conditions. 5. Correcting the term \"learning rates\" to \"convergence rates\" in Line 224.", "Paraphrase: Summary: This theoretical paper analyzes the performance of gradient descent for objectives that satisfy less restrictive conditions than strong convexity and smoothness (lower restricted secant inequality and upper error bound). It demonstrates that gradient descent is optimal among all firstorder algorithms for this class of functions. Strengths: Wellwritten and rigorous theoretical work. Proves optimality of gradient descent for the given class of functions. Weaknesses: Practical relevance and impact are unclear. Nonconvex objectives can lead to local minima which limit the applicability of gradient descent. The optimal step size for the given class of functions is smaller than the standard choice for strongly convex and smooth objectives. Questions: 1. Why is gradient descent optimal for the given class of functions even when it is known that momentumbased methods can accelerate for strongly convex and smooth objectives 2. Does the optimality of gradient descent for nonconvex objectives suggest that it may outperform momentumbased methods in this setting 3. Clarification on equation (9) Corollary 1 Theorem 1 and Theorem 2. 4. Typographical error in reference [17]: \"nesterov\" should be \"Nesterov\".", "Paraphrased Summary This study examines the complexity of gradient descent (GD) under Residual Smoothness and Strictly Convex (RSI and EB) conditions which extend the commonly studied smooth and strongly convex settings. The paper demonstrates the linear convergence of GD in these settings establishes interpolation conditions and shows the optimality of GD under these conditions using the PEP framework. Strengths Concise analysis providing both upper and lower bounds Wellorganized and accessible presentation Interesting finding regarding the independence of interpolation conditions Weaknesses Assumptions could be further relaxed Questions 1. Is the assumption of a convex optimal set restrictive for neural networks given that their landscapes may be nonconvex 2. How do the interpolation conditions for RSIEB settings differ from those for strongly convex and nonconvex smooth settings and why 3. Could the analysis be further compared to existing literature Minor Points Define the HB algorithm formally in Section 5.2. Correct the typo in Corllary 1 (change \"\u2227\" to \"\u2229\"). Clarify the distinction between extrapolation and interpolation in Appendix A.", "Paraphrase: This paper builds on prior research exploring alternative assumptions to smoothness and strong convexity that still allow for linear convergence of gradient descent. It analyzes firstorder methods under two such conditions: the restricted secant inequality as a lower bound and smoothness towards the optimum as an upper error bound. The results reveal that gradient descents worstcase convergence on functions satisfying these assumptions is optimal as a matching lower bound for firstorder methods is provided. The construction of the lower bound differs from traditional approaches used in smooth convex optimization. It exploits the fact that the given conditions are weaker than smoothness and strong convexity allowing for the introduction of hidden local deviations. Strengths: Novel results Interesting proof technique Clear presentation of technical details Weaknesses: Motivation is not adequately articulated The choice of the specific conditions studied is not justified Questions: Why were these particular conditions chosen for analysis among the numerous possibilities Have other conditions that imply the ones examined already been investigated Do the results suggest that the class of functions considered is too broad for practical application or contains unrealistic characteristics Can specific examples be provided to illustrate the applicability gap between the studied conditions and standard smoothness and strong convexity While the paper acknowledges the limitations of the conditions in terms of applicability the discussion could be strengthened by providing concrete examples of where they are beneficial."], "unb1wyXf-aC": ["Paraphrased Statement: Summary: This study presents a novel superresolution technique for 3D medical images and segmentation masks enhancing brain diagnostics. The method an extension of the 2D Deep Backprojection Network (DBPN) doubles the resolution (from 2mm to 1mm). Evaluation: Compared to basic upsampling methods the proposed method demonstrates improved performance as evidenced by higher Dice scores and improved visual quality. Strengths and Weaknesses: Strengths: Detailed technical descriptions and a clear explanation of the problem method and experiments. Visually appealing comparisons to illustrate method performance. Innovative approach of upsampling both the image and segmentation mask. Weaknesses: Lack of novelty as it directly builds upon the 2D DBPN network. Minimal performance differences between various loss function combinations used for model optimization. Limited evaluation in comparison to existing 3D superresolution methods. Questions: Is this the first 3D superresolution model for medical image analysis If not how does it compare to existing methods", "Paraphrase: The authors have expanded a previously developed 2D image enhancement technique (superresolution) to 3D neuroimaging data. In addition they have modified the network to generate enhanced segmentation maps alongside the superresolved images. These segmentation maps have been assessed against both ground truth data and their ability to detect frontotemporal disorders. The researchers have also evaluated the impact of various loss functions used during training on the predictive accuracy of the models. While the superresolution techniques improved conventional evaluation metrics (PSNR and SSIM) compared to lowerresolution data these metrics did not significantly improve when compared to models trained with different loss functions. Superresolution models enhanced predictive power for frontotemporal disorders in many regions but some areas showed reduced predictive power after superresolution. Overall the paper is wellwritten and provides a comprehensive overview of the research. The authors have thoroughly explored the topic and present a detailed analysis of the downstream effects of segmentation superresolution in a disease model. They have also acknowledged previous research and highlighted their own contributions to the field. The primary limitation of the study is the limited impact of some of the claimed innovations. Expanding a model from 2D to 3D while relevant for neuroimaging is not a particularly groundbreaking advancement. Similarly the inclusion of segmentation maps as an additional output channel while convenient and potentially computationally efficient does not significantly enhance the impact of the work since the segmentation maps could be recalculated using the enhanced highresolution dataset. Nonetheless the strengths of the study including its clarity comprehensiveness and analysis of downstream effects outweigh its weaknesses.", "Paraphrase: This study created a superresolution method for 3D brain scans. They tested how well it could spot brain changes caused by neurodegenerative diseases. The method was trained on 3D brain data to improve both the original image and the separate labels. It was based on the 2D Deep Back Projection Network (DBPN) which was updated to work in 3D and with multiple outputs. The method was tested in practical clinical tasks such as finding brain shrinkage in a group of related but different disorders that are known to show different patterns of atrophy. Strengths and Weaknesses: Strengths: Evaluated superresolution in clinically relevant tasks like measuring brain changes in different disorders. Weaknesses: Limited methodological innovation. Unclear organization and some sections were difficult to follow. More suitable for a medical imaging conference or journal.", "Paraphrase: This paper introduces a new approach for simultaneous image enhancement and label segmentation of 3D MRI data improving both image quality and accuracy of segmentation. The method modifies existing image enhancement techniques to work with 3D data and allows for indirect evaluation by tracking changes in brain structures of individuals with neurodegenerative diseases. The authors compare different loss functions for optimizing the model and demonstrate its effectiveness in identifying diseases in multiple datasets. Strengths: The problem of concurrent image enhancement and segmentation is medically relevant. The proposed method builds on existing image enhancement techniques making it technically feasible. Weaknesses: The evaluation process is poorly explained making it difficult to fully understand the results. Key details such as training testing and validation data splits are not clearly stated. The differences in results using different loss functions are minimal and may not be consistent across different data subsets. The baseline comparisons do not include the latest approaches proposed by other researchers. Questions: How did the authors determine the specific weight ratios for the different loss functions Does \"training for 2 epochs or until convergence\" mean that the maximum possible training time is 2 epochs", "This study introduces a novel approach \"concurrent super resolution and segmentation\" (CSRS) to enhance the clarity of 3D neuroimaging and refine image segmentation by employing \"perception super resolution.\" They evaluate CSRS using clinical datasets from publicly available sources such as the Human Connectome Project and others. The methods efficacy is tested for quantifying changes in MRI scans across neurodegenerative diseases. However the results only include comparisons between the new approach and the original resolution model using ttests (bootstrapped) without accounting for multiple testing. The implications of these findings are briefly discussed. Strengths: The technique proposed is intriguing. Weaknesses: Insufficient details regarding MRI data preprocessing hindering replication efforts. Methods not clearly presented. Absence of a clinical or neuroscience perspective in the experimental design. Brief and superficial discussion. Poor organization and lack of coherence in the supplementary materials. Despite these limitations the technique holds promise and with clearer communication it could become a significant advancement in neuroimaging.", "Paraphrase: This research proposes a novel method for improving the quality of 3D brain image segmentation a process that helps analyze brain scans. The method involves a carefully designed technique for upsampling lowresolution images to enhance their quality and a new loss function (a mathematical equation) that helps optimize the performance of the method. Since highresolution ground truth data (accurate representations of the brain) is not readily available the method is evaluated using a metric that is clinically relevant. The research team demonstrates that the proposed method significantly improves the detection of brain atrophy (shrinkage) in five different neurodegenerative diseases both over time (longitudinally) and across individuals (crosssectionally). Strengths and Weaknesses: Strengths: Addresses a significant challenge in medical imaging. Demonstrates expertise and understanding of the field. Provides a thorough explanation of the method and its justification. Utilizes rigorous experiments to evaluate performance including confidence intervals. Weaknesses: The originality of the method could be strengthened by further justifying its uniqueness. The neural network and loss functions used are not entirely novel in medical imaging. While the evaluation paradigm is novel a deeper discussion of why it is new would be beneficial. The research claims an impact on population studies of neurodegenerative diseases which could be expanded upon. Questions: Could the authors explain which aspects of the loss functions are new Can the authors provide a more detailed discussion on the novelty of the evaluation paradigm Can the authors elaborate on the stated populationlevel effect of the proposed method"], "qVtbqSwOxy6": ["Summary: This paper explores the Anchor Unaligned problem (AUC) in multiview clustering using anchorbased methods. It proposes an alignment technique between multiview anchor sets to address this issue. The paper presents promising results demonstrating the effectiveness of anchor alignment. Strengths: Novel approach with clear illustration of the AUC issue. Proposed flexible framework (FMVACC) for addressing the AUC problem. Innovative strategy for matching unmeasurable multidimensional anchors. Impressive experimental results showing the benefits of anchor alignment. Opensource code available for reproducibility. Weaknesses: Lack of complexity analysis for both the proposed method and existing baselines. Need for careful proofreading and typo corrections. Insufficient explanation of obtained permutation matrices in Figure 3. Related work could benefit from a broader survey of multiview clustering models. Questions: 1. Comparison and possible application of the proposed method to the PVC settings. 2. Clarification of definitions and constraints in Eq. (3). 3. Consideration of space and time complexity in comparison with existing baselines. 4. Further discussion on the obtained permutation matrices in Figure 3. 5. Inclusion of a wider range of multiview clustering approaches in the related work section.", "Paraphrased Summary: This research explores multiview anchor graph fusion approaches where anchor graphs in different views are not inherently aligned. The study emphasizes this issue and presents a framework for aligning these graphs. The clear and accessible writing makes for an engaging read. The papers contributions include highlighting the significance of multiview anchor alignment and showcasing the effectiveness of the proposed FMVACC method through experimental results. FMVACC not only enhances performance but also offers increased flexibility when combined with existing largescale baselines. Strengths: 1. The paper is wellwritten and easy to follow. 2. It identifies the previously overlooked problem of multiview anchor alignment opening avenues for further research. 3. The authors introduce a novel generalized flexible multiview anchor graph framework enhancing versatility and performance compared to fixed anchor sampling. 4. The alignment module proves effective on both simulated and realworld data with notable performance improvements observed when added to existing baselines. 5. The method demonstrates scalability for largescale scenarios supported by theoretical analysis and experimental results. Weaknesses: 1. While focusing on multiview graph methods the literature review should include other multiview clustering approaches such as multiview NMF and ensemble clustering. 2. The authors do not provide details on initializing singleview anchors which is a crucial factor. 3. Minor formatting inconsistencies such as inconsistency in the use of AUP should be addressed. Questions: 1. Table 2 includes RMKM a nongraph method. This should be addressed in the related work section. 2. Elaboration on initializing singleview anchors is needed. 3. Formatting inconsistencies such as the use of AUP and \\mathcal in complexity analysis require attention. 4. The introduction should include a discussion of other multiview clustering strategies. 5. It would be beneficial to compare the performance of the proposed method to recent multiview scalable graph clustering methods such as the extension of LMVSC and NMF integration into multiview anchor graph study.", "Paraphrased Summary: This research explores multiview graph clustering particularly addressing the issue of unaligned anchors. To address this challenge a novel anchor graph fusion framework is introduced incorporating an anchor alignment module. This framework aims to resolve the anchor unaligned problem (AUP). Evaluation on various datasets demonstrates improved performance and effectiveness of the proposed method. Strengths: Strong and practical motivation Innovative method for obtaining permutation matrix proven effective both theoretically and empirically Pioneering research on AUP in multiview graph clustering Weaknesses: Figure 2 requires improvement for clarity to convey the frameworks concept better Lack of detailed explanation on experimental settings including parameter tuning and assessment of AUP in realworld datasets Visualization of anchor graphs on UCI digits shows minimal differences between aligned and unaligned cases reducing the evidence of effectiveness Questions: 1. Explain the distinction between Equations 8 and 9 providing a detailed mathematical proof. 2. In training the permutation matrix is computed using Equation 9 which is a relaxed form. How is the binaryvalued permutation matrix obtained during inference"], "yKDKNzjHg8N": ["Paraphrased Statement: This research presents new metrics First Split Learning Time (FSLT) and Second Split Forgetting Time (SSFT) which assess datapoints in a dataset based on their rate of acquisition during training and their susceptibility to being forgotten when retrained on a separate portion of the training data. These metrics are analyzed in relation to mislabeled infrequent and intricate samples across various datasets including synthetic images and sentiment classification demonstrating their ability to differentiate these types of samples effectively. Additionally the paper offers theoretical justifications supporting the experimental findings. Strengths: Simplicity and accessibility of the introduced metrics due to their familiarity to those familiar with deep learning. Proven effectiveness through extensive experiments. Contributes to understanding model and dataset behavior and guiding model training improvements. Wellwritten paper with clear organization and minimal grammatical errors. Weaknesses: Potential limitations in overparameterized deep models due to effects mentioned in the paper. Absence of experiments with larger models to demonstrate discrepancy with smaller models. Minor Comments: Remove space before colon on line 109. Add a condition i \u2260 j to the set \\mathcalIi \\cup \\mathcalIj \\phi on line 164. Consider increasing font size in figures for better readability. Question: Regarding rare examples is using FSLT to identify them necessary given the simplicity of counting datapoints per class", "Paraphrase: Summary Researchers present a new method for assessing the difficulty of training examples. Previous approaches have relied on metrics like the number of exposures needed for correct classification or label changes during training. The proposed method SecondSplit Forgetting Time (SSFT) measures the time it takes to \"forget\" a training example after the model has been finetuned on a different data subset. The authors claim that SSFT can differentiate between examples that are difficult due to noisy labels and those that stem from a rare subpopulation. They also explore other SSFT applications and argue that it is robust to variations in model architecture optimizer and other factors. Strengths and Weaknesses Strengths: 1. Novel method that provides insights into datasets and aids in identifying and rectifying noisy examples. 2. Wellexecuted paper with a clear presentation. 3. Precise mathematical definitions for concepts like \"rare\" \"noisy\" and \"complex\" examples although specific to the analytical model studied. Weaknesses: 1. Limited experimental scope a wider range of datasets should be tested particularly for identifying label noise. 2. Lack of comparison between generalization improvement after removing noisy examples using SSFT and training on the complete dataset. 3. Sensitivity to learning rate acknowledged but not empirically demonstrated a heatmap showing Pearson correlations for different learning rates would be useful. 4. Unclear or imprecise mathematical notation in some instances. 5. Use of a shallow architecture (ResNet9) despite the importance of testing deeper models. Minor Concerns: Authors should use \"number of seen examples\" instead of \"number of epochs\" for better accuracy. Clarification needed on whether models are trained on the complete dataset or one subset only when comparing generalization improvement after removing noisy examples. Questions: 1. How is the comparison made between generalization improvement after removing noisy examples using SSFT and training on the complete dataset 2. Provide a figure showing the robustness of SSFT to learning rate changes. 3. Define \u03c1(t) in Equation 5. 4. Why is ResNet9 used instead of deeper architectures like ResNet18 5. Explain the rationale behind defining \"complex examples\" as those with a high signaltonoise ratio.", "Paraphrase: Summary: The paper presents a novel method for analyzing how examples are learned and forgotten in deep neural networks used in various domains. It successfully identifies challenging examples such as rare classes and misclassified ones. Strengths and Weaknesses: The paper is wellregarded with implications for communities using hard examples noisy labels and rare classes. It stands out for its: Strengths: Simple yet effective approach Wellwritten content Extensive experiments across tasks and domains Rigorous ablations to demonstrate the methods efficacy Clear results Useful sections highlighting the methods practical benefits and limitations Weaknesses: Requires a separate holdout set for training to analyze dynamics Retraining can be computationally expensive and timeconsuming Synthetic dataset description in Section 4.2 could be improved for clarity Originality and Quality: Novel approach to classifying examples Theoretically sound with strong intuitions and results Clarity and Significance: Wellwritten and organized Includes thorough related work ablations and method explanations Builds upon previous research (e.g. Toneva et al.) and has implications for areas like curriculum learning Questions: It seems logical that neurons activated for rare classes would not be overwritten frequently. Could there be more nuanced factors at play It would be beneficial to test the method with synthetic examples including misclassified examples for rare classes.", "Paraphrased Statement: This study examines how training time and the difficulty of examples relate to each other. It introduces a new metric SecondSplit Forgetting Time (SSFT) to enhance existing metrics. It investigates three types of difficult examples: mislabeled uncommon and intricate. The paper supports the claim that SSFT and FirstSplit Forgetting Time (FSLT) can effectively detect noisy labels enhance generalization on noisy data pinpoint areas for improvement and tolerate random variations. Additionally it explores the theoretical underpinnings of SSFT using a basic example. Strengths: Proposes a fresh measurement to determine example difficulty. Analyzes three categories of difficult examples demonstrating the effectiveness of the new metric through detailed experiments. Presents a wellwritten paper. Weaknesses: Theoretical findings are derived from a simple environment. Questions: This paper introduces a novel and intriguing metric and its empirical evaluation is thorough. Overall it is a wellexecuted study."], "uLYc4L3C81A": ["Paraphrase: Summary Research Problem: The study examines how to make transformerbased language models infer faster by using earlyexit strategies during inference. These strategies end the models processing before the last layer based on whether the output meets a confidence threshold. The proposed method CALM aims to exit the processing as early as possible while still ensuring the models performance is consistent with the full model. Analysis and Proposed Method: The researchers analyze error sources during early exits and suggest a decreasing confidence threshold strategy. This allows for earlier exits as the model progresses through the sequence. They use statistical testing techniques to determine confidence thresholds that guarantee performance bounds. Theoretical Guarantees: The authors prove that their method meets performance bounds using statistical hypothesis testing principles. Empirical Evaluation: The proposed method shows potential speedups of up to 3x although there are some limitations. Strengths: Provable performance guarantees Distributionfree analysis Considers both local and global performance Insights into transformer behavior No labeled data required Comparison of confidence measures Weaknesses: Lack of comparison to baselines Computational cost of some confidence measures Potential limited practical benefit of performance guarantees Some counterintuitive results on layer importance Questions: Clarification on the meaning of the result on line 166 (performance without early exits) Do earlyexit predictions ever match fullmodel predictions at layer i but disagree later Clarification on the statement that perfect local consistency implies global consistency", "The authors have addressed my concerns leading me to raise my rating to 8. While T5like transformer language models are crucial in NLP previous methods proposed for early exit of tokens with straightforward outputs have not been widely adopted in practice. This paper analyzes such models with an oracle to determine potential speedups and proposes new early exit approaches for T5 evaluated on tasks like summarization and translation. However the paper lacks clarity in comparing its performance to previous works and does not present the full model baseline or results from Elbayad et al and Schwartz et al in its main results table. Additionally the discussion of potential issues with these methods and their applicability in batched inference scenarios is missing. Questions raised include how Table 2 would appear with the inclusion of comparisons to the full model and previous research and what the actual wall clock speedups of the proposed methods are.", "Paraphrased Summary: The Confident Adaptive Language Modeling (CALM) framework addresses errors in early exiting a strategy used in Transformer language models to optimize decoding speed. CALM calibrates local and global predictions to enhance accuracy. Empirical results in text summarization machine translation and Squad demonstrate the frameworks effectiveness. Paraphrased Strengths: Rigorous and logical analysis of error sources in early exiting. Adopts statistical testing methodologies for validation. Clear and accurate mathematical notation. Detailed insights on error propagation and local errors. Paraphrased Weaknesses: The complexity of the proposed technical solution in section 4 is questionable. Paraphrased Questions: 1. Will additional softmax operators for each layer (Equation 6) create a significant computational bottleneck An ablation study would be helpful to assess the benefits. 2. While the LTT framework (Equation 5) provides technical robustness could a simpler hyperparameter selection method achieve similar results and improve generalization in practical applications", "Paraphrase: Summary: This research introduces Confident Adaptive Language Modeling (CALM) which allows for dynamic early exiting during the decoding process. The study provides theoretical proof that local confidence estimates can result in confident global output generation. Strengths and Weaknesses: Strengths: CALM is practical and speeds up inference particularly for expensive autoregressive decoding with large language models. It is founded on the learnthentest framework. Weaknesses: The oracle early exiting uses surprisingly few layers (less than 2 on average) which raises questions about its feasibility. There is still a noticeable gap between the oracle and the proposed methods. Assumptions could be relaxed for improved efficiency. Questions: Why does the oracle early exiting use such a low number of layers What approaches could be taken to minimize the gap between the oracle and proposed methods Which assumptions could be relaxed for better efficiency Will the patterns observed in Figure 2 hold true for other tasks Why does the softmaxbased measure lead to the highest speedup among the tested measures"], "lCGYC7pXWNQ": ["Paraphrase: Summary This study outlines a method for incremental video learning using frame condensation and instancebased prompts. Compressing videos into single frames enhances memory efficiency but it also eliminates the temporal component. Instancespecific prompting helps to preserve some of this lost information. The technique excels in experiments on HMDB51 UCF101 and SomethingSomething V2 compared to existing approaches. The writing is intelligible. Strengths The method is introduced logically with clear justifications and rationale. The proposal is straightforward yet effective for incremental learning. Experiments demonstrate consistent improvements over competitors across three benchmarks. The writing is clear and easy to follow. Weaknesses Experiments involve only the TSM backbone architecture. The method is specific to incremental video learning its effectiveness in other domains remains unclear. A related question is efficient video classification using condensed frame instancespecific prompting. Questions Would the method improve general video classification efficiency (memory and compute) not just accuracy Demonstrating superiority to TSM would enhance the proposals credibility. Are the parameters \u0398 in Equations (3) and (6) shared Since Equation (3) applies to condensed frames only while Equation (6) encompasses condensed frames and prompts (which counterbalance motion) it is worth exploring. Will the method perform well with backbones other than TSM", "Paraphrased Statement: Summary This study introduces FrameMaker a method that utilizes a condensed frame generated from linearly combined video frames. It employs instancespecific prompts to account for the lost spatiotemporal details in the condensed frame. Strengths Reduces redundancy in raw video frames for action recognition. Clearly written and easy to comprehend. Weaknesses Optimization of the condensed frame is challenging due to lost spatiotemporal details (Fig. 2 Lfc). Memory comparison is biased because FrameMaker stores additional prompts. A detailed analysis of prompt storage space is needed. The evaluated CIL baselines are weak stronger methods (e.g. AANets DER) should be used for comparison. Additional Comments Line 98: \"Temporal Shift Module\" should be corrected to \"Temporal Shift Module.\"", "Paraphrased Statement: Summary: This paper presents FrameMaker a new technique for distilling video datasets into a form suitable for incremental learning tasks. FrameMaker consists of two parts: Frame Condensation which computes an optimized weighted average of the exemplars in a video and InstanceSpecific Prompt which generates a pixellevel bias that helps the resulting frame train a classifier effectively for a specific class. Strengths: FrameMaker surpasses all previous methods on three different datasets as shown in Tables 1 and 2. The figures throughout the paper clearly illustrate the method and its advantages. Extensive experiments highlight the importance of each component of the method. Weaknesses: The paper contains several grammatical errors that could be corrected using a grammarchecking tool. Visualizations of the sum of the condensed frame and the prompt would be helpful to demonstrate how the prompt modifies the image. The formatting of quotations varies throughout the paper it is recommended to use consistent curly quotation marks (``` ` and ` \u2018 `) for both opening and closing quotes. Questions: Can the authors provide visualizations of the sum of the condensed frames and prompts to show how they differ from the condensed frames It would be interesting to explore the connection between FrameMaker and recent methods in \"Dataset Distillation\" within incremental learning particularly the similarities between learning the prompt and feature regression approaches. An ablation study where only the prompt is learned could be valuable to understand its role in the method.", "Summary The paper introduces FrameMaker a system that reduces the memory footprint of incremental video learning models. It does this by condensing video examples from past tasks into a single frame. The authors propose a technique to compute a weighted average over uniformly sampled frames to create the average frame. This average frame is then combined with a learnable prompt and used as input to the classifier. Experiments show that FrameMaker improves accuracy and reduces memory usage compared to existing methods. Strengths and Weaknesses Strengths: The proposed FrameMaker method significantly reduces memory footprint per class. It requires a video model that can operate on singleframe inputs making it applicable to existing methods like TSM. Extensive ablation studies provide insights into video incremental learning challenges. Weaknesses: The paper does not discuss related work in video summarization and keyframe detection. The proposed method may not be suitable for tasks differentiated by frame ordering (e.g. moving objects left to right). The instancelevel prompts have the same dimension as the input image which may be unnecessary. The experiments do not include certain baselines such as using an unweighted average of frames or taskspecific prompts. Additional evaluation metrics beyond visualizations would enhance the analysis of frame condensation quality. Questions and Concerns: How does FrameMaker compare to video summarization and keyframe selection techniques How effective is FrameMaker for temporally challenging tasks and 3D video backbones What alternative prompting strategies have been explored Why are certain baselines missing and what additional evaluation metrics could be used"], "zXE8iFOZKw": ["Paraphrase: This research introduces a novel offlinetoonline reinforcement learning algorithm for handling domain shifts between offline datasets and online executable environments. This approach is particularly relevant for realworld applications as it addresses the gap between limited realworld data and readily available simulators. To overcome the significant dynamics mismatch between the offline dataset and the online simulator the proposed method H2O incorporates two regularizations: 1. Adaptive Value Regularization: Penalizes transitions with large dynamics gaps. 2. DynamicsCorrected Bellman Error: Corrects the Bellman error based on dynamics differences. These regularizations result in penalty rewards that discourage transitions with high dynamics gaps effectively mitigating the effects of the dynamics mismatch. Strengths and Weaknesses: Strengths: Novelty: Addresses the unexplored problem of offlinetoonline reinforcement learning for domain shifts. WellStructured Paper: Easy to understand and follow. ProofofConcept: Demonstrates successful transfer from a normal simulator to an unreal dynamics simulator for the MuJoCo HalfCheetah and a simtoreal transfer with a wheeledlegged robot. Weaknesses: Limited Experimental Coverage: H2O is only tested on HalfCheetah. Heuristic Regularization Parameters: The choice of dynamics gap weights and regularization coefficients is not fully justified. Confusion in Experimental Settings: The paper uses a different setting than previous work raising questions about the validity of comparisons. Unnecessary Density Ratio Estimation: H2O directly models transition dynamics from offline datasets making the use of density ratio estimation via discriminators questionable.", "Paraphrase: Summary: This research introduces a method for training using a combination of offline simulation data (possibly inaccurate) and realworld data (accurate dynamics). The technique is an extension of the CQL offline reinforcement learning (RL) algorithm incorporating features for integrating additional offline data. This includes adaptive value regularization based on estimated dynamics gaps from learned classifiers and reweighting the Bellman error based on varying dynamics. The method is tested in simulated MuJoCo environments with modified dynamics and a realworld experiment involving a wheeled robots balancing task. Strengths: Addresses the challenge of learning from real offline data and potentially inaccurate simulators. The H2O objective formulation provides a clear interpretation of confidence levels in stateaction pairs based on data availability or dynamics gaps. The analysis in Section 5 is wellgrounded. Strong experimental results especially in the realworld wheeled robot experiment demonstrate the effectiveness of this method in practical scenarios. Weaknesses: The ablation study findings are unclear. Performance improvements in simulation experiments seem to result primarily from Bellman updates reweighting rather than value regularization. The claim that removing the value regularization leads to significant degradation is disputable. The writing could be more precise and clear. Clarity Enhancements: Improve clarity in Section 6.1 by clarifying that original MuJoCo environments are the \"real\" environments and the offline dataset is realworld data. Explain how the Section 4 objective is used in the main algorithm. Provide an earlier definition of \\omega as a characterization of dynamics gaps for samples. Questions: Why is the performance of H2Oa (with a fixed uniform \\omega) so similar to H2O Shouldnt it eliminate the algorithms ability to detect offdynamics transitions Does the dynamics ratio \"dr\" depend on the learned \\omega If so why does H2Oadr perform differently from H2O NitpicksTypos: L128: Notation for behavior policy introduced but not used in Equation 3 L141: \"sateaction\" should be \"stateaction\" L165: Maximization term notation differs from Equation 4 L173: \"closeform\" should be \"closedform\" Figure 2: \"epoches\" should be \"epochs\"", "Paraphrase: Summary: This study assumes that there is a flawed simulator for online reinforcement learning (RL) and a fixed dataset for offline RL. It introduces a novel policy evaluation objective that penalizes Q values where the difference between simulation and realworld outcomes is significant. The discrepancy is assessed using two discriminators trained using DARC. Experiments in both simulated and real environments demonstrate that this algorithm improves the utilization of imperfect simulations and limited accurate offline data. Strengths: Clear and wellstructured presentation. Validation in real and simulated environments supporting the claim of reducing the simulationtoreality gap. Originality of the proposed method. Weaknesses: Some sections could benefit from further clarification. Questions: Distribution of human performance on the collected realworld robot dataset. Exact and proportional forms of the closedform solution for l173. Ratio of training data from source and target domains used in DARClike training. Potential exploration of environments with uneven simreal gaps. Typos: \"incorporating\" should be \"incorporate\" (l55) \"only restricted\" should be \"are only restricted\" (l56) \"which does not impacted\" should be \"which is not impacted\" (l296)"], "qmy23tNBvbh": ["Paraphrase: The authors present conditions for the existence of the normalizing constant in normalizing a kernel exponential family (Proposition 5.1). They then define \"deformed\" kernel exponential families (Section 5.2) and establish a similar normalization theory (Corollary 5.2). They show that deformed kernel exponential families can represent densities with finite support (Appendix D.4). These results are applied to continuous kernel attention which is formulated as an expectation over a density (Equation 2). The authors use kernel (and deformed kernel) exponential families to represent this density. Experiments demonstrate improved performance compared to other methods. Strengths: Proper treatment of normalizing constants. Introduction of new theory with potential independent applications. Thoughtful and principled approach to generalizing attention. Weaknesses: Equation (1) lacks context or reference. Potential confusion over the use of finite spans in the RKHS representation (line 235). Algorithm 1 assumes knowledge of representer coefficients which may require additional justification. Questions: Motivation for studying time warping could be clarified. Can kernel and deformed kernel exponential families be used without normalizing constants and when is this justified How does the authors notion of attention relate to deep learning selfattention", "Paraphrased Statement: Summary: This paper presents two new multimodal attention densities based on kernel exponential families and kernel deformed exponential families for use in continuous attention mechanisms. The authors have also analyzed the normalization approximation capabilities and properties of these densities. Experiments have shown that the proposed multimodal attention densities outperform unimodal continuous attention in many cases. Strengths: Introduces the concept of kernel deformed exponential families a sparse multimodal density class. The paper is wellpresented overall. Weaknesses: The motivation for using multimodal attention density is not clearly articulated. The work is largely incremental applying kernel methods to existing concepts like exponential families and continuous attention. The experiments are limited using only two metrics and a small number of datasets. Questions: What is the purpose of the \"Time warping\" section The proof of Lemma 5.3 in the Appendix assumes knowledge of \u03b1exponential which may not be clear to all readers. The Related work section contains an extra word: \"They only used unimodal (deformed) exponential family densities.\" The dimensions of the matrix in Equation 6 may be incorrect. The experimental results in Tables 2 and 3 lack standard deviation data. Reporting additional time and space complexity including wallclock time would be useful.", "Paraphrased Statement: This study extends the continuous attention model proposed by Martins et al. (2020) by introducing a kernel exponential family density instead of a linear parameterization of the score function. The authors propose kernel deformed exponential families similar to the entmax formulation of Martins et al. (2022) but with a unique form for the score function. Despite computational challenges in using numerical integration for the forward pass the authors demonstrate its efficiency for kernel exponential attention. They validate the new formulation in three experiments: multimodal continuous attention Time Warping ECG heartbeat classification Automotive symptom detection and text classification. While the paper comprehensively explores theoretical aspects including Proposition 5.1 and Corollary 5.2 its density may overwhelm readers. The authors could improve clarity by citing previous works for certain proofs and considering moving the text classification experiment to the main text. Questions: Have you encountered convergence issues with numerical integration What hyperparameters can be adjusted for better optimization How does the computation time of continuous kernel sparsemax compare to continuous sparsemax", "Paraphrased Summary: This paper generalizes previously published results on continuous attention mechanisms to a class of probability density functions called kernel deformed exponential families. The authors investigate three key aspects of these density families: approximation of normalization assessment of context functions and applicability to realworld data. Strengths and Weaknesses: Strengths: Extends previous findings on continuous attention mechanisms. Demonstrates practical applications of the proposed methods. Weaknesses: Excessive use of undefined notation and terminology. Limited theoretical novelty given existing research. Practical performance may be compromised due to lack of reliable theoretical analysis of numerical integration. Questions: 1. Specify the domain and range of the function V. 2. Clarify the randomness associated with the variable T. 3. Provide a rationale for using the notation \u2133\u2081. 4. Define a proper functional. 5. Define the L2 inner product over measure Q used in Equation (2). 6. Define negative differential entropy. 7. Explain the derivation of Equation (3) and define A(f). 8. Define the norm F in Equation (6). 9. Clarify the statement about individualized time scales. 10. Explain time warping functions more intuitively and discuss the conditions for hi. 11. Introduce U before using it in Equation (7). 12. Include the main theorem from Section 5.2.2 in the main text. 13. Improve the clarity of Section 5.3. 14. Correct the typo in line 316 to \"to use\". 15. Correct the typo in line 77 to \"improved\". 16. Clarify and combine the separate sections on contributions."], "p3w4l4nf_Rr": ["Paraphrased Statement: Summary: This research explores the challenge of learning in repeated congestion games. It introduces novel efficient algorithms with excellent regret guarantees for these scenarios: Centralized games with both bandit and semibandit feedback Decentralized games with both bandit and semibandit feedback Centralized games for Markov congestion games with both bandit and semibandit feedback Strengths and Weaknesses: Strengths: The study provides significant contributions delivering efficient algorithms for various types of online congestion games. The employed techniques are innovative and straightforward. The claimed results appear valid. Weaknesses: 1. The notion of \"sample complexity\" (as referred to by the authors) is somewhat vague. It is suggested that it may actually refer to \"computational complexity.\" However this concept is not adequately discussed after the introduction. This aspect is crucial as highlighted in the results table on page 2. The authors should elaborate on the rationale behind these complexity bounds. 2. If the interpretation of \"sample complexity\" as computational complexity is correct simulations would be necessary to demonstrate the practical efficiency of these algorithms. Otherwise it implies that the purportedly efficient algorithms may not be so in practice. Questions: The main queries concern the claims regarding \"sample complexity\" as outlined in the \"Weaknesses\" section. Minor Comments: 1. In Definition 3 it is suggested that the dependency of the Nash regret on \u03c0 should be specified as it influences both the accumulated reward (second term) and the baseline reward (first term). 2. It is unclear how to efficiently compute the solution of Equation 5. 3. In Theorems 1 and 2 the dependency of regret on \u03b4 is not stated.", "Paraphrase: Summary: This work investigates the problem of determining Nash equilibria in congestion games and Markov congestion games which represent a straightforward extension of congestion games to the Markov setting. Novel algorithms are introduced for centralized and decentralized settings in congestion games with sample complexities independent of the number of player actions (potentially exponential in game size). These algorithms are effective for both partial (cost of chosen facilities) and full (total cost of all selected facilities) feedback. The centralized algorithms are adapted to independent Markov congestion games yielding sample complexities unaffected by the action count. Strengths and Weaknesses: Originality: Introduces the first algorithms for finding Nash equilibria with bandit feedback in congestion games with complexity guarantees independent of action count. Utilizes standard techniques (optimism in face of uncertainty FrankWolfe method Goptimal design specific reward structure) for algorithm development. Quality: Technically sound and wellexplained. Clarity: Writing is generally clear but contains some errors. Presentation of results could be improved by prioritizing main technical findings over introductions and notation. Significance: Improves sample complexity for learning Nash equilibria in congestion games with bandit feedback. Decentralized algorithms do not extend to independent Markov congestion games which may limit their practicality. Results are relevant to a niche audience in algorithmic game theory and online learning. Question: 1. Regret Minimization: Current techniques cannot be applied to minimize individual player regret because they are designed to optimize the overall Nash regret.", "Paraphrase: This research explores congestion games which find practical applications in various fields. The authors introduce innovative algorithms to learn these games using semibandit or bandit feedback including both centralized and decentralized approaches. They define a new \"Markov congestion game\" and develop a centralized algorithm for learning its dynamics. Crucially they establish theoretical upper bounds on learning regret and sample complexity that do not depend on the size of the action space overcoming the \"curse of dimensionality.\" Strengths and Weaknesses: Strengths: Explores learning congestion games with (semi)bandit feedback an area with limited previous research. Proposes distinct algorithms from existing UCB methods to address this problem. Provides novel theoretical results independent of action set size avoiding computational complexity issues. Wellwritten and clearly structured. Weaknesses: The paper raises several questions: Can a UCBtype algorithm be used for the decentralized case If not what challenges arise and why is the FWGoptimal design approach advantageous The sample complexity for algorithms using FW remains dependent on parameters m and F. Is it possible to reduce this dependence"], "ytnwPTrpl38": ["Paraphrased Statement: The paper combines GoalConditioned Reinforcement Learning (GCRL) with Causal Graphs (CGs) treating the CG as an unknown variable. It proposes using interventional data to estimate the likelihood of possible CGs and then using the most probable CG to learn a factorized actionconditioned model and agents policy. After reviewing the rebuttal the strengths and weaknesses of the paper remain as follows: Strengths: Clear and wellwritten. Experiments on planning using transition models are a promising step. Experimental setup tests generalization and robustness. Weaknesses: Individual elements of the approach are not novel but rather a combination of known techniques. The authors could consider using gradientbased methods for learning causal graphs instead of independence testing. The paper lacks evaluation on RL benchmarks specifically designed to assess causal reasoning. An \"attention\" or GNNbased baseline for predicting future states could provide a valuable comparison.", "Paraphrased Statement: This Summary Paper addresses the challenge in modelbased reinforcement learning (RL) and planning. The proposed method decomposes the transition model into a causal graph and parametrized functions. The causal graph is represented as a latent variable learned through Variational Inference (VI). The transition loss is then used to optimize the function parameters. The paper demonstrates promising results on synthetic domains particularly in a causalitybased domain. The strengths of the paper include its clarity wellwritten nature and the provision of code for understanding. The authors also present a mathematical formulation that ensures identifiability and performance guarantees. However the paper claims to focus on \"goalbased RL\" but it seems that this is not always necessary. The authors should clarify the role of goals. Additionally while the causal discovery formulation is clear the implementation details are not fully explained. The authors should elaborate on this and provide references to the code.", "Paraphrase: The study proposes an algorithm that learns causal relationships from surroundings and uses them for action and planning. It presents experiments on generalizing outside of training data either with false correlations or by combining concepts. Strengths: Causal models may improve generalization particularly in reinforcement learning (RL). They focus on relevant variables and their interactions addressing a critical issue in RL. The OOD generalization experiments are noteworthy. Weaknesses: Assumptions in the algorithms identifiability proof may not hold true in highdimensional data. Latent variables often complicate causal identifiability in lowlevel data but this is not addressed. Interventions on latent variables are unknown which is not discussed. Spurious correlation experiments could be solved without learning causal mechanisms just identifying variables. Its unclear if the causal structures in the stacking and door opening tasks are meaningful. More complex and informative causal environments (e.g. \"Alchemy\") could be used to test the algorithms ability to capture underlying structures. Questions: Are the causal structures in the experiments sufficiently complex to test the algorithms capabilities Could the spurious correlation experiments be solved without understanding causal mechanisms"], "wOUH1VQ9Rcj": ["Revised Summary: The authors present a generalization of causal discovery methods for identifying the structure of latent variables in a linear structural equation model where each latent variable has only one observed measurement. Strengths and Weaknesses: Clear exposition with illustrative examples Generalization of recent results to a special case Succinct presentation in Section 5 Concerns about theoretical validity and algorithmic clarity Questions: 1. \"Necessarily being one all results in this paper still hold\": Clarification on dependence of results on downstream objectives 2. Example insight for Proposition 1: Understanding the GIN condition 3. Elaboration on GIN condition enabling full identification: Comparison with previous methods 4. Recursion idea in GIN paper: Impact on this work 5. Recursion after source discovery: Intuition for overcoming the lack of access to source 6. Verifying impossibility claims: Mathematical tools and intuition 7. Exogenous ancestors in Theorem 1: Clarification 8. Alternative formulation of faithfulness in Assumption 1: Justification 9. Disadvantages of requiring precise measurement model: Handling multiple latent variables causing an observed node 10. Proof of Theorem 2: Connection to maxflow mincut formulation 11. Guarantee of information preservation in Theorem 3: Formalization 12. Finding the correct value for transformed independence tests: Algorithm and evidence 13. Transformation of estimating the dimension of \u03a9(ZY) in Theorem 4: Clarification 14. Efficiency of checking all solutions in Section 5: Explanation", "Paraphrased Statement: Summary: This study investigates the recovery of causal graphs involving hidden variables from data containing measurement errors. The authors introduce the Transformed Independent Noise (TIN) condition which verifies whether a linear transformation of certain measured variables is independent of other measured variables. Using TIN the ordered group structure of the causal model can be determined. Experiments on synthetic and realworld datasets support the methods effectiveness and reliability. Strengths: Clear and wellorganized writing Informative illustrations and annotations Solid methodology with technical details Weaknesses: Limited experiments on realworld datasets with measurement errors Lack of comparison with other ordered group decomposition methods like [1] and [2] Question 1: Is it possible to use the method directly for causal structure recovery Answer: This question cannot be answered from the provided information. The statement only discusses the recovery of causal graphs involving hidden variables not causal structure recovery in general.", "Paraphrased Statement: This paper proposes a method called the Transformed Independent Noise (TIN) condition to determine the causal relationships between variables that are not directly observed. The TIN condition is based on the assumptions of acyclic models causal sufficiency linear functions and nonGaussian error terms. The authors show that identifying the causal structure of the variables is possible with these assumptions. Simulations and realworld applications demonstrate the effectiveness of the proposed method. However the paper assumes that there are no unmeasured confounders between the observed and target variables which is a limitation. Additionally the proofs and explanations are primarily in the appendix making it challenging to fully understand the paper without reading them."], "xLnfzQYSIue": ["Paraphrase: Summary: The researchers enhance the Top Two sampling technique commonly employed in Bayesian Bandit Interaction (BAI) tactics. They demonstrate the BAI strategys asymptotic optimality for nonparametric bandits with finite support a finding not previously established. Strengths and Weaknesses: This research broadens the scope of Top Two tactics prominent in Bayesian BAI. This advancement holds great significance. Recently the Top Two approach has gained attention for its practicality. The studys findings further validate its merits. Notably the results for bounded nonparametric distributions are particularly insightful and offer a broader perspective on bandit analysis. Questions: None.", "Paraphrase: Summary: This paper explores Top Two algorithms which aim to identify the optimal \"arm\" (choice) in a multiarmed bandit problem. It has two key contributions: 1. Introduces the first Top Two method with a theoretical guarantee (betaoptimality) when the available arms have limited rewards. 2. Provides a general framework for identifying desirable characteristics that a Top Two algorithm must possess to have theoretical guarantees. Strengths: This is the first Top Two algorithm for bandit problems that has a theoretical guarantee for problems with bounded rewards. The analytical approach can be applied to broader reward distributions if the algorithm meets the properties outlined in the paper. Experiments with realworld data demonstrate the superiority of the proposed algorithms compared to existing methods. Weaknesses: The proof only applies to instances with a single optimal arm. Removing this assumption would be a valuable extension. Some elements of the presentation such as definitions and lemmas could be further explained for clarity. Questions: Aside from the identified errors and concerns the paper is deemed solid and could expand the potential of Top Two algorithms. Specific errors and areas for improvement are highlighted including missing definitions and incorrect notation.", "Paraphrased Statement: Summary: This study examines various toptwo algorithms applicable to broader distribution types including singleparameter exponential families and nonparametric bounded distributions. It compares their strengths and weaknesses. Strengths and Weaknesses: Theoretically the study expands on prior analyses done for Gaussian distributions. While the paper suggests multiple algorithms it does not definitively establish their novelty. Other algorithms such as the empirical best arm are commonly used in toptwo algorithms and theoretical analyses. In fact when the leader is the empirical best arm the analysis becomes simpler than when the leader is determined by Thompson sampling (TS). Empirically it is understood that the randomness in TS or resampling enhances exploration. Toptwo algorithms which directly employ posterior quantities may be less efficient because these quantities can be imprecise due to random and unfavorable observations. Questions: 1. What does I in TCI stand for What differentiates the challengers TCI and TC(I) 2. It appears that the experiments did not include algorithms using RS as the challenger. Could the reason be that RS can be timeconsuming It is possible that algorithms using RS could outperform the ones included in the experiments. For Gaussian distributions good approximations of RS exist. Is it feasible to develop approximations of RS for the general distributions examined in this paper", "Paraphrase: This study examines \"TopTwo\" algorithms for the explorationonly multiarmed bandit problem with limited distributions. The authors explore various leader and challenger selection strategies which include popular methods like TTTS and T3C. They develop a framework for evaluating the sample complexity of TopTwo algorithms and provide experimental results. Strengths: For bounded distributions this work presents a general approach for creating asymptotically optimal algorithms without the need for computationally intensive explorations. This could be useful for practical applications as most realworld problems involve bounded distributions. The paper is wellwritten and comprehensive. Weaknesses: Given the thorough investigation of TopTwo algorithms it would have been helpful to see more experiments with a wider range of arms. Additionally the lack of finitetime guarantees is a limitation although the authors acknowledge this. Lastly the authors could emphasize the original aspects of their analysis to distinguish it from previous research. Question: The authors have observed that KLLUCB performs worse than uniform in one of their experiments. They have requested an explanation for this result."], "sADLRl2STMe": ["Paraphrased Statement: Summary: The authors introduce a new regularization method that enhances the performance of shallow linear neural networks on tasks involving lowrank matrix completion and factorization. They provide theoretical insights into the behavior of these models under vanilla gradient descent and ADAM demonstrating that adaptive optimization allows shallower networks to achieve better solutions. Strengths: Simplicity and effectiveness of the proposed method backed by sound theoretical and empirical justifications. Welldesigned and comprehensive experiments with clear and interpretable results. Clarity and organization of the text and figures. Weaknesses: Lack of explicit connection between the dynamics equations and empirical findings. Limited understanding of the behavior of the dynamics equations particularly in terms of acceleration regimes. Narrow scope of findings limited mainly to shallow networks. Misleading title that does not accurately reflect the focus on shallow networks. Terms such as \"implicit regularization\" could be clarified to distinguish between natural regularization properties and specific optimization techniques. Questions: Clarify the hierarchy of theorems corollaries and lemmas. Describe the conditions under which normalization accelerates or dampens the trajectory. Rephrase the statement \"This similarly holds for unregularized Adam\" to accurately convey the intended meaning. Provide explicit proofs of Lemmas 2 and 3 in the appendix. Highlight the superiority of the proposed penalty for all adaptive optimizers in Table 1 using bolding.", "Paraphrased Statement Summary: This paper introduces a novel penalty term for gradient descent defined as the ratio of the nuclear norm (trace) to the Frobenius norm of the weight matrix. Experimental findings in the context of matrix completion demonstrate that linear networks trained with this penalty exhibit reduced sensitivity to network depth when optimized with Adam or Adamax but not with standard gradient descent. Notably even in the extreme case of a 1layer linear network the model trained with Adam and this penalty achieves performance comparable to deep networks exhibiting low rank and spectral sparsity. Strengths and Weaknesses: This paper offers a significant contribution by proposing a new penalty for gradient descent. The authors conduct a comprehensive experimental analysis encompassing: Baseline models without the penalty Impact of the penalty on vanilla gradient descent and Adamoptimized models Investigation of the degenerate case (1layer linear network) The paper also includes a theoretical analysis of the dynamics under common assumptions and empirical results examining generalization performance weight matrix rank and singular value dynamics. The authors acknowledge the papers limitations leaving open questions for future research on the penaltys applicability to nonlinear networks different tasks and datasets and the reasons for its compatibility only with Adam optimization. Suggestions for Improvement: Improve the clarity of the paper by including the explicit equation for the proposed penalty in Section 1. Revise Section 3.5s title to \"Comparison with Other Penalties and Optimizers\" for greater specificity. Consider including repetitions in the experimental setup to establish confidence intervals for the experimental analysis. Modify Section 4s title to \"Results on RealWorld Data.\"", "Paraphrase: Gradient descent for deep linear networks (DNNs) naturally drives the network toward solutions with low matrix ranks in matrix completion factorization tasks. This work introduces an explicit penalty term that mimics the rankreducing effect of deep linear networks. This penalty is independent of network depth. A singlelayer linear network with this penalty achieves the same rank reduction as deep matrix factorization methods with comparable performance. The penalty works effectively with the Adam optimizer but not with gradient descent. This observation may indicate an interaction between the innate biases of optimizers and explicit regularizers. Experiments demonstrate this difference between Adam and gradient descent. Additionally realworld tests on MovieLens100k data show the methods competitiveness against existing approaches that often rely on specialized architectures or additional information. Strengths: Unique approach to rank minimization through an explicit penalty. Depth invariance of the proposed method. Enhanced generalization and rank reduction performance even with limited data. Effective with the Adam optimizer. Weaknesses: Depth invariance may limit its potential applications. Lack of theoretical explanation for the discrepancy between optimizers. Questions: Could the authors provide examples of where depth invariance is beneficial Can they offer theoretical insights into why different optimizers behave differently with the proposed penalty", "Paraphrase: Summary: This study proposes a new regularization method for deep linear networks. Theoretical analysis and empirical results demonstrate its effectiveness in singlelayer deep networks with SGDAdam optimizer for matrix completion tasks involving lowrank matrices. The main experiment shows that the proposed regularizer improves performance in simple models on a recommender system task. Strengths: 1. Clear and engaging writing style that combines illustrative examples with theoretical insights. 2. Comprehensive synthetic experiments clearly demonstrating the advantages of the proposed method over alternatives. 3. Extensive parameter optimization experiments. 4. Technically rigorous analyses. Weaknesses: 1. Unclear writing that sometimes obscures the main message. 2. The method may not be directly applicable to nonlinear networks though the main experiment suggests otherwise. 3. Limited experimental evaluation conducted only on a single dataset and setting. 4. The theoretical advantage described as a form of preconditioning is not fully explored or justified. 5. Mathematical presentation requires clarity and refinement. 6. Synthetic experiments use fullbatch learning which does not accurately simulate stochastic gradients. Details and Questions: 1. \"Depth\" is unnecessary in the context of deep linear networks. 2. The task goal is unclear: recovering unseen entries setting rank or prediction generalization. 3. The setup of student network weights is confusing. 4. \"Singular value separation\" is undefined. 5. The trajectory of singular values should be defined explicitly. 6. The convergence rates in equations (3) and (6) require proof. 7. Figure 12 comparisons are not fully supported by the data. 8. The recommender system experiment requires more details. 9. The rank definition should be provided in the main text. 10. \"Acceleration\" should be defined or exemplified. 11. The regularization method for nonlinear networks is unclear. 12. \"Without any factorizations\" is ambiguous as model representations involve factorization. 13. The motivation for the specific preconditioning used is not clear. 14. The factorization of Kronecker product property needs clarification."], "kHrE2vi5Rvs": ["Summary Strengths Examines the training of neural networks for solving computationally hard combinatorial optimization (CO) problems. Introduces a novel \"SymNCO\" loss function to guide networks towards finding nearoptimal solutions while also learning problem and solution symmetries. Claims that SymNCO can enhance existing neural CO solvers and improve performance on four CO problems (TSP CVRP PCTSP and OP). Weaknesses Small Problem Sizes: The CO instances considered are significantly smaller than those tackled by prior works raising concerns about scalability. Inconsistent Terminology: The paper uses various jargon and acronyms that may not be clear to readers. Unclear Methodology: Some aspects of the SymNCO methodology such as solution sampling and the integration of the loss functions are not adequately explained. Incomplete Related Work: Missing discussion of relevant prior works especially in the context of graphbased CO problems and reinforcement learning. Limited Generality: SymNCO is specifically designed for policygradient RL and graphbased Euclidean CO problems. Its generalizability to other ML paradigms and nonEuclidean problems is not explored.", "Paraphrased Statement: This research focuses on using deep reinforcement learning to solve Traveling Salesman Problems (TSPs) and related routing problems in an approximate manner. The papers main technical contribution is to recognize the frequent presence of symmetries in routing problems and their solutions such as rotational symmetry and the cyclical nature of the paths. It proposes new loss functions that gently incorporate these symmetries into the AM (Koo et al.) and POMO (Kwon et al.) encoderdecoder architectures. Training models with the proposed loss functions (SymNCO) yields better results than models trained with previously suggested loss functions on both randomly generated routing problem instances and realworld examples from TSPLib.", "Paraphrase: Summary: Combinatorial optimization problems often feature symmetries. This paper utilizes these symmetries to enhance the training of neural networks designed to address such problems. It defines two types of symmetries: problembased and solutionbased. Method: The paper introduces a regularization approach to help neural networks learn these symmetries. It evaluates the approach on four common combinatorial optimization problems. Strengths and Weaknesses: The paper is wellwritten and easy to follow but the derivation of Equations 5 and 6 is unclear. The authors do not define Lss and Lps but instead modify the gradients directly. This makes it difficult to verify the mathematical soundness of the approach. However the experimental results appear promising. Questions: Could you provide details on how Equations 5 and 6 were derived What does \"gr\" refer to in Table 1 Did you use POMO AM or PointerNet to obtain the results in Tables 1 and 2 Could you compare the three versions (no SymNCO POMO AM PointerNet) against their baselines Could you extend the comparison with EGNN by training EGNN with your loss"], "nrOLtfeiIdh": ["Paraphrased Statement: Summary: The research aims to enhance prediction accuracy by considering the quality of input data. A good classifier may still make mistakes due to poorquality data. To address this the RECOURSENET is proposed to assess data suitability for classification and suggest alternative environments for capturing better images. RECOURSENET then trains a classifier on highquality images or images generated from user input. Contributions: Presents a unique threelevel framework for recourse mechanisms without human involvement in training. Strengths: Provides a novel approach to accuracy enhancement by focusing on input data quality. Introduces a recourse mechanism to improve data quality and prediction accuracy with potential applications in medical diagnosis. Offers an intuitive and comprehensive explanation of the method. Conducts a thorough ablation study to demonstrate the benefits of each component. Weaknesses: Experiments are conducted on limited datasets. Inconsistent notation and writing are present. The mechanisms two propositions and their benefits are not fully clear. Questions: A more detailed theoretical analysis is recommended to explain how the proposed propositions benefit the design of the mechanism. The assumption that \u03b2ij \u03b2 \u2264 \u03b5 appears to be restrictive.", "New Statement: Summary: RecourseNet is a system that identifies and adjusts input data (instances) to improve classification accuracy. It does so by leveraging alternative \"environments\" within the models latent space. RecourseNet comprises three components: a classifier a trigger that selects instances for modification and a recommender that makes the actual modifications. The training process and conditions for successful recourse recommendations are detailed. Experiments on ShapeNet Speech Commands and a synthetic dataset showcase RecourseNets performance. Strengths: Provides recourses as alternative environments particularly beneficial for images. Novel optimization problem design with clever tricks for solving it. Thorough ablation studies justify design choices through experimental results. Weaknesses: Ambiguous problem definition: Is it noisy training data classification or alternative environment discovery Potential concern about relying on adverse examples during training for generalization. Inappropriate use of \"recourse\": Recourse typically involves actionable changes to input instances while RecourseNet performs preclassification data modifications. Lack of suitable baselines for comparison in experiments. Questions: How do adverse examples contribute to generalization if they are corrected during training What is meant by \"upstream classifier\" (it appears downstream in the diagram) Notation clarification: Is \"D\" for objects in the training set Explanation of training objective (Eq. 1): How are missing objects handled in the summation Clarification on \"analytical form of Z\" (L126). Implementation details in Algorithm 1: How is the TRAIN() function used to solve Eq. 5 How is Rec constructed How is xir computed Justification for the approximation in L157. Purpose of computing \u03c0 (L169): Isnt \u03c0 already calculated in Algorithm 1", "Summary The proposed model suggests a feedback loop where users can refine the models predictions by providing additional data with slightly altered settings (e.g. increased brightness or reduced zoom). The model consists of three components: one that predicts a target class one that determines the potential benefit of resampling and one that recommends specific resampling parameters. The method requires access to multiple views of each instance with corresponding values that quantify the variation in the image transforms. Strengths and Weaknesses Concern: The practicality of the recourse resampling method in medical settings is questionable due to the high cost of acquiring additional images and the models need for extensive training on annotated data with varying imaging parameters. Originality: The idea of recourse resampling is novel and potentially useful. Quality: The paper is wellwritten but the experimental evaluation is limited to synthetic and nonsynthetic datasets with precise view parameters. More realistic medical datasets need to be tested. Significance: If reframed in a way that accommodates more realistic data sources the concept of recourse could prove valuable. However the current implementation lacks realworld significance. Questions 1. Is the method truly applicable in medical settings given the cost and logistical challenges 2. Has the model been evaluated using randomly resampled instances to assess the efficacy of the recourse predictor and recommender 3. Have the proposed method been compared with existing fewshot learning models such as ProtoNets and Matching Nets which also benefit from increased sample size", "Paraphrased Summary: This research introduces RecourseNet a framework that optimizes teacherstudent interactions to provide focused and beneficial feedback. RecourseNet aims to improve learning efficiency by assisting learners in accessing relevant samples and helping predictors determine the best instances to recourse. The authors propose a learning objective that simultaneously optimizes the teachers guidance and the predictors decisionmaking. While the objective cannot be directly optimized they develop an approximation method to facilitate training. Experiments demonstrate RecourseNets superiority over simple baselines highlighting the importance of instancespecific recourse. Paraphrased Strengths and Weaknesses: Strengths: Clear motivation and problem formulation Novel and practical idea of instancespecific recourse recommendations Promising learning objective Effective experimental results Contribution of new datasets Weaknesses: Concerns about the original learning objective and its potential trivial solutions Crude approximations of the learning objective Limited baselines for comparison Lack of confidence intervals in experimental results Paraphrased Questions: How were hyperparameters selected Selection process for the 5 classes in Figure 4 Correction of a mathematical error in Equation (1) Typos and grammatical errors in text"], "zdmYnIRXvKS": ["Summary The researchers have developed a model of a spiking neural network that can be optimized using specific loss functions including a reconstruction error and a regularizer. They have also shown that under certain assumptions this model is equivalent to a simplified network with \"leaky integrateandfire\" neurons. In this context the simplified network is considered \"optimal.\" The model has been used to study network dynamics and the researchers have linked different model components to biological processes. Strengths The model provides a link between generalized leaky integrateandfire networks and the optimization of loss functions. The analysis is rigorous and technically sound. The model is complex and has many potential applications for further analysis. The model has the potential to bridge the gap between existing spiking network models and models that are more biologically interpretable. Weaknesses The connection between the model and the concept of \"efficient coding\" needs to be clarified and the assumptions behind this connection should be stated explicitly. The technical sections of the manuscript could be clearer and more accessible. The transition from equation (6) to equation (9) is not explained. The analysis of the networks dynamics is limited and the results could be interpreted more fully. It would be valuable to see how the network performs for different configurations and stimuli. Minor Comments The figures are small and could be made larger. Questions Why are the chosen loss functions meaningful The justification for function 3a is clear but function 3b is less intuitive. What are the properties of the simplified network What are the effects of the network parameters and the noise level on network behavior", "Summary The researchers created a neural network model that aims to maximize the transmission of information. This optimization resulted in several biologically accurate characteristics such as: Generalized leaky integrateandfire neuron dynamics Lowrank connectivity Varied decoding weights and currents across neurons Strengths and Weaknesses This work advances the field of developing biologically plausible neural networks designed to optimize information coding. While it introduces realistic features its novelty is somewhat limited due to its close resemblance to previous research. The paper is technically sound but it could benefit from improved organization and a clearer presentation of contributions. The significance of the findings is challenging to assess due to the difficulty of validating the biological plausibility of the features. Quantitative validation methods would enhance the studys credibility. Questions for the Authors Propose experiments that could validate the models results. Explain how the different features contribute to improved coding efficiency.", "Paraphrase: Summary: The paper claims to have developed a biologically realistic spiking model that demonstrates efficient coding in a generalized leaky integrateandfire network. The model incorporates excitatory and inhibitory units with fast and slow synaptic currents homeostatic currents like spiketriggered adaptation and rebound current varying firing thresholds and resets diverse postsynaptic potentials and structured connectivity. Strengths and Weaknesses: Weaknesses: 1. Formatting: The manuscript should follow NeurIPS format (Introduction Related Work Method Results Discussion Conclusion). 2. Objective: The purpose of the inhibitory population to minimize the distance between excitatory and inhibitory neurons is unclear. 3. Biological Accuracy: Lines 109111 incorrectly state that only Na and K ions affect voltage changes. 4. Mathematical Model Derivation: Its unclear how Equation (8) can be rearranged to obtain the LIF model. 5. Lack of Experimental Support: The paper lacks evidence to demonstrate the models effectiveness in practical applications. 6. Insufficient Insights: The authors primarily focus on mathematical descriptions without providing background knowledge or insights. 7. Poor Writing Style: The writing is unclear and lacks precision. Typos: 1. Line 12: Remove \"and\" at the beginning of the line. 2. Line 340: Remove \"how\" at the beginning of the line. 3. Line 79: Add quotation marks before \"and\" in \"...of the cell type Y and M ...\"."], "yRhbHp_Vh8e": ["Paraphrase: This study introduces Grounded Video Situation Recognition (GVSR) a new task involving detecting events in videos over time identifying the actions (verbs) actors and objects involved and specifying the time frame of each event. The authors have developed a dataset for GVSR based on VidSitu incorporating multiple annotations per video (5 events per 10second video). They have also established evaluation metrics to assess performance in handling multiple concurrent events. Additionally human performance benchmarks have been collected on the validation set. The proposed model VideoWhisperer extracts object embeddings from video frames using an object detector and video features from the entire video using a pretrained video backbone. These features are integrated by a transformer encoder which is followed by a transformer decoder for structured prediction of the event details (semantic role labeling for videos). Evaluation and ablations reveal the models effectiveness in both localized (eventspecific) and nonlocalized (VidSitustyle) video situation recognition. Strengths and Weaknesses: Strengths: Tackles a challenging problem in finegrained video understanding. Introduces a new dataset (Grounded VidSitu) for structured video situation recognition. Provides ablations to guide future model improvements. Weaknesses: Model complexity may not be optimal. Lacks a section on limitations and social impact. The term \"Grounded\" is ambiguous. The modeling section requires clarification especially regarding the highlevel strategy and ROTDVOTE networks. Qualitative examples and discussion would enhance comprehension. Questions: What are specific examples where the approach excels and struggles How crucial is the selection of video representations (object embeddings vs. video features)", "Paraphrased Statement: Task Summary: VidSitu predicts actions involved entities and relationships among actions in video clips. This paper introduces \"Spatiotemporal Grounding\" (GVSR) as an extension to VidSitu focusing on identifying and resolving entities across actions. Proposed Approach: A novel Transformer model (VideoWhisperer) is proposed that jointly predicts all task outputs (verbs roles nouns and grounding) in one step. The model consists of three stages: Contextualizing features for video and objects Predicting action labels (verb roles) and semantic roles (noun captions) Localizing entities through attention scores in the Transformer model Advantages: Predicted outputs are generated jointly Improves entity captioning accuracy Localizes verbs accurately without explicit grounding annotations Contributions: Localization annotations for testing in VidSitu VideoWhisperer model for endtoend singlepass prediction of structured outputs Strengths and Weaknesses: Importance of grounding semantic predictions is highlighted GVSR extends an existing task and improves evaluation metrics Main contribution is localization annotations and the VideoWhisperer model Joint prediction simplifies architecture and allows for crosssubtask modeling Potential for scalability Entity captioning remains challenging and imprecise but GVSR alleviates some evaluation issues Questions: How have the authors addressed the imbalanced distribution of verbs in VidSitu Could computing event embeddings from subsampled frames instead of full video reduce computational overhead References: [a] Cui Yin et al. \"Classbalanced Loss Based on Effective Number of Samples\" CVPR 2019 [b] Lin TsungYi et al. \"Focal Loss for Dense Object Detection\" CVPR 2017", "Paraphrase Summary This study focuses on video situation recognition aiming to predict structured information such as events relationships actions and wordverb pairs. The Model: VideoWhisperer The authors introduce VideoWhisperer a threestage Transformer model: 1. Stage 1: Extracts contextualized embeddings for video features and key objects. 2. Stage 2: Attends and pools object embeddings to locate answer regions. 3. Stage 3: Generates captions to describe wordverb pairs in the video. Evaluation Tested on the VidSitu dataset VideoWhisperer outperforms baseline models. Strengths Clear and concise writing Decomposing the task into stages appears technically sound New data annotation for the dataset Weaknesses Uncertain about the novelty of the task decomposition approach Lack of comparison with baseline models in terms of computational complexity Experiments only conducted on a single dataset with limited baselines Further evaluation on other video situation understanding datasets and implementation of additional baselines would strengthen the papers credibility", "Paraphrased Summary: This paper presents a Transformer network structure for identifying and describing situations in videos which: Enhances earlier Semantic Role Labeling (SRL) approaches for videos by adding spatiotemporal relevance. Eliminates the need for linking roles to verbs allowing for endtoend Grounded Video Situation Recognition (GVSR). The VideoWhisperer Model: The proposed model consists of: A Transformer encoder that learns embeddings for objects in each frame and events with connections between them. A Transformer decoder that identifies entities for each role using the object and event embeddings. Another Transformer decoder that predicts captions for each role based on the entity predictions. Evaluation: To assess localization accuracy the authors manually annotated bounding boxes around roles in video clips. These annotations will be made public and may be incorporated into the official evaluation tool. However they are not used during training making the model weaklysupervised. Experimental Results: Extensive experiments demonstrate the models effectiveness and the importance of each design choice. On the SRL benchmark the proposed approach outperforms previous work and offers spatiotemporal localization of roles. Strengths and Limitations: The model is theoretically sound and supported by experiments. It enables simultaneous prediction of actions roles spatiotemporal localization of roles and captions unlike previous approaches. The low role prediction accuracy (45) may limit overall performance. This could be addressed in future research to further improve the model."], "l2CVt1ySC2Q": ["Paraphrased Statement: Summary: The authors propose an extension to existing Rademacher complexity bounds for more accurate analysis of practical neural network architectures such as convolutional layers and skip connections (used in Residual Networks). This extension employs two capacity measures: the Lipschitz constants of layers and the (2 1) group norm distance of convolutional filter weights to their initial values. Experiments using a modified ResNet18 model trained without batch normalization on CIFAR10 CIFAR100 and Tiny ImageNet datasets demonstrate that constraining these capacity terms (reducing group norm and Lipschitz values) can lead to performance comparable to models with excess capacity despite similar capacities in tasks of increasing difficulty. This suggests that neural networks are compressible with respect to weight norms similar to the known compressibility in terms of parameter counts. Strengths and Weaknesses: Strengths: Clear motivation and appropriate references for proposed capacity terms Extension of complexity measures to more realistic settings including convolutional layers and skip connections Weaknesses: Intuition behind capacity terms may not be immediately clear Reproducibility concerns regarding Lipschitz constant calculation for different layers Claim of independence between weight norm and parameter count does not fully hold up in practical settings Questions: Can weight norms and parameter counts be considered independent in the context of realworld neural network training given the use of weight decay and initialization methods that depend on parameter counts Why are layerwise Lipschitz constants important given that optimization is not performed layerwise Isnt the Lipschitz constant over all parameters more relevant than the layerwise constant Is 1x1 convolution on a 1x1 spatial feature map equivalent to a linear combination or dot product over channels rather than matrix multiplication", "Paraphrase: This study introduces novel bounds for Rademacher complexity in neural networks along with empirical analysis of their impact. Notably the bounds jointly consider skip connections and convolutions while normalization techniques are excluded. A comparison to previous research is included. Strengths: Clear presentation of the relationship to existing literature especially regarding the inclusion of convolutions and skip connections. Theorems are contextualized with other work highlighting advantages and drawbacks. Figure 1 effectively represents the state of the art. \"Relation to Prior Work\" sections provide valuable literature overview. Weaknesses: Lack of consideration for normalization techniques in the network. Linear scaling of layers in ResNet18 may not impact capacity making it unclear how weight norm limitation affects capacity. No justification for the term \"compression.\" Significance: Previous studies addressed convolutions and skip connections separately this study addresses them jointly. The study highlights excessive capacity in networks and how it can be reduced. Implementations are currently cumbersome and not fully practical. Clarity: The paper is wellstructured and clear in presentation. The proof sketch for Theorem 3.2 and its motivation are welldescribed. Use \"vacuous\" should be replaced with a more common word. Figure 1 lacks a caption and legend. TinyImageNet200 requires additional explanation. Experiments: The alternating projection procedure is applied every 15th SGD update step not \"increased to 15.\" Table 1 states \"the testing error is on a par with the unconstrained case\" although the errors differ. More technical details and clarification on the experiments would be helpful. The empirical generalization gap and its importance should be explained. Originality: The paper clearly defines the relationship to existing literature and outperforms its bounds. The nontrivial bounds and empirical evaluations are significant contributions. Quality: Figure 1 needs improvement. The paper lacks references for performance with batch normalization disabled. Insufficient discussion on the quality reduction due to the lack of normalization. Questions: Why can Lipschitz constraints be applied to h around line 215 How can Rademacher boundaries be established for normalization techniques like group and batch normalization Why are larger networks and datasets not used in the experiments Why are the experiments run for 200 epochs despite faster convergence times for ResNet50 Can capacity reduction be achieved using other optimizers that consider layers separately such as LARS or LAMB", "Paraphrase: This study derives generalization bounds for convolutional and ResNet networks. Both parameter counting and normbased approaches are examined for each type of network. Convolutional Neural Networks (CNNs) Normbased bound (Theorem 3.5 equation 1): Similar to [1] but uses L2 norms instead of L\u221e norms resulting in a slightly different scaling based on the regime. The input dependence for each layer is broader but offset by the absence of a postpooling width factor. The L2 norm in the batch direction provides an advantage against outliers but worsens the dependence on the number of classes. Parameter counting bound (Theorem 3.5 equation 2): Similar to [3] but with better constant management and a cleaner proof. Improves over [2] by stating that sample complexity increases with the number of parameters. ResNets Parameter counting bound: Similar to [3] but focuses on a different problem and offers the first bounds on ResNets. A clear exercise but lacks original content. Sample complexity is again proportional to the number of parameters. Normbased bound: A significant extension of [1] addressing the increased connectivity of the ResNet architecture. A successful effort though tedious due to the large number of connections. Strengths and Weaknesses Strengths: Comprehensive and wellorganized work. Clear summary of the field and common misconceptions. Progress since previous submission. Weaknesses: No significant originality in techniques. Some issues not corrected since last submission. Numerical constants: A slight error in the treatment of numerical constants involving a constant of 12 instead of 144. The improvement over [2] is partially illusory due to a different problem formulation. Bold claim regarding the optimality of constants in [4] given the difficulty of proving L\u221e norm results. Improvement over [3]: Purely in terms of proof technique and clarity. Both bounds fundamentally depend on the square root of the number of parameters with distance to initialization only present in logarithmic factors. Comparison with [1]: The parasitic factor in the generalization bound could be avoided in [1] for the twolayer case. [1] may be superior in this specific scenario."], "n0dD3d54Wgf": ["Paraphrase: The paper explores continual learning (CL) on resourcelimited edge devices. It highlights the lack of reporting on actual training costs (FLOPs) in previous CL studies which is crucial for practical applications on edge devices. The paper proposes adaptations of sparse training techniques for CL including: Dynamic weight sparsification: Sparsify weights based on their magnitudes and gradients and combine with a \"collapse and expand\" approach at class boundaries. Data reduction: Reduce the training data by removing misclassified samples. Lowgradient weight updates: Reduce the number of parameter updates by ignoring parameters with small gradients. Integrating these techniques with rehearsal methods results in significant savings in time and memory with no loss of accuracy and even slight improvements in forgetting. Strengths: Reduces resource requirements for CL unlike previous work that focused only on maintaining accuracy. Practical and applicable in realworld scenarios. Novel in terms of its focus on resource efficiency. Weaknesses: The techniques are relatively straightforward and lack significant technical novelty. However proposing a practical solution to a new problem may still be valuable. Questions: The appendix refers to a nonexistent Section 6.", "Paraphrased Statement: The authors propose investigating continual learning from the perspective of training efficiency. They suggest using network sparsification techniques in a continual learning context. Their findings indicate that sparser networks are not necessarily inferior to dense networks as continual learners and may even outperform some nonsparse approaches. They validate the efficacy of their SparCL method through ablation studies and an experiment on an edge device. Strengths: Wellreasoned intuition that noisy overparameterized learning processes can benefit from pruning. Clear and accessible presentation. Weaknesses: The claim of using a stateoftheart competitor roster is questionable as some recent methods are missing. DER is used as a reference baseline but the improved version proposed in the same paper is not included. An analysis of where and how the network is pruned (across layers and tasks) would provide valuable insights. A comparison with alternative buffer reduction approaches would be beneficial. Alg.1 has a missing \"end\" statement and an unclear condition. The inclusion of figures in vector format would enhance readability. Ethical and societal impact considerations should be addressed in the main paper. Specific Questions: Why is DER not considered among the competitors Is it possible that representation learning approaches like CO2L and DualNet could also benefit from sparsification Can the authors comment on patterns of weight pruning within the backbone network", "Summary The paper introduces Sparse Continual Learning (SparCL) a technique for efficient continual learning on edge devices utilizing sparsity. The authors claim SparCL achieves accelerated training and preserved accuracy through synergies in weight sparsity data efficiency and gradient sparsity. They incorporate a dynamic data removal (DDR) process to eliminate uninformative training data and a dynamic gradient masking (DGM) technique to reduce gradient update sparsity. SparCL reportedly uses 23 times fewer training FLOPs and achieves close to the highest accuracy within 1.7 of the best SOTA. The method has been tested in both class incremental and task incremental settings on split CIFAR10 and split tiny ImageNet datasets. Strengths Combination of dynamic data removal and sparsity Gradient sparsity and proposed dynamic gradient masking Consideration of weight importance in rehearsal buffer data Weaknesses Unclear explanations in some sections particularly regarding taskaware dynamic masking Questions about the reported memory footprint and training FLOPs savings given additional overhead from dynamic gradient masking and dynamic data removal Comparison to rehearsalbased methods for memory footprint and training which inherently require more resources Unclear effects of setting the buffer size to 0 on accuracy Minimal improvement over SOTA (e.g. PackNets 94.50 accuracy vs. SparCLs 95.19 with a 500image buffer) Potential scalability limitations when learning a large number of tasks due to buffer memory and mask overhead Questions Clarification on the potential impact of additional overheads on training FLOPs and memory footprint Explanation of the accuracy differences when increasing the buffer size from 200 to 500 images Perspectives on the scalability of the proposed approach for learning hundreds of tasks", "Paraphrased Statement: This paper proposes \"Sparse Continual Learning\" a continual learning method that prevents neural networks from \"forgetting\" previously learned tasks while simultaneously improving training and inference efficiency. The framework uses three main components: enforcing sparsity in network weights employing a replay buffer for task selection and implementing sparse gradient truncation. Its efficacy is demonstrated on public benchmarks and realworld edge devices. Strengths: Clear and concise presentation Positive results on mobile devices Weakness: Sparsity is not a novel concept for enhancing deep model efficiency so its application to continual learning is not particularly groundbreaking. Only weight sparsity directly addresses catastrophic forgetting which is crucial for continual learning. The quantitative comparisons in Table 1 are outdated as the most recent comparison method is from 2020 limiting the support for claims of significant accuracy improvements. Overall while the paper suggests that sparsity can increase continual learning efficiency and accuracy its novelty and soundness are questionable for acceptance at NeurIPS. Questions: The methods reliance on a replay buffer could limit its applicability to more advanced continual learning methods. Demonstrating the flexibility of the method by showing its compatibility with different model architectures and datasets could enhance the papers strength."], "x56v-UN7BjD": ["Summary This paper combines momentum techniques with tree aggregation in the context of differential privacy to improve the convergence rates for differentially private empirical loss minimization. Strengths Improved rates: The paper proposes faster algorithms with improved convergence rates compared to existing methods. Technical advancement: The algorithms avoid privacy amplification techniques using instead multiple random shuffles of data. Relevance: The problem is crucial in differential privacy and machine learning. Weaknesses Limited improvement: The improvements over existing methods appear incremental. Limited outofsample analysis: The paper primarily focuses on convergence rates rather than outofsample performance. Lack of credit to prior work: The paper fails to acknowledge that the technique of recursive gradient estimation has been previously explored in the literature. Questions Notation: The use of [xy] and [yz] intervals can be confusing when x is already used for data points. Typos and inconsistencies: Several typos and inconsistencies make the paper difficult to read. Algorithmic details: Some algorithm details are unclear such as the nature of Si[1] in Algorithm 1. Noise generation: The paper does not fully explain the need for different types of noise depending on t. Privacy analysis concerns: The repeated noise estimates and rebuilding of estimators may not be fully accounted for in the privacy analysis. Proof error: The proof of Lemma 11 contains a minor error in the first inequality.", "Paraphrased Statement: Summary: The study presents novel algorithms for privacypreserving Empirical Risk Minimization (ERM). These algorithms leverage tree aggregation in momentumnormalized Stochastic Gradient Descent (SGD) from Cutkosky and Mehta (2020) to achieve superior convergence rates compared to existing methods. Strengths: The utilization of tree aggregation for momentum is an innovative approach in this domain. The proposed algorithms demonstrate improved convergence rates. Weaknesses: The algorithms may necessitate a more substantial memory footprint to store tree aggregation nodes. Questions: The studys findings appear promising. However it would be helpful if the authors could further clarify why they chose normalized SGD as the foundation for their algorithms.", "Summary (Paraphrased): The paper introduces a novel privacypreserving algorithm and demonstrates its enhanced convergence performance compared to existing differentially private normalized Stochastic Gradient Descent (SGD) algorithms. This improvement stems from the use of a treeaggregation technique which reduces the noise required for differential privacy preservation. Strengths: Improved error guarantees for smooth nonconvex problems using the treeaggregation technique. Refined analysis of normalized SGD yielding similar error rates as previous studies without the need for privacy amplification or full batch gradient descent. Weaknesses: Algorithm 2 exhibits a worse dependency on the dimension (d) and privacy parameter (\u03b5) compared to prior algorithms. The new algorithm has a significantly higher runtime complexity proportional to N113d23. Privacy amplification may not be directly applicable to the treeaggregation method. The paper lacks experimental evaluations to verify the algorithms convergence properties. Questions: The explanation of differential privacy for each sample (lines 103111) assumes that gradients are derived from the same sample. Can the authors clarify if this applies to GD (gradient descent) rather than SGD There is a missing step size term in the statement for Algorithm 1s wt1 update."], "voV_TRqcWh": ["Paraphrased Statement: Summary: This research proposes a new generative model based on the Poisson equation. The model adds a scalar variable z to each data point representing a hyperplane centered on the true data point. During training the model constructs an empirical Poisson field using the current batch of data and uses a forward ODE to calculate the normalized field VBL. The model f(\u03b8) is then trained to regress on VBL using an L2 loss. For backward propagation the authors start with a large z (zmax) and gradually decrease it to z0 to generate samples. This approach addresses the issue in the original VEVP ODES where the l2 norm of perturbed training data and the standard variance of the Gaussian perturbation kernel can have a strong correlation. Experiments demonstrate the models effectiveness. Strengths: Proposes a new SDEODEbased generative model. Uses Poissons equation potentially reducing the correlation between norm(t) and z. Produces samples with less computational cost. Wellwritten and easy to understand. Weaknesses: Requires extensive computation during training limiting its application to highresolution data. The empirical Poisson field is based on a subset of the data potentially introducing bias. Enlarging the batch size may not fully mitigate the bias issue. Questions: 1. While the model claims comparable or better performance based on scores the samples in Figure 3 appear to have more artifacts compared to those in [1]. Can this be explained Additionally the paper only provides 64 x 64 samples while [12] can generate 256 x 256 samples. 2. Table1 and Table2 compare sampling time using NFE. Since PFGM uses a different ODE equation is there a time difference in running each ODE step Can you also compare the total sampling time using wallclock time 3. How sensitive is the model to the hyperparameters MAXSTEP zmin and zmax Theorem 1 assumes r approaches infinity. Does zmax need to be carefully selected to meet this requirement References: [1] Improved techniques for training scorebased generative models. [2] ScoreBased Generative Modeling through Stochastic Differential Equations", "Paraphrased Statement: Summary: The paper suggests employing the Poisson equation in higher dimensions as a generative model. It demonstrates its advantages in terms of speed and accuracy through numerical examples. Strengths and Weaknesses: Strengths: The paper presents an intriguing physicsbased approach to generative modeling. Weaknesses: Despite the promising concept the authors seem to have prioritized showcasing the model in examples rather than establishing mathematical rigor. The paper is marred by numerous typos and oversights: The formula in equation (2) is only valid for dimensions N \u2265 3. The text fails to mention this assumption. \"Radical projection\" should be \"radial projection.\" The function f(x) is defined without restrictions which may compromise the vector fields Lipschitzness and the existence and uniqueness of the corresponding flows. p(x) is initially introduced as a probability distribution but is later equated with \u03c1(x) which was defined as a density. The proofs and writing lack mathematical rigor overlooking technical considerations. For instance the paper does not address the guaranteed existence of a unique C2 solution to the Poisson equation. Questions: What is the mathematical justification for relating the time derivative of x to E(x) Can the heuristic argument in lines 8485 be replaced with mathematical arguments There are potentially many generative models and vector fields for a given Poisson equation geometry. Why were only a hemisphere and its vector field considered", "Paraphrase: Summary Researchers have developed a physicsbased generative model called Possion Flow Generative Model (PFGM). In PFGM the data distribution corresponds to a source function and dual ordinary differential equations map it to a positive hemisphere with radius r. The model generates samples by starting from a point on the hemisphere following the negative electric field and ending on the data distribution. PFGM was tested on CIFAR10 and CelebA datasets and outperformed or matched existing image generation models including scorebased methods. Additionally PFGMs generation process is much faster than previous methods. Strengths Innovative approach inspired by Poissons equation in physics Efficient generation process Augmented dimension prevents data collapse Weaknesses May perform better on larger datasets requires testing on a third dataset Comparison with ViTbased generative models is missing Questions Further investigation of PFGM on a third dataset is recommended. Comparison with ViTbased image generation models would be informative."], "yQDC5ZcqX6l": ["Paraphrased Summary: A novel biclustering approach BICOT and its regularized version BICOT\\lambda are introduced. BICOT generalizes multiple existing biclustering models and can be simplified to an optimal transport problem under specific conditions. It demonstrates comparable computational complexity to COOT but superior performance. Experiments show that BICOT\\lambda outperforms stateoftheart methods on various datasets. Strengths and Weaknesses: Strengths: Generalizes and reduces to several existing biclustering models. More efficient in terms of complexity and memory. Demonstrated effectiveness through theoretical and experimental evaluation. Weaknesses: Determining the number of clusters relies on external metrics. Unclear if hyperparameters were crossvalidated independently. Baseline methods hyperparameter settings are not provided. Some comparison methods may not have been fully optimized. Questions: Clarify the relationship between Equations (8) and (9) and the role of L. Compare L in COOT and BICOT discussing any similarities or differences. Explain why image comparisons with COOT are not possible despite its experiments on image datasets. Provide insight into how the choice of L affects BICOTs results. Explore ways to determine an optimal L for a given quality measure. Analyze the effects of a dense L on the algorithms asymptotic complexity. Determine the impact of the regularization function in BICOT\\lambda. Describe how kmeans clustering hyperparameters were chosen. Typos: Line 99: Remove \"Z\" from L(b)ij. Line 101: Replace \"between\" with \"between the\" in the second instance. Line 191: Correct to \"in is.\" Line 280: Change \"till\" to \"still.\"", "Paraphrase: Summary This paper presents a general framework for identifying clusters within data (biclustering) using a technique known as \"optimal transport.\" Two methods are developed within this framework: one that produces distinct (hard) clusters and another that produces gradual (fuzzy) clusters. The methods are tested using six benchmark datasets demonstrating their effectiveness and efficiency. Strengths and Weaknesses Strengths: Originality: The proposed methods use optimal transport a novel approach in biclustering. Clarity: The paper is wellwritten overall. Weaknesses: (1) It is unclear why the proposed methods perform better than existing approaches. (2) Crucial details of the experiments are missing such as parameter selection and comparisons to other methods. Significance The paper contributes to biclustering by leveraging optimal transport and demonstrating its effectiveness in solving the problem for \"dyadic data\" (binary data). Questions 1. The introduction lacks clarity and formalism making it difficult to understand the limitations of existing biclustering methods. 2. It is unclear if the original constraints are included or ignored when adding the rank constraint. 3. The definition of the \"antiadjacency matrix\" is arbitrary and lacks a rigorous mathematical basis. 4. The dimensions of the matrices r and c appear to differ which raises questions about the validity of their mathematical operations. 5. The use of the same notation (C) for both the solution matrix and the assignment matrix can lead to confusion. 6. It is unclear how the objective functions (8) and (9) are related. 7. The reported time complexity of CCOT in this paper differs from that presented in an earlier study. 8. Details of how to choose the parameters r and c and determine the rank k are missing. 9. The small number of experimental runs (only 10) and the observation of zero standard deviation in some cases raise concerns about the reliability of the results.", "Summary The authors propose a novel biclustering algorithm for dyadic data that combines the block seriation problem with optimal transport (OT). They formulate an optimization problem BCOT which can be solved to obtain biclusters i.e. simultaneous clustering of rows and columns. In certain cases the solution of BCOT allows for the calculation of an approximation of the optimal transport map with bounded rank. Additionally they introduce a fuzzy variant BCOT\u03bb and connect BCOT to existing biclustering algorithms. Strengths Originality: Combining the block seriation problem with OT for dyadic data is a novel idea. Quality: The paper provides proofs for its propositions and the code is publicly available. Clarity: The paper is wellstructured and contextualizes the proposed method in relation to existing work. Significance: The proposed method outperforms existing OTbased biclustering methods for documentterm clustering. Weaknesses Originality: While the method is novel it relies heavily on previous work and its originality mainly lies in its improved performance for dyadic data. Quality: Some statements are vague and there are suspected errors in some formulas. The motivation for the work beyond the trendiness of OT is not clearly stated. Clarity: The paper lacks selfcontainment requiring readers to refer to other works. The train of thought is not always clear. Significance: The overall significance of the results is not fully established. The methodology of the empirical evaluation has some concerns including the choice of datasets baselines and metrics. Minor Remarks Structure: Consider restructuring the introduction and using pointers to sections. LanguageGrammar: Avoid long sentences and improve precision in language. Questions Explain the row and column exemplar distributions and their relation to the weights. Provide details on the approximation of the optimal transport map and the rank of Z and W. Justify the time complexities reported for the baseline methods and clarify the restricted class of cost functions considered. Consider using simulated data to establish ground truth and assess the method. Explain the choice of datasets and baselines for the empirical evaluation. Discuss applications and significance of the work in the context of dyadic datasets."], "qHGCH75usg": ["Summary: This paper introduces a novel method for intrinsic exploration rewards. It learns a latent representation of the environment and predicts future latent values. The reward is based on the error between predicted and actual future latents so states that are more difficult to predict receive higher rewards. This method shows impressive performance on Atari games and DMHARD8. Strengths: Excellent performance on Atari and DMHARD8 benchmarks. Simpler approach compared to previous methods like GoExplore and Agent57. Weaknesses: Lack of standard information on benchmark evaluations particularly for Atari: Sample complexity (number of frames) is not stated. Stochasticity injection method (noops sticky actions) is not specified. Comparison to RND shows potentially lower performance but its unclear why.", "Paraphrase: The authors introduce a method inspired by \"Bootstrapyourownlatent\" that enables model learning to produce intrinsic rewards for exploration. This approach utilizes two parallel networks: one predicts future actions by k steps and the other is an exponentially moving average of the first. The predictions are done on a latent space using a cosine loss function for training. The difference between the network predictions serves as the reward signal for the reinforcement learning agent encouraging exploration. Strengths: Clear and intuitive theoretical basis Wellpresented and easy to understand Extensive experimental evaluation including firstperson environments and challenging exploration problems in thirdperson Atari games Weaknesses: Not a groundbreaking algorithmic development Missing citations to related works in literature Questions: How does the method perform in the presence of distracting elements Additional experiments with noisy environments would be beneficial.", "Paraphrase: This research expands on the BYOL approach from 2020 evaluating its use in curiositydriven exploration. It develops a world model using an RNN and a selfsupervised loss mechanism. This mechanism potentially combined with an external task loss trains an RL policy to play Atari and DMHard8 games. The research demonstrates superhuman performance on the ten most challenging Atari games and favorable exploration results on Montezumas Revenge. Notably it does not rely on human input. The study compares the approach to other curiositydriven methods using the metric of mean CHNS (clipped human normalized score). In various experiments the method outperforms baselines achieving superhuman results with extended sequence training. Additionally the exploration results on Montezumas Revenge demonstrate the methods ability in a purely intrinsic exploration setting where it explores more rooms compared to baseline methods. Strengths: The paper presents a solid and wellwritten idea. Weaknesses: The concept of forward prediction and BYOL are not novel but combining them with an RNN is logical. Quality: The research is of high quality with no significant flaws or experiments needing repetition. Clarity: The paper is clear and wellwritten. Significance: The results are not groundbreaking but are important for the selfsupervised learning and curiositydriven learning communities. BYOL has gained attention and this extension is valuable for researchers in these areas. Questions: An inquiry remains regarding the sensitivity of the lambda parameter in the DM8 experiments. Further investigation of model performance on larger DMHard8 instances would be valuable if feasible.", "Summary Paraphrase: This study introduces BYOLExplore a novel exploration technique for reinforcement learning (RL). It uses an encoder trained with BYOL to generate intrinsic rewards for exploration. BYOLExplore combines a closedloop RNN for past interaction history an openloop RNN for future prediction and a target encoder obtained by averaging previous encoder weights. By minimizing the difference between the RNN prediction and the target representation the encoder is trained. Experiments show that BYOLExplore outperforms existing baselines RND and ICM. Strengths and Weaknesses: Strengths: Adaptable to existing RL algorithms Utilizes temporal information in observations Intrinsic reward based on latent representation capturing transition dynamics Weaknesses: Marginally contributes to the field of fitting transition models and using training loss as intrinsic reward Questions: 1. Computational efficiency and training time of BYOLExplore compared to ICM and RND 2. Adequacy of BYOLExplores intrinsic reward in capturing epistemic uncertainty 3. Preference for actions distant from initial observations due to intrinsic reward accumulation over trajectory steps"], "w0QoqmUT9vJ": ["Paraphrased Statement: This research presents a comprehensive family of graph neural network (GNN) architectures called kOSANs which generalize and unify various recently developed GNNs. kOSANs utilize message passing on all ordered subgraphs of up to k vertices in the input graph. Strengths: Provides theoretical insights into the expressive power of kOSANs proving that they are comparable to kWL but less powerful than (k1)WL. Allows for efficient implementation due to message passing and independent subgraph processing. Innovatively employs ordered subgraphs while maintaining permutation invariance in the overall architecture. Introduces a datadriven subgraph sampling approach to address the challenge of selecting subgraphs for effective performance. Weaknesses: Limited experimental results mainly comparing random sampling with datadriven sampling. Performance on ZINC dataset (assuming edge features) falls short compared to other message passing GNNs. Questions: 1. Consider explicitly mentioning that the theoretical results provide upper bounds on the expressive power of many existing methods emphasizing the significance of this finding. 2. In the related work section acknowledge that some previous studies have demonstrated the ability to compute functions beyond 2WL and modify the statement to reflect this. 3. Clarify the O(n klog k) algorithm for computing the maximum a posteriori (MAP) of the subgraph sampling distribution for ordered subgraphs. 4. Discuss the diversity loss used in IMLE sampling in the main paper. 5. Provide an explanation of the discrepancy in units used for MAE values reported for Alchemy and QM9 datasets and suggest a method for comparing them to other works.", "Paraphrase: Summary: This paper introduces a subgraphbased Graph Neural Network (GNN) algorithm called kOSAN. It demonstrates that increasing the subgraph size enhances the algorithms expressive power. kOSAN has limited power compared to (k1)WL but is incomparable to kWL. To overcome arbitrary subgraph selection the paper proposes a datadriven variant of kOSAN. Experiments show that datadriven subgraph selection outperforms random sampling in prediction accuracy. Subgraph GNNs have emerged as a promising approach and this paper provides strong theoretical underpinnings. Strengths: Introduces a new subgraph GNN model with guaranteed theoretical expressive power. Weaknesses: While the theoretical results are robust the experimental evaluation is limited. The authors only show the superiority of the datadriven approach over random sampling but they do not compare the proposed subgraph GNN to other existing GNN implementations considering the added complexity of the proposed method. Questions: 1. The papers structure could be improved by providing a highlevel overview of the proposed approach before delving into detailed mathematical formulations. 2. Empirical results even on synthetic data would be beneficial to demonstrate the higher expressive power of kOSAN. Minor Corrections: Define t(g) at line 154.", "This study introduces a unifying framework Ordered Subgraph Aggregation Network (OSAN) for various Graph Neural Networks (GNNs). The framework leverages subgraph information in various forms. Most existing GNNs can be expressed through OSAN which operates by: 1. Selecting a kvertex tuple as an anchor. 2. Inducing a kordered subgraph and initially coloring each vertex based on the subgraphs atom type (similar to isomorphism type) along with the ktuple anchor. 3. Running a conventional GNN over the graph for each possible ktuple anchor. 4. Aggregating the results into a final representation. The study demonstrates that kOSANs form a hierarchy and can simulate popular subgraphenhanced GNN architectures. The framework provides a theoretical understanding of the expressive power of these architectures. Due to the high computational complexity of kOSANs a practical implementation is proposed where only a handful of anchor subgraphs is used. These subgraphs are selected in a datadriven manner by sampling from a learnable subgraph distribution using the perturbandMAP framework. Experimental evaluations show that datadriven sampling outperforms uniform random sampling and provides improvements over nonsubgraphenhanced GNNs. Strengths: Unifies diverse GNN architectures under the OSAN framework. Provides theoretical insights into the limitations of subgraphenhanced GNNs. Introduces a learnable algorithm for subgraph sampling. Weaknesses: Complex and dense writing style making it difficult to follow. Limited experimental evaluation with only a few baselines and datasets. Datadriven subgraph sampling requires selecting the number of subgraphs and vertices per subgraph which can be challenging.", "Paraphrased Summary: Graph representation learning particularly using graph neural networks (GNNs) has made significant progress recently. This research introduces a framework that combines ideas from subgraphenhanced GNNs to create expressive graph representations. The framework includes both a unified framework (linking recent work) and a method for learning to sample from subgraphs using techniques that allow backpropagation through discrete distributions. Strengths: The approach to learning subgraph selection using data is innovative. The paper is generally wellwritten and clear. The propositions and theorems seem valid. Weaknesses: Performance on the ZINC dataset is somewhat weaker than ESAN and no comparisons are made with datasets used in related studies (e.g. MUTAG Proteins PTC). Is there a specific reason for this omission Despite theoretical proposals subgraph sampling is implemented using unordered subgraphs (as mentioned in section 5 and appendix F.1). Can you provide results for the proposed framework at least on a sample synthetic dataset Post Rebuttal Updates: After reviewing the authors rebuttal which includes justification for smaller datasets ordered experimental results and a timing table the reviewer has decided to improve the scores. Questions: 1. Can you provide timings for other datasets as well 2. In line 161 of page 4 do you mean \"union\" or that both strategies work What are the practical performance differences between the two"], "lgj33-O1Ely": ["Paraphrased Summary: This research aims to recreate a wholebody human model from multiple selfshot videos focusing on specific body parts including the face hands and body. By utilizing multiple videos of different body parts this method enhances local details though with the drawback of requiring more training time and extra videos. Experimental findings demonstrate the effectiveness of the proposed framework and its individual components. Strengths: The proposed network can leverage information from various part videos to improve local details. The partbased modeling approach effectively enhances details in complex models like humans. A unified model is generated by transforming sample points from observation to canonical space using partbased deformation fields followed by geometry and appearance composition. Superiority of the approach is demonstrated through extensive experiments and comparisons. New datasets are introduced for evaluating partbased models benefiting the computer vision and graphics communities. Weaknesses: Nonrigid ray transformation may not completely eliminate unnatural color artifacts. Prior posedependent deformations can limit the application of the model in animation. Questions: More qualitative comparisons and analysis are requested regarding the use of nonrigid ray transformation. Further discussion on the limitations of posedependent deformations is desired to improve the models applicability in animation.", "Summary This paper aims to create realistic human body reconstructions with highly detailed hands and faces using separate hand and face video recordings. Strengths The reconstruction method is wellstructured combining geometry and appearance effectively. Results show improved visual quality compared to existing methods. A new dataset (SynTotalHuman) has been developed. Weaknesses The reliance on parametric models (e.g. SMPLH) may limit the technical significance of the work. The finer detail in the hands may be primarily attributed to the SMPLH model rather than the proposed method. The examples shown only feature bare hands raising concerns about the effectiveness of the method for hands with clothing or other objects.", "Paraphrased Statement: The study introduces a technique that uses only monocular selfportrait videos to generate fullbody avatars. The technique focuses on modeling the body head and hands separately and then fusing their neural volumetric renderings. Experiments show that the proposed method outperforms existing methods. Strengths: Produces detailed and realistic avatars under different views and poses. Compared favorably against similar techniques and thoroughly evaluated. Weaknesses: While the FLAME model is used for the head its full capabilities may not be leveraged. Facial expressions remain static leading to unnatural results. The technique lacks detailed explanations including an undefined \"Lbody loss\" and the unclear role of the latent code. The texture appears static across views due to the nonrigid ray transformation. Its impact on the stiffness of the results is unclear. The techniques performance compared to traditional graphics pipelines is not discussed. Questions: Can the authors address the weaknesses mentioned above How does the technique compare to traditional deformation transfer methods"], "plu6AK3qs5T": ["Paraphrase: Summary The authors devise a new clipping method (clipping all gradients regardless of magnitude) due to the frequent incidence of clipping in realworld datasets. The method requires minimal hyperparameter tuning and is as efficient as DPSGD with gradient clipping. The authors theoretically analyze the convergence of DPSGD with the proposed clipping method for nonconvex problems. Empirical results on realworld datasets showcase the methods efficacy. Strengths and Weaknesses The theoretical analysis overstates the methods advantages slightly. The authors rely on a strong assumption (Assumption 5.3) to analyze convergence which may not hold true in certain scenarios such as linear regression. As a result their claim of the method matching the convergence rate of SGD is somewhat exaggerated. The AUTOS method incorporates the clipping threshold R into the learning rate \u03b7 eliminating the need to adjust R. However this may necessitate more careful tuning of the learning rate \u03b7 as R is indirectly included. While the empirical results are valuable the theoretical contribution is somewhat diminished by the reliance on a strong assumption. This may compromise the papers acceptance at NeurIPS. Minor Questions 1. What do the numbers in the Figure 1 heat map represent Questions 1. Can Assumption 5.3 be relaxed 2. Does AUTOS necessitate more meticulous tuning of \u03b7 3. Do Abadis clipping and AUTOS employ varying learning rates in the experiments", "Paraphrased Statement: Gradient clipping is necessary in Summary DPSGD to control the sensitivity of the output information function. However tuning the clipping bound can be challenging as it significantly affects network performance. This work proposes replacing gradient clipping with gradient normalization to address this issue. Strengths: Clear and concise presentation. Simple and practical automatic tuning method. Extensive and persuasive experimental results. Weaknesses: Layout issues (e.g. broken link in Figure 6 incorrect figure reference in line 151). Limited exploration of the \"gamma\" hyperparameter. Questions: While the authors claim the network is insensitive to the choice of \"gamma\" the ablation study only involves one epoch of training. A more thorough evaluation over the full training process would provide a better understanding. The automatic clipping method may distort gradient distributions even when gradients are within the clipping bound. Tuning parameters that control this distortion (e.g. \"gamma\") may still be necessary.", "Summary: Differentially private SGD (DPSGD) includes gradient clipping and noise addition to protect privacy. The clipping parameter and noise scale determine the privacy level. By adjusting the noise scale based on the privacy budget the clipping parameter becomes a hyperparameter that needs tuning. This paper proposes removing the need to tune the clipping hyperparameter. Strengths: The proposed method removes the need to tune the clipping norm parameter. The private gradient becomes independent of the clipping norm requiring only the choice of a different parameter. Weaknesses: Another method exists that decouples learning rate tuning from clipping norm tuning. Significance: The contribution of this work lies in simplifying the tuning process for DPSGD. The paper lacks clarity on the magnitude of the problem posed by tuning the clipping parameter in practice. The authors could provide ruleofthumb guidelines for choosing the clipping parameter based on learning rate and architecture. Questions: How does the proposed method compare to the decoupling method in [1] Could the authors elaborate on the extent to which tuning the clipping parameter is problematic in practice How sensitive are the NLP and ImageNet tasks to learning rate without privacy Does the choice of architecture affect the sensitivity to clipping parameters Why are the ablation studies in Figure 14 performed on different datasets than Figure 1", "Paraphrased Statement: This paper introduces a new approach for automatic gradient clipping in differential privacy eliminating the need for an additional parameter. This reduces the number of experiments required to tune differential privacy parameters. The paper provides both theoretical analysis and practical experiments to demonstrate the convergence of the proposed optimizer. Strengths: Aims to reduce the cost of differential privacy training. Wellwritten paper with intuitive explanations and illustrative experiments. Demonstrates strong performance on realworld models and datasets using the proposed AUTOSV clipping method. Weaknesses: Normalizing gradients per example is similar to setting a very small clipping threshold (which conflates the effects of this threshold and another parameter). Lack of a study comparing AUTOSV to vanilla DPSGD clipping with varying clipping thresholds and AUTOSV parameters. Questions: Can the authors conduct a study comparing AUTOSV to vanilla DPSGD clipping on a dataset where AUTOSV performs better Can the authors explore the impact of adjusting the clipping threshold and AUTOSV parameters in this comparison"], "kImIIKGqDFA": ["Paraphrased Statement: Summary: The paper introduces a novel approach called Adaptive Gradient Variance Modulator (AGVM) for training models with large batch sizes. AGVM is straightforward to implement and wellreasoned. Experiments on various dense prediction tasks including object detection instance segmentation and panoptic segmentation demonstrate AGVMs effectiveness. Notably the paper claims the ability to train with a batch size as high as 10000. Strengths: Simplicity and ease of implementation Applicability to a wide range of visual prediction tasks Weaknesses: Performance degradation with increasing batch size particularly noticeable in object detection tasks Inconsistent speedup ratio with increased GPU utilization Questions: 1. Table 2 reveals a significant performance drop when using batch sizes of 1024 and 512 compared to 32 and 256 especially in object detection. This performance loss cannot be ignored in largebatch size settings. 2. The abstract suggests batch size scalability to 10000 but the experiments in Table 5 do not fully support this claim. Firstly the training uses a small backbone (ResNet18). What is the performance with a more widely used backbone such as ResNet50 Additionally how do other detection methods compare to RetinaNet Furthermore the reported results for a batch size of 10000 are significantly lower than those for a batch size of 32. 3. Table 4 only reports training time without convergence information. Moreover the training speedup becomes less consistent with increased GPU utilization. For instance 1536 GPUs yield a training time of 4.2 minutes while 32 GPUs require 148 minutes.", "Summary This paper introduces the Adaptive Gradient Variance Modulator (AGVM) for largebatch optimization of dense visual tasks like object detection and segmentation. The AGVM addresses the issue of inconsistent gradient variances across modules in these tasks. To stabilize optimization the authors propose AGVM which evens out gradient variances and implements moving averages. AGVM has both theoretical underpinnings and experimental validation of its efficacy. Strengths and Weaknesses Strengths: Clear and wellwritten paper Novel AGVM method with theoretical justification Extensive experimental evaluation across tasks and optimizers showing comparable or superior performance to previous optimization methods Weaknesses: 1. Anchor Module Selection: Why is the backbone chosen as the anchor for AGVM Can other modules be used without training failures 2. Batch Size Impact: Table 8 shows results for specific batch sizes but it omits those for other sizes like 128 and 512. AGVMs training time is reduced by 20 times when the batch size increases 30 times from 32 to 960 but it does not achieve linear speedup. The reason for this and the contribution of data loader IO should be clarified. 3. AdamW Optimizer: Results for AdamW are missing in Table 3 despite its effectiveness in small batch sizes. 4. Gradient Variance as Root Cause: To support the claim that high gradient variance causes largebatch dense visual training failures a comparison of gradient variance iterations with and without AGVM (similar to Figure 1) would be beneficial. 5. Related Work: A fair comparison and discussion with the related work in [r1] which also addresses gradient variance misalignment in largescale DNN training should be included. 6. Theoretical Analysis: The rigor of Eq (7) in the appendix which is currently presented as a numerical observation rather than a theoretical analysis should be improved. 7. Theoretical Contribution: The theoretical contribution could be strengthened by providing a formal guarantee for AGVMs linear speedup with respect to the number of workers or minibatch size.", "Paraphrased Statement: This study addresses the significant performance degradation experienced during large batch training in object detection and segmentation. To address this problem the authors introduce Adaptive Gradient Variance Modulator (AGVM) an algorithm designed to handle exceptionally large batch sizes. The method has been validated through comprehensive experiments on MS COCO and ADE20K datasets demonstrating its effectiveness. Strengths and Weaknesses: Strengths: Clear and structured presentation Wellexecuted experiments showcasing the methods compatibility with various deep learning architectures and optimization algorithms on MS COCO and ADE20K Weaknesses: Potential sensitivity of \\mut(i) to \\Phit(i) in Equation (4) leading to large \\Phi values when Gt1i and Gt2i are similar. Lack of justification for the exclusive focus on object detection and segmentation tasks despite the potential applicability to image classification. Absence of discussion on the methods effectiveness in addressing imbalance issues in object detection. Questions: Can the proposed method be extended to image classification tasks Is the method robust to the class imbalance challenges encountered in object detection", "Summary Authors investigate the issue of largebatch optimization for complex \"dense\" computer vision tasks like object detection and instance segmentation. Their aim is to train these models with very large batches using multiple GPUs. To achieve this they examine the behavior of gradient variance and observe that the variance between different submodules (e.g. detector backbone vs FPN) becomes mismatched during training with large batch sizes. Based on this they propose Adaptive Gradient Variance Modulator (AGVM) an optimizeragnostic technique designed to equalize gradient variance across submodules. Experiments on multiple vision tasks show that AGVM can handle extremely large batch sizes. Strengths and Weakness The paper offers a practical solution for largebatch training supported by compelling experiments and hyperparameter sensitivity analysis. By expanding the applicability of largebatch training authors not only enable training models in minutes on GPU clusters but also make it feasible to train \"dense prediction\" tasks outside of compute clusters such as in federated learning or volunteer computing where largebatch training helps mitigate high communication latency. However there are concerns regarding the papers positioning and quality. Questions 1. Is AGVM specific to dense prediction tasks or can it be applied to other modular tasks 2. What exactly does the \"extra overhead\" reported in Table 1 represent How does it scale with network bandwidth and the number of GPUs Minor Comments Typos: L159: \"any ... platform e.g. PyTorch\" or \"the ... platform i.e. PyTorch\" L192: \"Pytorch\" vs \"PyTorch\" Figure 1: Consider using different color schemes to avoid undue associations. Table 3: Explain why batch 1536 is highlighted in bold. The deliverables are not currently available at the provided link."], "n6QYLjlYhkG": ["Summary Paraphrase: This paper introduces a hybrid approach that combines an autoregressive model and an energybased model to model event sequences. Unlike previous methods that solely use autoregressive models this approach trains an energybased model to rank samples generated by the autoregressive model. The sample with the highest weight among a set of candidate samples is then selected as the models output. To train the energybased model two objective functions (BinaryNCE and MultiNCE) are employed which assign lower energy to true sequences and higher energy to noise. Additionally a regularization term is introduced to enforce a margin based on \"edit distance\" between the actual and predicted outputs. The proposed approach demonstrates superior performance compared to existing methods on the Taobao and Taxi datasets achieving results that are twice as good in terms of RMSE and OTD metrics. Strengths and Weaknesses Paraphrase: The paper is wellwritten and presents impressive empirical results. However the reviewers raise several questions and concerns: Major Questions: The dataset size used to train the transformer model is relatively small. The reviewers inquire about any specific techniques employed to achieve good performance with limited data. The two larger models (2x in size) yield the same results as the smaller baselines. The reviewers seek an explanation for this observation. An alternative to training a separate energybased model could be to assign weights using the likelihood of the autoregressive model itself. The reviewers ask for results using this approach. The paper acknowledges the similarities between its approach and previous textgeneration methods. The reviewers suggest that the contributions of the paper be more clearly stated specifically the architectural modifications made to adapt the hybrid model to event sequence data. More details are requested to clarify the importance sampling method output distribution and how the integral over time is approximated. Minor Questions: The integral in Equation 56 is missing. The reviewers ask whether increasing the number of samples improves the results. They point out that the regularization term uses the test metric in optimization and question whether this could bias the results. They inquire about the effect of class imbalance in the BinaryNCE objective. The reviewers suggest that the framework could be viewed as a GAN with the autoregressive model as the generator and the energybased model as the discriminator.", "Summary Paraphrase: The paper presents a twopart model for generating event sequences focusing on longterm forecasting. The first part is an autoregressive model for continuoustime event data. However such models have been shown to lead to errors that accumulate over longterm predictions. To address this the second part includes an \"energy function\" that reweights the probabilities generated by the autoregressive model favoring sequences that are globally coherent and plausible. The authors provide training methods for this model and compare its performance to other models on two datasets. Strengths and Weaknesses Paraphrase: Strengths: The paper is wellwritten and easy to understand. The experiments are thorough and explore different aspects of the model. The model consistently outperforms baselines in longterm forecasting. Weaknesses: The authors claim that autoregressive models fail at longterm forecasting due to cascading errors but provide no supporting evidence. The energy function is trained by minimizing a loss that assumes all autoregressive model generations are incorrect which seems overly strong. The model generates sequences by selecting the \"best\" from multiple autoregressive model generations. It is not clear if baselines use this same technique and whether it could improve their performance. Questions: 1. Can the authors provide evidence for cascading errors in longterm event forecasting 2. How does the noisecontrastive training of the energy function promote global coherence and plausibility in generated sequences 3. Do baseline autoregressive models also use \"bestofM\" decoding and if not why not", "Paraphrased Statement This research introduces a new model called HYPRO which combines two models to enhance the prediction of future event sequences over long periods: Autoregressive models (NHP and AttNHP): These models generate potential predictions. Energy function (Transformer): This function assesses the plausibility of the predictions and weights them to produce realistic outcomes. HYPRO can be trained using two different loss functions. Additionally a novel regularization technique is proposed to ensure that predicted sequences differ significantly from random noise. In experiments on two datasets HYPRO outperforms existing methods. Strengths: HYPRO combines two models for event sequence prediction which is a novel approach. A unique regularization technique ensures meaningful sequence differences. Experiments demonstrate the effectiveness of multiple training objectives. HYPRO achieves superior performance on benchmark datasets. Experimental details are clearly presented in the appendix. Weaknesses: The core concept of HYPRO has been explored in text generation limiting its novelty. The regularization techniques improvements are not statistically significant. The experiments primarily focus on short prediction horizons despite the stated goal of longterm prediction. The computational efficiency benefits of HYPRO are not quantified in experiments. Questions: Why were additional benchmark datasets omitted from experiments How do HYPROs training and inference times compare to baseline methods"], "uytgM9N0vlR": ["Paraphrased Statement: This study systematically evaluates the performance of pretrained graph neural networks (GNNs) under various settings. They analyze the impact of key factors such as training objectives data partitioning input features dataset size and GNN architecture. Their findings show that selfsupervised pretraining does not consistently improve GNNs over nonpretrained models. While some improvements are observed with additional supervised pretraining they tend to diminish with richer features or more balanced data. The researchers suggest that the complexity of moleculebased pretraining may limit the transferability of knowledge for downstream tasks. Strengths: Original idea to benchmark GNN pretraining across multiple settings. Comprehensive study of various factors affecting pretraining performance. Clear and wellorganized presentation. Weaknesses: The choice of downstream datasets (BBBP BACE TOX21 TOXCAST SIDER) for evaluation may not be representative enough. Lack of insights into why graph pretraining is ineffective in many settings and succeeds in some. Questions: 1. Why were specific downstream datasets (BBBP BACE TOX21 TOXCAST SIDER) used Are there reasons for not considering other popular datasets like ClinTox MUV or HIV 2. Can the authors provide empirical analysis to explain the factors contributing to the success or failure of graph pretraining in different settings", "Paraphrased Statement: Summary: This study investigates the effectiveness of pretraining Graph Neural Networks (GNNs) for molecular representation. It examines various aspects including objectives data splitting features dataset size and GNN architectures. Findings: Selfsupervised pretraining alone does not significantly improve downstream molecular property prediction tasks. Data splitting input features and hyperparameters may be more critical than pretraining strategies. The undercomplexity of molecular graph pretraining may hinder performance. Strengths: Clear presentation and informative results. Weaknesses: Limited evaluation to molecular property prediction. Instability of the MoleculeNet benchmark may bias conclusions. Evaluation only includes basic pretraining strategies and augmentations. Conclusions about pretraining inefficacy may not be definitive due to insufficient exploration of advanced techniques and augmentation strategies. The hypothesis about the role of complexity in molecular pretraining requires further substantiation.", "Paraphrase: Summary: Selfsupervised pretraining of Graph Neural Networks (GNNs) on molecular data has been proposed to improve downstream tasks in computational chemistry such as drug discovery. However a recent study challenges this claim arguing that current SSL methods do not provide significant advantages over nonpretrained models. Strengths: The study systematically tests different factors in the pretraining pipeline split downstream task data by scaffolds (improving generalization evaluation) and investigates the impact of rich features. It reveals surprising results such as no significant performance improvement from pretraining methods. Weaknesses: Aside from minor formatting issues the study appears sound. It complements previous positive and negative findings on GNN pretraining promoting diversity in experimental settings for community benefit. Questions: Are the two pretraining datasets commonly used in the methods investigated Could the datasets be less suitable for pretraining contributing to the lack of observed gains", "Paraphrased Statement: Summary: This study thoroughly examines the pretraining of Graph Neural Networks (GNNs) for predicting molecular properties. It investigates various factors such as features architectures datasets objectives downstream tasks and hyperparameters. The findings indicate that selfsupervised pretraining alone usually provides minimal improvement while supervised pretraining is beneficial when downstream tasks lack extensive features or have unbalanced data splits. Strengths: The study is groundbreaking methodical and comprehensive examining numerous design choices used in previous research. The key findings particularly the observations in section 5 provide valuable insights for future studies and are wellsupported by experiments. The paper is clearly written and accessible to readers. Weaknesses: The specific contrastive method assessed is not the most optimal. Research shows that data augmentation techniques are critical for biochemical molecules and the current study employs attribute masking which may not be the ideal method. The experiments do not explore combinations of selfsupervision objectives. The study lacks investigations into more extensive architectures (e.g. deep GNNs and transformerbased GNNs). Questions: Including another contrastive learning baseline using node dropping and subgraph augmentation would enhance the study. Evaluating performance using multiple or all selfsupervised pretraining objectives would provide additional insights. Incorporating more scalable architectures into the experiments would expand the scope of the research.", "Paraphrased Summary: This paper investigates the impact of using selfsupervised Graph Neural Network (GNN) pretraining on molecular graphs. The research team evaluated the effectiveness of different popular pretraining objectives and discovered that the benefits are often minimal. Additionally they found that while pretraining may lead to improvements these gains can diminish when using richer features or more evenly distributed data splits. The study emphasizes that hyperparameter selection can significantly influence the accuracy of downstream tasks potentially outweighing the impact of the pretraining approach. The paper provides detailed guidance on when pretraining may and may not be beneficial based on experimental results. Strengths and Weaknesses: Strengths: The research question is relevant and explores an important problem in GNN research. The paper presents a comprehensive evaluation through extensive experiments to support its claims. The findings challenge previous assumptions about the benefits of selfsupervised pretraining in GNNs. Weaknesses: The findings are limited to small graphs and it remains unclear if the results would generalize to GNNs trained on graphs from different distributions. Questions for the Authors: Have the authors considered if the findings would apply to GNNs pretrained on graphs from different distributions such as more general domains Minor Edits: Line 18: Remove the extra space after \"GNNs\". Line 21: Correct the typo \"nurturing\" to \"neutral\". Line 184: Correct the typo \"scenarios\" to \"cases\"."], "rTvH1_SRyXs": ["Paraphrased Statement: The authors introduce the Local Function Approximation (LFA) framework demonstrating that it encompasses many existing local explanation methods. Their \"No Free Lunch Theorem of Explanations\" establishes that no explanation method can consistently outperform others for all instances. This is supported by experiments across various datasets and models. Strengths: The research addresses the crucial problem of unifying explanation techniques. The authors build on prior work and provide a formal framework to strengthen their arguments. The evaluation considers several explanation methods and datasets. Limitations: The papers density and complexity make it difficult to comprehend. The authors are encouraged to clarify concepts and avoid condensed writing in favor of more detailed explanations particularly in Section 3. Questions: While the authors note similarities between LFA and LIME they do not address LIMEs feature selection process. Can they clarify if their findings hold even when feature selection is included The authors should provide a visualization similar to Figure 3 that demonstrates the effect of removing critical features on the permutation test results. Understanding the models ability to recover in such cases is crucial.", "Paraphrase: This paper proposes a framework for understanding black box ML models through \"local function approximation explanations\" (LFA). The authors provide theoretical insights that connect several existing explanation methods to LFA. Through both theoretical and practical experiments they demonstrate the usefulness of LFA. Strengths: The LFA framework is clearly presented. The theoretical results provide valuable understanding. The combination of theoretical and practical findings showcases the practical value of LFA. Weaknesses: The paper is somewhat limited in scope focusing on LFAbased explanations and using fidelity as the sole measure of explanation quality. However the authors explore this specific area of ML explanations in great depth.", "Paraphrase: Summary: This research introduces the Local Function Approximation (LFA) framework which aims to merge local feature importance explanations (using gradients and perturbations). Strengths and Weaknesses: The LFA framework has potential benefits. It offers a comprehensive view of explanation methods and provides a foundation for developing new approaches. It also promotes the concept of \"model recovery\" for explaining methods. However the frameworks practical value is uncertain. It remains unclear if the LFA can effectively guide practitioners in selecting an explanation method for specific datasets. Additionally some ideas in the LFA framework are not entirely novel in the explainability field. Questions: a) What are some practical applications of the LFA framework for data scientists b) Can the LFA framework address limitations in existing explanation methods c) How can the LFA framework help determine which explanation method is most suitable for debugging models or detecting bias d) How does the LFA framework enable the evaluation of different explanation methods that it encompasses", "Paraphrased Summary: This paper introduces a generalized framework (LFA) that encompasses various local explanation techniques. Using this framework the authors argue that no single explanation method can effectively perform local analysis across all scenarios due to the \"no free lunch\" theorem. Strengths: Easytounderstand and supported by experimental results. Covers a comprehensive range of local explainers within the LFA framework. Weaknesses: The \"no free lunch\" theorem is not considered particularly novel as previous research has demonstrated the limitations of neighborhood samplers. The paper lacks additional results to show LFAs robustness against issues like overfitting. PostRebuttal: After reviewing the authors response the reviewer revised their score and asked for additional clarification on the specific insights provided by the \"no free lunch\" theorem that were not previously known."], "mNtFhoNRr4i": ["Paraphrase: The paper evaluates the effectiveness of hierarchical classification for deep learning models. It introduces a new scoring mechanism for internal nodes based on summing the softmax probabilities of the leaf nodes. Two loss functions are proposed for this scoring method. In Section 5 the paper describes a method for generating curves representing the relationship between node probabilities (p(y)) and varying thresholds (\u03c4). The curves visualize the accuracy and precision of hierarchical classification methods. The paper also includes experiments involving simulated unseen classes incorporated into the hierarchy. Strengths: Novel combinations of loss functions and the proposed method in Section 5 (if valid). Comprehensive evaluation of various classification methods. Relevant and interesting topic. Weaknesses: Limited readability and brevity. Extended journal version recommended. Unclear concept of C(y\\haty) (line 181). Insufficient explanation of equation (7) and the modifications made to deepRTC. Inadequate clarification of equation (17) and the specific cost function used. Complexity and potential errors in the Section 5 methodology: Double ordering criteria for sorting pairs. Ambiguous use of y in the definition of zk and its relation to \u03c4. Unclear process for merging the sequence of pairs into \u0394. Lack of explanation for the effectiveness of the method. Questions: Are the probabilities p(y) for all methods normalized If so over which set do they sum Detailed description of the curve generation process in Section 5. Clarification on the sorting criteria used for the pairs. Interpretation of equation (7) and equation (17) based on the specific cost function choice. The implication of giving higher weight to nodes where the prediction is on the path from y in equation (17). Additional Suggestions: Citing relevant predeep learning papers on hierarchical classification could enhance the work.", "Summary The study investigates hierarchical classification tasks where predictions can be made at varying levels of detail. Inspired by the ImageNet dataset the authors introduce a new metric based on \"Operating characteristic curves\" and use it to compare existing loss functions on the iNat21Mini dataset. They also propose two novel loss functions: \"softmaxdescendant\" which assigns scores to internal nodes in the classification hierarchy and \"softmaxmargin\" a softened version of the structured hinge loss. The authors analyze how different loss functions perform under various conditions including indistribution and outofdistribution data. Strengths Relevance to the research communitys interest in hierarchical classification. Weaknesses Lack of theoretical or practical justification for using \"Operating characteristic curves\" as a metric. Poor organization and clarity making it difficult to understand the authors contributions. Extensive discussion of previous methods without clearly establishing their relevance to the proposed metric and loss functions. Inclusion of the poorly performing \"softmaxdescendant\" loss function without clear motivation. Questions What are the main contributions and motivations of this work Why is the proposed metric important and how is it superior to existing methods What are the key insights gained from comparing different loss functions Given the poor performance of \"softmaxdescendant\" why is it included in the study", "Paraphrase: This study presents a novel method for evaluating hierarchical classifiers that predict nonleaf nodes at various operating points. The findings suggest that the widely used softmax classifier performs adequately in many scenarios though a structured classifier may excel in classifying unfamiliar classes. Additionally two novel loss functions are proposed one of which consistently outperforms the softmax classifier. The experiments were conducted using ImageNet and iNat classification tasks. Strengths: Clear and concise writing Extensive literature review Succinct yet informative comparisons with existing methods Significance: Evaluating hierarchical classifiers at multiple operating points is crucial for assessment. The study provides a valuable evaluation framework due to the lack of prior work in this area. Explanations of why multiple operating point evaluation is not widely adopted would enhance the significance. Weaknesses: Consider improving the clarity of key methods such as Eq17 (demonstrating its superiority) and the process of section 5 (using an algorithmic block). Questions: Are there limitations to Eq17 Could you provide data on its accuracy in the common classification setting that predicts only leaf classes"], "pD5Pl5hen_g": ["Paraphrased Statement: This research investigates an approach for optimizing a function with both smooth and strongly convexconcave characteristics. The proposed algorithm achieves the lowest number of gradient evaluations required for this class of problems making it the first optimal algorithm for such scenarios. Strengths: Meets the theoretical lower bound for gradient evaluations. The algorithms structure is theoretically optimal. Weaknesses: The implementation of the proposed algorithm may be complex. Unlike the Wang and Li (2020) algorithm it includes an additional logarithmic term. Suggestions: Conduct experiments to provide empirical support for the algorithms optimality.", "Paraphrase: Summary This study introduces an efficient firstorder algorithm for solving minimax optimization problems with both smoothness and strong convexityconcavity properties. The algorithm incorporates different levels of convexity and concavity to optimize performance. Strengths Provides an optimal algorithm for minimax optimization with varying convexity and concavity parameters. Uses a reformulation method that involves pointwise conjugate functions and MoreauYosida regularization to simplify the problem. Employs an Extra Anchored Gradient method to solve subproblems efficiently (O(1) complexity). Achieves a firstorder oracle complexity that matches the theoretical lower bound. Weaknesses Implementation of the algorithm is complex due to multiple loops and numerous parameters. Practical performance may be limited. Analysis considers only variations in convexity and concavity while the lower bound established by Zhang et al. (2021) also includes smoothness parameters. It is unclear if the algorithm remains optimal in that more general setting. Questions Can the proposed algorithm be efficiently implemented in practice How does the algorithm perform in more general settings considering different smoothness parameters as well", "Paraphrased Statement: This paper presents a new algorithm for solving a specific type of optimization problem involving both convex and concave functions. The algorithm involves converting the problem into a minimization problem and then using a modified version of the proximal point algorithm to solve it. The algorithm uses an inexact method for computing the proximal operator via an additional gradientbased method. Strengths: The problem being addressed is significant in machine learning. The paper is wellstructured and easy to understand. The proposed algorithm is innovative and achieves optimal complexity in terms of gradient computations. Weaknesses: A previous study by Wang and Li (2020) provided a more detailed analysis of the smoothness assumption. Their study assumes a different smoothness condition than the one used in this paper. It is unclear if the proposed algorithm remains optimal under this modified assumption. Questions: Can the authors clarify if the proposed algorithm is still optimal under the smoothness assumption used by Wang and Li (2020) What are the key challenges in analyzing the algorithm under this modified assumption", "Summary The paper introduces an algorithm for solving minimax problems of the form: min(xy) r(x) F(xy) h(y) where r and h are proximable and F is differentiable smooth and strongly concaveconvex. By converting the minimax problem into a strongly convex minimization problem the algorithm applies accelerated gradient descent to the Moreau envelope of the minimization problem. The gradient of the Moreau envelope is approximated using a generalization of the Extra Anchored Gradient method which reduces the complexity of the algorithm to a logarithmic factor. Strengths Improved complexity compared to baselines. Clear presentation of the algorithms main ideas and proof sketch. Novel reformulation of the minimax problem into a strongly convex minimization problem. Weaknesses Lack of discussion about related work and potential limitations. Missing related works that consider different Lipschitz smoothness parameters and solve problems with bilinear coupling. Proof and results are not fully rigorous. Numerical instability due to small values of mux. Complexity is overstated as additional logarithmic terms are missing from the proof. Questions Clarify the misleading title and message by acknowledging other minimax parameterizations and their impact on the algorithms optimality. Discuss the limitations of the parameterization studied in the paper and its impact on recovering optimal convex rates. Cite and compare the algorithm to related works on bilinear coupling and dualization. Provide a more precise proof with explicit calculations of the suboptimality gap. Address the numerical stability issue and consider alternative methods that achieve suboptimal linear convergence without instability. Explore the possibility of extending the formulation to nonstrongly monotone minimax objectives."], "lTZBRxm2q5": ["Paraphrased Summary: The paper introduces a novel approach to modeling neural stochastic differential equations (SDEs) using an alternative to traditional Brownian noise. This alternative noise is defined specifically and the authors demonstrate how standard neural SDE methods can be adapted to approximate the resulting stochastic process. They present theoretical results that support the use of these methods and prove convergence properties. The approach is then evaluated through various experiments. Strengths and Weaknesses: Strengths: Development of nontrivial theoretical results. Weaknesses: Theoretical: Unclear and potentially nonrigorous derivations. Equation (5) is approximated but treated as exact in subsequent derivations. Section 4.4 defines (12) as an approximation or equivalent to (4). Theorem 1 treats the Hurst exponent as constant though previously defined as a function. Lack of a clear summary of the proposed method for implementation. Experiments: Baseline performance in Section 5.1 is comparable to the proposed method. Proposed method in Section 5.2 does not fit the true function well. Section 5.3 shows overfitting of the proposed method. Section 5.4 acknowledges the need for exploratory findings but the extra cost of finding optimal parameters raises questions about the superiority of the proposed approach over standard methods. Questions: Can the authors address the specific concerns listed under \"weaknesses\"", "Summary Neural Stochastic Differential Equations (SDEs) are extended to include a class of nonBrownian driving forces. Discretizing the nonBrownian driver using a RiemannStieltjes method is aligned with an EulerMaruyama discretization of a separate SDE reducing it to a neural SDE with random coefficients. The randomness is specifically dependent on a Gaussian process which is sampled efficiently using sparse inducing points. Strengths Technical rigor: The paper proposes a novel approach to neural fractional SDEs by overcoming the scalability limitations of sampling fractional Brownian motion. Computational efficiency: The authors discuss the varying computational costs of sampling and decomposition methods emphasizing their importance in practice. Limitations: The authors acknowledge and clearly state the limitations of their approach. Weaknesses Accessibility: The paper assumes a high level of knowledge in stochastic processes and may not be selfcontained for all readers. Terminology: The term \"adjoint method\" is used ambiguously potentially causing confusion. Presentation: Some typos and improvements in mathematical clarity could enhance readability. QuestionsSuggestions for Future Work Comparison of the proposed approach with simpler neural SDEs with Brownian drivers. Improved presentation and accessibility of the results. Investigation of reasons for nondifferentiability of the fBM kernel and potential alternatives. Verification of the proposed methodology using larger datasets and smaller step sizes. Adaptation to higherorder SDE solvers and adaptive timestepping.", "Paraphrased Summary: This paper investigates a modified noise model for stochastic differential equations (SDEs) that incorporates a Hurst exponent. The Hurst exponent scales the Wiener process integration potentially making it equivalent to an Ito SDE with a different scaling parameter. The authors propose learning the Hurst exponent from data using Gaussian process sampling and variational inference. Strengths and Weaknesses: Weaknesses: Lack of clear motivation for the importance of learning the Hurst exponent. Insufficient description of the proposed method including the optimization problem parameterization and gradient propagation through samples. Overlapping nature of the Hurst exponent with other parameterizations of the scaling term in SDEs reducing the practical impact of the exponent. Minor clarity issues in the writing. Strengths: Exploration of a different noise model in SDEs. Questions: Why is learning the Hurst exponent valuable in practice How does it compare to methods that do not consider the Hurst exponent How can gradients be propagated through the sampling stage What is the significance of the Hurst exponent within the context of the model Are there any applications of the proposed noise model that illustrate its advantages", "Summary This study introduces a novel category of neural stochastic differential equations (SDEs) with conventional stepsizes. It develops an approximation technique for the noise integral in neural SDEs using sparse Gaussian Processes making them computationally feasible. Experiments demonstrate the methods effectiveness on sequential financial time series and image recognition tasks. Strengths 1. Provides a comprehensive mathematical exposition of previous work on neural SDEs aiding understanding for readers with limited experience. 2. Evaluates the method across diverse tasks including function reproduction financial data analysis and image recognition. 3. The GP method and overall presentation are accessible and straightforward. Weaknesses 1. While the results mention evaluation they lack quantitative comparisons particularly for image recognition and rely more on qualitative analysis. Numerical analysis would enhance the results. 2. The author highlights weaknesses in prior work but does not always clearly link them to the proposed solution. For instance in section 4.1 the issue of unscalable random variables in the method from [64] is discussed but its implications for the methods performance and the authors algorithms superiority are not fully explored. 3. GANbased SDE solvers which are more powerful models may be considered for comparison instead of latent SDEs. Additionally experiments using alternative approximation methods besides the Hurst function could be valuable. Questions 1. Are there any MNIST experiments that report accuracy metrics 2. In section 4.1 you state that for the method in [64] \"the number of random variables prevents optimized discretize methods to scale.\" Can you explain this issue in more detail and its potential negative consequences in practice How does your method address this problem", "Paraphrased Statement: This paper presents an improved version of the Neural SDE (NSDE) model by incorporating a more general noise source. Specifically a RiemannLiouville fractional Brownian motion (RL fBM) with a timevarying Hurst function replaces the standard Brownian motion in the NSDE. To enhance the computational efficiency sparse Gaussian process techniques are employed in sampling the RL fBM. Furthermore the resulting model is expressed as a standard NSDE with random drift and diffusion vector fields by using inducing points. A convergence analysis based on rough path theory supports this approximation. The paper concludes with various numerical experiments demonstrating that the proposed model performs on par with the existing NSDE approach can recover Hurst functions reasonably well and improves scorebased generative modeling with SDEs."], "vRrFVHxFiXJ": ["Paraphrased Statement: The study introduces chemCPA a novel model that predicts druginduced transcriptomic changes in specific cell lines. It utilizes a transfer learning approach to leverage large bulk RNAseq datasets and predict perturbations at the singlecell level even with differences in gene sets between the source and target domains. chemCPA employs a pretrained drug representation to generalize predictions to novel compounds. The authors evaluate the models performance through extensive benchmarking validating its encoder selections transfer learning effectiveness and generalization capabilities. Finally they propose an uncertainty measure that correlates with chemCPAs ability to generalize to unseen compounds.", "Paraphrase: The authors present a method for predicting gene expression changes in response to drug treatment. They encode gene expression profiles using a neural network but prevent the model from storing information about the cell line and drug in the hidden representation. The drugs molecular structure is also incorporated as features. Encoded embeddings are decoded and compared to experimental expression values using a loss function. The authors claim that their model can predict \"counterfactual\" expression profiles for drugs. However the use of this term is unclear and raises questions about the evaluation process. The R2 metric used for evaluation is calculated across genes for each sample. The authors also mention \"disentanglement\" but Table 1 uses R2 as the metric. The authors fail to distinguish their method from previous work and provide comparisons to published approaches. Additionally there are concerns about the appropriateness of Gaussian likelihood for RNAseq data and the use of the term \"bulk RNAseq\" in reference to the L1000 dataset. The caption for Figure 3 incorrectly claims that latent embeddings are identical for drugs targeting the same pathway and the caption for Table 4 contains an error.", "Summary Paraphrase This study introduces the chemCPA model which integrates drug structural information to analyze expression changes induced by novel drugs. The model features a new encoderdecoder architecture and outperforms other moleculeencoding networks. Additionally it leverages transfer learning to enhance generalization using available highthroughput screening (HTS) data. Strengths and Weaknesses Paraphrase Strengths: ChemCPA predicts gene expression responses to unseen drugs based on their molecular structures. Pretraining on HTS datasets improves the models performance. ChemCPA offers advantages such as disentanglement of latent space interpretability of drug effects and incorporation of molecular structure. Weaknesses: ChemCPAs performance is lower on differentially expressed genes (DEGs) than on all genes. Benchmarking against similar models in simpler perturbation scenarios could be valuable. Questions 1. How is the subsetting of 977 genes performed using ensemble gene annotations 2. Have the authors explored other datasets mentioned in the Compositional Perturbation Autoencoder (CPA) paper"], "n3lr7GdcbyD": ["Paraphrase: This study introduces a modified form of the MonteiroSvaiter (MS) acceleration method that eliminates the logarithmic factor in its bisection procedure during each iteration. The authors implement the MS oracle using existing oracles and a new adaptive MSNewton oracle. By employing the Taylor descent step in implementing the MS oracle the resulting algorithm closes the logarithmic gap between existing upper and lower bounds for minimizing convex functions with Lipschitz pth order derivative addressing a notable open challenge in optimization theory. The authors present numerical evaluations that reveal insights into the impact of momentum on secondorder methods and identify potential avenues for future research. Strengths and Weaknesses: The study resolves the open problem of bridging the gap between upper and lower bounds in minimizing convex functions with Lipschitz pth order derivative making a significant contribution. Nesterov highlighted this problem as an \"open and challenging question in Optimization Theory\" in his 2018 textbook. The proof is intricate and difficult to verify however the authors provide proof outlines and numerical confirmations for the proposed schemes enhancing confidence in their accuracy. The authors also suggest adaptable MSoracle implementations for the secondorder case and demonstrate their effectiveness through numerical evaluations. Overall this is a substantial theoretical study wellpresented and wellreasoned. The technical nature of the content appears unavoidable for this area of research (developing optimal methods). Acceptance is highly recommended. Questions: How are the MS framework and the inexact highorder proximal points framework from [1] related They exhibit comparable convergence rates. Can the proposed MS framework serve as a highlevel process in the bilevel minimization approach Is it feasible to derive lowerorder methods that \"surpass\" conventional Complexity Theory limits in the MS framework similar to those in [1]", "Paraphrased Statement: This paper presents a modified version of the MonteiroSvaiter (MS) acceleration method that eliminates the need for bisection while maintaining optimal performance (within constant factors). The original MS method introduces acceleration by solving an implicit equation for a dynamic regularization parameter but this typically requires bisection limiting its practical efficiency. This work addresses this issue by developing an MS variant that: Eliminates bisection Requires minimal parameter tuning Performs effectively in practice Preserves the optimal complexity bounds of the original MS method Additionally the authors propose an adaptive alternative to the cubic regularization oracle that: Automatically adjusts the parameter M based on the Hessians Lipschitz constant Achieves optimal Hessian evaluation complexity for functions with Holder Hessian of order \u03bd The paper also includes a firstorder implementation of the adaptive oracle and numerical experiments that demonstrate the effectiveness of the proposed methods. Notably the experiments suggest that momentum can be detrimental in logistic regression. Strengths and Weaknesses: The paper is wellwritten and clearly defines the contribution and connection to existing literature. The authors effectively explain the technical details of their work and provide additional context through introductory explanations. The inclusion of Section 3.1 is particularly beneficial for comprehension. Questions: Are there any existing acceleration methods that eliminate the need for bisection How does the adaptive parameter tuning in the proposed method compare to other adaptive approaches What are the practical implications of the finding that momentum can be harmful for logistic regression", "Paraphrased Summary This paper introduces a significant improvement to the MonteiroSvaiter (MS) algorithm eliminating the need for solving a complex nonlinear equation at each iteration which previously added logarithmic factors to the algorithms complexity. This improvement enables the construction of optimal tensor methods for any order without these logarithmic factors. Specifically for the secondorder method a novel procedure is proposed for solving the subproblem efficiently. This procedure serves as an alternative to the standard \"line search\" and is more efficient. Strengths and Weaknesses Strengths: Resolves the issue of logarithmic factors in the complexity of optimal tensor methods. Introduces a highly efficient \"line search\" alternative for secondorder methods. Weakness: The supplementary material could be improved. Major Remarks: Clarify the termination conditions for loops in Algorithm 2. Explain the identity in the display between lines 895896. Simplify the proof of Lemma 3 by using a different notation. Minor Remarks: Explain why the final convergence bound in Theorem 1 is independent of \u03bb0. Justify the formula for C\u03b1s in Theorem 1 and its difference from a previous formula. Provide guidance on choosing \u03bb0 in the experiments. Correct the formula for the Lipschitz constant in line 293. Rename M in Proposition 1 to avoid confusion. Clarify the assumptions and consistency of the sets Q\\T and numbers r\\t T in Lemma 1 and Section C.3. Explain the implications for the case s 1 in C.3. Verify the claim that r\\t \u2265 1 in line 688. Add \u03bb1 0 to Lemma 9. Correct the typos and grammatical errors identified.", "Paraphrased Statement: This paper presents an improved version of the MonteiroSvaiter (MS) framework which enhances the efficiency of inexact backward optimization methods in convex optimization problems. The new variant eliminates the bisection step in the original MS method resulting in a logarithmic reduction in theoretical complexity matching optimal bounds up to constant factors. Additionally it eliminates the need for parameter tuning as the regularization parameter is adaptively adjusted without requiring prior knowledge of LipschitzH\u00f6lder constants or orders. The paper demonstrates that the proposed algorithm achieves optimal complexity bounds and provides detailed discussions on its application to cubic regularization. Experiments validate the theoretical claims. Strengths: Theoretical improvements to the existing MS framework Weaknesses: Difficulty in following the presentation Lack of clarity on technical components Questions: The MS oracle introduced in the paper does not seem to depend on the regularization parameter \\lambda. This raises concerns about the algorithms complexity bounds. The paper claims to achieve no parameter tuning but it is unclear how the parameters s and c are adaptively selected. Minor Questions: It is claimed that the MS oracle is satisfied for Taylor descent step only when p \u2265 2. Related Literature: Inexact and accelerated proximal point algorithms (Salzo and Villa) Catalyst acceleration for firstorder convex optimization from theory to practice (Lin et al.)"], "xL7B5axplIe": ["Summary The paper introduces Conservative Dual Policy Optimization (CDPO) a method that uses two update strategies: a Referential Update and a Conservative Update. It has been demonstrated that CDPO performs similarly to the PSRL algorithm in terms of regret while guaranteeing a monotonic improvement in the policys performance. Empirical results confirm its effectiveness. Strengths CDPO is straightforward to apply and produces good results. The theoretical basis of the method is wellsupported. The literature review is thorough. Weaknesses The motivation for creating CDPO is unclear and the paper can be hard to understand. The algorithm is only tested on three MuJoCo environments. Its performance in larger environments (such as Ant and Humanoid) remains unknown. The extent to which CDPO improves sample efficiency over other methods is not specified. Questions Can the authors provide a clearer explanation of the motivation for developing CDPO Will CDPO be tested in larger MuJoCo environments Can the authors quantify the improvement in sample efficiency achieved by CDPO compared to other approaches", "Paraphrase: Summary: This paper introduces a new algorithm called Conservative Dual Policy Optimization (CDPO). It combines: A referential update that optimizes a reference model similar to Policy Space Response Learning (PSRL). A conservative update that ensures a range of randomness maximizing model value expectations. CDPO has a similar regret bound to PSRL while offering monotonic policy improvement and global optimality. The paper outlines the CDPO framework and provides theoretical and empirical evidence. Strengths: CDPO effectively addresses exploration and learning challenges in uncertain environments. It is simple theoretically sound and empirically effective. Questions: The paper is wellstructured and answers most questions. Section Outline: Section 2: Technical background on modelbased RL and regret analysis. Section 3: Explanation of Offline First Utility (OFU) and PSRL frameworks including complexity and performance bounds. Section 4: Concise and clear presentation of the CDPO algorithm. Section 5: Robust theoretical analysis of CDPO. Section 6: Empirical evaluation on various domains comparing it against stateoftheart methods. CDPO outperforms particularly in highdimensional state spaces with uncertainties.", "Paraphrased Summary: This study tackles the challenge of modelbased planning in reinforcement learning (RL) when using nonlinear models in complex environments. Traditional planning approaches often fail in such scenarios due to model errors. The authors propose a cautious update algorithm (CDPO) that involves: Greedily updating a reference policy based on the leastsquares model estimate Updating the behavior policy while maintaining proximity to the reference policy and optimizing expected performance relative to the models posterior distribution Strengths and Weaknesses: Strengths: Wellwritten and easy to follow Thorough placement within existing literature Addresses a crucial problem Weaknesses: Limited experimental domains Algorithms performance relies on the convergence of optimizations (4.1) and (4.2) Lack of detailed implications for result (5.6) Additional Questions and Comments: How does the nonconvergence of optimizations (4.1) and (4.2) impact the results and experiments The assumption that the model class is insufficient to learn an accurate model of HalfCheetah needs to be supported by evidence such as model prediction loss analysis. The hypothesis that performance differences are due to better exploration could be validated in finite domains where exploration is more visible.", "Paraphrased Summary: This paper addresses the issue of excessive exploration in posterior sampling reinforcement learning (PSRL) by introducing Conservative Dual Policy Optimization (CDPO). CDPO incorporates a \"Referential Update\" and a \"Conservative Update\" to mitigate overexploration. The paper demonstrates that CDPO achieves similar theoretical regret bounds as PSRL and outperforms it in experiments on MuJoCo environments. Strengths: Clear and coherent writing. Thorough theoretical analysis of CDPO. Empirical evidence of CDPOs improved performance over PSRL. Weaknesses: 1. Lack of Justification for Overexploration: The authors claim that PSRL suffers from overexploration due to model instability but they do not provide sufficient analysis or evidence to support this. 2. Conflicting Empirical Results for Hyperparameter Tuning: Experiments show that a timedecaying exploration weight (\u00b5) performs worse than a fixed value which contradicts the intuition of gradually shifting towards exploitation as the environment is explored. The authors should provide a more detailed analysis of \u00b5 and its impact on performance. 3. Increased Time Complexity: CDPO solves two modelbased problems potentially doubling the computational time compared to PSRL. The authors should consider whether the performance improvements justify the increased runtime. 4. Simple Benchmark Environments: The experiments use relatively simple environments excluding more complex MuJoCo benchmarks. The authors should explore CDPOs performance in more challenging settings. 5. Outdated Baseline Methods: The baseline methods used for comparison are outdated and the authors should include more recent stateoftheart algorithms."], "uzqUp0GjKDu": ["Paraphrase: Summary: This study tackles unsupervised learning with hidden label shifts. Each data point has a domain label but the true classification label is unknown. The approach assumes that the class distribution remains consistent across domains while the label distribution varies affecting the data distribution. The authors draw inspiration from topic modeling and expand it to continuous domains. Their proposed method performs better than the SCAN method on CIFAR and FieldGuide datasets. Strengths: 1. The work provides new insights into the conditions that enable unsupervised learning moving beyond existing techniques based on similarity assumptions. 2. The connection between the latent label shift problem and topic modeling is wellreasoned. 3. The research is original and clearly presented. Weaknesses: 1. Clustering in the algorithm may encounter challenges in highdimensional data potentially hindering the effectiveness of the method. 2. The experiments could be more comprehensive as different variants of the method are used on different datasets. Questions: 1. Does the reliance on a pretraining backbone still necessitate feature similarity assumptions 2. How does DDFA (RI) perform on FieldGuide datasets 3. Why is the high QYD error on the FieldGuide2 dataset not reflected in the final performance 4. Are the limitations of the work explicitly outlined", "Paraphrased Statement: Traditional unsupervised learning methods group data based on similarities within a representation space. However actual labels may not align with these clusters if they represent misleading features (e.g. adult vs. caterpillar in mothbutterfly classification). This paper introduces an unsupervised learning technique using labelshifted domains. Given unlabeled datasets from different domains where the withinclass structure (p(xy)) remains consistent but the class distribution (p(y)) varies (e.g. over time or location) labels can be uniquely recovered. In the tabular setting the technique is analogous to topic modeling where data points represent words domains represent documents and labels represent topics. Assumptions are provided for recoverability using nonnegative matrix factorization. Extending the approach to continuous data involves clustering examples based on vectors [p(dx)]d estimated by a classifier trained across domains. Topic modeling is then applied to the discretization and results are converted back to p(yd x) using Bayes rule and the classifier. Experiments show that the approach outperforms other unsupervised methods when feature space similarity does not imply the same label. Strengths: Originality: Establishes a novel connection between label shift and topic modeling. Quality: Sound theoretical and empirical results. Presentation: Wellwritten and easy to follow. Significanc: Potentially useful for handling spurious features in datasets a challenge in areas like medical imaging. Weaknesses: Quality: Lack of theoretical results for the DDFA method. Significance: Uncertain practical relevance due to the need for clean examples of label shift and multiple domains with balanced class distributions. Questions: 1. Can the approach be applied to the Waterbirds dataset How would it handle backgroundbased clustering and subgroup imbalance 2. Could an ablation on a naive representation space clarify the importance of discretizing with f(x) 3. How are Theorem 2 and DDFA related 4. Definition of \"Alpha\" in line 325.", "Summary: The authors propose a framework (DDFA) for unsupervised learning when labels change across domains (latent label shift). Using simulated experiments they demonstrate that DDFA can enhance unsupervised classification accuracy by incorporating domain knowledge. Strengths: The assumption that instances that shift together should be grouped together is intriguing and the authors use topic models effectively to identify these shifts. The paper is wellstructured and the analysis is thorough. Weaknesses: The assumption of label shift may not always apply to realworld datasets. Questions: Concerns exist about the validity of the label shift assumption in realworld data. For example if a specific label is consistently associated with different contexts in different domains topic models may not accurately group together instances of that label. It is unclear how to obtain the distribution q(yx) in Algorithm 2 as topic models do not directly assign words to specific topics.", "Paraphrase: This research introduces a novel experimental setup called Unsupervised Latent Label Shift where data from multiple domains have varying label distributions but similar classspecific characteristics. To address this challenge the paper proposes a Deep Distribution Factor Analysis (DDFA) framework that solves the problem without supervision. The authors prove theoretically that the matrix factorization solution used in the DDFA framework is identifiable. Experimental results validate the effectiveness of the proposed framework. Strengths: 1. Clear and concise writing. 2. Novel and practical experimental setup. 3. Efficient use of matrix factorization to calculate label distribution reducing the computational load. 4. Comprehensive analysis of the identifiability of the proposed solution. Weakness: 1. Lack of ablation study to investigate the relationship between the number of clusters (m) and the number of classes (k) in the \"Discretize\" section. 2. No citation provided for the referenced paper [1]."], "xnI37HyfoP": ["Paraphrased Statement: This research examines the tensor completion problem for tensors with nonnegative entries. The study introduces a nonnegative equivalent of the nuclear norm denoted as \\Vert \\cdot \\Vert whose 1ball is defined as a convex combination of rank1 \\0 1\\tensors. The authors propose solving the problems dual to \\Vert \\cdot \\Vert minimization to recover unknown nonnegative tensors from a subset of observed entries. This approach parallels nuclear norm minimization for general tensors. The study demonstrates that this method can recover the unknown tensor from a statistically optimal number of unknown samples. However the minimization problem is shown to be NPhard. Simultaneously the authors establish that the BCG algorithm can solve the problem efficiently. Additionally a heuristic algorithm for the oracle is proposed and evaluated against synthetic data. Strengths and Weaknesses: Strengths: Focuses on tensors with nonnegative entries a common occurrence in practice. Proposes a novel approach for the problem. Shows improved performance compared to existing methods for lowrank nonnegative tensors. Weaknesses: Clarity issues in technical proofs and overstated claims. Nonstandard notation makes readability challenging. The paper does not include details on the BCG algorithm. Questions: 1. Clarify the informationtheoretic bounds achieved by cited papers. 2. Confirm whether the recovery rate in Corollary 4.3 is k\\cdot \\rho or k4\\cdot \\rho and specify any necessary conditions. 3. Explain the nature of the NPhard oracle early on for better understanding. 4. Identify potential assumptions for polynomialtime implementation of the oracle. 5. Provide clearer definitions and interpretations of variables in Proposition 3.1 and Corollary 3.3. 6. Report performance comparisons on realworld datasets and against recent approaches. 7. Correct the discrepancy in the informationtheoretic bound statement. 8. Clarify the meaning of \"NPhard to solve to any accuracy.\" 9. Revise the claim about efficient computation of global minima to avoid confusion about P NP. 10. Explain the discrepancy between the experimental results and the theoretical suggestion of almost exact recovery. 11. Define the exponential growth rate in the claim about \"exponentially more samples.\"", "Paraphrase: Summary: The authors have proposed an efficient algorithm for completing nonnegative tensors that they claim runs in linear time. Strengths and Weaknesses: The paper is wellwritten and clearly presents the algorithm. The algorithm is original and significant. However concerns exist about the authors claims based on Figures 3 and 4. Concerns: Data for the BCG algorithm in Figures 3 and 4 do not appear linear. Data in Figure 3 is noisy while data in Figure 4 exhibits upward curvature indicating nonlinear growth with larger n values. Numerical experiments do not provide clear evidence that the algorithm operates close to the IT threshold. Questions: The paper should provide more convincing numerical evidence to demonstrate the algorithms scalability and efficiency. The authors should explain why a local optimization algorithm can achieve optimal solutions. Its unclear how the algorithms performance might change with increased tensor rank.", "Paraphrased Statement: Summary The paper presents a method for filling in missing values in nonnegative tensors (multidimensional arrays) using a novel tensor norm. This norm serves as an approximate measure of tensor rank and has useful properties for optimization. The norm is derived by showing that the convex hull of nonnegative rank1 tensors with a maximum entry of 1 is identical to the convex hull of all binary rank1 tensors. As the latter forms a polytope with vertices corresponding to binary rank1 tensors linear integer programming can be applied to optimize over it. Based on this the norm of any nonnegative tensor is defined using these polytopes. The paper analyzes the norms properties proving its NPhardness and establishing its stochastic complexity. It also demonstrates how to efficiently find the norms global minimum using integer linear programming. Numerical results indicate that this method outperforms existing methods in terms of estimation accuracy and can complete tensors with fewer known values. Strengths The problem of tensor completion is significant and has applications in various fields. The proposed norm provides a convex approximation of tensor rank facilitating optimization. The norms properties are rigorously studied including its NPhardness and stochastic complexity. An efficient algorithm is developed for calculating the norms global minimum. The paper is wellstructured and the proofs are clear. Weaknesses The proofs could be moved to an appendix to enhance readability. More intuitive explanations visualizations and detailed algorithm explanations would be beneficial. It would be valuable to evaluate the algorithm with hyperparameters beyond the ground truth which is of practical significance. Questions How does the algorithm perform with hyperparameters that differ from the ground truth"], "z0M3qHDqH20": ["Paraphrased Statement: This study presents a novel and advanced approach for accelerating MRI reconstruction. The authors provide detailed explanations of the models components and justify most of their design decisions. However some concerns and suggestions are raised: Strengths: Wellwritten and easytounderstand paper Innovative stateoftheart approach for accelerated MRI reconstruction Most architectural design choices are wellsupported Intuitive design elements (e.g. high and lowresolution processing) Weaknesses: Some explanations are unclear especially regarding the benefits of longrange dependency modeling using Transformers in MRI reconstruction tasks Ablation studies are inconclusive and lack key experiments with Adjacent Slice Reconstruction (ASR) and Residual learning Evaluation experiments could be improved to assess the models efficiency and fairness of comparisons Questions: What specific benefits do Transformers provide for longrange dependency modeling in MRI reconstruction given the importance of locality captured by convolutions How would E2EVarNet perform with ASR and Residual learning added Would this eliminate the need for the complex HUMUSBlock What competitive advantages make HUMUSNet preferable to E2EVarNet such as computational efficiency reduced training data requirements or increased robustness The table comparing models should include the number of parameters for each to ensure fair comparisons. Typos: Line 193: \"a significant challene\" should be \"a significant challenge\" Line 199: \"In this work\" should be \"In this paper\"", "Paraphrased Statement: This paper presents a technique for speeding up MRI reconstruction using advanced deep learning networks. Inspired by advancements in image processing a novel network architecture is introduced that combines traditional convolutional layers with selfattention mechanisms. This architecture called HUMUSNet is employed within a deep unfolding algorithm to enhance image quality. The network is trained directly using ground truth images demonstrating competitive results on benchmark MRI datasets. However the rationale for using a transformerlike architecture and the specific contributions are not fully clear and additional technical details and analysis are needed to strengthen the papers overall impact.", "Paraphrase: Summary: This paper introduces HUMUSNet a neural network (NN) architecture that combines convolutions and transformer blocks for magnetic resonance imaging (MRI) reconstruction using compressed sensing. The method surpasses current stateoftheart results on the fastMRI dataset with eightfold acceleration (8x). The code is provided in the supplementary materials. Strengths: 1. HUMUSNet presents a novel approach for building MRI models using hybrid convolutionaltransformer architectures. 2. The paper thoroughly explains the design choices and includes an ablation study of the model. Weaknesses and Questions: 1. The experiments were performed with an acceleration ratio of 8x. Its unclear whether this approach would be effective for higher or lower acceleration ratios. Did the authors consider validating the method at higher acceleration ratios How do they anticipate the methods performance at ratios such as 8x or lower like 4x or 2x 2. The authors mention that the current architecture requires fixedsize inputs. However both the extractor and reconstruction blocks utilize convolutions which should not pose a problem. Could the authors clarify why fixedsize inputs are necessary", "Paraphrase: Summary: HUMUSNet is a neural network designed to reconstruct highquality magnetic resonance imaging (MRI) images captured using multiple receiver coils. It employs a series of building blocks (HUMULBlocks) that mimic an optimization algorithm used for image reconstruction. Each HUMULBlock consists of three components: highresolution feature extraction lowresolution feature extraction and highresolution image reconstruction. The core component of HUMULBlock is MUST (MUltiscale residual Swin Transformer network) which combines residual Swin Transformer blocks with a hierarchical encoderdecoder architecture. MUST enables multiscale processing and reconstruction of both fine details and global image structure. HUMUSNet also utilizes adjacent MRI slices to reduce artifacts in the reconstructed images. Performance: HUMUSNet achieved top results in the fastMRI knee multicoil x8 challenge outperforming other stateoftheart reconstruction techniques. It also demonstrated good performance on three different datasets. Ablation studies confirmed the effectiveness of the proposed design. Strengths: Uses selfattentionbased Transformers to address limitations of convolutions. Multiscale structure in MUST allows for both fine detail reconstruction and computational efficiency. Achieved second place in the fastMRI leaderboard. Performs well on diverse datasets. Weaknesses: Lacks radiologists evaluation on specific images for better understanding of reconstruction quality. Analysis of HUMULBlocks denoising effect is missing. Ablation study on the effects of different cascade configurations is not provided. Questions and Detailed Comments: 1. The method for generating tokens from image patches requires clarification. 2. The statement about HUMULBlock only operating on the regularization term conflicts with the use of kspace domain in the unrolled architecture. 3. The MRI image resolution is unspecified making comparisons difficult. 4. Explanation of terms like \"token representation\" \"learned linear mapping\" and \"positional encoding\" is needed. 5. The meaning of \"Tokens corresponding to different image patches...in a concise latent representation\" needs further explanation. 6. The dimensions in Figures 2 and 3 are difficult to read. 7. The role of reshape blocks in Figure 3 is unclear. 8. The dimension of concatenated kspace slices and input to HUMULBlock should be specified. 9. The calculation of \"10 times\" in Line 329 requires clarification."], "w_jvWzNXd6n": ["Paraphrase: Summary The SBMTransformer is a proposed variation of a transformer architecture that dynamically adjusts the sparsity of its attention blocks based on the input sequence data. This allows for linear evaluation cost in the number of edges chosen adaptively using a generative model for community detection (SBM) and a fast sampling method (fastRG). The architecture supports flexible sparsity patterns across layers enabling the learning of full attention models when necessary. However it significantly reduces computational complexity as demonstrated in experiments on the LRA dataset. The paper builds on theoretical results on sparse attention transformers and applies these findings to the proposed architecture leading to Theorem 1. The limitations are discussed with GPU architectures used for large transformers being currently inefficient for sparse tensor operations. Strengths and Weaknesses This work combines sampling methods with gradient descent optimization providing theoretical efficiency guarantees supported by experiments. While maintaining performance it is not currently wellsuited for large GPU clusters. The paper lacks accessibility for readers unfamiliar with SBM and generative modeling. It could benefit from further explanation of how the SBM model translates to the SBMTransformer. Theorem 1s proof requires additional clarification to enhance intuition. The proof in the Appendix relies on previous works with Lemma 2 providing expressibility for specific sparsity patterns. The extent of novelty should be elaborated upon. Table 2s results suggest that applying a small sparsity regularizer has an insignificant impact on overall sparsity and performance. Rephrasing or omitting this claim is recommended. Minor Comments Grammatical errors: \"the the\" in Table 1 Expand on bipartite structure (lines 145164) Provide motivations for using a 2layer MLPdh to dh (line 145) Discuss limitations imposed by SBMs assumption of edges following a Poisson distribution Explore potential limitations in learning outliers or imbalanced clusters", "Paraphrase: Summary This study proposes a Transformer variant that utilizes random adjacency matrices derived from a stochastic block model (SBM) to mask selfattention matrices. This method aims to reduce the computational expense of creating each N x N selfattention matrix. During forward propagation the proposed method derives SBM parameters from query and key embeddings using a MultiLayer Perceptron (MLP) and generates a selfattention mask using a quick random graph sampling method. In the backward pass gradients are transmitted through this discrete sampling step using a straightthrough estimator. The authors demonstrate that the proposed SBMTransformer can approximate any function within the context set forth by Yun et al. (2020) and Zaheer et al. (2020). Empirically they show that the SBMTransformer: 1. Can simulate a typical Transformer with full attention on a synthetic task 2. Surpasses multiple optimized Transformer variants on average on the Long Range Arena benchmark. Strengths and Weaknesses Originality As stated in the paper using sparse attention masks to reduce the computational cost of selfattention has been explored in previous research (e.g. axial attention (Ho et al. 2019) BigBird (Zaheer et al. 2020)). The key methodological innovation here is the use of a stochastic block model to generate a random attention mask during each forward pass. To the authors knowledge this approach has not been explored in prior studies. Quality Theoretical Results The paper establishes that the proposed Transformer variant has a universal approximation property (Theorem 1). This result is valuable as a sanity check indicating that the proposed method is a reasonable approach. The technical contribution is considered minor as the proof of the result largely applies existing techniques from Yun et al. (2020) and Zaheer et al. (2020). A minor limitation of the analysis is that it employs a modified version of the SBMTransformer architecture compared to the one used in the empirical evaluation specifically requiring Mii 1 unlike in the version used to obtain the results in Table 1. Empirical Evaluation A strength of the paper is the finding that the proposed SBMTransformer architecture outperforms the baseline efficient Transformer architectures on average in the LRA benchmark tasks (Table 1). However a significant weakness of this comparison is that it does not adequately consider the accuracycost tradeoff of each evaluated method. For example it is unclear from the paper whether the SBMTransformer outperforms the given baselines when controlling for a metric of computational cost such as the perexample floatingpoint operation (FLOP) count during inference. Such controls are crucial because the SBMTransformer introduces additional operations (such as a 2layer MLP used for calculating the SBM parameters) the costs of which must be offset by the increased sparsity of the selfattention matrix. Clarity The presentation is generally straightforward and easy to follow. Significance As mentioned in Section 6 the unstructured sparsity of the resulting attention maps in the SBMTransformer formulation is not compatible with fast matrix multiplication on current GPU hardware. Therefore the proposed method requires longer wallclock times for inference which limits its practical value for practitioners. The authors argue that the method (1) \"enables multihead attention with long sequences\" and (2) that practitioners can adjust the computational cost of inference by limiting the number of edges sampled. These arguments are not entirely persuasive as written. In particular claim (1) requires further justification in light of (a) existing efficient Transformer architectures that achieve subquadratic computation and (b) techniques like memoryefficient attention [1] that demand subquadratic memory. As for claim (2) the benefits of this additional flexibility (e.g. in terms of accuracy versus cost) should be evaluated empirically. Questions As discussed in the previous section does the SBMTransformer advance the accuracy versus cost tradeoff frontier in comparison to current efficient Transformer architectures Since random sampling under the SBM is required even during inference practitioners typically find nondeterministic prediction at inference undesirable. Can this sampling step be replaced with a deterministic operation", "Paraphrased Summary: This paper introduces a novel attention mechanism where the sparsity pattern is learned adaptively by modeling the attention mask as a bipartite graph. The graph is created using the Stochastic Block Model (SBM) which allows for the training of a cluster embedding matrix. The authors show how to incorporate this mechanism into both forward and backward propagation. In forward propagation equations (58) are used to model the mask matrix introducing a cluster embedding matrix and multilayer perceptrons (MLPs) for token membership determination. They also implement a StraightThrough Estimator (STE) to enable gradient backpropagation despite the discrete sampling of the mask. The authors demonstrate the versatility of the attention mechanism as a universal approximator and evaluate its performance on the Longrangearena (LRA) benchmark. Strengths and Weaknesses: Originality: The concept of using SBM to model attention sparsity is novel resembling classic pruning techniques. Quality: Pros: Presentation of a novel dataadaptive sparse attention mechanism. Cons: LRA benchmark evaluation may not be fully convincing due to recent advancements in transformerbased models. Lack of reported runtime comparisons with existing methods. Potential GPU limitations due to sparse matrix multiplication. Clarity: Excellent paper clarity with a minor suggestion to clarify the definition of \"SBM\" and expand upon Theorem 1 in the main text. Significance: The problem of attention sparsity is important but the practicality may be limited by GPU compatibility. Questions: Can the authors detail the additional time and space complexity of the SBM mechanism How does this compare to other efficient attention mechanisms"], "sGugMYr3Hdy": ["Paraphrase: This research suggests using teacherlearner relationships in Goalconditioned Reinforcement Learning incorporating ideas from pedagogy and pragmatics. It introduces Bayesian Goal Inference (BGI) a rewardshaping technique that guides policies towards behaviors that reveal the goal from the trajectory. BGI is applied in two contexts: enhancing a teachers behavior to make it less ambiguous (\"pedagogical teacher\") and improving a learners ability to interpret teacher demonstrations and predict its own goals (\"pragmatic learner\"). The method is evaluated in a toy setting and a simulated robotic blockpushing environment demonstrating improvements in goal achievement and goal inference for both teacher and learner. Strengths: Simplicity and adaptability to any Goalconditioned Reinforcement Learning with Latent Fidelity (LfD) method. Demonstrated effectiveness in various settings. Enhanced goalreaching performance alongside goalinference improvement. Weaknesses: Limited testing to a simple blockpushing environment. Uncertain impact of noisy BGI predictions in more complex environments. Predominant benefit from the learners use of BGI. Questions: Could the driving benefit arise from onpolicy updates encouraging more distinct trajectories by either the teacher or learner How ambiguous are the environments as measured by the Ambiguity Score and what percentage of trajectories in the NaiveLiteral methods result in ambiguous policies How does the method fare in environments with higherdimensional spaces noisy BGI predictions or more complex tasks How computationally expensive is BGI prediction and how scalable is it to continuous action and goal spaces What is the impact of the method when goal inference becomes more challenging or a less accurate BGI method is used", "Paraphrased Statement: Summary This proposal presents a new way for machines to learn from human demonstrations. It uses ideas from how humans learn to solve the problem of not knowing what the goal is when there are multiple possible goals. The proposed Bayesian Goal Inference framework helps the machine learner understand the goals and improve its learning with the guidance of the human demonstrator. Strengths and Weaknesses The papers idea is original and wellpresented especially with the helpful video in the supplementary material. However there are a few concerns that need to be addressed. Questions Major concerns 1. Motivation: Why is it important for machines to infer goals when they are known during demonstrations 2. Lack of baselines: There are no comparisons to other methods such as Hindsight Experience Replay which could help evaluate the proposed methods effectiveness. 3. Missing details: The description of the learning algorithm used GCRL is insufficient and needs more explanation. 4. Purpose of goal inference: If the goal is known during GCRL learning what is the point of inferring the goal 5. Lack of theoretical guarantee: The proposed method is mainly an empirical study without a theoretical foundation. Minor concerns 1. Continuous goal spaces: How will the method handle continuous goal spaces in robotics tasks where the mapping between state and goal space is assumed to be known 2. Sensitivity to reward shaping designs: Are the choices made in reward shaping designs crucial for the methods performance 3. Insufficient seeds: Five seeds are not sufficient for evaluating RL algorithms. 4. Ambiguous presentation: Some parts of the paper need proofreading and clarification such as the reference to multiple learners where there is only one.", "Paraphrased Summary: Problem: Teaching assistants often face challenges communicating goals clearly to learners especially when multiple goals are involved. This ambiguity makes it difficult for learners to effectively infer goals and perform tasks. Approach: The study proposes a system that addresses goal ambiguity by utilizing teaching concepts from cognitive science. Teachers are pret rained to master all goals and learners infer goals from demonstrations using Bayesian inference. Teachers provide feedback on inferences assisting learners in improving their predictions. Teachers also selfinfer goals from their own trajectories rewarding themselves for pedagogical behavior that improves task performance. Methods: The system is trained in two stages: 1. Pretraining the teacher to master all goals using Goal Conditioned RL. 2. Learner infers goals and receives feedback while the teacher tracks selfinferred goals and rewards pedagogical actions. Metrics: The system is evaluated on four metrics: 1. Goal inference accuracy: Learners ability to infer goal from demonstrations 2. Own goal inference accuracy: Teachers ability to infer goal from own demonstrations 3. Goal reaching accuracy: Learners ability to reach goals 4. Goal inference accuracy x Goal reaching accuracy: Combined measure of goal inference and task performance Results: The system is tested in two environments (DrawTwoBalls and FetchBlockStacking) and demonstrates: Improved goal inference accuracy and goal reaching accuracy compared to a naive approach Faster learning with fewer demonstrations Strengths: Addresses an important problem in teaching assistants Utilizes sound concepts from cognitive science and machine learning Clearly written and wellorganized Weaknesses: Lack of detailed explanations on goal discovery and architectural details Limited results to a single task Insufficient comparison to more robust baselines", "Paraphrase: This study incorporates concepts from pedagogy and pragmatism to train reinforcement learning (RL) agents. Pedagogy allows teachers to provide clear demonstrations while pragmatism enables learners to identify the intended goal of those demonstrations. The algorithm involves two stages: 1. Teacher Phase: A goalconditioned teacher policy is trained using offpolicy data. It receives rewards for correctly predicting the intended goal of its own trajectory using Bayesian Goal Inference (BGI). 2. Learner Phase: A new learner policy learns from the teachers demonstrations receiving rewards for both reaching goals and predicting the intended goals of the teacher and its own actions. Experiments suggest that using pedagogical teachers (who provide clear demonstrations) and pragmatic learners (who can infer goals) improves learning from demonstrations. Strengths and Weaknesses: Originality: The paper presents a unique combination of pedagogy pragmatism BGI and goalconditioned RL in multigoal environments. Quality: The experimental results are wellpresented and address the stated contributions. Specific details are provided including architecture seeds and variance. Clarity: The paper is wellwritten and easy to follow. The authors clearly motivate the work and suggest potential applications. Significance: The results are important for areas like robotics where obtaining demonstrations is costly. Researchers can leverage this approach to refine existing policies into pedagogical teachers. Nits: A minor correction in Figure 5s caption. The use of a comma in line 119. Questions: Is the teachers ability to master all goals a requirement Could cooperative training (combining phases 1 and 2) improve results How is the initial goal chosen in each phase and could strategic selection impact performance Would increasing demonstration count benefit Generative Prediction Neural Network (GPNN) over BGI"], "msFfpucKMf": ["Summary The paper proposes a method for training option policies that can be combined in any order regardless of the next task. The approach involves reframing the problem as a twoplayer game where the environment orders tasks and the agent learns individual policies for each task aiming to maximize the completion of as many tasks as possible in the worstcase scenario. The convergence of the value iteration algorithm to an optimal policy in this setting is proven and two practical algorithms based on Qlearning and Soft ActorCritic are provided. Experiments in two continuous stateaction domains show that the algorithms outperform baseline methods. Strengths and Weaknesses The twoplayer framing is novel and provides a theoretical basis for the approach. Practical algorithms are supported by theoretical guarantees and empirical evidence. The paper provides a clear example of the racing car domain to illustrate the concepts. The method simplifies the problem of option discovery by assuming that subtasks are predefined. The relationship with other optiondiscovery approaches is unclear. Skill chaining approaches should be considered as baseline comparisons. The performance of the algorithms when faced with random or MCTS adversaries raises questions about the need for gametheoretic framing. The formulation of the multitask MDP relies on specific knowledge and assumptions. The abstract statement about maximizing worstcase performance could be rephrased. The variance of the results is unclear. Questions 1. How does the papers formulation relate to skill chaining approaches which also aim to discover and chain options 2. Why is it necessary to frame the problem as a twoplayer game if a random adversary performs similarly to MCTS 3. Is the formulation of the multitask MDP based on jump probabilities a common approach and does it introduce domain knowledge that gives the agent an advantage 4. Can the sentence in the abstract about maximizing worstcase performance be clarified 5. What is the representation of variance in the results graphs", "Summary Paraphrase: The study aims to develop algorithms that can solve complex tasks by breaking them into a series of smaller tasks or \"subtasks.\" These algorithms are designed to maximize the agents performance even in scenarios where an adversary selects the order of subtasks. Strengths and Weaknesses Paraphrase: Originality: The framework for training robust policies is similar to previous work but the authors introduce a unique aspect where the adversary can interact with the environment multiple times within the same episode. However a more comprehensive discussion of related research is needed. Quality: The algorithms are welldesigned and didukung by theoretical guarantees. Experiments on complex environments show faster learning compared to baselines. Concerns remain about the limited number of learning frames and the lack of a baseline that performs flat reinforcement learning on the entire task. Clarity: The text is generally clear but the mathematical sections could benefit from better explanation and organization. The descriptions of the NAIVE and DAGGER baselines could be improved. Significance: The development of robust and generalizable options is important for reinforcement learning. However the applicability of the proposed framework is limited due to the assumption that any sequence of options corresponds to a meaningful task and the requirement for a dynamically changing environment based on subtask selection. Questions: How does your work differ from unsupervised environment design and robust minimax RL approaches In what other realworld scenarios would your framework be useful Why does the MADDPG baseline perform so poorly even compared to a random adversary", "Paraphrased Statement: Summary: This research focuses on enhancing reinforcement learning (RL) performance in longhorizon tasks. The proposed approach adversarially selects tasks to hinder the success of individual policies. It has been evaluated across various domains and has shown positive results. Strengths: Wellwritten and structured paper Clear motivations and theoretical explanations Comprehensive empirical evaluation results Weaknesses: Lack of definition for A1 and A2 in Section 3 making it difficult to understand the differences between their actions. Limited discussion of related work failing to highlight the methods distinctions from previous research. Absence of sufficient analysis of experimental results to demonstrate the collaboration between the two agents in training. Questions: Considering the multitask setting more complex tasks could be included in experiments to justify the need for subpolicies. How would the ROSAC approach perform in more realistic tasks What happens if the agent struggles with all subtasks and makes minimal progress during training indicating an extremely challenging initial subtask"], "sRKNkpUMQNr": ["Paraphrase: Summary: This paper introduces an approach to enhance the speed of Generative Adversarial Networks (GANs) by maximizing the mutual information between a pretrained model (teacher) and a newly trained model (student). Although calculating the exact mutual information between continuous distributions is difficult the paper focuses on maximizing a lower bound of this quantity. The approach demonstrates significant numerical improvements in both conditional and unconditional GANs. Strengths: 1. The idea of using mutual information maximization for GAN distillation is novel and addresses a specific challenge. 2. The motivation is clear and the technical details appear to be sound. 3. The experiments cover various tasks and show positive quantitative results. Weaknesses: 1. There is a lack of qualitative results to adequately evaluate the approachs effectiveness. Figure 2 of the paper does not provide a clear indication of which method produces better quality results. It is also unclear why the Vector Energy Model (VEM) does not enforce similarity between the students and teachers outputs particularly since the mutual information is maximized during distillation. 2. The approach has only been evaluated on GANs generating 256px images while previous methods have been applied to 1024px StyleGAN2. Its performance on highresolution synthesis is uncertain. 3. The approach requires the use of an additional energybased neural network and backpropagation for parameter optimization which can introduce additional computational overhead. A comparative analysis of the computational time required for this additional module should be provided. Questions: Refer to the weaknesses highlighted above.", "Paraphrase: Summary Generating complex images on devices with limited processing power can be challenging. Distillation is a technique that helps create networks that perform well even with reduced complexity. This paper introduces a new distillation method for generative adversarial networks (GANs). Method The method uses a lower limit (bound) on the mutual information between the \"teacher\" network (which generates highquality images) and the \"student\" network (which is less complex). The authors create a set of methods that maximize this lower bound. The optimization alternates between updating parameters of the student and teacher networks. This approach can be combined with existing network compression algorithms. Strengths and Weaknesses Can be combined with existing compression algorithms. Improves performance (FID and mIoU) for networks with a fixed number of parameters compared to other compression algorithms. Clear writing with minor corrections needed. Code is available. Suggestions for Improvement Clarify that p(ts) p(tx) p(sx) p(x) since mutual information measures the overlap between student and teacher code. Provide an example of a perfect q(ts) that maintains the bound: decoder of students output and teachers encoder. Reword \"student generator learns to maximize the variational lower bound\" to \"generator is trained by maximizing ...\" Simplify the arrows in Figure 1 to avoid confusion (e.g. distinguish between data flow and abstract connections). Define and reference abbreviations (FID MACs mIoU). Questions What do the vertical arrows in Figure 1 represent What is the parameterization size of the q(ts) network for different tasks (as shown in Supplementary Information Figure 4) What is the computational cost of adding VEM (Variational Equilibrium Maximization) to other distillationcompression methods Can VEM be used without a distillation method", "Paraphrased Statement: Summary: \"Variational EnergyBased Model for Generative Model Compression\" presents a method for compressing generative and structured prediction neural networks. It combines teacherstudent distillation with a variational energybased model optimized through Markov Chain Monte Carlo (MCMC). Using a flexible variational distribution the method surpasses previous compression approaches on imagetoimage and image generation tasks when combined with existing methods. Strengths: The authors provide thorough comparisons with other compression methods detailing the proposed methods background methodology and alternatives. The research is clearly presented with accessible code and supplementary materials. The proposed method has shown significant improvements in compression performance for smaller to mediumsized models. Weaknesses: The methods performance improvements are less evident for large highperformance models. The proposed method follows a similar approach to previous work lacking significant originality. Largescale experiments such as those involving StyleGAN2 are limited showing mixed results. The authors do not explore the methods applicability to recent models or nonface domains. Questions: Are there technical limitations preventing the methods application to recent patchbased latent models like VQGAN and ViTVQGAN If there are assumptions limiting the methods applicability they should be clearly stated. Why are some comparisons with VID and CRD missing in certain experiments and are there specific reasons why these methods are not applicable in some cases"], "qYc8VnmUwbv": ["Paraphrase: This study examines a learning scenario where the adversarys actions are restricted to a distribution with a bounded density. This concept blends offline and online learning. However prior research on this smoothed setting shows a substantial gap between the regret bounds for computationally limited and unlimited learners with the latter exhibiting exponential growth in the T\u03c3 ratio. This work presents effective algorithms that achieve logarithmic regret bounds closely matching the optimal regret under unlimited computation. Results (Linear Classification): Theorem 5 establishes a highprobability regret bound close to the lower bound provided in Proposition 6. Corollary 8 extends this to affine classification. Theorems 9 and 10 generalize to linear classification with feature maps. Theorems 11 and 12 handle multiclass classification. Algorithm and Analysis: Algorithm 1 retains a set of consistent hypotheses and predicts based on their John ellipsoids center. The analysis employs a decay analysis (Lemma 3) and a geometric result (Lemma 4) showing that mistake probabilities shrink after errors. Strengths and Weaknesses: Strengths: Addresses the gap between computational and informational bounds in optimal regret. Simple algorithm with detailed analysis. Weakness: Limited to the realizable setting making agnostic extensions challenging. Questions: 1. Are there instances where alternative choices of wt fail to guarantee decay in both volume and surface area leading to significant regret 2. What are the main obstacles in extending the results to the agnostic setting", "Paraphrased Statement: Summary: This study analyzes the statistical properties of smoothed online learning for generalized linear functions. It establishes a statistical \\log(Tsigma) regret bound for Kwise linear classification and generalizes these results to settings involving linear classifiers in overparameterized polynomial feature spaces and piecewise regression with access to an ERM oracle. A geometric characterization of disagreement regions induced by generalized linear classifiers is developed along with a general anticoncentration bound for the determinant of certain matrix averages. Strengths and Weaknesses: Contributions: Explores the statistical characteristics of particular classes of realizable smoothed online classification problems (e.g. affine thresholds Kclass affine classification piecewise affine regression) that exhibit optimal regret bounds. Presents an efficient algorithm for noiseless contextual bandits that achieves the optimal horizondependent regret bound with logarithmic factors. Provides an alternative approach using the perceptron algorithm that results in polynomial regret under a \"directional smoothness\" parameter. Suggested Improvements: Enhance the organization and presentation for clarity. Provide a conclusion section. Questions: Improve the presentation to make the logic clearer. Elaborate on related work and provide more detailed comparisons. Explore potential algorithm improvements. Include experimental results to validate the theoretical findings. Grammar and Writing: Avoid starting sentences with references (e.g. \"Rakhlin et al. [2011] proposed smoothed adversaries\"). Ensure consistent tense usage throughout the paper. Punctuate equations at the end of sentences (e.g. equation (2.2)).", "Paraphrase: Summary: This paper addresses smooth online learning where the adversary selects contexts from a distribution that is \"smooth\" relative to a given dominating measure. The player knows this measure. For such problems a statisticalcomputational gap exists. This work aims to eliminate this gap for linear function classes in high dimensions. Main Contribution: The paper presents new algorithms for smooth online learning that achieve optimal regret guarantees while being computationally efficient. The key idea involves showing that error probability with respect to the dominating measure bounds the regret with respect to other smooth measures. The authors leverage a result that relates error probability to the size of the disagreement region. Strengths and Weaknesses: Strengths: Strong technical work Wellwritten with helpful proof sketches Interesting and novel techniques Weaknesses: Limited robustness of results to dominating measures Questions: 1. Define \"overly strong measures\" in Remark 2. 2. Are the results robust to changes in dominating measure How can we design agnostic algorithms 3. Are the algorithms agnostic to the value of sigma 4. How can we ensure sigma is O(1) in dimension 5. Can the results be extended to function classes without natural parametrizations", "Paraphrased Statement: The authors explore a subproblem within the field of online learning aiming to bridge the gap between the computational efficiency of polynomialcomplexity algorithms (poly(Tsigma)) and the exponential complexity of existing algorithms. This subproblem involves limiting the adversarys strategy by restricting them to choosing contexts with a density bounded by a known measure (mu). Strengths and Weaknesses: Strengths: Introduction of a computationally efficient algorithm that reduces the computational gap for the restricted problem. Novel application of the John ellipsoid within the algorithms foundation. Utilization of Villes inequality to bound the algorithms runtime. Independent theoretical interest of the results and techniques employed. Weaknesses: The study focuses on a specific subproblem within online learning rather than addressing the broader problem. No specific questions or concerns are raised in the review."], "wYgRIJ-oK6M": ["Paraphrase: The authors present new ways to turn transformer models into binary code using knowledge transfer. They aim to build a student transformer model that is just as good as the standard BERT model but uses less energy. Their main contributions are: 1. A way to turn transformer models into binary code that makes knowledge transfer better. 2. A way to improve the performance of binary models by transferring knowledge to them in several steps. The student model according to the research performs just as well as the standard BERT model on the GLUE benchmark and in answering questions. Strengths: A thorough overview of the background information and relevant research that is necessary to comprehend the suggested strategy. A welldefined description of the proposed strategy. Tips for making the model more reproducible. The authors present a thorough comparison with similar work on the GLUE benchmark. The studys results show that a transformer model that is effective has performance that is comparable to that of earlier SOTA models. Weaknesses: It is unclear how the models performance may be impacted by the initialization of parameters and the selection of hyperparameters. Questions for the Authors: Could you be more specific about how you chose the teacherstudent hyperparameters To reduce variance it may be helpful to conduct several random executions. However how pricey might this process become Do you think the results or findings would be different if different pretraining goals were used in the LM assignment (e.g. the following sentence)", "Paraphrased Statement: Summary: This paper presents various enhancements to the binary BERT model consisting of: Employing varied quantization schemes tailored to output distributions (e.g. 01 quantization for selfattention layers) Utilizing an elastic binarization function with rescaling and shifting capabilities Incorporating a multistep distillation and quantization schedule The resulting BiT model exhibits improved performance on tasks in GLUE and SQuAD datasets. Strengths: Effective techniques enhance the performance of binarized BERT models The proposed methods are wellreasoned Clear and concise writing with ample empirical evaluations Weaknesses: Lack of explicit discussion on the connection to recent related works such as BiBERT and Reactnet The paper doesnt highlight similarities between the elastic binarization function and the activation function in Reactnet The paper fails to address the hardware implications of BiT models which require specialized computation compared to traditional binary networks The authors do not provide insights into the training costs associated with the multistep distillation scheme Questions: Please elaborate on the relationship between the proposed techniques and existing related works. Analyze the hardware complexity and cost associated with the BiT model compared to traditional binary and ternary networks. Provide details on the training costs incurred by the multistep distillation scheme.", "Paraphrase: Summary: This study explores the quantization of transformer neural networks to extremely low precision (12 bits). Quantization aims to reduce model size (particularly for mobile devices) and accelerate evaluation on hardware without dedicated floatingpoint accelerators. Moreover binary activations enable the development of energyefficient highperforming hardware for neural network inference with enhanced parallelism and reduced power consumption. The paper summarizes previous efforts in quantizing convolutional networks and provides technical advancements for successful lowprecision transformer quantization. These techniques include normalization separate quantization for positive activations and distillation paths over intermediateprecision quantization. Using these methods the authors significantly improve the BLUE score closing the gap with the fullprecision model. A detailed ablation study analyzes the impact of each technique. Strengths and Weaknesses: Originality: The pursuit of extreme quantization is not novel its motivations have been discussed previously. However the successful quantization of transformer networks is significant as they have become prevalent in deep learning. While prior attempts at binarizing transformers led to performance degradation this research presents a multifaceted approach that yields highquality quantized models. Quality: The paper defines clear objectives establishes a welldefined experimental setup and provides comprehensive experiments and ablation analysis. It elucidates the significance of all employed methods and demonstrates the effectiveness of the proposed approach outperforming earlier quantized transformer models notably. Clarity: The work is wellmotivated and effectively references prior research. It offers a thorough account of the experimental evidence supporting the rationale behind each decision. Significance: While the underlying techniques are not entirely novel the paper establishes a benchmark for extreme transformer quantization with 1bit precision. This milestone provides a foundation for future research and motivates the development of specialized hardware for lowprecision neural networks. Questions: 1. Has 101 quantization been investigated as an alternative to the proposed 11 quantization potentially offering moderate cost with improved performance 2. How does quantization affect the accuracy of vision transformers compared to textual transformers"], "z64kN1h1-rR": ["Paraphrased Statement: Summary: This paper addresses an issue in offline RL pessimism estimation for ensemble methods that utilize shared targets for all ensemble updates. It proposes an alternative approach where each ensemble is updated individually and pessimism is applied during policy updates. The paper derives the update formulas for both methods in the NTK framework and demonstrates that the shared target method can lead to optimism. Synthetic data simulations support this finding. The paper also evaluates the proposed method on several offline RL benchmarks and shows its competitive performance. Strengths: Clear and wellstructured writing Sound technical analysis Identification of the flaw in existing methods and proposal of a more appropriate approach Extensive and convincing experiments Weaknesses: Theoretical results are based on the NTK assumption which may not fully capture realworld scenarios Varying hyperparameter settings for different tasks in the experimental results Questions: Major Questions: 1. Why is a second LCB term added to the QLCB estimation during policy update when the shared target already includes a pessimism term 2. Could the following baselines be considered: a) taking the minimum over ensembles with individual targets or b) bootstrapping each ensemble from its target and subtracting a common LCB term Minor Questions: Formula overflow in the algorithm box Small value of \u03b3 (0.1) in section 3.2 Inconsistent yaxis scaling in Figures 2 and 3 Interpretation of dots in Figure 3 Proof of line 763 for \u03b8lin(0) 0", "Summary This paper explores uncertainty estimation in offline reinforcement learning (RL) offering an alternative to introducing pessimism. While previous methods have used ensembles for uncertainty estimation this paper identifies a critical issue in incorporating lower confidence bounds (LCBs) into actorcritic algorithms. It demonstrates that regressing multiple Q functions to shared pessimistic target values and performing policy evaluation based on the LCB can lead to overestimation of the Q function. To address this the paper proposes a simple modification: instead of regressing to the shared LCB estimate regress to independent Q targets during the Bellman backup stage. Empirical results show improved performance in challenging tasks that require \"stitching\" (combining multiple policies). Additionally the paper examines the effectiveness of various ensemble methods in RL and highlights the performance gap compared to deep ensembles suggesting future research opportunities. Strengths and Weaknesses Strengths: Clear and easytofollow writing Identification and theoretical analysis of a flaw in LCB incorporation which has been overlooked in the literature Comprehensive empirical evaluation across various aspects including ensemble size and hyperparameters Exploration of transferring efficient ensemble methods from supervised learning to RL Weaknesses: Lack of explicit explanation of ensemble methods and subtle differences that impact results Uncertain whether overestimation from shared targets is observed in challenging tasks as suggested by the theoretical analysis Section 4.2 seems tangential to the main focus of the paper Questions: Clarification on ensemble methods used in the literature and the specific differences from the proposed method Explanation of how shared targets introduce \"doubling pessimism\" in both evaluation and learning steps Intuitive insights on when shared targets lead to overpessimism or optimism based on transition correlation and C matrix properties Rationale behind using a policy that plays the same action in the next state in the toy example Explanation for the observed performance decrease of shared targets with decreasing ensemble size", "Paraphrase: Summary This research explores \"ensemblebased pessimism\" in offline reinforcement learning (RL) from both theoretical and practical perspectives. Theoretical Analysis: Mathematically demonstrates using Neural Tangent Kernels (NTK) that under certain conditions pessimistic targets may surprisingly lead to optimistic Qvalue estimates. Empirical Evaluation: Introduces MSG an algorithm that trains multiple Qnetworks independently. Evaluates MSG on D4RL and RL Unplugged tasks showing improved performance when combined with CQL (Conservative QLearning). Strengths: Formal theoretical analysis of offline RL methods using Qensemble pessimism. Experimental validation of the proposed MSG algorithm. Weaknesses: NTK and Gaussian assumptions may limit broader applicability. MSG requires CQL for good performance making it challenging to identify the source of improvement. Incomplete experimental results for the D4RL benchmark. Questions: Why does the pessimism term for method 2 potentially become positive while it remains negative for method 1 How are equations 20 21 and 22 derived from equation 19 How are equations 31 and 32 derived from equation 30", "Paraphrased Statement: The paper questions the common design of ensemble critics in offline RL where critics share a pessimistic target function in their training. The authors argue that this approach can lead to optimistic critics and provide theoretical analysis to support their claim under the NTK assumption. They introduce an offline RL algorithm called MSG that leverages this insight and achieves stateoftheart results on offline RL benchmarks. The authors emphasize that separating the targets of critics is crucial for the superior performance of MSG."], "xOqqlH_E5k0": ["Paraphrase: Summary: This study presents a deep unrolled network based on proximal gradient descent (PGD) for snapshot compressive hyperspectral imaging. It incorporates a memory assistant descent module for gradient descent and a subnetwork with crossstage selfattention for proximal mapping. The proposed method proves superior to existing approaches in experimental evaluations. Strengths and Weaknesses: Strengths: Deep unrolling network is a valuable technique for snapshot compressive sensing. Modeling correlations between different iteration stages in deep unrolling networks enhances performance. Weaknesses: Novelty: Memory assistant deep unrolled networks have been explored previously (e.g. [1]). The relationship and differences with the current method should be clarified along with a comparison between them. Interpretability: Deep unrolled networks typically offer high interpretability but connecting different iteration stages diminishes this feature. Additionally using ConvLSTM to model correlations in gradient descent across stages deviates from the PGD approach. For hyperspectral snapshot compressive sensing other optimization techniques (such as ADMM) provide fast closedform solvers for data fidelity potentially rendering gradient descent and the memory assistant module redundant. Experimentation: PSNR and SSIM metrics only assess spatial fidelity between reconstructed and ground truth HSI. Spectral fidelity metrics (e.g. SAM ERGAS) are crucial especially considering the proposed spectral loss function. Writing: Citations for RMSProp and Adam are missing in line 61. There is no space between \"0\" and \"otherwise\" in line 195. Typographical errors should be corrected throughout the manuscript.", "Summary This work presents a novel deep learning architecture for snapshot compressive hyperspectral imaging inspired by proximal gradient descent. The architecture comprises two recurrent modules: the Gradient Update module (based on ConvLSTM) and the Proximal Mapping module (incorporating convolutions selfattention and triplet attention). The authors benchmark their method against several recent approaches on various datasets. Strengths Excellent performance on noisefree data Comparison with numerous stateoftheart methods Evaluation on multiple datasets to demonstrate superiority Ablation studies validate the contributions of different components Weaknesses Quantitative results are limited to noisefree data despite noise being mentioned in the measurement model No visual comparison against the topperforming method (HDNet MSTL) on real data with noise Quantitative results lack error bars Justification for using different recurrent architectures for similar tasks is not provided The role of triplet attention in the Proximal Mapping module is not fully explained It is unclear that the unrolled architecture offers advantages over traditional iterative methods The use of the same notation for convolutions and matrix multiplication could be confusing Questions Can visual comparison against HDNet MSTL on real data with noise be provided Can quantitative results on synthetic data with noisy measurements be included Can an ablation study with varying iterationstage numbers be conducted Can the justification for using different recurrent architectures for gradient update and proximal mapping be clarified Can the purpose of triplet attention in the Proximal Mapping module be elaborated Can the batch size be specified Minor Issues Consistent use of notation between convolutions and matrix multiplication Clarification of \"padding number\" Correction of typo in line 79 Omission of term C(k1) in equation 1011 Provision of citation for PyTorch Improved readability of Table 1 Indication of spectral channels shown in visual comparisons Zoomedin display of real data results for better comparison", "Paraphrased Statement: This paper introduces a technique for reconstructing a hyperspectral image from captured snapshots. Strengths: Not specified in the provided statement. Weaknesses: The papers organization needs improvement. The technical contributions and theoretical basis of the proposed approach are not sufficiently explained. Several instances of unclear explanations exist. Questions: The introduction should highlight the novel contributions and advantages of the proposed approach compared to previous methods. A clear description of the theoretical foundation supporting the proposed algorithms is necessary.", "Paraphrased Summary: This paper presents a deep unrolling networkbased approach for reconstructing spectra from snapshot hyperspectral data acquired by Compressive Acquisition and Spectral Separation Imaging (CASSI). The method combines modelbased and learningbased techniques and introduces novel components such as memoryassistant descent blocks crossstage attentive proximal subnetworks and a new loss function. Experiments demonstrate its superiority over existing unrollingbased and endtoend methods. Strengths and Weaknesses (Paraphrased): Strengths: Incorporates deep unrolling for spectral reconstruction from CASSI data. Achieves better results than iterative and endtoend methods in experiments. Weaknesses: Lacks discussion on algorithm initialization and its impact on convergence. Fails to address model complexity running time and comparison to endtoend methods. Quotes results directly from other papers raising concerns about proper citation practices. Provides no quantitative comparison using real data. Raises questions about the practical utility of reconstructing highfrequency spectral details from CASSI data given its limited spectral accuracy."], "nJWcpq2fco3": ["Paraphrased Summary: The paper introduces a framework for creating representations of trajectory data with missing segments. These representations are probability distributions that reflect the uncertainty of unobserved parts. The probability distribution can be used to: Fill in missing segments of trajectories (interpolation extrapolation and continuoustime sampling) Compare trajectories Adjust the given trajectory A distance metric is established for the distribution latent space. Selfsupervised triplet loss is used to train the framework maximizing the distance between similar trajectory segment representations and minimizing it between dissimilar ones. Experiments using human pose data and established baseline models (Variational RNN and Trajectron) demonstrate the frameworks effectiveness in prediction and interpolation tasks. Additionally qualitative results show how the learned representations can be manipulated to alter speed and temporal offset. Strengths: Clear and concise presentation Novel approach to representing trajectory data as a probability distribution Comprehensive exploration of problem formulation Generality for various applications Weaknesses: Lack of empirical evidence for some claims: Multimodality of prediction is not demonstrated in experiments. The use of learned representations for trajectory comparison is not evaluated against prior work. Minor Errors: Line 141: Division operation should be used instead of subtraction. Line 192: Should refer to \"Trajectron\" instead of \"Trajectrons.\"", "Summary This study introduces TrajRep a novel representation learning approach for spatial trajectories. TrajRep employs an encoderdecoder structure optimized using reconstruction loss and a selfsupervised triplet loss that enhances the learned representations. The triplet loss incorporates a detailed analysis of trajectory structure to define positive and negative pairs and utilizes a distance function based on conditional scenario box embeddings. TrajReps performance is evaluated using a human movement dataset. Strengths Innovative approach for trajectory modeling particularly the decoding method in Equation (2) and the distribution formalization in sections 2.4 and 2.5. Impressive qualitative experiments showcased in Figure 5. Thorough review of related work. Weaknesses Poor organization with essential details relegated to supplementary materials. Lack of justification for certain design choices such as the distance function and the value of \u03b1. Insufficient experiments including baseline comparisons ablation studies hyperparameter sensitivity analysis and training curves. Questions Organization: Figure 6 should be included in the main text. Section 2.3 needs clarification. Box embedding formalization should be highlighted in section 2.5. Support: The claim of generalizability to any spatial trajectory is unsubstantiated by the dataset used. The basis for choosing the boxembedding distance function should be justified. The labels in Figure 6 need explanation. Experiments: More baselines ablation studies and sensitivity analysis are necessary. Training curves would shed light on convergence during training. Conclusion The study requires improvements in presentation and experimental rigor before it can be considered for publication. The authors responses to the questions address some of the concerns but additional effort is needed.", "Paraphrased Statement: This research introduces a framework that encodes spatial trajectories into probability distributions within a learned latent space. This representation enables the framework to: Predict future and past trajectories Reconstruct continuous trajectories from discrete inputs Interpolate between trajectory segments Compare trajectories Modify existing trajectories The framework consists of an encoder trained with selfsupervised triplet loss and a decoder trained using regression loss. It was evaluated on human movement datasets outperforming baselines such as VRNN and Trajectron. Strengths: Clear and understandable paper Framework capable of performing multiple tasks without retraining Improved performance compared to older baselines Issues and Suggestions: Use more recent baselines for comparison Other Comments: Line 192: Typo should be \"Transformer\"", "Paraphrase: Summary: This research introduces a method for predicting and interpolating trajectories by learning their representations in a hidden space. A transformer encoder is used to learn the parameters of a probability distribution while a decoder reconstructs trajectory points from their embeddings in the hidden space. A triplet loss ensures similarity between embeddings from the same trajectory segment and a reconstruction loss guides the decoder. Strengths: Novelty compared to previous methods which often focus on autoregressive predictions or insufficient modeling of future representations. Clear and wellwritten presentation exemplified by demonstrations of the networks interpolation and extrapolation capabilities in various contexts. The method allows for sampling from a distribution offering advantages such as flexible sampling intervals and generalization to new latent spaces. Superior performance to Trajectron a stateoftheart method on a human pose prediction task. Weaknesses: Limited ablation studies such as: Influence of the number of samples on performance. Impact of the choice of encoding distribution. Sensitivity to irregular sampling from the distribution. Lack of analysis of noncompliant trajectories (e.g. hand waving) and repetitive trajectories in the learned latent space. Evaluation limited to the specific task of joint space trajectory prediction with no consideration of other domains (e.g. autonomous driving crowd prediction). Absence of context aggregation which could hinder generalization outside of joint space prediction. Questions: Followup questions related to the weaknesses mentioned above such as the impact of various factors on network performance and the potential limitations of the lack of context aggregation."], "q-tTkgjuiv5": ["Paraphrased Summary: This paper investigates methods for fairly distributing graphical resources considering two fairness measures: maximin share (MMS) and envyfreeness up to one item (EF1). In this problem resources are represented as nodes in a graph and the goal is to allocate or partially allocate these nodes to a group of agents. Each agent has their own valuation function for the edges in the graph and their satisfaction with a particular allocation is determined by the maximum matching weight in the subgraph corresponding to their allocated nodes. For the case of homogeneous agents (i.e. agents with identical valuation functions) the paper demonstrates that a basic greedy approach results in an MMS allocation for unweighted graphs. However this approach lacks a guaranteed approximation guarantee in more general scenarios. Consequently an alternative algorithm is proposed that yields a \u215bMMS allocation in polynomial time. Since achieving tight MMS approximations can lead to suboptimal social welfare the paper also explores EF1 fairness to balance fairness with social welfare preservation. Theorem 3.6 establishes a polynomialtime algorithm that generates an EF1 allocation with an approximate optimal social welfare outcome. For heterogeneous agents (i.e. agents with varying valuation functions) the paper proves that no algorithm can provide both bounded approximation for MMS and better than 1n approximation for social welfare under EF1 fairness. Nevertheless the paper analyzes two specialized cases: 1. When agents have binary weight functions Algorithm 3 produces an EF1 allocation with a 13 approximation for social welfare. 2. When there are only two agents Algorithm 4 generates an EF1 allocation with a tight \u2153 approximation ratio for social welfare. Strengths and Weaknesses: Strengths: Originality: This work presents a novel framework for fair division of indivisible graphical items with combinatorial valuations using MMS and EF1 fairness concepts. Quality: The notations are clearly defined and the theorems and lemmas are formally presented and proven. Clarity: The paper is wellorganized and comprehensible with proper references to related studies. Weaknesses: Related Works: It would be beneficial to explore other relevant works that combine graph partitioning and fair division. Approximation Ratios: While the paper establishes certain approximation guarantees providing lower bounds for the results would add to their significance. 14 Approximation: In Lemma 3.3 the paper states that the approximation ratio is at least 14. However it would be helpful to clarify why the ratio cannot be \u00bd instead.", "Paraphrased Statement: This paper introduces a novel type of fair division problem that involves allocating indivisible resources (items) to multiple groups (agents) who have complementary preferences. Each agents preferences are represented by a weighted graph where nodes represent items and edge weights reflect the value of pairing the items connected by those edges. The value of a collection of items is calculated by finding the maximum weight matching in the graph induced by the nodes representing those items. The motivating example for this model is a company with departments (agents) that need to pair up engineers (items) to maximize their productivity. Other scenarios include situations where employees need to be paired to complete tasks that generate value. The study focuses on achieving fair and efficient allocations. Fairness is measured using two criteria: envyfreeness up to one item (EF1) and maximin share (MMS). Efficiency refers to the maximization of total social welfare. Several positive and negative results are presented regarding the efficient calculation of allocations that guarantee a specific fraction of agents MMS or a certain level of social welfare subject to constraints such as guaranteeing EF1 for homogeneous or heterogeneous agents. Strengths: The model is innovative and potentially interesting to researchers in fair division of indivisible items. The theoretical findings are significant and the proofs provided are persuasive. The motivation for the problem is realworld inspired and offers avenues for future research. Weaknesses: The relevance to NeurIPS is unclear. The motivating examples lack concrete examples and references. The significance of the results is not explicitly highlighted. The writing style needs improvement. Theorem statements and arguments could be more precise and clear.", "Paraphrased Statement: Summary This paper introduces a novel resource allocation problem where resources are represented as vertices in a graph and the goal is to divide these resources among N agents. Each agents utility is determined by the maximum matching in the induced subgraph of their assigned partition. The paper focuses on two fairness measures: Maxmin share (MMS): Maximizing the minimum utility among agents Envyfree up to one item (EF1): Ensuring that no agent envies anothers allocation after removing up to one item Main Contributions: A 1.8approximation algorithm for the MMS version when all agents are identical A constantfactor approximation algorithm for the EF1 variant Hardness results and improved approximations for specific cases Strengths and Weaknesses: Strengths: Readability and ease of understanding Weaknesses: Limited Motivation: The hypothetical example of \"pairing up employees\" does not provide a convincing justification for the problem definition. Rehashed Algorithms and Analysis: The proposed algorithms and analysis are largely unoriginal reusing established ideas from existing approximation algorithms such as maximum matching and independent edge decomposition. Questions: Motivating Examples: Clarify if there are any practical applications or use cases for the proposed problem definition. Novelty and Contribution: Emphasize the specific novel contributions of the paper that distinguish it from existing works.", "Summary This research examines a variant of the indivisible item allocation problem where items correspond to vertices in an edgeweighted graph. An agents value for a bundle is determined by the maximum weight matching on the subgraph formed by the bundles vertices. In the homogeneous setting all agents share the same graph representing their valuations. In the heterogeneous setting agents have differing edge weights. This model aims to address situations where workers need to collaborate in pairs. The goal is to find assignments (allocations) that maximize overall benefit (social welfare) while ensuring fairness (MMS or EF1). The approximation ratio is not defined in the conventional way as the social welfare is compared to the optimal value without fairness constraints akin to the concept of \"price of fairness.\" Strengths and Weaknesses Strengths: Comprehensive results for both homogeneous and heterogeneous settings Approximation algorithms with provable guarantees The MMS algorithm employs novel techniques including greedy scheduling edge weight rounding and special case handling. Positive results for special cases with binary edge weights or two agents. Weaknesses: The models applicability may be limited to specific scenarios where workers operate exclusively in pairs. Complexity results (e.g. hardness of approximation) are largely absent. Questions Are there tight examples demonstrating the tightness of the 18MMS approximation algorithm"], "zYc5FSxL6ar": ["Paraphrased Statement: This research addresses the challenge of modular reinforcement learning (RL) for systems with numerous degrees of freedom (DoF) such as humanoid characters. The authors propose a twopart solution: 1. Unsupervised Clustering: Group actuators based on their impact on the value function and agent morphology using affinity propagation. 2. AttentionBased Architecture: Create a hierarchical controller where the policy generates lowdimensional synergy actions that map to individual actuator actions. Experiments demonstrate that the method outperforms baseline methods in locomotion tasks with diverse morphologies. An ablation study confirms the significance of synergy learning and the attentionbased model. Strengths: Synergy Learning: Reduces control complexity in highDoF systems. Performance: Achieves high learning performance in challenging control tasks. Analysis: Ablation study and synergy visualization provide insights. Applicability: Potentially suitable for other highDoF control problems. Weaknesses: Task Specificity: Most effective for humanoid tasks comparable results on other tasks. Task Variety: Limited to a single locomotion task additional tasks would strengthen the method. Implementation Details: Missing information on task sharing synergy computation and learning interleaving. Questions: 1. Do all tasks share critic and actor weights in multitask training 2. Are synergies for different morphologies computed independently 3. What is the frequency of interleaving actorcritic updates with synergy updates 4. While the proposed synergy formulation addresses limitations of neuroscientific approaches are there potential benefits of using a more neuroscienceinspired approach", "Paraphrased Version: Summary: The authors present a novel reinforcement learning algorithm that combines unsupervised learning for modular control in multitask environments. They propose autoclustering body joints into synergy groups based on their value function contributions to tasks. This enables control in the synergy space rather than the joint space. A mapping function then converts synergy actions back to joint actions. The algorithm outperforms existing baselines in multiple task benchmarks. Strengths: The proposed algorithm demonstrates significant improvement over baselines in benchmarks. The authors provide thorough ablation studies. The timelapse visualization of synergy group propagation (Figure 5) is valuable. Weaknesses: The explanation of affinity propagation in the background section needs clarification. Undefined notations such as the Q function in Equation 5 can be confusing. A diagram of the proposed network architecture for Section 3.2 would enhance understanding.", "Summary The authors introduce a novel framework called SynergyOriented LeARning (SOLAR). This framework utilizes the redundancy in degrees of freedom (DoF) in robot control. It is inspired by the human bodys \"muscle synergy\" concept where the central nervous system reduces the DoF of the musculoskeletal system. In SOLAR this synergy is learned without supervision. Experimental results demonstrate the effectiveness of this approach. Strengths and Weaknesses The application of muscle synergy to modular reinforcement learning (unsupervised) is innovative and intriguing. Experimental results support the effectiveness of SOLAR. The analysis of the learned synergies yields insightful results. Questions To what extent does SOLAR align with traditional kinematic or muscle synergies (often analyzed using PCA) A mathematical comparison would be beneficial. Many ML researchers may be unfamiliar with muscle synergy while those studying human body control (e.g. musculoskeletal system CPG) may have limited knowledge of ML. This paper has the potential to bridge these communities. The connection between SOLAR and the conventional muscle synergy concept particularly in relation to the selfattention mechanism should be clarified. Do the authors hypothesize that human muscle synergies are organized in this manner Are there any implications regarding human muscle synergy based on the findings of this study Such a discussion could enhance the papers impact."], "vgIz0emVTAd": ["Paraphrased Statement: This paper introduces DISCO a method for defending classifiers against adversarial attacks. DISCO consists of an encoder and a local implicit module. During inference DISCO takes an image (clean or perturbed) and a pixel location as input and outputs a clean RGB value. This value is expected to remove any adversarial perturbations improving classifier accuracy. DISCOs Strengths: Demonstrated high robust accuracy on various datasets and with different attack methods. Unlike adversarial training DISCOs robustness is independent of the specific attack or data used. DISCOs Weaknesses: The technical descriptions of the encoder and local implicit module could be clearer. The method builds upon existing ideas of adversarial removal with limited novelty. There are some typos in the paper. Questions: 1. What benefits does the local implicit module provide over a global module Is it solely for efficiency 2. What were the parameters of the PGD attack used for training and testing 3. Why does the \"K\" column in Table 9 only have 3 values for defense but 5 values for attacks", "Summary Paraphrase: Researchers present a method using local implicit functions to fix adversarial image perturbations. This approach is inspired by work on image manifolds and implicit representations for 2D and 3D data. Instead of altering the entire image the method focuses on individual pixels using information from a small surrounding area which is then adjusted using an MLP (multilayer perceptron). By minimizing the L1 distance of the MLPs output compared to the original pixel color the model can effectively remove distortions. Experiments demonstrate the proposed approachs superiority over previous defense methods in several settings with improved transferability. Strengths: Isolates defense module training from the base classifier. Reduces the number of defense network parameters significantly. Enhanced robustness and transferability compared to existing defenses. Weaknesses: Essentially an image smoothing technique with inherent drawbacks: Slow inference due to processing each pixel patch through the MLP. Vulnerability to nonnormbounded attacks and other forms of adversarial manipulation. Questions: 1. Clarification is needed regarding the methods description as a projection on the \"image\" manifold instead of the \"patch\" manifold and how global information is incorporated into the implicit module. 2. More detailed timing statistics are requested to fully assess the computational efficiency of the method. 3. The excessive use of references could be streamlined by reducing the number in the main text and providing a separate \"see also\" section. 4. Minor language errors should be corrected for clarity.", "Paraphrase: Summary This research presents a novel defense technique against adversarial attacks that employs implicit functions. It takes an image that has been attacked and predicts the clean original image. The proposed \"implicit module\" analyzes the information around each pixel to infer the value of the pixel itself. Experiments demonstrate the effectiveness of this approach on CIFAR10 CIFAR100 and ImageNet datasets. Strengths and Weaknesses Strengths: Simplicity and ease of understanding Demonstrated effectiveness in experiments Weaknesses: Limited novelty with similar strategies being used in denoising methods Requires two forward passes for image restoration and classification while adversarial training methods only require one Uneven writing quality with some sections lacking specific details or clarity", "Summary Paraphrase: DISCO is a defense against attacks on image models. It removes distortions from images by using a local implicit representation. This representation is built by a convolutional network then decoded by a neural network that considers the pixels location and nearby features. DISCO is trained on paired clean and distorted images. It is efficient and generalizes well even with limited training data. It is also robust to various attacks and can be combined with other defenses to enhance robustness. Strengths: Robust across datasets norms and architectures Efficient in terms of computation data and parameters Generalizable requiring only a small amount of training data Transferable to different attacks Weaknesses: Acknowledgment of LIIFs contribution to the underlying approach is insufficient May reduce standard accuracy compared to adversarial training defenses Excludes recent testtime defense approaches in its survey Does not evaluate transfer attacks from the underlying classifier to the defenseclassifier combination", "Paraphrased Statement: Summary: The authors introduce DISCO a method that employs localized manifold projections to mitigate adversarial perturbations. Their goal is to generate the original RGB value for a perturbed image at a specific pixel location. DISCO relies on the assumption that the necessary manifold projection can be localized to specific image patches rather than the entire image given the relationship between natural and perturbed images. Strengths and Weaknesses: Strengths: Wellwritten paper Important problem being addressed Thorough experimental evaluation Weaknesses: Limited originality Lack of indepth theoretical analysis Questions: How does DISCO compare to existing methods in the field such as those by Xu et al. Li et al. and Bai et al. in terms of performance"], "yfrDD_rmD5": ["Paraphrase: Summary: This paper extensively investigates how different model parameters contribute to data influence measurement. It highlights two key observations: Lowerlayer parameters capture more informative gradients. Parameters used by numerous training examples experience a \"cancellation effect\" (gradients cancel each other out). Inspired by these insights the paper proposes a novel data influence method that replaces parameters from the last layer used in TracIn with those from the embedding layer. Empirical findings validate the insights and the effectiveness of this new method. Strengths: The paper presents two valuable insights about data influence. The proposed method effectively considers both low and highlevel similarity. The paper is wellwritten and clearly presented. Weaknesses: The paper uses ambiguous terms like \"share logic\" and \"unique logic.\" Explanations and evidence are needed. An ablation study should be conducted to demonstrate the impact of using special tokens only on the full dataset. Removing data points could affect the training process. Its unclear how this was addressed to ensure consistency across methods. The influence of data samplers random seed on model performance is not discussed. Questions: Is gradientbased similarity (S) essential for the proposed method There are some typos in the paper such as \"parametersin\" in line 36 \"share\" logic\" in line 194 and \"multiple periods.\" in line 215. The appendix contains Figure 4 but its unclear if its meant to be \"Appendix Figure 4 or on\".", "Paraphrased Statement: Summary: This research presents TracInWE a method that calculates data influence scores based on both the overall training input and individual words in the input. TracInWE aims to reduce the \"cancellation effect\" issue caused by data influence extraction methods that rely on last layer weights. It is inspired by TracIn a previous method that extracts influence from the last layer. Strengths: TracInWE is built on the premise that existing data influence methods may exhibit cancellation across attributions resulting in a loss of influence for training examples. This observation could lead to improvements in explainable NLP methodologies. Weaknesses: Example 3.1 presents a unclear comparison between the sparse representation of the example and the dense representation of word embeddings used by TracInWE. While word embeddings represent input data data influence is primarily determined by output layers linked to the loss function. Using embedding space for data influence extraction may have limitations in deep neural network training. It remains unclear how TracInWE addresses fundamental issues like vanishing gradients and learning relevance when calculating influence based on the embedding layer. The justification for removing bias from the TracIn calculation to reduce cancellation is not fully explained. Question: What is the interpretation of the \"cancellation ratio\"", "Paraphrase: Summary: This study reexamines common approaches to calculating the influence of training data particularly those that focus solely on the final layer. It explores ways to enhance the accuracy of estimating individual examples influence. In natural language processing (NLP) models are frequently large often incorporating pretrained language models as a foundation. Typically influence functions are computed only from final layers. The paper demonstrates that lastlayer representations in text classification models can experience a \"cancellation effect\" when calculating the influence function on training samples. To address this issue the paper introduces a technique called TracInWE. It modifies the existing TracIn method to compute influence on the word embedding layer rather than the final layer. Experiments conducted on three text classification tasks show that TracInWE outperforms other influence computation methods applied to the last layer. Strengths: Highlights limitations of existing influence computation methods based on last layer calculations. Demonstrates improved performance with the proposed method through empirical evaluation. Addresses a critical research area in Explainable AI (XAI). Weaknesses: The method is essentially a basic extension limiting technical novelty. Questions: Have you attempted to visualize the reduction in the cancellation effect for each layer in the model"], "ofRmFwBvvXh": ["Paraphrased Statement: Summary: The study investigates deep convolutional neural networks (CNNs) for image classification. Key findings include: Error rates for approximating smooth functions using CNNs in different mathematical spaces Bounds on sample errors Estimates for misclassification error in resulting classification algorithms Strengths: CNNs are widely used for image classification and have unique analysis challenges due to their convolutional structure. The study considers unbounded target functions and introduces a truncation approach for error estimation. It explores an \"almost manifold\" structure in the input space leading to improved error rates. Weaknesses: The authors do not clarify the absolute continuity assumption in their proofs. A typo exists in the proof of Theorem 4.5 regarding the domain definition. The meaning of the network parameter \\(\\mathcal W\\) should be clarified. Comparison: The authors provide a comparison for approximation error results but not for misclassification error estimates. Questions: The reviewer raises several questions regarding the absolute continuity assumption the typo in the proof of Theorem 4.5 and the network interpretation of \\(\\mathcal W\\).", "Paraphrase: Summary: This research introduces improved error limits for approximating functions in Sobolev spaces using deep convolutional neural networks (CNNs). The limits depend polynomially on the space dimension and highlight the connection between network depth filter length and approximation error. By assuming the functions domain is approximated by a lowdimensional manifold the error bounds are considerably enhanced mitigating the \"curse of dimensionality.\" This result is utilized to derive an error bound for the excess risk in classification tasks separating stochastic and approximation errors. The bound is then further specified for the crossentropy loss function. Strengths and Weaknesses: The contribution appears valuable but its significance compared to prior work is unclear. The paper is wellwritten and accessible with clear definitions of mathematical concepts. The structure progresses from general bounds to specific applications increasing understanding. The assumption of an approximate manifold domain is realistic. A toy example comparing theoretical and practical error bounds would enhance understanding of the estimates accuracy. The exclusivity of Sobolev functions may not fully capture why CNNs excel in certain applications where spatial invariance is crucial suggesting a gap between theory and practical uses. Questions: Is a toy example for comparing theoretical and practical error bounds feasible Why are Sobolev functions considered and what potential gap exists between their theoretical approximation and common CNN applications", "Summary: This paper introduces new error bounds for approximating functions in Sobolev and H\u00f6lder spaces using deep convolutional neural networks (CNNs). These bounds depend polynomially on the ambient space dimension providing tighter approximations than previous studies. Furthermore they showcase improved tightness when target functions lie on lowerdimensional manifolds mitigating the curse of dimensionality and enabling faster convergence. The authors apply these bounds to analyze binary classification tasks showing that the difference between expected and empirical misclassification risk can be bounded by a sum of stochastic and approximation errors. The stochastic error bounds do not require uniformly bounded CNN parameters providing flexibility in parameter values. Approximation error bounds are inversely proportional to the overall bound of the approximating function. The paper includes explicit calculations of these bounds for common binary classification loss functions. Strengths and Weaknesses: Strengths: Novel contribution with detailed substantiation and calculations for practical applications. Weaknesses: Limited to binary classification tasks and overlooks recent related work. Questions: 1. The authors could extend their analysis to include other functional approximation tasks such as regression and denoising where smoothness is crucial."], "wSVEd3Ta42m": ["Paraphrase: Summary The authors address the issue of learning a policy that minimizes risk aversion (CVaR) in a distributional reinforcement learning setting. While previous methods have proposed a simple approach to CVaR optimization this work demonstrates that it does not optimize the desired objective. The authors introduce a theoretically sound method for iteratively deriving a CVaRmaximizing policy supported by experimental evidence. Strengths Theoretically \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u0430\u044f 1\u0448\u0430\u0433\u043e\u0432\u0430\u044f \u0438\u0442\u0435\u0440\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0441\u0445\u0435\u043c\u0430 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u043e\u043b\u0438\u0442\u0438\u043a\u0438 \u043c\u0430\u043a\u0441\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u044e\u0449\u0435\u0439 CVaR \u0432 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0445 \u0443\u0441\u043b\u043e\u0432\u0438\u044f\u0445 RL. \u0410\u043d\u0430\u043b\u0438\u0437 \u0438 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 \u043c\u0435\u0442\u043e\u0434\u043e\u0432 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0439 \u0447\u0442\u043e \u043d\u0435\u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 1\u0448\u0430\u0433\u043e\u0432\u043e\u0433\u043e CVaR \u043d\u0435 \u0434\u043e\u0441\u0442\u0438\u0433\u0430\u0435\u0442 \u0446\u0435\u043b\u0438. \u042d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0441 \u0441\u0438\u043d\u0442\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043e\u043f\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0442\u043e\u0440\u0433\u043e\u0432\u043b\u0435\u0439 \u0438 ATARI. \u0421\u043b\u0430\u0431\u043e\u0441\u0442\u0438 \u0410\u043d\u0430\u043b\u0438\u0437 ATARI \u0432 \u044d\u0442\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435 \u043e\u0447\u0435\u043d\u044c \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d. \u0425\u043e\u0442\u044f \u043f\u0440\u043e\u0441\u0442\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0442 \u043f\u0440\u043e\u0432\u0435\u0441\u0442\u0438 \u0431\u043e\u043b\u0435\u0435 \u0442\u0435\u043e\u0440\u0435\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u043c\u0435\u0442\u043e\u0434\u0430 \u0431\u043e\u043b\u0435\u0435 \u0441\u043b\u043e\u0436\u043d\u044b\u0435 \u043e\u0431\u043b\u0430\u0441\u0442\u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0442 \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u0443\u0431\u0435\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u044d\u043c\u043f\u0438\u0440\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435. \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0432 \u0442\u0430\u043a\u0438\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445. \u041e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u0440\u0430\u0437\u0434\u0435\u043b \u0437\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u0438 \u0431\u0443\u0434\u0443\u0449\u0438\u0445 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0439. \u0425\u043e\u0442\u044f \u0442\u0430\u043a\u0438\u0435 \u0440\u0430\u0437\u0434\u0435\u043b\u044b \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c\u0438 \u043e\u043d\u0438 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043e\u0431\u0449\u0435\u043f\u0440\u0438\u043d\u044f\u0442\u043e\u0439 \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u043e\u0439 \u0438 \u0436\u0435\u043b\u0430\u0442\u0435\u043b\u044c\u043d\u044b. \u0427\u0442\u043e\u0431\u044b \u043e\u0441\u0432\u043e\u0431\u043e\u0434\u0438\u0442\u044c \u043c\u0435\u0441\u0442\u043e \u0434\u043b\u044f \u043e\u0431\u0441\u0443\u0436\u0434\u0435\u043d\u0438\u044f \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0430 \u043c\u043e\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u043d\u0435\u0441\u0442\u0438 \u0432 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435. \u0412\u043e\u043f\u0440\u043e\u0441\u044b 1. \u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0441 Dabney et. al. 2018 (a) \u043a\u0430\u0436\u0435\u0442\u0441\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0432\u0432\u043e\u0434\u044f\u0449\u0438\u043c \u0432 \u0437\u0430\u0431\u043b\u0443\u0436\u0434\u0435\u043d\u0438\u0435. \u0426\u0435\u043b\u044c\u044e \u0440\u0438\u0441\u043a\u043e\u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u043c\u0435\u0442\u043e\u0434\u0430 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043c\u0430\u043a\u0441\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f \u043d\u0438\u0436\u043d\u0435\u0439 \u0433\u0440\u0430\u043d\u0438\u0446\u044b \u0434\u043e\u0445\u043e\u0434\u043d\u043e\u0441\u0442\u0438. Dabney et. al. 2018 (a) \u043c\u043e\u0436\u0435\u0442 \u043d\u0435 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c CVaR \u0434\u043e\u043b\u0436\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u043d\u043e \u0438\u0445 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0442 \u043a\u043e\u043d\u043a\u0443\u0440\u0435\u043d\u0442\u043e\u0441\u043f\u043e\u0441\u043e\u0431\u043d\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0432 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445 ATARI (\u0447\u0430\u0441\u0442\u043e \u043f\u0440\u0435\u0432\u043e\u0441\u0445\u043e\u0434\u044f\u0449\u0438\u0435 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044e \u0441\u0440\u0435\u0434\u043d\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439). \u0412 \u044d\u0442\u0438\u0445 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u0445 \u043c\u044b \u0432\u0438\u0434\u0438\u043c \u0447\u0442\u043e \u043e\u0442\u0431\u043e\u0440 \u041c\u0430\u0440\u043a\u043e\u0432\u0430 \u0443\u0441\u0442\u0443\u043f\u0430\u0435\u0442 \u043d\u0435\u0439\u0442\u0440\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 \u0432 \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 \u0440\u0438\u0441\u043a\u0430. \u0427\u0442\u043e \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0432 \u0431\u043e\u043b\u0435\u0435 \u043f\u0440\u043e\u0441\u0442\u044b\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445 \u043a\u043e\u0433\u0434\u0430 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c \u0437\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u0442\u0441\u044f \u0441 \u0442\u0435\u043c\u0438 \u0436\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 \u0447\u0442\u043e \u0438 \u0443 Dabney et. al. 2018 (a) \u0438 \u0432 \u0442\u0435\u0447\u0435\u043d\u0438\u0435 \u0431\u043e\u043b\u0435\u0435 \u0434\u043b\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0438\u043e\u0434\u0430 \u0432\u0440\u0435\u043c\u0435\u043d\u0438 (\u043e\u0431\u044b\u0447\u043d\u043e 200 \u043c\u043b\u043d \u0448\u0430\u0433\u043e\u0432) 2. \u041a\u0430\u043a \u044d\u0442\u043e\u0442 \u043c\u0435\u0442\u043e\u0434 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u0432 \u0434\u0440\u0443\u0433\u0438\u0445 \u043e\u0431\u043b\u0430\u0441\u0442\u044f\u0445 \u041e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d \u043b\u0438 \u043e\u043d Asterix", "Paraphrased Summary The authors investigate learning policies that consider risk in reinforcement learning (distributional RL). They focus on Conditional Value at Risk (CVaR) as a measure of risk. They start by explaining the concepts of risksensitive policies and discuss the differences between static and dynamic CVaR problem formulations. They then propose a simple approach called the Markov actionselection strategy which aims to maximize the alphaCVaR of return distributions at each state. However they show that this approach does not lead to an optimal policy. To address this issue they introduce an augmented Markov Decision Process (MDP) with an additional parameter that represents a threshold. This parameter tracks the \"returntogo\" needed to reach the initial threshold. Using this augmented MDP they demonstrate that it is possible to recover an optimal CVaR policy under certain assumptions. They propose a distributional Bellman operator that employs the optimal actionselection strategy in the augmented MDP. They show that this operator has a fixed point that is equal to the optimal return distribution. Based on this they develop Algorithm 1 that optimizes a neural network policy using the new distributional Bellman operator. Experimental results show that this approach outperforms alternative methods in synthetic and realworld RL environments. Strengths and Weaknesses Strengths: Novel theoretical advancements in CVaR distributional RL Potential for further research such as proving the contractive property of the Bellman operator Clear presentation of the problem and contributions Weaknesses: Abrupt conclusion without summary remarks Limited discussion and analysis of empirical results Lack of intuitive explanations for the obtained results", "Paraphrased Statement: This study introduces a novel robust reinforcement learning technique that aims to maximize both static and dynamic Conditional Value at Risk (CVaR). It demonstrates that the current action selection mechanism in distributional reinforcement learning (DRL) deviates from the CVaR goal. A novel approach is proposed that is shown to effectively learn to optimize both static and dynamic CVaR. Experiments illustrate the superiority of the proposed method in the context of option trading compared to riskneutral and Markov action selection strategies. Strengths and Weaknesses: Strengths: Addresses a significant problem. Provides insightful theoretical and experimental findings. Weaknesses: Lacks clear comparison and positioning against other robust RL approaches. Evaluation may benefit from inclusion of relevant missing baselines. Questions: What does it mean when distributional RL fails to converge to static or dynamic RL How does the proposed method compare to other existing literature on robust policy optimization such as riskconstrained reinforcement learning with percentile risk criteria risksensitive decisionmaking and tractable objectives for robust policy optimization", "Paraphrased Statement: This study explores a method for learning a riskaverse strategy using Conditional ValueatRisk (CVaR) via reinforcement learning (RL) based on probability distributions. The authors propose a modified optimal operator for CVaR that satisfies the fixedpoint condition. They also introduce an algorithm that combines the Quantile Regression Distributional Double QNetworks (QRDQN) with the proposed operator. Experiments on simulated and realworld data demonstrate the algorithms effectiveness. Strengths: The paper is clearly written and understandable. It identifies a limitation of the standard distributional Bellman operator for CVaR and presents a suitable modification. Weaknesses: The theoretical framework does not provide strong guarantees for the algorithms convergence. Questions: 1. Although the modified operator \\tilde\\mathcalT\\psi may not ensure convergence of the distribution can C\\alpha(\\tilde\\mathcalT\\psi U) still converge to C\\alpha(Z\\pi) for any U 2. The proposed algorithm is compared to IQN (Dabney et al. 2018a) rather than using the same underlying structure (IQN or QRDQN) for approximating distributions. 3. The experimental results on Atari games indicate a concentration of the estimated distribution using the proposed algorithm. Are there any advantages or impacts associated with this concentration"], "mMT8bhVBoUa": ["Summary: A novel method is proposed for variational inference in function space using Wasserstein distance instead of KL divergence. The authors demonstrate how to use Gaussian measures for the prior and approximate posterior to approximate the Wasserstein distance for efficient loss optimization. They also introduce two parameterization methods for the variational posterior: one based on sparse Gaussian processes and one using deep neural networks. The paper provides experimental results in 1D regression UCI regression and image classification (including OOD detection). Strengths: Technically sound and promises improved inference in neural networks. Easy to follow and selfcontained. Weaknesses: The assumption of Gaussian posteriors could be limiting especially considering recent findings suggesting nonGaussian posteriors may perform better. Limited experimental evaluation with potential concerns about comparison to other methods. Minor reference section issues and inconsistent parameterization notation. Questions: Can the example in Figure 1 be expanded to include comparisons with different models (e.g. GWISVGP GWIDNNSVGP SVGP) How are prior parameters and inducing points handled (fixed crossvalidated or optimized)", "Paraphrased Statement: Summary The authors propose using the Wasserstein distance between Gaussian distributions in infinitedimensional function spaces for generalized variational inference. This method can be applied to distributions over functions including Gaussian processes and Bayesian neural networks. Strengths and Weaknesses Strengths: Clear and mathematically sound presentation. Novel application of Wasserstein inference in function spaces. Potential advantages over standard variational Gaussian processes. Weaknesses: Low performance in experiments raising concerns about efficacy. Lack of ablation studies to evaluate method components. Unclear applicability to Bayesian neural networks as suggested in the title. Questions Major: Impact of inducing point selection on performance and estimated distances. Theoretical guidance on optimal selection strategies. Comparison of the proposed method to a standard sparse Gaussian process implementation. Extension to Bayesian neural network inference in function space using stochastic weights to capture uncertainties. Explanation of low performance in experiments and comparison to stateoftheart baselines. Minor: Omission of relevant baselines: functional SVGD and functional repulsive ensembles. Comparison of runtimes with baselines.", "Paraphrase: Summary: The paper introduces \"generalized variational inference in function space\" (GVIFS) a probabilistic regression and classification framework. It modifies the standard functionspace ELBO formulation by replacing KL divergence with Wasserstein2 metric \\(\\mathcalW2(\\mathbbQF \\mathbbPF)\\) to regularize \\(\\mathbbE\\mathbbQ[\\log p(yF)]\\). This substitution stems from generalized variational inference and challenges with KL divergences. GVIFS uses Gaussian measures (GMs) for predictive function parametrization but Gaussian processes (GPs) with additional assumptions provide an equivalent approach. The paper proposes two GVIFS variants: one with mean and covariance parametrized by a sparse variational GP and another with mean parametrized by a neural network and covariance parametrized by a sparse variational GP. It presents experimental results in regression classification and outofdistribution detection comparing GVIFS to existing probabilistic methods. Strengths: Strong technical soundness. Addresses significant problems in classification regression and OOD detection. Demonstrates significant performance improvements over baseline methods. Weaknesses: The motivation from \"generalized\" variational inference is weak. Alternative interpretations are proposed linking GVIFS to GP regression with Wasserstein2 regularization or Bayesian inference with early stopping. Experimental Evaluation Suggestions: Include a comparison with a DNN with the same architecture and objective as DNNSVGP but with predictive mean and uncertainty output by the network. Include a comparison without Wasserstein2 regularization. Include a sparse GP method in Table 1 for comparison. Minor Issues: VAEs use neural networks to parametrize variational posteriors so DNNSVGP is not fundamentally different. The supplied code lacks comments. Questions: Are model architectures and training procedures for GVIFS and baseline methods comparable How accurate is the approximate Wasserstein2 metric in practice Can the GP influence both predictive mean and uncertainty in the DNNSVGP case"], "yewD_qbYifc": ["Summary The authors address reinforcement learning (RL) in scenarios with numerous acting agents known as \"multiagent RL.\" They propose a mechanism to prioritize the order in which actions are selected for each agent. By doing so they can pick the action with the \"lowest lexicographic value.\" Assessment Despite the papers intriguing premise its quality is difficult to evaluate due to several issues: Unclear Writing: The paper contains numerous errors including unreadable figures missing definitions and spelling mistakes. Low Visual Quality: The figures are lowresolution and challenging to read. Questions Connection to Previous Work: How do the findings relate to \"MultiAgent Reinforcement Learning is a Sequence Modeling Problem\" Importance of Tiebreaking: Why is breaking ties crucial and how does it connect to social conventions mentioned in the introduction Significance of Lexicographic Ordering: Why is maintaining small lexicographic orderings important Suggestions for Improvement Enhanced Writing: The authors should improve the papers grammar and clarity. Improved Visuals: The figures should be enhanced in quality and legibility. Detailed Explanations: The authors should provide more justification for their claims and add references when appropriate. Standard Academic Format: The paper should follow conventional academic writing standards and avoid colloquialisms. Spelling and Grammar Check: The authors should use a spellchecker and proofread the paper carefully.", "Summary This paper aims to address multiagent reinforcement learning (RL) problems involving large discrete action spaces by incorporating a priority convention into the RL process. This convention creates a sequence of improved actions as demonstrated through experiments on various multirobot tasks. Strengths Comprehensive literature review Weaknesses 1. Writing and Terminology: The papers writing is poor using vague and meaningless terms. Terms like \"microscopically sequencable\" and \"consistent argmax\" lack clear scientific definitions confusing the reader. 2. Priority Convention: The actual priority convention is not explicitly described or defined. It is unclear how the convention is decided set and affects the training process. In identical robot setups the significance of priority switching is not explained. 3. Need for RL: The paper does not discuss the role of RL when priority conventions are already in place. In many cases establishing priority conventions can obviate the need for RL by defining heuristics. 4. Experimentation: The experiments unfairly bias the proposed algorithm by not providing the baseline DQN with priority information. The experiments are not robust enough to demonstrate the superiority of the proposed method over standard nonheuristic multiagent RL algorithms. Questions and Recommendation This paper does not meet the standards for publication in NeuRIPS due to its weaknesses in writing methodology and experimentation. The authors need to reconsider their approach define their terms precisely and conduct more rigorous experiments.", "Summary Paraphrase: The paper proposes a technique to handle the difficulty of handling large discrete action spaces in MultiAgent Markov Decision Processes (MMDPs) where the number of potential actions grows exponentially with the number of agents. To do this it integrates agreedupon priority rules (agent priorities and action priorities) into Reinforcement Learning (PCRL). The method leverages Recurrent Neural Networks (RNNs) to generate Qvalues for the system one dimension of the action vector at a time while upholding priority conventions. Additionally an equality auxiliary loss is proposed to ensure that the V(s) are equal for each RNN output corresponding to each action dimension. The paper argues that the priority conventions do not compromise the identification of the optimal actions. The proposed approach is evaluated in coverage planning and Pong domains. Strengths and Weaknesses: Strengths: The proposed approach appears sound and demonstrates performance comparable to DQN with significantly fewer parameters. Weaknesses: The paper can be difficult to understand in some places and contains grammatical errors. The paper lacks key citations from recent MultiAgent Reinforcement Learning (MARL) literature particularly on improving exploration in large action spaces. The idea of using RNNs to predict one dimension of the action vector at a time resembles previous works. The proposed approachs applicability seems limited to scenarios with full observability and homogeneous agents which is not typical in realworld multiagent problems. The evaluation only compares against jointaction DQN neglecting recent advancements in MARL with rolebased learning and CTDE approaches. The paper lacks evaluation on largescale multiagent RL domains like StarCraft II. Questions: Can the authors address the weaknesses identified in the Strengths and Weaknesses section How would changing priority conventions (e.g. action priorities) affect the convergence and final solution How can priority conventions be defined for largescale domains like StarCraft II Since priority conventions are specified before learning is it possible that the PCRL network fails to learn them effectively"], "kS5KG3mpSY": ["Paraphrased Statement: This paper suggests incorporating energybased models (EBMs) into latent spaces prior to using a latent variable model. Instead of using MCMC sampling the EBM is learned via noisecontrastive estimation (NCE). The discrepancy between the prior and latent posterior is approximated via truncated Langevin dynamics rather than a recognition network. This process is repeated to achieve multiple \"stages\" of density ratio estimation. The paper demonstrates its application in image generation reconstruction and anomaly detection. Strengths and Weaknesses: The experiment in Section 5.4 on Langevin dynamics analysis is noteworthy showcasing the advantages of a latent variable model with a prior learned by an EBM. This leads to stable Markov chains with good mixing properties. Question: Based on Figure 4 it appears that using additional stages yields diminishing marginal returns. Is this interpretation correct and how do you suggest determining the optimal number of stages", "Summary The authors present a novel method for finding hidden patterns in data. It combines an energybased prior model with a deep decoder. To estimate the prior model they suggest using an extended version of the Telescoping densityRatio Estimation approach. They test the effectiveness of their method on multiple imagerelated tasks including generating realistic images and detecting outliers. Strengths The new approach is thoroughly described and justified appearing technically sound. It uses the telescoping idea from TRE but applies it innovatively to estimate generative models. The approach consistently outperforms existing methods on the tasks tested. The authors provide indepth analyses of the methods components and parameters. Weaknesses The paper lacks a discussion of the potential drawbacks of the telescoping ratiobased prior model such as increased complexity during learning and inference. The decoder model used in the study is relatively simple compared to current stateoftheart generative models for tasks like generating CelebAHD images. The computational cost of learning the model compared to simpler alternatives is not discussed.", "Paraphrased Statement: Summary: This paper introduces a new method for training energybased models (EBMs) in latent space using multistage density ratio estimation. The goal is to address the \"density chasm\" problem where models struggle to estimate the posterior distribution accurately and achieve high classification accuracy. Approach: The multistage density ratio estimation approach is inspired by the telescope density ratio estimation (TRE) approach. It trains the EBM through multiple stages where each intermediate density ratio is treated as an expert model. This allows for sharper distribution estimates compared to a single model. Training: Instead of using maximum likelihood estimation (MLE) the method employs multistage negative crossentropy (NCE) for training the prior models and shortrun Langevin dynamics (LD) for inferring the posterior. Results: Empirical evaluations on SVHN CelebA and CIFAR10 datasets demonstrate the effectiveness of the proposed method. Ablation studies confirm the importance of the number of stages and the use of adaptive training. Strengths: Novel idea of multistage density estimation for EBM training Comprehensive ablation studies Experimental validation on realworld datasets Opensource code availability Weaknesses: Incremental research based on TRE Sequential training approach which may be less efficient than simultaneous training Lack of theoretical analysis and proofs for efficiency and effectiveness Questions: Efficiency comparison with MCMCbased methods Justification for selecting the number of stages based on FID scores Potential for improved efficiency with a large number of stages compared to standard EBM models using shortrun LD for inference"], "yb3HOXO3lX2": ["Paraphrased Statement: Summary This research examines how reward functions in Markov decision processes (MDPs) can be manipulated. The authors define an \"ungameable\" reward function as one that leads to nearly identical preferences among policies. They also explore \"simplifications\" a subset of ungameable reward functions. The theoretical focus is on the existence of ungameable rewards for stochastic and finite policy classes such as deterministic policies. Strengths and Weaknesses The research topic is significant and intriguing. The theoretical findings are interesting but their scope is limited by the definition of gameability used. The authors acknowledge the need to consider policies beyond optimal ones. However scrutinizing every conceivable policy (within a policy class) inevitably produces feeble conclusions. The definition of ungameable reward functions necessitates that the preference structure over policies remains virtually unchanged. While such a strict definition of gameability may serve as a starting point its insights and deductions could be misleading. The authors suggest that learned rewards are likely gameable and cannot be safely maximized but this conclusion is only valid for their narrow definition of gameability. A more practical approach would involve examining specific optimization frameworks like policy gradient. Questions Definition of Ungameable Reward Functions: The authors propose an ungameability definition that equates to inducing the same preference structure over all policies. It would be beneficial to clarify that this is equivalent to: `R1 and R2 are ungameable if `\u2200`\u03c0` `J1(\u03c0)` `J1(\u03c0)` \u21d2 `J2(\u03c0)` \u2264 `J2(\u03c0)`. This definition is highly restrictive especially when considering general policy classes. It also suggests a minor distinction between ungameable reward functions and simplifications with the only difference being an additional requirement. Definition of Reward Functions: The authors initially define a reward function as a function from `S` \u00d7 `A` \u00d7 `S` to a probability distribution over real numbers. However they later treat the expected reward as a scalar. It would be helpful to confirm that they are indeed focusing on reward functions from `R` `S`\u00d7 `A` to `R`. Practical Impact: The paper explores the theoretical existence of ungameable reward functions and simplifications. It would be valuable to know if this work holds any potential practical implications. The procedure in Theorem 3 can test for the existence of nontrivial simplifications but it is unclear why this would be particularly useful. Furthermore it is unknown if the procedure can generate a simplification or solely determine its existence.", "Paraphrase: This paper formally describes a problem called \"reward gaming\" which occurs when optimizing a substitute reward function leads to negative outcomes when evaluated against the true reward. The authors define \"gameability\" as the relationship between the true reward and the substitute reward when a policy can be improved according to the substitute reward but degraded according to the true reward. They also define \"simplification\" as a special case where the substitute reward is less complex than the true reward. Theoretically the authors show that its impossible to have nontrivial \"ungameability\" when the policy set is not restricted. When the policy set is finite nontrivial ungameable reward pairs always exist and simplifications exist under certain conditions. For infinite policy sets ungameable pairs sometimes exist. Strengths and Weaknesses: This is the first paper to formally characterize reward gaming for general reward functions. Reward gaming is common in AI but has been poorly understood. This paper provides important insights into when its safe to optimize a substitute reward. The main weakness is that the definition of gameability is strict and ungameable pairs have only been proven to exist for finite policy sets. Relaxing this definition could be a fruitful area of future research. Questions: The paper suggests that Markov rewards may not be suitable for defining specific tasks. A related study (\"On the Expressivity of Markov Reward\") explores this topic further and could provide additional insights. Inverse reward design and decoupled approval are alternative approaches to reward optimization. Are they also viable options for avoiding reward gaming", "Paraphrased Statement Summary This paper introduces the concept of \"reward gaming\" in reinforcement learning. Reward gaming occurs when optimizing with respect to a substitute target results in inferior performance on the actual target. The paper defines a more general concept called \"gameableungameable\" where two reward functions are gameable if and only if there exist two policies that exhibit different value preferences under those functions. The paper demonstrates the following: For nonstationary policies probabilistic stationary policies and policy sets containing an open set only equivalent ungameability exists. Finite policy sets are always nontrivially gameable. An example of an infinite policy set (without an open set) shows that ungameability is not always conclusive. Strengths Introduces and formally studies the concept of reward gaming. Provides clear definitions of key concepts. Theorem 1 and Theorem 2 are essential to the papers arguments. Weaknesses The asymmetry property in proxy optimization is not fully utilized. Definition 2 and the rotating contour lines proof for Theorem 2 could benefit from more intuitive explanations. Question The definition for \\(\\mathcalF (\\Pi)\\) in Lemma 1 and Theorem 3 is missing from the main text.", "Summary This research introduces and investigates gameability in reinforcement learning (RL) where improving performance under an approximation of the true reward (proxy) leads to reduced performance under the true reward. The study offers the first theoretical framework for gameability and demonstrates that nongameable reward functions that are not trivial can only exist for finite policy sets. Strengths First formal definition of gameability Innovative and original theoretical framework Clear illustrative example (Figure 1) Focus on optimization without assuming agents discover optimal proxy policies Weaknesses Lack of practical applications or tools for addressing gameability problems Limited discussion on equivalent rewards and related studies Unclear and opaque writing and results in certain sections Questions Policy modifications that maintain proxy value but lower true reward: Are they problematic Nonmonotonic policy search in deep RL: Can true reward still decrease over time Condition on lines 8084: Is it sufficient for the second unbalanced example Definition of kleeny star on line 138: Is it appropriate given its association with optimality in MDPs Figure 3: Purpose and connection to the theory Figure 5: Interpretation of different colors axes and component C Concerns about narrow tasks and potential gaming under simplified reward functions Defining gameability without reward optimization (line 390)"], "xpdaDM_B4D": ["Paraphrased Statement This paper introduces Fewshot Learning with Hard Mixup (FeLMi) to address data scarcity. FeLMi involves six steps: 1. Pretrain the model on the main dataset using crossentropy loss and a supporting selfsupervised loss. 2. Train a linear model based on the trained feature extractor to assign pseudolabels to the original dataset. 3. Filter pseudolabels based on uncertainty. 4. Generate additional data through mixup between novel and original data. 5. Select challenging mixup samples based on prediction margins. 6. Finetune the model using filtered original data novel data and challenging mixup data. While FeLMi achieves strong results it relies on an enhanced pretraining method and only one of its steps (selective hard mixup) is truly original. The study also suggests that the standard mixup step (step 4) has minimal impact. Questions Is the standard mixup step (step 4) necessary for the effectiveness of FeLMi", "Summary Paraphrase: This paper introduces FeLMi a mixup approach for fewshot learning that incorporates a marginbased uncertainty criterion. To address overfitting it augments data with mixup samples. The method includes six stages and utilizes selfsupervised representation learning for pretraining. The evaluation covers three fewshot learning benchmarks. Strengths and Weaknesses Paraphrase: Strengths: Novel data augmentation method inspired by recent advancements. Wellwritten and clear presentation. Weaknesses: Complex and multistage model with varying criteria. Reliance on multiple techniques (selfsupervised learning active learning). Limited evaluation on small datasets. Moderate accuracy gains on CIFARFS dataset. Questions Paraphrase: Why not use episodic pretraining instead of transfer learning How does standard pretraining without selfsupervised learning compare What is the time complexity of the method Clarify the selection of \"ksmallest margin values as hard examples.\" Provide finetuning implementation details (e.g. MLP on ResNet12) and explain the use of a \"temperature coefficient\" of 4.0.", "Paraphrased Statement: This paper introduces a method that enhances fewshot sample data during finetuning to improve accuracy. It uses a manifold mixup process and selects hard samples from the mixup based on a margin for finetuning. The approach is applied in different settings and shows promising results. Questions: 1. The paper mentions using selfsupervised loss during pretraining in addition to crossentropy but equation 1 only includes crossentropy. Please clarify the complete loss function. 2. How is the entropy threshold for filtering pseudolabels for base classes determined 3. How does this method compare to the \"Charting the Right Manifold Manifold Mixup for Fewshot Learning\" paper presented by Mangla et al. at WACV 2020 4. Why are standard benchmarks like tieredImagenet and CUB as well as different architectures like WRN missing from the evaluation Are the compared methods evaluated using the same pretrained model Over how many episodes are the average accuracies reported 5. What are the domain shift results for settings like mini to CUB as explored in previous studies such as \"A closer look at fewshot learning\" by Chen et al. in 2020", "Rephrased Statement: This research investigates two techniques for enhancing fewshot image recognition: 1. Combining pseudolabeling with entropybased label filtering for representation learning. 2. Novel \"novel and base\" manifold mixup with entropybased filtering for adapting existing representations to new categories. When optimizing hyperparameter settings the proposed approach demonstrates competitive performance on standard fewshot image recognition benchmarks. Ablation studies analyze the contributions of each technique. Strengths: Clear and concise writing Detailed experimental setups and implementation details Wellorganized experimental results Thorough experiments and ablation studies using standard benchmarks Highly competitive results (nontransductive) Weaknesses: Limited explanation of how the approach functions Lack of qualitative examples of entropybased filtering Recommendations: Include qualitative examples of entropybased filtering in the main text. Display examples of hard cases selected by mixup. Identify classes that benefit most from pseudolabeling mixup and entropy filtering. Sensitivity analysis of hyperparameters would be beneficial. Using different hyperparameters for 1shot and 5shot setups may deviate from traditional approaches. Measures should be taken to prevent overfitting to specific benchmarks. Authors Response: The authors response adequately addresses these concerns. Questions: See weaknesses section for followup questions on the approachs mechanism and sensitivity to hyperparameters."], "lWq3KDEIXIE": ["Paraphrased Statement: Summary: This paper suggests incorporating local estimates (estimates of model parameters within specific subsets of the data) into the training of probabilistic models. Due to the difficulty in obtaining exact local estimates the paper considers using approximate probabilistic models to enforce these estimates in the loss function. The focus of the paper is on learning cutset networks (a type of probabilistic model) with potentially inconsistent local estimates. This is formulated as a constrained optimization problem with the local estimates as constraints. The paper uses a Lagrangian relaxation approach resulting in a weighted joint optimization problem. Finally the paper presents a technique called momentmatching for optimizing cutset networks. This technique exploits the properties of cutset networks to reduce the variance of the loss function leading to faster convergence. The paper evaluates its approach on the 20Datasets benchmark demonstrating that models trained with local estimates outperform models trained without them as measured by KL divergence against an oracle model (a mixture of cutset networks). Strengths and Weaknesses: Strengths: The optimization formulation appears to be sound. The writing is generally clear. Weaknesses: Impractical Assumptions: The assumption of having large access to local estimates seems impractical and is not welljustified. The paper lacks experiments with real local estimates from real datasets. Limited Experimental Evaluation: The experiments only compare two methods (Q vs. R) with R having a clear advantage due to additional information (local statistics). The paper lacks ablations and comparisons with other methods and probabilistic models. Focus on Cutset Networks: The papers emphasis on cutset networks is not welljustified. It should either focus solely on cutset networks or include studies on other types of probabilistic models. Questions: How can we obtain real local estimates in practical scenarios What are the advantages and disadvantages of probabilistic circuits compared to cutset networks", "Paraphrased Statement: This study introduces an algorithm to create a simplified probabilistic model (TPM) from data containing both complete and incomplete information. The resulting TPM is a cutset network (or a mixture thereof) a model that allows for efficient probabilistic computations. Using a gradientbased approach the algorithm is designed to learn the parameters of this model while incorporating incomplete information as a guidance constraint. This approach surmounts the computational hurdles faced by traditional probabilistic models like Bayesian networks and Markov networks which require complex inference to verify the consistency of incomplete data. The authors emphasize the novelty of utilizing TPMs for this learning task and believe that the practical benefits of TPMs will be widely recognized in various applications where learning models from incomplete data is crucial. Strengths and Weaknesses: Strengths: The problem addressed is practical and relevant especially considering the use of TPMs to overcome limitations of traditional models. Experiments demonstrate the effectiveness of the learning approach. The application of TPMs may increase the frequency of this learning scenario in practice given the computational challenges faced by traditional models. Weaknesses: No major surprises or novel developments were identified for those familiar with the topic.", "Paraphrase: Summary: This paper explores a training scenario where training data is combined with noisy local estimations. It focuses on Cutset Networks a class of probabilistic models known for their tractability. This tractability allows for efficient gradient calculations in the learning process. The paper introduces an iterative gradient ascent approach. To enhance convergence it proposes a momentmatching variant. Instead of optimizing the original goal involving both local estimations and training data the paper considers local estimations as a postprocessing step after training the network using standard techniques. Strengths and Weaknesses: Strengths: Clear and accessible writing Detailed description of the contribution Convincing empirical evaluation Weaknesses: Insufficient evidence to support the claim that access to local estimations is common in realworld applications Questions: 1. Can the authors provide stronger evidence that local estimations are widely available in practical applications 2. Why not compare the proposed momentmatching approach with a method that optimizes the objective in Equation 5 using stochastic gradient ascent Minor Comment: Consider including the standard deviation in the results or providing the full table in supplementary material despite its small size.", "Paraphrased Statement: The paper explores how to create probabilistic models that are easy to work with (TPMs). It introduces a technique based on gradients to develop a TPM from local estimates that may not always agree. The method involves initially learning a TPM from a given dataset. Then it refines the model by adjusting its parameters using local probability estimates defined for specific variable combinations (local marginal distributions). Parameter updates are guided by a gradient descent approach with an objective function that aims to minimize the differences between the learned model and the local estimates. Evaluations show that the proposed method generates usable models superior to those learned solely from data. Strengths and Weaknesses: Strengths: The paper is wellwritten and organized making it easy to understand. Most concepts are clearly introduced and discussed. The empirical evaluation effectively demonstrates the methods effectiveness. Weaknesses: Assumption of Marginal Estimates: The assumption that marginal probability estimates are readily available may not always be realistic. Empirical Evaluation: The experimental setup in Section 5 particularly the generation of local estimates appears artificial. Comparison with Previous Methods: It would be informative to compare the proposed method with existing approaches such as IPFP especially when local estimates are consistent. Sensitivity to Learning Rate: The methods sensitivity to the learning rate should be investigated."], "uzn0WLCfuC_": ["Paraphrased Statement: Original Statement: \"Summary: Expected Improvement (EI) is commonly used in Bayesian optimization (BO) to select the best action. This paper extends EI to the contextual bandit setting including both linear and nonlinear cases. Despite the lack of thorough analysis of EI this paper provides regret analysis for EI in linear and neural contextual bandits. Additionally the authors demonstrate the effectiveness of their proposed algorithm through empirical results.\" Paraphrased Statement: \"EI is a popular method for selecting actions in BO. This paper expands EI to the contextual bandit setting considering both linear and nonlinear scenarios. While EI has been widely used its analysis has been limited. This paper provides a regret analysis of EI for linear and neural contextual bandits. Furthermore the authors present successful empirical results for their proposed algorithms.\" Strengths and Weaknesses: Weakness: \"(1) The two proposed algorithms LinEI and NeuralEI appear similar to Thompson Sampling (TS). For example in LinEI the expected reward is computed for each arm before making a selection. This is akin to taking the expectation over the normal distribution in TS. Therefore the novelty and significance of the proposed algorithms and their analysis are questionable.\" Questions: \"(1) Please clarify how LinEI and NeuralEI differ substantially from the expected version of TS which involves adding expectation over sampled rewards. (2) Explain the specific challenges encountered in analyzing LinEI and NeuralEI that distinguish them from LinTS and NeuralTS.\"", "Paraphrased Summary: This study employs the expected improvement (EI) strategy for contextual bandits which differs from the commonly used upperconfidence bound (UCB) and Thompson sampling (TS) approaches. EI can be used with both linear and neural network predictors when contexts are adversarially chosen but rewards are realizable (conditional). Strengths: Novel application of EI in contextual bandit algorithms. Connection to neural tangent kernel (NTK) theory. Weaknesses: Lack of clear motivation for using EI (beyond being a new approach). Theoretical equivalence to UCB and TS in terms of regret bounds with no empirical evidence of superiority. Computational complexity compared to UCB and TS is not addressed. Questions: Is the mean of the subgaussian distribution explicitly zero (as it is sometimes defined) Explain the differences between LinEI and LinUCBLinTS in terms of the optimistic arm selection process. Resolve the apparent contradiction between assuming conditionally subgaussian residuals and stating that the regret bounds hold regardless of the actual reward distribution.", "Paraphrase: The authors present a new contextual bandit algorithm that uses expected improvement (EI) and analyze its regret. This algorithm modifies EI for multiarmed bandits (MABs) by combining it with pure exploitation. The paper contributes to the literature by exploring EI which has been underutilized in contextual bandits for balancing exploration and exploitation. They introduce two novel EIbased algorithms one for linear payoffs and one for deep neural networks. Strengths: EI provides new insights for handling exploration and exploitation in contextual bandits. The proposed algorithm improves on the regret upper bound of Thompson Sampling (TS) by a factor of \\sqrtd. Weaknesses: The justification for modifying EI with pure exploitation is unclear. The benefits of the proposed algorithm over LinUCB are not fully explored. Questions: What are the reasons for modifying EI with pure exploitation If the EI value is small wouldnt the algorithm already choose the greedy action What is the regret upper bound without the modification Are there experiments comparing the proposed algorithm (LinEI) to TS and LinUCB in terms of regret", "Summary: This research extends the expected improvement heuristic used in Bayesian optimization to the contextual bandit setting creating a novel type of algorithm. The authors propose two new algorithms with a method for setting an explorationexploitation threshold that achieves a regret rate of order \u221aT even in settings with adaptive adversaries. Empirical evaluations demonstrate the strong performance of these methods. Strengths: Establishes a connection between expected improvement heuristics best arm identification and Bayesian optimization. Provides theoretical proofs and competitive regret rates for linear and neural contextual bandits. Demonstrates the practical value of the proposed methods through experiments. Presents a comprehensive explanation and relates expected improvement to other relevant literature. Weaknesses: The authors could provide more details on how the algorithms manage to function effectively against adaptive adversaries. Questions: Are \u03a6 and \u03c6 intended to represent the cumulative and probability density functions of the normal distribution as introduced on line 143"], "nE8_DvxAqAB": ["Paraphrased Statement: Summary: This paper introduces: Ego4D: A benchmark dataset for egocentric video analysis. EgoClip: A curated version of Ego4D used for pretraining. Frozen: A video learning model with a new loss function (RgoNCE). EgoMCQ: A validation dataset based on Ego4D. Ego4D provides pretraining data and the model demonstrates effectiveness on downstream tasks. Strengths: Beneficial curation of Ego4D. Clear explanations and supportive materials. Promising experimental results. Weaknesses: Its unclear if the contributions come from Ego4D or the proposed methods. The results may be influenced by Ego4Ds quality not solely the methods. Suggesting \"EgoClip\" as a standalone entity may be misleading. The work may be more suitable for the benchmark track given the primary focus on data. More video recognition evaluations could enhance the evaluation. Question: How were videos with multiple shared views manually removed from EgoClip", "Paraphrased Statement: This paper presents a technique for videobased language learning in egocentric perspectives (from the first person point of view). The key contributions include: A new method for generating positive and negative sample pairs for training using overlapping nouns and verbs in video captions. A new evaluation framework for the Ego4D dataset along with evaluations on Epic Kitchens and Charades Ego datasets. Strengths and Weaknesses: The proposed sample mining method is innovative and effective. The results show improvements on various datasets and tasks. The stateoftheart comparisons highlight the impact of different pretraining datasets and settings. The results are comprehensive demonstrating the benefits of the proposed components. Minor Comments: The claim that Ego4D is \"massivescale\" is questionable as it is significantly smaller than other datasets. The results show substantial improvements over previous work implying that Ego4D may not have fully \"unlocked\" possibilities in this domain. There are typos and grammatical errors throughout the paper suggesting the need for careful editing. Some notation (e.g. the subscript i in Eq. 1) adds unnecessary complexity and could be simplified without compromising clarity. Question: The EGOClip method should be applicable to thirdperson videos because it does not rely on features specific to egocentric perspectives (such as egomotion).", "Paraphrased Summary: This paper focuses on improving video language pretraining for videos captured from a firstperson perspective (known as egocentric videos). The authors propose: 1. A cleaned version of the Ego4D dataset to create a large training corpus for video and text together. 2. EgoNCE a novel pretraining approach that addresses two challenges in egocentric videotext pretraining: distinguishing similar actions in different scenes and different actions in similar scenes. 3. EgoMCQ a new development set based on the Ego4D dataset. Strengths and Weaknesses: Strengths: The topic of egocentric video pretraining is important and has the potential to benefit the research community. The experimental results generally support the authors claims. Weaknesses: The model architecture lacks significant novelty. The actionaware positive sampling technique is specific to the Ego4D dataset and may not generalize to generalpurpose egocentric videos. The sceneaware negative sampling approach is not particularly novel. EgoClip performs better than thirdperson view data alone but it would be interesting to explore the benefits of combining firstperson and thirdperson datasets. The challenges addressed in the paper are not unique to egocentric videos. The contribution of cleaning the Ego4D dataset is relatively minor. The need for a new development set (EgoMCQ) is unclear given the existence of existing development sets for egocentric video tasks. Questions: It would be helpful to include a datasheet for the EgoMCQ dataset.", "Paraphrased Statement: The paper presents EgoClip a dataset containing 3.8 million videotext pairs extracted from Ego4D a largescale dataset of firstperson videos. To enhance the adaptation of videotext contrastive learning to the egocentric domain the paper introduces EgoNCE a pretraining objective that leverages egocentricaware positive and negative sample mining. Furthermore the paper develops EgoMCQ a new benchmark designed to closely resemble EgoClip. This benchmark enables effective validation and exploration of algorithms for EgoClip. EgoClip is derived from Ego4D by applying specific criteria to select videotext pairs with minimal misalignment and irrelevance. The resulting dataset consists of 2.9K hours of video with 3.85 million narrations covering 129 different scenarios. The paper proposes a variablelength clip pairing strategy that identifies temporal segments centered around narration timestamps. EgoNCE an extension of InfoNCE is introduced for videolanguage pretraining. EgoNCE incorporates actionaware positive sampling and sceneaware negative sampling to adapt InfoNCE to egocentric videos. EgoMCQ a multiplechoice question benchmark is designed to overcome the limitations of standard videotext retrieval due to the presence of semantically similar captions in Ego4D. Experiments on five egocentric benchmarks demonstrate the superiority of EgoClip and EgoNCE in enhancing pretraining for egocentric downstream tasks. Ablation studies confirm the effectiveness of EgoNCE and EgoMCQ."], "q9XPBhFgL6z": ["Summary This work defines \"harm\" qualitatively using causal models in relation to autonomous AI systems. The authors reason that harm is often discussed qualitatively making a qualitative approach necessary. They review causal models introduce their definition of harm and apply it to examples. Strengths Extends previous work on harm by defining it causally. Welldescribed and understandable definition. Extensive examples provide insights into the definition. Weaknesses Assumes no noise terms in the systems structural equations which should be explicitly stated. The relationship between exogenous and endogenous variables seems more restrictive than the standard definition in causal inference assuming endogenous variables are fully determined by exogenous factors. Minor Comments Typos in lines 104 and 166. Questions How does the definition differ from the probabilistic structural causal models approach How are exogenous variables treated in relation to endogenous variables in the definition and what are the implications", "Summary: This paper suggests a causal definition of harm criticizing the lack of causality in existing philosophical definitions. It presents examples showing how this new definition addresses various situations and enhances reasoning in different contexts. Strengths: 1. Tackles the important issue of defining harm aiming to provide a consistent definition. 2. Clear and wellwritten accessible even to those unfamiliar with causality. 3. The proposed definition of harm in section 3 appears wellconceived. Weaknesses: 1. Relies heavily on the assumption of default utility. 2. The Batman and Robin example may not accurately conclude that Robin was harmed as it ignores potential confounders. 3. The default utility in Example 4 is incomplete as it does not account for the importance of the tip amount. Questions: 1. How would the definition of harm be affected by dependent exogenous variables in the causal graph 2. How are confounders managed in the causal definition of harm", "Paraphrase: Summary: This study introduces a qualitative framework for defining harm allowing for conceptual evaluations in reallife situations to minimize harm. Strengths: Novel framework for harm assessment. Practical applications to reallife problems. Weaknesses: Lack of quantitative definition for utility function may hinder realworld implementation. Questions: The framework only considers harm to a single agent while many reallife situations involve multiple agents. The absence of a quantitative utility function makes it difficult to minimize overall harm or apply the framework in complex scenarios (e.g. trolley problem). It is unclear how to address these limitations or whether they are inherent to the framework.", "Paraphrase: This is an urgent review that the reviewer did not choose based on their interests or expertise. The reviewer also did not have enough time to provide a thorough assessment. Therefore caution is advised when reading this review and the confidence level has been adjusted accordingly. The paper proposes a formal definition of \"harm\" using structural causal models (SCMs) inspired by Judea Pearls counterfactual theory of causality. The authors extend SCMs with a \"default utility\" (referred to as causal utility model) and define three conditions (H1 to H3) that determine whether an event causes harm to an agent. They provide a rationale for their definition by comparing it to others. Finally they discuss several philosophical examples to demonstrate the advantages of their proposal. Strengths: The topic is timely and relevant especially in the context of artificial general intelligence alignment. Using causal models provides an interpretable modeling framework. The formalization effectively captures complex philosophical concepts like harm. Weaknesses: It is questionable whether NeurIPS is the most appropriate venue for this work but its importance on a societal scale is undeniable. The presentation is somewhat dry and could benefit from illustrations and visualizations. The mathematical account lacks analysis of induced properties such as feasibility of computations and conditions for satisfying the conditions. Questions to Improve the Paper: How can the definition handle temporal considerations How can the computational intractability of harm computations be overcome Under what circumstances can the causal model ensure the accuracy of harm computations", "Summary The paper presents a precise definition of harm based on the modified HalpernPearl concept of actual causation. Key additions to the definition include a utility function that maps observations to values between 0 and 1 and a \"default utility.\" The authors provide seven scenarios and argue that their definition accurately identifies the presence or absence of harm in each case. Strengths and Weaknesses Originality: The idea of using causation to define harm is not original. The specific definition introduced is novel in its use of Halpern and Pearls causal models. However some concepts (default utility and the idea that preventing worse outcomes does not constitute harm) are not original as they are implied in Halpern and Hitchcocks extended causal model. Quality: The definition resolves an issue with a previous attempt to define harm using causation. The justification for the definition is lacking. No theoretical results or empirical evaluations are provided. Clarity: The paper is wellwritten and easy to follow. Significance: Formalizing harm is important for developing responsible AI systems. Questions: 1. Why not use Halperns extended causal model 2. Is harm possible without causation 3. How does harm relate to blame 4. How can a qualitative definition of harm use utility functions that map to numbers instead of an ordering 5. Are there known counterexamples to the definition of harm"], "owZdBnUiw2": ["Paraphrase: Summary: The proposed approach aims to improve the costeffectiveness and performance of Convolutional Neural Networks (CNNs) in video action recognition by utilizing a twostream approach. The \"ample\" stream processes all frames efficiently using lower resolution and fewer channels. The \"focal\" stream processes a smaller subset of frames at higher resolution. A navigation model guides the focal streams frame selection based on input from the ample stream using a Gumbel softmax function. The study demonstrates the effectiveness of this approach on ActivityNet SomethingSomething and MiniKinetics datasets achieving high accuracy with relatively low computational cost. Strengths and Weaknesses: Originality: The studys primary weakness is its lack of originality as similar concepts have been explored in prior publications. The idea of combining a low spatial resolution high frame rate stream with a high spatial resolution low frame rate stream with lateral connections has been previously introduced in \"SlowFast Networks for Video Recognition\" (2019). A direct comparison between the proposed approach and SlowFast is missing which could provide more insight into their differences and similarities. Clarity: The paper is generally clear but there is some difficulty understanding the implicit temporal modeling in Section 3.2. The explanation of how to sample frames from a Gaussian distribution could be improved as Gaussian distributions are continuous not discrete. Quality: Overall the studys quality is good but the omission of related work (e.g. SlowFast) is a significant drawback. Significance: The studys significance is limited due to its lack of originality. A proper comparison with SlowFast would be necessary to assess the performance differences between the two approaches. Questions: Why was TSN (a sixyearold method) used as a baseline in Table 1 despite its age", "Summary This paper presents the Ample and Focal Network (AFNet) for video recognition. AFNet employs two branches: Ample Branch: Processes several neighboring frames with reduced resolution feature maps. Focal Branch: Selectively focuses on fewer frames with higher computational cost. Utilizes both intermediate predictions from the ample branch and raw input frames. The authors claim that AFNet achieves a better balance between accuracy and computational cost compared to existing methods. Experiments on various datasets show that AFNet outperforms several baseline models in terms of accuracy. Strengths: Intuitive design with lightweight processing of dense frames and selective attention to specific frames. Comprehensive structure with both qualitative and theoretical explanations. Comparative results on multiple datasets demonstrating improved accuracycomputation tradeoff. Weaknesses: Lack of information on the efficiency of sparse convolutions used for frame selection. Relatively modest accuracycomputation gains compared to baselines. No public code release for result reproducibility. Omission of relevant citations in the related work section. Questions: How is frame selection performed using Ln in Eq. 6 Does it involve thresholding Why is the v notation used in Eq. 1 potentially causing confusion What decay schedule was used for tau during training Is the significant improvement over TSN attributed to potential overfitting issues", "Paraphrased Statement: This research presents an efficient framework called Ample and Focal Network (AFNet) for recognizing videos. AFNet has two branches: 1. Ample Branch: Preserves all input features using lightweight calculations. 2. Focal Branch: Selects specific frames to extract features reducing computational costs. Combining these branches AFNet focuses on crucial information while minimizing computation. Experiments on multiple datasets show that AFNet performs better than existing methods. However concerns remain: 1. Technical Contribution: AFNets twostream design (high spatiallow temporal and low spatialhigh temporal) resembles existing methods like SlowFast. 2. Frame Selection: The proposed navigation module selects informative frames but similarities exist with the Scsampler method. 3. Experiment Settings: Using 12 frames (instead of 8) for experiments does not adequately demonstrate the efficiency of AFNet. Questions: 1. What does the model learn to select in different navigation module stages 2. Explain the generation of logit pnt for frame t using Eq. (3) and the identical values for pnpntt1T. 3. Discuss the use of RT in Eq. (16) and its potential limitations on frame selection. 4. Clarify how AFNet achieves higher accuracy with fewer frames. 5. Provide reasons for not comparing AFNet to efficient frameworks like MoViNet. 6. Include missing action recognition methods like TwoStream Network and TC3D in the comparisons.", "Paraphrase: Summary: This paper focuses on enhancing action recognition by leveraging a maximum number of frames from videos while minimizing computational costs. The authors introduce a novel architecture with two branches: one processing lowresolution videos and the other sampling informative frames for detailed analysis. Strengths: Innovative idea of endtoend frame selection Comprehensive experimental validation of the proposed methods effectiveness Weaknesses: Lack of clarity in the description Absence of a pilot study to demonstrate the information loss claimed by the authors Similarity to SlowFast networks a renowned action recognition model without proper acknowledgment Questions: The assumption that information loss leads to poorer performance is questionable given previous findings that even a single frame may suffice for certain actions. Instead demonstrating the benefits of using all frames with a computationally unrestricted model may be more convincing. The fusion strategy used to combine outputs from two branches is overstated as it is a simple weighted average. Using residual connections between layers in the focal branch is debatable as different layers potentially require different frames resulting in temporal misalignment of features. The authors claim that their method lacks a temporal modeling module is unclear as W2 in Eq. 2 seems to imply frame interaction.", "Paraphrase: This paper introduces AFNet an efficient video recognition network with two branches. The Ample Branch handles frequently sampled input frames with reduced channels while the Focal Branch focuses on salient frames identified by a selection module. Experiments show AFNets superior performance and lower computational cost compared to existing methods. Strengths: Simple and effective AFNet architecture with low computational cost. Wellorganized and wellwritten paper. Weaknesses: Lack of novelty in individual AFNet components such as the twobranch design and navigation module. Computational cost during training cannot be reduced due to the use of Gumbel softmax. Need for more experiments with stronger backbones and larger frame counts. Questions: 1. Clarification on the GFLOP calculation for RT0.8 which appears counterintuitive given the architecture. 2. Consideration of an alternative approach where a linear function is learned to transform original frames instead of frame sampling."], "lYZQRpqLesi": ["Paraphrased Statement: Summary: This study presents an alternative training method for creating a supernet called AST which allows for dynamic inference with adjustable sparsity rates. Experiments on large datasets demonstrate the effectiveness and training efficiency of AST compared to traditional supernet approaches. Strengths: The gradient correction mechanism is wellconceived. The manuscript is clear and the AST method is accessible. Weaknesses: The practical value of AST is questionable. Unstructured pruning methods do not offer significant acceleration on hardware and ASTs benefit lies in parameter savings without compromising performance. However AST trains multiple subnets for dynamic inference which does not reduce inference costs like structured methods. The resulting supernets have more parameters than classic sparse training methods. The experiments on CIFAR10 with selected patterns are not convincing for realworld hardware applications. AST lacks novelty as it combines existing pruning and supernet training techniques. The performance of AST is inferior to other sparse training methods such as GraNet. Questions: Does Eq.5 for gradient correction work when the overlap between subnets is minimal It is recommended that the authors perform experiments on structured sparsity to compare AST against established sparse training models like SNet USNet and OFA.", "Paraphrase: Summary: This paper presents an algorithm that trains multiple sparse neural networks with varying sparsity levels simultaneously. The authors propose a gradient correction technique that aligns the update directions of the gradients among the different models. Strengths: Efficient training to derive multiple sparse networks in a single run. Clear illustrations of the proposed methods. Organized structure. Weaknesses: Subnetwork performance lags behind competitors in some cases. Comparisons limited to one other method. Training costs not reported in a metric comparable across methods. Memory costs of maintaining masks for each subnetwork not fully discussed. Computational cost of ASTGC not adequately considered. Missing references in the Appendix. Questions: Provide a formal definition of a subnet. Include a pseudocode to clarify the relationship between the subnetworks. Correct typos. Include subnetwork sparsity levels in Figure 8. Specify the number of experimental runs. Discuss the relationship to other work on deriving multiple sparse networks such as [1].", "Summary Paraphrase: This study combines sparse training and dynamic inference to create multiple subnetworks with varying sparsity levels. The proposed method (AST) outperforms traditional sparse training approaches by generating various sparse neural networks with a single training run. Unlike dynamic inference methods AST avoids multiple forward and backward passes for each subnetwork. Strengths and Weaknesses Paraphrase: Strengths: Combination of sparse training and dynamic inference for improved efficiency Clear and concise paper Transparent experimental setup demonstrating the effectiveness of AST Extensive empirical study providing valuable insights Weaknesses: Lack of na\u00efve baselines such as pruning a dense model with finetuning or a sparse model trained with GraNet Relatively lower accuracy of AST compared to its backbone technique GraNet Insufficient explanation of the relationship between GraNet and AST Numerous typos and inconsistencies requiring refinement Inconsistent terminology and unclear comparisons in Table 3 Low readability of tables suggesting organization by accuracy Questions Raised: Why does AST have lower accuracy than GraNet Is it due to insufficient updates for each subnetwork Can the performance gap between AST and GraNet be closed by increasing the training time for each subnetwork What are the differences between \"inner group\" and \"inter group\" and how do these groups affect the subnetwork performance", "Summary Paraphrase: This research proposes a method for training sparse neural networks. Key features include: Sequential training of multiple sparse subnets similar to Reptile. Gradient correction to address conflicting weight updates between subnets. Observations for sparse subset training: Allowing variations in subnet architecture enhances learning capacity. Intermittent increase in sparsity stabilizes training. Strengths and Weaknesses Paraphrase: Strengths: Novel approach of sequential and recursive sparse subnet training. Improved performance compared to existing sparse network algorithms. Wellwritten paper with comprehensive literature review. Weaknesses: Limited experimental evaluation. Insufficient experimental validation of some proposed techniques. Questions about the algorithms novelty. Questions: Is the overall framework a specialized form of Reptile How do \"negative gradient\" and \"conflicting weight updates\" impact learning and why is gradient correction necessary Are the observations in Section 3.3 sufficiently supported by experiments Is the final sparse network an ensemble of all trained subnets or just the last one"], "m_JSC3r9td7": ["Paraphrased Statement: Summary: This research suggests a method for estimating direct and indirect effects from observational data using deep neural networks. The method is semiparametric in nature. Strengths and Weaknesses: Strength: The papers theoretical and experimental results seem valid though the reviewer has not verified the derivations. Weakness: The paper can be challenging to understand due to extensive mathematical notation. Clearer explanations would enhance its accessibility. Uncertainties: NoveltyOriginality: The reviewer is unsure of the level of innovation in the proposed method as it requires further investigation. Questions: The paper raises no significant questions for the reviewer.", "Paraphrase: Summary: This research introduces DeepMed a technique that uses neural networks to lessen bias in semiparametric mediation analysis. Unlike previous research it eases the assumptions of sparsity providing neural networks with greater adaptability and expressiveness. These advancements have been confirmed through experiments on both simulated and realworld data. Strengths and Weaknesses: The paper is wellwritten. Questions: In remark 10 the researchers speculate that DeepMeds lack of semiparametric efficiency in instances 4 and 5 is attributed to Adams implicit regularization. To test this theory it would be worth repeating these tests with regular SGD as the optimizer and verifying if DeepMed gains efficiency.", "Paraphrased Statement: Researchers have developed DeepMed a causal drug analysis method that emphasizes specific causal effects called Natural Direct and Indirect Nonlinear Effects (NDENIE). DeepMed uses highly effective estimation techniques and outperforms other approaches in both simulated and realworld data. Strengths: Comprehensive theoretical analysis of the statistical properties of NDENIE estimators and DeepMed. Weaknesses: The evaluations of DeepMeds performance rely mainly on synthetic experiments rather than more comprehensive testing.", "Paraphrased Summary DeepMed is a novel method for mediation analysis that employs deep neural networks (DNNs) for semiparametric modeling. It allows for more flexible modeling of nuisance functions which reduces sparsity constraints in DNN training. Strengths and Weaknesses DeepMed leverages the multiplyrobust property of intentiontotreat (ITT) estimators to relax sparsity constraints in DNNbased nuisance function fitting. However the secondorder bias and properties of the estimator within the sample splitting scheme have been previously discussed in other works. The use of DNNs for nuisance function fitting is novel and practical. Synthetic experiments demonstrate the effectiveness of DeepMed. Questions The authors suggest using a(d x m) and a(d x) to address the curse of dimensionality in fitting f(m x d). However this approach may introduce incompatibilities in modeling a(d x m) and a(d x) while maintaining a coherent joint distribution over X D M and Y. The authors should provide remedies for this issue. The crossworld ignorability assumption should be explicitly stated as Y(d m) independent of M(d) given X. The definition of X as 0 1p in Section 2.1 could be more general as the arguments do not rely on a limited state space for X."], "ttQ_3CiZqd3": ["Summary This paper introduces a principled extension of the \"Minimizing Control for Credit Assignment with Strong Feedback\" approach. It applies to implicit models with equilibrium states and adopts the \"least control principle\" (LCP) which assigns credit by minimizing control input to reach an equilibrium that minimizes loss. Training Recasting the problem as a constrained control optimization problem the paper demonstrates how to train the feedforward and feedback weights: Feedforward weights: Minimize the amount of control via a local learning rule. Feedback weights: Specify control as a linear mapping of a lowpassfiltered loss gradient onto neuron weights. Theoretical Guarantees The paper provides conditions under which least control solutions lead to solutions of the original learning problem. Additionally it shows the connection between LCP and predictive coding and introduces energybased control dynamics. Experimental Results The LCP framework is applied to several models and tasks including MNIST CIFAR10 and implicit neural representations. The results are comparable to recurrent backpropagation demonstrating the effectiveness of this approach for credit assignment. Strengths Solid theoretical foundation Experimental validation for various models and tasks Singlephase learning capability Weaknesses Limited comparison with existing work Unclear scope of the theory regarding partial control Concerns about the CIFAR10 experimental setup and the potential use of hidden backpropagation Questions Is there a fundamental difference between LCP and SDFC for feedforward connectivity patterns Does the theory extend to the case of partial control How is the encoder trained in the CIFAR10 experiments and does it involve backpropagation through complex operations", "Paraphrase: Summary: This study creates a theoretical model for optimizing neural networks at equilibrium. It transforms the initial constrained optimization problem into a new optimization problem that resembles a \"leastcontrol\" problem. The leastcontrol problem is solved using a twostep iterative process: firstly guiding the networks behavior with a control signal to minimize fixed points that satisfy the initial constraints and secondly taking a gradient step to reduce the final cost of the control. Motivations: When the control is carried out under certain feasible conditions the proposed method is proven to produce solutions that exactly match the initial optimization problem while leaving flexibility for explicitly defining the control. The model uniquely combines credit assignment and activation dynamics into a single dynamic resulting in a localized weight update rule that operates both spatially and temporally. This formulation can also connect to models of free energy minimization providing a fresh perspective on the learning objective of such models as a relaxation of the proposed leastcontrol objective. Experiments: The study conducts various experiments to demonstrate the learning abilities of these systems in supervised image classification implicit neural representation and metalearning. The approach exhibits competitive performance against recurrent backpropagation (RBP) a stateoftheart technique for training equilibrium models. It can also incorporate metalearning with comparable success to gradientbased methods. Strengths and Weaknesses: Strengths: Clear and logical development wellcrafted technical problem formulation diverse experiments demonstrating applicability. Weaknesses: Some technical details are relegated to supplementary material while methodological choices for solving the leastcontrol problems experimentally are not fully explained in the main text. Questions: 1. Elaborate on the statement that the weight update becomes a local Hebbian rule. 2. Clarify how the weight update equation (4) can be interpreted as a local Hebbian rule. 3. Extend the assumption of convexity in Proposition 3 to encompass locally convex functions. 4. Provide more information on the computational cost of training this system including a comparison with RBP regarding optimization efficiency.", "Paraphrase: Summary: This research presents \"Least Control Principle\" a novel approach to training neural networks using local learning rules. It has a close connection to predictive coding and energybased models. The authors demonstrate its versatility by applying it to recurrent neural networks (RNNs) and metalearning tasks. Post Rebuttal Update: The authors have addressed many of the reviewers concerns leading to a score adjustment. Strengths: Interesting and wellwritten work providing valuable insights into a developing field. Demonstrations showcase the methods flexibility. Intriguing relationship to energybased models and alternate parameter dependence. Minor Weaknesses: Lack of evaluation on a wellknown benchmark. The RNN and metalearning experiments are valuable but less established making comparisons difficult. The authors should consider evaluating the method on a classic vision task like ImageNet. Insufficient discussion of practical limitations such as equilibration time (as in predictive coding) and compute requirements. The computational details of the experiments are missing. Consequences of nonzero alpha are not fully explored. The theorems and propositions may not hold entirely when their conditions are not satisfied. Empirical evidence to show that they still approximate those conditions would be beneficial. Exposition: Line 114: Clarify that target values apply only to supervised objectives. Section 2.2: Revise the paragraphs to provide a clearer explanation of Theorem 2 before discussing its generality and examples. Equation 4: Describe Hebbian learning before contrasting it with backpropagation. Explicitly show parameter updates for specific examples in Section 3 to illustrate their Hebbian nature. Questions: The reviewers score will be increased if the authors address the identified weaknesses.", "Paraphrased Statement: This study introduces the \"leastcontrol principle\" as a substitute for backpropagation in recurrent neural networks. This principle aims to minimize the control required to maintain stability. The researchers outline conditions necessary for this principle to hold and relate it to traditional optimization and energyminimization constraints. Validation experiments demonstrate comparable performance to backpropagation in supervised learning (MNIST CIFAR10 natural images) and metalearning (Omniglot). Strengths: Provides a strong theoretical basis with mathematical proofs. Links the proposed method to the original learning problem and other optimization interpretations. Demonstrates applicability in supervised and metalearning settings. Weaknesses: Does not surpass backpropagation in performance. Fails to highlight additional advantages of the framework such as scalability or biological plausibility. Title could be more specific and informative. Questions: Explain the intuitive meaning of minimizing \"optimal control at equilibrium.\" Outline the advantages and disadvantages of the proposed framework compared to backpropagation. Explore experiments to empirically demonstrate the advantages. Clarify the comparison with iMAML and justify the exclusion of MAML."], "vDeh2yxTvuh": ["Paraphrased Statement Summary This paper examines the properties of two optimization algorithms that minimize nonconvex functions: Stochastic Weight Averaging (SWA) and SharpnessAware Minimization (SAM). Methods The study compares the minima reached by SWA and SAM using linear interpolation and dominant Hessian eigenvalues. Benchmarks are conducted across various tasks in natural language understanding graph representation learning and image classification. Observations The analysis of minima reached by SWA and SAM is novel. The study compares SAM and SWA comprehensively across various benchmarks. The paper is wellwritten and provides clear explanations with supporting evidence. Weaknesses The role of the base optimizer in the optimization process is not addressed. A 2D visualization depicting movement along random directions is missing. The saddle point hypothesis lacks supporting evidence such as an Eigen Spectral Density plot. Questions Would additional observations on the robustness of the optimizers hyperparameters enhance its practical value Are there tasksmodels that utilize a pretrained model but would the results differ if trained from scratch Can the hypothesis that train and test minimizers are not correlated be further evaluated using linear interpolation plots on an OOD test set While the paper claims to provide computational details in the Appendix they are not readily available. Why does Algorithm 1 return \u03b8SWASAM instead of \u03b8SWA", "Paraphrase: The study thoroughly compares stochastic weight averaging (SWA) and sharpnessaware minimization (SAM) using: Loss Landscape Analysis: Explores the landscape of the loss function to understand the performance of each method. Benchmarking: Evaluates the methods on a diverse set of datasets to assess their effectiveness across domains. Surprising Findings: SWA and SAM complement each other with their combination (SWASAM) outperforming either method standalone. The performance of SWA and SAM varies across domains highlighting the importance of a comprehensive benchmark. Strengths and Weaknesses: Strengths: Comprehensive benchmark study not previously available. Demonstration of the complementarity of SWA and SAM. Diversity in benchmark datasets showcases the significance of domain variation. SWASAM offers consistent performance across benchmarks. Weaknesses: The methodology for obtaining solutions (\\thetaNF) for each optimizer in section 3.1 is unclear. SWA was implemented with a nonstandard learning rate schedule which may affect the interpretation of results. Questions: 1. What base optimizers were used for each data domain (Vision NLP GRL) Could SWAs lower performance on NLP tasks be due to the optimizer choice 2. How was \\thetaNF computed for SWA and SAM What do \\thetaNF \\thetaSWA and \\thetaNF \\thetaSAM represent in Figure 2 3. Why was a cyclical or constant learning rate schedule not used for SWA Could evaluating SWA with a constant learning rate schedule provide insights into the impact of learning rate scheduling on the results", "Paraphrase: Summary: This research focuses on comparing the performances of two renowned optimizers for optimization with flat minima: Stochastic Average Momentum (SAM) and Stochastic Weight Averaging (SWA). The authors conduct extensive experiments to analyze the different characteristics of SAM and SWA and combine them to enhance performance. Strengths: 1. Provides comprehensive experimental results in various tasks to understand SAM and SWA. 2. Proposes a straightforward and reproducible combination method. 3. SWASAM outperforms SAM and SWA in certain tasks. 4. Presents clear and accessible visuals and explanations. Weaknesses: 1. Omits significant experiments from related works such as SAMs substantial impact on ViT training on ImageNet. 2. Lacks comparisons with advanced techniques like GSAM. 3. SWASAM underperforms SAM in certain experiments with negligible performance differences in others. 4. While addressing an important topic the proposed combination method may hinder the papers novelty. Questions: 1. Did the experiments utilize a distributed setup which can significantly impact SAMs performance and the analysis 2. How do data augmentation which enhances generalization interact with SAM and SWA potentially affecting their performance", "Paraphrased Summary: This research compares two widely used methods for finding flat minima: Stochastic Weight Averaging (SWA) and SharpnessAware Minimization (SAM). By studying the geometry of loss landscapes the paper suggests a combination of SWA and SAM. Experiments on various tasks demonstrate the strengths and weaknesses of each method shedding light on why and when they may fail. Strengths: Addresses an important practical problem with extensive experiments across different domains. Provides a comparative analysis of flat minimaseeking algorithms. Wellwritten and straightforward to understand. Weaknesses: (W1) Robustness to different settings: It is unclear if the results generalize beyond specific hyperparameters or random seeds. (W2) Credibility of linear interpolation analysis: The assumption of symmetric valleys may not always be valid potentially limiting the conclusions. (W3) Novelty and motivation: The proposed combination of SWA and SAM lacks originality and the motivation needs strengthening especially considering potential caveats of Hessian analysis. (W4) Limited interpretation: While the paper identifies cases where SWA and SAM fail it lacks detailed explanations for the failures. Further analysis such as global flatness and connectedness studies could provide additional insights. PostRebuttal Questions: Do SWA solutions and nonflat baseline solutions follow similar optimization trajectories potentially explaining observation 1 In the GINCode2 task is the Adam optimizer used for both SWA and SAM or only for nonflat solutions The paper mentions that SWA and transformers may not work well together. Can the appendix provide an explanation for this observation"], "lArVAWWpY3": ["Paraphrase: Summary: The authors contributions include: Establishing convergence rates through empirical calculations Demonstrating robustness against data impurities Enhancing computational strategies for Steins distance (SW) Strengths: The paper provides numerous new theoretical findings with seemingly rigorous convergence rates. Weaknesses: Empirical findings are limited and more could have been presented for complex distributionsdatasets to validate the theorys robustness. The paper lacks a comparison of Theorem 1 to other convergence rates (e.g. Equation 7.120 from a cited study). The logconcavity assumption needs further elaboration on its practical implications for machine learning practitioners. The three sections of the paper appear disjointed and could potentially be independent studies. The paper does not explore distributional SW or the possibility of computing theorems for this distance. Questions: None were raised.", "Paraphrased Statement: Summary: This article presents a detailed theoretical analysis of the statistical convergence rates and resilience of sliced and maxsliced Wasserstein distances. It establishes a precise empirical convergence rate for sliced Wasserstein distances which is sharp up to logarithmic factors for logconcave distributions. The article also examines the resilience of (max) sliced Wasserstein distances to data contamination. It demonstrates the computational guarantees of sliced Wasserstein distances using finite slices by deriving a Monte Carlo error bound. While it lacks formal computational guarantees for maxsliced Wasserstein distances it analyzes a previous subgradientbased local optimization algorithm and proves its computational complexity bound. Experiments verify the theoretical claims regarding projection and sample complexity the computational cost of subgradient maxsliced Wasserstein distance and its resilience to data contamination. Strengths: The article offers an indepth analysis of statistical properties resilience and computational guarantees for sliced and maxsliced Wasserstein distances. Its theoretical underpinnings are sound and serve as a valuable reference for researchers working on sliced Wasserstein distances. Weaknesses: The technical aspects of the article including notation and equations can be challenging to navigate. Enhancing the accessibility of the work would benefit researchers not primarily focused on theoretical aspects of sliced Wasserstein distances. Questions: 1. In Theorem 1 the assumption of \u03bc being logconcave is common in machine learning applications. However if this assumption does not hold it may be necessary to explore alternative measures or consider different distance metrics. 2. The terms \"one sample\" and \"two sample\" refer to comparisons between the same distribution and between different distributions respectively. Clarification in the article would help eliminate confusion. 3. Demonstrating the enhanced resilience of maxsliced Wasserstein distances compared to standard Wasserstein distances in a machine learning model is a potential area for future research.", "Summary The Sliced Wasserstein distance (SW) has gained popularity in machine learning due to its computational efficiency compared to the Wasserstein distance. Recent studies have investigated the theoretical properties of SW to justify its use. SW has been shown to preserve topological properties of the Wasserstein distance while achieving dimensionindependent sample complexity. However SW has limitations and alternative metrics such as the maximum Sliced Wasserstein distance (maxSW) have been proposed. This paper provides a theoretical analysis of SW and maxSW refining existing guarantees and addressing unsolved questions. Contributions 1. Convergence rates: The convergence rates of empirical distributions to true distributions under SW and maxSW are derived for logconcave or compact support distributions. These rates are faster or more explicit in the data dimension than previous results. 2. Robustness to outliers: The robustness of SW and maxSW to outliers is studied. The minimax risk for robust estimation under SW and maxSW for corrupted distributions is characterized showing that SW does not suffer from the \"curse of dimensionality\" unlike the Wasserstein distance. 3. Approximation errors: Guarantees are established for errors induced by approximating SW and maxSW. The error in Monte Carlo estimation of SW is bounded and convergence guarantees are provided for a subgradient method to approximate maxSW. 4. Experiments: Synthetic data experiments illustrate the theoretical findings. Strengths and Weaknesses Strengths: Addresses important theoretical questions related to SW and maxSW. Quantifies efficiency robustness and estimation quality of SW and maxSW in various settings. Wellwritten and clear. Weaknesses: Dense and may be challenging to digest for readers unfamiliar with the topic. Missing discussion of an important related work: \"Minimax Confidence Intervals for the Sliced Wasserstein Distance\" (Manole et al. 2019). Questions: Clarify discrepancies in equations and proofs. Provide a more realistic example for dependence on data dimension in convergence rates. Explain the use of optimal transportation cost formulation in the proof of Lemma 1. Confirm the order of the second term in Proposition 3. Provide more detailed explanations for the observed behavior in experiments.", "Paraphrased Summary: This paper focuses on SlicedWasserstein (SW) and MaxSlicedWasserstein (MaxSW) distances and presents significant theoretical advancements in: 1. Empirical convergence rates showing that SW and MaxSW achieve comparable sample complexities for logconcave distributions. 2. Robustness to data contamination establishing minimax rates for SW and MaxSW under bounded moment projections and covariance eigenvalues. 3. Computational complexity of MaxSW optimization demonstrating that subgradientbased local optimization converges efficiently without entropic regularization. The results extend and improve existing knowledge providing tighter bounds and less restrictive assumptions. While the paper lacks extensive experimental evaluation its theoretical contributions are substantial and applicable to various applications."], "wKf5dRSartn": ["The paper introduces a new combinatorial quantity STDmin which is smaller than the Recursive Teaching Dimension (RTD). The authors show that STDmin is upper bounded by VC dimension resolving a longstanding open question in the field. This result challenges the previously held belief that a property called GM Collusion Freeness is necessary for a parameter to be upper bounded by VC dimension. However it remains unclear how artificial STDmin is and whether it has practical implications for sample compression in learning. The paper is a significant contribution to the open question of relating passive and active learning but its overall value for resolving the issue remains to be seen.", "Paraphrased Statement: This paper examines the concept of batch teaching and aims to find a practical teaching complexity measure that is limited by a linear function of the VC dimension a factor that influences sample complexity in the PAC model. Before presenting the new parameter STDmin the paper examines existing teaching dimensions (TD RTD NCTD STD) and their features. The authors then define three desired qualities for a \"reasonable\" teaching model: class monotonicity domain monotonicity and the antichain property. To be considered linear in VC dimension the parameter must also avoid the \"GMcollusionfree\" property. The authors propose STDmin as a solution to these criteria. While STDmin satisfies the specified properties and has a linear relationship with the VC dimension it lacks a specific attribute known as unambiguity. The paper is clearly written and technically detailed. It presents a strong justification for developing a new teaching parameter by assessing existing parameters and their limitations. The potential implications of this work on learning and teaching are noteworthy. Overall this paper is highly regarded and deserves to be approved.", "Paraphrased Summary: Researchers have developed a new theoretical model that establishes a linear bound for teaching a concept to a learner based on the concepts complexity (VapnikChervonenkis dimension or VCD). This bound improves upon previous research which established only quadratic bounds for batch learning models. Unlike traditional machine learning problems this model focuses on the question of how much data is needed to teach a particular concept effectively. To prevent cheating the model constrains interactions between teacher and learner to ensure they do not share unauthorized information. Strengths: Clear definition of the problem and improved bound. Weaknesses: Highly abstract and theoretical making it more suitable for a theoretical machine learning publication. The notion of teachability seems artificial and purely of theoretical interest. Questions: The introductions mention of coding tricks was unclear and could be clarified further. It seems counterintuitive that teaching would not improve the bound below VCD given the teachers goal of helping the learner. The paper should emphasize that the definitions and model apply to batch learning. Concrete examples would enhance the accessibility of the paper for a broader audience.", "Paraphrased Statement: Summary: This paper examines the issue of training machines to \"teach\" concepts to learners using labeled data in a way that minimizes the number of labeled samples needed. The paper introduces and investigates a novel teaching complexity metric called STDmin which has several advantageous properties compared to existing complexity measures. The authors prove that STDmin is upperbounded by the VapnikChervonenkis (VC) dimension of the concept being taught becoming the first teaching complexity measure with this property. Strengths and Weaknesses: Originality: Studies a fundamental and intriguing problem. Proposes a new teaching complexity measure with desirable features. Demonstrates an upper bound on the new measure using VC dimension. Quality: Technically sound. Thoroughly incorporates relevant prior research. Clarity: Wellwritten and easy to understand. Significance: Potential for advancement in the field of machine teaching. Introduces a significant and potentially influential concept."], "rO6UExXrFzz": ["Summary Paraphrase: This study investigates the uniform covering number of Gaussian Noisy Neural Networks (GNNs). It introduces a novel framework for computing the covering number and proposes a metric for comparing different generalization metrics. Strengths and Weaknesses: Strengths: Original mathematical framework for calculating covering numbers. Sound and clear proof. Weaknesses: The framework is limited to bounded activation functions. Questions: 1. Why is normalization required for the L2extended metric 2. How does the bounded covering number and small capacity affect the performance of GNNs 3. What is the rationale for separating the first layer from other layers in the analysis 4. Can the study provide insights into why adding noise to neural networks reduces overfitting 5. Why dont the authors discuss other relevant literature on noisy neural networks such as Gulcehre et al. 2016", "Paraphrased Summary: This research presents a new metric to gauge the capacity of sets of functions that have been combined (composite function classes). Motivation: A limited capacity is observed in individual function classes yet their composition may exhibit an unbounded capacity. To address this discrepancy a novel capacity measure is proposed for randomized function classes. Key Features: Utilizes TV or Wasserstein distance metrics to measure the proximity between functions due to the introduction of randomness. Ensures that the capacity of a composite class is bounded by the covering numbers of its component subclasses. Connects the boundedness of the proposed measure to the generalization performance of deep neural networks. Strengths and Weaknesses: Strengths: Introduces a novel and significant concept for measuring the capacity of composite function classes. Potential utility in establishing generalization bounds for neural network function classes with limited activations. Weaknesses: Lacks an explicit example of nonLipschitz activation functions. Proposition 8 may not sufficiently demonstrate the benefits of the proposed measure. Theorem 17 requires bounded output values for the conversion from randomized to original function classes which may limit its applicability. Questions: Can the proposed measure handle ReLU activations If so in what situations does it outperform existing metrics", "Paraphrased Summary: This paper studies the capacity limitations of composing noisy functions. It shows that even when component functions have limited capacity their composition may not. However adding a small amount of noise to the functions bounds the compositions covering number. Strengths: Novel approach to generalization with noise in function composition. Results relate covering numbers of noisy functions to nonnoisy counterparts under various distance metrics. Noise smooths outputs reducing capacity and improving generalization for functions with high Lipschitz constants. Experiments demonstrate improved bounds with minimal noise. Weaknesses: Bounds still have exponential dependence on network depth and width. Polynomial depth dependence has been achieved in other work under different assumptions. Question: Is there a connection between the generalization bounds presented here and certifiable adversarial robustness using Gaussian smoothing"], "sn6BZR4WvUR": ["Paraphrase: Summary: This paper examines a set of deterministic dynamical systems that evolve slowly compared to their fast counterparts. This group encompasses Gradient Descent (GD) and its variants as well as the MPGD algorithm proposed by the authors. The authors demonstrate that as the learning rate approaches zero the dynamics of MDGD converge to a Stochastic Differential Equation (SDE) influenced by a heavytailed Levy stable process. They then calculate a generalization bound for this SDE in the symmetric scenario. Lastly the authors investigate the implicit bias introduced by these dynamics and find that it penalizes the Hessian of the loss function in a setting of weak perturbation. These theoretical findings are supported by empirical evidence. Strengths: The paper is wellwritten and clear presenting a series of original results. The various theoretical outcomes and empirical observations are complementary. The authors suggest that their framework could potentially be used to analyze other optimization techniques. Weaknesses: Important assumptions for the different theorems are only provided in the appendix and they lack explanation or justification. This hinders an assessment of the generality reasonableness and limitations of the results. Questions: Could you provide more detail on the assumptions required for the various theorems mentioned in the paper", "Paraphrased Statement: Summary: Using random gradients (stochastic gradient descent) often leads to better performance than using fixed gradients (deterministic gradient descent) in neural network training. Inspired by the notion that chaotic systems behave similarly to random systems this study introduces a chaotic version of deterministic gradient descent by adding perturbations driven by chaos theory. Theoretical Findings: The researchers provide theoretical evidence that their method improves generalization performance. The method approximates gradient flow on a function that discourages large variations in gradients (Hessian trace). Empirical Results: On a simple problem their method finds a wider valley minimum compared to standard gradient descent which gets stuck in a narrow minimum. On a regression task their method outperforms deterministic gradient descent with optimized hyperparameters for both methods. On the CIFAR10 classification task their method again surpasses deterministic gradient descent with optimized hyperparameters. Strengths and Weaknesses: The theoretical foundation of the method warrants further evaluation. The researchers have not fully optimized the baseline deterministic gradient descent method and the proposed methods superiority may not hold with optimal baseline settings. The researchers acknowledge that their theory does not fully explain their empirical observations suggesting that other factors may contribute to the methods effectiveness. Questions: Does the proposed method still perform better than deterministic gradient descent with an optimized step size Does the assumption that chaos mimics randomness accurately explain the methods success", "Paraphrase: This paper proposes the Multiscale Perturbed Gradient Descent (MPGD) algorithm a global optimization framework designed to enhance the generalization performance of predictive models. Strengths and Weaknesses: While the theoretical foundation of MPGD appears sound its superiority over Stochastic Gradient Descent (SGD) is not sufficiently demonstrated. Questions: 1. What are the advantages of MPGD over SGD 2. The papers numerical experiments should include SGD and regularized SGD algorithms as benchmarks for performance comparison.", "Paraphrased Statement: Summary: This paper introduces a modified gradient descent algorithm that incorporates a carefully added chaotic element. The researchers establish a connection between the algorithm and its Stochastic Differential Equation (SDE) limit involving a heavytailed L\u00e9vy process as the step size approaches zero. They derive a generalization error bound for the algorithm and demonstrate the regularization effect of the chaotic term which penalizes the Hessian matrix. Empirical evidence supports the regularization effect. Strengths: 1. The paper is wellwritten and accessible even for nonexperts. It clearly presents the main idea and emphasizes mathematical accuracy. 2. The paper introduces a novel gradient descent variant comprehensively analyzing its SDE limit generalization bound and regularization effect supported by numerical experiments. Weaknesses: 1. The paper is highly technical and may benefit from supporting examples or comparisons with vanilla Gradient Descent. Questions: 1. Regarding the generalization bound for convex or strongly convex loss functions does the proposed algorithm enhance or diminish the convergence rate 2. Do the algorithm parameters (\u03bc \u03c3) influence the generalization bound 3. Aside from numerical experiments can qualitative results be provided to identify situations where the chaotic regularized GD algorithm outperforms standard GD"], "znNmsN_O7Sh": ["Paraphrased Statement This research paper introduces a method for creating a 3D scene representation centered on objects. This representation is built from multiple scene views. Like other works the method employs slot attention for segmenting scenes but uses raycasting for rendering 2D views. However the authors propose a Scene Representation Transformer (SRT)based decoder that requires only one evaluation per ray query regardless of the number of objects represented. This improvement makes the model faster than previous approaches while maintaining strong quantitative performance on various datasets. Strengths Clear Presentation: The paper clearly describes the problem method and results making it easy to understand. Efficient Decoding: The model significantly reduces computational costs by requiring fewer evaluations per ray enhancing its speed and scalability. Strong Performance: The model outperforms ObjSURF both quantitatively and qualitatively on different datasets. Weaknesses Insufficient References: The paper lacks references to relevant works particularly the research on ROOTS by Sungjin Ahns team and other related studies. Limited Comparison: While the paper compares against ObjSURF it does not fully compare against UORF on simpler datasets or more complex scenes with high object counts. This omission hinders a comprehensive comparison to competing approaches. Incomplete Evaluation: The use of FGARI metrics for segmentation evaluations can be misleading when objects are not tightly segmented. Reporting regular ARI metrics would provide a more accurate assessment. Additional Questions How is the background handled in the model How does the learned initial slot representation impact performance and generalization capabilities with varying object counts How does the model compare to UORF on simpler datasets How do segmentation metrics change when using the ARI metric", "Paraphrase: Summary: This research aims to automatically separate scenes into distinct objects without additional guidance. This approach is based on the concept that creating new perspectives requires an understanding of geometry which can also aid in identifying items. The research introduces a groundbreaking method using transformers to learn objectcentered 3D representations addressing limitations of existing methods by handling complex scenes efficiently. The proposed method employs a slot attention mechanism with a lightfield rendering formulation guided by a viewpointdependent latent representation derived from these slots. An early fusion scheme is utilized to minimize computational costs by combining slots early and running the decoder only once. Unlike previous approaches the proposed method eliminates the need for depth supervision by utilizing a decoder based on SRT (Structure Representation Transformation) for efficient rendering. Strengths: The introduction clearly defines the goal of scene composition and geometry learning from observations. The related works section provides comprehensive context and highlights key differences from similar approaches. The approach is innovative and straightforward achieving promising performance on complex scenes and serving as a valuable baseline. Comparisons with existing methods demonstrate superior accuracy and efficiency particularly for complex scenes. Ablation studies effectively analyze the impact of various components. Weaknesses: The methods novelty is limited being primarily an adaptation of slot attention to SRT with an early fusion scheme. Qualitative evaluations suggest the Spatial Broadcast (SB) decoder outperforms the proposed Slot Mixer (SM) decoder in accuracy despite quantitative results indicating otherwise. Questions: Why does the qualitative evaluation differ from the quantitative results regarding the accuracy of the SM decoder Could the SM decoder enhance its performance by incorporating explicit background modeling to prevent oversegmentation", "Paraphrased Statement: This research concentrates on creating various perspectives of a scene and enhances the \"Scene Representation Transformer\" by utilizing a slotted bottleneck representation. This strategy facilitates the learning of distinct representations through regular view creation objectives resulting in more accurate disentanglements compared to previous unsupervised approaches. Additionally this representation enables object elimination (though not advanced editing functions like translation or scaling offered by prior NeRFbased unsupervised disentanglement techniques). The proposed method is assessed using CLEVR and MultiShapeNet datasets showing superior performance in both view synthesis and scene decomposition accuracy compared to ObSURF (a previous method). However the bottleneck representation limits the models expressiveness leading to reduced precision in view synthesis when compared to the original SRT model. Strengths: Straightforward and easytounderstand approach. The concept of incorporating slots in the SRT encoder is logical and valuable for the community. Informative ablations on decoder architecture choice. Notable improvements over ObSURF demonstrating advantages of incorporating slots. Insightful ablation highlighting the importance of view synthesis for accurate decomposition. Weaknesses: The \"slot mixer\" vs. \"broadcast\" decoder comparison is useful but the technical contribution is limited primarily combining ideas from previous works. A decline in the quality of generated views which could be explored further. The claim of \"3000x faster\" rendering is misleading as it stems mainly from SRTs speed advantage over volumerendering methods not this works specific contribution. Overall Impression: While this work lacks exceptional technical innovations or groundbreaking empirical findings it presents a wellexecuted and practical combination of existing techniques. It represents a valuable contribution to the community and warrants consideration for acceptance.", "Paraphrase: Summary The paper introduces a framework called Object Scene Representation Transformer (OSRT) for scene understanding. OSRT combines scene representation with Slot Attention creating a \"bottleneck\" that allows objectcentric representations to emerge from the learning process. The framework is efficient and performs well. Strengths and Weaknesses The paper is wellwritten and accessible. The technical details are sound and the claims are reasonable. The experimental evaluation is comprehensive and the performance is promising. However as the reviewers expertise is in representation learning rather than 3D understanding they raise concerns about the use of inductive biases in the OSRT framework. Inductive biases can help improve performance in lowdata lowcompute scenarios but may limit the models scalability to more complex data and compute resources. The datasets used in the paper are rendered so the objects and scenes have limited variation. The reviewer suggests that the models performance may not generalize to realworld scenarios with more complex and diverse data. The reviewer emphasizes that their comments are based on their personal research experience and should not be considered definitive. They await other reviewers comments and will make a further recommendation after discussion. Questions How well does OSRT perform on realworld datasets with more complex and diverse data What are the potential tradeoffs between using inductive biases and relying more on data and compute resources"], "tro0_OqIVde": ["Paraphrased Statement: This paper introduces a new operation called \"gnConv\" and a network called \"HorNet\" for computer vision tasks. Inspired by the success of vision Transformers the authors aim to create an architecture with adaptive longrange and higherorder spatial interactions. They achieve this by introducing a recursive form of gated convolution capable of higherorder explicit spatial interaction beyond the common twoorder. The authors demonstrate consistent improvements over Swin Transformer and ConvNeXt in established computer vision benchmarks including image classification (ImageNet) semantic segmentation (ADE20K) and object detection (COCO). After considering the authors rebuttal the reviewer maintains their original rating of 8 acknowledging the papers simplicity effectiveness and thorough experimentation. However they raise two questions: 1. Clarification on the extent of HorNets slower performance compared to ConvNeXt. 2. Potential for HorNet to achieve stateoftheart performance on ImageNet classification compared to models like CoAtNet. They also suggest that the paper should credit earlier works on higherorder interaction such as LSTMs gating mechanisms and ICCV 2017s \"SORT SecondOrder Response Transform for Visual Recognition.\"", "Paraphrased Statement: Summary: This paper introduces \"gnConv\" a new deep neural architecture based on convolutions. It employs \"gated convolutions\" that combine outputs from linear projection layers and depthwise convolution layers. The architecture incorporates higherorder interactions through recursive gating. Experiments demonstrate its effectiveness for image classification (ImageNet) object detection (COCO) and semantic segmentation (ADE20K). Strengths: Wellwritten paper with detailed experiments. Innovative combination of deep learning techniques resulting in a unique architecture for image tasks. Weaknesses: Limited experimental comparison. The authors should include recent stateoftheart methods for comparison (e.g. Dynamic Group Transformer MViTv2 Pale Transformer). Lack of throughput analysis. The proposed methods numerous small matrix multiplications and depthwise convolutions may hinder parallelization and result in slower performance than comparable methods. Questions: Provide theoretical insights into the benefits of higherorder interactions for network performance. Conduct additional comparisons with recent works and evaluate throughput for all methods.", "Paraphrase: The paper introduces the Recursive Gated Convolution (gnConv) a module that enables higherlevel spatial interactions through gated convolutions and recursive arrangements. This module can be easily integrated into existing transformer and convolutional neural networks. Building upon the proposed module the HorNet architecture is introduced demonstrating strong performance in image classification on ImageNet object detection on COCO and semantic segmentation on ADE20K. Strengths: Clear and wellwritten Thorough experimentation across multiple datasets and tasks Novel and effective design of gnConv Weaknesses: The rationale for pursuing higherorder interactions is not fully explained. The \"highorder interaction\" may be more accurately described as elementwise rather than spatialwise due to the use of elementwise multiplication (Mul) instead of matrix multiplication (MatMul). Alternatively it could be interpreted as channelwise interactions. The paper lacks a complexity analysis for varying values of n."], "nSe94hrIWhb": ["Paraphrased Summary: This paper introduces a novel and efficient method for computing topological descriptors (persistence diagrams) of large graphs. It utilizes two key insights: Only specific types of nodes contribute significantly to highdimensional topological features. Simplifying the graph to its kcore (nodes with degree \u2265 k) eliminates those nodes that are redundant for calculating topological features up to dimension k1. Removing dominated nodes further reduces graph size while preserving topological features. Experiments show that this approach significantly reduces the size of graphs from realworld and benchmark datasets. Strengths and Weaknesses: Strengths: Provides a principled approach for graph size reduction in persistent homology algorithms. Demonstrates that certain graph substructures can be ignored for higherorder topological feature calculations. Easy to implement. Clear and concise writing. Weaknesses: Does not explore the potential insights afforded by persistent homology when applied to reduced and pruned graphs. A practical application illustrating the benefits of the proposed reduction schemes would strengthen the paper. Additional Comments: Cite Hofer et al. for combining topological concepts with machine learning. Define terms like \"filtration\" for nonexpert readers. Use \"\u010cech complex\" instead of \"Cech complex.\" Improve the introduction to Section 4.2 and explain CoralTDA clearly. Correct the broken \"SYN\" dataset. Ensure consistent scale labeling in figures (use log scale with base 10). Address typos and minor grammatical errors. Questions: 1. How is the homotopy equivalence in Lemma 5 obtained 2. Explain the complexity calculation in Remark 9. 3. Can the approach be combined with filtration learning methods (e.g. Hofer et al.s Graph Filtration Learning) 4. Provide estimates of the pruning process time. 5. Does the method support arbitrary filtrations including VietorisRips complexes", "Paraphrased Summary: The authors propose two data reduction techniques that preserve the calculations used in persistent homology. These methods have advantages including their simplicity and wide applicability. However their computational costs during preprocessing remain unexplored raising concerns about their feasibility. Paraphrased Strengths and Weaknesses: Strengths: The algorithms are easy to implement and highly general. They effectively reduce data size. Weaknesses: The full computational complexity of the preprocessing step is \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0439. It is unclear how computationally demanding the algorithms are. Paraphrased Questions: Question 1: How practical are these algorithms in terms of computation time Question 2: What is the meaning of Figure 7 Question 3: Is the achieved data reduction sufficient to make persistent homology computations feasible for large datasets (e.g. datasets reduced from 1M to 0.5M vertices may still be too large for practical calculations)", "Summary: The paper combines two methods to accelerate the calculation of persistence diagrams for large graphs. The first method collapses similar vertices while the second analyzes the graphs structure to further reduce its size. By preserving the persistence diagram these techniques significantly improve computation times. Strengths and Weaknesses: Strengths: Focuses on a significant computational challenge in persistent homology. Clear and sound mathematical arguments. Weaknesses: Insufficient positioning compared to existing methods. Incomplete experimental results lacking comprehensive running time comparisons. Questions: If PrunIT is an adaptation of strong collapsibility should the experiments primarily focus on the novel CoralTDA algorithm Why is the research not contrasted with the method described in \"httpsarxiv.orgabs1809.10945\" Why are only persistence diagrams in dimension 0 evaluated as they are computationally simpler Supplementary: Line 625 requires additional details to clarify the crucial assertion that \"none of the vertices of S are removed from Gi.\""], "pGLFkjgVvVe": ["Paraphrased Statement: Instead of using Euclidean metrics the paper utilizes Riemannian distance metrics to achieve a more accurate estimate of uncertainties in outofdistribution (OOD) data. This approach is then applied to modelbased Reinforcement Learning (RL) settings. The authors propose a novel pullback metric for dynamic models and demonstrate its effectiveness on various RL benchmarks. Strengths: Wellwritten and clear presentation of the research. Impressive results indicating significant improvements in uncertainty estimation. Weaknesses: The focus is restricted to dynamic systems which may limit the metrics generalizability to broader uncertainty measurement tasks. The motivation for using parametric and nonparametric methods for uncertainty estimation could be further clarified. The paper could provide more context on offline RL and its relevance to the research. Questions: 1. How did the authors determine the choice of metric for the pullback metric for model dynamics How is the time derivative and geodesic computation performed 2. Is there a distinction between measuring the quality of OOD samples and the discrepancy of data examples 3. Why is \"high model error\" labeled twice in Figure 1 without any indication of \"low model error\" 4. Are the encoder and decoders parametric functions If so what are the specific details of this implementation 5. Does the equation in Definition 2 represent the curve length form from Equation 1 6. What is the rationale behind introducing an ensemble of decoders and how does it impact the subsequent experiments 7. How does the proposed epistemic uncertainty estimation method differ from the classical bootstrap ensemble method used in the baseline 8. Is the fourroom experiment designed with discrete or continuous actions How would increased sampling of the latent space affect the results and the likelihood of obstacles being encountered 9. What does \"Data Score\" in Table 1 represent 10. Why was tSNE employed for the intersection example latent space visualization and what is the dimensionality of the latent space", "Paraphrased Statement: Summary: To handle uncertainty in modelbased offline RL the authors offer a Riemannian metric that combines uncertainty from actual dynamics and external data. Their GELATO algorithm (with a charming name) employs this metric to determine an uncertaintyaware reward function by learning the shortest path on a distorted plane. This path combined with techniques like KNN and bootstrapping leads to enhanced uncertainty quantification. GELATO demonstrates strong performance in datasets for control and autonomous driving. Strengths and Weaknesses: Strengths: Clearly composed and compelling presentation of concepts motivation and experimental findings. Presents a novel framework for assessing model dynamics distance in latent space offering a more geometrically meaningful metric than Euclidean distance. Indepth review of successful and unsuccessful scenarios in various domains. Wideranging applications of the method as its geodesic capture approach can be applied to various RL estimates like Q values and rewards. Weaknesses: A minor error in referencing in line 19. A typo in line 29 where \"Riemannian\" is misspelled. Questions: Section 4.3s initial explanation could be improved for clarity. In this section the term \"M\" is frequently used to refer to the manifold except for one occurrence.", "Paraphrase: Importance of Quantifying Uncertainty in Offline Reinforcement Learning: Offline reinforcement learning agents may encounter novel situations when deployed online. To avoid such situations it is crucial for agents to assess uncertainty and adjust their actions accordingly. Proposed Method for Uncertainty Estimation: This work introduces a method to quantify uncertainty using knearest neighbors in a latent space. The distance metric used for neighbor selection is defined as geodesic distance within the latent space which is learned using a Variational Autoencoder (VAE). Strengths and Weaknesses: The proposed method is unique and builds on existing research. It is clearly explained and demonstrated effectively. The significance lies in enabling offline RL agents to quantify uncertainty which is essential for deployment in online environments. Questions: How adaptable is the method to different encoder and decoder architectures Is the derived metric applicable to more complex VAEs Can similar metrics be developed for more general generative modeling techniques"], "vmjckXzRXmh": ["The paper introduces a basic linear classification technique built upon a pretrained feature representation using a standard selfsupervised learning approach. The technique modifies a prototypical network approach by using a variant of Mahalanobis distance instead of Euclidean distance. Strengths: Simple and easytoimplement linear classification algorithm Theoretical derivations and guarantees (though based on unrealistic assumptions) Improved performance on the BREEDS dataset when compared to linear probing Weaknesses: The algorithm itself is not new or novel The theoretical bounds rely on impractical measurements and singleinstance bounds that are sensitive to outliers The empirical evaluation on BREEDS is not a true unsupervised domain adaptation experiment but rather a subpopulation shift task Overfitting of the linear probing baseline and a small dataset for selfsupervised pretraining may contribute to the performance gains Lack of experimental details and reproducibility due to proprietary code", "Paraphrased Statement: Summary: This paper investigates the ability of contrastive learning to improve model performance in crossdomain scenarios through linear transfer. Specifically it mathematically demonstrates that contrastive learning captures cluster identities and intercluster connections. Strengths and Weaknesses: Originality: The authors claim their theoretical analysis provides weaker and more practical assumptions compared to previous work. However verifying this claim requires further evaluation. Quality: The papers empirical validation is limited as it presents results on only one dataset (BREEDS) and compares its method (PFA) solely against linear probe. Enhanced experimental evidence is necessary. Clarity: The paper is wellwritten and easy to understand. Significance: While the paper addresses linear transferability it may have limited relevance for modern deep learning applications which often involve nonlinear transfer. Questions: Refer to the concerns raised in the \"Weaknesses\" section about the limited empirical evaluation and the potential lack of broad applicability.", "Paraphrased Statement: Summary: This paper explores the effectiveness of contrastive learning for unsupervised domain adaptation when unlabeled data is available for both source and target domains but labels are only accessible for the source domain. It involves three steps: 1. Train representations using contrastive learning on the combined unlabeled data. 2. Train a linear classifier on source representations using source labels. 3. Transfer the linear classifier to the target representations. This approach known as linear transferability performs well under specific conditions where data from the same class across domains is more similar than data from different classes. The paper formally defines these conditions using an augmentation graph. Linear transferability requires three properties of the graph: minimal edges outside classes high expansion within classes and high expansion across the same classes in different domains. The analysis builds upon the spectral contrastive learning framework proposed by HaoChen et al. The paper also introduces a new algorithm (PFA) that utilizes preconditioning to enhance the source linear classifier. Strengths: The phenomenon of linear transferability and its enhancement by contrastive learning is intriguing. The assumptions and analysis are wellgrounded in previous work on domain adaptation and contrastive learning. The paper is clear and accessible. The proof sketches provide valuable insights into the analysis. Weaknesses: The paper lacks discussion of potential future directions. The superiority of PFA over logistic regression is unexplained. The assumptions are intuitive but not experimentally verified. It would be beneficial to explore exceptions to the assumptions.", "Paraphrased Statement: Summary: The paper investigates how representations learned through contrastive learning on a specific population can be transferred to a different population to enable linear classification between the same two classes. The Positive Pair Graph (PPG) plays a crucial role in understanding this phenomenon. Key Findings: For linear transferability connections between example clusters within the same class across different subpopulations should be stronger than connections between clusters of different classes in different subpopulations. The Preconditioned Feature Averaging (PFA) algorithm which involves preconditioning before averaging features improves linear head performance in transferring to outofdomain settings. Strengths: Addresses a significant and understudied problem in selfsupervised learning. Clear writing and intuitive explanations. Offers a graphtheoretic explanation for successful empirical approaches in machine learning. Weaknesses: Some theoretical tools and insights overlap with previous work. Questions and Suggestions: Can the paper include a comparison to the baseline contrastive approach of using the InfoNCE loss and training a linear layer on top during finetuning"], "vfCd1Vt8BGq": ["Paraphrased Statement Summary: Researchers have developed various generalization bounds for machine learning models using mutual information. These bounds differ based on how they measure mutual information (e.g. between the hypothesis and the entire dataset or a subset) and whether they condition on other variables. This work presents an alternative approach that removes a single instance from the training set. Its advantages include its fast computation (linear in the number of examples) and its nonvacuous nature. It shows that measuring mutual information between model predictions and data yields similar bounds that are tighter due to the data processing inequality. Finally the authors relate these bounds to stability and flatness. Strengths: Introduces a novel approach for analyzing generalization. Establishes connections to stability and flatness. Weaknesses: No tightness results and bounds can be vacuous in most situations. SGD bound appears to increase with the number of SGD steps which should decrease. Details: The approach introduces a new theoretical framework for bounding generalization error. It suggests a nonvacuous measure of local minima flatness that avoids pitfalls in previous methods. The authors emphasize the nonvacuous nature of their bounds but experimental results do not fully support this claim. The SGD bound contradicts the expected behavior of bounds decreasing with increased SGD steps. The experiments use small datasets and pretrained models which may limit the validity of the bounds. Questions: Can you doublecheck the bound in Lemma 4.1", "Summary This paper investigates the use of conditional mutual information (CMI) to derive generalization bounds for neural networks. It improves upon prior work by reducing the computational complexity from exponential to linear in dataset size. Strengths and Weaknesses Strengths: Wellwritten and accessible even for nonexperts. Addresses an important problem in the machine learning community. Potential for high impact if the authors claims about novelty are accurate. Weaknesses: Difficult to fully evaluate novelty without expertise in the field. Typographical error on page 2 paragraph 2 line 3. Lack of clarity regarding the role of U in Theorem 4.1. High computational requirements due to the need to train N models for each datum. Comments for Improvement: Provide a clearer explanation of U in Theorem 4.1. Discuss the computational complexity of the method and suggest ways to reduce it. Consider evaluating the method on larger datasets using techniques such as finetuning the last layer of a DNN.", "Paraphrased Statement: Summary: This study introduces \"looCMI\" a calculable and understandable bound for conditional mutual information (CMI). Its inspired by leaveoneout crossvalidation and assumes that a generalized algorithm should be unaffected by minor changes (e.g. excluding a random training sample). The bound is then interpreted in the function space for a tighter bound (flooCMI). The paper provides empirical evaluations and detailed analysis. Strengths and Weaknesses: Strengths: Intriguing assumption that a generalizable algorithm shouldnt be affected by small perturbations. Proposed bounds are simple and logical. Wellwritten and accessible paper. Weaknesses: Discordance between empirical results in the main paper and supplementary material for low sample sizes. Ablation studies on the number of samples particularly for larger numbers are missing. Pretraining dataset size may have a more significant impact than the finetuning dataset size but there is no comparison across different pretraining dataset scales. Minor: Key code lines for looCMI and flooCMI should be specified. A conclusion section should be added. Questions: 1. Does the small sample size lead to high randomness in previous empirical results Have ablation studies been conducted on varying sample sizes 2. How does the size of the pretraining dataset affect the quality of the proposed bounds", "Paraphrased Statement: Summary: This study introduces a new leaveoneout conditional mutual information (looCMI) technique for approximating generalization risk specifically two informationtheoretic upper bounds: looCMI and flooCMI. These bounds are reportedly more economical than traditional CMI as they halve the computational cost. The authors provide theoretical justification and report preliminary empirical findings on an image classification problem. Strengths: Introduction of new informationtheoretic generalization bounds Sound theoretical analysis demonstrating desirable properties of the bounds Weaknesses: Lack of comparison with existing methods (e.g. fCMI from [1]) Limited empirical results only covering simple datasets Absence of stability analysis and error bars in Figure 3 Insufficient demonstration of the bounds effectiveness in empirical settings (e.g. across different models and datasets) Questions: 1. Are these bounds applicable for model training or solely for model selection What is their computational cost 2. The zero training errors reported in Table 1 raise concerns about data overfitting."], "pyLFJ9TBZw": ["Paraphrased Statement: Summary: This study challenges the common assumption in multitask learning that target variables are conditionally independent. Instead it proposes that a hidden confounding factor may influence multiple targets. To address this the authors introduce Generative Multitask Learning (GMTL) which incorporates the confounding factor into its loss function. GMTL can be combined with traditional Multitask Learning (DMTL) loss using an adjustable weight. Experiments show that increasing the weight of GMTL loss improves accuracy. Strengths: Addresses the issue of confounding factors in multitask targets for the first time. GMTLs objective is based on sound logic. Experimental results support the proposed approach. Weaknesses: Assumptions about the existence and nature of the confounding factor are not clearly stated. Experiments assume a specific data structure with only two targets raising questions about extending the method to multiple targets. Questions: Under what circumstances can we assume the presence of a confounding factor and why do the experiments meet these conditions How can GMTL be extended to handle multiple targets and what assumptions are made about the confounding factor in this case", "Summary: This research paper introduces a method called Generative Multitask Learning (GMTL) to address the issue of targetcausing confounding in generative models. It aims to identify unvarying information across different interventions (or in cases of target shift) using an additional auxiliary target. Experimental findings indicate that GMTL enhances robustness against target shift. Strengths: 1. Introduces the concept of \"anticausal learning\" for multitask learning. It leverages causal factorization of the joint distribution to utilize invariant information in addressing target shift problems. 2. Demonstrates the effectiveness of the proposed methods through experimental evaluations. Weaknesses: 1. Questions the transformation of p(y y\u2019x) in Eq. (5) to p(yx)p(y\u2019x) in Eq. (6) given that y and y\u2019 are not independent conditional on x in Figure 1(c) of GMTL. 2. Raises the need for an effective method to determine the hyperparameter \\alpha in the absence of prior information. Questions: Refer to the weaknesses section for the specific questions raised by the reviewer.", "Summary The paper introduces a method (GMTL) that enhances the robustness of multitask learning models to target shift a situation where the prior probability of labels changes. Unlike traditional formulations GMTL assumes that the observed data is causally generated from the targets which are in turn influenced by an unobserved confounder. This differs from the commonly held assumption that tasks are conditionally independent given the input data. The proposed method leverages Bayes rule and factorizes the likelihood by task to modify discriminative multitask learning approaches and account for potential changes in the label distribution. Experiments on synthetic and reweighted data demonstrate that GMTL offers improved robustness to target shifts compared to standard methods that ignore their possibility. Strengths and Weaknesses Strengths: Novel approach that exploits knowledge of the data generating process to enhance model robustness. Sound theoretical foundation and convenient implementation. Weaknesses: Incomplete framing of targetcausing confounding. Concerns about modeling relaxation choices. Limited exploration of performance expectations. ReFraming The paper frames GMTL as addressing a flaw in discriminative multitask learning (DMTL) under alternative assumptions. However the DMTL assumption of conditional independence between tasks is testable allowing users to assess its suitability for their specific problem. The authors should acknowledge the similarity of GMTL to causal graphical approaches which offer potential for alternative methodological options. Relaxing the GMTL Objective The authors relaxation of p(yyx) for convenience could compromise model accuracy. Theoretical justification or experiments comparing this relaxation to a joint modeling approach should be provided. The role of crossstitch networks in modeling p(yyx) jointly should be clarified. Interpretation of \u03b1 The interpolation parameter \u03b1 should be framed as part of a distributionally robust optimization problem for the given uncertainty set. Performance guarantees for different \u03b1 values should be explored especially the worstcase average performance across the uncertainty set. Minor Note The renormalization of normalized probabilities obtained from q1\u03b1 needs to be addressed. Questions Evaluate the impact of relaxing p(yyx). Justify the choice of \u03b1based interpolation for GMTL. Analyze model performance across the target shift uncertainty set."], "uAIQymz0Qp": ["Summary: This study presents DMAP an algorithm that enables reinforcement learning to adjust policies in locomotion environments where the character lacks a fixed body. DMAP employs a feature encoder based on attention. Strengths and Weaknesses: The concept is intriguing and the algorithm demonstrates effectiveness. The paper clearly justifies the architectural decisions for the DMAP pipeline. It analyzes the impact of various components through ablation studies. Questions: Due to excessive information in the main paper and omitted details some questions arise. Expanding ablations in the appendix and clarifying the model description would enhance reproducibility. The changing character is akin to a changing environment resembling zeroshot adaptation or metalearning. Comparing methods from these areas with DMAP in terms of results and efficiency would be valuable. In distributed joint controllers does the included proprioceptive state reflect the entire character or just a specific joint How are K and V matrices generated from independent representations What is the nature of the cells in V and is its dimension a hyperparameter Figures 3 and 4 could be combined for clarity. The oracles performance appears to be inferior to DMAP. Are there explanations for this In the ablation described in Section 5.4 are eis removed from the input of each omega If so does this make it similar to TCN In Figure 6 why does the hip controller only observe the front knee velocity and the knee controllers only observe the back knee velocity Separate policies may slow down computation. How does learning speed compare to methods with a single policy Ablating the significance of distributed joint controllers could involve providing a single policy with the full E(t) for total control. Are separate policies practical for more intricate characters like humanoids", "Paraphrased Statement: Summary: This research examines how to adapt to changing bodies during continuous control tasks. Unlike typical reinforcement learning (RL) settings that focus on maximizing rewards this study evaluates how well policies can handle morphological changes in the test environment. The approach involves separating each proprioceptive channel and creating local embeddings. An attention mechanism dynamically gates these embeddings to solve the control task. During training bodies are simulated with a fixed distribution of perturbations and performance is evaluated in a test environment with unseen values. Key Result: The proposed architecture performs as well as or better than an \"oracle\" that knows unseen body configurations during the test. Strengths: Addresses a realistic challenge faced by humans and animals and may be applicable to robotics as bodies become more complex. Clear experiments and writing. Demonstrates emergent symmetries in the body space. Weakness: The authors did not explore sequencetosequence approaches which may be more dataefficient or handle drastic body amputations. Questions: How does the approach perform under more extreme morphological changes such as broken legs Can the speculation about untangling mechanisms contributing to robustness be further investigated", "Paraphrase: Generalizing learned behaviors in Reinforcement Learning (RL) for agents with changing physical characteristics (morphology) is challenging. This paper explores a technique for learning policies that use past observations to determine which parts of the environment the agents should focus on when making decisions. The policy adapts to changes in the size and length of limbs across four different agents. Evaluations show that the proposed method outperforms existing approaches. Strengths: Endtoend learning of policies for adapting to morphological changes. Sensible network design enabling the development of meaningful attention mechanisms. Focus on indicators of a robust motor system (proprioception force control and attention). Weaknesses: Unclear motivation: Limited discussion on the prevalence of situations where limb length and thickness change frequently. Lack of clarity in the experimental section: Difficulty linking experiment design to claims. Insufficient experimental comparisons to prior work especially methods like MetaRL (Pearl et al. 2019) and adaptation in dynamic environments (Nagabandi et al. 2018 2019). Limitations of the attention module: Need for further analysis on the amount of data and morphologies required for effective learning. Questions: What are the limitations of the attention module How many morphologies or tasks are necessary to learn a generalizable representation", "Paraphrased Statement: Summary: Researchers explore the challenge of robot locomotion with varying body shapes and sizes. They introduce a new neural network architecture inspired by biological principles to control individual joints and distribute sensory information across body parts. The architecture is compared to existing methods: A baseline that relies solely on body movements which struggles with body variations. An idealized method that uses prelearned information about body changes. The new method outperforms both baselines and provides valuable insights through ablation studies and visual representations. Strengths: Clear and organized presentation. Interesting and practical challenge of body variations in locomotion. Use of system identification to analyze the approach. Impressive performance compared to baselines. Informative visuals. Weaknesses: Lack of comparison with similar hierarchical reinforcement learning approaches. Relies on SAC training algorithm which may influence results. The presented tasks may be relatively easy potentially limiting the impact of the new approach. Suggestion to explore more challenging environments or develop harder tasks to further test the algorithms capabilities."], "mMdRZipvld2": ["Paraphrased Summary This paper introduces a method to measure the functional similarity between neural networks. Existing techniques rely on comparing the intermediate outputs of two networks which can be influenced by input data. This leads to false similarity scores. The proposed method removes the impact of input data by using a regression model. The resulting similarity scores correlate strongly with domain similarity and outofdistribution (OOD) accuracy. Strengths The method formulates the data influence as a regression problem making it easy to solve for deconfounded similarity. It can be applied to any existing similarity measure improving performance in most cases. Weaknesses The purpose of measuring functional similarity for transfer learning and OOD generalization could be clarified. The method measures similarity between layers of networks not the entire networks. Averaging layerwise similarities may not be adequate for deep networks. Questions In Section 2.1 the twostage similarity measurement method is unclear. Why is the first stage (Eq. (1)) necessary The invariance properties discussed in Section 3.3 seem irrelevant. Why are they important if they dont affect interexample similarity Suggestions Clarify whether Eq. (2) allows for similarity between different layers such as m1 and m2. Correct the typo \"decounfounded\" in line 88. Define K0 in Section 3.1 after Eq. (3). Add the legend \"Averaged similarity\" to Figure 4 (right) as in Figures 2 and 3.", "Paraphrase: This study explores the impact of various similarity metrics used to compare neural networks layerbylayer. It examines how these metrics such as CKA and RSA can be affected by confounding factors resulting in inaccurate representation of functional similarity. To address this the authors propose a simple adjustment that removes these confounding factors by eliminating the input similarity structure from the representation similarity structure. Through extensive experiments they demonstrate the improved performance of the deconfounded similarity measure under various conditions. While the fix does not directly enhance model performance it offers valuable insights into the internal workings of neural networks. Overall the paper is commendable for its clear and concise presentation strong experimental validation and the provision of code and additional materials. While the problem it tackles is relatively specific it makes a meaningful contribution to meta and transfer learning research and can lead to improved model interpretation and training practices.", "Paraphrased Summary: The paper proposes an improved method for calculating representational similarity metrics (CKA and RSA) which measure the similarity between representations from different layers of the same or different networks that process the same objects. This method called \"deconfounded CKA\" (dCKA) incorporates an idea from biostatistics to remove the influence of interobject similarity. Experiments on networks trained from different initial conditions finetuned networks and transfer learning setups show that dCKA: Is more sensitive than regular CKA Better reflects the intended notion of functional similarity Strengths: Innovative idea of deconfounding Demonstrated effectiveness through empirical results Weaknesses: Limited explanation of the intuition behind deconfounding Missing experimental details such as the selection of kernels used in dCKA Questions: 1. What is the meaning of \"vec()\" in equation (4) 2. How well does dCKA perform in the task of identifying the most similar layers between networks trained from different seeds 3. In equation (3) there is a repeated term \"dKf1m1.\" Is this an error 4. Which kernels were used in the experiments for dCKA"], "wiGXs_kS_X": ["Paraphrased Statement: Logical Credal Networks (LCNs) allow for expressing uncertain knowledge in the form of probabilistic statements about the probability of logical formulas. LCNs are represented as sets of formulas that define upper and lower bounds for probability values. Semantics and Structure: An LCN model is a probability distribution over interpretations that satisfy the constraints in the LCN. The network structure of an LCN can be represented as a graph where each atomic formula is independent of its nondescendant nonparent variables given its parents. This Markov condition simplifies to that of Bayesian networks and Markov Logic Networks when the LCN represents a specific case. Inference: Inference in LCNs involves computing lower and upper bounds for probabilities of formulas. The proposed approach uses a nonlinear constraint program with a variable for each interpretation. The program is nonconvex making inference NPhard in the number of atomic formulas (n). Scalability is limited to problems with up to 11 atomic formulas. Applications and Evaluation: LCNs have been evaluated on random LCNs Mastermind puzzles (outperforming Bayesian networks) and credit card fraud detection (achieving higher accuracy than other methods). Strengths and Weaknesses: Strengths: Expressive constraints allowing for a wide range of knowledge representation Generalization of Markov conditions for LCNs Applicability to Bayesian networks and Markov Logic Networks Weaknesses: Limited scalability due to the need to consider all interpretations Inference is NPhard in the number of atomic formulas Suggestions for Future Work: Developing algorithms that avoid materializing all interpretations Restricting the language to improve scalability", "Paraphrased Summary: This research introduces a new graphical model and language for representing probabilistic constraints in logical formulas both propositional and firstorder. It provides greater flexibility than previous languages by assuming variable independence by default instead of explicitly declaring it. Any dependencies must be specified explicitly. The paper introduces Markov conditions for these models and shows through smallscale experiments that their enhanced expressiveness improves inference quality. Strengths and Weaknesses: Originality: The proposed model is novel for expressing probabilistic logic constraints. Quality: The paper is wellwritten and understandable with a solid proposal and promising initial results. Clarity: The model is presented clearly. Significance: The new model offers advantages for logical inference enabling probabilistic bounds without making inference trivial. Limitations and Questions: Scalability: Exact inference in the model may be challenging for largescale problems. FOL Grounding: The paper addresses FOL problems by grounding them into propositional ones which may limit scalability. FOL Solver Adaptation: It is unclear if solvers for fragments of FOL could be used for more efficient inference. Approximation Schemes: The paper does not discuss potential approximation schemes for scalability leaving it as future work. Comparison to Other Approaches: The paper primarily compares to other probabilistic graphical models but not to certain probabilistic versions of answer set programming or PSAT solvers. Typos: Line 195 and others: Equation references are missing parentheses (unusual but may be intentional). Line 217: Parenthesis missing after P(ab\\not c)", "Paraphrase: This paper presents a new probabilistic logic that uses probability statements with intervals extending Markov Logic Networks and Credal Networks. The authors define the syntax and semantics of this logic (IntervalValued Logic LCN) and show that inference is a nonlinear task. Experiments demonstrate that LCN outperforms existing approaches in Mastermind with Uncertainty and Fraud Detection Data. Strengths: Novel contribution that addresses a gap in the literature on probabilistic logic. Clear and accessible presentation. Promising experimental results. Weaknesses: Inference is treated as a generic nonlinear problem without exploiting taskspecific properties for faster solutions. The grounding operation and its implications for inference are not fully explored. The authors do not formally establish the extension of LCN to both Markov Logic Networks and Credal Networks. Existing techniques for optimizing multilinear and fractional multilinear functions in Credal Networks could be adapted to LCN and should be discussed. Questions: Can the hardness of LCN be established using complexity results for Credal Networks Can LCN be represented as a larger Credal Network allowing for inference using existing methods What is the relationship between LCN with sharp probability assessments and Markov Logic Networks How do different decisionmaking approaches perform with LCN when multiple outputs are considered Why are the execution times for different methods in Table 3 so similar How can loopy propagation techniques for Credal Networks be applied to LCN", "Paraphrased Summary: A new probabilistic model called Logical Credence Networks has been proposed. This model enables specifying probability ranges for logical rules either as conditionals or marginals. A mapping from logical to graphical structures is proposed to represent dependencies. An inference procedure based on solving equations within the ground truth theory generates upper and lower bounds on marginal probabilities. Experiments on synthetic structures mastermind puzzles and fraud detection data demonstrate the models effectiveness. Strengths: Flexibility to specify probability ranges rather than single values. Improved intuitiveness of probabilities by constraining them based on dependencies. Weaknesses: Lack of clarity in parameter learning with examples suggesting learning occurs based on test data. Limited scalability for practical applications. Uncertain comparison to MLNs and Problog which involve learned weights. Similarity to existing models such as PSL (Probabilistic Soft Logic). Questions: How is parameter learning implemented in this model Are there potential concerns about scalability and applicability to realworld scenarios How does this model compare to similar approaches like PSL and what is its unique contribution"], "pBpwRkEIjR3": ["Paraphrase: This paper introduces a new type of deterministic and stochastic bilevel optimization (BO) problems. Unlike classic BO problems the outer function in these problems includes an additional nonsmooth term making the model more versatile (e.g. it allows for L1 regularization). The paper proposes three algorithms: two for solving deterministicstochastic BO problems in the new form and an accelerated version of one of them. Strengths: Expands the scope of BO problems necessitating new algorithms. The algorithms achieve a superior convergence rate compared to existing methods under similar assumptions. Weaknesses: The paper only mentions L1 regularization as a scenario where a nonsmooth outer function is required potentially limiting its applicability. It could benefit from additional examples or applications. Questions: 1. Algorithm 1 (line 8): The computation of the partial derivative \\omegat needs further clarification. 2. Algorithm 2: The use of \\etat in step size needs more explanation. Why is it different from the use of \\lambda in Algorithm 1 3. While the proposed algorithms outperform baselines the paper could provide more insights into the source of this improvement. Is it primarily due to the Bregman distance used instead of h(x) 0 Providing such explanations would enhance the theoretical value of the work.", "Paraphrase: Summary: This paper introduces the Bregman distance to bilevel optimization proposing three methods (BiOBreD SBiOBreD ASBiOBreD) to handle both deterministic and stochastic bilevel problems. These algorithms achieve the best target accuracy (\\epsilon) and improved condition number (\\kappa) compared to other benchmarks. The analysis is applicable even for nonsmooth outer functions. Experiments demonstrate the superior performance of these algorithms. Strengths and Weaknesses: Strengths: Improved condition number in convergence analysis. Demonstrated superior performance in various experimental settings. Standard and clear assumptions and convergence analysis. Weaknesses: Some Lemmas (2 and 4) in the main body lack explanations. The theoretical analysis is standard without highlighting technical innovations. No ablation studies comparing algorithm performance in different settings (e.g. batch sizes). The algorithm is analyzed for nonsmooth functions but experiments use smooth functions. Questions: 1. Provide explanations for Lemmas 2 and 4 to clarify their meaning. 2. Discuss the technical innovations compared to previous work [21] specifying which works in the main body and supplementary are being referenced. 3. Conduct ablation studies to compare the proposed algorithms performance in different settings. 4. Consider nonsmooth experiments to verify the robustness of the algorithm in practice.", "Paraphrased Summary: This paper analyzes a nonconvex optimization problem with a strongly convex innerobjective and nonconvex outerobjective. Using Bregman distance the paper develops deterministic and stochastic optimization algorithms accompanied by convergence analysis. Theoretically the proposed algorithms with Bregman distance enhance performance regarding condition number (\u03ba) while variance reduction techniques further improve the dependence on accuracy (\u03b5) by half an order. Numerical experiments demonstrate the algorithms efficiency. Strengths and Weaknesses: Strengths: Improved theoretical bounds clear presentation and sound numerical experiments. Weaknesses: Predictability of improvements as variance reduction techniques enhance dependency on \u03b5 and Bregman distance assists with \u03ba. Questions: Q1 (Algorithmic Design): Are there additional design efforts beyond substituting exact gradients with gradient estimates Q2 (Theoretical Analysis): Are there theoretical considerations beyond incorporating biases from gradient estimation"], "v_0F4IZJZw": ["Paraphrased Statement: This study explores adaptive training techniques to reduce toxicity while preserving language model quality. It examines three key aspects: training corpus model size and parameter efficiency. Contributions and Observations: Selfgenerated datasets are more effective for detoxification than domainadaptive training on nontoxic subsets of pretraining data. Larger language models exhibit greater susceptibility to toxicity from training corpora. Adapters offer a better balance between toxicity reduction and perplexity compared to prefix tuning. Strengths: Originality: Proposes a selfgenerated dataset for detoxification and studies the effectiveness of detoxifying approaches on massive language models. Quality: The paper is wellwritten and presents information clearly. Performance improvements and reductions are documented and results from multiple random seeds are included in the appendix. Significance: The findings provide insights for future research on tradeoffs between toxicity and utility in language model design. Weaknesses: Clarity: The paper focuses on generative language models but uses broader terms like \"language model\" in the title and other sections which could lead to confusion. Significance: While the proposed approach demonstrates improvements in toxicity reduction it consistently underperforms baselines in terms of language model quality. Practical guidance for choosing the best approach for a balance between toxicity and utility is limited. Applicability: The study focuses solely on generative language models. Its findings may not be applicable to encoderonly language models used in downstream tasks. Questions: Q1: What are the model architectures of the different scales used Q2: How does the performance of the proposed approach compare to prior domainadaptive training methods mentioned in the related work Q3: How does the performance of reinforcement learningbased approaches like InstructGPT compare to the proposed approach Q4: Can combining domainadaptive training (SGEAT) with decodingtime approaches (e.g. Rejection Sampling) further improve results Q5: Is it possible to combine the proposed selfgenerated dataset approach with domainadaptive training approaches like DAPT and achieve a better tradeoff", "Paraphrased Statement: Summary: This study investigates domainadaptive training methods to mitigate language model toxicity considering training data size and model efficiency. The proposed SelfGeneration Enabled domainAdaptive Training (SGEAT) approach which utilizes selfgenerated nontoxic data for detoxification outperforms existing methods. The research also demonstrates that toxicity stems primarily from the training data rather than model size and that larger language models pose greater challenges for detoxification. Adapter layers added to standard language models enhance model performance and efficiency while preserving detoxification levels. Strengths: Addresses the crucial problem of toxicity in language models. Innovative use of SGEAT for automated generation of detoxifying training data. Exhaustive experimentation and ablation studies support the proposed methods. Evaluation of detoxification performance as well as the quality and utility of detoxified models. Valuable insights on the relationship between toxicity training data and model size. Weaknesses: Lacks evaluation of SGEATgenerated training data when used with different language models. Detailed analysis of training samples is absent despite the evaluation of finetuning epochs. Discrepancy between the claimed lower toxicity and the observed trend in Figure 2. Evaluation of SGEAT with adapter layers only includes perplexity and utility metrics not toxicity. Questions arise about the control and duplication of SGEAT standard outputs as well as the exclusive generation of SGEAT augmented data from nontoxic SGEAT standard samples. Questions: Refer to the major weaknesses section for specific questions.", "Paraphrased Statement: This study investigates the impact of model size training dataset and parameter optimization on domainadaptive training (adjusting pretrained models on specific data to eliminate bias). The authors introduce a new method called \"SelfGeneration Enabled DomainAdaptive Training\" (SGEAT) where a selfgenerated dataset is used to refine the model. SGEAT can also be combined with a detoxification technique applied during decoding. The authors find that SGEAT effectively reduces model bias while model size has minimal impact. This suggests that bias originates from the training data. They also observe that detoxifying larger language models is more challenging. Strengths: Detoxifying language models is crucial for responsible deployment. SGEATs efficacy is noteworthy indicating potential benefits from exposure to selfgenerated data. The study employs both automated and human evaluation methods. Weaknesses: Some results may be expected and less novel such as the tradeoff between detoxification and performance and the difficulty of detoxifying larger models. Questions: How can the underlying assumptions of SGEATs effectiveness be further explored What are the limitations of the research and in what areas could it be improved", "Paraphrase: This research explores language detoxification in various contexts examining factors such as training corpus size and modeling efficiency. It introduces a novel detoxification method involving training language models (LMs) with a smaller selfgenerated nontoxic corpus compared to the Open Web Text Corpus (OWTC). Experiments demonstrate the crucial role of the training corpus with larger model sizes necessitating greater training efforts or larger corpora. Additionally the study highlights the importance of the adapter which reduces detoxification while enhancing the language models perplexity. The research provides insights into critical components for designing detoxification models and emphasizes the need for future originality in proposed methods. Questions: 1. Why use average accuracy utility ToxScore data indicates accuracy making average accuracy utility unnecessary. 2. Representation of models in LM section: GPT3 is a 175B parameter model so GPT2 small medium and XL representation seems more appropriate. 3. Experiment with same dataset size for OWTC: Figure 2 should compare models trained on the same amount of data to demonstrate overfitting. 4. Diversity evaluation of SGEAT(heuristic): Evaluate diversity using distinct1 distinct2 distinct3 or other metrics to explain the increased toxicity of SGEAT(heuristic). 5. Validation set for PPL evaluation: Clarify whether the validation set for PPL evaluation is derived from OWTC nontoxic text or each training corpus. 6. Formation of DEXPERTS with SGEAT (augmented): Explain how DEXPERTS with SGEAT (augmented) is created. Did you train both expert and antiexpert models with SGEAT (augmented) or only the detoxified one"], "kjR8GiwqCK": ["Paraphrase: This paper examines regret minimization in ergodic MDPs without discounting using the average reward criterion. It extends the seminal work by Burnetas and Katehakis (1997) in this context and adapts the IMED bandit algorithm (Honda and Takemura 2015) to this setting. The resulting algorithm IMEDRL achieves asymptotic optimality in matching the lower bound established by Burnetas and Katehakis. Numerical simulations demonstrate the practical efficiency of IMEDRL. Strengths and Weaknesses: Clear and wellwritten presentation Effective exposition of the complex theoretical framework Clear distinction between existing results and original contributions However the originality is limited due to its reliance on established theoretical and algorithmic concepts. The theoretical analysis provides some novel insights such as the concept of a skeleton. The practical significance lies in the ability of IMEDRL to match the theoretical lower bound while being practical and efficient. Questions: 1. How does the worstcase regret of IMEDRL compare to that of UCRL and PSRL 2. What practical advantages does IMEDRL have over other exploration algorithms Minor Corrections: Line 19: \"communicative\" should be \"communicating\" Definition 1: \"s1\" should be \"s\" or vice versa Line 92: Remove \"infamous\" from \"UCB\" Line 124: \"communication\" should be \"communicating\" Footnote 2: Add a closing square bracket Line 258: Replace \"numerical issues\" with \"empirical aspects\" Line 273: Add a space after \"PSRL\"", "Paraphrase: The paper analyzes MDPs (Markov Decision Processes) with an ergodic assumption and introduces a new index policy called IMEDRL. The IMEDRL index is adapted from the IMED policy for multiarmed bandits. The paper provides a regret bound for IMEDRL that matches the lower bound established for ergodic RL problems. Strengths: The IMEDRL index is a novel approach in RL differing from common optimismbased algorithms. The regret bounds for IMEDRL are established for ergodic MDPs matching both upper and lower bounds. Numerical experiments demonstrate the superior performance of IMEDRL compared to previous algorithms with regret guarantees. Weaknesses: The ergodic assumption is restrictive as most practical MDPs are not ergodic. The authors argue that focusing on ergodic MDPs is justified due to the existence of regret lower bounds for this class of MDPs. However regret lower bounds have also been established for the more general class of communicating MDPs. The papers notations are sometimes challenging to follow with definitions hidden within statements or lines. Questions: Can the IMEDRL index be applied to communicating MDPs and if so are weaker regret bounds possible Is there a connection between the IMEDRL index and optimistic value estimates in RL", "Summary This paper investigates how to minimize regret in averagereward Markov decision processes (MDPs) with an infinite horizon. It introduces a novel algorithm inspired by the Indexed Minimum Empirical Divergence (IMED) bandit algorithm and demonstrates that the algorithms regret matches the optimal regret bound for this problem. Strengths: The paper is wellwritten and clearly presented. The proposed algorithm is theoretically sound and empirically competitive. Weaknesses: Essential details are relegated to the Appendix (e.g. limitations of previous work) making the papers contributions less apparent upfront. Questions: 1. Asymptotic vs. Minimax Regret: How does the asymptotic problemdependent lower bound compare to the typical minimax lower bound Does the algorithm also guarantee finitetime regret bounded by \u221aT 2. Extension to Communicating Settings: What are the challenges in extending the algorithms analysis to communicating MDPs 3. Regret Definition Comparison: The regret definition used in this paper differs from the one in the UCRL algorithm. How do these two regret definitions relate 4. Extension to Linear Function Approximation: Can the algorithm be readily extended to incorporate linear function approximation"], "xWvI9z37Xd": ["Paraphrased Statement: Summary: This paper introduces a new method called WAST for automatically identifying important features (unsupervised feature selection). WAST is inspired by Quick Selection (QS) but uses a more efficient search technique by adding weights to the autoencoder based on their previous importance rather than randomly regenerating them like QS does. Strengths and Weaknesses: WAST performs well on various benchmarks often surpassing supervised baselines especially for selecting a large number of features. It is also more efficient than QS which is already an improvement over other similar methods. While the benchmarks evaluate feature selection based on accuracy retention supervised variable selection typically assesses whether it selects the correct features in artificial datasets with known true features. Although unsupervised feature selection has different objectives WASTs competitiveness with supervised methods raises questions about its ability to identify not just useful but also the most relevant features. Technical Advance and Questions: WASTs technical improvement over QS is relatively minor. However QS uses denoising autoencoders to encourage feature identification while WAST does not. This suggests that the approach may work effectively for different reasons. Moreover the competitiveness of WAST with supervised methods despite their varying objectives highlights the need to explore the relationship between unsupervised feature selection and feature ranking in supervised variable selection.", "Paraphrase: Summary: The study introduces an innovative unsupervised technique for feature selection using autoencoders enhanced by the attention mechanism. Inspired by the attention framework it incorporates the weightzeroing scheme of the autoencoder recursively leading to improved training efficiency and promising performance in various tasks. Strengths: Clear differentiation from existing approaches and explicit presentation of contributions Wellstructured presentation with accurate statements and references Demonstrated novelty and effectiveness through mathematical evidence Consideration of potential limitations and societal implications Weaknesses: Use of outdated benchmark datasets (suggesting TIMIT and YesNo for spoken digit recognition) Lack of comparison with handcrafted and taskspecific features (e.g. MFCC for spoken digit recognition) Absence of exploration of potential similarities and differences with DropConnect Questions: 1. How was the hyperparameter \\(\u03bb\\) in equation 3 selected to balance the two components 2. Is the output processed through a softmax function during network propagation 3. How does the proposed approach differ from DropConnect a widely used method", "Summary The authors enhance a previous autoencoderbased approach for identifying important features by inducing sparsity. They leverage the impact of input and output neurons on the reconstructed output to improve the \"drop and grow\" method. Across 10 datasets the updated approach significantly reduces computational costs compared to earlier methods. Strengths Clear and simple language Detailed experiments and coherent presentation Comparison to other conventional methods Opensource code availability Development Potential Incremental improvement over the original method with intuitive but theoretically unsupported adjustments. Declining prominence of explicit feature selection in the field may limit the impact of the study. Questions 1. How does the proposed approach compare to advanced methods in computer vision natural language processing and graph machine learning 2. Why are only input and output neurons subjected to the \"drop and grow\" process and not hidden neurons 3. While Section 5.1 notes improved classification accuracy with WAST it does not provide an explanation for the underlying reasons.", "Paraphrase: Summary: The authors propose an unsupervised feature selection method using Dynamic Sparse Training on an autoencoder with a single hidden layer. The number of features and sparsity level are hyperparameters. Despite using 10 times fewer training epochs than other methods (10 versus 100) the method achieves comparable accuracy to both supervised and unsupervised techniques. Strengths: Reduces training epochs by 10fold Evaluated on datasets from various domains (image speech time series text bio) Simple and easy to understand Uses a single loss function without additional regularization Outperforms CAE and MCFS on real datasets Weaknesses: Misses recent related work (e.g. DUFS) Claim of robustness in noisy environments and highdimensional data is unsupported by experimental evidence. Chosen datasets are not challenging (number of samples is larger or similar to the number of features) Supervised comparison methods are mostly classical nonneural networks and do not represent an upper accuracy bound. Number of target selected features is not given as a hyperparameter Experiments were only run 5 times per method and dataset which should be increased to 10 times. Questions: Can you provide a more detailed comparison to the QS method even though the improvement is marginal on most datasets Is the maximum accuracy for AEFS on FashionMNIST in Table 2 a typo Did you reimplement other methods or use them asis Please provide numeric support for your claim of consistency and stability. Why are some results missing from Table 2 Have you tested the effect of sparsity hyperparameter on performance (accuracysparsity tradeoff)"], "lkrnoLxX1Do": ["Paraphrased Summary Statement: The SelfIR method leverages the complementary characteristics of noisy shortexposure and blurry longexposure image pairs to perform selfsupervised image restoration. Paraphrase: SelfIR utilizes the contrasting properties of noisy and blurry image sets to guide its selfsupervised noise reduction process. Strengths and Weaknesses Strengths: Exploiting the complementary nature of blurry and noisy image pairs is a sound approach. Colearning for deblurring and denoising (Section 3.3) demonstrates some novelty. Impressive experimental results on synthetic datasets and comparable performance to other methods on realworld datasets. Weaknesses: Originality: Section 3.1 resembles Noise2Noise [15] and Section 3.3 shows similarities to Neighbor2Neighbor [9]. The paper needs to clarify how these sections differ from previous works. Quality: Dividing images into patches in Section 3.2 may be arbitrary and using SSIM for similarity measures lacks justification. Clarity: The colearning process in Section 3.3 needs more detailed explanation. Significance: While the paper emphasizes the potential of SelfIR for realworld scenarios most experiments focus on synthetic images. Questions 1. Is SelfIR primarily collaborative learning selfsupervised learning or both 2. How does Section 3.2s sharp patch detection contribute to Section 3.3s colearning 3. Is there any potential mismatch between the patchdividing strategy in Section 3.2 and the subsampling in Section 3.3 4. Does the method assume knowledge of the nonuniform blur kernel in Equation (1) 5. Are there visual comparisons of denoised realworld sRGB images", "Summary This study introduces a selfsupervised technique for enhancing blurred and noisy images. It leverages the idea that each restoration task can be addressed by utilizing the other degraded image. Thus the method combines the sharpness of the noisy image with the clarity of the blurred image. However the methodologys true selfsupervised nature is questionable as it appears to require recovering the original noisy image from the blurry and augmented noisy pair. Strengths and Weaknesses Strengths: The approach is innovative in terms of selfsupervised design utilizing observations about the nature of deblurring and denoising. The experimental results demonstrate promise assuming the concerns raised are resolved. Weaknesses: The authors should reference additional works on burst image denoising and deblurring beyond the limited selfsupervised image denoising papers cited. The categorization of the GoPro datasets blurrysharp image pair (IB and IN) as IB and IN pair (not I) is incorrect. The sharp image in GoPro is still IN as it undergoes a short exposure and noise is still present. The authors should clarify this or demonstrate that the method outperforms the original image (D(IB I)). It is unclear whether the noise in the noisy image is real or synthetic. The paper does not address how the blurry and real images can be effectively aligned especially when training with aligned data from a synthetic GoPro dataset. Typos \"without than accept a shoddy option\" should be \"without (sharp areas) than to accept a shoddy option.\" \"patches.For\" should be \"patches. For\" \"178 subsampling images\" should be \"subsampled images\" \"Hero 4 Black\" should be \"Hero4 Black\" Questions Does the method address the \"ghost artifact\" in blurry images by interpolating frames before averaging sharp images as suggested in the cited paper by S. Nah et al.", "Paraphrased Statement: This study tackles the challenge of capturing highquality images despite the tradeoff between blurry images caused by long exposures and noisy images caused by short exposures. Strengths: The novel concept of utilizing both blurry and noisy images to leverage their complementary information. The method effectively trains deblurring and denoising models using this complementary data leading to improved image restoration performance. Weaknesses: The method requires simultaneous capture of both long and shortexposure images using two sidebyside cameras which may be impractical for many applications. The realworld dataset used for evaluation is limited in size (61 images) raising concerns about its representativeness and the generalizability of the method. The optimization process involves hyperparameters that could impact the methods sensitivity to parameter selection. The study has not explored the possibility of incorporating intermediate exposure time images or alternative noise modeling approaches such as those employed in diffusion modeling."], "mTra5BIUyRV": ["Paraphrased Summary: The goal is to create a ranking of the top items that maximizes a specific measure while ensuring fairness across potentially overlapping sensitive groups. However the specific sensitive groups are not known with certainty and only the likelihood of an item belonging to a particular group is available. The paper proposes a novel fairness constraint that ensures that the ranking satisfies the upper bound limits for the number of items from each sensitive group in the top positions of the ranking with high probability. This constraint is then converted into a linear program to find the optimal ranking. The authors provide theoretical guarantees for the fairness and effectiveness of their approach. They also show that their fairness constraint is optimal in a certain sense. Strengths and Weaknesses: Originality: The expectation constraint proposed is new. Quality: The technical approach appears sound and the authors address various theoretical aspects and practical considerations. Clarity: Overall the paper is wellwritten and clear. However some specific sections could be reorganized or expanded for improved readability. Significance: The proposed work is relevant and significant due to the importance of ranking and the consideration of fairness in a general setting. The paper also points to potential avenues for improvement in future research. Questions: Why an upper bound on group occurrences In some cases a lower bound can provide equal representation for sensitive groups. The paper does not explicitly discuss the reasons for choosing an upper bound constraint. Unintended consequences: Its unclear if there are any potential negative consequences of emphasizing upper bounds rather than lower bounds for sensitive group representation.", "Summary This study examines a novel fair ranking problem where probabilities estimate group memberships for each item. The authors propose an approximation algorithm that approximately satisfies fairness conditions and maximizes utility. In experiments the approach outperforms baselines in fairness and balances utility and fairness effectively. Strengths and Weaknesses Strengths: Clearly written with robust theoretical components Strong empirical evaluation Original problem setup and algorithm application Weaknesses: Dense with details relegated to supplementary material Uncertain significance of improvements over prior work Lack of specific examples in the introduction Originality Introduces a novel setting for ranking with probabilistically known protected attributes Develops a unique algorithm and problem formulation Clarity Clear overall but could benefit from concrete examples in the introduction Some typos and omissions of details in the main text Quality Technically rigorous from a theoretical perspective Experiments primarily measure fairness using weighted risk difference other metrics could yield different results Significance Advances fair ranking theory by studying probabilistic group membership scenarios Inspires future research in this area Questions How do results differ with equal false discovery rates for all groups How does performance vary with different fairness metrics How do algorithms fairness and competitiveness change with different sample sizes How would results compare to other algorithms with a larger number of groups", "Summary This work introduces fairness constraints for ranking systems that ensure utility and fairness (measured by representation diversity) with high probability. These constraints are applicable to models with limited knowledge about sensitive attributes where each attribute has a probability distribution over groups and noise is independent for each attribute. The main theoretical result (Theorem 3.1) provides bounds on fairness violations and utility for solutions found using these constraints. The work also presents a practical relaxation of these constraints enabling the use of existing optimization methods. Empirical results on multiple datasets demonstrate the relationships between fairness utility and a key parameter (phi). Strengths Clear explanation of why previous approaches fail and why the proposed method is necessary. Informative appendix with a thoughtful discussion of group membership modeling. Weaknesses Limited empirical results: Some claims about the methods superiority at different noise levels are not supported by the data. The sensitivity of the method to noise levels is not fully explored. Unclear group membership model: The model for handling multiple sensitive features and overlapping group membership is not clear from the main body of the paper. Experiments only show results for disjoint group membership. Focus on equal representation: Empirical results are only shown for equal representation constraints. There is no discussion of the consequences of using different fairness constraints. Questions How does the choice of phi affect the solution and what does it represent Why is it notable that the method can achieve higher representation diversity for a given value of phi What is the computational cost of the simulation procedure and how noisy are individual iterations at different values of phi", "Paraphrase: Researchers introduce a framework for fair ranking while considering the potential for randomization in protected attributes. They incorporate group fairness requirements with probability information about perturbations in these attributes. This framework addresses multiple nondisjoint attributes and various fairness constraints including proportional and equal representation. Experimental results reveal that the algorithm generates rankings with enhanced fairness compared to benchmarks while maintaining a comparable or superior balance between fairness and utility. Strengths: Readable and wellpresented Offers a solution for fair ranking with multiple attributes and fairness constraints Proves theoretical soundness and provides empirical evaluation on diverse data sets Outperforms baseline methods in terms of fairness Weaknesses: Assumes strong and specific conditions limiting its applicability to a niche audience Requires citations for certain concepts (e.g. equal representation) Lacks comparison with related work addressing fair ranking with noisy protected attributes Additional Notes: Consider adding a period to line 22. Correct the grammar on lines 224 225 276 and 311. Provide citations for equal and proportional representation on lines 32 and 34. Investigate the related work mentioned in [1] and consider including it as a baseline. Clarify the meaning of \"bit complexity\" on line 267. Questions: Are \"socially salient\" and \"protected\" attributes mutually exclusive Which fairness constraints were used for [54] as a baseline How was constraint (6) on line 221 derived"], "oDoj_LKI3JZ": ["Paraphrase: This paper presents a solution to optimizing hyperparameters for sparse Gaussian process (GP) kernels. The approach uses Markov chain Monte Carlo (MCMC) to maintain a posterior distribution of hyperparameters considering uncertainty in their space. It incorporates techniques to improve computational efficiency such as separating the sampling of two variables and to handle highdimensional inputs using a \"doubly collapsed\" Evidence Lower Bound (ELBO). Experiments demonstrate its effectiveness compared to other methods. Strengths: The Bayesian sparse GP model can handle large datasets and has an efficient sampling scheme. The approach is theoretically sound and wellmotivated. The authors illustrate the problem of kernel hyperparameter optimization and compare their method to others. Weaknesses: More comparison methods could be included in the experiments. The choice of benchmark datasets should be better justified. Minor Issue: Equation 2 is incomplete as it omits the dependency on the variable `\u03b8` in the expressions for `p(yf)` and `p(f)`.", "Paraphrased Statement: Summary: This paper presents a novel extension of the variational sparse GP framework to eliminate the need for sampling inducing points. This is achieved by analyzing the optimal variational distribution of hyperparameters leading to a method for collapsing the inducing points out of the ELBO. The resulting ELBO is intractable but its gradient can be estimated by sampling hyperparameters from the optimal variational distribution using Hamiltonian Monte Carlo (HMC). This significantly improves sampling efficiency for highdimensional data. Empirical results on several benchmarks show promising performance. Strengths and Weaknesses: The paper is wellwritten but some details require clarification particularly Equation (14). While the mathematical derivations appear sound the final approximation in Equation (14) remains unclear. Minor errors in terminology (e.g. referring to an unlabeled equation) should be corrected. The proposed method is novel and addresses a critical challenge for inference with highdimensional data. However the empirical study is limited to lowdimensional datasets raising concerns about the methods applicability to more practical scenarios. Questions: 1. Explain the warm start strategy introduced in Algorithm 1. 2. Investigate if the choice of sampler implementations for SGPRHMC SVGP and HMC influences the observed runtimeRMSE differences. 3. Expand the empirical study to include more recent sparse GP methods to fully assess the relevance of the proposed method. 4. Provide a more comprehensive explanation of Equation (14) to address the confusion mentioned.", "Paraphrased Summary: This paper introduces a new approach for Gaussian Process (GP) inference that combines a sparse GP (utilizing inducing variables) with Bayesian hyperparameter inference. The method aims to reduce overfitting compared to traditional approaches that rely solely on maximizing the marginal likelihood. Experiments show improved performance over existing sparse GP models on both synthetic and realworld benchmark datasets. Strengths: Acknowledges the importance of considering hyperparameter uncertainty in GP inference. Proposes a practical method to incorporate this uncertainty enhancing the utility of GPs. Provides wellmotivated contributions citing relevant related work. Offers a selfcontained and logical presentation. Presents a technically sound method with a clear algorithm for implementation. Demonstrates promising empirical results on various tasks with reasonable computation times. Weaknesses: Compares to relatively old GP inference methods (SGPR and SVGP). It would be valuable to compare against more recent approaches or explore the compatibility of the proposed method with them.", "Paraphrased Statement: Summary: Introduction of a new inference method for sparse variational Gaussian process regression within a Bayesian framework. Method enables inference of hyperparameters without sampling inducing variables. Algorithm utilizes Titsias (2009) exact bound to simplify the full ELBO (including hyperparameters) resulting in increased efficiency. Evaluation: Assessment on synthetic and benchmark regression datasets demonstrates superior performance over existing sparse inducing point methods in terms of RMSE predictive loglikelihood and computation time. Inconsistent outperformance of basic SGPRMLII approaches is unclear. Strengths and Weaknesses: Novelty and Significance: Leverages new insights into the ELBO offering a valuable contribution to sparse GP regression methods. Improves understanding of GP inference in sparse conditions despite limited practical impact due to the prevalence of MLII approaches with comparable performance. Quality: Foundation on sound principles and wellexecuted algorithm. Presentation and discussion of experiments could be improved. Clarity and Presentation: Reclassification clarification is suggested (regressiononly focus). Enhance Table 2 with explanations references and distinction of the proposed approach. Provide equation references for Algorithm 1 elements. Clarify Figure 2 caption for better understanding. Guide interpretation of Figure 3 highlighting preferred SGPRHMC variant. Define \"delta\" in Table 4. Specific Questions and Comments: Section 5.2 omits comparison of SGPR (MLII) and SGPRHMC (Adapt 2) performance in Table 4 and Figure 4. Conditioning on X is introduced briefly and inconsistently throughout the paper. Inconsistencies in terminology (\"kernel\" vs. \"covariance function\"). Missing comma in Equation (1). Clarification of \"this\" in line 261. Use of contractions should be avoided."], "rrYWOpf_Vnf": ["Paraphrased Statement: This paper analyzes the optimization process for optimizationbased PDE (partial differential equation) solvers. It: 1. Unifies the objective functions of PINN and DRM for solving elliptic PDEs. 2. Establishes upper and lower error bounds for gradient descent in a Reproducing Kernel Hilbert Space using the unified objective function. Key findings: 1. The upper and lower bounds often coincide indicating that gradient descent can achieve optimal accuracy. 2. Gradient descent achieves statistical optimality faster when using higherorder derivatives (\"Sobolev Acceleration\"). Numerical experiments support these theoretical findings. Strengths: 1. Provides a theoretical explanation for Sobolev Acceleration. 2. Clear presentation and detailed experiments. Weaknesses: 1. Gradient descent is modified in this study. While this is acknowledged including results with the original gradient descent would enhance understanding. 2. Typos and symbol mismatches should be corrected. Questions: 1. Provide an intuitive explanation of Sobolev Acceleration. 2. Illustrate how the eigenvalues of differential operators change with order clarifying the statement about large eigenvalues for highorder operators.", "Summary: This paper investigates the statistical optimality of gradient descent in solving certain inverse problems. By representing the target function in a Reproducing Kernel Hilbert Space the authors demonstrate that gradient descent applied to a generalized objective function achieves statistical optimality. This implies that utilizing a Sobolev norm as the objective function during training implicitly accelerates the process in terms of training efficiency. Strengths: Explores the statistical limitations of solving PDEs using parameterized models. Proves that gradient descent with a general objective function achieves statistical optimality. Explains the advantage of PINN (PhysicsInformed Neural Networks) over DRM (Deep Residual Machines) in terms of implicit acceleration due to the Sobolev training objective. Weaknesses: 1. Importance of Gradient Descent: The authors claim that the use of gradient descent is crucial but do not fully explain why it is more significant than stochastic gradient descent which is commonly used in practice. 2. Notation Issue: Line 97 refers to \"f\" and \"f\" which is confusing as the goal is to recover \"f\". 3. Object \\mathcalL\\beta2: The authors use \\mathcalL\\beta2 without defining it given that \\mathcalL is defined in Line 111. 4. Sobolev Norm Definition: The definition in Definition 2.2 differs from the standard Sobolev norm. The authors should clarify how the standard norm relates to the one used. 5. Parameters: The parameters \\alpha \\beta p and q are essential in Theorem 3.2 but their meanings are not explained. 6. Dimension Dependence: The analysis does not consider the impact of the dimension \"d\" which is critical for PDE solvers. 7. Boundary Conditions: The paper assumes periodic boundary conditions (torus) but other practical conditions like Dirichlet and Neumann boundary conditions require consideration. 8. Accuracy in Figure 4(b): The accuracy shown in Figure 4(b) appears insufficient to demonstrate the successful recovery of the PDE solution. Questions: Why is the choice of gradient descent particularly important in this context What is the meaning of \\mathcalL\\beta2 How does the definition of the Sobolev norm in Definition 2.2 compare to the standard definition What do the parameters \\alpha \\beta p and q represent How does the analysis account for the dimension of the problem How do boundary conditions other than periodic boundaries affect the analysis Is the accuracy shown in Figure 4(b) adequate to substantiate the claim of PDE solution recovery", "Paraphrase: Summary This study analyzes the statistical limitations of the gradient descent algorithm for solving inverse problems with a specific focus on neural networks used as partial differential equation (PDE) solvers. The analysis establishes when the algorithm can achieve optimal convergence. It also explores the impact of early stopping on physicsinformed neural networks (PINNs) and deep residual machines (DRMs). Strengths and Weaknesses Exploring the convergence of deep learningbased numerical methods for PDE solving is crucial for their advancement. The proposed theorem provides insights into these methods and is supported by empirical observations on PINNs and DRMs. One limitation is that the analysis applies only to gradient descent. Extending it to accelerated gradient descent commonly used in deep network training would enhance its practical relevance.", "Paraphrase: This paper investigates the statistical optimality of a slightly modified gradient descent algorithm for solving elliptic equations with kernel functions. This problem is a generalization of kernel regression. The paper establishes two main theoretical results: 1. A minimax lower bound (Theorem 3) for the problem. 2. An analysis of the gradient descent algorithm on identically and independently distributed (i.i.d.) observations (Theorem 3.2). Combining these results the paper demonstrates the statistical optimality of the modified gradient descent algorithm. The key assumptions for this optimality include a critical capacity condition that is satisfied by ridge regression and certain toy examples of partial differential equation (PDE) solvers. The paper compares its results to existing analyses in the literature which focused on more restricted instances of the problem. Additionally it provides insights into the influence of loss function on the convergence of the gradient descent algorithm based on its theoretical analysis and assumptions. Strengths: Establishes a theoretical minmax lower bound for a generalization of kernel regression. Wellwritten and clearly presented. Assumptions are clearly stated and motivated by examples. Interesting to demonstrate the optimality of gradient descent for solving PDEs with kernels. Weaknesses: Significance of the result is unclear as it may be possible to translate results from kernel regression to this setting. Experimental comparison of the modified gradient descent algorithm with its unmodified counterpart would be valuable. Questions: Do the papers assumptions hold for neural tangent kernels as mentioned in their motivation Why is it not possible to apply existing kernel regression results to obtain the same conclusions How does the modified gradient descent algorithm in this paper differ from standard gradient descent on kernel regression Is there a representer theorem for the modified kernel regression Can you elaborate on its connections with early stopping techniques", "Paraphrased Statement: Summary: The authors investigate the accuracy limitations of using early stopping gradient descent to find the solution (u) to Poissons equation (\u0394u f). The measurement function (f) is estimated through noisy observations (y) at random points (x) with E[y x] f(x). The DRM and PINN methods can be used to solve Poissons equation by defining an objective function as an integral. The integral is approximated with empirical risk and then gradient descent is employed. The authors establish lower bounds for smoothness levels of f u and the operators (A1 and A2) defining the integral objective. They also demonstrate that early stopping gradient descent can achieve matching upper bounds in various scenarios and discuss instances of suboptimality. Strengths and Weaknesses: Strengths: Interesting results Innovative upper bound proof technique Standard yet effective lower bound proof Weaknesses: Mathematical complexity and density Potential for enhanced readability for machine learning students such as providing examples and clarifying concepts Questions: 1. Remove the negative sign in equation (2) for consistency. 2. Clarify the usage of A in line 97 considering that A\u0394A is not invertible. 3. Correct the interchange of f and u in lines 9192 and line 97. 4. Ensure consistency between the relationships of A A1 and A2 in lines 94 and 186 under Assumption 2.1(d). 5. Provide explicit gradient steps for DRM and PINN in simple terms potentially in a supplementary document."], "zGvRdBW06F5": ["Paraphrase: Main Idea: This paper focuses on deploying lifelong learning on resourceconstrained edge devices (e.g. IoT devices microcontrollers). The study introduces a design approach that combines algorithm and system codesign to fit the limited computational capacity of these devices. Strengths: The paper proposes a practical approach for deploying lifelong learning on edge devices with only 256KB of memory. The proposed methods (gradient calibration and sparse update) are simple yet effective in model adaptation. The framework meets the practical demands of edge environments and provides a valuable solution for machine learning analysis on devices with limited resources. Experimental results demonstrate promising performance in model size memory cost and accuracy. Weaknesses: 1. The efficacy of the proposed compression and learning strategies depends on the specific hardware architecture. It is unclear if the methods can generalize to other commodity devices beyond the specific STM series used in the experiments. 2. The experiments focus on classification tasks with limited epochs during transfer learning. The evaluation should consider the performance of segmentation and detection tasks as well. 3. Longterm (multiepoch) training is essential for lifelong learning on edge devices. Further investigation is needed to determine if the proposed method can handle training from scratch rather than relying on finetuning pretrained models. 4. Reporting the training and inference speed or time cost would provide insights into the practical feasibility of deploying the framework on edge devices.", "Paraphrase: This research presents a new framework for training models on devices with limited 256KB memory. It introduces QuantizationAware Scaling (QAS) to enhance the stability of quantized training with mixedbit precision. Additionally it uses a sparse update technique to reduce memory consumption. The framework includes a Tiny Training Engine (TTE) designed for implementation on MCU systems. Strengths: Clear and wellwritten presentation Comprehensive hardwaresoftware design for MCU training Ample experimental evidence supporting the proposed methods Weaknesses: Slow inference speed on MCU devices raises questions about the practicality of ondevice training Lack of justification for the necessity of ondevice training over clouddevice model updates Potential limitations on neural network structure for ondevice training Questions: 1. Provide training time comparisons on MCU devices for different models and datasets. 2. Explain the benefits and use cases of ondevice training compared to clouddevice parameter updates. 3. Clarify whether the framework supports all neural network structures or if it has specific requirements.", "Summary This research proposes a systemalgorithm codesign approach for transfer learning on tiny microcontrollers with limited memory (less than 256KB). The focus is on optimizing quantizationaware training gradient sparsification and compiler optimizations using Tiny Training Engine. Positive and negative aspects are highlighted below: Strengths Ondevice training is challenging and transfer learning for tiny devices is of interest. Significant contributions to optimizing quantized networks and training support compilers for tiny devices. Demonstrated memory savings compared to conventional deep learning frameworks. Weaknesses The authors discount existing trainingtime adaptation techniques (e.g. updating only normalization layers) without considering the fusion of these layers into convolution weights during deployment. This raises questions about the effectiveness of training only normalization layers and its impact on the proposed quantization scaling method. The improvements to quantizationaware training optimization are presented as general but it is unclear if they apply to nontransfer learning scenarios. The research lacks comparison with other gradient sparsification techniques. The code for Tiny Training Engine is not open source. PostRebuttal Update The authors have addressed concerns with additional experiments and clarifications leading to an increased rating. The authors are encouraged to include a simple comparison between standard quantizationaware training and quantization scaling on real quantized graphs in the final version.", "Paraphrase: Summary: This research introduces a method for reducing memory usage during ondevice training by using quantizationaware scaling. This system simplifies the backward computation graph using a Tiny Training Engine allowing for sparse updates. The effectiveness of this method is shown in the Visual wakeword experiment. Strengths and Weaknesses: Strengths: Clearly written with detailed explanations and practical techniques for ondevice training. Demonstrates a working system on a microcontroller. Combines various techniques (QAS DCE inplace update) effectively. Weaknesses: Does not cite relevant prior work (e.g. LSQEsser et al. 2019). Ignores alternative methods like HFP8 (Sun et al. 2020) for comparison. Questions: Evaluation is based on a limited dataset broader testing is needed. Ablation study does not evaluate the impact of QAS against a baseline without gradient scale."], "vF3WefcoePW": ["Paraphrased Statement: Summary: This research presents a novel approach to lowmemory highspeed neural networks known as logic gate networks. Each output neuron connects to only two input neurons and training focuses on learning one of sixteen possible Boolean functions based on the binary inputs. Training utilizes realvalued functions of logic functions and averages across all 16 possibilities during testing. Inference is highly efficient on CPUs because it involves executing logic gates. Results show that logic gate networks perform similarly to other methods on small datasets. Strengths: Introduces a novel concept. Aims to achieve ultrafast neural network inference using logic gates. Draws inspiration from sparse neural networks and binary networks. Results are wellsupported. Writing is generally clear and concise. Significance: Logic gate networks show potential for efficient and fast inference. Results are not groundbreaking but demonstrate a promising research direction. Weaknesses: Writing quality could be improved. Network architecture is not explained in detail. Architectural and training design choices lack justification. Paper includes apologetic language about potential improvements. Comparison to baseline architectures is not sufficiently explained. Lack of information on training time. Questions and Suggestions: Line 195: It should read \"increasing n by a factor of 10 tau should be decreased...\" Section 4.2: The paragraph on output neuron aggregation is unclear. Consider removing the statement about developing from scratch. Discuss future explorations of convolutions and other connectivity patterns. Quantify the higher training cost for logic gate networks. Explain potential improvements to reduce the practical cost of training. Revise the sentence about evaluating cost to account for additional factors beyond binary operations.", "Paraphrase: Summary The authors introduce the Logic Gate Network (LGN) a neural network where neurons are represented by binary logic gates. The network employs 16 logic operations and during training neuron inputs are relaxed to probabilities for differentiability. Each neuron has a 16dimensional weight vector that when converted to a categorical distribution is used to compute the activation as a weighted sum of logic operations. At inference time the network is binarized for faster computation. The LGN was tested on classification tasks including the MONK Adult breast cancer dataset MNIST and CIFAR10. Compared to fully connected networks the LGN accelerated inference by 12 orders of magnitude and reduced memory consumption significantly while sacrificing some accuracy. Strengths and Weaknesses Strengths: The LGN enables binarized logic networks to perform well in classification tasks while reducing memory footprint and accelerating inference. Practical considerations and tricks shared by the authors enhance network performance and appeal to edge device deployment of neural networks. Weaknesses: The relationship between the LGN and deep learning literature could be better clarified. It is unclear if the LGN can achieve similar functionality as other networks or occupy a unique niche for efficient inference with reduced accuracy. Questions: Whether the accuracy of the binarized network would be higher than the \"soft\" network during training. If more than 16 logic functions can be incorporated and if it would enhance performance. Why grayscale resolution and binary embedding were used for CIFAR10 classification. What the significance of the sparse neural network rows in Tables 4 and 5 is given that they lack inference times and provide sparsity metrics not applicable to the LGN.", "Summary This paper presents differentiable logic gate networks where logical operations are parameterized using logic gate neurons offering 16 binary logic operations. Learning is achieved through soft logic operations and attention parameters for the operations. Evaluation on standard classification tasks suggests faster inference than other methods. Strengths and Weaknesses Weaknesses: Lack of Originality: The use of soft logic and attention mechanisms for differentiable logic learning is not novel. Incomplete Literature Review: The paper fails to adequately connect to prior works in the field. Confusing and Unconvincing Comparisons: Comparisons with existing methods are often unconvincing or missing. Potential Design Flaws: Neuron inputs may be limited excluding certain logical combinations. Output may be positiondependent affecting translational invariance. Poor Writing: Confusing statements grammatical errors and informal presentation. Significance: The proposed network demonstrates fast inference speeds. However it has limitations in training time and may oversimplify layer connections. Questions: Motivation for the work. Benefits of using the network compared to other neuralsymbolic models. Layer connectivity design and its justification. Empirical analysis of network behavior (e.g. feature space shifts classification grouping). Recommendations: Rewrite the paper to address concerns. Redesign experiments to include training time and analyze network behavior.", "Paraphrased Statement: Summary: This research introduces deep logic networks capable of training with backpropagation and binary value inference. These networks consist of logical gates organized into layers with random connections between gates. An output layer aggregates gate outputs for optimization using crossentropy or squared error loss. The types of gates are model parameters optimized by Adam. Experimental results demonstrate that these networks perform well on classification tasks including MONK Adult Census Breast Cancer MNIST and CIFAR10. Strengths and Weaknesses: Strengths: Novel concept of Differentiable Logic Gate Networks Model learns gate types rather than weight matrix connections Random connections between gates enhance scalability Results demonstrate improved accuracy reduced operations and faster inference compared to prior models Weaknesses: Requires further exploration of limitations (e.g. architectural robustness highdimensional inputs random connection variance) Questions: 1. Line 195: \"tau\" should be \"n\" 2. Line 334: PTX is unclear 3. Table 5 lacks training time information for Diff Logic Nets"], "nE8IJLT7nW-": ["Summary: This paper introduces a novel \"Biologically Inspired Transformer\" architecture that incorporates a modified attention mechanism. This mechanism allows models to divide the visual field into distinct peripheral regions enabling them to prioritize essential information and enhance training efficiency. The proposed architecture demonstrates improved performance in classification tasks compared to a baseline model. Strengths: Innovative attention mechanism that facilitates visual field partitioning which can improve model focus and training efficiency. Wellwritten and motivated presentation of the paper with clear explanations and illustrative figures. Comprehensive ablation study that examines various design decisions and highlights the significant contributions of different elements. Weaknesses: Limited evaluation dataset: The models performance is only assessed on ImageNet leaving room for uncertainty about its effectiveness on other datasets. Inferior performance to FocalS: The proposed model exhibits lower performance than FocalS with comparable computational and model size metrics raising concerns about its superiority in the transformer field. Unsuitable evaluation dataset: ImageNet may not be an ideal dataset to demonstrate the models strengths as it typically contains a single object in the image potentially obscuring the models ability to focus on multiple objects efficiently. Lack of segmentation evaluation: Given the models claimed ability to generate segmentation masks (Figure 6) it would be valuable to evaluate the quality of these masks on segmentation datasets. Questions: Have the authors considered evaluating their model on additional datasets and tasks to provide a more comprehensive assessment How do the authors address the concerns raised by the inferior performance of their model compared to FocalS Have the authors conducted evaluations on segmentation datasets to assess the quality of the segmentation masks generated by their model", "Summary This study presents a new visual transformer model that leverages peripheral vision principles in its positional encoding. The model employs a multihead peripheral attention (MPA) module where contentbased and positionbased attention are calculated independently and combined. The positionbased attention design ensures that pairs of locations with equal distances have similar attention scores. Peripheral projection enhances diversity while peripheral initialization allows for larger spatial receptive fields. The model outperforms similar models on the ImageNet classification task primarily due to the positionbased peripheral attention in the MPA. Strengths Concisely incorporates peripheral vision into transformerbased computer vision models. Straightforward definition of relative positional encoding aligned with human visual system. Performance evaluation and ablation studies demonstrate the importance of peripheral attention. Weaknesses ImageNet classification may not be the ideal task to showcase the benefits of peripheral mechanisms. Room for improvement in presentation clarity. Questions Major Clarify the peripheral projection concept and its role in attention computation. Define the \\mathcalN(\\cdot) function in equation 7. Explain the vertical discontinuity in attention patterns after peripheral projections. Discuss why multilayer design is not essential for performance improvement. Minor Table references should match their appearance order. Consider exploring how peripheral vision influences model behavior like in humans. Include references to previous biologicallyinspired peripheral vision models using CNNs.", "Paraphrase: This study draws inspiration from the human visual systems foveated nature where objects are processed with varying levels of resolution and information compression depending on their distance from the central point (fovea). Recent vision transformers (ViTs) employ selfattention mechanisms which require significant data to learn effective attention patterns across different layers. To address this data inefficiency this paper introduces a Multihead Peripheral Attention (MPA) mechanism within a standard DataEfficient Image Transformer (DeIT). Within MPA learned positionbased attention maps are combined with contentbased attention maps to create a \"Multihead Peripheral Attention\" map. Experiments using the ImageNet dataset show that the Peripheral ViT (PerViT) model incorporating MPA outperforms standard ViTs in classification tasks achieving performance comparable to recent pyramidal architectures. The paper acknowledges that while MPA draws inspiration from peripheral vision it should not be overstated as a direct adaptation. Further research is needed to explore the connection between peripheral vision and the proposed model. Additional experiments are suggested to evaluate the models training efficiency contribution of specific components and performance on diverse datasets.", "Paraphrased Summary: This research introduces a transformer model that mimics human peripheral vision. The model enhances multihead attention with peripheral attention comprising content attention (similar to scaled dot product attention) and position attention. Position attention is learned through a neural network utilizing fixed relative position embeddings. The learned positionbased attention divides the image into distinct regions resembling peripheral vision. Quantitative and qualitative evaluations demonstrate the effectiveness of this approach. Strengths and Weaknesses: Strengths: Proposes a novel transformer architecture inspired by human vision. Extensive qualitative results aid in comprehending the approach. Weaknesses: Motivation for using neural networks to learn positionbased attention is unclear. Potential difficulty in isolating the performance benefits of peripheral encodings. Gains are relatively small which could potentially be achieved through simpler engineering adjustments. A more robust approach would involve using an optimally trained ViT model as a baseline. Questions: How is the neighbor function formulated and how does it affect positionbased attention learning A baseline with fixed position embeddings varying by layer could highlight the advantages of learned position embeddings. Did the authors explore this Is it feasible to apply the peripheral positional encoding to the ViT implementation described in [A]"], "mhP6mHgrg1c": ["Summary: ORIENT is a novel subset selection framework that enhances Supervised Domain Adaptation (SDA) learning efficiency. It uses Submodular Mutual Information (SMI) functions to select a small and diverse subset of training data from the source domain that closely resembles data in the target domain. ORIENT generalizes existing SDA methods like GLISTER GRADMATCH and a CRAIG variant by incorporating different SMI functions. Experiments indicate that ORIENT significantly reduces learning time while maintaining accuracy comparable to baseline methods. Strengths and Weaknesses: Strengths: Novelty: ORIENT unifies several known SDA methods under SMI functions making it a unique approach. Quality: The presented results are robust and convincing. Clarity: Writing is generally clear despite minor typos. Significance: ORIENT provides valuable contributions: Unifying various SDA methods as special cases of ORIENT. Applicability to various machine learning models including neural networks with minimal adaptation. Demonstrated substantial learning time reduction (up to 23x) in experiments. Weaknesses: Questions: In Algorithm 1 line 4 \"\u03b8 is updated as \u03b8 \u204c arg min \u2112(A).\" This suggests a single or a few stochastic gradient steps. However without changing A it could result in the same \u03b8 for L iterations.", "Paraphrase: Summary: The authors propose using submodular mutual information measures to select datasets for domain adaptation. Their algorithm starts with a random subset iterating between training on the selected data and selecting new data based on submodularity and gradient similarity. The algorithm is claimed to be more efficient than training on the full dataset. When combined with dSNE ORIENT it outperforms fulldata training on three out of 12 adaptation directions and underperforms on three others. Strengths and Weaknesses: Strengths: Uses wellestablished techniques (submodular mutual information and optimization). Provides approximation guarantees. General framework allows for future research on new similarity measures and mutual information metrics. Weaknesses: Paper is abstract and superficial lacking technical details on the proposed submodular mutual information measures. Experimental results are inconclusive with fulldata selection often being competitive with the proposed algorithm. Questions: How is speedup computed in Figure 4 Do the authors fix the number of passes on the dataset when computing speedup What are the underlying ideas behind the proposed submodular mutual information measures their pros and cons and how do the experimental results support these assumptions How should the selected dataset size be determined Could more frequent or less frequent dataset selection improve performance Why dont the authors compare against other dataset selection baselines The authors claim 3x speedup but could dataset selection on full data without domain alignment achieve similar results", "Paraphrase: Summary: For Supervised Domain Adaptation (SDA) a framework is proposed for selecting a targeted subset from source domain data to enhance training speed and improve performance. Strengths: The approach addresses a unique and practical problem. It prioritizes data utilization over complex model building offering potential for performance gains. The frameworks effectiveness is supported by extensive experiments. The presentation is clear and wellexecuted. Weaknesses: The method for calculating the similarity matrix is relatively basic. Exploring more sophisticated or learned similarity measures could improve the framework. Questions: Can this framework be extended to Unsupervised Domain Adaptation (UDA)", "Paraphrased Statement: Summary: To enhance the speed of existing Source Domain Adaptation (SDA) techniques a subset of the source dataset is selected that closely resembles the target dataset. Model training is then carried out using only this subset reducing computational efforts. Additionally as the subset is more relevant to the target data SDA is explicitly implemented. The similarity of the subsets is assessed using a Similarity Measure Index (SMI) where pairwise data similarity is calculated as the cosine similarity of the loss gradient with respect to model parameters. The proposed algorithm alternates between loss optimization (training) and optimal subset selection based on SMI. Simulations on benchmarks demonstrate that computational efficiency is achieved without significantly compromising accuracy. Strengths and Weaknesses: While the strategy of limiting training to a subset appears advantageous it has limitations: 1. Valuable information contained in the unselected subset is neglected. 2. The contribution of the unselected subset may extend beyond similarity to the target set. Furthermore since SDA is explicitly performed it is essential to compare the proposed methodology against advanced SDA approaches particularly scalable ones. However the simulations do not include these comparisons and instead use alternative subset methods as baselines. Without such comparisons the practical utility of the method remains unestablished. Rebuttal: Some concerns have been addressed. Questions: 1. How does the initial selection of the subset affect the final accuracy Can this be analyzed through simulations 2. As subset initialization is random reporting mean and standard deviation across multiple initializations would provide a more accurate representation of the results."], "oWqWiazEb62": ["Paraphrased Summary This paper introduces a technique called distributionally robust optimization (DRO) that can handle machine learning problems with constraints expressed as expected values. Instead of estimating the variance of constraint violations the proposed DRO uses an empirical version of the constraints to create margins. This ensures that the solution meets the expected value constraints exactly. The proposed method works with standard stochastic optimization in its dual form and has been shown to be effective for classification tasks that require fairness. Strengths Provides a thorough analysis of exact constraint satisfaction in ML fairness. Introduces a more convex and computationally efficient DRO formulation that does not require variance estimation. Includes practical adaptations to make the method feasible for realworld applications. Clearly presents the method and its advantages. Weaknesses Currently applies only to singleconstraint problems expressed as an expectation. Exactness may be affected by plugin estimates used in handling multiple constraints. Questions Further explanation of the dual form of DRO in Section 2.1. Discussion on the relationship between the chisquare divergence and the \"pivotalness\" of the solution.", "Paraphrased Statement: Summary: This study addresses the issue of optimizing stochastic functions under constraints that depend on data specifically in the form of E[g(\u03b8Z)] \u2264 0. To ensure that these constraints hold for the entire population distribution not just the sample this paper proposes a distributionally robust variant of the problem. This variant minimizes the empirical loss subject to a constraint that EP[g(\u03b8Z)] \u2264 0 for all probability distributions P sufficiently close to the empirical distribution based on the phidivergence measure. Key Findings: This approach ensures calibrated constraint satisfaction asymptotically meaning that as the sample size increases the constraints are met with a probability exactly equal to 1 minus the userspecified \u03b1. The paper provides examples and techniques for solving the associated optimization problem. Strengths: Clear and wellstructured writing. Introducing constraints in this way is practically valuable. Experimental results suggest that the theoretical findings hold even for moderate sample sizes. Weaknesses: The paper lacks discussion on potential computational challenges in solving the optimization problem which is usually nonconvex. Limited exploration of the impact of approximately solving (1.3) on constraint satisfaction. Questions: Why do the experimental results consistently approach the asymptotic limit from below suggesting an overpermissive distributionally robust modification Are there adjustments to the method to improve constraint calibration for smaller sample sizes", "Summary: This research tackles the challenge of ensuring that machine learning models conform to exact constraints particularly when those constraints represent ethical concerns such as fairness. The authors introduce a novel approach grounded in Distributionally Robust Optimization theoretically proving that models trained with this approach will likely adhere to constraints across multiple data distributions. Strengths: Clear and accessible writing Addresses a significant issue in machine learning Strong theoretical basis Weaknesses: The proposed algorithm lacks guarantees on solution quality. The theoretical analysis doesnt fully align with the motivating example of group fairness constraints. The experimental evaluation is limited in terms of dataset diversity and baseline comparisons. Overall Assessment: Despite these shortcomings this paper contributes to the field of machine learning by introducing a theoretically sound approach to constraint satisfaction. It raises important questions regarding the gap between theory and practical application particularly in the context of fairness constraints. With further refinements this work could significantly advance the development of ethical and reliable machine learning systems."], "lXUp6skJ7r": ["Paraphrase: This paper introduces AdvStyle a novel data augmentation technique for enhancing the generalization capabilities of semantic segmentation models. By leveraging adversarial style features AdvStyle creates challenging stylized images during training which helps prevent overfitting on the source domain. The effectiveness of AdvStyle is demonstrated through experiments on two benchmark datasets. Strengths: AdvStyle significantly improves the performance of baseline models on unseen domains including both segmentation and classification tasks. Weaknesses: 1. AdvStyle is designed to enhance the appearance diversity of training images but it cannot guarantee performance on specific unseen domains lacking interpretability. It provides limited improvement in generalization for lighting changes. 2. The comparison with SOTA classification baselines like MEADA and L2D is missing leaving doubt about whether the performance gain is solely due to the baseline performance. 3. The superiority of AdvStyle over current SOTA augmentation methods is not clearly demonstrated. More comparisons should be provided especially in the main paper. Questions: 1. The method employs a gradientbased approach for updating the style feature similar to adversarial sample generation. It is unclear how the parameter \u03b3 in Eq. (3) is controlled. 2. Why does the method use separate learning rates for \u03bc and \u03c3", "Paraphrased Statement: Summary: This study addresses the challenge of testing models trained in one domain on a different domain where there is a shift in characteristics. It presents a method that enhances testing performance on real unknown data when training on synthetic data for semantic segmentation in autonomous driving. The key insight is that in urban environments for autonomous driving a consistent domain shift occurs in the mean and variance of image data across different datasets and acquisition conditions. The authors propose targeting these changes during training to improve generalization performance. The method consists of two phases: 1. Modifying the mean and variance of input images to create adversarial samples. 2. Updating model weights using both adversarial and unperturbed samples. The method is evaluated on synthetic versus real datasets for semantic segmentation in autonomous driving demonstrating significant improvements over established baselines. The proposed method is particularly effective because the perturbation space is lowdimensional (6vector). Strengths and Weaknesses: The method is wellexplained and easy to implement. The datasets used are relevant and realistic. The authors have a clear motivation for their approach and demonstrate its effectiveness. Concerns and Questions: However there are concerns that the method may be more accurately characterized as image augmentation (IA) rather than domain generalization (DG). This has implications for the choice of baselines and subsequent analysis. If considered as DG the methods effectiveness is unclear for other DG tasks that do not rely heavily on the image formation process. If considered as IA the method may not be compared fairly with stateoftheart IA techniques. The authors are encouraged to provide their thoughts on these concerns and to consider expanding their method to incorporate other explicitly modeled image formation processes.", "Paraphrase of Summary: This study aims to overcome difficulties in semantic segmentation due to varying domains. A straightforward and efficient technique involving adversarial augmentation is proposed. This technique modifies global imagelevel statistics. By leveraging adversarial learning the adversarial style features appear to effectively learn attributes of different datasets leading to enhanced performance. Strengths: AdvStyles concept is straightforward and readily applicable. Figure 1 clearly demonstrates the impact of style changes on segmentation performance including the significant decline in mIoU when only scene colors are modified. Figure 5s tSNE visualization supports the AdvStyle theory showing that adversarial style features capture some attributes of other datasets. Extensive testing includes comparisons with common data augmentation techniques leading DG methods and additional experiments on classification tasks. Weaknesses: Figure 1 states that imagelevel mean and variance are used as style features. However its unclear how these features are computed and applied. Line 70 mentions \"6dim feature for each example.\" Therefore it is uncertain if the style features are simply the mean and variance of RGB channels. If so why not explore other color or texture spaces This would be a valuable ablation study missing from the paper. Questions: Do target domain data entirely avoid usage during training If so Table 1 results are based solely on training on GTAV data and applying it to realworld validation data. Given the global application of channelwise augmentation could local augmentation further enhance performance"], "xpR25Tsem9C": ["Paraphrased Statement The authors have addressed my concerns raised in the rebuttal. The addition of new experiments in the revised manuscript has strengthened the papers contribution to the field. Based on these improvements I believe the paper is now suitable for acceptance to the conference. I have adjusted my review score accordingly. Strengths: The paper presents a novel approach for imputing heterogeneous data with missing values addressing a common realworld challenge. Empirical evaluations demonstrate the models superiority over existing methods across various datasets and tasks. The work is wellwritten and accessible with the authors providing code and guidance for users. Weaknesses: Despite the focus on simplifying the implementation the complexity of the model may hinder its broader adoption within the Neurips community. The use of HMC poses challenges in tuning and ensuring reliable mixing. The paper should provide more detailed guidance on these practical considerations including overcoming pitfalls diagnosing mixing issues and evaluating the robustness of the method to hyperparameter choices. The training time of the HMCbased model is significantly higher than competing methods. While the improved performance may justify the overhead in practical applications its important to ensure fair comparisons with baseline nonHMC methods. Consider exploring higher sample sizes for VAEs and utilizing alternative sampling techniques such as IWAE. Questions: Address the weaknesses highlighted in the \"Weaknesses\" section. Provide more detailed guidance on the practical aspects of using HMC in the model.", "Summary HHVAEM is a novel approach for learning and acquiring missing features using various data types. It combines the flexibility of hierarchical latent space models with the precision of Hamiltonian Monte Carlo (HMC) inference. HHVAEM extends VAEM by replacing its second stage with a hierarchical generative model trained using HMC which incorporates minor but effective enhancements. The original aspects of HHVAEM include: Integrating VAEMs hierarchical model with HMC Employing a specific hierarchical model known as the reparametrized autoregressive latent space model Numerical experiments demonstrate HHVAEMs significant performance improvements. Strengths and Weaknesses Strengths: Quantifiable and consistent performance improvements in experiments Weaknesses: Insufficient discussion and validation of design choices Performance improvement may be expected when combining richer models and more accurate inference methods Suggested Improvements: Organize baseline methods for ablation study Discuss performance differences in terms of HHVAEMs design choices Provide more experimental details Fix notational inconsistencies Clarify the difficulty in combining hierarchical models and HMC Explain the implementation or justification of higherorder derivative computations Questions: What is the nontrivial aspect of combining hierarchical models and HMC How are higherorder derivatives computed in HMC optimization", "Paraphrased Statement: This study utilizes a Hierarchical Variational Autoencoder (HVAE) with Hamiltonian Monte Carlo (HMC) to impute missing heterogeneous data. The HVAE enables users to model hierarchical structures and nonGaussian distributions potentially improving likelihood modeling and data generation. Strengths: Innovative concept relevant to existing research Sound and appropriate approach for the given problem Clear and wellwritten manuscript with relevant references Weaknesses: Inconclusive results due to limited dataset use Evaluation does not clearly demonstrate the benefits of imputing data in hierarchical models Lack of experiments focusing on highfidelity generation despite the motivation of the paper Questions and Suggestions: Conduct experiments emphasizing highfidelity generation Consider expanding the dataset repertoire to enhance the significance of the results If additional datasets cannot be obtained the paper may require revision to better align with the original motivation of hierarchical VAE usage", "Summary: The study combines hierarchical Variational Autoencoders (VAEs) with Markov Chain Monte Carlo within Variational Inference (MCMCwithinVI). The researchers emphasize the importance of the hierarchical structure of VAEs in addressing missing data issues. Strengths: Innovative combination of hierarchical VAEs and MCMCwithinVI enhancing flexibility and inference quality. Introduction of a new objective that optimizes two distinct Evidence Lower Bounds (ELBOs) of the model. Weaknesses: Reliance on assumptions that are not explicitly stated. Lack of clarity regarding the scope of the study. Questions: 1. Zero imputation may not always yield lower bounds on observed data likelihoods depending on missing data assumptions. What assumptions are made in the study What modifications are needed if missing data is conditional on other dimensions 2. Provide a clear equation or sampling statement for the prediction model particularly for mapping from marginal zs to xhatu. 3. Explain how it was confirmed that the reparameterized version relaxes dependencies and maintains the standard HMC method for posterior sampling. 4. Report heldout test NLLs for observed features in addition to missing features and target y considering missingness assumptions and differences in modeling tasks. 5. How are target prediction NLLs handled for data with missing y 6. Consider revising the title and text to reflect the focus on missingness and its assumptions. 7. Avoid ending subsections with equations and include concluding remarks. Recommendation: Weak accept. The study presents promising results but the discussion of assumptions and metrics requires clarification. Addressing the questions raised can improve the clarity and impact of the work.", "Summary The authors introduce a novel generative modeling approach for incomplete and heterogeneous datasets. To achieve this they propose a Hierarchical Variational Autoencoder (HVAE) architecture which separates the \"noise\" variables from the model parameters. By training the model using a combination of variational inference and Hamiltonian Monte Carlo (HMC) they focus on imputation and active learning as key applications. Their approach outperforms other VAEbased models for heterogeneous data across various tasks including imputation missing value prediction active learning and anomaly detection. Strengths Introduces a \"predictive enhancement\" concept integrating generative and discriminative ideas. Proposes a unique HVAE architecture that directly models the noise variables simplifying inference. Develops a novel HMC approach for VAE contexts optimizing the likelihood lower bound. Conducts extensive experiments demonstrating the effectiveness of their approach. Weaknesses Insufficient explanation of the motivations and novelty of individual contributions. Lack of justification for the HVAE architecture change and the ELBO modification. Unclear description of the mutual information estimate in the active learning application. Limited use of evaluation metrics in the imputation experiments potentially leading to biased results. Additional References Ghahramani Z. Jordan M. I. (1995). Learning from incomplete data. MIT Center for Biological and Computational Learning Technical Report 108. Joy K. A. Albrecht S. V. Amin S. Whitaker R. T. (2021). Capturing Label Characteristics in VAEs. ICLR. Li Z. Pan S. J. Jin L. (2019). Are Generative Classifiers More Robust to Adversarial Attacks ICML. Smieja M. Burbidge C. (2018). Processing of missing data by neural networks. NeurIPS. Ipsen A. Sporring J. Larsen L. K. (2022). How to Deal with Missing Data in Supervised Deep Learning. ICLR. Stekhoven D. J. B\u00fchlmann P. (2012). MissForest nonparametric missing value imputation for mixedtype data. Bioinformatics 28(1) 112118."], "n4wnZAdBavx": ["Summary The authors present MASIA a method for improving learning and communication in multiagent reinforcement learning (MARL). MASIA constructs a shared representation that distills information from multiple agents. This representation allows agents to access relevant information without transmitting extensive messages. To encourage informative representation the authors introduce two loss functions: a decoding loss for reconstructing the full state and a predictive loss for anticipating future states. In various MARL environments MASIA surpasses existing methods in terms of training speed and stability. Analysis reveals interpretable communication vectors and clustering among agents. Strengths MASIA effectively addresses the complex problem of developing stable emergent communication. The method combines several novel concepts including autoencoding and rollout objectives resulting in improved agent performance. The comprehensive evaluation using diverse MARL domains and robust statistics enhances the credibility of the findings. Weaknesses Literature Review: The authors overlook relevant works on emergent communication biases and Lin et al.s autoencoderbased communication method. Compactness Claims: The authors assertions about compactness need clarification. While the autoencoding loss incentivizes informative representations it doesnt guarantee compactness. Ablation Studies: A more comprehensive ablation study is necessary to fully understand the contributions of different MASIA components particularly the autoencoding loss. Clarity Issues: Certain aspects of MASIAs design such as the information available to agents during decentralized execution need further explanation. Minor typos and errors should be corrected before publication. Questions for Authors 1. Information Access: How do agents access information during decentralized execution compared to centralized training What inputs feed into the information aggregation encoder at test time Is zt used then 2. Aggregation Encoder Inputs: What are the inputs to the Information Aggregation Encoder Does it include communication or is communication solely generated as zt from historical observations", "Paraphrased Statement: Summary The authors have developed MASIA a technique designed to promote effective communication in cooperative multiagent reinforcement learning (MARL) environments. MASIA employs an information aggregator and extractor which learns compact and relevant message representations for agent communication. These representations leverage selfattention mechanisms and predictive representation utilizing an additional model network for the latter. MASIA consistently outperforms existing MARL baselines in tasks where communication plays a critical role. Ablation studies demonstrate the effectiveness of MASIAs representations and its ease of integration with standard MARL algorithms. Strengths and Weaknesses Strengths Wellarticulated intuitions and approaches Extensive experimentation demonstrating algorithm effectiveness Compatibility with existing MARL algorithms Weaknesses Scalability concerns for large agent populations (tens or hundreds) similar to other CTDE frameworks Questions 1. The use of the unobservable state \"s\" in the reconstruction loss for information aggregation requires clarification. It appears that \"s\" represents the set of observations made by all agents under the CTDE framework. 2. The impact of each loss component on MASIAs performance is unclear. The appendix suggests weighting losses using parameters \u03bb1 and \u03bb2 but their specific effects on the algorithm are not discussed.", "Paraphrase: Summary: This paper explores multiagent reinforcement learning (MARL) where agents share information through communication at every step. The authors develop a centralized training scheme where all agents observations are gathered and processed. They also propose a mechanism that helps each agent focus on only the relevant parts of the global information. This mechanism can be added to existing MARL methods. The paper outlines the main concepts and provides experimental results and comparisons. Strengths and Weaknesses: Strengths: The idea of using a focus of attention mechanism during centralized training is innovative. It can potentially enhance interagent communication. Weaknesses: The papers presentation is confusing and difficult to follow. The algorithm is not clearly defined and notation is inconsistent. The paper lacks mathematical analysis and theoretical foundation. The scalability of the proposed approach may be limited due to the need to collect and process all observations. It is unclear whether the approach is intended for modelbased or modelfree MARL scenarios. The selection of numerous parameters and hyperparameters is not discussed. The interpretation of the simulation results is challenging without extensive crossreferencing of cited works. The conclusions on the effectiveness of local communication channels are not surprising. Questions: Why not start with a predetermined set of neighboring agents and build on that assumption The comparisons between algorithms are difficult to interpret. A table summarizing the communication methods observation models and suitable scenarios could be beneficial. What is the precise reason behind the low performance or failure of TarMAC NDQ and TMC in specific environments Explain why FullComm succeeds in environments with minimal observation redundancy. What is the meaning of \"message instability\" in relation to TarMAC and NDQ and how does this affect their performance"], "wVc4Qg5Bhah": ["Paraphrased Summary: Researchers introduce a fast secondorder algorithm that combines aspects of both global convergence rates (O(\\sigma\\sqrtT) and O(1T3)) for minimizing functions with smooth Hessians through either stochastic or deterministic methods even without knowing specific problem parameters. This is achieved by: Utilizing an extragradient framework with secondorder information Averaging past iterates and gradients to update the algorithm Adapting the step size automatically to handle noisy feedback and eliminate the need for complex parameter estimation Strengths and Weaknesses: Strengths: Clear and wellwritten paper Surveys related work thoroughly Significant practical relevance as a universal and adaptive globalconvergence secondorder method Interesting adaptive step size policy that may have broader applications in other secondorder methods Weaknesses: Minor typos: Line 115: \"the operator F satisfies eq (1)\" is unclear Line 122: \"(EG)\" likely refers to xt Xt xt12 Xt12 Potential improvement for numerical section: Benchmarking against Cubicregularized Newton an alternative secondorder method with practical implementation and stability", "Paraphrased Summary The authors propose a novel accelerated secondorder optimization algorithm for convex functions with Lipschitz continuous second derivatives achieving a convergence rate of O(1 T3). This rate matches that of the Accelerated Cubic Newton Method (ACNM) without requiring information about the Lipschitz constant or using line searches. Instead the algorithm accumulates information about the deviation of gradients from their linear approximations to compute the next iteration. Notably the algorithm can handle both deterministic and stochastic oracles adapting to the oracles variance without algorithmic modifications. Strengths and Weaknesses The proposed algorithm offers a new approach to constructing accelerated secondorder methods featuring adaptability to stochastic oracles and a unique approach to deviation estimation. However it has the limitation of working only with bounded feasible sets and exhibiting a convergence rate dependent on the feasible sets diameter unlike ACNMs dependence on the initialtosolution distance. Questions and Suggestions Use clearer notation for stochastic gradients and Hessians to indicate randomness. Remove the \"implicit algorithm\" and directly incorporate its updates into Algorithm 1 for better clarity. Discuss the deterministic case before outlining the stochastic case for easier understanding. Provide more details about the complexity of nonEuclidean projection in Algorithm 1 step 2. Revise Eq. (8) to make the algorithm scaleinvariant. Present the complete 3term convergence guarantee in Theorem 3.2. Consider a simpler choice for b\\t to simplify Theorem 3.2. Specify the choice of a\\t and b\\t in Theorem 3.3 and Proposition 3.1. Explore the choice a\\t (B\\t b\\t)2 for greater consistency with the proofs. Clarify the experiment settings including the functions and constraints used. Explain the observed superlinear convergence of ExtraNewton. Minor Corrections Correct the convergence rate to O(L D3 T3) throughout. Define the constant L in line 78. Clarify that the methods obliviousness to the feasible sets diameter is only within a specific range. Simplify Assumption 2.1 since it is automatically satisfied for compact sets. Clarify the use of Euclidean and operator norms in Section 2. Correct the typos and provide missing definitions in Eqs. (Taylor) (EG) (5) and Algorithm 1. Introduce a notation for \\\\min\\\\ \\\\ \\\\tilde\\\\nabla\\t \\\\ \\\\ \\\\nabla\\t \\\\ \\\\ in Eq. (20) for brevity. Clarify the purpose of the expectation sign in Eq. (8) and the missing expectation sign in the displays between lines 732733 and 769770.", "Paraphrased Statement This paper introduces an algorithm for solving optimization problems with constraints that achieves a convergence rate of O(1 \\sqrtT) for both stochastic gradient and Hessian estimators and O(1T3) in the deterministic case. The algorithm combines elements of the extragradient and Newton methods along with an adaptive step size scheme. Strengths Novel approach that leverages techniques from online learning and optimization. Wellmotivated with clear explanations of the derivation. Weaknesses Nonimplementable in practice due to the presence of a constraint set making the subproblem in line 2 of Algorithm 1 difficult to solve. Insufficient experimental results to support the algorithms performance. Unclear technical definitions in the theorems. Questions How to implement the algorithm given the nonsolvability of the subproblem in Algorithm 1 How the convergence rate would change in practical settings where the Newton step is obtained through iterative methods like CG. Comparison of running times with other algorithms in higherdimensional problems. Why the Newton method for least squares is not expected to require more than one iteration to find the optimal solution. Clarification on the missing or typographical errors in Theorem 3.1. Explanation of the equation after line 236 and its significance. Definition of Bt in Algorithm 1. Realworld applications where the noise level of the oracle is unknown. Potential for improved convergence rates under changing variance conditions. Missing definitions of D in the main paper.", "Paraphrased Summary: The paper introduces a new algorithm called ExtraNewton for solving smooth and convex optimization problems in secondorder. ExtraNewton uses an adaptive step size rule that automatically adjusts to unknown parameters of the problem making it versatile. It also incorporates an optimistic weighting scheme for decision variables and gradients resulting in an accelerated convergence rate of O(\\sigma\\sqrtT 1T3) for stochastic problems. Numerical experiments demonstrate ExtraNewtons effectiveness compared to existing methods. Paraphrased Strengths and Weaknesses: Strengths: Adaptive step size rule that does not require prior knowledge of problem parameters Accelerated convergence rate for stochastic problems Clear exposition and sound technical approach Weaknesses: Assumption of a compact solution set which can limit applicability Numerical performance for stochastic problems is not as strong as expected Lack of intuition behind the adaptive step size rule Paraphrased Questions: Can the compact set assumption be removed and what impact would this have on the convergence rate How does ExtraNewton compare to cubic regularization methods for deterministic problems Are there alternative weighting schemes that could improve convergence performance Is ExtraNewton scalable to highdimensional problems with stochastic finite sum structures"], "lTKXh991Ayv": ["Summary: The study investigates adversarial attacks on \"traffic forecasting models\" which aim to predict realworld traffic conditions. It proposes a general framework for such attacks in both transparent and opaque settings. The approach is empirically validated using realworld data including ablation studies and theoretical analysis. Strengths and Weaknesses: Originality: High as it addresses a novel domain and presents novel contributions in analysis and evaluation. Quality: High with wellsupported arguments despite some minor missing details. Clarity: Average with understandable English but room for improvement. Significance: High as it explores a neglected deployment scenario of machine learning and provides a valuable basis for future research. Questions: WhiteBlackbox Definitions: The whiteboxblackbox terminology used lacks clarity. Elaborate on the attackers assumptions in each setting such as training data and model architecture access. Feasibility: Provide an analysis of the practical feasibility of the proposed attacks considering costbenefit tradeoffs and potential resource requirements. Wide Adoption: Back up the claim about the widespread adoption of MLbased traffic models in modern transportation systems with more substantial evidence. TrainingTesting Time: Specify the trainingtesting time of the models including whether parallel GPUs were used or single GPUs per run.", "Paraphrase: Summary This paper investigates how vulnerable spatiotemporal traffic forecasting models are. The authors introduce a practical framework for carrying out adversarial spatiotemporal attacks. Experiments confirm the effectiveness of their approach. Strengths The first effort to target spatiotemporal traffic forecasting models with attacks. The code is available allowing readers to replicate the findings. The authors provide both empirical and theoretical evidence. Weaknesses When extending the attacks to the blackbox setting the authors use a surrogate model trained by querying the target models. This can result in a large number of queries which reduces efficiency. The authors should address this issue. Questions Can the authors elaborate on the efficiency concerns regarding the use of surrogate models in the blackbox setting", "Paraphrased Statement: This paper introduces a gradientbased attack method specifically designed for traffic prediction models. It presents a twostep framework for realistic spatiotemporal attacks and demonstrates their effectiveness in degrading model performance. The technique strategically targets highly influential nodes in the network based on gradient magnitudes enabling efficient and impactful attacks. Strengths: The problem addressed is clearly defined and discussed. The paper is wellwritten and organized. Extensive testing supports the efficacy of the attacks. Weaknesses: The method lacks originality as it primarily adapts existing adversarial attack approaches for spatiotemporal prediction models. The proposed TDNS technique is simply a modified version of the PGD algorithm with targeted nodes. The attack is tailored to traffic prediction models limiting its applicability. The mathematical expression for Ladv is not explicitly provided. The deviation of the prediction model from the pretrained model is not adequately described. The effectiveness of the attack is not evaluated against various target models limiting its generalizability. The topology (adjacency matrix) is kept constant during adversarial sample generation despite the potential impact on graphbased forecasting models. Questions: Why is only negative saliency score considered (line 143) How would absolute value considerations affect the results Why is the topology of Gt left immutable despite its potential for modification during model training Does the time interval (e.g. 5 minutes 1 hour) influence attack performance", "Paraphrased Summary: This research introduces a cuttingedge attack method targeting spatiotemporal traffic forecasting models. Theoretical analysis has been performed to determine the worstcase scenario following the attack. Extensive testing on realworld data demonstrates the effectiveness of the attack. Additionally the study reveals how adversarial training enhances model robustness. Strengths and Weaknesses: Strengths: Clear and concise writing Innovative approach to introducing adversarial robustness in spatiotemporal modeling Robust attack backed by theoretical analysis Thorough experimental evaluation Weaknesses: The generalizability of the method beyond traffic forecasting models is unclear. Questions: As noted in the weakness can the method be applied to other spatiotemporalbased applications"], "uFSrUpapQ5K": ["Paraphrase: This paper investigates how to evaluate actions that have not been taken in the past (known as offpolicy evaluation) when the available data does not cover all possible actions (`deficient support`). This is common in realworld settings where the number of actions is large or new actions are added over time. There are two major challenges in this scenario: The standard \"importancesampling\" method for estimating rewards of untried actions is biased. The assumption that all actions are covered by the data (known as \"full support\") is often unrealistic. This paper introduces two new estimators that address these challenges: 1. Pseudoinverse (PI) Estimator: Assumes that actions can be described by their features. Constructs a system of equations based on the assumption that the rewards are linear combinations of features. Solves this system to estimate the rewards of untried actions. 2. Action Similarity Estimator: Assumes that the rewards for untried actions can be approximated by a weighted average of the rewards of similar actions. Uses a weighting function to determine the similarity between actions. Empirical results using realworld datasets show that these new estimators perform better than the original importancesampling method both in terms of accuracy and robustness. Strengths: This paper addresses a significant problem in offpolicy evaluation that has not been adequately studied before. The proposed estimators make realistic assumptions that are more applicable in realworld settings. The empirical results demonstrate the effectiveness of the new estimators. Weaknesses: The PI estimator relies on the assumption that rewards are linear functions of action features which may not always be true. The similarity estimator requires careful selection of the weighting function which can be challenging in practice. The paper only compares the proposed estimators to a single baseline (importancesampling) and it would be beneficial to include additional baselines. Further analysis of the biasvariance tradeoff between the estimators would be helpful.", "Paraphrased Statement: This paper tackles the difficulty of using inverse propensity weighting for offpolicy evaluation when certain actions have zero probability under the recorded policy. To overcome this the authors introduce two assumptions: positivity under side information and linearity with respect to action features. These assumptions allow previously proposed estimators for offpolicy evaluation in slate recommendations to be used. An additional nearestneighbor estimator is also proposed. Strengths: Addresses a significant and practical problem. Provides clear and straightforward solutions to achieve identifiability. Demonstrates strong empirical performance. Weaknesses: Both assumptions are inherently difficult to verify. Includes several hyperparameters whose optimal settings may be unclear in practice. Questions for the Authors: How can practitioners assess whether violations of the proposed assumptions are likely How do the proposed estimators perform compared to outcome models particularly the similarity model", "Summary This paper tackles the challenge of evaluating offpolicy performance when the subset support assumption is violated. The proposed solution involves estimating rewards for unseen actions based on rewards observed for logged actions. Previous methods assumed parametric reward functions but this work utilizes nonparametric kernel regression to infer rewards for unseen actions. Strengths Potential appeal to a broader audience Clear presentation of a straightforward concept Weaknesses Potential for further improvement under the given assumptions Lack of indepth empirical investigation Questions 1. Line 39: Clarify that the logging policy requires a nonzero chance of taking all actions under the evaluation policy not all possible actions. 2. Line 157: Verify if constraints are needed for w such as w0 and \\sumaw(aa\u2019) 1 for all a\u2019. 3. Similarity estimator: a. Is importance sampling necessary Direct expectation computation may be more efficient. b. Small pi0(as) values might increase variance in the denominator. c. Explore alternative ways to reduce computation time for direct expectation estimation. 4. Baseline: Conduct ablations with diverse feature vectors (e.g. random projection cosine basis) to strengthen the baseline. 5. Dataset size: Include ablations on the impact of dataset size on the estimators. 6. Similarity metric: Implement a crossvalidation procedure to optimize the choice of similarity metric w and underlying feature for both baseline and proposed methods. 7. Section 4.3: Investigate whether wx is truly independent of x as this may affect performance. 8. Table 1: Clarify the normalization process used.", "Paraphrased Statement: This paper introduces two algorithms for estimating the value of actions in a setting where some actions are not observed in the data. The first algorithm relies on a specific structure for the expected reward while the second assumes the similarity of actions. Both algorithms come with guaranteed accuracy bounds and the paper provides experimental evidence to evaluate their performance. Strengths and Weaknesses: Strengths: Wellwritten and addresses a research gap Clear and concise presentation Convincing empirical evaluation Weaknesses: Limited to binary action features (reason not fully explained) Predefined weight matrix for the similarity algorithm rather than learned Possible oversimplification of the reward linearity assumption Lack of guidance on choosing between the two algorithms"], "zTQdHSQUQWc": ["Summary The paper introduces FiLM a novel forecasting model using Legendre projections. It comprises two new components: Legendre Projection Unit (LPU): Transforms time series data into a lowdimensional subspace. Fourier Enhanced Layer (FEL): Enhances LPUs projections using Fourier analysis. Strengths FiLM improves performance on benchmark datasets compared to other models including FEDformer. LPU and FEL can enhance the performance of various forecasting architectures. Theoretical analysis supports the design of FiLMs components. FiLM is simpler and faster than Transformerbased models. The paper is wellwritten and clear. Weaknesses Recent studies indicate that Transformerbased models may not achieve the highest performance in longhorizon forecasting. Comparisons should include nonTransformer and simpler models. FiLM does not consider relationships between time series in its forecasting process. The training speed evaluation is not conclusive and should include simpler baselines. FiLMs complexity scaling with the number of time series needs further evaluation. Some relevant baselines (e.g. NBEATS NHiTS) are missing from the comparisons. Questions Why are FiLMs performance results different in ablation tables (2 5) compared to the main result table How does FiLM handle relationships between time series or is it a univariate model How were the hyperparameters (M N) for the main result table chosen Were they optimized using the validation set", "Summary This study presents two techniques for enhancing long time series modeling: Legendre Projection Unit (LPU): Compresses time series using Legendre polynomials as a basis. Frequency Enhanced Layer (FEL): Performs a lowrank approximation and selects Fourier transform frequency modes. These techniques are not specific to any domain or task and can be applied to various time series modeling tasks. The authors provide theoretical and empirical evidence demonstrating the models effectiveness. Strengths Clear presentation Informative visualizations (Figures 4 and 5) explaining LPU and FEL implementation Comprehensive ablation studies addressing performance questions Strong performance in longterm time series forecasting compared to popular deep learning methods Theoretically sound methods for noise reduction and capturing structure at different timescales Modular design allowing for the incorporation of these techniques into existing time series models Potential for wide application Weaknesses The use of orthogonal functions and Fourier frequency reduction for time series modeling is not particularly novel. Questions To enhance clarity the authors could provide an introduction to the notation used such as the meaning of subscripts and superscripts in the formulas before line 117.", "Summary: FiLM (Frequencyimproved Legendre Memory Model) uses Legendre polynomials Fourier analysis and lowrank approximation for longterm time series forecasting. It combines known techniques into a novel approach and provides bounds on error accumulation. Strengths: Combines multiple existing components into a new time series forecasting method. Explores the potential of Legendre polynomials for modeling longrange dependencies. Provides theoretical analysis and empirical evaluation on realworld datasets. Weaknesses: The abundance of results makes it challenging to understand what is presented and why. The evaluation lacks comparisons with competing methods that use lagged values. Inconsistencies are present in the results between tables. Questions: 1. Why are different datasets selected for different experiments 2. How do LPUs compare to linear layers with lagged values 3. What is the source of the results in the \"FiLM\" column of Table 2 and why do they differ from other tables 4. What is the relative improvement referred to in Table 6 and how is it calculated"], "sexfswCc7B": ["Paraphrased Summary: This study investigates sentence repetitions in text generation and introduces a training method (DITTO) to address it. By penalizing repeated sentences in synthetic training examples DITTO significantly reduces repetition while maintaining competitive performance in text generation and summarization tasks. Strengths: Thorough empirical analysis of the repetition problem in text generation. Introduction of a wellmotivated training objective to mitigate repetition. Weaknesses: While the empirical analysis is valuable the claim that previous sentence exposure is the sole cause of repetition is oversimplified. Overstatement of the methods effectiveness in reducing repetition. Questions: In Section 3 could you explain how appending a random training corpus sentence to pseudorepetitive sentences improves performance Clarify why the results in Fig. 4 indicate lower repetition rates for UL despite its higher reported repetition rates in previous tables.", "Summary This work introduces a novel method for reducing the repetition of sentences in autoregressive sequence generation. The method involves creating an artificial set of negative samples by repeating sentences from the training corpus up to the maximum prediction length. A loss function is then used to penalize the model for producing these negative samples during finetuning. The method was compared to various other approaches and demonstrated superior quality for generated sequences. Both automatic and human evaluations were conducted for openended generation while automatic evaluation was used for directed generation. Strengths and Weaknesses Originality Proposes a novel approach to mitigating sentencelevel repetition. Weaknesses Vague justification for the idea. Quality Provides a wellwritten codebase with instructions for running experiments. Includes comparisons with multiple methods to avoid potential bias. Weaknesses Incomplete review of related work. Lack of transparency in experiment comparison particularly in relation to model training length. Clarity Clear experimental protocol. Weaknesses Vague and incomplete explanation of the proposed method. Significance Potential contributions to future work by providing a general method for using external negative samples for optimization. Weaknesses Limited justification for the specific loss function design. Absence of a comparison with the Contrastive Generation framework for text generation. Questions How does the analysis differ from previous work by Holtzman et al. Why do sentence repetitions occur How does the method address the potential for the model to break the selfreinforcing loop Why was the loss function designed in the specific way described How fair is the comparison between the proposed method and other approaches given the different training and evaluation protocols", "Summary Paraphrase: This paper explores the problem of repetition in naturally generated text. It identifies a \"loop\" issue in the decoding algorithm that leads to increased repetition compared to humanwritten text. The paper analyzes specific instances of repetition and conducts experiments using standard metrics. It introduces a \"selfreinforcement effect\" where the model is more likely to repeat when similar context has been generated previously. To address this the paper proposes a simple training objective called DITTO that aims to minimize repetition. The paper evaluates DITTO using openended text generation and text summarization tasks demonstrating its effectiveness. Strengths and Weaknesses Paraphrase: This paper focuses on a critical challenge in natural language generation. The analysis of the \"loop\" issue is thorough and persuasive. The proposed DITTO technique is grounded in experiments and analysis. The paper is generally clear and straightforward. DITTO is simple to implement. The paper presents comparisons with previous work and demonstrates DITTOs efficacy. Human evaluation further supports the effectiveness of DITTO. However there is a concern regarding the function of DITTO (Equation 1). The equation appears to penalize the model for reducing repetition which is counterintuitive. If the equation is understood correctly it should be modified to penalize positive \"overlap\" between current and previous predictions. Additional Questions: Equation 1 aims to penalize \"onehop\" similarity. However it may not capture more complex repetition patterns where sentences are slightly different but contribute to overall repetition. Is it possible to modify the equation to address this"], "wUctlvhsNWg": ["Paraphrased Statement: Currently federated multikernel learning approaches struggle with communication costs and diversity in data. This paper introduces a personalized online federated multikernel learning technique that addresses these issues. Strengths: Improves communication efficiency with random feature approximation. Achieves personalization through kernel subset selection. Delivers superior performance in numerical simulations compared to existing methods. Provides theoretical analysis that establishes regret bounds for clients and the server. Weaknesses: The use of random feature approximation is not novel. The primary contribution lies in kernel subset selection for personalization which may be considered limited. Limited comparisons with other personalized federated learning approaches. Experiments conducted on datasets with small feature sizes. Questions: How to choose the dictionary of kernels for different tasks The paper does not provide guidance on kernel selection. Does the regret have a lower bound If so can it show the optimality of the proposed method The paper does not provide a lower bound for regret making it difficult to assess optimality. What are the advantages and disadvantages of multiple kernel learning compared with deep learning The paper does not discuss the comparison with deep learning. Why does the paper use multiple kernel learning for personalized online federated learning The paper does not explicitly justify the use of multiple kernel learning for this purpose.", "Paraphrased Statement: Summary: This paper introduces a new online federated Multiple Kernel Learning (MKL) framework called POFMKL which utilizes Random Fourier transformation (RF) approximations. To reduce computational overheads and enhance communication efficiency each client updates only a fraction of kernel parameters. Theoretical analysis demonstrates that POFMKL achieves sublinear regret. Empirical evaluations on multiple datasets confirm the methods effectiveness outperforming baseline approaches. Main Contributions: Development of a personalized federated MKL method to address heterogeneity in federated learning. Proof of convergence rate for the proposed algorithm. Communication cost savings compared to existing algorithms both theoretically and experimentally. Strengths and Weaknesses: Pros: Technically robust and wellpresented with clear motivations and explanations. Convergence guarantee provided for the algorithm. Contributions and originality highlighted explicitly. Convincing and promising experimental results. Cons: Complexity analysis of the proposed algorithm and comparisons with other methods would be valuable. Highlevel intuition could enhance understanding of the method section. Questions: 1. How would the algorithm be affected if only a subset of clients participated in each iteration of online federated learning 2. Can you provide a detailed explanation of how Equation 7 was derived", "Summary Paraphrase: This research investigates the challenge of multikernel learning in online and distributed environments. To mitigate the \"curse of dimensionality\" the study employs a random feature (RF) approximation approach utilizing globally estimated parameters across clients. Specifically each client allocates a customized weight to each kernel and randomly selects a subset of kernels at each step to reduce communication costs. The paper demonstrates that clients achieve sublinear regret compared to their hindsightoptimal RFapproximated kernel. Experimental results support the theoretical findings and highlight the advantages of the proposed method over alternative online multikernel federated learning techniques. Strengths and Weaknesses Paraphrase: Strengths: The paper is wellwritten and accessible. The proposed idea is meaningful and the theoretical results are sound. Weaknesses: Originality and Related Work: The paper should more thoroughly discuss its similarities and differences with previous work particularly [12]. Both methods use RF approximation and random kernel selection but [12] employs global kernel weights while this paper personalizes them for each client. The paper should also highlight relevant personalized federated learning literature and discuss its contributions in comparison. Technical Soundness: Some notation could be clarified to improve clarity. Problem (1) should be expressed without relying on parameters. Theorem 2 could be improved by removing unnecessary coefficients and considering the bound for local regret. Questions: It would be helpful to provide a lower bound on pi\\k t to ensure the validity of Theorem 2.", "Paraphrase: This study introduces a unified framework for training online federated Multiple Kernel Learning (MKL) models for clients in an efficient manner. It combines random feature approximation with federated learning and proposes a novel Personalized Online Federated MKL (POFMKL) algorithm. This algorithm enables MKL models to achieve higher communication efficiency with proven theoretical guarantees. Experimental results support the theoretical claims. Strengths: The proposed method uses random feature approximation to select a subset of kernels providing a new approach for high communication efficiency and realtime approximation. Compared to existing online federated MKL algorithms POFMKL offers a scalable and adaptive model where each client chooses a subset of kernels and sends partial updates based on communication bandwidth. Weaknesses: The presentation of experimental results could be improved for clarity and intuitiveness. It would be valuable to evaluate the performance of the algorithm on realworld online learning tasks such as online regressions and time series predictions. Providing details on the convergence time of the proposed algorithm would enhance the evaluation. A comparison with a wider range of online Federated MKL algorithms would be beneficial including MKOFL [1] which achieves similar theoretical guarantees but selects only one kernel per client. Clarification is needed regarding the lack of citations for baselines mentioned in lines 276277 (OFSKL and OFMKLAvg) and whether they are variations of FedOMD. Questions: How does POFMKL compare to MKOFL [1] in terms of advantages and disadvantages Are OFSKL and OFMKLAvg separate methods or variations of FedOMD"], "wsnMW0c_Au": ["Paraphrase: This research demonstrates that online gradient descent (OGD) achieves a regret of O(T23) for a specific category of nonconvex problems. It builds on earlier research by Amid and Warmuth which established a connection between gradient flow (for a reparametrized function) and mirror flow (for the initial function). This study extends this by demonstrating that discretetime OGD on a reparametrized objective exhibits sublinear regret. It does this by drawing upon the O(\\sqrtT) regret analysis of discretetime online mirror descent (OMD) for the same type of problems. By carefully assessing the discrepancy between OMD for the original problem and OGD for the nonconvex reparametrization the OGDs sublinear regret can be established. Strengths and Weaknesses: Positives: Fills a gap in the literature by establishing discretetime results for a previously demonstrated continuoustime finding. Negatives: The authors could provide insights into the benefits of conducting discretetime online learning on reparametrized functions considering the increased regret compared to using OMD directly for convex problems. The papers title should be more specific to reflect the applicability of the method to a particular problem class and its reliance on the OGDOMD connection from Amid and Warmuths work. Minor: Line 200202 requires clearer derivation. Typo on Line 215: \"both both.\"", "Summary (Paraphrased) This study analyzes a potential equivalence between online gradient descent (OGD) on nonconvex losses and online mirror descent (OMD) on convex reparametrizations of those losses. While specific instances of this equivalence have been identified in previous research this paper aims to establish a broader theoretical framework. Key Results Theorem 3: Provides conditions under which OGD on nonconvex losses can be approximated by OMD on convex losses resulting in an O(T23) regret bound for OGD. Section 3.1: Presents examples of OMD on convex losses that can be approximated by OGD on nonconvex losses. Theorem 7: Proves conditions for OGD on nonconvex losses to be approximated by OMD on convex losses. Strengths Wellwritten paper especially the introduction. The exploration of equivalence between nonconvex and convex online optimization is a promising area of research. Weaknesses General Theory Coverage: It remains unclear whether the theory covers meaningful cases. Example Inconsistencies: The examples in Section 3.1 may not fully satisfy the conditions outlined in Theorem 3. Quantification of Convexity Parameter: Theorem 7 does not specify how the strong convexity parameter c0 of the regularizer R depends on q and \u0303f. Remarks Strong Convexity of R: Theorem 3 assumes R is 1strongly convex but this condition is not explicitly stated or verified in the provided examples. Proof of Theorem 7: The transition from the displayed equations to the ODE in the proof is not clear. Questions Example Scaling: How does G in Theorem 3 scale with T Is R 1strongly convex in all three examples of Section 3.1 Theorem 7 Proof Clarification: Please explain the derivation of the ODE from the given equations.", "Paraphrased Statement: Summary: This paper demonstrates that under specific conditions online mirror descent for convex losses can be expressed as a perturbed variant of online gradient descent for potentially nonconvex losses with a regret bound of O(T23) up to a reparameterization. Using similar reasoning the paper also explores when online gradient descent for nonconvex losses can be proven regretless through reparameterization. Strengths and Weaknesses: Strengths: The approach is simple reasonable and novel. Weaknesses: The first example in Section 3.1 may be incorrect as it includes online exponentiated gradient descent on a nonconvex set which violates an assumption of the paper. The other examples in Section 3.1 (log barrier and tempered Bregman divergence) are less common than exponentiated gradient descent. Section 5 which provides conditions for online gradient on nonconvex losses to be equivalent to online mirror descent on convex losses lacks concrete examples. Typos are present in the paper. Questions: Is the first example in Section 3.1 indeed incorrect Is there any practical application of Theorem 7 in Section 5"], "pcgMNVhRslj": ["Paraphrased Statement: This paper presents a new transformerbased architecture for video action recognition. It uses KMA to generate an alignment matrix that aligns spatial features for temporal processing which then utilizes 1D temporal operations to enhance video representation. The method performs well on various action recognition datasets. Strengths: Unique use of KMA for spatial alignment in temporal operations. Wellwritten and supported with sound mathematical derivations and impressive results. Robust performance across different backbones. Weaknesses: Limited analysis of the models learned features. Lack of clarity in certain explanations. Questions: Does the temporal sampling rate impact the methods performance (e.g. input downsampling alternate frame alignment) Does the model effectively capture motion representation (as per the \"arrow of time\" experiment) How do global and local motion patterns influence performance (e.g. correlation with SSThV2 dataset having more global motion)", "Paraphrased Statement: Summary: The study aims to tackle the challenge of capturing temporal context from video frames for action recognition. It highlights the limitations of current approaches using factorized and joint convolutions. To address this it proposes framebyframe alignment to enhance the mutual information of frame representations. The Alignmentguided Temporal Attention (ATA) method is evaluated on popular action recognition datasets (Kinetics400 SomethingSomething V2) and demonstrates improvements over baseline models. Strengths: Clear presentation with helpful visualizations. Comprehensive analysis of existing temporal modeling approaches and their limitations. Theoretical insights explaining the methods advantages. Ablation studies confirm the effectiveness of ATA over simple averaging and attentionbased methods. Combination with TimeSformer yields stateoftheart results surpassing even larger variants (TimeSformerL TimeSformerHR). Weaknesses: Lack of detailed comparison and discussion with other attentionbased methods. It is unclear how framebyframe alignment differs from global attention. The computational complexity of the proposed method is not fully explained. Questions: Why does the computational complexity increase when using more input frames (32) compared to baseline TimeSformer with 8 frames Can you provide a more comprehensive comparison and analysis of ATA with various attention methods including vanilla attention and efficient factorized variants", "Paraphrased Statement: This paper introduces a method for recognizing actions in videos. It addresses challenges in spatialtemporal operations for factorized (2D1D) and joint (3D) models. To overcome these the paper proposes Alignmentguided Temporal Attention (ATA) for modeling temporal sequences. ATA effectively captures temporal information while remaining computationally efficient. The paper also provides theoretical support for the effectiveness of framebyframe alignment. This alignment increases mutual information between frames potentially capturing more taskrelevant information. Extensive experiments demonstrate the effectiveness and adaptability of ATA. It not only achieves high performance on video action recognition benchmarks but also serves as a versatile plugin for video learning tasks. Strengths: Clear and wellorganized writing. Understandable figures. Convincing experimental results. Weaknesses: Lack of sufficient originality and innovation. The alignment method in ATA appears similar to that in a previous study (reference [4]). Questions: 1. Clarify the differences between the alignment method in this paper and that in reference [4]. 2. Elaborate on Figure 1: How are the shared task representations (red line) obtained Why is the shape of the taskrelevant information a trapezoid Is the taskrelevant information derived from RGB videos 3. Explain why the paper does not compare its results with stateoftheart approaches (e.g. articles by Liu et al. and Li et al.). 4. Provide justification for choosing cosine similarity in the KuhnMunkres Algorithm for alignment calculations. Include ablation studies on the impact of similarity metric choice.", "Paraphrased Statement: Summary: This study focuses on enhancing video action recognition by leveraging temporal modeling. It introduces an Alignmentguided Temporal Attention (ATA) module that uses similaritybased feature alignment. The ATA module aims to maximize the correlation between consecutive video frames and theoretical analysis demonstrates that increased correlation leads to more taskrelevant information and improved performance. Experiments on the Kinetics400 and SomethingSomething V2 datasets confirm that ATA can enhance the performance of various image backbones for video recognition. Strengths: The theoretical analysis provides a rationale for the ATA modules benefits. Weaknesses: The ATA module has an alignment and a dealignment component but its unclear how dealignment impacts performance. The experimental results in Tables 1 and 2 show modest improvements compared to other models with no convincing explanation for the relatively weaker performance. The ablation study in Table 3 evaluates the ATA module at increasing depths but it doesnt provide insights into its performance at different locations independently. Questions: 1. Why are the results in the last line of Table 4 highlighted despite not being the best 2. For clarity it would be beneficial to include the symbols from equations (17) and (18) in Figure 5."], "yP0vpghGoLF": ["Paraphrase: Summary: Optimistic Gradient Method (OGM) is an improved version of Extra Gradient (EG) requiring only one gradient calculation per iteration. OGM is applicable to minmax optimization problems such as game optimization adversarial learning and GAN training. This paper analyzes OGM for variational inequality problems with smooth and monotone operators. It demonstrates that the last iterate of OGM converges sublinearly to the solution set in both constrained and unconstrained cases. Performance Estimation Problems (PEPs) are utilized to construct suitable Lyapunov functions and establish convergence. Strengths and Weaknesses: Clarity: The paper effectively outlines the differences between OGM and EG and explains the PEPbased analysis techniques. It also provides a clear comparison between OGM variants. Originality: While last iterate convergence is not a novel concept the papers comparative analysis of OGM variants is notable. Quality: The presented proofs are accurate and wellreasoned. The paper is primarily theoretical with limited experimental results in Appendix D that could benefit from further explanation. Significance: Last iterate convergence is crucial as the final result is often derived from the last iterate rather than the average. Limitations: The paper does not address the stochastic setting commonly used in training large machine learning models. Questions: 1. The paper seems to have similarities to \"Tight LastIterate Convergence of the Extragradient and the Optimistic Gradient DescentAscent Algorithm for Constrained Monotone Variational Inequalities.\" Clarify the key differences between the results. 2. In line 9696 the claim that PEG is widely used in practice should be supported by citations to realworld models that employ it. 3. In line 238 \"nonnegative\" should be replaced with \"nonpositive.\"", "Summary The paper analyzes the Past Extragradient (PEG) algorithm for solving mathematical problems involving inequality constraints. The algorithms performance is measured by the convergence rate of certain error functions. The papers main result establishes convergence rates of O(1N) for unconstrained problems and the same rate for constrained problems where N is the number of iterations. Additionally the paper demonstrates a convergence rate of O(1\u221aN) for a measure of the problems difficulty. The proof involves two key components: A numerical analysis to bound the worstcase convergence rate. A descent criterion based on a potential function which establishes the specific convergence rates. The paper highlights that PEG achieves similar convergence rates to the related EG algorithm but with a different proof technique and weaker assumptions. Strengths and Weaknesses The paper is clearly written and accessible even to readers new to the field. The proofs are presented in a detailed and rigorous manner. The main theorem is of interest to the research community. Questions and Comments The paper could benefit from further discussion on the differences between PEG and EG and how different problems or instances might benefit from using PEG. The paper could mention related updates for the EG algorithm enhancing the context of the discussion. Typographical error: In line 188 remove the tilde symbol from GPEG and GOG.", "Paraphrased Statement: Summary: This paper presents the first analysis of the final iteration convergence rate of the Past Extrapolation Gradient (PEG) method also known as the Optimistic Gradient (OG) method for solving monotone variational inequalities without constraints. Using the Performance Estimation Problem (PEP) technique the authors obtain numerical convergence rate bounds based on problem parameters step sizes and iteration count. However PEP does not provide analytical bounds. To derive analytical bounds the authors identify useful inequalities (Lemmas 3.1 and 4.1) which show that certain quantities decrease monotonically unlike the standard EG method where the gradient norm decreases monotonically. These inequalities are used to construct potential functions yielding analytical O(1N) bounds for the unconstrained and projected PEG methods. Strengths: Analysis of the final iteration convergence rate for the widely used OG method. Example 1.1 highlights the relevance of studying PEG beyond recent accelerated EG methods. Clever use of PEP to identify monotonic properties and guide the development of potential functions. Convergence rates for the projected PEG are expressed in terms of the standard fixed point residual. Weaknesses: Similar results on EG were recently obtained by [Cai et al. 2022]. Lack of intuition and explanation regarding the two different potential functions particularly for the constrained case. Questions: The abstract could be clarified to avoid confusion regarding the O(1N) rate of EGtype methods without the Lipschitz Jacobian condition being previously known. Are the derived bounds close to the exact rate bounds obtained through PEP"], "uV_VYGB3FCi": ["Paraphrased Statement: This research introduces \"Code Editing\" a technique that optimizes encoder latents to improve rate control. Previous applications of latent optimization have focused on enhancing ratedistortion performance but this method explores its potential as a continuous rate control mechanism. Additionally an adaptive quantization step size is proposed to mitigate the ratedistortion decay caused by latent optimization. The research also introduces a pixellevel spatial rate allocation strategy to provide finer control over the bitrate. Strengths: Latent optimization enhances ratedistortion performance across a wide range of bitrates (0.11.0 bpp). Perpixel rate control enables finetuned control over the bitrate for each image. The method does not necessitate decoder transmission of additional information. Weaknesses: Results for very high bitrates (1.0 bpp or higher) are not provided. Code editings effectiveness at high rates remains uncertain. The ROIbased coding examples lack demonstrations of bit allocation on complex masks (e.g. text or faces) and its impact on overall image fidelity. The quantization step size is currently determined manually through grid search. Questions: 1. Ratedistortion performance at high rates (1.0 bpp or higher) 2. Code editings effectiveness with complex ROI masks (e.g. shifting ROI among multiple faces) 3. Impact of finer quantization step size granularity 4. Capacity requirements of higherrate models for code editing flexibility", "Summary: The study explores ways to enhance flexibility in neural image compression (NIC) by modifying representations during the encoding process. They develop a new representation (\"y\") by adjusting both the quantization precision and the latent values. By freezing the model parameters they can increase the encoding time without adding additional computational overhead. Strengths: Novel concept of using adaptive representations to achieve flexible compression rates. Comprehensive analysis using ablation studies and optimization for perceptual quality and adaptive rate optimization. Encouraging results demonstrating the potential of the approach. Weaknesses: W1: Ambiguous and confusing formulation in Section 2.1 regarding the meaning of \"connected\" and the simulation of quantization noise. W2: Incorrect statement in line 112 that relates probability mass function (PMF) to the differentiated cumulative distribution function (CDF) which is only applicable to a specific prior distribution not universal in NIC. W3: Questions about the evaluation methodology including whether range coding was used to calculate bitrates and if measures were taken to prevent model cheating. W4: Inconsistent use of distortion metrics citing Blau and Michaelis while employing LPIPS in perceptual optimization. Conclusion: The reviewer assigns a \"Reject\" rating but is willing to reconsider if the authors: Q1: Explore adapting the hyperparameter \"Delta\" during training. Q2: Provide a more precise formulation in Section 2.1 and ensure that the evaluation is robust and trustworthy. Confidence Rating: 45 indicating that the reviewer may adjust their assessment if additional information is provided.", "Paraphrase: Summary: The authors introduce an enhanced version of Code Editing an image compression technique. This new version employs semiamortized inference which allows control over the bitrate and an adaptive quantization step size that addresses performance degradation at low bitrates. Strengths: The enhanced Code Editing method effectively solves the performance decay issue at low bitrates as demonstrated in Figure 1. Weaknesses: The new components of the method are not clearly explained. Amortized inference strategies in neural image compression are not novel as previously proposed by Yang et al. (2020). Variable bitrate compression with adjustable quantization step size has also been explored by Choi et al. (2019). The enhanced Code Editing approach has not been compared to traditional semiamortized inferencebased neural image compression (Yang et al. 2020) in terms of ratedistortion (RD) curves. Quantitative comparisons with earlier studies (e.g. overhead for variable bitrate support) would enhance the argument for the superiority of this study. Questions: How does the enhanced Code Editing method differ from Yang et al. (2020) in terms of RD curves and is this difference significant Why does the combination of semiamortized inference and adaptive quantization step size enhance performance at low bitrates What factors contribute to the lowrate results in Figure 3 approaching the baseline"], "lMrpZ-ycIaT": ["Paraphrased Statement: The researchers have explored using various popular NLP models for neuroimaging analysis. They have successfully trained models that predict either the complete or partial (masked) input sequence. These models perform remarkably well particularly causal sequence models. The team has demonstrated that the trained models can generalize to previously unseen datasets. This line of research is crucial for advancing the use of machine learning in neuroimaging given the difficulty of obtaining labels. Strengths: Extensive and large dataset Adaptation of several key NLP models Simple and straightforward pretext task Weaknesses: Pretext tasks lack a specific connection to predicting brain states The potential for using datasets to predict meaningful outcomes like autism or depression was not explored No explanation provided for the superiority of causal models Questions: 1. Why was dimensionality reduction using DiFuMo necessary instead of starting with data in R(txp) space 2. What influenced the decision to focus on brain decoding rather than mental disorders 3. Why is the training code not publicly available", "Paraphrase Summary This study evaluates various natural language processing (NLP) models for learning representations from fMRI data including seq2seq transformers and transformerbased BERT models. Unlike NLP models trained with crossentropy loss these models use reconstruction loss. The proposed objectives were pretested on 34 datasets and evaluated on two publicly available datasets. Results suggest that selfsupervised pretraining is applicable to downstream tasks like mental state decoding. While promising the evaluation is weak and the approachs utility remains inconclusive. Strengths and Weaknesses Strengths Novel application of causal and masked data modeling to fMRI data Demonstrated viability of masked selfsupervised training for fMRI embedding pretraining Consideration of a wide range of NLP approaches Weaknesses Unfair evaluation: Reconstruction error introduces bias in all models except for the seq2seq autoencoder potentially encouraging models to learn identity functions. Downstream evaluation should include the complete model evaluating the input data X not just a linear baseline. Inconclusive evaluation: Accuracy numbers alone are insufficient. Standard metrics like ROC AUC or F1 or confusion matrices should be used for comprehensive evaluation. Confidence intervals and statistical significance for improvements should be presented. Lack of supervised comparison: The models should have been pretested with supervised learning to determine if selfsupervised embedding is more effective. Unfounded claim: The study does not support the claim that homogenous datasets limit pretrained model generalizability. Further experimentation is needed to investigate this aspect. Conflicting preprocessing: The authors claim minimal preprocessing but spatial smoothing and detrending are standard practices. DiFuMo is also a matrix factorization embedding method. Incorrect BERT training: BERT is pretrained on Masked LM or Next Sentence Prediction not both simultaneously as used in the study. Questions Would the results change using balanced evaluation metrics How would supervised embedding compare to selfsupervised How does the complete model perform on the input X compared to other models", "Paraphrase: This study assesses the ability of selfsupervised learning models from natural language processing (NLP) to predict brain activity. Specifically the authors investigate: Autoencoders with LSTMs: Models that encode and reconstruct brain signals using LSTM units. GPTstyle autoregressive models: Models that generate brain signals sequentially predicting the next value based on previous ones. BERTstyle masked autoregressive models: Models that predict masked values in brain signals similar to predicting missing words in text. Next sentence prediction task adapted to fMRI data: Models that determine whether two brain signal sequences belong to the same or different experiments. The authors trained all four architectures on a large dataset of fMRI experiments and found that they all performed well. Models with higher capacity exhibited better performance. Pretrained models outperformed traditional linear models on downstream task classification tasks."], "p9zeOtKQXKs": ["Paraphrase: Main Points: The paper examines gradient bias in MetaReinforcement Learning in depth. Two sources of bias are identified: compositional bias and multistep Hessian bias. A general bound is provided for the bias and variance of the final metagradient. Offpolicy training and the LVC estimator are used to mitigate the two biases. A derived algorithm is evaluated on 8 Atari games. Strengths: The paper provides a comprehensive analysis of gradient bias that is applicable to various algorithms. The GMRL framework proposed is relevant to major metaRL frameworks. The decomposition of bias terms is new and valuable. Weaknesses: The papers actual contributions are questionable. The theoretical analysis decomposes gradient bias but the experiments have limited relevance to the theory. Offpolicy training is a simple solution to compositional bias not a novel correction technique. The paper does not offer a new algorithm that balances bias and variance as suggested by Theorem 4.5. Experiments in Section 5.1 merely demonstrate that replacing samplebased estimators with exact solutions improves performance which is unsurprising. The main contribution over existing literature is unclear. Other papers have discussed bias in metaRL including compositional bias. The papers organization is unclear and difficult to follow. Questions: Line 52: Clarify the impact of gradient bias. Line 204: Correct the typo. Line 216: Explain why \\hat \\DeltaJ is less significant than \\DeltaC and \\DeltaH. Line 218: Explain the implications of the variance term and provide examples of algorithms that balance bias and variance effectively. Line 219: Improve the punctuation usage. Clarify the set over which the maximization is performed. Line 318: Define the DiCE estimator.", "Paraphrased Statement: Summary: This study investigates the bias of gradientbased metareinforcement learning (metaRL) algorithms. The authors propose a bias bound consisting of two components: compositional bias and multistep Hessian estimation bias. This bound provides insights into current metaRL algorithms. Strengths: Clear and wellwritten paper. Comprehensive background and analysis. Bias theorem potentially guides future metaRL algorithm design. Experiments support the theoretical findings. Weaknesses: Concerns about convergence in experimental results (Figures 2 and 3). Limited knowledge of prior theoretical work on bias bounds in metaRL. Authors focus on supporting the theorem through existing algorithm modifications rather than proposing novel ones. Questions: Could the authors extend the experimental iterations Is it possible to combine both sources of bias in the analysis Are there plans to explore new algorithm designs based on the theoretical insights Can the authors provide more references to related theoretical work on bias bounds in metaRL", "Paraphrased Statement: Recent Focus and Contributions: Gradientbased metareinforcement learning (GMRL) has gained attention within the NeurIPS community. This paper provides theoretical bounds on the bias and variance of metagradients estimated in commonly used GMRL algorithms. Two sources of bias are identified: \"compositional bias\" from small batch sizes in inner loop steps and \"multistep Hessian bias\" from approximate adaptation in the inner loop. These insights are used to adapt existing variance reduction techniques to GMRL which can reduce bias and improve GMRL performance. Strengths: Analyzes a topic of interest in the NeurIPS community. Provides theoretical motivation for applying variance reduction techniques to GMRL. Presents a clear overall exposition. Weaknesses: Impact of theoretical results is unclear. Limited methodological novelty. Experimental evaluations are limited in scope and method. Questions: How specific are the conclusions to GMRL Can they be applied to supervised metalearning How does LVC mitigate bias at greater inner loop depths Why is LVC chosen as the primary evaluation method Could other methods be considered", "Paraphrased Statement: Theoretical Background: The research examines two sources of bias in gradient estimates for metareinforcement learning methods that optimize both a metalearner and a gradientbased RL learner. An upper bound is established for the \"compositional bias\" which arises from the practice of estimating innerloop gradients. The bias of Hessian estimates using automatic differentiation systems is also analyzed. Empirical evaluations confirm the theoretical findings using tabular Markov decision processes (MDPs). Strengths: Investigates a general problem in gradient estimation for metareinforcement learning. Weaknesses: Concerns have been raised about the validity of the bias estimates: The proof for Proposition 3.1 may be incorrect. The compositional bias analysis may not demonstrate its strict existence but rather provides an upper bound. The compositional bias is claimed to decrease with sample size which suggests it may be an error rather than a bias. The multistep Hessian bias analysis is unclear. The tradeoff analysis (Theorem 4.5) appears selfreferential and lacks significant insights. Questions for Authors: 1. Can you prove that the compositional bias is strictly greater than zero 2. How do you address the concern that the compositional bias is not a true bias but rather a finitesample error that vanishes with increasing samples 3. How does the correlation shown in Figure 1 relate to the identified compositional bias"], "q85GV4aSpt": ["Paraphrased Statement: Summary: This paper strongly advocates for using Mean Squared Error (MSE) loss over the conventional CrossEntropy (CE) loss supported by multiple arguments. MSE is shown to: Have better calibration Possess a stronger generalization bound Exhibit greater robustness to adversarial attacks Strengths and Weaknesses: Strengths: Wellstructured paper with clear motivations and a balanced blend of theory and intuition Novel and significant findings Adequate experimental evidence to support claims Fair and critical evaluation of alternative approaches Weaknesses: Missing citations for previous research supporting MSE over CE Some grammatical errors Confusing use of \"separable\" and \"nonseparable\" terminology Extension to multiclass classification using simplex coding is not easily comprehensible Code for experiments not provided Questions: Theorem 3.3 suggests a bounded margin for MSE but it is unclear how this compares to the maxmargin solution achieved by CE loss. Figure 2 illustrates confidence measurements. However MSE outputs numerical values rather than probabilities so the method for measuring confidence is unclear.", "Paraphrased Summary The study investigates the use of square loss in classifying tasks using both onehot and scaled simplex features. Motivated by theoretical predictions of improved model calibration and feature selection the authors analyze generalization error and model calibration in a largewidth Neural Tangent Kernel (NTK) setting. They derive error and calibration bounds under specific dataset conditions. Experiments demonstrate that square loss with scaled simplex features enhances performance on CIFAR and improves resilience to adversarial examples. Strengths and Weaknesses The generalization error and calibration analysis is novel and intriguing. The empirical results are promising and demonstrate the benefits of square loss in both training and adversarial attacks. The authors suggest that future work should compare their results to generalization bounds for wide neural networks with simultaneous scaling of width and data. Experiments could be expanded to datasets with a higher number of classes such as ImageNet or language modeling. Visualizing or quantifying the differences in decision boundaries between crossentropy and squared loss cases could provide further insights especially in adversarial contexts.", "Paraphrase: Summary: This paper extensively reviews the literature and compares the Cross Entropy (CE) loss function with the Square Loss (SL) loss function. The analysis suggests that SL has comparable accuracy to CE while offering improved robustness and calibration. Experiments using both synthetic and real data support these claims. Strengths and Weaknesses: Strengths: Comprehensive review of the field Clear and logical organization Theoretically sound analysis of CE calibration issues Practical implications for understanding gaps between theory and practice Weaknesses: Some concepts need further clarification Experimental results could be improved to better demonstrate the theoretical findings Questions: Does Theorem 3.1 imply that the optimal loss minimizer is not necessarily the minimizer of the empirical loss Theorem 3.4 echoes findings from previous work (paper [9]) on CE miscalibration near prediction probabilities of 0 or 1. How original are the results presented in the paper Although Theorem 3.4 suggests weak calibration at extreme prediction probabilities the experimental results in Fig. 2 show miscalibration at lower and moderate prediction probabilities. Is there a discrepancy between the theoretical and experimental findings Table 1 shows that SL may not always outperform CE. Are there specific conditions where CE might be preferable", "Paraphrased Statement: Summary: This research investigates the square loss function for classifying overparametrized neural networks in the Neural Tangent Kernel (NTK) regime. It uncovers qualities related to generalization error stability calibration error and the behavior of separable and nonseparable cases. For nonseparable cases the study establishes a rapid convergence rate for misclassification and calibration errors. In separable cases the misclassification rate exhibits an exponentially fast improvement. Additionally the authors prove the existence of a nonzero margin providing theoretical guarantees for robustness. They support their theoretical findings with experiments using practical neural networks. The key conclusion is that while the square loss achieves comparable generalization errors to crossentropy loss it offers significant advantages in robustness and model calibration. Strengths: 1. This work expands the study of square loss in classification providing a deeper theoretical understanding through investigations of key properties. 2. The main conclusions are supported by both theoretical proofs and empirical results. 3. The paper is wellwritten and straightforward. Weaknesses: 1. The experimental results primarily focus on the CIFAR10 dataset which may not be sufficiently convincing. 2. It remains unclear how the results of the square loss in Table 1 using simplex coding differ from previous work that suggested poor performance of square loss with onehot encoding in largeclass number cases."], "klElp42K9U0": ["Paraphrased Statement: Abstract This paper suggests a technique for choosing hyperparameters and algorithms in offline RL. The current approach of training on a fixed dataset with hyperparameter tuning on a validation set is inadequate because datasets can be small with high variance and algorithms are sensitive to hyperparameters. The authors propose using dataset resampling and averaging OPE estimates on the validation set to assess algorithm hyperparameters. They demonstrate the effectiveness of their method on various tasks (D4RL MIMICIII Sepsis Robomimic Tutorbot) showing that it typically finds superior policies compared to existing methods. Strengths and Weaknesses Comprehensive evaluation demonstrates the methods usefulness. Uses multiple reasonable baselines and diverse domains (healthcare robotic control). Practical significance in offline RL due to its broad applicability and superior performance to the usual hyperparameter selection approach. Clear and wellstructured writing. Questions Computational cost of the evaluated approaches. Whether the superior performance of SSRRRS 5 comes at a significant computing cost. The impact of OPE accuracy on the methods performance particularly in highdimensional domains.", "Paraphrase: Summary This study presents SSR a method for selecting the optimal policy in offline RL. SSR leverages repeated random sampling (RRS) to generate overlapping trainingtest pairs from the offline dataset. Each algorithmhyperparameter combination is assessed multiple times using these pairs. SSR chooses the policy with the highest average performance. The authors provide theoretical and experimental support for SSR which outperforms other policy selection techniques particularly in lowdata settings. Strengths Addresses the crucial issue of hyperparameter tuning in offline RL. Introduces a novel approach using RRS which has not been previously applied in offline RL. Provides a simple and wellreasoned concept. Highlights the limitations of current OPE and OPL methods and effectively addresses them. Demonstrates SSRs superior performance on various benchmarks. Written in an accessible manner. Weaknesses Concerns about the robustness of SSR regarding the chosen offline RL algorithm. Questions the effectiveness of SSR based on the choice of OPE. Questions: Can increasing the number of samples (K) always improve policy performance disregarding computational costs Minor Suggestions: Clarify the legend in Figure 4(b) to avoid confusion.", "Summary This study focuses on addressing model and hyperparameter selection in offline reinforcement learning (RL) a crucial issue given the sensitivity of offline RL algorithms to their hyperparameters and specific details. Unlike supervised learning Offline Policy Evaluation (OPE) estimates based on validation sets become unreliable with small sample sizes and lengthy trajectories. The paper introduces SSR (Repeated Random Sampling) for algorithm selection. This approach involves randomly dividing trajectories into trainvalidation (trainval) sets multiple times conducting algorithm training and OPE separately on these sets. The algorithm configuration yielding the highest average performance on the validation set is then chosen and used to retrain the entire dataset. Empirical evaluations across various domains including healthcare and robotics demonstrate superior performance of SSR compared to baselines such as BVFT CV. Strengths Addresses a critical problem in offline RL: hyperparameter selection. Proposes a simple and wellmotivated method (SSR) for algorithm selection. Conductes a thorough ablation study to evaluate the significance of retraining training repetitions and dataset size. Weaknesses The method may be perceived as heuristic with Theorem 1 providing only limited theoretical justification. The computational cost can be high due to multiple iterations of training which is timeconsuming in RL. The pipelines sensitivity to the choice of OPE estimate and its interplay with training algorithms is not explored. Questions Does the correlation exist between the training algorithm and OPE used for selection Is there any interplay between these factors The empirical study spans different domains it would be informative to categorize them based on MDPPOMDP properties and dataset composition to understand the methods performance in various settings. Explorations on reducing the time complexity of the algorithm given its multiple training iterations. Minor Clarification on the yaxis of Fig 4(a) and a correction for the legend of Fig 4(b) (missing SSRRRS 5).", "Summary: This study suggests a technique for offline policy selection which involves dividing the dataset randomly and selecting the best hyperparameters before using the entire dataset for training. It compares the algorithm to existing benchmarks and evaluates it using various datasets. Strengths: Addresses a significant challenge in offline reinforcement learning. The algorithm is theorybased and contrasted with comparable methods. Systematic evaluation using multiple reinforcement learning domains and comparisons with the top policy selection methods. Weaknesses: Certain claims lack supporting evidence or justification. Questions: Q1. The evidence for the theorems assumption that the behavior policy is random and highreward trajectories are rare is not provided. How does this assumption relate to practical settings where data is gathered from a deployed policy Q2. The assertion in line 174 that WIS estimates the behavior policys return is unsupported. Provide proof or references. Q3. The claim in line 207 that RRS converges to leavepout crossvalidation as K approaches infinity is unclear. Explain the connection between p (validation set size) and RRS (data split by 2).", "Paraphrased Statement: This study aims to enhance offline reinforcement learning (RL) with limited data by exploring data partitioning. The authors examine why common data splitting methods such as no splitting and crossvalidation fall short. They then propose a novel strategy called repeated random sampling to optimize algorithmhyperparameter (AH) selection. Experiments demonstrate the superiority of their method called SSRRRS in improving policy performance. Strengths: Innovative exploration of data splitting for offline RL. Introduction of the novel repeated random subsampling (RRS) strategy which combines ensemble learning and crossvalidation concepts. Solid justification for claims and theorems through both theoretical and empirical analysis. Weaknesses: Lack of comprehensive parameter sensitivity analysis for the RRS method particularly guidance on selecting an optimal splitting number (K). Questions: 1. The authors state that validation sets with limited highreward trajectories limit policy evaluation. However it is unclear how the validation set influences predictions made by a policy trained on the training set. 2. In RRS is it possible to leverage policies trained during previous splits as pretraining for subsequent splits"], "mxzIrQIOGIK": ["Paraphrased Statement: Summary: This paper analyzes the online multiobjective learning problem within the online convex optimization (OCO) framework. Its key contribution lies in demonstrating that minimizing regret using the Pareto suboptimal gap (PSG) can be transformed into a singleobjective optimization problem. Subsequently the online mirror descent method can be employed to achieve static and dynamic regret bounds for multiobjective learning. Strengths and Weaknesses: Strengths: Novelty: The paper introduces a unique reduction in the full information setting converting PSGdefined regret into a singleobjective optimization problem. Clarity: The paper is wellstructured and clearly written making it easy to follow. Weaknesses: Exaggerated Title: The papers title suggests a broader scope than its actual focus on multiobjective optimization in OCO (full information) which does not fully encompass the broader field of multiobjective online learning. Theoretical Concerns: There are concerns regarding the proofs and main theorems: The dynamic regret bound (O(T23\u221aT13)) might not be optimal in the full information case. The proofs appear to follow standard OMD algorithm analysis lacking significant technical contributions. Experimental Setup: The experimental parameter setting requires knowledge of \u221aT which is typically unavailable in practice. Reproducibility: The supplementary material only provides code for the minnorm solver not the proposed method. Questions: What is the discrepancy between the static regret formulations in lines 177 and 210 Can it be bounded The supplementary material lacks the full code implementation of the proposed algorithm. How does the algorithm handle nonstationarity to achieve dynamic regret given the unknown temporal variability \u221aT The paper claims a connection to multitask deep neural networks but this link seems tenuous given the convex function requirement in OCO.", "Paraphrased Statement: Summary: This paper explores the concept of multiobjective online learning. It introduces new metrics called static and dynamic regret based on the Pareto suboptimality gap. The authors demonstrate that OMD with the minnorm solver has linear regret inspiring the development of innovative algorithms. A novel algorithm Doubly Regularized Online Mirror Multiple Descent (DROMMD) is proposed which regularizes the composite weights. DROMMD achieves static and dynamic regret bounds that scale linearly with time and match the optimal bounds in the case of a single objective. The paper also includes experiments comparing DROMMD to a linearization baseline and OMD with the minnorm solver. Strengths: Clear and comprehensive presentation Multiobjective online learning is a significant problem with practical applications The proposed algorithms may have widespread impact Introduction of new regret notions and algorithms backed by experimental validation Demonstration of why MGDA is not suitable for multiobjective settings Weaknesses: Incomplete theoretical comparison with the linearization baseline Ambiguous discussion of the advantages of DROMMD over the linearization baseline especially considering its computational complexity Unclear advantages of adaptive regularization strength selection Lack of lower bounds for multiobjective regret leaving the optimality of the proposed approach unclear Questions: Definition of regret in Figure 1 and its importance in interpreting experimental results Clarification of the term \"optimal weights\" mentioned in the paper", "Paraphrased Statement: This research investigates multiobjective online convex optimization where loss functions are vectorvalued and the objective is to minimize the Pareto suboptimality gap indicating the distance to the Pareto frontier. The authors introduce Online Mirror Multiple Descent (OMMD) a generalized version of Online Multiple Gradient Descent that supports different regularizers. OMMD is proven to have optimal dynamic regret making it effective for online learning tasks involving multiple objectives. Strengths and Weaknesses: Strengths: Clear and concise writing Proven strong results using standard techniques Minor Weaknesses: Limited discussion on related work in multiobjective online learning Lack of detailed explanation of the implementation of OMMDs projection operator for different regularizers Questions: The meaning of the term \"the \\sumt \\lambdat\\lambda0\" in Theorem 4.6 remains unclear.", "Summary Paraphrase: This research investigates online optimization with multiple objectives. It introduces a multiobjective online convex optimization framework defining dynamic and static regret. The vanilla minnorm method in MGDA is analyzed revealing potential linear regrets. Consequently a regularized version of online mirror descent is proposed inspired by regularization in FTRL. Theoretical analysis yields both dynamic and static regret bounds for this regularized method matching the optimal regret bounds in singleobjective optimization. Strengths and Weaknesses Paraphrase: Strengths: The multiobjective online convex optimization framework is clear and useful. The proposed algorithm is wellconceived and theoretically sound. The empirical performance is satisfactory. Weaknesses: Question: The study could benefit from exploring applications of online convex optimization with significant impact."], "u_7qyNFwkP8": ["Summary The paper presents theoretical bounds for fair cake cutting algorithms under different fairness criteria and introduces a new model for understanding a specific type of algorithm called Moving Knife protocols. Part I: RW Model for Cake Cutting The paper analyzes the \"RW model\" of cake cutting where players have different valuations for pieces of cake. It presents upper and lower bounds on the number of queries (requests for information) required for different fairness criteria: Epsilon Envy Free Each player believes their piece is at least as valuable as any other players piece (up to an error of \"epsilon\") and it is connected. For 3 people an upper bound of O(log(1epsilon)) queries and a lower bound of Omega(1epsilon) queries are proven. For more than 4 people an upper bound of O(nepsilon) queries and a lower bound of Omega(1epsilon) queries are proven. Epsilon Perfect Each player believes that all other players pieces are approximately the same size. For 2 players only an upper bound of O(1epsilon) queries and a lower bound of Omega(1epsilon) queries are proven. Epsilon Equitable There is a constant \"c\" such that each player believes their piece is within \"epsilon\" of \"c\" times the value of the entire cake. For 3 players and connected pieces a lower bound of Omega(log(1epsilon)) queries is proven. Part II: Moving Knife Model The paper introduces a new model for Moving Knife protocols which captures all current such protocols. It shows that all Moving Knife protocols can be simulated efficiently in the RW model by O(log(1epsilon)) queries. This allows for the derivation of lower bounds for Moving Knife protocols. The upper and lower bounds match indicating the optimality of known Moving Knife protocols. Strengths Introduces new definitions of fairness in cake cutting. Provides upper and lower bounds for RW and Moving Knife models. Establishes the equivalence of RW and Moving Knife models. Weaknesses The paper does not always specify the number of cuts used in its algorithms. Questions How does the new model of Moving Knife protocols relate to nonapproximate versions of the protocol The proofs are technically sound but the paper could provide more emphasis on the key ideas.", "Paraphrase: Strengths and Weaknesses: The paper effectively introduces and justifies the problem of query complexity in \\epsEF cake cutting. However it lacks recent references from the past five years which makes it outdated. The technical advancements in the paper are not particularly noteworthy so its primary value is in raising questions which are more significant when the context is current. It is questionable whether the paper aligns well with the scope of NeurIPS even with a broad interpretation. Questions: How does the paper relate to the work by Br\u00e2nzei and Nisan (EC19) on communication complexity in cake cutting How does the proposed protocol for three agents compare to the protocol by Deng et al for monotone valuations", "Paraphrase: The paper investigates fair methods for dividing a cake among multiple people improving the current theoretical understanding by establishing better boundaries for these methods. It focuses on three fairness concepts: envyfreeness proportionality and equitability (and their \"perfect\" combinations). Due to obstacles the paper concentrates on approximate versions of these concepts. It establishes a lower bound for the number of queries needed to solve three related problems: approximate envyfreeness approximate equitability and approximate perfection with double cuts. It also gives tight upper bounds for two of these problems (the third already had a matching lower bound). While there was no prior knowledge for approximate envyfreeness the other two concepts had similar upper bounds for three players (but not two) that were still inaccurate. The paper demonstrates how the RobertsonWebb query model (involving an external questioner gathering players preferences) can be used to simulate a wide range of cakecutting techniques. Strengths Progresses the fields understanding and reduces gaps in some situations. Weaknesses Incremental in its findings and techniques. Relevance to the wider community is unclear. Lacks practical applications or experimental results. Questions None", "Paraphrased Statement: Summary: This study examines approximate fairness allocation issues among n players including envyfree perfection and equity. It explores the query complexity in the RobertsonWebb (RW) model. The authors establish lower and upper bounds for approximate envyfree allocation (where no player prefers another players portion to their own) regardless of the number of players. For approximate perfect and equitable allocations enhancements were made for the case of n2 players. To attain their upper bounds the authors define \"movingknife protocols\" (MKPs) within the RW model. Strengths and Weaknesses: Strengths: New theoretical upper and lower bounds for approximate allocation. For epsilonenvyfree allocation efficient algorithms with O(log 1epsilon) complexity for 3 players and O(n epsilon) complexity for n players. The upper bound for the 3player case matches the established lower bound. Formalization of MKPs within the RW model unifying two previously unrelated techniques. Weaknesses: Aside from approximate envyfree allocation the authors approach did not enhance existing upper or lower bounds for n 3 players except for n 2. The lower bound for approximate envyfree allocation is not stringent for n 4 players."], "oWx_9VJgyV7": ["Paraphrase: Summary: A method for detecting keypoints from point clouds is proposed. A deep learning model learns an implicit function that maps each location to a probability of being a keypoint. To make the method shapeaware a deep learning model learns the keypoint field alongside an occupancy field to investigate the impact of shape information on keypoint detection. The innovation lies in combining the learning of implicit fields with keypoint fields. The methods effectiveness is evaluated against other methods using a benchmark. Strengths: Clear visualizations. Easy to understand. Weaknesses: Unconvincing motivation. Unclear effectiveness based on experimental results. Questions: Why is it appropriate to learn the probability of keypoints as a field instead of selecting them among input points How does surface reconstruction improve keypoint detection when point clouds provide constraints Why is the method claimed to be unsupervised when it uses ground truth occupancy information Has the combination of shape reconstruction and keypoint detection been explored previously What does the \"surface constraint\" term refer to in the loss function and how does it relate to occupancy field learning Explain the purpose and calculation of the repeatability loss. How is the occupancy probability used to represent the inverse distance between queries and input points How are keypoints extracted via optimization and why is this necessary How are slices selected for visualization of the saliency field How is the number of keypoints determined Why are comparison results different even with the same input Why do results without surface reconstruction perform better than those with it How is the comparison with other methods fair when ground truth occupancy supervision is used", "Paraphrase: Summary: This paper introduces an unsupervised approach that uses point cloud data to predict 3D keypoints. It proposes innovative losses that promote consistency surface constraints and sparsity. The method outperforms existing ones on public datasets. It employs a twoheaded architecture that separates occupancy and saliency modeling allowing them to function independently. Strengths: Innovative use of a continuous space approach. Novel architecture and loss functions. Clear presentation and stateoftheart performance. Weaknesses: Limited discussion of related work. Questions: Are there any newer methods for comparison (Current reference is from 2020.) Clarify the difference between discrete and continuous space. Contrast the proposed method with \"R2d2 Reliable and repeatable detector and descriptor.\"", "Paraphrased Statement: Summary: The research paper is wellstructured and effectively conveys the motivation and technical aspects of its approach. Notably the proposed method for estimating saliency fields based on sparse keypoints appears innovative and has demonstrated its ability to generate robust keypoint detection results. Key Points: Strengths: The paper is clearly written and presents both the motivations and technical details of the approach in a straightforward manner. The idea of estimating saliency fields from sparse keypoints is novel and has proven effective in producing consistent and repeatable keypoint detection outcomes. Weakness: In its current form the approach requires iterative gradient descent and queries the implicitly defined saliency field during inference time resulting in a high computational cost that limits its suitability for practical applications. Questions for Further Clarification: The paper could benefit from additional visualizations showcasing the methods keypoint repeatability under SE3 transformations and varying point sparsity levels. The plots in Figure 5 alone may not provide sufficient visual evidence to adequately demonstrate the quality of the proposed method in terms of repeatability.", "Paraphrase: Summary: The paper introduces SNAKE a new unsupervised method for extracting 3D keypoints from point clouds using implicit neural representations. SNAKE simultaneously reconstructs shapes and detects keypoints during training. This approach enhances semantic consistency repeatability and geometric registration accuracy. Strengths and Weaknesses: While the concept of combining shape reconstruction and keypoint detection isnt novel SNAKE employs implicit representations and outperforms GANbased approaches. The method is simple yet effective with intuitive loss functions that contribute significantly to its success. Extensive experiments demonstrate SNAKEs superiority over competitive baselines with both qualitative and quantitative evidence. The paper is clearly written and supported by understandable figures. However the paper lacks a direct comparison with UKPGAN a notable baseline with public code. Questions: 1. Can UKPGAN be trained from scratch for the experiments given that pretrained models are unavailable 2. Can alternative datasets be used for comparison if UKPGAN training proves infeasible 3. The registration experiments depend on D3Feat descriptors for detected keypoints. While D3Feat is widely used in UKPGAN experiments it may not provide optimal features for SNAKE. Could this be a limitation of the evaluation protocol"], "wZk69kjy9_d": ["Paraphrased Statement Strengths: Implements a straightforward approach by combining existing techniques in modelbased RL goalbased RL and exploration to create a hierarchical RL algorithm. Effectively uses a latent world model for exploration and as a basis for dividing tasks into manageable subpolicies. Demonstrates improved performance across various RL domains indicating a wellengineered and robust algorithm. Weaknesses: Lacks significant conceptual insights into hierarchical RL relying more on engineering than providing novel ideas. Fails to compare with similar hierarchical approaches like feudal networks making it difficult to assess its relative superiority. Presents inconclusive benchmarking results raising questions about the algorithms performance and the potential impact of exploration bias. Provides limited qualitative analysis of subgoal utilization leaving the division of labor between the high and lowlevel policies somewhat unclear. Hyperparameter tuning and development time are not fully addressed potentially limiting the algorithms accessibility and reproducibility.", "Paraphrased Statement: This paper introduces a modelbased RL technique for creating a hierarchical policy that works directly from pixel inputs without the need for predefined highlevel actions. The policy consists of four main parts: 1. A world model that predicts environment dynamics state representation and rewards. 2. A manager policy that chooses goals in a discrete space representing hidden states. 3. A goal autoencoder that translates chosen goals into a representation suitable for the world model. 4. A worker policy that selects lowlevel actions to achieve the managers chosen goal. The method is tested in environments with sparse rewards and on standard benchmarks. It outperforms baselines in sparse reward settings with the gap widening as the environment complexity increases. On standard benchmarks it matches the performance of Dreamer when the worker is trained with both the taskspecific reward and a reward that encourages reaching the managers goal. Strengths: Can learn hierarchical policies without the need for prespecified highlevel actions. Highlevel actions are simply states to reach making them easy to interpret. Matches stateoftheart performance on standard benchmarks. Performs well in settings with sparse rewards. Includes extensive ablations in the supplementary material. Wellwritten paper. Weaknesses: Evaluation does not fully meet the promise made in the introduction regarding the length of time horizons. The method is particularly helpful in settings with sparse rewards so that may be a more appropriate focus for the paper. Suggestions for Improvement: Add summary statistics for the standard benchmarks in the main paper.", "Paraphrase: Summary This article presents a novel Reinforcement Learning (RL)based planner designed specifically for environments with long time horizons and sparse rewards. The planner comprises two main components: World Model: Utilizes an offtheshelf encoder to capture raw pixel input and a goal autoencoder to extract a more abstract world representation. Policy Manager: Consists of a manager policy for predicting the next goal and a worker policy for selecting the next immediate action. To encourage exploration the authors employ two types of rewards: an exploration reward and a goal reward. Extensive experiments are conducted on benchmark tasks including longhorizon navigation and Atari games. Strengths Clear presentation and userfriendly organization. Efficient handling of longhorizon and rewardsparse environments (as demonstrated by the Ant Maze results). Novel approach of using an abstract representation for abstract action selection. Weaknesses Dependence on offtheshelf components which may not significantly enhance originality. Lack of theoretical justification for the training of the manager policy with an asynchronously trained world model decoder. Potential competition between exploration and goal rewards as the former may exceed the latters value. Unclear rationale for changing the goal every k steps. Relatively weaker performance compared to baselines in Atari games. Questions 1. Theoretical Explanation for Manager Policy Training 2. Balancing Exploration and Goal Rewards 3. Justification for Periodic Goal Changes 4. Reasons for Dreamers Poor Performance in Certain Tasks 5. Analysis of the Methods Performance in Atari Games"], "thirVlDJ2IL": ["Paraphrased Statement: The authors address the challenge of accurately estimating means (\u03bci) from m samples drawn from a mixture of k Gaussian distributions with identity covariance matrices. The estimation accuracy depends on the minimum distance between means (\u0394) the number of samples (m) the dimension of samples (d) and the number of clusters (k). Ideally the means should be estimated with minimal error and within a runtime that scales polynomially with these parameters. The authors provide a solution for lowdimensional scenarios (d log k) which has been an unsolved problem. Their algorithm leverages samples from the mixture to extract parameter estimates. It operates efficiently in dimensions O(log k log log k) if \u0394 \u2265 d\u221a(log k) with a runtime polynomial in k. The algorithm involves the Fourier transform of the mixture and estimation of the characteristic function through a carefully designed sampling strategy in the frequency domain. Strengths: Clear and wellwritten presentation of the problem and contributions. Demonstration of the proof for d 1 as a foundation for the general proof. Contribution to a wellstudied problem with potentially significant implications. Weaknesses: Lack of numerical verification of the algorithms performance. Limited discussion of the algorithms practical limitations. Questions: Are there specific sample size requirements for estimating means with a given error What are the practical limitations of the algorithm in terms of dimensions number of clusters and separation between means", "Paraphrase: This paper introduces a Fourierbased method to estimate the mean parameters of spherical Gaussian distributions while analyzing its computational complexity. For lowdimensional distributions the algorithms complexity is polynomial in the number of clusters. The paper thoroughly examines the complexity bound considering the number of dimensions clusters and the distinctiveness of the mean parameters. Additionally the algorithm can be adapted to learn mixtures of other common localized distribution families. Strengths: Clear writing style aids readers in comprehending the key contributions without delving into technical complexity. Appendix proofs are wellpresented and detailed. Weaknesses: Absence of experimental validation. Simulation studies are needed to: Enhance understanding of the theoretical findings. Exhibit the algorithms efficacy compared to other mixture modeling techniques. Additional Questions: Can you provide simulations demonstrating the algorithms complexity with visual aids How does the complexity of the Fourierbased algorithm compare to prominent mixture modeling algorithms like ExpectationMaximization", "Paraphrased Statement: This paper focuses on learning Gaussian mixture models in which the means of the Gaussian components are uniformly distributed within a sphere with an identity covariance matrix. Previous research has established that: For a specific separation parameter (\u0394) between means the number of dimensions (d) cannot exceed a certain threshold when the sample size is polynomially small. It is possible to achieve a specific minimum distance between means (\u0394) if the sample size is polynomially large. It is not possible to achieve a certain separation distance if the sample size is considerably large. This paper fills in the knowledge gap by demonstrating the optimal tradeoff between \u0394 and d in the intermediate range. Specifically it shows that \u0394 is proportional to d\u221a(log k) where k is the number of Gaussian components. The algorithms design is based on: 1. Sampling ln(kp) data points from the mixture where p represents the probability of a data point being close to a mean vector. 2. Using a novel statistic calculated by estimating the expectation of the Fourier transform under truncated Gaussian to determine if a specified mean guess is close to or distant from an actual mean vector. Strengths and Weaknesses: This paper addresses a significant problem in machine learning. The technique used has potential applications to other mixture models. Suggestions for Improvement: Enhance the organization and flow of the paper particularly in Section 1.2 and the algorithm description. Provide more details in the main paper about the effectiveness of sampling from the mixture for mean estimation. Questions: None apart from the presentation feedback."], "suplyBhTDjC": ["Paraphrase: Summary The researchers have created a Clairvoyant Multiplicative Weights Updates (CMWU) algorithm for studying normalform games. This new technique achieves the best possible regret level using a fixed step size. Additionally the researchers have developed an efficient decentralized version of CMWU that also achieves the optimal regret in learning a Correlated Equilibrium (CCE). Strengths: The suggested CMWU is innovative and reduces regret bounds against existing techniques. The proof includes original elements such as the contraction property of the MWU mapping. The decentralized version of CMWU is a novel decentralized multiagent online learning approach. Quality and Significance: The paper is wellwritten and all claims are supported by evidence. The CMWU algorithm is practical for decentralized online implementation which is crucial for multiagent game theory. The specified regret bound gauges performance based on game history rather than timeaveraged history providing a more comprehensive measure. The established regret bound improves on existing methods and marks a significant advancement in learning normalform games. Questions: Why are the regret bounds in Table 1 referred to as \"Rate of Convergence\" Provide references for Theorem 2.4. Could you offer experimental evidence to support the improvements shown in Table 1 Is the assumption of a fixed number of players limiting Correct the typo in line 159: \"implementatio\" should be \"implementation.\"", "Paraphrase: The paper proposes an algorithm called Clairvoyant Multiplicative Weights Update (CMWU) for uncoupled learning. CMWU produces a series of strategies some of which minimize regret. The idea behind CMWU is based on the observation that perfect predictions of future losses in the Multiplicative Weights Update (MWU) algorithm lead to constant regret. CMWU efficiently approximates these perfect predictions through fixedpoint iterations of a contraction. Strengths and Weaknesses: While CMWU builds on known principles its main result is significant and overlooked before. The algorithms simplicity is both a strength and a surprise. However a key limitation is that only a specific subsequence of the CMWU iterates minimizes regret not the entire sequence. This limitation should be acknowledged and explored further. Questions: Can CMWU experience a decline in regret bounds against adversarial opponents as seen in other accelerated regret algorithms Can CMWU be used as an anytime algorithm without prior knowledge of the time horizon (T) This would be a valuable addition to the work.", "Paraphrase: Summary This research explores the use of noregret algorithms for solving general games. The proposed CMWU algorithm achieves an O(nVlogmlogT) regret improving previous results by a factor of O(log3 T). The key innovation is incorporating future decisions into the current decisionmaking process in MWU. Strengths The CMWU algorithm outperforms the previous best algorithm by O(log3 T). The idea of using future information to update current decisions is novel. The authors successfully address computational challenges under certain assumptions. Weaknesses The computational complexity of the CMWU update raises concerns: Its general computation is challenging. While the authors demonstrate linear convergence under assumptions it is unclear: a) If an \u03b5approximate solution requires O(log (1\u03b5)) iterations. b) If errors accumulate over rounds leading to higher iteration counts. The assumptions related to the maximum eta require clarification for implementing the regret bound. The deduction rule is generalized but its significance is unclear. The paper lacks clarity for readers unfamiliar with the field. Questions Can CMWU be applied to learning with expert problems If so what is its regret How does CMWU relate to OMWU in expert learning", "Paraphrase: This study introduces the Clairvoyant Multiplicative Weight Update (CMWU) a simplified version of Optimistic MWU capable of solving generalsum games with a convergence rate of log(t)t. It follows the concept of the proximal point method by computing the fixed point of an exponential weight update. If all players follow this update rule their average iterates approach an approximate Correlated Coarse Correlated Equilibrium (CCE) with accuracy proportional to the step size. CMWU approximates the fixed point using a smaller step size because the exponential weight update becomes a contraction mapping. Players synchronize to find the fixed point with accuracy up to 1t in t steps making the process efficient and decoupled. Strengths: Simple implementation and analysis Stateoftheart convergence rate for CCE in a distributed setting Weaknesses: Requires stronger coordination among players than Optimistic MWU which may reduce its practicality. For instance the decoupled version of CMWU requires synchronized updates of a shared variable which could fail or be subject to timing differences. Unlike Optimistic MWU CMWU is not a noregret algorithm against arbitrary adversaries. Questions: What happens if synchronization for the decoupled version of CMWU fails or players use different update periods Is there a computational complexity lower bound for this setting potentially limited to a specific class of algorithms"], "uOQNvEfjpaC": ["Paraphrase: Summary: This paper introduces a novel solution for phrase grounding in opensource environments with minimal supervision. The proposed approach utilizes only image input leveraging pretrained models including CLIP and a captioning model. Experimental results demonstrate the methods exceptional performance across multiple datasets. Strengths: 1. Simplicity and effectiveness: The method combines powerful pretrained models to achieve impressive results without text input. 2. Promising empirical performance: Strong results have been obtained on various benchmarks. Weaknesses: 1. Limited novelty: The paper primarily relies on established components lacking significant innovation. 2. Lack of details: The paper does not provide insights into how the method operates effectively in a weakly supervised setting. The contribution of the pretrained model (CLIP) to the generalization ability is not clear. Ablation studies are necessary to validate this. 3. Related work not considered: The paper overlooks relevant research on using pretrained models for zero or fewshot settings such as techniques involving prompts and finetuning. Questions: 1. Have the authors investigated the performance of alternative models instead of CLIP and BLIP 2. Have comparisons been made with pretrained methods such as CPTColorful Prompt Tuning Frozen or Flamingo", "Paraphrased Statement: This study employs a pretrained CLIP model for three tasks: 1. Object Localization: Identifying the location of objects in an image. 2. Phrase Grounding: Linking words in a description to specific image regions. 3. Weakly Supervised WordbyLanguage (WWbL): Generating image masks and captions. The model uses VGG to encode the image and CLIP to encode the text then creates a heat map. It employs four losses: 1. Foreground Loss: Ensures the heat map aligns with the text input for the foreground (objects). 2. Background Loss: Prevents the heat map from matching the text input for the background. 3. Relevancy Heatmap: Measures the difference between the generated heat map and CLIPs relevancy heatmap. 4. Regularization Loss: Maintains appropriate heat map values. Inference is straightforward for tasks 1 and 2 while task 3 uses an algorithm that first proposes regions and obtains captions for them. Evaluations on CUB (object localization) VG Flickr and ReferIt (phrase grounding) demonstrate the models superiority. On the proposed WWbL task it also performs well. Strengths: Leveraging CLIP for object segmentation tasks makes sense. Simple and effective design based on CLIPs assumption that objects have higher embedding similarity to the text than the background. Introduces the new WWbL task. Weaknesses: May not handle ambiguous captions well leading to incorrect segmentation. Lacks analysis of the impact of loss function coefficients and optimization strategies for different applications. Questions: How do different loss function configurations affect performance and can they be optimized for specific applications What are the limitations of the model in handling ambiguous captions", "Paraphrased Statement This paper introduces a novel task that combines image region localization with natural language descriptions. It utilizes pretrained visionlanguage models (CLIP and BLIP) to generate matching scores and candidate captions respectively. Additionally an encoderdecoder network is employed to create foreground masks for grounding. The proposed method outperforms previous approaches on three benchmark datasets demonstrating strong performance in weaklysupervised openworld settings. Strengths Provides a new perspective for the visionlanguage community and openworld applications. Presents comprehensive techniques supported by thorough experiments. Achieves stateoftheart results on multiple benchmark datasets for three evaluation tasks. Concerns Novelty: The proposed framework may not meet the innovation standards required for NeurIPS. It heavily relies on CLIPlike pretrained models and lacks significant original contributions. Furthermore it claims to be \"purely visual\" but utilizes textual information from BLIP during inference which requires further discussion. Empirical Fairness: It is unclear whether comparisons with previous methods are fair given the reliance on pretrained information from CLIP and BLIP. The potential overlap between datasets used for comparison and those involved in pretrained models should also be addressed. Writing Quality: The writing and presentation could be improved for clarity. Typos and expression issues are present throughout the paper. Examples of typos include: \"A twostage inference time procedure\" \"A twostage inference procedure\" \"during the inference test\" \"during inference\" \"a encoderdecoder architecture\" \"an encoderdecoder architecture\" \"An SGD optimizer\" \"A SGD optimizer\" Questions Elaborate on the papers novelty and contributions. Discuss the fairness of the empirical evaluations and address potential concerns regarding dataset overlap.", "Paraphrase: Summary The research proposes a task called WeaklySupervised OpenWorld PhraseGrounding which involves generating captions for image regions and then identifying the grounded caption. The paper also presents a method for training a mask generator using image and text with limited supervision (leveraging CLIP). Strengths Introduces a new task with a reasonable baseline. Develops a technique for training mask generators with weak supervision demonstrating promising performance in benchmarks like Flickr30K. Weaknesses The new task lacks novelty or significant challenges as it seems to be a combination of image captioning and localization. The method for solving the task is straightforward combining existing techniques like selective search BLIP and a localization network. The paper does not provide a clear discussion of the proposed methods novelty or its comparison to prior weaklysupervised localization methods. The use of a \"classification\" models heatmap for localization is not particularly innovative. The papers core contribution is unclear: if the task is the focus its uniqueness is not fully discussed and if the method is the contribution its comparison to existing work is lacking. Additional background information on BLIP would be beneficial. The paper should reference MDETR. GLIP [42] does not utilize CLIP initialization."], "yTJze_xm-u6": ["Paraphrase: Summary The paper introduces a method for adapting source model parameters to enhance performance in domain adaptation scenarios where no labeled target data is available. The method employs a Bayesian inference framework to introduce perturbations to the source model parameters enabling simultaneous adaptation to the target domain while preserving the models discriminability. A parameter sharing strategy is utilized for efficient adaptation. The authors establish a theoretical connection between Bayesian learning and their proposed approach. Experiments demonstrate the effectiveness of the method on various domain adaptation frameworks. Strengths The approach tackles a common issue in domain adaptation: how to adapt the source model to the target domain without compromising its performance on the source domain. The method prevents \"catastrophic forgetting\" and facilitates adaptation by directly modeling perturbations. The adaptive prior distribution for different convolutional kernels addresses the variance in kernel parameters in the pretrained model. The connection to Bayesian learning provides a theoretical foundation for the method. The approach is efficient to train and does not introduce significant additional parameters. The analysis of weight uncertainty reveals the varying levels of uncertainty in different layers which could guide future research in sourcefree domain adaptation. Weaknesses The paper assumes that neither source data nor labeled target data is available which may not always be the case. The method may degrade performance on the source domain which may not be a concern if only target domain performance is important. The assumption that the perturbation distribution has a zero mean may lead to a suboptimal solution. The parameter sharing strategy while reasonable may not be considered a significant contribution. Questions The paper does not discuss limitations of finetuning that may not apply to domain adaptation such as overfitting or the need for labeled target data. Can the authors shed light on these limitations Is the method applicable only to computer vision tasks The paper only presents an instantiation for convolutional layers.", "Paraphrased Summary: Problem: Adapting a model trained on a source domain (with labeled data) to a target domain (with unlabeled data) without access to the source data. Proposed Solution: Introducing perturbations to the source model learned through variational Bayesian inference to enable adaptation to the target domain. A parameter sharing strategy reduces the number of perturbations. Key Features: Flexible framework that integrates with existing methods. Minimal number of learnable parameters. Separates domain adaptation into invariant (source model) and varying (perturbation) parts. Theoretical justification based on Bayesian neural networks. Extensive experiments demonstrate competitive performance. Strengths: Lightweight and easy to implement with previous methods. Minimal trainable parameters. Generalization guarantees from Bayesian neural networks. Comprehensive experimental details and available code. Weaknesses: Effectiveness of variational model perturbation despite low parameter count requires further explanation. Classification accuracy may not accurately reflect domain shifts (alternative metrics suggested). Unknown effectiveness on architectures besides CNNs (e.g. ViT). Assumption of identical priors for source and perturbation model weights needs justification. Absence of results for standard sourceneeded domain adaptation setting (e.g. CDAN). Questions: 1. Explain the effectiveness of variational model perturbation with minimal parameters. 2. Use Adistance instead of classification accuracy to measure domain shifts. 3. Test effectiveness on architectures besides CNNs. 4. Provide results for standard sourceneeded domain adaptation setting. 5. Justify the assumption of identical priors for source and perturbation model weights. 6. Include results for other model perturbation implementations in the main text. 7. Investigate the impact of target dataset size on performance.", "Paraphrase: Summary: Sourcefree domain adaptation (SFDA) presents a challenge where the source data becomes unavailable after the source model is trained. Common approaches involve finetuning the source model on the target domain or incorporating adapters into it. However finetuned models tend to overfit to the target domain while adapters may struggle with large domain shifts due to frozen layers. To address these issues this work proposes perturbing the parameters of the source model to adapt to the target domain. This perturbation approach leverages variational Bayesian inference to learn domainspecific knowledge enabling the model to generalize across domains. It maintains the discriminative ability of the source model while adapting to the target domain. Strengths: Clear and compelling motivation General and adaptable framework Introduction of a novel probabilistic framework for SFDA Theoretical connection to Bayesian neural networks Comprehensive experimental validation Weaknesses: Lack of detailed description of model training or pseudocode for better clarity No mention of loss functions used in the expected likelihood Lack of explanation on optimizing the objective function in line with the local reparameterization trick Questions: Why not tune hyperparameters to fully exploit the methods potential How does the perturbation method preserve the discriminative power of the source model especially for source domain examples Why does Table 2 show 10 winners for 6 tasks What is the significance of bold numbers in the \"source\" column of Table 3", "Paraphrased Statement: This paper introduces a new technique called \"model perturbation\" for solving the \"sourcefree domain adaptation\" (SFDA) problem in machine learning. It uses a probabilistic approach to update the parameters of the source model allowing it to adapt to a new target domain without significantly changing its source domain weights. This approach is novel and technically sound and it has been tested on various benchmark datasets showing promising results in offline generalized and continual SFDA scenarios. Strengths: Utilizes a probabilistic model approach in SFDA for the first time. Efficient and compatible with other SFDA methods. Comprehensive experimental evaluation with promising results. Weaknesses: The methodology section lacks clarity especially regarding the objective function and parameter update details. Confusing descriptions in the experimental section particularly the use of the term \"training phase\" in different contexts. Experimental results only include accuracy and not variance despite the methods focus on optimizing variance. Unclear mathematical expressions such as the assumption of equal variances in the prior and posterior distributions. Insufficient explanation of the choice of zeromean isotropic Gaussian distribution for the posterior distribution family. Questions: What is the specific form of the objective function How is the main parameter \u03c3 calculated and updated Can the authors clarify the use of the term \"training phase\" in the experimental section and provide details on the source model generation process Why was the zeromean isotropic Gaussian distribution chosen for the posterior distribution"], "sWNT5lT7l9G": ["Paraphrase: Summary: Researchers have improved the ability of neural networks to generalize to new tasks using Successor Features (SFs) and Universal Successor Feature ActorCritic (USFA) algorithms. USFA combines SFs with neural networks which also generalize through function approximators. Key Innovation: This work extends USFAs by establishing theoretical bounds on generalization performance. These bounds are used to constrain the estimates of function approximators when predicting action values for new tasks. Evaluation: Experiments in simplified environments demonstrate that using these constraints outperforms USFAs. Strengths: Clear and wellwritten paper Relevance to researchers in SF and related fields Includes both theoretical and empirical findings Weaknesses: Limited scalability to larger tasks and domains Focus on toy environments which may not generalize to more complex ones Lack of comparison with alternative approaches such as Decision Transformer or GATO Question: The scalability of this approach to larger domains and tasks remains an open question.", "Paraphrase: The paper investigates how reinforcement learning can be transferred from source tasks to target tasks when reward functions change but the state and action spaces remain constant. The paper identifies a limitation in the GoalOriented Policy Iteration (GPI) algorithm with Universal Successor Feature Approximators (USFAs) for this scenario. USFAs can result in large approximation errors on target tasks if their solutions deviate significantly from those of source tasks. The paper proposes a solution to address this limitation by constraining the approximation error of the actionvalue function on new target tasks using upper and lower bounds. Experiments demonstrate improved zeroshot transfer performance compared to the baseline GPI with USFAs on synthetic scavenger tasks and robotic locomotion tasks. Strengths: Addresses an important open problem in reinforcement learning. Presents clear and rigorous writing making it easy to understand. Introduces a significant theoretical contribution extending existing results on the value of optimal policies for new tasks. Proposes an elegant method Constrained GPI to improve GPI. Weaknesses: Does not discuss the limitations of the work or provide directions for future research. Relies on prior knowledge of feature vectors which may not be practical in realworld scenarios. Questions: Can Constrained GPI be extended to scenarios where rewards are not linearly decomposable How would Constrained GPI perform using learned feature vectors instead of predefined ones Minor Clarifications: Spell out the acronyms in Figures 2 and 3. Provide a more detailed explanation for Figure 2b.", "Paraphrased Statement: The paper describes Universal Successor Feature Approximators (USFAs) which utilize the smooth transition of value functions across tasks. However USFAs can have large approximation errors for tasks with different parameter vectors. To address this the paper proposes constraining successor feature training to reduce approximation errors on unseen tasks. Surprisingly similar constraints applied at test time can also improve generalization. The proposed method enhances task generalization performance in specific domains. Strengths: The constrained GPI method has a strong theoretical foundation. It outperforms baselines in terms of task generalization. Weaknesses: The baseline methods used for comparison are limited. Including recent advancements in bilinear value network decomposition (Q functions) for generalization on novel tasks would be valuable. The evaluation is confined to a narrow range of domains (scavenger and reacher). A more comprehensive assessment on various goalconditioned tasks such as those in fetchgym would strengthen the study. Recommendation: The paper may be improved by conducting further evaluation on diverse tasks and comparing the method to more advanced baselines."], "r__gfIasEdN": ["Summary Many advanced models struggle with changes in data distribution during the prediction stage. This paper identifies that negative training examples can create bias that hinders models from handling unseen data. To address this the authors propose training two models: one for positive examples and one for negative examples. Paraphrase Identification The task involves identifying whether two sentences share the same meaning. What Has Been Done Before The authors reviewed relevant work on distribution shift debiasing and outofdistribution detection. They established the novelty of their approach. Main Contributions Identified that negative examples introduce bias limiting generalization. Proposed training separate models for positive and negative examples and automatically combining them during prediction. Developed a new method for detecting outofdistribution examples based on perplexity. Achieved stateoftheart results for outofdistribution performance while maintaining good indistribution prediction. Results The authors demonstrate their findings through experiments: Verifying the bias in negative examples. Validating the outofdistribution detection method. Showing the effectiveness of balancing positive and negative models under distribution shift. Strengths Strong empirical support for new research insights. Clear presentation and easytounderstand paper. Achieved stateoftheart results for outofdistribution performance while maintaining indistribution accuracy. Questions No questions were identified in the provided text.", "Summary Paraphrase: This research focuses on the distribution shift between training and test data in paraphrase identification tasks. It proposes methods to mitigate the bias introduced by negative examples: Training separate autoregressive language models on positive and negative examples only. Combining these models with a discriminative model and automatically determining their weights based on tokenlevel perplexity. This method is effective even without prior knowledge of the distribution gap between the training and test data. Strengths and Weaknesses Strengths: Demonstrates the vulnerability of existing methods to distribution shift. Automatically adjusts model weights for each test case based on perplexity. Weaknesses: Some formulations lack precision (specifically the weight calculation). Results presentation could be improved (e.g. avoiding redundant figures and using appropriate graphical representations). Questions Justification for using autoregressive models instead of instancebased likelihood computation. Definition of P(s2s1Y) in Eq. (4). Concatenation of s1 and s2 before perplexity calculation: How adjacency of the last and first tokens is handled. Advantages over separate scoring. Rationale for choosing the Weibull distribution and its parameters. Value of M in Eq. (8) and the implications of setting it as a large constant. Clarification of the term (12 P()) in Eq. (3) versus (P() 2).", "Paraphrased Statement: This study aims to improve paraphrase identification by addressing biases in negative example distributions across different datasets. Starting with a Bayesian formulation three solutions are proposed: 1. OutofDistribution Predictor (OODP) detects samples from different distributions. 2. Automatic Ensemble of Generative Positive and Negative Model (GAP) combines generative models for positive and negative examples. 3. Extended Ensemble with Discriminative Model (GAPX) adds a discriminative model to GAP. Experiments show that the OODPGAPGAPX model significantly outperforms other models and baselines in scenarios with distribution shifts. Strengths: Observationdriven solution. Clear and intuitive presentation. Significant improvements in diverse scenarios. Weaknesses: Bayesian formulation is not novel and overlaps with existing work. GAP and GAPX models exhibit limited empirical improvement over the simpler OODP model. Complex models with more components than OODP. Suggestions: Break down long paragraphs and focus on highlevel intuitions. Correct references to equations. Train the distribution model on validation data to avoid detecting shifts from training data. Questions: 1. Why dont GAP and GAPX models show more improvement than OODP 2. Why is the distribution model trained on validation data instead of training data", "Paraphrase: Original Statement: This paper investigates the distribution shift issue in paraphrase identification. The researchers confirm that negative examples contribute primarily to the distribution shift issue. To resolve this they create two distinct models\u2014a positive model and a negative model\u2014and combine them during inference. The weights are adjusted dynamically based on the distributions similarity between inference pairs and training pairs. Experimental results demonstrate the proposed approachs strong transfer learning capabilities. Strengths: The motivation is intriguing. The proposed technique stems from their observation that negative examples do not generalize well to outofdistribution data. They propose training two separate models for positive and negative examples to resolve this. The combination weights are adjusted dynamically during inference. The proposed model outperforms baseline models significantly. Table 1 demonstrates substantial performance enhancements over baseline methods. Weaknesses: Prominent baseline models are absent. Multitask learning is also a valuable solution to the distribution shift issue. It would be beneficial to include a baseline model that combines all tasks. Distribution shift is a wellknown and critical issue. The authors should contrast the proposed approach with relevant literature to demonstrate its effectiveness. The proposed technique is intended solely for paraphrase identification involving inpair data. It is unclear how the proposed approach would perform in various NLP tasks."], "ldl2V3vLZ5": ["Paraphrase: This paper proposes S3GC a new scalable graph clustering method. S3GC leverages both graph structure and node features by using a singlelayer Graph Neural Network (GNN) encoder. It employs selfsupervised contrastive learning which is a unique contribution to graph clustering. S3GC combines three main components: a graph convolutional encoder a random walk sampler and a contrastive loss formulation. The encoder combines graph structure and node features while the sampler captures local node neighborhoods. The loss function encourages the encoder to learn from graph neighborhood information. Strengths and Weaknesses: Strengths: Easytofollow presentation Importance of scalable clustering in practice Effective and scalable model that addresses existing limitations Can handle large graphs with billions of edges Extensive experiments with comprehensive analysis Weaknesses: S3GCs components are largely based on existing designs with some novelty in combining graph and node features. A potential error in line 214 where \"r\" may be missing from the time complexity calculation. Missing details on the running times of different methods. Questions: 1. Verify if line 214 is missing \"r\" in the time complexity equation. 2. Provide detailed running times for each method as efficiency is important for scalability. 3. Explain the discrepancy between the abstracts claim of significant outperformance and the results in Table 3 which show some cases where this may not hold. 4. Elaborate on the specific areas where S3GCs components deviate from existing designs and introduce novelty.", "Summary This study explores graph clustering with node features. While existing methods focus on graph embeddings for clustering they heavily rely on graph information which can be noisy or expensive to process. The proposed method S3GC leverages both node attributes and graph structure to create embeddings that are more easily separated into clusters. Experiments demonstrate that S3GC outperforms existing clustering algorithms and produces higherquality clusters. Strengths Comprehensive review of related works Simple and scalable method Adherence to standard experimental setup Strong performance on large datasets in comparison to baselines Clear and accessible writing style Weaknesses Limited novelty compared to previous methods (e.g. Node2Vec) Lack of systematic parameter tuning Ambiguous notation in Equation 5 Confusing naming of S3GCI in Table 1 Lack of clear evidence for observations in Section \"Setup and observations\" Arbitrary choice of parameter values in Table 1 without justification MVGRLs better performance on small datasets needs further investigation Exclusion of Gemsec as a related work despite its relevance to clusterable graph embeddings Questions Why is there a discrepancy in notation between \\alpha and alpha in Equation 5 Can the name S3GCI in Table 1 be revised for clarity How does the quality of node attributes affect DGIs performance as mentioned in the observation in Section 247 Can the choice of parameter values be justified as in the observation in Section 249 Why does MVGRL outperform S3GC on smallscale datasets What components of MVGRL contribute to its performance on smallscale datasets Why was Gemsec not included in the baselines given its relevance to the research", "Paraphrase: The paper introduces a method called S3GC (Scalable SelfSupervised Graph Clustering) that uses graph neural networks (GNNs) and node features to create clusterable representations via contrastive learning. S3GC employs a singlelayer GCN to capture feature and structural information. For structure encoding it uses a normalized adjacency matrix and a khop diffusion matrix. To apply the contrastive loss S3GC generates positive and negative samples. Positive samples are similar nodes obtained through biased random walk sampling while negative samples are randomly chosen. S3GC exhibits scalability to graphs with 100 million nodes a challenge for existing GNN approaches. It achieves improvements of up to 5 in Normalized Mutual Information (NMI). Strengths: Simplicity and intuitiveness Clarity in presentation Use of both graph and node features making it suitable for scenarios with noisy data Extensive experiments on a synthetic dataset Single GCN layer with normalized adjacency and diffusion matrix facilitating scalability and higherorder neighborhood information Space and time complexity provided for clarity Demonstrated effectiveness on various datasets Weaknesses: Potential for greater impact in an applied conference Insights on linear separability and clusterable contrastive representations are not novel Scaling using diffusion matrix has been previously explored Absence of a baseline comparing contrastive learning methods with singlelayer GCN and 1hop perturbations MVGRL may achieve similar scalability and competitiveness on larger datasets Limited performance on homophilous graphs Applicability to heterogeneous graphs is unclear Lack of theoretical justification for the method Ablation studies using only the normalized adjacencydiffusion matrix are not provided S3GC is not listed as the stateoftheart despite superior results reported for MVGRL Questions: Is there an ablation that uses only the normalized adjacencydiffusion matrix in the GCN Should MVGRL results be included in Table 1 for completeness"], "mSiPuHIP7t8": ["Paraphrase: This paper presents the GraphDE framework which combines unbiased learning with outofdistribution (OOD) detection for graph data. GraphDE uses a generative approach to model the joint distribution of data and labels along with a recognition model that determines OOD status. The loss function is meticulously designed to ensure interpretability and effectiveness. Extensive experiments demonstrate GraphDEs efficacy serving as a potential reference for future research. Strengths: Integrates unbiased learning and OOD detection resulting in OOD detection during debiased learning. Offers an interpretable loss function. Produces impressive experimental results. Weaknesses: Questions: 1. Threshold Calculation (Eq. 3): How is the threshold determined 2. Feature Space Variation (Line 107108): Can GraphDE account for variations in the feature space 3. Distribution Parameterization (Line 132): Why are both distributions parameterized by the same \u03b8 4. Typo (Line 134): Should p(XAe) be p(AXe) 5. Term in Eq. (7): Should p(e) be p(ei) 6. Scalar Assumption (Line 181182): If p(e) is a scalar why is it not canceled out in Eq. (8) 7. Constant p(e) Assumption: Are training and test data assumed to have the same p(e) 8. Label Prediction for OOD (Eq. 9): Is a label still predicted even if OOD is detected 9. Role of GraphDEv: Given that the posterior can be computed analytically why is GraphDEv outperformed by GraphDEa 10. Ablation Study (Figure 4): Despite Lcl being the core of debiased learning its removal does not impact performance. Why is this the case", "Paraphrased Statement: This paper tackles two significant challenges: 1. Training a GNN model that is not biased towards outofdistribution (OOD) samples in the training data. 2. Detecting OOD samples in test data using a single framework. To address these challenges the paper introduces GraphDE a novel method based on variational inference. It employs a probabilistic generative model to capture the distribution of both indistribution (ID) and OOD samples. Furthermore it integrates classification and OOD detection modules into a unified learning framework. Through experiments GraphDE demonstrates its effectiveness in both debiased learning and OOD detection tasks. Strengths and Weaknesses: Strengths: Tackles two relevant research problems in graph learning: debiased learning and OOD detection. Introduces a unified framework for addressing both tasks. Achieves promising results in experimental evaluations. Weaknesses: The generative model does not explicitly model the feature distribution (X) which could lead to limitations in capturing differences between ID and OOD distributions. The choice of a simple distribution (p(a1)12) to model OOD data may be insufficient to capture realworld OOD scenarios. Potential for leakage of OOD data patterns into the model during training which could affect OOD detection evaluation fairness. Limited consideration of baseline methods for comparison. Questions: Is it reasonable to designate larger graphs in datasets or images with Gaussian noise as OOD samples given that they may not be generated from a different distribution The statement that p(XAe) is composed of an ID and OOD component (Line 134) may contain a typo as the assumption is that A is generated from X.", "Paraphrased Statement Summary This study explores bias mitigation and outofdistribution (OOD) detection for graph neural networks (GNNs). The researchers present GraphDE a probabilistic generative model designed to capture distribution shifts in graph data. GraphDE consists of three modules: Recognition Model: Infers environment variables Structure Estimation Model: Detects outliers and OOD test data Classification GNN Model: Classifies graph data The study provides theoretical and experimental evidence supporting the proposed method. Strengths Clear motivations and a helpful visual representation of the problem Improved performance on tested datasets Sensitivity analysis and ablation studies for deeper understanding Weaknesses Limited novelty: The approach combines existing techniques (variational inference and GNNs) without substantial innovation Lack of efficiency analysis: Time complexity and practical time costs are not provided Incomplete experimental comparisons: Results only compare with specific baselines and do not include recent graphspecific OOD methods Unclear experimental support: Claims about mutually beneficial module interactions and specific environment variable behavior are not adequately verified Overlapping problem formulation with previous work Missing related works on graph debiased learning Questions See above for specific questions and suggestions for improvement", "Paraphrased Summary: The authors present a method to simultaneously handle outliers in training data and create a model for detecting outofdistribution (OOD) data. They propose a probability mixture model using a binary environment variable and demonstrate its effectiveness in both removing bias from learning and detecting OOD data. Strengths and Weaknesses: Strengths: The approach combines optimization for learning and OOD detection which is logical. The experiments use appropriate datasets and baselines. Ablation studies provide insights into the design choices. Weaknesses: The section introducing Propositions 1 and 2 lacks supporting evidence and unclearly explains their significance. The assumptions about the environment variables perfect recognition and model generation are too stringent. The methods performance on largescale datasets from OGB would demonstrate its scalability. The writing could benefit from editing by a native English speaker. Questions: 1. Can the method address joint distribution shift during testing not just featurecovariate shift 2. Is the assumption that the adjacency matrix is influenced by node features valid Can the method handle scenarios where this assumption doesnt hold 3. Why are Figure 3 and Table 1 presented differently and could they be visualized similarly", "Paraphrased Statement: This research paper addresses the challenge of removing bias from training data and detecting outofdistribution (OOD) samples in graph data. The authors observe that previous approaches often handle these tasks separately ignoring the connections between training outliers and OOD test samples. To address this the paper introduces GraphDE a model that unifies debiasing learning for training data and OOD detection for test data within a single probabilistic framework. Experiments using various GNN backbones demonstrate the superiority of GraphDE over existing methods. Additionally theoretical analysis supports the models effectiveness. Strengths: 1. Focuses on a significant and realworld problem. 2. Proposes a novel unified framework to tackle debiasing and OOD detection simultaneously. 3. Clear and accessible writing. 4. Strong experimental results with consistent improvements over baselines. 5. Provides theoretical justification for the model. Weaknesses: 1. Does not discuss potential limitations or scenarios where GraphDE may not perform optimally. Questions: Why does GraphDE underperform OCGIN or other baselines on specific datasets (e.g. Collab in terms of AUPR DrugOOD in terms of FPR95) Further discussion on these cases could be beneficial."], "mkEPog9HiV": ["Paraphrase: Summary: This paper introduces the Neural Sewing Machine (NSM) a method for 3D garment modeling that preserves the garments structure. Unlike previous methods that model garments based on specific classes NSM models garments as combinations of basic panels and their stitching relationships. These panels are encoded using principal component analysis (PCA) from a large dataset and the stitching relationships and 3D structure are encoded in UV position maps. Given an image NSM predicts PCA coefficients for the basic panels that correspond to the garment in the image. These coefficients are then used to predict the UV position maps using a CNNbased network. NSM incorporates several training targets to facilitate supervised learning and its reconstruction results outperformed previous methods. The structurepreserving modeling enables controllable garment editing and interpolation. Strengths: Incorporating sewing patterns into garment modeling and unifying it across basic garment classes. Outperforms previous methods in both qualitative and quantitative terms. Ablation study demonstrates the effectiveness of the proposed training targets. Controllable garment editing is an interesting application. Weaknesses: Experiments were conducted on a synthetic dataset which may not fully demonstrate the practicability of the method on realworld data. The sewing pattern topology of the basic panels is solely represented by the UV position maps which may lead to geometric issues when stitching panels. NSM appears to be limited to cloth reconstruction in the canonical space and it is unclear if it can handle realworld images of clothed humans in various poses. Typo: Table 3 headers should be \"four Lrec.\" Missing References: [1] Xiang et al. \"Monoclothcap Towards temporally coherent clothing capture from monocular rgb video\" (2020). [2] Hong et al. \"Garment4D Garment reconstruction from point cloud sequences\" (2021). [3] Bhatnagar et al. \"Combining implicit function learning and parametric models for 3d human reconstruction\" (2020). [4] Alldieck et al. \"Learning to reconstruct people in clothing from a single RGB camera\" (2019). Questions: 1. Section 4.2: The input for reconstruction from sewing patterns is the predicted PCA coefficients and the sewing pattern topology. 2. Innerpanel structurepreserving loss: This loss encourages an isometric mapping between UV and 3D coordinates by penalizing deviations from a rigid transformation between the two spaces. 3. Controllable garment editing: It is not clear from the paper if NSM allows users to draw the panel shape by hand.", "Paraphrased Statement: This research proposes a novel method leveraging sewing patterns to create 3D garments. The patterns undergo dimensionality reduction and the resulting coefficients guide the generation of masks and UV maps. These maps facilitate the construction of a 3D mesh. The proposed approach introduces unique loss functions that enforce garment behavior constraints including deformation sewing edge alignment and surface normalcy. The method excels in singleview garment reconstruction and allows for interactive editing and blending. Strengths: Innovative approach to garment modeling based on sewing patterns. Enables physically accurate and userfriendly garment generation. Weaknesses: Does not account for fabric elasticity or material properties. Limited to specific garment patterns and panel configurations. The generation of the final 3D mesh from position maps is not adequately explained. Editing functionality requires userdefined pattern input limiting its practicality.", "Paraphrased Statement: This research investigates the challenge of creating and reconstructing 3D garments a complex aspect of generative modeling. One challenge is finding a suitable representation for 3D garments that encompasses the wide range of designs encountered. This paper presents a novel approach using sewing patterns as the representation. The authors propose a framework called \"Neural Sewing Machine\" that generates garments with various topologies. They address challenges in encoding and decoding using this representation and introduce three loss functions to ensure structural integrity and shape accuracy. They argue that their method outperforms existing techniques. Strengths: Unified Sewing Pattern Encoding: The encoding scheme allows for representing garments with diverse shapes and topologies. PCA is employed to capture the inherent variations in clothing. 3D Garment Decoding: A principled approach is used to decode the encoding by learning 2D UV maps and mask maps. Structure Preserving Losses: The proposed losses are crucial for maintaining accurate garment structure. Mean Geodesic Length Error: This metric evaluates the quality of topology indicating the superiority of the proposed method. Weaknesses: Insufficient Related Work: The paper lacks a comprehensive discussion of related work making it challenging to contextualize its contributions. Unfair Comparisons: Comparisons with generalpurpose baselines are inadequate as specialized garment generation papers are not included. Limited Ablation Studies: The ablation for UV position maps could provide insights into alternative representations. Clarity and Captions: Figures lack captions and are not always clear hindering the ability to interpret results. Surface Normal Loss: The novelty of the surface normal loss is questionable as it is a wellknown concept in 3D reconstruction. Analysis of Mean Geodesic Length Error: A more thorough analysis of this metric is needed including the effects of sample size. Questions: 1. Isometric Parameterization: The statement about the 2D UV map being an isometric parameterization may be inaccurate. 2. Superiority over Physicsbased Simulators: The argument for the proposed methods superiority over physicsbased simulators is unclear. 3. Validation Set: The paper mentions using part of the data for testing without specifying a validation set which could compromise experimental evaluation. 4. Controllable Garment Editing: The procedure for controllable garment editing is not adequately explained.", "Paraphrased Statement: This research presents a cuttingedge neural sewing machine (NSM) for modeling 3D garments. The NSM is a learning system that can efficiently capture garment attributes enabling the easy creation and editing of 3D garments. It uses a unified sewing pattern encoding module to represent patterns which are then translated into a 3D garment using UV position maps with masks. Specific loss functions help maintain the inner panel structures of the garments. Extensive tests show that the NSM outperforms previous stateoftheart methods. Strengths: Using sewing patterns for 3D garment modeling is innovative and valuable aiding in reconstruction and editing. Experiments confirm the superiority of the proposed method over existing techniques. Weaknesses: The paper claims that previous methods struggle with reconstructing and manipulating complex topologies additional examples of intricate or simple shapes could be beneficial. Evaluations were only done on synthetic data it would be useful to use realworld images. The method may only reconstruct garments in canonical poses and local details (such as wrinkles) may be absent even if the input image includes them. Penetrations between the reconstructed garments and the 3D human body are not addressed during reconstruction and manipulation. Questions: The paper appears to be wellwritten providing more realworld examples as suggested in the weakness section could be an improvement."], "yoBaCtx_a3": ["Paraphrase: Summary: This study investigates identifying hidden switched linear dynamics (with a known number of modes). The proposed approach offers a computation complexity that grows linearly with the number of samples unlike the exponential growth seen in the MILP method. However it remains exponentially complex with the number of modes and the state dimension. The approach employs a tree search algorithm enhanced by a volumebased pruning technique to limit the trees depth. Strengths and Weaknesses: Weaknesses: Exponential dependence on the number of modes and state dimension persists. Lack of discussion on why the exponential dependence is unavoidable. The exponential dependence on the number of modes stems from Lemma 6. Explorations for alternatives are warranted. Comparison solely focuses on the MILP approach ignoring other related work (e.g. Ozay et al. clustering approach). Limited pseudocode (only Algorithm 1 for node expansion) hinders understanding of the entire algorithm. Questions: Algorithm 1 Line 4: How are Q1...Qm chosen Algorithm 1 Line 6: Clarification needed on the source of (xixi\u2019). Line 232: The mode j should be explicitly specified as the one corresponding to \\mu(xixi\u2019) \\in [m]. Line 224: The origin of the 2mN computation should be clarified. Table 1 Benchmark 2: Inconsistency in run times for N1000 and N10000. Line 236: The introduction of a \"stronger\" assumption should be connected to Theorem 1s reliance on it. Lemma 4 and Lemma 5: Definition of \"iteration\" and clarification of its usage in Line 257.", "Summary This work addresses the problem of inferring a switched linear system model from state trajectory data without observing the underlying mode transitions. The goal is to determine transition matrices and associate each data set transition with the corresponding mode minimizing an error criterion. The authors propose a problem relaxation and present a corresponding algorithm proving its correctness and analyzing its time complexity. The algorithm is evaluated on synthetic data a handwriting prediction task and a traffic data set. Strengths and Weaknesses Strengths: Novel problem relaxation approach Algorithm and analysis are based on established principles Weaknesses: Presentation and Quality: Language and grammar issues Formatting problems Technical arguments require better explanations Insufficient references Lack of relevant experimental details Clarity: Presentation needs improvement Algorithm description is incomplete Some arguments and explanations are difficult to follow Significance: Identifies a relevant problem in switched linear system identification Algorithm appears to be functional and efficient Comparison with prior work and state of the art is insufficient Questions Adaptability of the algorithm to output measurements Correction of an incorrect statement in Algorithm 1 Existence of leaf nodes with \u03bc\u03bc\u2217 Lower bound derivation in Lemma 5 Justification for the choice of \u03f5 in experiments Implications of using Chebychev centers instead of MVE Details on the MILP approach Additional Remarks Technical terms are not defined \"Traces\" and \"guards\" are undefined The term \"observable\" is incorrectly used Extension to SARX models is not explained in detail Supplementary material could be more explicit and detailed", "Summary The authors propose a method for learning switched linear dynamics from state transition pairs. They leverage a margin of tolerance to ensure that when the set of feasible matrices becomes too small no solution can satisfy the constraints. This enables pruning of the mixedinteger program (MIP) search tree. The tree size is bounded using cuttingplane arguments by selecting candidate solutions as the centers of ellipsoids with maximum volumes within constraint polytopes. The method is evaluated on synthetic data handwriting data and highway driving data. Strengths and Weaknesses Strengths: Wellwritten paper with clear proofs Novel idea with potential applications beyond the proposed setting Weaknesses: Ambiguity in pruning leaves due to complex constraint polytopes Questionable design decisions and experimental conclusions: SDP solvers can handle large problems so it should be explored if solving for Chebyshev centers instead of maximum volume ellipsoid centers would enhance performance. The provided error bars are not informative for timing data in one benchmark raising concerns about accuracy. The rationale for spacing handwriting data equally is not fully justified. The extracted \"behaviors\" from highway data trajectories are not clearly distinguishable. Typos and Other Issues: Typos in \"Markov\" and a confusing sentence on lines 101102 Nonstandard use of colons Minor grammatical and formatting errors Missing reference citations Figure 1 is difficult to read Questions: How are constraint polytopes volumes computed Why was handwriting data preprocessed to maintain roughly equidistant points", "Summary: This study introduces an algorithm for learning switched linear dynamical systems. The proposed algorithm exhibits linear scalability with respect to the problem size as demonstrated through evaluations using benchmark systems. Strengths: The problem is highly relevant and has been subject to ongoing research. The analysis of the proposed method is intriguing. Weaknesses: The comparisons performed are limited hindering the assessment of the proposed approach. Switched systems can also be modeled using complementarity constraints and a comparison with such techniques would be beneficial. The proposed method is currently limited to autonomous systems. It remains unclear if it can be extended to controlled systems (e.g. xk1 Axk Buk). Questions: 1. Comparison with Methods Using Complementarity Constraints: It is difficult to determine if the proposed method is more efficient than those using complementarity constraints without a direct comparison. Reference materials such as \"Contactnets by Post et al.\" and \"https:arxiv.orgpdf2203.10013.pdf\" can provide insights for such a comparison. 2. Extension to Controlled Systems: The applicability of the proposed method to controlled systems has not been explored. It remains uncertain if the method can handle systems with inputs (e.g. xk1 Axk Buk). 3. Dealing with Data Imbalance: Practical systems often exhibit imbalances in data between different modes. The proposed algorithm may be affected by such imbalances. 4. Scalability with Number of Modes: Practical systems often involve a large number of possible modes. It is unclear how the proposed method scales with respect to the number of modes in the system."], "uPyNR2yPoe": ["Paraphrase: This research introduces \"Alignment\" a novel selfsupervised intrinsic reward mechanism for reinforcement learning in decentralized settings with limited visibility. Drawing inspiration from zoological principles of selforganization Alignment aims to foster agent coordination by encouraging them to anticipate the expectations of their teammates. Experimental evaluations on multiagent particle and football environments demonstrate the superiority of Alignment over traditional sparse and curiositydriven intrinsic rewards. Strengths: Proposes a novel Alignment intrinsic reward for decentralized training with limited visibility. Demonstrates Alignments effectiveness on multiple MARL benchmarks. Weaknesses: Alignment may conflict with the decentralized training paradigm as agents cannot share their expectations. Alignments reliance on predicting other agents expectations faces challenges when those expectations differ significantly. The use of tables instead of learning curves may limit the visibility of sample efficiency comparisons. Questions: 1. How do agents communicate their expectations in a decentralized training setting 2. How does an agent reconcile contrasting expectations from its teammates 3. Why are learning curves not used to visualize performance instead of tables", "Paraphrased Statement: Summary: The authors introduce a new reward called ALIGNment which encourages agents to: Predict their own actions (ALIGNself) Predict their teammates actions (ALIGNteam) Prevent opponents from predicting their actions (ALIGNadv) Experimental results demonstrate that ALIGNment enhances coordination and zeroshot cooperation. Compared to three previous approaches ALIGNment achieves stateoftheart results in most tasks across various environments. Strengths: Clearly explained method with comprehensive experimental evaluation Consistently superior performance in most tasks Weaknesses: The term \"alignment\" has a preexisting meaning in AI potentially leading to confusion. A different term such as \"Predictability\" is recommended. Figure 1 could be simplified for better understanding. The dense result tables would benefit from aggregation and normalization. Bolding only maximum performance does not account for potential noise. Additional Questions: Did the authors train the Sparse Curioself and Curioteam methods themselves or did they obtain results from previous work If they trained them themselves they should clarify and provide evidence of equal tuning effort as for their own method.", "Paraphrased Statement: Summary: This research proposes an intrinsic reward mechanism that fosters cooperation among teammates and unpredictability towards opponents in multiagent systems with decentralized training and limited visibility. The reward is calculated by predicting future states from other agents perspectives using a dynamics model. The deviation from these predictions serves as a reward to supplement extrinsic rewards in environments with limited incentives. Strengths and Weaknesses: Strengths: Introduces a novel intrinsic reward that encourages coordinated behavior. Presents a thorough evaluation on six cooperative and competitive tasks. Demonstrates success in symmetrybreaking experiments. Weaknesses: Claims of superior performance over baselines may be exaggerated due to lack of statistical significance. Results are not consistently better than curiositydriven intrinsic rewards. Comparison to related work (e.g. Ndousse et al. 2021) is needed to assess the significance of the approach. Questions: 1. Has the intrinsic reward approach been compared to other nonintrinsic reward methods for sparse learning (e.g. Ndousse et al. 2021) 2. Under what conditions does the alignmentmisalignment approach perform well 3. In the centralized training experiments was the actual dynamics model used to calculate the reward or a proxy as in the decentralized case 4. Are there suggestions for improving performance using alignment for heterogeneous agents 5. Was zeroshot coordination evaluated with curiositydriven models", "Summary The authors propose an intrinsic reward that helps agents in multiagent settings align with the predictions of their neighbors. This reward is fully decentralized and improves coordination among agents. The proposed method is compared to curiositybased intrinsic reward approaches in various environments including Google Research football and shows performance improvements. Strengths: The paper is clear and wellwritten. The proposed method addresses the problem of enhancing predictability in multiagent reinforcement learning without requiring centralized training. Experiments demonstrate the benefits of the proposed method. Weaknesses: The method may not be novel enough for acceptance at NeurIPS. Concerns exist about the proposed method (see \"Questions\" below). Experiments are lacking on common benchmarks like SMAC and sparse environments. Questions: 1. The proposed reward encourages agents to visit predictable states. While the approximation of neighbors dynamic models is empirically effective its not exact. Can we conclude that the performance improvement is solely due to alignment enhancement or other factors 2. Since the proposed reward is opposite to noveltybased rewards could it potentially hinder exploration 3. Curiositybased approaches are generally not effective in densereward environments. Experiments should be conducted in both dense and sparse environments for a fair comparison. 4. It would be valuable to analyze the performance throughout the learning process. 5. Experiments on the SMAC environment are missing. The authors should also compare their method to curiositybased baselines like [1]. 6. In Section 4.2 the future predictions from agent js perspective are restricted by the portion of js observation that agent i can see. However in fully decentralized training agents should be unaware of each others observations.", "Paraphrased Statement: This research resolves coordination challenges in multiagent reinforcement learning specifically in decentralized training and partial observability situations. The method uses a selfsupervised reward derived from alignment. Agents are incentivized to align their actions with expectations when cooperating and to oppose expectations when competing. The reward is calculated using a dynamics model for each agent predicting the next observation based on current actions and observations. This model estimates neighboring agents expectations of each agents future state. The reward is decentralized eliminating the need for centralized training or centralized execution. Experiments show that the proposed reward improves performance across various cooperative and competitive problems in the MultiAgent Particles environment and Google Research Football. The algorithm outperforms existing baselines for both types of problems. Strengths and Weaknesses: The paper is wellwritten and provides illustrative examples of the techniques behavior. The method is straightforward and easy to implement. The empirical evaluation is comprehensive and includes comparisons with baselines. Section 5.7 highlights the benefits and limitations of alignment rewards such as performance degradation with decreased dynamics model accuracy or reduced effectiveness with heterogeneous agents. Question: Regarding the statement \"Alignment scales well with increased agent count except for heterogeneous navigation\" it is uncertain if this holds true for hundreds or thousands of agents. The summation (L159) could introduce noise hindering coordination."], "xvLWypz8p8": ["Paraphrase: The paper proposes a new generalization bound for ensembles of classifiers (voting classifiers) using a Dirichlet distribution to model the ensembles members. This bound is an alternative approach for analyzing the generalization performance of ensembles. It is theoretically sound and experimentally promising. Strengths: Creative combination of different theoretical concepts into a novel bound. Clear presentation and ease of understanding. Seemingly robust mathematical foundations (though not extensively verified). Innovative use of the Dirichlet distribution as a prior. Potential for future research directions. Weaknesses: Some minor issues with referencing that can be easily addressed. Questions regarding the initialization of ensemble member weights. Relatively high experimental error compared to other ensemble methods. Questions: 1. Why is a uniform distribution used to initialize ensemble member weights instead of the weights derived from the ensembles algorithm (e.g. equal weights for random forests) 2. What factors contribute to the performance gap between the proposed method and established ensemble methods like random forests 3. Can the generalization bound be directly applied to other voting classifiers such as AdaBoost and does the optimization of ensemble member weights affect its effectiveness", "Paraphrase: Summary: This research explores the theory of pattern classification using multiple categories. A novel PACBayesian bound is proposed for majority voting within ensembles of classifiers that are finite in size. It utilizes Dirichlet posterior distributions. Additionally a variation is suggested as the objective function for a training algorithm designed to optimize the model itself. These bounds are evaluated against leading alternatives in an empirical study. The findings demonstrate significant improvements over the state of the art. Strengths and Weaknesses: The contribution offers a substantial advancement compared to existing methods despite its close resemblance to previous work by Zantedeschi et al. (2021). The technical aspects are sound and the paper is exceptionally wellwritten (clear). Questions: Minor typos exist in the paper: Line 93: \"complimentary\" should read \"complementary\" Line 470: \"pacbayesian\" should read \"PACBayesian\"", "Paraphrase: This study extends a generalization bound for random majority votes from previous research by establishing a new margin bound for majority vote algorithms using Dirichlet priors. By connecting misclassification loss to stochastic voting loss the authors demonstrate the new bounds superior prediction capabilities. Additionally they develop an optimizable version of the bound that performs well when compared to other approaches. Strengths and Weaknesses: While complex this paper significantly advances the understanding of ensemble classifiers particularly regarding the margin justification for their generalization. The use of a stochastic voting bound to establish the majority voting result is an ingenious approach and the resulting bound outperforms previous work both theoretically and empirically. However the lack of a precise statement of Zantedeschi et al.s bound and the reason for the variations in \"our bound\" results in Figure 2 could have been addressed for clarity."], "xvlaiSHgPrC": ["Paraphrased Summary: This paper investigates the \"cost\" or privacy loss incurred when combining interactive differential privacy (DP) mechanisms concurrently compared to sequential or noninteractive combinations. The paper shows that for pure DP mechanisms the cost is the same regardless of the combination method. For approximate DP mechanisms the paper proves a similar result for the widelyused Renyi DP measure. This suggests that in the context of DP parallel (concurrent) composition does not provide any privacy advantage over sequential composition. Strengths: The results are significant and advance our understanding of DP composition. The proof techniques are intriguing and innovative. The paper is wellstructured and appears technically sound. Weaknesses: The use of two different proof techniques for different privacy measures is seen as a disadvantage. The authors intuition for the Renyi DP proof may not be clear to all readers. There are some typos and writing inconsistencies. Questions: Can a general highlevel argument be developed for comparing sequential and concurrent composition for various privacy measures Are there any implications for other approximate DP measures such as approximate zCDP Minor Writing and Typographical Errors: Line 16: [Dwork et al. 2006] instead of [Dwork et al. 2016] Line 90: (\\varepsilon\\delta) instead of (\\alpha\\varepsilon) Line 180: IT(ANb) instead of IT(NbA) Line 213: Terms missing \\in S Paragraph 216223: Proof technique differs from supplementary material Supplementary material Equation 18: Difference between Eb and \\mathcalEb Supplementary material Equation 21: Use \"max\" instead of \"max()\" Supplementary material Equation after line 513: Second term in max missing Supplementary material Equation after line 603: Terms should be M1jb and Mkjb", "Paraphrased Summary: Goal: This research explores the issue of combining systems to achieve differential privacy in parallel. Contributions: Formal proofs are provided for differential privacy guarantees when combining systems in parallel under various privacy notions. Results from Vadhan and Wang (2021) on concurrent composition with pure differential privacy are extended. Extensions include other privacy concepts like Renyi DP and zeroconcentrated DP. Strengths and Weaknesses: Strengths: Important problem with novel results. Enhances our understanding of privacy by extending sequential composition theorems to concurrent scenarios. Wellwritten and accessible to a relevant audience. Weaknesses: Needs some editing for clarity such as: Inconsistency in using \"mechanism\" and \"system.\" Missing superscript in the equation on page 2. Questions: How can an adversary leverage query history to extract additional information from a current query", "Paraphrased Statement: Summary: The paper investigates optimal composition theorems for privacy in interactive systems by building on the framework established in [Vadhan and Wang 2021]. It effectively addresses an unresolved question posed in that paper. Strengths: The paper introduces simplified definitions for interactive systems and concurrent composition which are more accessible than the original definitions proposed in [Vadhan and Wang 2021]. Questions: Additional questions and inquiries will be addressed separately.", "Paraphrased Summary: This paper investigates the simultaneous execution of privacypreserving mechanisms in an interactive setting. It establishes optimal parallel composition properties for approximate differential privacy (DP) and R\u00e9nyi differential privacy (RDP). Strengths: Addresses a significant challenge for deploying DP interactively. Expands upon existing optimal composition theorems that only apply to sequential composition. Weaknesses: The paper does not fully consider two cases in the interactive setting: The algorithm sending a message to the adversary before the mechanism. The mechanisms response to the adversarys query before the algorithms response. The immediate implication of Corollary 1 from Theorem 2 requires further justification as zCDP permits varying Renyi divergence parameters. The proof for RDP assumes a messaging order without providing a formal justification. The order lemma cited does not apply to RDP. Questions: As indicated in the weaknesses."], "xjXN3wEvCGG": ["Paraphrased Statement: The paper presents Demonstration Informed Specification Search (DISS) a method that uses simulated annealing to learn Deterministic Finite Automata (DFAs) from examples. DISS employs a complex proposal distribution called the \"Surprise Guided Sampler\" (SGS) which involves incremental path exploration and gradient estimation to identify mislabeled paths by candidate DFAs. While DISS outperforms other methods in a toy gridworld domain its complexity and limited experimental results raise concerns about its practical utility. Additionally the papers lack of motivation excessive symbolism and questionable relevance to the NeurIPS community limit its significance. Questions: What is the importance of learning DFAs from examples Can the kernel of the SGS idea be simplified and demonstrated in an abstract setting to enhance clarity", "Paraphrase: This study introduces an efficient method for deriving LTL formulas (represented as DFAs) from expert demonstrations. By recognizing that the optimal planner adheres to a maximum entropy approach the algorithm can significantly reduce its search space compared to prior methods that exhaustively consider DFAs. Strengths and Weaknesses: Strengths: Efficient inference algorithm for converting demonstrations into DFA specifications in MDP environments. Reduces the number of DFAs considered during inference. Leverages knowledge of the maximum entropy planner to refine the algorithm. Weaknesses: Additional baselines are necessary for comparison. The task could be framed as a program synthesis problem that would benefit from neurosymbolic approaches. The motivation for synthesizing DFAs specifically for goal specification should be clarified. Suggestions for Improvement: Formulate the problem as a supervised learning task and utilize pretrained models to map data points (environment DFA trajectory) to DFAs. Develop a probabilistic contextfree grammar (PCFG) to generate DFAs and use neural networks to predict their probability distributions. Elaborate on the differences between DFA synthesis for goal specification and other program synthesis tasks. Writing Changes: Provide a clearer explanation of the \"goal space\" and its limitations. Add captions to Figures 1 and 2 and include a brief description of the hypothesis space. Consider incorporating the example from reference [21] to enhance readability and provide a more intuitive explanation. Clarify the location of the light blue dotted path (line 53).", "Paraphrase: The paper explores the challenging problem of learning specific formal descriptions of tasks (i.e. DFAs) from expert demonstrations in a Markov Decision Process (MDP). This is motivated by the advantages of symbolic structures like DFAs such as better nonMarkovian goal handling goal composition and resilience to environmental variations compared to rewardbased methods. However the search space for DFAs is vast even for small examples. To address this the paper proposes DISS (DemonstrationInformed Specification Search) an approximate solution. DISS constructs a structured search space of labeled examples and tasks. A knowledgeguided hillclimbing algorithm efficiently navigates this space to identify the task that best aligns with the demonstrations. The key innovation in DISS is the \"surpriseguided sampler\" which generates labeled examples. The method demonstrates superior performance over two baselines in experiments conducted on a gridworld. Strengths: The problem of learning formal goal representations from demonstrations is significant and challenging. DISS presents a novel and promising approach for this task. Rigorous algorithmic development and introduction of DISS and SGS components. Weaknesses: The papers clarity needs improvement with omitted or assumed definitions and interchangeable terminology. The experiments are limited in scope and a deeper analysis of learned DFAs and their optimality would enhance the paper. The impact of the method is unclear outside of gridworld domains. Questions: 1. What is the definition of \\varphi(\\xi) 2. How accurate and optimal are the learned DFAs compared to ground truth 3. Are there nongridworld applications where these methods can be tested 4. What is the computational complexity of DISS and how does it compare to the baselines"], "um2BxfgkT2_": ["Paraphrased Statement: The study explores the potential of using unmodified Transformer models (like Vanilla Transformers and Vit) for graph learning tasks. The researchers propose the Soft Graph Transformer (SGT) model which uniquely utilizes edge features as tokens (instead of just node features like in previous graph learners). Identifier features are designed to capture local graph information allowing the application of Transformer techniques without additional modifications. The study also establishes the theoretical equivalence of SGT to kIGN and kWL models. Strengths: 1. Innovative use of edge features as tokens. 2. Theoretical analysis demonstrating SGTs equivalence to powerful graph models (kWL GCN). 3. Justification for the orthogonality of node identifiers. Weaknesses: 1. Lack of clear explanation for why Graph Transformers should function identically to other Transformers. 2. Empirical results do not show a clear advantage of SGT over other Graph Transformers despite its higher resource requirements. 3. Limited comparison with other Graph Transformers that may have adopted similar acceleration techniques (e.g. Linformers). Questions: 1. Do other baselines (e.g. GraphFormers) utilize Laplacian positional embeddings 2. What are the potential reasons for SGTs underperformance compared to other Graph Transformers", "Paraphrased Statement: Summary: This research introduces a novel approach for encoding graphs as sequences to enable direct application of powerful transformer architectures. Through carefully constructed positional encodings the paper demonstrates that the proposed method matches the expressive capabilities of secondorder invariant graph networks. Experiments on synthetic and realworld (PCQM4M) datasets validate several claims and show comparable performance to existing models. The proposed method has the potential to scale well to larger graphs due to its compatibility with efficient transformer implementations. Strengths and Weaknesses: Strengths: Serializes graphs for leveraging the advancements of transformer architectures. Proof of equivalence to a secondorder invariant graph network with the proposed positional encodings. Validation of theoretical claims through experiments. WeaknessesQuestions: Scalability and Adaptability: How is the node identifier matrix dimension determined and how does it scale to large graphs How are graphs of different sizes handled in a single batch Edge Directedness and Hypergraphs: Is there a difference in modeling directed and undirected edges How does the approach generalize to modeling higherorder hypergraphs Experimental Results: Limited experimental evaluation to only one realworld dataset (PCQM4M). Proposed method underperforms existing graph transformers on PCQM4M despite the claimed potential for computational efficiency. Lack of discussion on hyperparameter tuning and analysis of performance gaps. Other Questions: Can the proposed method apply to other nodelevel or edgelevel tasks Where can the code and data used to generate the experimental results be found", "Paraphrase: This paper introduces the Soft Graph Transformer (SGT) a variation on graph transformers. It treats nodes and edges as individual tokens enhanced with token embeddings. Theoretically the authors demonstrate that this method is at least as expressive as invariant graph networks which are already more expressive than messagepassing graph neural networks. SGT outperforms all graph neural networks and rivals transformer variations with specialized graphspecific architectures. Strengths: Clear motivation: Applying standard Transformers directly to graphs. Simple yet effective algorithm proven theoretically. Comprehensive theoretical analysis. Weaknesses: Limited experimental evaluation (only one dataset considered). Lack of ablation studies to assess the impact of specific components (e.g. token embedding type). Unconvincing results that could be strengthened with expanded experiments. Questions: None.", "Paraphrase: Summary: The researchers propose using Transformers on graphs by treating nodes and edges as tokens. They show that this approach can match the expressiveness of graphinvariant networks. Adding node and type identifiers to the selfattention mechanism allows it to approximate any permutationinvariant linear operator on the graph. The proposed Soft Graph Transformer (SGT) model uses orthogonal random features and Laplacian feature vectors for node identifiers. SGT outperforms a GNN baseline on the PCQM4Mv2 dataset. Strengths: Innovation in treating nodes and edges as tokens and incorporating node and type identifiers. Demonstration of SGTs equivalence in expressiveness to graphinvariant networks with equivariant linear layers. Weakness: Evaluation on a limited dataset (PCQM4Mv2). Questions: It is unclear if the node and type identifiers are learned or predefined."], "peZSbfNnBp4": ["Paraphrased Summary: This study introduces techniques to enhance the accuracy of machine learning models evaluated on test sets from distinct domains. It employs methods such as weighted model averaging and ensemble strategies. Experimental results demonstrate the effectiveness of the proposed approach. Strengths and Weaknesses: Originality: While the methods are not novel their application to domain generalization is innovative. Quality: The approach is sound and comprehensive. However some experimental arguments require further substantiation. Clarity: The presentation requires improvement. Excessive crossreferencing hinders readability and the experimental details dispersed across multiple appendices make it difficult to assess the works reproducibility. Significance: Based on experiments the ensemble method outperforms the current stateoftheart while the model averaging method shows comparable performance but offers computational efficiency. The practical utility of these results in realworld applications is evident. Questions: Ablation Study 1: The claim that initializing averaging close to model initialization improves outdomain performance is not sufficiently supported by experimental results. Ensemble Analysis: The authors suggest that the output of an averaged model is comparable to the model evaluated at the average model parameters. Justification for the improved performance of this approach is lacking. Model Selection: The paper contends that indomain validation performance is not reliable for model selection in a pool of trained models. The authors provide no guidance on how to select models in practice. Spearman Correlation: The table shows negative correlation between early stopping and spearman correlation for some datasets. The claim that higher correlation is better is contradicted by the results. Proofreading Errors: Several minor errors in formatting and grammar exist.", "Summary This paper introduces two innovative models for enhancing outofdistribution (OOD) generalization performance in DomainBed benchmark. These models are evaluated against various pretrained encoders and existing baselines. Model 1: Stochastic Model Averaging (SMA) Introduces a novel parameter averaging strategy during training inspired by the optimization technique of tail averaging. Averages parameters from a predetermined starting point. Eliminates hyperparameter tuning associated with Stochastic Weight Averaging (SWA). Model 2: Ensemble of SMAs (EoA) Ensembles multiple SMAs. Achieves stateoftheart (SoTA) results on DomainBed and mitigates instability in OOD performance during training. Additional Contributions Proposes a new model selection procedure for SMA that leverages early stopping and inference with SMA rather than with the online model. Presents a BiasVariance decomposition adapted to a domain generalization setting to explain the effectiveness of ensembling and MA. Strengths Novel concept of ensembling SMAs. SoTA results for EoA on DomainBed. Extensive empirical validation and ablation studies. Stable OOD performance of MA models compared to online models. Practical benefits of SMA and the model selection procedure. Theoretical insights into the success of ensembling and MA. Weaknesses Limited novelty of SMA and the model selection procedure as they are based on existing ideas. Ensembling SMAs is mostly supported by empirical results without a theoretical explanation. Theoretical analysis has limitations related to applicability to the experimental setup. Unjustified statements in the theoretical section including the superiority of EoA over standard ensembling. Fairness of comparison to baselines (EoA and the ensemble baseline see more training data). Questions How realistic is the theoretical analysis regarding the experimental setup Why does EoA outperform traditional ensembling theoretically How valid is the second order approximation used to analyze the behavior of tail averaging Additional Experiments Suggested Comparison of SMA to SWA or SWAD indomain. Evaluation of EoA on a fairer trainingvalidation split.", "Paraphrased Summary: This paper proposes a method of \"simple model averaging\" during training which significantly enhances the correlation between indomain validation accuracy and outofdomain test accuracy. Moreover ensemble models created using these averaged models exhibit even higher rank correlations compared to ensembles of nonaveraged models. The method improves performance on the DomainBed benchmark where reliable model selection is crucial for domain generalization. Strengths: Clear and comprehensive analysis Extensive experiments with diverse datasets Straightforward and effective method (model averaging or learning rate decay) Weaknesses: The connection between simple moving average (SMA) and learning rate decay is not explicitly mentioned. The method is not entirely hyperparameterfree as it requires specifying a starting iteration (t0) and an averaging frequency. The impact of SMA or learning rate decay on other domain generalization methods remains unexplored. The selection of t0 for models trained from scratch is not addressed. Some results are relegated to an appendix instead of being included in the main paper. Questions: Does SMA or learning rate decay benefit other domain generalization techniques such as IRM VREx CORAL etc. How should t0 be selected for models trained from scratch without pretraining Could Table 789 be incorporated into the main paper as they provide valuable insights", "Paraphrased Statement: Summary: The authors demonstrate that chaotic behavior occurs even during the training optimization of a single model. They propose a simple model averaging technique that substantially enhances domain generalization performance. They also show that moving average ensembles of models from multiple runs can further boost performance. The authors present results on several popular datasets to illustrate the effectiveness of their approach. Strengths: 1. Figure 1 effectively conveys the authors motivation and intuition. 2. The paper is wellwritten and the authors clearly demonstrate the efficacy of their method despite its simplicity. Weaknesses: 1. The paper should discuss the complexity and run times of the proposed method compared to existing approaches. 2. The authors should acknowledge the limitations of their approach. Questions: 1. SMA Abbreviation: The abbreviation \"SMA\" is used before its definition in Line 54. 2. Results on PACS Dataset: The authors present average results on the PACS dataset. However the perdomain results in the appendix show significantly worse performance on Photos. Is this surprising or expected and could some discussion on this be provided 3. Time Complexity of SMA and EoA: It appears that EoA may have higher time complexity than SMA but the performance gains are not substantially greater. Can the authors comment on this 4. Combination with Other Methods: Would applying SMA or EoA on top of existing methods like MIRO improve performance Are there any limitations to combining SMA with other approaches 5. Scalability for Large Number of Domains: How would the approach scale when the number of domains is significantly large Do the authors anticipate any issues 6. Typo in Figure 1: The right yaxis label in Figure 1 should be \"training\" instead of \"trainig.\""], "oprTuM8F3dt": ["Paraphrased Statement: This research introduces a strategy that incorporates a pretrained codebook into an implicit neural representation for 3D scenes. It utilizes two attention modules termed codebook attention and coordinate attention to leverage the pretrained knowledge. This approach known as Codebook Coordinate Attentional Implicit Neural Representation (CoCoINR) outperforms existing methods on the DTU BlendedMVS and H3DS datasets under two evaluation settings: \"sparse views\" (1632 views) and \"few views\" (58 views). Strengths: The integration of a pretrained codebook with attention modules is innovative. The visual quality of the results is exceptional. CoCoINR demonstrates improved performance compared to benchmarks in the experiments conducted. Weaknesses: The comparison with stateoftheart methods is limited to \"sparse views\" and \"few views.\" More comprehensive evaluation is needed. Its unclear whether the improved performance solely stems from the model or the simultaneity of model and benchmark design. The role of the codebook in the models performance needs further clarification. The ablation study is qualitative and could benefit from more quantitative results and additional ablations. The clarity of the architecture description could be enhanced. Questions: Why use a VQGAN pretrained codebook designed for 2D images in a 3D reconstruction task Does the codebook contribute more to color generation or object shape representation Can the model be improved by reducing MLP layers or utilizing a smaller codebook size What is the impact of removing the \"skip connection\" in the \"Implicit Neural Representation\" module Could adding more coordinate attention modules to the \"Implicit Neural Representation\" and fewer to the \"Neural Renderer\" enhance performance", "Paraphrased Statement: Summary: This study aims to enhance the effectiveness of training coordinatebased representations by minimizing camera viewpoints required for training. The authors introduce a codebook attention module and a coordinate attention module to provide additional information to implicit representations. The effectiveness of the method is evaluated through novel view synthesis tasks demonstrating improvements over existing methods on selected datasets. Strengths: The concept of incorporating prior knowledge into implicit representation is innovative. The paper is wellorganized and easy to understand. Weaknesses: While there are improvements over baseline methods they appear to be modest. Quantitative and qualitative results for specific datasets (e.g. DTU dataset in Table 1 and Figure 2) exhibit discrepancies requiring further explanation. Ablation studies are not included limiting the ability to determine the specific contributions of the proposed modules. Information on model complexity (e.g. number of weights) is lacking which could provide insights into potential overfitting. Questions: Why is there a discrepancy between the quantitative results in Table 1 and the qualitative results in Figure 2 for the DTU dataset", "Paraphrase: Summary: This study proposes a technique for using a pretrained ImageNet codebook (not specific to specific scenes) to create neural 3D representations like NeRFs. A codebook attention module turns the codebook into a scene descriptor while a coordinate attention module combines it with positional encoding features. The idea is that this encourages the network to understand the semantic link between the input point and the scene allowing for \"extrapolation.\" Strengths and Weaknesses: Strengths: Shows that the codebook method used in VQGAN and VQVAE can be used for 3D representation learning expanding 2D ImageNetbased features to 3D. The extension to 3D is notable even if the architecture appears to be incremental. Weaknesses: No quantitative ablation study. Experiments on \"perscene learnable features vs. fixed codebook\" and \"impact of codebook attention\" are qualitative only. Improved results from VolSDF could be due to better initialization or more computation. It would be useful to empirically test novel views vs. observed views. Questions: How were the baseline MLP features obtained (e.g. as compared in Figure 4)", "Summary: CoCoINR is a novel neural implicit representation model that leverages pretrained image prototypes to reconstruct realistic images and geometry using sparse multiview images. It employs attention modules (codebook and coordinate attention) to incorporate scene priors into the learning process enabling the generation of highquality results with fewer images. Strengths: 1. Integration of a VQ codebook into NeRF training leads to improved geometry reconstruction. 2. Codebook attention effectively introduces scene priors into the learning process. 3. Clear and easytounderstand writing. Weaknesses: 1. Subset view selection criteria are unclear. 2. Extrapolated view synthesis results are not shown. 3. Lack of comparison to related methods (e.g. PixelNeRF GRF) that introduce external priors. 4. Limited exploration of the impact of codebook size and richness on performance. 5. Unclear how CoCoNeRF handles background regions and how attention mechanisms contribute to background geometry. 6. Minor discrepancy in the number of images used for Figure 2. Questions: 1. Compare CoCoINR with existing methods that incorporate scene priors (e.g. PixelNeRF GRF). 2. Demonstrate how CoCoVolSDF and CoCoNeRF separately represent the scene and quantify their individual improvements. 3. Provide more details on subset view selection and consider showing extrapolated view synthesis results. 4. Conduct ablation studies on hyperparameters."], "sL7XH6-V21e": ["Paraphrase: Researchers present a theoretical analysis of deep random forests that combine layers of random forests created using Breimans method into a fully random forest. In a regression setting the output of each layer is the average label of samples within each leaf. Each layer uses a combination of input features and outputs from previous layers. The authors focus on twolayer deep forests and provide theoretical results to explain their empirical performance. They show that adding multiple layers enhances performance by isolating new features that contribute to the regression function (Proposition 1) both in infinite and finite sample sizes. Proposition 2 demonstrates that the variation in the true regression function within a cell is bounded with probability as the sample size increases. Theorem 3 confirms the consistency of traditional random forests and establishes the consistency of twolayer deep forests (if the sample size per tree outpaces the number of leaves). Theorem 4 compares the convergence rates to an infinite forest for the original Breiman forest (O(1M)) and the twolayer random forest (O(1M2)). Assumptions 1 and 2 are employed: Assumption 1 simplifies the analysis but may have limited realworld applicability. Assumption 2 ensures that the influence of individual samples diminishes as the sample size grows. Experiments on simulated data support the theoretical claims. Strengths Clear presentation accessible to nonexperts. Welldefined concepts. Theoretical justification for empirical observations. Contribution to the understanding of deep forests. Weaknesses Assumptions lack realworld analysis and motivation. Equations 14 and 15 have missing parentheses. Questions Can the assumptions be further explained and their applicability in realworld settings be discussed How can the assumptions be relaxed while maintaining theoretical results Can the limitations of the assumptions be experimentally tested", "Paraphrase: Summary: This paper investigates the correlation between forest depth and tree count in deep forests a type of machine learning model that combines deep neural networks and random forests. The authors use experiments to validate their analysis. Strengths: Clear introduction and background information Mathematical analysis and derivations Weaknesses: Limited analysis of depth and tree count for novelty Lack of experiments on common tasks like image recognition or natural language processing After Rebuttal: The authors revised their paper based on reviewer feedback. While the authors acknowledge the importance of deep forests the reviewer expresses uncertainty about the generalizability of their findings. The authors address this by adding results from experiments on realworld datasets. However the reviewer suggests showing how much the existing best results on the housing dataset can be improved using the authors findings. Considering the authors response the reviewer raises the score to borderline accept acknowledging the authors responsiveness and the potential importance of their findings. Questions: 1. Why is the analysis of forest depth and tree count particularly significant 2. Are the experiments in the paper applicable to realworld applications in image recognition or natural language processing", "Paraphrase: Summary: The \"Depth is More Powerful than Width in Deep Forest\" paper analyzes the behavior of Deep Forest (DF) and proves its consistency at a rate of O(1M2) where M is the number of trees in the forest. The study examines twolayered DF and its convergence in two scenarios: one with significantly more data points than leaves and another with comparable numbers. The paper presents three theorems: one establishing DFs consistency and two describing its convergence speed in terms of tree count demonstrating that \"depth is more powerful than width.\" Experiments support the claim. Strengths and Weaknesses: The paper is wellwritten addressing a crucial topic. DF is a relatively recent ensemble approach that has received limited theoretical attention. The paper extends established Random Forest theory to DF effectively providing valuable insights. The results align with expectations and underscore the importance of rigorous theoretical analysis. Surprisingly the papers findings appear applicable to practical DF implementation. Questions: 1. Can Theorem 24 be applied recursively to multilayered DF 2. If so does this improve the convergence rate to O(1M3) O(1M4) and so on with each layer 3. Is this an unusually strong conclusion"], "wwyiEyK-G5D": ["Paraphrased Statement: This research enhances the KAT model for Visual Question Answering tasks that require external knowledge (OKVQA) by incorporating regional visual representations. This approach achieves superior performance compared to previous methods. Regional tags are utilized to retrieve more relevant implicit knowledge from GPT3. Additionally regional features are employed to extract explicit information from WikiData which serves as an extra cue for the FiD answer generator. Strengths: 1. Addresses the lack of regional features in existing SOTA approaches. 2. Introduces regional descriptions in various formats for different stages of the REVIVE framework enhancing implicit knowledge retrieval and answer generation. 3. Empirical results demonstrate the effectiveness of incorporating regional features. Weaknesses: 1. Lack of clarity in explaining how regional features contribute to the improved performance. 2. The qualitative example in Figure 3 raises concerns about potential misleading information introduced by regional tags. 3. It remains unclear how explicit knowledge from WikiData is aligned with the question. Originality and Quality: The paper presents an innovative approach that utilizes regional features to improve OKVQA performance. The writing is clear and concise. Significance: While the idea of using regional features is significant further clarification on their impact is needed. Additionally it would be beneficial to conduct an ablation study to isolate the contribution of regional descriptions and WikiData knowledge.", "Summary This research explores a knowledgebased visual question answering (VQA) system that leverages external knowledge bases to answer questions based on a given image. The model utilizes powerful pretrained models (such as CLIP GPT3 and Vinyl) to process visual features generate captions retrieve external knowledge and predict answers. The paper emphasizes the value of visual information in knowledgebased VQA highlighting that previous approaches have often overlooked this aspect. Strengths The models ability to effectively incorporate visual information sets it apart from previous knowledgebased VQA methods. The evaluation on the OKVQA dataset demonstrates significant performance improvements showcasing the benefits of using visual features. An ablation study confirms the importance of each component in the models architecture. Weaknesses While the paper acknowledges that some previous research has incorporated visual information in knowledgebased VQA prediction modules it would be beneficial to more thoroughly discuss and compare these existing methods. The text in the figures could benefit from improved resolution to enhance readability. Questions Was a comparison made between GLIP and Faster RCNN object detectors to assess their impact on final performance Are plans in place to release the code and models for public access", "Paraphrased Summary: The authors identify two limitations in current knowledgebased VQA methods: 1. Visual features are extracted without considering the relationships between object regions. 2. Visual features are not fully utilized in the final answering model. To address these limitations they propose REVIVE a method that leverages object region information both in knowledge retrieval and the answering model. Experimental Results: REVIVE achieves stateoftheart performance on the OKVQA dataset. Strengths and Weaknesses: Strengths: REVIVE outperforms existing methods. The authors conduct thorough analyses of design choices and hyperparameters. Weaknesses: REVIVE lacks significant technical novelty relying primarily on engineering tricks. The most effective components in REVIVE are not novel including explicit knowledge retrieval (KAT) and ensembling. Other proposed components show minimal performance improvements. Additional Questions: Explain the necessity of each component in REVIVEs methodology. Provide technical justifications for why each component is required for improved performance."], "vaxPmiHE3S": ["Paraphrased Paper Highlights This paper introduces the EGRU model a continuoustime eventbased variant of the GRU network designed for energy efficiency in deep learning. The EGRU unit produces events only when its internal states exceed a customizable threshold resulting in sparse activity. The authors developed backpropagation algorithms for EGRU training and expanded it to a discretetime version for ease of implementation in popular machine learning frameworks. Experimental evaluations demonstrate that EGRU achieves performance comparable to current recurrent neural network architectures on various tasks. Strengths The event generation mechanism in EGRU significantly decreases computational operations while maintaining comparable performance to the more computationally demanding GRU model. Weaknesses Limited exploration of the delaycopy task with EGRU. Impact of input size on gesture prediction accuracy with EGRU is unclear. Performance of language modeling tasks decreases with increasing EGRU units. Questions Can EGRU replace LSTMGRU models to reduce computational complexity in computer vision and natural language processing applications Is the event generation mechanism applicable to the LSTM model", "Paraphrased Statement: Summary: This paper introduces EGRU an eventbased Gate Recurrent Unit (GRU) designed for efficient inference and learning in scenarios with sparse activity. By only activating a subset of units at any given time EGRU reduces computational requirements. Strengths and Weaknesses: Strengths: Originality: EGRU is a novel and unique idea in the field of sparse neural networks. Quality: The paper provides sound technical arguments and supports its claims with theoretical analysis and experimental results. It thoroughly discusses related work and covers a wide range of tasks in its experiments. Clarity: The paper is wellwritten and organized making it easy to understand. Significance: EGRU has the potential to improve energy efficiency and scalability by reducing computational costs for both inference and training. Weaknesses: Comparison to Pruning: The paper does not explicitly compare EGRUs activity sparsity during inference to other methods such as pruning a GRU as described in \"EXPLORING SPARSITY IN RECURRENT NEURAL NETWORKS.\" This could provide further insights into EGRUs efficiency in sparse settings.", "Paraphrased Summary: This study presents EGRU a unique eventbased continuoustime variation of gated recurrent models. To train this system the researchers developed an eventbased form of backpropagation. Extensive evaluations across various benchmarks demonstrate that EGRU performs comparably to stateoftheart recurrent models and exhibits significant activity sparsity during training and inference. Strengths and Weaknesses: Strengths: Novel approach Wellwritten paper Thorough experimentation Weaknesses: Ablation study not provided Lack of specific details in the experimental section Key citations omitted Questions: Comparison should include models from [15] which proposed sparse RTRL for addressing scalability issues in neural networks. The paper should demonstrate the efficiency of EGRU compared to other sparse RNNs. Hyperparameter optimization details are unclear. Provide statistical significance by reporting average performance and standard error over multiple trials. Discuss the robustness and stability of EGRU across different settings and seeds. Quantify the computational efficiency of EGRU in FLOPS for training and inference in comparison to other RNNs."], "wfKbtSjHA6F": ["Paraphrased Summary: This study demonstrates the effectiveness of lottery tickets for tasks with limited data. Through experiments the authors establish that sparse lottery tickets outperform dense networks of comparable size or randomly pruned networks in such scenarios. Moreover they found that lottery tickets are resilient to changes in data distribution caused by domain shifts or synthetic transformations. While most results are based on the CIFAR10 dataset the authors also considered other small datasets with diverse applications including imbalanced classes. Strengths: Novel application of lottery tickets in datascarce settings. Rigorous experiments provide convincing evidence for the efficacy of lottery tickets. Consideration of lottery tickets in combination with data augmentation techniques for limited data. Clear and wellstructured presentation. Thoughtful discussion section exploring potential reasons for lottery ticket effectiveness. Weaknesses: Limited exploration of different architectures beyond ResNet18. Expected results based on prior studies in NLP (although not in image classification). Questions: How are sparse winning tickets and robust models identified in Fig. 4 In the context of pretrained models (e.g. SimCLR) how is finetuning performed when using lottery tickets which typically involve restarting weights from initialization", "Paraphrased Statement: Summary: This study examines the effectiveness of sparse winning tickets (WTs) in scenarios with limited data. Using iterative pruning and rewinding techniques the authors train compact neural networks that outperform traditional dense networks in lowdata settings. Augmentation techniques enhance the performance of sparse WTs. Moreover the study demonstrates that sparse WTs are superior to dense models in longtailed classification tasks even when trained with autoaugmentation. Strengths: 1. The study highlights the promising fewshot learning capabilities of sparse models supported by experimental results. 2. Experiments in Section 5 reveal distinctive characteristics of sparse WTs that contribute to dataefficient learning. 3. The study employs sound experimental techniques and makes well \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043d\u044b\u0435 observations. Weaknesses: 1. While not a significant flaw the combination of iterative magnitude pruning (IMP) and augmentation appears to drive the most significant improvements for dataefficient learning. The authors could emphasize this aspect in the discussion. 2. Exploring other architectures (e.g. transformers VGG) could broaden the studys scope and determine if the findings are architecturespecific. 3. The study does not cite or discuss the work of Liu et al. [1] who also investigated pruning for fewshot learning. Overall: The study provides a valuable contribution by uncovering novel properties of sparse WTs particularly their effectiveness in dataefficient learning tasks. Questions: 1. Can the authors provide the specific parameters used for their augmentation setup", "Summary This paper investigates the use of sparse winning tickets (obtained via iterative magnitude pruning) to enhance image classification performance in lowdata settings (50100 samples per class). The study finds that sparse winning tickets outperform the full network by 1015 but only with data augmentation. Without augmentation performance is comparable to dense networks. This empirical contribution suggests that sparse winning tickets improve performance on tiny datasets. Strengths Provides a solid empirical contribution to sparse network and small data research. Raises questions for further exploration. Offers extensive experiments investigating winning tickets in various lowdata scenarios. Weaknesses Lack of indepth analysis of experimental results. Weak explanations for findings. Contribution 4 (line 60) is poorly supported. The discussion section includes additional experiments but lacks explanations for the results. Sections 5.1 and 5.2 lack explanations for the importance of network capacity connectivity and weight distribution. The implications of sparse winning tickets preserving filters and residual connections are unclear. The connection between output norms and representation generalization is not explained. The abstract introduction and contribution section should explicitly state the papers empirical nature. Potential Solutions Elaborate on the results to improve the papers depth. If not focus on identifying the phenomenon of sparse winning tickets outperforming dense networks on small datasets. Clearly indicate that the paper is purely empirical. Small Changes Replace \"recognizers\" with \"classifiers\" in the title. Explain the inductive bias provided by pruning (line 33). Improve citations (reference published versions of papers). Reorder experiments in section 3.2. Move section 3.5 (datasets) before 3.4 (Augmentation strategies). Clarify the type of memorization tested (line 220). Tone down overstatements about memorization avoidance (lines 235237). Provide a brief explanation of the CKA metric (line 316). Questions Why do winning tickets improve performance How does the number of model parameters and dataset size affect results Why is data augmentation necessary for sparse winning tickets to perform well on very small datasets Why do sparse winning tickets preserve filters and residual connections How are output norms related to representation generalization Why does data augmentation enhance winning ticket performance on 1 data scenario but not on larger datasets", "Paraphrase: Summary: This study demonstrates that neural networks with sparse connectivity generated using the lottery ticket hypothesis can effectively recognize objects even with limited datasets. Findings: Extensive experiments on various datasets show that sparse networks remain robust in lowdata conditions. Specific characteristics such as noise density preservation in early layers are observed in these networks. Strengths: The paper is wellwritten and presents novel findings. Extensive experiments provide substantial evidence. Weaknesses: The study primarily focuses on CIFAR datasets. Using a more challenging dataset like ImageNet could strengthen the results. Although the findings are interesting the methodology largely relies on existing methods reducing the novelty of the work."], "wGF5mreJVN": ["Summary: The paper introduces an agent that learns to navigate webpages by jumping between hyperlinks. Using Transformer models that analyze local content the agent selects the next link it follows. Trained through imitating random navigation patterns the model outperforms existing systems in navigation and factchecking tasks. Strengths: The system is welldesigned and effective exceeding prior systems in navigation and fact verification benchmarks. The authors provide detailed experimental analyses exploring different navigation setups and model design choices. Weaknesses: The paper lacks clarity on its technical contribution. It appears to primarily apply existing techniques (Transformer encoder and behavioral cloning training). The experiments do not offer significant insights or implications for the research community. The authors do not explain why RL approaches perform worse than the proposed RFBC method. The study is limited to a Wikipedia graph which differs from the realworld web. Questions: Do the authors have insights on why RL performs poorly compared to RFBC Suggestions for Discussion: Include references to other research on hyperlink graph navigation in Wikipedia such as: https:arxiv.orgabs2203.15827 https:arxiv.orgabs1911.10470 Highlight the difference between the proposed work and these papers focusing on its longerhorizon navigation with a target in mind.", "Paraphrase: Summary: This study explores web navigation using an agent trained to navigate through a Wikipedia graph. The agent is trained using supervised learning on sampled trajectories generated with various methods (e.g. random forward trajectories reverse trajectories shortest paths). The trained agent can efficiently navigate the Wikipedia graph and performs well on fact verification tasks. Strengths and Weaknesses: Strengths: Effective web navigation and competitive fact verification performance. Clear and wellwritten paper. Weaknesses: Lack of comparison to the relevant work of Nogueira and Cho (2016) which had a significantly lower success rate in a similar setting. The reason for this performance difference is not addressed. Questions: What is the performance of the proposed method if reinforcement learning is employed after behavioral cloning (RFBCRL) Provide more details on training such as hardware used and training time. Clarify the specifics of Table 2 line 301 and Table 4.", "Summary Paraphrase: This paper explores the web navigation issue and presents a method for behavioral cloning of random trajectories on Wikipedia graphs. The method is applied to the FEVER verification task. Experiments indicate its effectiveness in learning entity embeddings and navigating policies leading to competitive verification performance. Strengths and Weaknesses Paraphrase: Strengths: 1. The method outperforms baseline methods and scales well to large Wikipedia graphs. 2. The paper provides detailed analysis of results and limitations. Weaknesses: Major: 1. The proposed trajectories lack originality simply using random walks with probabilities determined by neighboring vertices. 2. The method is only compared to basic methods not stateoftheart alternatives making its effectiveness difficult to gauge. 3. A comprehensive efficiency analysis comparing the methods running time to others would enhance understanding. Minor: 1. Typographical errors exist such as \"Table 2\" instead of \"Table 4\" on page 8 line 301. Questions: See the major weaknesses listed above for specific questions."], "tmer8WAEzV": ["Paraphrased Statement: Summary: This research presents a modified version of Restricted Riemannian Hamiltonian Monte Carlo (RHMC). This approach is suitable for sampling from probability distributions defined indirectly through constraints on a mathematical manifold. Specifically the constraint here is represented as c(x) Ax b. This simplification allows for faster computations potentially enabling the algorithm to handle highdimensional target spaces (e.g. up to 100000 dimensions). Strengths and Weaknesses: The contribution of this work appears to be incremental as the concept of constrained HMC has been explored previously by Brubaker et al. The present algorithm can be viewed as a specialized case of Brubakers Constrained Riemann Manifold HMC (CRHMC) which is acknowledged in the paper. However the authors claim that CRHMC requires eigenvalue decomposition and becomes inefficient for large problems. This is not necessarily true as Brubakers approach does not explicitly require eigenvalue decomposition. The efficiency improvements in this work come from imposing additional assumptions and restrictions (lines 192220). To demonstrate the robustness of these assumptions more comprehensive empirical experiments would be beneficial. The current experimental results are limited (testing against a single benchmark) and only suitable for sampling from uniform distributions. Questions: It is suggested to compare the proposed algorithm against other constrained HMC variations (e.g. Brubakers or more recent CHMC implementations). It is unclear whether the experimental target densities used are uniform. If not how is the CHRR algorithm used effectively If they are uniform then the use of Riemannian HMC (which relies on gradients) is unnecessary.", "Paraphrase: This study introduces a Markov Chain Monte Carlo (MCMC) algorithm based on Constrained Hamiltonian Monte Carlo (CHMC) to sample from a logconcave distribution with constraints. The method involves computational techniques to handle highdimensional settings and exploit sparsity patterns in constraints of the form Ax \u2264 b where A is a sparse matrix. The algorithm achieves stateoftheart results in various challenging scenarios. It has also been incorporated into a popular bioinformatics library. Additional Questions and Remarks: The authors should further clarify the role of sparsity in the constraints. It is unclear whether the authors consider MCMC methods with or without acceptancerejection steps. The paper should specify whether it solely focuses on constraints of the form Ax \u2264 b or more general implicit manifolds. The use of barrier functions within HMC should be clarified for novelty. The authors should explain how they prevent the Markov chain from stepping outside the convex set under constraints. The paper does not discuss parameter tuning for the leapfrog step size and momentum refresh rate. The reported ESS should specify the functional of the trajectory being considered.", "Paraphrased Summary: The authors introduce a method for sampling from a distribution using a modified version of Riemannian Hamiltonian Monte Carlo that enforces constraints. This method combines linear algebra techniques and a Riemannian metric based on the Jacobian of the constraints and a barrier function. The effectiveness of the method is demonstrated through experiments. Strengths and Weaknesses: Originality: The method extends an existing approach making it computationally more efficient. The related work is acknowledged but briefly discussed. Quality: The technical aspects of the paper appear sound supported by theoretical analysis and experimental validation. Clarity: While the paper is generally wellwritten Section 2 requires more elaboration for nonexperts. The geometrical intuition behind the proposed metric and its construction is not sufficiently explained. Significance: The method offers an efficient solution to sampling under constraints and is likely to find use in various applications. Questions: 1. Why should the metric not be invertible 2. How are the associated null spaces relevant to the problem 3. Why is the metric constructed as in Eq. 2.6 4. How is the function \u03c6 defined in practice The authors could improve the accessibility of Section 2.1 by providing discussions of the geometrical properties and intuition behind these steps potentially including figures to enhance understanding.", "Paraphrased Statement Summary This research presents a version of Riemannian Hamiltonian Monte Carlo (RHMC) with constraints that preserves sparsity and complies with constraints. The research demonstrates that the method can efficiently sample poorly conditioned nonsmooth constrained distributions in extremely high dimensions exceeding 100000. This work is backed by theoretical findings on the accuracy computational efficiency and mixing time bounds of Constrained RHMC (CRHMC) and its discretized versions. Strengths Significance: Addresses a pressing problem in machine learning of interest to the broader community. Enables sampling of previously deemed intractable distributions. Demonstrates superior efficiency compared to existing methods in terms of convergence rate and sampling time. Quality and Clarity: Wellwritten and organized. Rigorous and highquality theoretical analysis. Extensive experiments supporting theoretical findings. Weaknesses Quality: The proof of Theorem 8 (on the correctness of the discretized CRHMC) may be flawed. It fails to account for certain conditions that are crucial for detailed balance to hold. The choice of implicit midpoint integrator over leapfrog integrator (which is volumepreserving) is not adequately explained. The paper avoids explicitly stating that CRHMC is ergodic or convergent despite relying on these concepts in later discussions. Presentation: The paper lacks a thorough review of related work making it difficult to distinguish the novel contributions of CRHMC from existing methods. The construction of constraint terms and the use of barrier functions are presented without proper references or context. Questions Is the proof of Theorem 8 correct Why is the implicit midpoint integrator preferred over the leapfrog integrator Can CRHMC be proven to be ergodic or convergent under certain conditions", "Paraphrased Statement: Summary: This study presents a method for sampling from logconcave distributions using Riemannian Hamiltonian Monte Carlo (RHMC) on compact convex sets. It includes a Lagrange multiplier for equality constraints and a Riemannian metric for inequality constraints derived from a selfconcordant barrier function. The method leverages algorithms for sparse matrix inversion for sampling from highdimensional distributions. Experimental results demonstrate significant speed improvements over previous constrained samplers. Convergence guarantees are provided bounding the convergence rate to the stationary distribution. Strengths: This paper offers strong practical results for a crucial but often overlooked issue and provides a comprehensive theoretical analysis. It follows a clear structure with helpful appendix organization. Weaknesses: The primary weakness lies in the lack of clarity regarding certain assumptions. For instance while the constraint set \\mathcalK is stated as convex it must be compact. This compactness assumption is implicit in many arguments. To ensure clarity these constraints should be explicitly mentioned in the problem setup and abstract. Additionally several minor typos and unclear arguments are present: \"MNMM\" should be \"MCMC\" in line 575. \"dynamic\" is used instead of \"dynamics\" in multiple instances e.g. line 603. \"P\" is reassigned in line 601 conflicting with the definition in Lemma 2. \\x\\A \\sqrtx\\top A x is required in line 798. Definition 5 implies \\mathcalK must be compact but this should be explicitly stated in the problem setup. The first inequality after line 967 may contain a typo. The factor in the second inequality after line 967 should be \\frac140 not 25. The last inequality after line 968 should be strict. Questions: Is it possible to extend this method to noncompact sets How can it be extended to nonconvex sets and nonlogconcave distributions What challenges arise in these extensions"], "pNHT6oBaPr8": ["Paraphrase This paper explores the \"amount of equivariance\" when the input and output domains are wholes or parts of a group. The authors propose a \"partial GCNN\" that learns a probability distribution p(u) on the group in each layer. Their method applies to discrete continuous and mixed groups. Experiments show that the partial GCNN can determine the specific group transformations required for different tasks. Strengths It acknowledges that transformations in data may be better represented by group subsets which is a novel concept. The analysis of group convolution for different domains is sound and the proposed method for obtaining the output domain by learning a group probability distribution is innovative. The paper is wellorganized and uses helpful illustrations. Weaknesses While the paper explores learning probability distributions for various group types additional experiments with groups beyond SE(2) E(2) and the flipping group would be beneficial. Equation (3) raises questions about how to obtain v1u. The paper could benefit from comparisons with a baseline equivariant network that uses a flattened layer instead of maxpooling preserving transformation information. There are minor typos and misalignments in references to figures and tables. Questions How is v1u defined in line 237 In tasks like MNIST6180 and MNIST6M can a fixed output domain be derived to improve results compared to convolution over the entire group", "Summary This paper proposes a method to learn degrees of shape preservation from data in image classification. The method can learn various levels of preservation from none to complete. Group elements which represent shape transformations are modeled as probability distributions. These distributions are learned by randomly sampling convolutions from different layers in a deep network. Experiments on toy and real data demonstrate good results that are comparable to traditional translationpreserving architectures. Strengths The paper explores a novel concept of learning partial shape preservation. The proposed method is straightforward and effective allowing the model to learn varying degrees of preservation at different network layers. The results indicate that learning partial preservation can improve performance or match that of standard models. Weaknesses The theoretical background is complex and could benefit from clearer explanations and examples. The Monte Carlo estimation is not well explained and some details from the supplementary material should be included in the main text. The use of CIFAR and MNIST datasets may limit the generalizability of the findings. Some figures and captions need improvement for readability. Section 3.3 requires further clarification. Training convergence is not illustrated and any challenges or techniques used to ensure stability should be discussed. Questions What is the base model structure used in T(2) settings Could group elements be dependent and how can this be addressed What is meant by \"mirror CNN\" in Section 5 Why are the numbers of group elements always 8 or 16", "Summary Paraphrase: The article introduces a technique for learning partial symmetries from data using a Monte Carlo group convolution with samples taken from a 1D uniform distribution. The distributions bounds are optimized using the reparametrization trick. The proposed method demonstrates improved performance in tasks such as detecting specific transformations where equivariant models struggle. It also compares favorably to baselines on image classification tasks (CIFAR10100). Strengths and Weaknesses Paraphrase: Strengths: Addresses a challenging problem: learning symmetries present in data and leveraging them. Intriguing results when each model layer exhibits equivariance to a distinct subset of the Euclidean group. Weaknesses: 1. Similarity to Augerinos [1] work which also uses reparametrization for uniform distribution bounds optimization. The difference lies in applying the technique to Monte Carlo group convolution samples rather than input augmentation. 2. Unstable training and unclear motivation for learning discrete distributions (evaluated only on a trivial 2element group of reflections). 3. Unfair comparison with Augerino in Table 2 (lower accuracy reported for the proposed method despite different architectures and training schemes). 4. Absence of evaluations on more complex tasks involving groups other than SE(2) subgroups as in Benton et al. [1] (SE(3) invariance in molecular property prediction colorspace invariance on STL10). 5. Close results in Tables 2 3 and 4 raising concerns about statistical significance (confidence intervals recommended). Questions: Further investigation into the intriguing patterns observed in Fig. 5 where layer bounds initially narrow and then widen. Clarification of the accuracy discrepancy for SE(2) 16 Partial on CIFAR10 in Tables 3 and 4. Correction of the statement in L85 that group convolutions require infinite time to compute (certain conditions allow for exact computation in finite time). Missing reference for \"Tab. \" in L331."], "lzZstLVGVGW": ["Summary Paraphrase: Researchers introduce a novel Earth system forecasting model that uses transformers. They divide the spacetime field into nonoverlapping cuboids. Within each cuboid attention is applied along with \"global vectors\" that link individual cuboids. Merging the cuboids produces predictions. The model achieves impressive results on synthetic and real datasets including moving objects precipitation forecasting and sea surface temperature anomaly forecasting. Strengths and Weakness Paraphrase: Strengths: Extensive evaluations on real and synthetic data with comprehensive ablation studies. The Nbody MNIST predictions show significant improvements in clarity compared to baselines. Weaknesses: The application of transformers to vision tasks is not particularly novel. Comparisons with physicsbased models would provide additional insights into the transformer approachs performance. Questions and Clarifications: Lines 238240: \"Onthefly generated digits\" refers to generating digits dynamically during training while this paper uses pregenerated digits for all models. Lines 184185: The \"coarsetofine procedure\" involves generating predictions at multiple spatial resolutions starting from a coarse resolution and progressively refining them to a finer resolution. Equation (9): The subscripts (abc) in Z(abc)\\textcode represent the three embedding layers (a b and c) used in the Vector Quantized Variational Autoencoder (VQVAE). Autoregressive Earthformer: It incorporates temporal correlations by considering previous time steps in the prediction process. While global vectors also consider temporal interactions autoregressive Earthformer explicitly models the sequential nature of time series data.", "Paraphrase: Summary: Authors Proposal: Spacetime Transformer: A new approach to predict events in Earth systems. Cuboid Attention Mechanism: Splits input data into nonoverlapping parts to improve efficiency by computing attention locally in parallel. Earthformer Model: Combines multiple Cuboid attention layers with varying settings. Shared Global Vectors: Bridging communication gaps in local Cuboid attention. Nbody MNIST Dataset: Synthetic dataset based on MNIST designed to resemble chaotic 3body motion. Strengths and Weaknesses: Strengths: Novelty: One of the first transformers for ICARENSO forecasting. Effectiveness: Demonstrated improvements on both synthetic and realworld Earth system datasets. Clarity: Detailed explanation and illustration of Cuboid Attention. Significance: Substantial performance enhancements. Weaknesses: Novelty: Architecture not unique within machine learning models. Comparisons: Missing comparisons with other transformerbased spacetime models. Questions: Nbody MNIST Relevance: Why is this dataset considered relevant to complex system handling Training Details: Information about input encoding and embedding dimensions is lacking.", "Summary Paraphrase: This research introduces \"cuboid attention\" a conceptual framework for understanding various spatiotemporal attention mechanisms. This framework encompasses standard selfattention and simplified variants like axial and divided spacetime attention. Moreover it proposes Earthformer a transformer model with cuboid attention blocks and global vectors that enhance information sharing among these blocks. The study also presents the Nbody MNIST dataset which features sequences of moving MNIST digits with chaotic motion. Earthformers performance surpasses existing algorithms on several datasets showcasing its effectiveness and potential as a benchmark for dynamic systems. Strengths and Weaknesses Paraphrase: Strengths: Introduces a new Nbody MNIST dataset with realistic dynamical system characteristics. Conducts a comprehensive study on the benefits of global attention vectors demonstrating significant performance improvements across attention variants. Presents a unified framework (cuboid attention) for understanding different attention mechanisms including axial and divided spacetime attention. Achieves impressive results on spatiotemporal datasets demonstrating Earthformers superiority. Weaknesses: Omits experiments on additional datasets such as action recognition for a more comprehensive evaluation. Lacks comparisons with generic models such as TimeSformer to further validate Earthformers significance. Questions: Clarifies whether the baseline model results were obtained by the Earthformer authors or the original model creators. Confirms if the Nbody MNIST dataset will be publicly released. Explains the reasoning behind naming the dataset \"Nbody MNIST\" despite only considering N3 which may create a perception of wider Nbody experiments. Clarifies the statement that \"cuboids do not communicate with each other\" in the context of sequential divided spacetime attention which would involve information exchange between cuboids.", "Summary Paraphrase: This paper introduces cuboid attention as an alternative to conventional attention for forecasting Earth systemlike data. The paper demonstrates its effectiveness on a toy Nbody MNIST dataset and two real datasets: precipitation nowcasting and El Ni\u00f1oSouthern Oscillation. Strengths: Novelty: The paper employs cuboid attention in a transformer architecture for climate forecasting. Motivation: The authors argue that existing CNNRNN architectures may not capture internal variability in climate. Weaknesses: Motivation: The paper does not justify why transformer architectures would excel at capturing internal variability. Evaluation Metrics: The paper relies on MSE and MAE metrics which may not fully capture forecasting performance in physics. Toy Model: The Nbody MNIST dataset is insufficiently representative of weather systems. Baseline Comparisons: The paper lacks thorough comparisons with appropriate baselines for the El Ni\u00f1o and nowcasting problems. Relevance to Earth System Data: The authors do not provide sufficient analysis to demonstrate the connection between the proposed architecture and its applicability to realworld climate forecasting problems. Questions: 1. Can the authors explain why transformer architectures are expected to capture internal climate variability better than other deep learning architectures 2. Can the paper be revised to include more detailed evaluation metrics and analysis of scalespecific forecasting performance 3. Can the results be directly compared against stateoftheart methods for ENSO and nowcasting 4. Can the authors provide a more comprehensive analysis of the architectures usefulness for physical earth system data even if it does not outperform some baselines"], "ogNrYe9CJlH": ["Summary The study presents a novel \"ReductiontoBinary\" (R2B) approach that ensures fairness for multiclass classification with nonbinary sensitive features. R2B converts the task into a series of binary debiasing problems proving optimality and fairness guarantees. Empirical evaluations show improvements over baselines and competitive results on synthetic and realworld datasets. Strengths Addresses multiclass classification with nonbinary sensitive attributes. Proposes a debiasing method with theoretical guarantees. Demonstrates effectiveness on datasets from various domains. Presents clear and wellstructured writing. Weaknesses Limited insights into the sources of improvement for R2B. Evaluation on relatively smallscale datasets raising questions about performance on larger datasets. Lack of runtime and training cost analysis. Comparable overall performance to other algorithms without a clear advantage. Questions Where do the improvements from R2B primarily originate Will R2Bs performance hold up on larger datasets What are the additional costs of implementing the debiasing procedure Can R2B provide a significant advantage over existing algorithms", "Paraphrased Statement: Summary: The study examines fairness in multiclass classification by converting it into a set of binary classification tasks for each class followed by combining the results. The authors employ existing methods like RTO for binary debiasing. The debiasing is applied in the preprocessing phase and the resulting \"debiased\" labels are compared with other approaches based on the DP fairness metric and model accuracy. Strengths and Weaknesses: The study expands the focus on fairness from binary to multiclass classification which is more practical. The reduction technique allows the algorithm to handle large datasets. The aggregation method (normalization) plays a crucial role but it is not clear how it prevents reintroducing bias. The experimental results show slight differences between methods making it challenging to determine the significance of the proposed technique. Questions: 1. Normalization Bias Prevention: Can the authors provide a way to confirm that the normalization method in Alg.1 does not reintroduce bias (or minimizes it) 2. Results Comparison Significance: How significant are the differences in DP scores between the proposed method and baselines (e.g. in Fig.2) and can they be considered meaningful 3. Grid Search Comparison: If the \\gamma threshold were optimized using a grid search instead of being set to a hard threshold would the R2B method still outperform other baselines like HTO 4. Concerns: The yaxis in Fig.3 is incorrect with a maximum value of 0.1 (or 110 for other values). The paper should define concepts like \"label debiasing\" and \"debiased label\" before using them. The placement of \"2\" in the l2 distance formula in line 149 appears incorrect. In line 155 \"correctly classifying\" should be replaced with \"misclassifying.\"", "Paraphrased Statement: Summary: The authors have devised a technique to extend existing debiasing methods to multiclass classification tasks. They treat the loss function as an L2 regression loss constraint using traditional differential privacy (DP). The optimization process leverages the ADMM algorithm where each iteration involves solving a binary classification subproblem. The theoretical analysis demonstrates the methods optimality and bounds on bias. The effectiveness is validated through experiments on synthetic and realworld datasets showing a reduction in bias without compromising accuracy. Strengths and Weaknesses: Strengths: Clear organization and readability Novel concept of converting multiclass problems into binary ones Extensive empirical evaluations Weaknesses: Reliance on L2 regression loss function instead of the popular crossentropy loss potentially impacting applicability. Concerns about the optimality theorems accuracy. Questions and Ambiguities: Line 140: \"Minimizing the expected 01 error\" should be \"maximizing the linear...\" Line 148: The reference (Yang et al. 2020) does not support the claim that squared loss is statistically consistent. Line 155: The phrase \"probability of misclassifying...\" should be corrected to \"correctly classifying...\" Inconsistency in table formatting: Boldface is applied selectively making it difficult to identify the best methods.", "Paraphrase: Summary: This research introduces a \"reductiontobinary\" method that promotes demographic fairness in multiclass classification tasks with nonbinary protected attributes. The method leverages the Alternating Direction Method of Multipliers (ADMM) to break down the task into smaller parallel subtasks. The effectiveness of the method has been demonstrated through experiments on various datasets. Strengths: Offers a preprocessing technique for fair multiclass classification with nonbinary protected attributes. Demonstrates the methods performance through experiments. Provides theoretical insights into the algorithm. Weaknesses: Limited novelty primarily extending the RTO method using ADMM. May not guarantee true fairness due to its reliance on demographic parity alone. Ignores the impact of label noise during label debiasing. Experiments are insufficient and focus on relatively simple training settings. Fails to address the performance of the method on largescale datasets.", "Paraphrased Statement: Summary: This research delves into addressing bias in models used for classifying multiple classes. The approach devised utilizes an established optimization algorithm and breaks down the debiasing process into smaller tasks for each class. This is followed by a step that consolidates the results. Experiments conducted on artificial and realworld datasets yielded encouraging outcomes. Strengths: Introduces a novel method to reduce bias in multiclassification models. Offers both theoretical foundations and practical results. Examines both traditional machine learning models and deep learning models. Weaknesses: While the proposed method (R2B) reduces demographic bias it may lead to a decline in accuracy particularly when dealing with a small number of classes (as seen in Figure 2 and Table 1). Error parity or the uniformity of error rates across groups can worsen in cases where the dataset exhibits significant bias. The research could benefit from an indepth analysis of how the methods effectiveness varies across different machine learning model families."], "yW5zeRSFdZ": ["Paraphrase: Summary: This study investigates quantizing Transformers for BERT models. It identifies that quantizing LayerNorm parameters can lead to performance degradation due to outliers. The authors propose a \"Gamma Migration\" technique to address this issue and a \"coarsetofine\" algorithm to identify outliers during token clipping. Strengths: Clear and wellstructured paper. Comprehensive references. Innovative technical approaches. Weaknesses: Lack of key information in results such as model parameter count. Limited experiments on different model sizes to assess the proposed approachs generalizability. Questions: Model Parameter Count: What is the total number of parameters in the models used in the experiments Impact of Quantization Paradigm: Could the authors test 4bit quantization with different bit allocations (e.g. 488 or 466) for PTQ This could provide insights into the memory reduction potential of 4bit quantization while maintaining performance.", "Summary: This research aims to improve the handling of outliers in special tokens which are crucial for quantization accuracy. The paper highlights the role of the gamma variable in LayerNorm in amplifying outliers. To address this issue an outlier suppression framework is proposed which includes Gamma Migration and Tokenwise Clipping. Experimental results demonstrate that the framework surpasses existing quantization methods and restores fullprecision BERT performance using 6bit posttraining quantization and 4bit quantizationaware training models. Strengths: The approach is grounded in the analysis of layer norm outliers. Extensive experiments show strong empirical performance across various tasks. The analysis and visualizations are informative and wellpresented. Weaknesses: One figure is missing from the paper. Some results lack justification such as the complementary nature of Gamma Migration and TokenWise Clipping. Questions: How do the findings of this paper compare to the dynamic scaling approach presented in \"Compression of Generative Pretrained Language Models via Quantization\" The paper repeatedly references Figure 2 but it is missing from the text.", "Summary Paraphrase: This paper aims to address the challenge of quantizing transformers particularly the issue of outliers. It analyzes the origin of outliers and identifies the scaling parameter in layer normalization (gamma) as an amplifier. To address this the paper proposes Gamma Migration which moves gammarelated computations to later layers. Tokenwise clipping is also introduced to mitigate outliers using a coarsetofine pipeline. The approach improves quantization performance over a baseline. Strengths and Weaknesses Paraphrase: Strengths: Empirically strong results including highquality 6bit PTQ results on GLUE. Relevance to the development and deployment of pretrained transformer language models. Motivated methods with intermediate quantifications. Weaknesses: Writing could be improved with undefined acronyms. Limited novelty as the ablation study suggests Gamma Migration has limited impact. Unclear if the analysis and methods are specific to text transformers or generalizable to other domains. Questions: 1. Analyze why Gamma Migration has limited efficacy despite being identified as the source of the outlier problem. 2. Extend the proposed methods to GPT2. 3. Evaluate if the analysis and methods apply to vision transformers or multimodal models. 4. Assess the performance of the methods on generation tasks such as BART."], "pF5aR69c9c": ["Paraphrase: Most previous onpolicy reinforcement learning methods restricted the distance to the most recent policy. However this constraint can hinder performance if the recent policy is inferior. To address this this paper introduces MCPO an onpolicy method that employs two trust region constraints: 1. Distance to the most recent policy. 2. Distance to a \"virtual policy\" computed based on stored past policies. The virtual policy parameters are weighted averages of previous policy parameters. The weights are determined by an attention network which is trained to maximize the estimated performance of the virtual policy. Empirical evaluations demonstrate that MCPO outperforms other onpolicy methods in various environments while ablation studies reveal the importance of each MCPO component. Strengths: Constraining both the recent policy and the virtual policy. Defining and optimizing the virtual policy with an attention network. Creating a 12dimensional feature vector for attention network input. Extensive empirical results showing MCPOs superiority. Weaknesses: Heuristic design of most MCPO components such as switching mechanisms and policy memory management. Limited reasoning for performance improvements resulting from these design choices. Questions: 1. Are there more ablation study results for additional MCPO component variations 2. Is there a specific reason for selecting BipedalWalkerHardcore for the ablation experiments Are there results from other environments", "Paraphrase: The paper investigates trustregionbased policy optimization methods in deep reinforcement learning. In these methods the policy update between iterations is constrained within a trust region. The paper highlights that constraining the policy update based on the old policy could limit improvement since the old policy may not be an optimal reference point. To address this issue the paper proposes an improved method called MemoryConstrained Policy Optimization (MCPO). MCPO introduces a \"virtual trust region\" defined as the KL divergence between the target policy and a linear combination of past policies. An attentionbased mechanism is used to learn the combination coefficients based on policy context features. The paper empirically evaluates MCPO on discrete and continuous control tasks and demonstrates its performance gains. Strengths: The idea of \"extrapolating\" the trust region using a virtual one is novel and intriguing. The attentionbased mechanism for learning the trust region weights is innovative. Weaknesses: Lack of convincing evidence: The argument that the old policy can define a suboptimal trust region lacks empirical support. Lack of technical details: Critical details about the virtual trust region calculation policy update process and optimization of attention weights are missing. Lack of insightful results: The paper focuses on overall training performance rather than analyzing the role of the virtual trust region at each iteration. Questions: Q1: Can entropy regularization alone achieve similar results to MCPO Q2: How is the return from the virtual policy estimated using importance sampling Q3: How is the virtual policy computed using the old policies and the current policy Q4: How is the attentionbased mechanism (\u03c6) optimized Q5: Why is the importance sampling weight (\u03b1t) calculated without conditioning on the state Q6: How does the impact of the virtual trust region on training vary across environments", "Summary The work tackles the issue of policy optimization in deep RLs policy gradient methods. As these methods use minibatch data for updates the policy updates can stray significantly from the areas where data was collected. The approach is to create a virtual policy a weighted combination of past policies with weights learned by an attention network. The policy updates are regularized to stay within a KLdistance region around the virtual policy and the previous policy. This encourages the new policy towards the virtual policy if the previous policy is poor or stuck in a local optimum. Empirical evidence shows improved performance compared to previous methods that form trust regions or constraints. Strengths Simple and straightforward concept of a virtual policy that adapts to past policies. Attention network effectively weights different policies. Strong empirical results across various domains. Weaknesses Lack of theoretical understanding of the virtual policys impact. Uncertain whether performance improvements are solely due to the virtual policy or optimizations. Writing Overall good quality and comprehensible. Missing definition of the finitehorizon setting. Typo in line 168 (\"optimize \\theta and \\phi alternately\"). Confusion in line 178 (\"N or M (L113115)\" for the number of past policies). Results in section 4.2 are discussed in the main body but relegated to the appendix for space constraints. Questions Policy Inclusion in Virtual Policy (Eqn. 2): Concern that a large number of policies may be added to the memory due to diminishing D(\\thetaold \\psi). How do the authors address this Hopper Behavior (Lines 253254): Clarification on what is meant by \"prefers an average restriction.\" Is it related to the environment or optimization procedure Impact of Regularization: How can it be verified that the regularization is working as intended keeping the new policy closer to \\psi when the old policy is stuck or poor Visualization of Virtual Policy Effects: Request for additional handdesigned experiments to illustrate the impact of the virtual policy particularly in cases where the old policy is constrained to be a suboptimal or local optimum policy."], "yJV9zp5OKAY": ["Paraphrased Statement: This study introduces a new method for augmenting data without the need for training a separate generative model. Instead it utilizes the intermediate layer representations of the target model to generate augmentations. The method is based on representation learning and aims to retain the minimal information necessary for label preservation. Extensive experiments across various datasets including semisupervised noisy label and medical image classification demonstrate the effectiveness of the proposed approach. After Rebuttal: The authors have addressed some concerns. Based on their responses I have revised my rating. Strengths: Innovative and straightforward concept for data augmentation The simplicity of the idea proves highly effective when properly implemented The paper is wellwritten clear and thoughtprovoking The theoretical underpinnings of the method are sound Robust evaluation with diverse tasks (semisupervised learning noisy label learning medical image classification) and datasets to demonstrate the efficiency of LPA3 Weaknesses: Lack of explicit details on how TCS selects hard positive data in Algorithm 1 which is crucial for the manuscripts selfsufficiency Limited experimental evidence to fully showcase the capabilities of LPA3 in semisupervised learning: comparison to only one augmentation method (InfoMin) on a single dataset (STL10) Unclear backbone network configurations in experiments reported in Table 2 Performance evaluation of LPA3 on CIFRA10100 in a supervised classification setting Lack of results on largescale datasets such as ImageNet Need for further analysis and justification for the observation in Fig. 5 that performance of LPA3 can decrease with augmentation of all data (authors explanation of \"useless data augmentations\" remains unconvincing) Questions: Please address the weaknesses mentioned above for a more comprehensive evaluation.", "Paraphrased Statement: Summary: This paper introduces a data augmentation method called LPA3 that generates adversarial examples while preserving the original label. LPA3 aims to maximize perceptual differences in input images while maintaining predicted class probability similar to the original image. Experiments show that LPA3 is effective in supervised semisupervised and noisylabel learning tasks. Strengths: Wellmotivated: Data augmentation is crucial for deep learning and is especially important when used on datasets with limited prior knowledge. Practical: LPA3 is easy to integrate into existing learning frameworks. Clear: The paper is written in an understandable manner. Weaknesses: Missing Related Work: The paper does not mention related works such as Adversarial AutoAugment which also uses adversarial objectives for data augmentation. Insufficient Experiments: The experiments are limited to small datasets. LPA3 is only compared to RandAugment on a single dataset. It would be beneficial to compare LPA3 to other stateoftheart data augmentation methods on larger and more diverse datasets. The mean and standard deviation of multiple runs should be presented. Adversarial Training: LPA3 uses an adversarial attacklike procedure but it aims to preserve the class label instead of altering it. It is unclear if LPA3 can be combined with adversarial training. It is also unclear if LPA3 affects the creation of adversarial samples for adversarial defense. Questions: Reliance on Representation Learning Network: LPA3 relies on a representation learning network for calculating perceptual and softmax class probabilities. Training Signal Quality: During early training the network may not be welltrained potentially leading to incorrect training signals for labelpreserving constraints. Applicability to Small and Complex Datasets: How does LPA3 perform on small and complex datasets where the representation learning network may have difficulty in generating accurate signals", "Paraphrase: Summary: The paper presents a data augmentation strategy that does not rely on predefined operations. Its aim is to create diverse and challenging training examples (hard positives) while maintaining the original images label. The approach utilizes a surrogate model based on the LPIPS distance and model predictions to determine the optimal augmentation for each image. The experiments demonstrate the effectiveness of the method for supervised semisupervised and noisy label learning tasks. Strengths: 1. The autoaugmentation concept is straightforward and easy to understand. 2. The paper provides a solid theoretical foundation for the LPA3 formula (Eq.2) which prioritizes augmentations that introduce minimal information about the original image while preserving its label. 3. LPA3 leads to significant performance improvements on different learning tasks. Weaknesses: 1. The implementation of the optimization problem has some limitations. The use of a neural network classifier to estimate the conditional entropy of Y given X can result in incorrect predictions. This could lead to undesirable augmentations that share the same classifier prediction as the original image but have a different label. Conversely there may be augmentations that preserve the images label but have significantly different classifier predictions which are not considered by the implementation. Virtual Adversarial Training suggests that augmentations should alter classifier predictions so preserving classifier predictions may not be optimal. 2. The semisupervised learning results are not entirely conclusive. The reported performance of FixMatch is lower than the original papers findings indicating potential variance in the results. The authors should address this and report the variance across multiple random seeds. 3. It is unclear whether LPA3 alone can improve semisupervised and noisy label learning. FixMatch uses additional data augmentation methods (RandAugment and CTAugment) so the paper should test LPA3 as a replacement for all data augmentation in FixMatch to determine its isolated impact. Questions: 1. For medical image classification strong augmentations like AutoAugment may not be optimal. Common data augmentations such as random crop color jitter and random horizontal flipping as used in Mean Teacher might yield better results.", "Paraphrased Statement: Summary: The presented method proposes a novel data augmentation approach that doesnt require prior knowledge of transformations that maintain labels. It is formulated as an optimization problem aimed at creating a challenging positive example for each training sample. As the models learning progresses the augmented data is refined. This method eliminates the need for additional generative models and is applicable to a range of tasks including supervised semisupervised and noisy label learning. The method is wellsupported and stems from a probabilistic graphical model representation of data augmentation and a formalization of ideal data representation and augmentation strategies. Implementation details and experimental results are provided for multiple datasets and tasks in supervised semisupervised and noisy label learning. Strengths and Weaknesses: The approach appears innovative with the novelty lying in the iterative refinement of augmentation data based on model learning. While the method deviates from mainstream approaches that rely on prior knowledge it is welldocumented and implementation details are provided. Experimental results demonstrate consistent and significant improvements over established baselines. Questions: Section 3 Line 132: The claim of \"decoupling randomness into two parts\" seems inaccurate as only one random variable (N) is mentioned with X as a deterministic function of N and Y. What constitutes the second random component Section 4: The concept of \"sufficient representation\" is mentioned but not clearly defined. Section 5.2: Additional details on TCS which is mentioned in Algorithm 1 but not described would be helpful. Datasets: The sizes of the datasets used in the experiments are not provided."], "rApvGord7j": ["Summary Paraphrase: The authors propose a method called FairBayesDPP to ensure Predictive Parity Fairness while maintaining accuracy comparable to regular classifiers. They demonstrate that under a specific condition Bayesoptimal classifiers that achieve Predictive Parity are GroupWise Thresholding Rules (GWTRs). If this condition is not met there may be no Bayesoptimal classifiers that satisfy Predictive Parity potentially leading to withingroup unfairness. Strengths and Weaknesses Paraphrase: Strengths Clear presentation and explanation of key findings. Efficient algorithm (FairBayesDPP) for achieving Predictive Parity Fairness without compromising accuracy. Incorporation of a sufficiency condition into the algorithm. Weaknesses Novelty questioned in light of prior work on Bayesoptimality and postprocessing under fairness constraints. Insufficient discussion of the broader fairness literature and implications of the proposed work. Lack of clarity on the implications of the sufficiency condition not holding and the relationship to other fairness measures. Assumes familiarity with postprocessing and thresholding methods in the fairness literature making it less accessible to newcomers. Questions: 1. Line 21: Attribution of algorithmic bias solely to historical biases should be nuanced. 2. Line 34: GWTR being a minimal requirement for withingroup fairness should be clarified. 3. Lines 3437: Explicit connection to postprocessing fairness should be provided. 4. Lines 4041: Further discussion on the lack of consideration for sufficiency measures in the fairness literature and why unconstrained optimization may not suffice. 5. Lines 8990: Clarification on \"perfect predictions that equal the label.\" 6. Line 91: Justification for the argument that label information is often unknown for some groups should be cited. 7. Line 109: Typo: \"latter\" should be \"later.\" 8. Definition 3.2: Connection to previous works and its implications should be elaborated. 9. Section 4: Importance and selection of cost parameter c should be discussed earlier and in more detail. 10. Lines 153155: Intuition behind the interpretation of condition 4.1 should be strengthened. 11. Line 1834: Implications and reasonableness of the condition should be explained. 12. Section 5: Clarification on grid search and selection of t values. 13. Footnote 4: Guidelines for performing statistical hypothesis tests should be provided.", "Summary Paraphrase: The study examines the optimal classification method that ensures fair outcomes under the predictive parity fairness constraint. The authors investigate whether this optimal classifier takes on a specific rule called a Groupwise Thresholding Rule (GWTR). They determine the conditions under which the optimal classifier will take the form of a GWTR and provide an algorithm to construct a GWTR that meets the predictive parity constraint. Experiments show that the proposed algorithm leads to fairer classifications than methods without fairness constraints. Strengths: Clear and wellwritten The discovery of the GWTR condition for the predictive parity constraint is significant The proposed algorithm for constructing GWTRs is practical Weaknesses: The study does not quantify the cost of enforcing predictive parity unlike similar studies for other fairness constraints. The condition for GWTR formation is sufficient but not necessary leaving the full characterization incomplete. The empirical comparison lacks comparisons with other existing fair learning techniques. Additional Observations: The authors are encouraged to discuss the cost of enforcing predictive parity. The proposed method should be evaluated for its statistical efficiency compared to other fair learning methods. The empirical comparison should be expanded to include existing fair learning techniques. Minor Issue: Appendix line 537: The reason for the nondecreasing function is not clear. Appendix line 542: Definitions of T0 and T1 appear identical may be a typo.", "Paraphrased Summary: This paper investigates predictive parity a type of fairness that aims for equal likelihood of positive predictions across groups. It reveals that: Optimal fair classifiers under predictive parity may or may not adhere to a specific type of rule. The paper introduces FairBayesDPP an algorithm for binary fair classification that estimates the optimal fair classifier subject to predictive parity constraints. Experiments support the effectiveness of the proposed method. Strengths: The focus on predictive parity is relevant. The theoretical analysis provides insightful findings. The paper is wellwritten and easy to understand. Weaknesses: The motivation for the paper is unclear as predictive parity has been studied extensively. The claims about the complexity of predictive parity constraints and the suitability of the proposed method need further justification. The paper lacks sufficient background on groupwise thresholding rules (GWTRs). The experiments do not include strong baselines to demonstrate the effectiveness of FairBayesDPP. Questions: 1. Explain why FairBayesDPP is particularly wellsuited for this problem. 2. Provide more context and discussion on GWTRs. 3. Implement baselines that compare the performance of FairBayesDPP to other approaches.", "Paraphrase: Summary: This paper explores the Predictive Parity (PP) concept of fairness and conducts theoretical and empirical analyses. It focuses on the use of groupwise thresholdbased classifiers (GWTRs) for fair Bayesoptimal classification and provides a sufficient condition for GWTRs to achieve optimal PP fairness. The paper also examines scenarios where this condition is met or violated. Strengths and Weaknesses: The paper presents a clear and straightforward exposition. The theoretical analysis establishes a link between costsensitive GWTRs and fair Bayesoptimal classifiers for binary classification. The paper proposes a twostage algorithm and evaluates its performance empirically. One potential drawback is the lack of discussion on a necessary condition for GWTRs to be fair Bayesoptimal classifiers. While the paper provides a sufficient condition a necessary condition would enhance the understanding of the issue. Questions: Q1: Necessary Condition Can the authors provide insights on the potential necessary condition for GWTRs to be fair Bayesoptimal or explain the challenges in deriving such a condition Q2: WithinGroup Fairness The paper describes the phenomenon of \"withingroup fairness\" but its unclear how this concept relates to PP fairness which is based on the proportion of real positive labels among predicted positivenegative instances. Can the authors clarify the role of withingroup fairness in their analysis Q3: Equation 3 Equation 3 appears to contain a typo. Is the lefthand side meant to be \u03b71(x) instead of \u03b7a(x) or is there a typo in the value assignment of A (a on the lefthand side and a\u00af on the righthand side) Q4: Algorithm 1 Recommendation In Algorithm 1 it is suggested that if Condition 4.1 is not satisfied other fairness notions should be considered. However since Condition 4.1 is a sufficient condition its violation does not necessarily imply the impossibility of achieving PP fairness. Can the authors clarify this recommendation"], "zLVLB-OncUY": ["Paraphrase: Summary: This research presents a novel framework called Contrastive Optimal Positives Generation (COPGen) for selfsupervised contrastive learning. COPGen focuses on learning unique latent transformations that reduce mutual information between generated positive pairs while preserving semantic consistency. Strengths: The paper is clearly written and accessible. The concept of leveraging a pretrained generative network for generating positive samples is innovative. Weaknesses: The experimental evaluation may have some limitations: The image resolution (112x112) used in pretraining is lower than in original studies. This may result in lower performance metrics for other methods compared to reported results. It would be valuable to determine if the generative models quality impacts the final outcome. The additional computational cost of incorporating a generative model in the training process should be investigated. Questions: Does the quality of the generative model affect the final result What is the additional training cost when adding a generative model", "Paraphrased Statement: This paper introduces COPGen a method for generating Contrastive Optimal Positives (COPs) for selfsupervised contrastive learning. COPGen utilizespretrained generative models to create COPs that minimize noise while preserving semantics. Theoretical analysis demonstrates the methods ability to remove redundant information while maintaining valuable features. Experiments demonstrate its efficacy. Strengths: Novelty and intriguing concept Effective experimental outcomes Weaknesses: Limited technical innovation Ambiguous theoretical analysis Outdated comparative contrastive learning approaches Questions: 1. Concerns about potential semantic deficiency in contrastive learning due to GANs mode collapse problem and potential solutions. 2. Clarification on Lemma 3.3s implication of semantic feature dependence on random noise. 3. Explanation of Figure 2s caption. 4. Request for code availability for testing on ImageNet1K. 5. Performance inquiries on transfer and semisupervised learning tasks.", "Paraphrased Statement Summary This research presents a method for generating optimal sets of positive pairs for contrastive learning. The pairs are created by identifying an instancespecific navigation function in the latent space that minimizes mutual information. The generated synthetic data is then used for selfsupervised contrastive learning with the approach outperforming previous methods on the Imagenet dataset and a downstream object detection task on PascalVOC. Strengths Clear presentation and wellwritten text Contributions clearly defined in relation to prior work Superior performance in experimental results compared to previous methods trained on synthetic images Consistency with prior work and successful application in downstream tasks Thorough ablation studies Weaknesses Limited discussion on parameter selection (e.g. choosing delta) Lack of comparison with prior work in terms of computational efficiency Suggestions for Revision Include discussions on the selection of delta and its impact on performance. Analyze different delta values and sample sizes to evaluate performance. Clarify the type of domain knowledge better preserved by the proposed method. Compare the computational cost of the proposed method to previous approaches. Conduct additional experiments with different random seeds to assess the significance of the results.", "Paraphrase: Summary: This work builds upon the GenRep approach utilizing a generative models data representation for representation learning. It enhances GenRep by allowing the fixed latent transformation to be learned. The method COPGen is designed as a latent space navigator that minimizes the mutual information between generated positive pairs while maximizing mutual information between genuine positive pairs. The paper is wellwritten and theoretically sound with an accessible concept commonly employed in frameworks like GAN. The performance exceeds that of GenRep as expected. Strengths: Unlike previous efforts that focused on enhancing contrastive learning with hard negatives this paper uses the InfoMin criterion to generate optimal positives. The authors provide robust mathematical support for their approach and the method is clearly described and exemplified. Weaknesses: The performance relies heavily on the trained BigBiGAN model. While a perfect GAN could potentially yield comparable or superior results to SimCLR this is unlikely with the model used. Minimax training objectives may face convergence challenges but this is not addressed in the paper. More training process details would be beneficial. Noncontrastive methods like BYOL and SimSiam should be considered in the references. Minor Points: The choice of 224 as the batch size could be aligned with other studies by using 256 instead. It may be useful to explore integrating this hard positive technique with noncontrastive frameworks like BYOL and SimSiam. Experiments on smaller datasets would be valuable if time constraints exist. Using a Lagrange multiplier to constrain the semantic distance between z and z could be considered. A trainingtest loss plot over epochs would provide insights into the convergence process of the minimax objective."], "qC2BwvfaNdd": ["Summary (Paraphrased): The authors propose a system that examines data characteristics and \"sculpts\" data during model training based on outcomes and feature differences within subgroups in a dataset. They categorize subgroups into \"Easy\" \"Ambiguous\" and \"Hard\" groups. Through experiments they demonstrate that the framework can detect uncertainties caused by data characteristics provide insights into subgroup feature attributes and potentially enhance model generalizability by excluding the \"Ambiguous\" group from training. Strengths and Weaknesses (Paraphrased): Strengths: A unique approach that emphasizes data characteristics to analyze model performance factors. The provision of a tool (DataIQ) to improve researchers understanding of data before modeling. An innovative idea that should be supported. Weaknesses: The proposal may not be fully mature and could serve as a conceptual framework for dataset evaluation and model training direction. The paper attempts to cover too many topics (e.g. variation robustness subgroup characterization model generalizability) within the limited space. A single focus would enhance clarity. Questions: 1. How does the \"Hard\" group differ from the \"Ambiguous\" group given that both have similar features but different outcomes 2. Are the data insights derived clinically meaningful and can any relevant biomedical research be cited 3. Table 2 shows only marginal improvements. Since the comparison involves both DataIQ and the GroupDOR modeling method could the authors provide a baseline model using DataIQ alone", "Paraphrased Statement: This paper presents a method for categorizing data examples based on their classification accuracy: correctly incorrectly or randomly. It breaks down the accuracy of each example into two components: uncertainty due to the models limitations (epistemic uncertainty) and uncertainty due to data variability (aleatoric uncertainty). Unlike previous methods that relied on epistemic uncertainty for categorization this method uses aleatoric uncertainty. Strengths: The method addresses an essential issue in understanding prediction difficulty for classifiers. The proposed approach clearly defines difficulty using a combination of epistemic and aleatoric uncertainty. The paper is wellorganized and the experiments demonstrate the methods effectiveness. Weaknesses: The methods applicability to data beyond tabular data is not explored. Questions: In the formula provided is the empirical process estimate reliable for uncertainty quantification considering the potential correlation among samples", "Summary The authors propose a method to identify three subgroups in a dataset that impact the performance of a machine learning model. These subgroups are Easy Ambiguous and Hard. The method uses the models training performance and uncertainty to identify these subgroups. The identified subgroups are useful for understanding model usage and improving the reliability of model predictions. Strengths The method is intuitive and simple to implement. The subgroups identified are meaningful and provide insights into model performance. The method addresses important requirements for machine learning operations (MLOps). The paper provides comprehensive analyses and experimental results to support the proposed method. Weaknesses The papers figures are low quality and difficult to read. Certain sections and paragraphs could be moved to supplementary materials to improve readability. The method to calculate Easy Ambiguous and Hard groups is not explicitly presented. Questions for the Authors Is the method dependent on the training dataset performance How does the method account for different model performances in Equations 1 and 2 How many ambiguous data points were found in each dataset", "Paraphrase: The authors argue that populationlevel measures of model effectiveness may not reflect model performance among various subgroups. They introduce `DataIQ` a method for classifying and segmenting patients based on model uncertainty at the patient level. They demonstrate the techniques ability to: Distinguish reliable data Enhance data gathering Facilitate model implementation Strengths and Weaknesses: Overall the paper is highly regarded despite a few shortcomings. Strengths: The paper identifies and justifies the need for modeling model uncertainty through comprehensive arguments and ablative experiments. Ablative studies such as assessing uncertainty changes with feature additions (Figure 6) and domain shift (Figure 7) are valuable. The comparison of model outputs with a Group DRO objective to improve subgroup performance complements the extensive experiments. Weaknesses: The method assumes that model parameters at each epoch are sampled from a distribution. While intriguing this assumption differs from an empirical distribution as model parameters are highly correlated. Questions: Have the authors considered alternative methods of incorporating randomness into their uncertainty modeling such as Bayesian priors on parameter `v` or dropout masks", "Paraphrased Statement: This research presents DataIQ a framework compatible with any iterative machine learning models. It evaluates the quality of tabular data samples by classifying them into categories based on their uncertainty. The framework measures this uncertainty by calculating the variance of prediction outcomes over training iterations. DataIQ demonstrates its effectiveness in improving feature acquisition training dataset comparison and model generalization across multiple realworld datasets. Strengths and Weaknesses: Originality: DataIQ innovates by quantifying aleatoric uncertainty (intrinsic data uncertainty) through prediction outcome variance rather than model parameter uncertainty. This enables new approaches to improving data quality such as feature acquisition. Quality: The variancebased uncertainty quantification is straightforward and comprehensive allowing for detailed analysis of data categorization and its impact on model performance. However the technical novelty of the method is limited. Clarity: The paper is wellwritten and visually clear making it easy to understand. Significance: The research topic is highly practical as data quality assessment is crucial for training effective machine learning models. However the methodological novelty is not particularly high. Questions: 1. Variance Distribution: How does the distribution of variance over training epochs vary among data samples and how might average variance impact uncertainty evaluation for samples with different training dynamics 2. Model Type Comparison: While DataIQ is shown to be robust within neural network models it would be informative to compare results across different model types (e.g. neural networks vs. gradient boosting) to determine whether aleatoric uncertainty is classdependent."], "nP6e73uxd1": ["Summary The paper proposes a sampling method that guarantees accuracy when measuring distances in the infinity metric particularly for measures with logconcave densities. This has applications in differential privacy. Strengths and Weaknesses Strengths: The paper provides a novel approach to sampling in the infinity distance. The observation on how to upgrade total variation guarantees to infinity distance guarantees is simple and intuitive. The simplicity of the method makes it easy to implement and understand. Weaknesses: The paper does not focus on sampling but rather on bridging the gap between total variation and infinity distance guarantees. The proofs are generally clear and easy to follow. Questions Concern about Corollary 2.5: The paper claims that the output of the algorithm is differentially private. However each iteration of the Dikin random walk reveals information about the function being optimized which depends on the sample. Minor Comments: The infinity distance is equivalent to the infinity R\u00e9nyi divergence. Theorem 2.2 should state that f is convex. The inconsistency between Theorems 2.1 and 2.2 in terms of their domains (polytopes vs. convex bodies) is unclear. Corollary 2.2 should also state that f is convex. Figure 1 could be moved to the appendix. The geometric distribution of the stopping time \u03c4 could be mentioned in Corollary B.4 to provide tail and expectation bounds. The inequality \u03c0(\u03b8) Vol(B(0R)) \u2265 1 could be stated earlier for ease of reference. PostRebuttal The authors have adequately addressed the concerns raised and the score has been increased.", "Paraphrased Statement The authors introduce a sampling technique for probability distributions that are continuous unimodal and have a bounded Lipschitz constant within a compact support. They demonstrate that this technique generates samples whose distribution is guaranteed to closely resemble the target distribution (within a certain distance measure called \"infinity distance\") with a computational complexity that improves significantly (scaling as a polynomial of log(\u03b5) and d where \u03b5 is a parameter controlling the approximation error and d is the dimension of the distribution). The core element of the technique is a rejection sampler that takes an initial sample with an approximate distribution that is already quite close to the target distribution (in terms of another distance measure called \"total variation distance\"). This rejection sampler produces a new sample guaranteed to meet the desired infinity distance requirement. The authors show that existing methods can be used to generate samples that meet the necessary total variation distance requirement which can be combined with their rejection sampler to achieve the desired result. They also demonstrate that this approach leads to more efficient solutions for certain differential privacy problems that have been studied previously. Strengths Addresses a relevant problem with a significant improvement in computational efficiency. Provides intuition for why previous approaches would not yield comparable results. Employs a straightforward rejection sampler with clearly presented proofs. Weaknesses Lacks experimental validation which would be beneficial in verifying the techniques performance. The description of one of the differential privacy problems is unnecessarily technical. Miscellaneous The supplementary material includes the full paper. A minor discrepancy in the description of the algorithms parameters is identified (allowing the inner ball of the distribution to be centered at any point versus requiring it to be centered at the origin). A potential error in a lower bound calculation is indicated. The meaning of the EP[] operator on specific lines is unclear.", "Paraphrased Statement: Summary: This research introduces an innovative approach to sampling from logconcave distributions providing a way to convert algorithms with TV distance guarantees into ones with infinity distance guarantees. It also showcases applications in differentially private optimization such as empirical risk minimization and lowrank approximation. Strengths: Original contribution that outperforms previous methods for infinity distance sampling from logconcave distributions. Technically sound and wellsupported with proofs. Clearly written and organized. Significant impact due to the widespread use of logconcave distributions and the exponential improvement in runtime from poly(1\u03b5) to polylog(1\u03b5). Weaknesses: Lack of experimental evaluation which would be beneficial for practical comparison with existing algorithms. Questions: Future research could explore the practical performance of the algorithm compared to previous methods on common logconcave distributions.", "Paraphrase: This paper introduces an algorithm to roughly sample from a convex continuous distribution. In some scenarios such as Bayesian statistics obtaining exact samples can be challenging due to only having partial information about the distributions formula. A common approach involves creating a Markov chain within the discrete constraints. However this method struggles to generate samples close to the actual distribution with a specific distance metric in a reasonable time. The algorithm presented in this paper can generate samples in polynomial time and guarantees a close approximation to the true distribution using a different distance measure. The algorithms runtime improves upon existing techniques particularly in higher dimensions. Applications include optimization privacy preservation and game theory. The methods innovation lies in converting a sample approximating the distribution in one distance measure to a sample approximating it in another distance measure. This involves using a continuous Markov chain which is simulated by convolving the initial sample with noise and employing rejection sampling. The paper is wellstructured and clearly written. The results are applicable and the overall approach is original clear and relevant."], "rH-X09cB50f": ["Paraphrased Statement: This paper investigates crossdomain fewshot learning by examining Domain Similarity and FewShot Difficulty. The authors observe that selfsupervised pretraining outperforms supervised pretraining in this context. Additionally they propose two improved pretraining strategies. Strengths: Wellwritten and structured manuscript addresses an important problem. Valuable observations on crossdomain fewshot learning. Interesting experimental designs provide insights. Weaknesses: Some observations are predictable. Domain similarity calculation using EMD may not accurately reflect actual dataset similarity. Limited visualization hinders understanding of differences between pretraining methods. Proposed improvement schemes yield minimal performance gains. PostRebuttal: The authors have addressed the concerns raised in the original review. They are expected to include additional results in the revised version to enhance the papers quality. Questions: 1. Consider supervised pretraining on labeled target domain data to further validate selfsupervised pretraining. 2. Do the observations apply to larger shots like k20 and k50 3. Perform a more detailed analysis on the impact of specific data augmentation techniques on selfsupervised learning performance.", "Paraphrase: This research examines the use of additional unlabeled data from the target domain for crossdomain fewshot learning (CDFSL) and explores the effects of sourcetarget domain similarity and the inherent difficulty of fewshot learning. The authors propose two learning methods: selfsupervised learning and mixedsupervised learning using the unlabeled target domain data. Through extensive experiments they make several observations regarding domain similarity and fewshot difficulty. Strengths: Thorough experiments Clear and easytofollow presentation Weaknesses: Questionable assumption: The assumption that unlabeled target domain data is readily available is problematic as in realworld scenarios obtaining such data may be challenging. Misleading descriptions: In Section 4 the paper implies that selfsupervised learning (SSL) is superior to supervised learning (SL) for base training but this is due to the use of different domains for training (target domain for SSL source domain for SL). Unclear class overlap: It is not specified whether the unlabeled data (for base training) and labeled data (for support and query) on the target domain share overlapping classes. Limited knowledge contribution: The observations and insights provided by the study offer limited value for understanding crossdomain fewshot learning as adding target domain data generally improves accuracy on that domain even if the samples are unlabeled. Questions: Do the unlabeled and labeled data share overlapping classes and how is this ensured How is the target domain identified in advance to prepare unlabeled images for training Why was Equation 5 chosen to measure fewshot learning difficulty Its rationale should be explained. Would changing the settings of \u03b2 and k in Equation 5 alter the observations made", "Paraphrased Statement: Summary: This article examines the issue of crossdomain fewshot learning in images focusing on pretraining and finetuning techniques. It provides empirical evidence supporting the effectiveness of supervised pretraining on base classes and selfsupervised pretraining on fewshot data. The article also introduces two metrics domain similarity and fewshot difficulty to analyze model performance. Finally it suggests a twostage pretraining approach combining supervised learning and selfsupervised learning to enhance performance in crossdomain fewshot learning. Strengths: Provides an experimental analysis and presents the research in a structured manner. Weaknesses: 1. The fewshot difficulty metric is based on the empirical upper bound of fewshot performance determined by pretraining the model with 20 labeled data from the target dataset and evaluating it on the remaining unseen data. However the selection of labeled data and construction of the fewshot classification task may impact the baseline results. It is unclear how class splits are controlled and how the fewshot classification task is constructed. 2. The implementation uses shallow backbones (e.g. ResNet18 and ResNet10). It is unclear if the conclusions hold for deeper backbones. Questions: How are class splits controlled and the fewshot classification task constructed Will the conclusions hold for deeper backbones"], "nax3ATLrovW": ["Paraphrase: The researchers present a unified approach to create graphs across various stages of the Electronic Design Automation (EDA) process and design a general Graph Neural Network (GNN) model for subsequent EDA tasks. Their method constructs a heterogeneous graph by combining celltocell connections (geometric information) and celltonet connections (topological information). Node and edge features are extracted based on the physical properties of cells pins and nets. They then propose a circuit GNN model that applies message passing separately on celltocell and celltonet connections. This generates representations of cells and nets for downstream tasks. Evaluation: The proposed method improves congestion prediction accuracy by 16.7 and reduces wirelength prediction error by 16.9. Criticisms: While the graph construction and GNN model are sound they lack technical originality. Constructing a bipartite graph based on celltonet connections is a standard approach. The GNN models novelty is limited as applying message passing individually per edge type is common for heterogeneous graphs. Stronger baselines that incorporate edge features should be used to assess the models contribution. Hyperparameters for baselines should be tuned for the specific dataset. The proposed method is not compared to DREAMPlace despite wirelength predictions goal of accelerating EDA design. Questions: Why generate features for GNN if the ground truth for wirelength prediction is known What is the rationale for using different messagepassing functions for different edge types How is the proposed work applied to graphs in other EDA stages such as dataflow graphs Could cell positions be encoded as node feature vectors instead of using geometric edges", "Paraphrased Statement: This research develops a modeling framework that tackles various issues in circuit design. The framework incorporates the following advancements: 1. A unique circuit graph that combines topological and geometrical data providing a unified representation that seamlessly integrates with different EDA tasks and stages. 2. CircuitGNN a novel messagepassing approach designed specifically for the proposed graph structure. It facilitates messagepassing along both topological and geometrical edges then fuses the messages to update cell and net representations. 3. Extensive experiments demonstrate the effectiveness of the proposed methods in both accuracy and speed. Strengths: 1. Thorough analysis of circuit EDA tasks and compatibility with machine learning methodologies. 2. Clear and detailed methodology description. 3. Significant improvements over existing methods as evidenced by the results. 4. Openaccess code provided for transparency and reproducibility. Weaknesses: 1. While the integration of topological and geometrical information is highlighted as a key contribution its practical significance may not be fully explained. 2. More quantitative details on the proposed models and baselines including the number of parameters operations and their types would be beneficial.", "Summary This paper presents a novel graph representation called Circuit Graph that incorporates diverse circuit information from logic synthesis and placement. The graph structure encompasses both circuit connectivity and physical cell locations. A matching graph neural network (GNN) is designed to extract circuit representations for various downstream tasks. Experimental evaluations demonstrate the graphs effectiveness in congestion and wirelength prediction tasks showcasing efficient NN computation. Strengths 1. Heterogeneous Information Integration: The Circuit Graph unifies multiple EDA design stage representations providing a comprehensive data structure with knowledge from various phases. 2. Generality for Future Work: The proposed graph structure offers a foundation for future EDA advancements. By incorporating additional features it could support tasks such as timing analysis. 3. Promising GNN Structure: The GNN follows the design of the Circuit Graph preserving its structure and presenting potential for effective representation extraction. Weaknesses 1. Representativeness of GNN Features: The paper does not assess the representativeness of the extracted GNN features. The shared input features for congestion and wirelength prediction tasks raise questions about knowledge transferability. 2. Insufficient GNN Formulation Details: The paper lacks justification for certain GNN design choices such as the use of a matrix and a vector in message passing and the employment of maxpooling for message fusion. 3. Importance of Raw Features in Readout: The inclusion of raw features in the readout for cells and nets warrants further evaluation. Their influence on performance should be examined to determine the true value of the extracted GNN representation. Questions 1. Transferability of GNN Features: Can the authors clarify the transferability of the extracted GNN features between different tasks 2. Practical Computation Efficiency: The paper needs to provide numerical comparisons of the GNNs training and inference times with traditional EDA routing tools to demonstrate its practical impact. 3. GNN Formulation Details: Why are a matrix and a vector employed in topological and geometric message passing respectively What is the rationale for using maxpooling for message fusion How significant are the raw features in the readout"], "lMMaNf6oxKM": ["Paraphrase: This paper introduces a new graph Transformer with high efficiency and scalability (GPS). The authors have reevaluated positional encodings (PEs) and structural encodings (SEs) by introducing local global and relative categories. They then incorporate these categories into the graph Transformer using local and global attention. The GPS layers developed by the authors demonstrate strong performance on various datasets. The proposed PE and SE definitions are widely applicable and the paper is wellwritten and structured. However there are some concerns regarding the methods and experiments employed. Weaknesses (Questions): 1. Please clarify the limitations of the proposed methods. 2. Could you provide more details on the specific concerns and issues with the experiments", "Paraphrased Statement: Summary: The study proposes a method for effectively applying transformers to graph data. It introduces a modular architecture that includes graph positional encodings structural encodings and graph features which are then processed by a set of transformer blocks and message passing neural networks. Strengths and Weaknesses: Strengths: Addresses a significant and topical task. Wellwritten technically sound and clearly structured. Evaluated on a sufficient number of datasets demonstrating performance variations with different random seeds. Provides a package integrated with graphGym. Weaknesses: Lacks a convincing justification for using transformer blocks. Claims of linear complexity are not fully supported by experimental results. Questions: How would the model perform if other architectures were employed instead of transformer blocks such as simple MLPs", "Paraphrased Statement: Summary: The authors of this work present a guide for constructing a scalable and effective graph Transformer with linear complexity. It features a wide range of message positional and structural encodings as well as local message passing and global attention mechanisms. The model demonstrates competitive performance on various benchmark datasets. Strengths: Linear complexity ensures scalability. Explores various positional and structural encodings. Accessible code and promising results. Weaknesses: A more structured presentation such as a pseudo algorithm would enhance understanding. Recent research suggests that dense attention maps may not be essential for Transformers. Clarifying the concept of a universal function on graphs is necessary to demonstrate how the framework achieves this capability. Questions: Is global attention truly necessary for graph Transformers given that recent studies suggest it may be unnecessary for standard Transformers A more detailed explanation of how the proposed framework achieves universal function approximation is required."], "vhKaBdOOobB": ["Paraphrased Statement: This study enhances the GhostNet model by addressing its limitation in capturing longrange connections among pixels. To achieve this it introduces a lightweight attention mechanism suitable for edge devices providing notable accuracy in ImageNet classification. While the paper has substantial merits there are minor concerns that warrant attention. Strengths: The proposed attention mechanism is novel and effective potentially serving as a promising approach for mobile networks. Thorough analysis demonstrates the deficiencies of selfattention for edge devices and its resolution. Impressive results on ImageNet surpass most mobile device models. Weaknesses: The claim that only half of the channels encode spatial information in a GhostNet module requires further proof. Performance on downstream tasks like object detection is acceptable given the low computations but not exceptional. The writing could benefit from improved English usage particularly in the use of definite articles and abbreviations. Tables 9 and 10 could be visually enhanced for better readability.", "Paraphrased Statement: The paper introduces a hardwarecompatible attention module and a lightweight neural network for general visual tasks. The attention module relies on fullyconnected layers to capture global information efficiently. The resulting lightweight architecture achieves stateoftheart performance in image classification and object detection. Strengths: Reduced latency for edge devices by using hardwarefriendly operations. Captures global information effectively using an efficient attention module. Achieves superior performance over existing lightweight models. Versatile for various visual tasks due to minimal taskspecific knowledge. Suggestions for Improvement: Explore the potential of nonlinear activation functions within the attention module. Provide clearer captions for the proposed architectures in the figures. Investigate the influence of attention mechanism further for inspiration within the research community.", "Paraphrased Statement: This study introduces a \"decoupled fully connected\" (DFC) attention mechanism that enhances the performance of image classification models like GhostNet and MobileNet particularly in efficiency. Unlike traditional attention in transformers that considers the entire image DFC processes information along horizontal and vertical axes in a reduced feature space. The results are integrated with standard network blocks. Performance evaluations demonstrate that this approach significantly improves accuracy with minimal impact on throughput exceeding previous achievements in efficient mobile image classification.", "Paraphrased Statement: Summary This research introduces a lowcost attention module designed for mobile environments where computational resources and latency are limited. Specifically the module consists of two depthwise convolutions followed by sigmoid attention. The module is incorporated into GhostNet resulting in performance improvements in several tasks such as image classification object detection and image segmentation. Strengths and Weaknesses Strengths: Comprehensive analysis of design choices through ablation studies. Extensive experimentation across various largescale tasks and datasets. Weaknesses: The proposed module in Equation 45 does not strictly adhere to the definition of depthwise convolution (missing kernel size parameters KH and KW). Limited novelty as the proposed techniques (decoupled depthwise convolutions and SElike attention) have been widely explored. Marginal performance improvement over comparable models under controlled computational complexity. Lack of clarity on latency measurements and comparisons with related work. Upsampling and downsampling techniques have been previously explored and their impact on performance is not fully explained. Questions: Clarify the notation in Equation 5 (replace \"a\" with \"a\\prime\" if necessary). Confirm if the output \"a\\prime\" is constant for all height (h) values in Equation 4. Provide more detailed information on the device used for latency measurements to facilitate reproducibility."], "u6p_NvZ23qt": ["Using Moreau envelopes this study analyzes the onesided generalization error. The main theorem states that if the empirical Moreau loss consistently exceeds the expected Moreau loss then the expected Moreau loss is less than the empirical loss plus complexity. Applying this result yields: 1. VC bounds for expected Moreau loss 2. Bounds for linear regression with squared loss and classification with squared hinge loss (expected loss not expected Moreau loss is bounded by using Moreau envelope properties) 3. Discussion of L1 and hinge loss implications and their relationship to Huber loss 4. Improved generalization bounds for general Lipschitz losses 5. Bounds for general smooth losses with interpolation (generalizing previous work) 6. Recovery of results on benign overfitting Strengths: Clear writing and easily interpreted results Sound approach and introduction of Moreau envelope analysis to generalization bounds Thorough literature review Weaknesses: Nonnovel consequences of the main theorem (similar or better results obtained with different techniques) Limiting the results to expected Moreau loss instead of expected loss in some cases Confinement to linear multiindex models and Gaussian covariates Question: Is the matrix Q in Equation (4) symmetric (in addition to being idempotent)", "Paraphrase: Summary: This study investigates the generalization abilities of highdimensional generalized linear models (GLMs). The authors assume Gaussian input data and a multiindex underlying data distribution. They derive upper bounds for the population Moreau envelope risk in various overfitting settings based on training loss and function class complexity. By specializing their findings they establish connections to existing results like optimistic rates for linear regression and benign overfitting. Strengths: The main results are novel and apply to GLMs with potential model misspecification. Results are wellpresented with intuitive explanations. Technical proofs appear sound. Connections to existing literature demonstrate the results generality. Weaknesses: Certain assumptions lack clarity and may be difficult to verify particularly in Section 4 and the results of Sections 5 and 6. The Gaussian input assumption is limiting. Applicability of results may be questionable due to the Gaussian input assumption and unverified assumptions. Questions: Major: 1. Can the Gaussian input assumption be relaxed for wellspecified models 2. How easy is it to verify Equation (6) and provide examples 3. Can Equation (9) be verified or relaxed for a wellspecified model without the Gaussian input assumption Minor: 1. Clarify the statement about the first term being \"negligible\" in Equation (158) given the focus on highdimensional settings. 2. Define the variables \"xis\" in Equation (5) and discuss their relationship to the coordinates of \"x\". 3. Determine if the term \"epsilon lambda delta\" in Equation (6) depends on \"n\". 4. Verify if the term \"C delta(w)\" in Equation (7) depends on \"k\" \"d\" and \"Sigma\". 5. Explain why the claim in line 245 is true since \"f(lambda)\" approaches 0 as \"lambda\" approaches 0. 6. Verify the convexity of \"tilde f\" in line 260 and explain how it leads to \"L(tilde f(w)) 0\".", "Paraphrased Summary: This research presents generalization bounds for different types of losses over linear predictors by using Moreau envelope theory. The study demonstrates that specific quantities based on a localized Gaussian width can control prediction errors within Moreau envelopes of the loss. This in turn leads to generalization bounds for the original loss. The framework is versatile accommodating model misspecification and a range of loss functions. Paraphrased Strengths and Weaknesses: Originality: The application of Moreau envelope theory to generalization bounds is novel. Quality: Improves upon existing results including both classical and recent findings. Restrictive Gaussian data assumption but essential for understanding this regime. Clarity: Wellwritten and easy to follow despite the technical nature. Significance: Appears significant. Questions: Intuition behind using the surrogate distribution in Theorem 1. Does it help analyze the interplay between noise and low data dimensionality Is the main challenge in extending the Gaussian data assumption the use of the GMT framework for analysis Brief discussion on the usefulness of analyzing Moreau envelopes of the loss would be helpful."], "uvE-fQHA4t_": ["Paraphrased Statement: Summary: This paper introduces a new generalization error bound for PACBayesian learning that incorporates the gradient norm of the loss function. By applying Herbsts argument and making certain assumptions about data and loss functions the bound estimates the average of the loss function and its gradient norm. Empirical experiments indicate that this bound can accurately reflect the behavior of deep neural networks. Strengths: The research is motivated by practical concerns and easy to comprehend. Using Herbsts argument for generalization error bound derivation is novel and the bounds assumptions (average boundedness and Lipschitz property of the loss function) are both novel and significant. The bound naturally applies to multiclass classification which is practically relevant. The empirical experiments particularly Figure 2 effectively demonstrate the influence of neural network depth on generalization ability. Weaknesses: Some assumptions may be challenging to verify or too stringent in realworld settings: The entropy analysis assumes input data x follows a Gaussian distribution (line 139) which is a restrictive assumption. The assumption in Lemma 3.3 that loss per label is balanced for any parameter w is difficult to verify and intuitively grasp. Questions: Regarding the Gaussian distribution assumption (line 139): Is it possible to relax this assumption to the feature or latent space making it more moderate and easier to validate Regarding the assumption in Lemma 3.3 (lines 233237): How was the conclusion that this assumption holds in practice derived from the given statistics (maximum standard deviation of 0.022 and mean value of 4.605)", "Paraphrased Summary: This paper enhances existing PACBayes generalization theory by establishing new generalization bounds. These bounds require known bounded expected loss and loss gradient norms across the data distribution. The assumptions apply uniformly throughout the weight space or when integrated over the weight prior distribution. The bounds are derived for a data distribution modeled as a Gaussian mixture. The paper empirically investigates the complexity terms in the bounds showing how they vary with the parameter lambda and network depth. Strengths and Weaknesses: Strengths: Novel advancements in PACBayes theory Empirical evaluation of the bounds Weaknesses: Comparison with related work that depends on gradient norms and average deviations is missing. Empirical evaluation lacks direct comparison between average and maximum gradients. Key assumption of data distribution as a Gaussian mixture is not explicitly discussed in the evaluation. Relevant expected loss and loss gradient quantities may not be correctly evaluated in the experiments. Unclear notation in some theorems. Lack of clarity in certain theoretical sections. Mixing of symbols for different quantities (e.g. sigma). Proofs for new results are in the appendix instead of the main text. Questions and Minor Issues: Is the assumption of a Gaussian mixture data distribution verified empirically What is the significance of expected loss and loss gradient norms over randomized classifiers Are the terms in Theorem 2.2 equivalent to bounds for subgaussian random variables Can Theorem 2.3 be extended to a single Gaussian draw Should the proofs for new results be included in the main text What is the specific definition of overfitting used in the paper", "Paraphrase: Summary: The paper introduces a new generalization bound based on PACBayesian theory that relates to the gradient norm of the loss function in the input space. It builds on the bound proposed by Alquier in 2016. Instead of assuming uniformly bounded loss the paper provides a more general bound assuming on average bounded loss and gradient norm. This new bound is proven for both linear and nonlinear models using Herbsts theorem and LogSobolev inequalities. The paper also includes an empirical study that demonstrates the effectiveness of the new bound in providing tighter generalization bounds compared to traditional methods. Strengths: Clarity and wellwritten. Provides a generalization bound with a more realistic assumption. Explores the connection between generalization and gradient norm. Rigorous mathematical proof. Weaknesses: Lack of experiments demonstrating the relationship between generalization bound gradient norm and testing performance. Potential issue with the exchange of expectation and integral operations in the proof. Missing explanation of \"d\" in Formula 23. Questions: The authors are requested to address the concerns raised regarding the exchange of expectation and integral operations and the missing explanation of \"d.\"", "Summary: The paper examines the PAC generalization bound and loosens the uniform bounds assumption that is commonly made in PAC literature. Instead it introduces two relaxed assumptions: average bounded loss and average bounded gradient norm. This allows the derivation of a new generalization bound that considers the lossgradient norm. The paper provides empirical verification of the bounds effectiveness. Strengths: Relaxes an unrealistic assumption that was present in previous PAC generalization bounds. The relaxation is wellreasoned and provides a more nuanced understanding of generalization behavior. Utilizes an average gradient norm to assess generalization ability aligning with recent research. The paper is wellwritten and clearly presented. Weaknesses: Assumptions on the data are somewhat restrictive (Gaussian or mixture Gaussian distribution for x balanced labels for y). The generalization bound neglects KL divergence and empirical loss terms which could lead to undesirable behavior. The paper lacks empirical demonstrations of the bounds impact on model performance and generalization gap. Questions: The paper should provide a more detailed explanation of the data assumptions and their implications when realworld data does not align with these assumptions."], "uIXyp4Ip9fG": ["Paraphrase: Summary: The authors introduce ASE a novel method for efficiently evaluating models with limited labeled data. ASE utilizes a surrogatebased estimation approach interpolating errors of unlabeled points. ASE employs an active learning process to improve the surrogate. The proposed acquisition strategy XWED optimizes surrogate learning for the final estimation task. Experiments demonstrate that ASE surpasses current stateoftheart methods in label efficiency. Strengths and Weaknesses: 1. Strengths: Introduction of a new algorithm for active testing based on surrogate estimation. Clear and concise writing style. 2. Weaknesses: Lack of exploration into alternative metrics for surrogate model evaluation (e.g. difference between expected and current accuracy). Limited evaluation of the batch mode setting where a specified number of topperforming points are selected in each round. Questions: 1. Alternative Evaluation Metrics: Could we explore the use of the difference between expected accuracy and current accuracy as an alternative metric to squared error How would ASEs performance compare to other active learning algorithms (e.g. BALD Entropy) using this metric 2. Extension to Batch Mode: Is it possible to evaluate ASE and other active learning algorithms in a batch mode setting where a batch of points is selected in each round This could be done by extending XWED similar to BatchBALDs extension of BALD.", "Summary: The paper presents a new approach called the Active Sampling Ensemble (ASE) which outperforms the existing Lure technique for active testing. The proposed neural networks for calculating the g and \u03c0 functions as well as the acquisition function also enhance active testing performance. The study explores various aspects of ASE in the context of distribution shift. Strengths: The ASE algorithm is easy to use and performs well in addressing active testing issues. The experiments effectively demonstrate the key features of ASE. Weaknesses: The theoretical analysis of ASE error is not wellelaborated. While the decompositions and analyses are helpful the properties I II and III are not clearly explained and include excessive context. The evaluation seems to rely heavily on empirical analysis due to the lack of theoretical foundations. Questions: 1. While approximating risk or loss can be beneficial minimizing loss may not always guarantee maximum classification accuracy. Please provide insights on how ASE balances these objectives. 2. Can you elaborate on the primary reason for reducing the variance of active sampling in ASE Is it primarily through updating procedures or architecture changes 3. Does the capacity of the neural network used for estimating \u03c0 impact the performance of ASE", "Paraphrased Statement: Summary: This paper introduces Active Surrogate Estimators (ASEs) a new technique for active testing. ASEs leverage a surrogate model to predict losses and utilize an improved acquisition strategy called XWED derived from BALD but incorporating data point weights based on predicted losses. Experimental results demonstrate the superiority of ASEs over existing methods such as Monte Carlo (MC) and Lossaware Uncertainty Reduction with Exploration (LURE). Strengths: Tackles a significant problem. Introduces novel and welljustified methods (ASEs and XWED). Demonstrates effective performance improvements. Presents a clear article structure. Weaknesses: ASEs and XWED involve small modifications of previous techniques (LURE and BALD). The paper lacks detailed descriptions of certain baseline methods used in the experiments (e.g. LURE BALD). Specific examples of applying ASEs including formulas for model specification and computation are missing. Section 5 could be condensed to allocate more space for other sections. The paper fails to compare the proposed active testing algorithm with methods designed to evaluate models without labels. Questions: 1. Why not directly multiply \\mathbbEY \\sim \\pi(\\cdotx) [\\mathcalL(Y f(x))] into \\mathrmBALD(x) instead of using equation (6) 2. Is there a particular reason why \\mathcalL(Y f(x)) is multiplied into each expectation in equation (6)", "Paraphrase: Summary: Extensive research is conducted in active learning. However previous studies have overlooked the cost of annotating the evaluation set. Active learning assumes expensive labeling costs so its essential to consider the cost of evaluation set annotations. Active testing the problem addressed in this paper has been underexplored. The work focuses on minimizing labeling costs for the evaluation set. The paper proposes two main ideas: an estimator function that predicts the expected error of a task model and an acquisition function to minimize the proposed error function. The paper evaluates its method on MNIST with distribution shift scenarios and error estimation without distribution scenarios on CIFAR10 CIFAR100 and FMNIST. Strengths and Weaknesses: Strengths: Novelty: The paper introduces an error estimation function (Active Surrogate Estimator) and a test data acquisition function. Experiment: The paper demonstrates performance with and without distribution shift highlighting the methods applicability in scenarios with a missing class (an extreme case of class imbalance). Sufficient experiments are conducted on CIFAR10 CIFAR100 and FMNIST to demonstrate effectiveness. The proposed method significantly outperforms competitors in both distribution shift scenarios and without distribution shifts. Analysis: The paper decomposes the error bound of the Active Surrogate Estimator and provides meaningful interpretations of each term. Weaknesses: 1. Weak Novelty and Limitations of Surrogate Estimator: Weak Novelty: While the paper presents both an Active Surrogate Estimator and an acquisition function for active testing the surrogate estimator concept is not entirely original. Equation (4) which represents the surrogate estimator is essentially a loss prediction of a task model using soft pseudo labels. The performance heavily relies on the surrogate models accuracy. Limitation: The success of Equation (4) depends on the surrogate models performance an aspect not analyzed in the paper. 2. Experiment with Strong Task and Surrogate Models: It is possible that the authors used strong task and surrogate models in the experiments. The high accuracy achieved suggests that the surrogate models performed well. Training both task and surrogate models on a large number of samples (80 of the training dataset for CIFAR10 and CIFAR100) may be due to the dependency of Equation (4) on surrogate model performance. 3. Missing Analysis and Limitation: The importance of surrogate model performance should be analyzed and addressed in the paper. 4. Missing Details or Experiments: The paper only shows one case for distribution shift analysis. The graphs in the paper display squared errors but the absolute scale of the true expected losses and the critical value for reasonable estimation error are not provided. Normalizing the graphs over true loss could provide a more meaningful perspective. Questions: 1. How sensitive is the RASE (Risk Active Surrogate Estimator) to the accuracy of the surrogate model 2. What is the absolute scale of the true losses in the experiments 3. How would the graphs look if they were normalized over true loss"], "xqyEG7EhTZ": ["Paraphrased Summary: This paper introduces optimizations that significantly reduce memory required for training transformer models. By decreasing memory usage larger batch sizes can be employed resulting in faster training times. Traditional methods for reducing memory consumption include checkpointing offloading and compression. This paper however presents novel modifications such as inplace GELU layer inplace Layer Normalization and Dropout. Evaluations were conducted on BERT RoBERTa and GPT2 models demonstrating enhanced training throughput and the feasibility of larger batch sizes during BERT training. Strengths and Weaknesses: Strengths: Demonstrates effective techniques for improving transformer model training using inplace layer computation. Addresses a crucial issue in training large models. The paper is generally clear and wellwritten. Weaknesses: Its questionable if this is the first paper to analyze memory footprint in transformer training as inplace computation is a common optimization technique. Lack of originality in the proposed method. More experiments and comparisons with existing techniques could solidify the papers claims. Questions and Suggestions for Improvement: Compare the proposed method with other techniques like CheckpointTempo to showcase its compatibility. Contrast it with compression techniques to demonstrate the tradeoff between accuracy loss and efficient training. Evaluate the method on a wider range of models that utilize GELU proving its broader applicability. Provide a comprehensive list of layers from various models (CNN RNN GANs) that can potentially benefit from inplace computation further reducing memory footprint in training. Table 2 should include throughput metrics and total memory usage highlighting the key improvements achieved. The caption for Figure 3 needs to be reformatted for better clarity.", "Paraphrased Summary: This study introduces Tempo a method to shrink the memory usage of transformer models during training without compromising accuracy. Tempo utilizes several optimization techniques including inplace operations recomputation and softmax engineering. It reduces memory requirements during the backward pass while preserving computational speed. Experimental evaluations demonstrate a twofold increase in batch size for BERTLarge pretraining at a sequence length of 512 leading to a 16 throughput improvement. Strengths: Ingenious techniques to enable recomputation from output to input reducing memory overhead. Efficient dropout avoidance that eliminates recomputation of unnecessary inputs. Comprehensive and wellwritten paper with helpful supplemental materials. Weaknesses: Tempo requires more memory than checkpointing techniques though this is expected. Its optimizations are primarily focused on transformerbased models. Evaluation inconsistencies: Different sequence lengths are used for different measurements which could affect comparisons. Questions: No data was provided in the given text to address the issue of the impact of increasing hidden layers on accuracy and memory footprint.", "Paraphrased Statement: Summary: This study presents a solution called Tempo which uses three main techniques (inplace GELU inplace LayerNorm and sublayer dropout recomputation) to cut down on the memory needed for training transformerbased models. Due to the reduction in memory use the same GPU can handle gradientbased optimization with bigger minibatches leading to faster training. Strengths: Outofmemory issues are a major concern when training transformerbased models especially large foundation models. The analysis of memory usage in transformer models during training is helpful. Tempo shows a significant performance boost compared to current baselines and checkpoint methods. Weakness: The new methods in Tempo are not particularly innovative. Their implementation seems straightforward. The study lacks a theoretical discussion on the lossy compression used in the Tempo system. Questions: The experiments seem to have been conducted on multiGPU machines. Did the benchmarks use any specific parallelization scheme", "Paraphrased Statement: This study introduces Tempo a collection of three straightforward optimizations that minimize the memory consumption of large language model (LLM) pretraining. Tempo significantly reduces the memory requirements for LLM pretraining (e.g. BERT RoBERTa GPT2). The authors demonstrate that this memory reduction translates into increased training efficiency. Strengths: Simplicity and effectiveness of the memory footprint reduction optimizations Comprehensive empirical evaluation demonstrating both memory savings and performance gains Clear and wellorganized paper Weaknesses: Concern about the originality of the proposed optimizations specifically: Inplace conversion of activation and normalization functions for memory reduction is not a novel concept Prior research and implementations have proposed similar techniques for reducing memory requirements Questions: 1. Can you provide details on the differences between inplace GELU and layer normalization compared to inplace ReLU and batch normalization Are there novel approaches used in converting these operations to inplace 2. Have you assessed the pretraining performance on alternative libraries with compiler optimizations enabled (e.g. PyTorch with JIT or TensorFlow with XLA) Could existing compiler optimizations potentially impact memory consumption and training efficiency Are the proposed optimizations complementary to these existing techniques"], "r-6Z1SJbCpv": ["Paraphrase: Summary: Researchers trained a powerful language model on a wide range of optimized hyperparameter configurations (HPO trajectories) from various tasks and algorithms. This model is fed information about the task algorithm and search space at the start and then learns to predict the best hyperparameters and optimized metric for each step in the HPO process. Strengths: Impressive results comparable to or even outperforming existing HPO algorithms. Potentially a groundbreaking approach for the HPO field. Insightful analysis through ablation studies. Clear and simple methodology. Weaknesses: No opensource code trained model or training data for reproducibility. Limited evaluation metrics (undisclosed and only presented in plots not tables). Lack of a comparison to stateoftheart BO methods like HEBO. Need for further investigation into the impact of metadata based on a model trained without it. Questions: Does the discretization of scalar inputs affect model performance How would results differ with normalized input values Why does training on a smaller dataset (RealWorldData) lead to better performance on a larger dataset (HPOB) Definition of \"temporal traintest splits\" (line 269). Method of calculating the Thompson Sampling utility function.", "Paraphrase: Summary: This paper proposes a hyperparameter tuning method called OptFormer which leverages metadata and a transformerbased architecture to optimize hyperparameter configurations for different tasks with varying search spaces. It learns from existing hyperparameter settings encoded as textbased metadata marking it as a unique approach in hyperparameter optimization. Strengths: Combines hyperparameter transfer with metadata in an \"openset\" setting allowing for different hyperparameter spaces across tasks. Introduces OptFormer a transformerbased tuner that predicts hyperparameter values and performance metrics using a sequencetosequence training paradigm where metadata is mapped to hyperparameter configurations. Demonstrates effectiveness through experiments on realworld and benchmark datasets outperforming existing methods in terms of prediction accuracy and hyperparameter optimization. Weaknesses: Limited information provided on the meta knowledge learned from textbased metadata and how OptFormer imitates other HPO algorithms. Concerns about potential bias if the learned prior is applied to unseen tasks or more complex HPO algorithms and hyperparameter spaces. Lack of ablation studies to validate the choice of transformer architecture. Questions on the calibration of OptFormer compared to GPbased methods and the reasons behind its performance on different datasets. Questions: How does OptFormer imitate other HPO algorithms and does it simply memorize their choices Can the proposed method adapt to more complex HPO algorithms and larger hyperparameter spaces Are there insights into why OptFormer exhibits better calibration than GPbased methods What is the ECE comparison between OptFormer and OptFormer (TS) Why does OptFormer perform better than GPUCB on the RealWorldData dataset despite GPs known calibration capabilities", "Summary Paraphrase: The paper introduces a novel metalearning technique for hyperparameter optimization (HPO) leveraging a transformer model. The model utilizes offlinegenerated data that encapsulates optimization problem details such as the search space and trial history. It then integrates with HPO methods like Thompson sampling or upper confidence bounds to recommend new hyperparameter configurations during inference. Strengths and Weaknesses Paraphrase: Strengths: Aims to develop a general metalearning approach for HPO applicable across datasets models and search spaces. Provides clear explanations of the model components including tokenization inference and decoding. Employs robust baselines and an ablation study to support the approach. Weaknesses: Limited evaluation of generalization beyond training data e.g. in neural architecture search or gradientfree optimization. Scalability with search space dimensionality is uncertain. Dataset generation methodology appears ad hoc potentially increasing size and requiring a smaller model. Pretraining details such as network architecture decisions and training challenges are not fully disclosed. Questions: Section 6.2: Clarification on how the predictive distribution p(y...) is computed given that transformers only predict discrete outputs. Section 6.4: Explanation of how Random Search is combined with Thompson sampling (Random SearchTS). Computational resources and training time required for the transformer model. Availability of the dataset and code for result reproduction.", "Paraphrased Summary: The authors propose using a Transformer model to optimize hyperparameters. They claim its the first such application and that it outperforms traditional HPO methods. Strengths and Weaknesses: Strengths: Thorough evaluations Conceptually sound Weaknesses: Limited novelty Poor readability Questions: 1. Why is maxi(yiyrand)(ymaxyrand) considered a good metric Why not use yi and plot it as a range 2. How can OPTFormer outperform GPUCB without any prior data given that GPUCB is designed for learning without priors Code access would allow for verification. 3. The study doesnt consider a wider range of black box optimization algorithms. Consider exploring the following repositories: https:github.comfacebookresearchbotorch https:github.comfacebookresearchnevergrad https:github.comfacebookresearchLaMCTS https:github.comuberresearchTuRBO"], "m16lH6XJsbb": ["Paraphrase: This study introduces an innovative method for data augmentation in imagebased reinforcement learning (RL) with the goal of improving policy generalization. Strengths: Clear motivation and novel concept of \"frequency augmentation\" for RL generalization. Adequate experiments and ablation studies demonstrate the effectiveness of the proposed approach. Weaknesses: Limited coverage of related work. Questions: 1. How will SRM perform in different camera perspectives 2. What would happen if a shallower encoder with three convolutional layers were used instead of the deep 11layer encoder in the study Suggestions: 1. Discuss the connection between SRM and recent advances in masked imagefrequency modeling. 2. Investigate the use of alternative mask shapes such as fanshaped and frequencyband masks. References: [15] Relevant research papers on masked imagefrequency modeling masked latent reconstruction for reinforcement learning and masked world models for visual control.", "Paraphrased Summary This paper explores how to transfer learned policies to new visual environments in deep reinforcement learning. It introduces a novel data augmentation technique called \"Spectrum Random Masking (SRM)\" that operates in the frequency domain. SRM can be easily incorporated into existing policy generalization algorithms such as DrQ and SVEA. The paper demonstrates the effectiveness of SRM on the Distracting DMControl benchmark where the environment background is altered with random colors or distracting videos. Key Strengths Clear and accessible presentation Intuitive visualizations (Fig. 2 and Fig. 3) illustrating the concept of frequencydomain augmentation Easytoimplement and plugandplay augmentation method that integrates with various policy learning algorithms Potential Weakness The paper lacks experiments on more challenging and realistic environments such as autonomous driving robotics or indoor navigation which limits the generalizability of the findings.", "Paraphrase: Summary: This paper introduces a novel data augmentation method called Spectrum Random Masking (SRM) that masks out frequency components. To make SRM adaptable the authors develop techniques to maintain a stable Q value after augmentation. The proposed method outperforms the SVEA approach in video background generalization tasks. Strengths: Clarity and comprehensibility Simple and practical implementation Superior performance to SVEA in video background scenarios Enhances the possibilities for incorporating powerful augmentations in RL Weaknesses: SRMs effectiveness is limited to observations with a broad spectrum (e.g. video backgrounds) it does not excel in color backgrounds. This reduces its versatility and realworld applicability. Limited testing environment: only DMCGB was utilized which may not adequately demonstrate its generalization capabilities (e.g. videos are copied and pasted onto the background). Questions: 1. When should SRM be applied For RL practitioners it is unclear when this technique is most appropriate. 2. Can the method be tested on a different task To further validate its generalization ability testing on a task other than DMCGB (e.g. DrawerWorld) would be valuable.", "Paraphrased Statement: This research introduces an innovative technique for enhancing data augmentation in reinforcement learning based on image observations. The method involves modifying specific areas of the frequency spectrum within the images. Compared to current data augmentation techniques the proposed approach leads to improvements during training and when applied to broader scenarios. Key Strengths and Weaknesses: Strengths: Clear explanations and helpful visualizations of the Spectrum Random Masking (SRM) method. Novel and effective addition to data augmentation techniques. Detailed analyses supporting the specific design choices in the default SRM. Comprehensive experimental evaluations. Weakness: Lack of realworld applications where data augmentation has a significant impact. Questions: While basic data augmentation methods require minimal computational overhead how computationally demanding is the SRM technique"], "u3vEuRr08MT": ["Paraphrase: This study examines how large language models remember and forget data during training based on factors like model size dataset size learning rate and task type (causal vs. masked language modeling). Memorization is measured as the accuracy of predicting correct labels (i.e. predicting the correct masked token) during training while forgetting is the decrease in memorization. The study finds that: Larger models memorize data more quickly and forget less. Memorization precedes overfitting. Memorization occurs for various dataset and language modeling types regardless of learning rate. Further analysis reveals that: Unique identifiers in the training data (e.g. numbers nouns proper nouns) are more readily memorized. Models exhibit a lower bound on forgetting that increases with model size. Strengths: The study sheds light on the crucial aspect of memorization in large language models. It challenges the traditional biasvariance tradeoff framework. It provides insights into the dynamics and scaling laws of memorization and forgetting in these models. Weaknesses: The study does not explicitly discuss hypotheses for the faster memorization with increased dictionary tokens.", "Paraphrased Summary: This paper investigates how large Transformer language models handle memorization. It shows that larger models tend to remember more specific details (unique tokens like nouns and numbers) from the training data. However they are also less prone to \"forgetting\" previously learned information. Strengths and Weaknesses: The paper is clear and provides a meaningful experiment to measure memorization. It uses a wide range of model sizes. However it is unclear which Transformer model family is used. The experiments are conducted on a relatively small dataset (WikiText103) which may not accurately reflect the behavior of models trained on larger data. The investigation into catastrophic forgetting is interesting but the study on memorizing parts of speech could benefit from exploring the role of word frequency. Questions: Why do the authors refer to BookCorpus as the \"RoBERTa\" dataset Which BookCorpus size (16GB or 160GB) is used in the experiments Are the results specific to causal or masked language models Would the catastrophic forgetting experiment yield different results if the \"special batch\" came from data very different from the training data", "Paraphrase: This paper provides a thorough exploration of data memorization in language modeling training. It reveals that larger models memorize data more quickly and that memorization precedes overfitting. Specific parts of speech particularly nouns and numbers are prioritized for memorization during training. To evaluate forgetting mechanisms a special insertion batch from the validation set is used to test models of varying scales. Strengths: Welldefined metrics provide detailed experimental results. Forgetting identifier experiments are meticulously designed. These nuanced findings contribute to understanding how and what data is memorized in transformerbased language modeling. Weaknesses: Specific examples showcasing POSwise memorization and forgetting mechanisms would enhance the demonstration. The analysis could benefit from considering the different sizes of POS categories (e.g. numbers have a smaller vocabulary than nouns). This may limit the interpretability of the findings on noun memorization.", "Paraphrase: Summary The authors investigate the phenomenon of memorization in language models where the model predicts the correct word with high probability based on its context. They observe that larger models memorize more quickly and never fully forget memorized samples. They also analyze memorization patterns based on part of speech finding that rare words often lead to memorized sequences. Strengths Examines the process of memorization during model training. Demonstrates the relationship between model size and forgetting behavior. Provides insights into the impact of memorization on privacy. Weaknesses The definition of \"memorization\" is questionable as some correct predictions may not indicate memorization. Certain scenarios such as multiple labels for the same context may complicate the memorization assessment. The analysis of partofspeech memorization is incomplete as it equates correct partofspeech predictions with memorization. The experiments on memorization and overfitting may require further clarification. The literature review lacks citations to relevant work and introduces concepts without proper context. Minor Issues The term \"generally monotonically decreasing\" should be replaced with \"generally decreasing.\" The implementation of reference implementations should be clarified. Ambiguous sentences need to be rewritten for clarity."], "v6NNlubbSQ": ["Summary This research explores an approach to estimating uncertainties in singledeterministic neural networks by using density estimation in the networks feature space. Specifically the authors employ a nonparametric density estimation method (kernel density estimation) which allows for fewer assumptions compared to other methods like Deep Uncertainty Quantification (DUQ). Contributions Implementation of nonparametric kernel density estimation for estimating density in epistemic uncertainty quantification. Improved outofdistribution detection on CIFAR100 ImageNet and text classification with rejection. Theoretical argumentation and consistency proofs for the proposed method in classification with rejection. Strengths Novel density estimation approach for epistemic uncertainty quantification. Improved OOD performance as indicated by ROC AUC on various benchmarks. Nonparametric density estimation eliminates assumptions about feature space distributions made by previous methods potentially leading to performance improvements. Rigorous evaluation with outofdistribution detection on challenging datasets. Weaknesses Difficulty comprehending the papers statistical details and machine learning concepts. Excessive use of alternative notation making the paper challenging to follow. Lack of clarity in the practical implementation of the method. Limited evaluation of aleatoric and epistemic uncertainty disentanglement. Reliance on a computationally expensive density estimation method and lack of evaluation of approximation quality. Questions How is the \"properly chosen bandwidth\" selected to ensure convergence of eta differences to the desired distribution Have other approximation methods for kernel density estimation been considered Would the authors consider revising the paper to present binarymulticlass details in the main paper and move derivations to the supplementary materials", "Paraphrased Statement: This paper introduces Nonparametric Uncertainty Quantification (NUQ) a userfriendly approach to predicting uncertainty using kernel density estimators. It foresees the distribution of future data using kernel density estimators (Equation 1) and measures uncertainty by calculating the squared magnitude of the kernel (Equation 2). NUQ distinguishes between uncertainty inherent in the data and uncertainty resulting from the models limitations. The paper demonstrates that NUQ can effectively estimate the minimum risk under certain conditions. Experiments on largescale image recognition and language processing tasks show that NUQ outperforms other uncertainty quantification methods. Strengths: Offers a straightforward and innovative way to quantify uncertainty based on kernel density theory. Provides theoretical guarantees for minimizing the risk of making decisions under uncertainty. Demonstrates strong performance in experiments on vision and language tasks. Weaknesses: Some technical details are missing or not easily accessible in the main text. For instance the meanings of p(x) and \\mathbfu are not explained clearly. The paper does not provide specifics on how \\sigma2 and p(x) are defined or estimated in the experiments. Hyperparameter optimization strategies are not discussed which can be important in practice when test or outofdistribution data are unavailable.", "Paraphrased Statement: Summary: In this work the researchers propose a technique for estimating uncertainty using a kernelbased estimator for the distribution of conditional labels. This method is flexible as it can incorporate embeddings from any neural network model. The researchers demonstrate its utility through empirical evaluations on tasks such as outofdistribution (OOD) detection and rejectionbased classification. Strengths: Adaptability: The method can integrate with any existing neural network model. Extensive Evaluation: The evaluations cover various baselines and models. Weaknesses: Difficulty in Comprehension: The methodology section may be challenging to understand. Lack of Traditional Model Uncertainty Metrics: The researchers did not use traditional model uncertainty measures such as mutual information as epistemic uncertainty benchmarks. They also omitted their distilled version for comparison. Questions: Line 2426: Is it incorrect to state that MaxProb solely represents aleatoric uncertainty Line 33: Confirm if \"Such methods are very perspective..\" is a typo. Line 4243: Identify the methods that require covariance computation. Equation 2: Clarify the definition and calculation of u. Line 129130: Define the \"standard deviation of data label.\" Related Work: Consider including references to [13] on uncertainty distillation and rejectionbased classification using Bayesian decision theory. Discuss their relevance and potentially compare the distillation method experimentally. Runtime: Evaluate the additional computational cost of approximating the conditional label distribution and its scalability with respect to model complexity and dataset size."], "tq_J_MqB3UB": ["Summary The paper introduces an approach to mitigate generalization issues in computer vision due to data or class distribution shifts. The authors connect previous work on weight averaging and standard ensembling and present a new analysis explaining when weight averaging fails under different shift scenarios. Their proposed method of averaging diverse weights improves results on a benchmark dataset suite. Strengths Wellwritten and organized. Demonstrates a strong understanding of ensembling and transfer learning. Convincing argumentation for weight averagings robustness under covariate shift. Illuminating analysis on the ineffectiveness of flatnessbased minimizers for mitigating outofdistribution errors. Weaknesses Does not acknowledge other current approaches for outofdistribution generalization such as SWAG Deep Ensembles and Loss Surface Simplexes. Exaggerates the extent to which the paper follows the DICE objective for decorrelating ensemble members learning procedures. Ignores the potential impact of neural collapse and learning subspaces on transfer learning. Lacks a comparison against simple deep ensembles in Table 1. Does not consider the practical scenario of finetuning models on target distribution data. Questions How can the inconsistency between Lemma 1 and Figure 1 be explained What is the precise definition of \"averagable\" members in an ensemble How can the tradeoff between training diverse members and ensuring their averagability be investigated further", "Paraphrased Summary: This paper introduces Diverse Weight Averaging (DiWA) which combines weights from multiple training runs with slightly altered hyperparameters. It suggests that DiWA is most beneficial when the error on outofdistribution (OOD) data is mainly due to covariance in the weights. The paper demonstrates better OOD performance for DiWA on the DomainBed benchmark. Strengths and Weaknesses: Compared to Ensemble Sampling (ENS) DiWA only requires a single forward pass during testing making it more efficient. Simulations and empirical data show that DiWA approximates ENS in training settings. Additionally DiWA can be combined with other techniques (e.g. LP initialization ERMMixupCoral) to further improve performance. Questions: What is the performance of ENS using models generated by DiWA What is the approximate computational cost of training 60 models for the final results Were error bounds in Table 1 not provided due to high computational expenses Minor Comment: The sentence \"\u2026why WA outperforms in OOD SharpnessAware Minimizer\u2026\" could be rephrased for improved clarity.", "Paraphrased Statement Summary The authors propose a simple way to enhance outofdomain generalization by averaging weights from multiple training runs instead of relying on a single run. They provide theoretical analysis to back up their weight averaging (WA) method and its enhancement. Experiments using the DomainBed benchmark showcase the methods effectiveness. Strengths Exploration of an intriguing and underdeveloped research area: determining when WA is successful and how to optimize it. Theoretical foundations for WA and the proposed approach. Weaknesses Potential overclaims: The abstract claims that WA is the best method for outofdistribution generalization in computer vision. However the introduction suggests this assertion is based solely on DomainBed observations. Limited contribution: The proposed method does not exhibit a significant performance advantage over existing approaches (e.g. MA) in the experimental results. In Table 1 under random initialization it performs comparable to SWAD. Computational expense: The method requires combining weights from multiple runs (20 in the experiments) which may incur unnecessary computational costs. Questions How is the optimal number of runs determined Does a higher number generally correlate with improved performance Why is MMD utilized in Proposition 3 Would other divergences yield similar results", "Paraphrase: Summary: This study explores the factors that enhance the generalization of outofdomain models through weight averaging. They propose a novel biasvariancecovariancelocality decomposition of the target domain error demonstrating the importance of diversity. Based on this they introduce Diverse Weight Averaging (DiWA) which combines the weights of independently trained models initialized identically. They show that DiWA surpasses weight averaging of checkpoints from a single training run and other methods on the DomainBed benchmark. Strengths: Clear and wellstructured writing free of errors. Novel decomposition analysis that explains why weight averaging is effective and motivates DiWA. Valid evaluation on the standard DomainBed benchmark with fair comparisons. Weaknesses: While DiWA outperforms singlerun weight averaging (WA) it may not be feasible for certain applications such as training large language models. Training multiple models for DiWA may be less efficient than scaling the model or extending training time. Question: It would be interesting to investigate the performance of DiWA and WA when combined with SharpnessAware Minimization (SAM) during training."], "o3HXEEBKnD": ["Paraphrase: This paper introduces the LabelAware global Consistency (LAC) regularization to address the Single Positive Multilabel Learning (SPML) problem where instances are only labeled with a single positive label. LAC utilizes manifold structure information to promote global consistency within labelspecific embeddings. This approach brings intraclass embeddings closer while separating interclass embeddings. Extensive experiments on benchmark datasets demonstrate the superior performance of the proposed method. Strengths: 1. Introduces a novel LAC regularization for SPML. 2. Exploits the clustering assumption in SPML effectively. 3. Provides clear visualizations and experimental results for accessibility. Weaknesses: 1. Lacks justification for certain statements (Page 5 Lines 150153). 2. Introduces attentionbased labelwise embedding modeling but neglects to discuss it in the main method description (Page 3 Line 104). Question: Table 1 reports LANs mAP on COCO as 64.1 while Table 2 shows the ablation method assuming negative achieving 68.5 on COCO. Given that both methods use the same setting the discrepancy of 4.4 between these results requires clarification.", "Paraphrased Statement: Summary This research focuses on the single positive multilabel learning (SPML) task where only the most prominent positive label is known for each instance. The authors introduce a labelaware global consistency loss specific to this task. Employing a twostep optimization strategy their method demonstrates strong performance across various benchmarks. Strengths and Weaknesses The paper is accessible and the proposed method is straightforward yet effective. The authors leverage the manifold assumption in SPML instead of clustering demonstrating its empirical advantages. Suggestions for Improvement 1. Enhance Figure 1 by illustrating the distinction between SPML and traditional multilabel learning. Avoid unnecessary elements that clutter the figure. 2. Clarify the labelwise embedding discussed before Equation 1 and its relationship to Equation 1. Explore its relevance in standard multilabel learning scenarios. 3. Address the difficulty of obtaining the true number of positive labels in realworld applications and propose strategies for handling such situations before conducting experiments. Questions 1. Is the label embedding in Equation 7 instancespecific Its indexing using K is confusing. 2. Evaluate the models performance without a pretrained backbone to determine its reliance on external feature extraction. 3. Consider additional evaluation criteria for SPML such as the accuracy of inferring latent labels. 4. Investigate the relationship between model performance and the number of positive labels in the dataset. PostRebuttal Comment The authors have addressed the raised concerns effectively.", "Paraphrase: Summary This study focuses on multilabel learning (MLL) with only one positive label per image. Each image has just one assigned label. To recover potential labels the paper applies pseudolabels to unobserved labels based on model predictions. For better labeling the study employs global consistency regularization for labelspecific embeddings to improve feature representation clarity. Experiments were conducted using benchmark MLL datasets. The results demonstrate the proposed methods superiority. Strengths and Weaknesses Strengths Offers an effective solution to MLL problems with single positive labels (SPMLs). Introduces labelaware global consistency to reduce feature representation dispersion. Wellmotivated and understandable method. Achieves impressive results and outperforms comparable methods on benchmark datasets. Provides ablation studies and hyperparameter sensitivity analyses. Weaknesses Lacks clarity in experimental setup such as the assignment of single labels to images. Does not consider label correlations a crucial factor in MLL. Shows significant superiority on COCO dataset compared to others without detailed explanation. Contains language errors needs careful proofreading. Questions 1. Can the proposed method handle scenarios where multiple labels are assigned to images instead of just one 2. In Section 4.4 the performance curves in Figure 3 show that the method improves performance with increasing threshold only on the COCO dataset. What is the reason behind this observation 3. Could incorporating similarity information between embeddings from different labels improve classification performance", "Summary This research focuses on a new MultiLabel Learning (MLL) task known as Single Positive MLL (SPML) where only a single positive label is known. The authors utilize Anchor Network (AN) for SPML and introduce two regularization techniques: PseudoLabeling Consistency (PLC) and LabelAware Global Consistency (LAC) to combat the issue of false negatives caused by AN. Experimental results demonstrate the superiority of the proposed method (AN PLC LAC). Strengths and Weaknesses Strengths: PLC and LAC effectively address the false negative issue in AN for SPML. The proposed method AN PLC LAC outperforms existing SPML methods. Clear and informative ablation study and case study. Weaknesses: Questions: PLC (Eq. 4): The indicator function in PLC may pose training difficulties. Please provide insights into the convergence of the proposed method. Intraclass connection matrix I: The matrix I has a size of B2 which can be computationally expensive. What are the sizes of I in your experimental datasets and how feasible is it to train with such a large matrix LAC (Eq. 7): LAC utilizes only the matrix I excluding \\barI. Please explain the role of \\barI in this equation."], "xbhsFMxORxV": ["Paraphrased Statement: Summary Classifying social text data can be difficult due to its brevity and lack of context. Previous methods have tried adding external information but it may not always be available. This paper introduces Hyphen a new model that uses public discourse (e.g. comments and replies) to improve classification. Model Description Hyphen consists of three modules: a candidate post encoder a public discourse encoder and a coattention mechanism between them. Individual comments are represented as AMR graphs which are merged into a macroAMR. This macroAMR is embedded using a Hyperbolic Convolutional Neural Network. The candidate post is embedded using a Hyperbolic Hierarchical Attention Network which transforms words into sentences and sentences into a document representation. The coattention mechanism combines these representations using a Hyperbolic Fourier Coattention mechanism. It transforms the representations extracts salient features and learns an affinity matrix to calculate attention weights between the post and public discourse signals. Evaluation and Interpretation Hyphen was evaluated on datasets for detecting fake news rumors hate speech and sarcasm. The results show that Hyphen is effective. Strengths Hyphen leverages public discourse and captures the hierarchical structure of social text. The Fourier coattention module allows the model to focus on important parts of the input and provides some degree of interpretability. Weaknesses The notation in the paper may require simplification. Some references are broken.", "Paraphrase: Summary: This research focuses on identifying fake news hate speech rumors and sarcasm on various social media platforms. It uses a novel approach that combines graph representation learning with attention mechanisms. Methodology: The candidate text (e.g. a social media post) is encoded using a hyperbolic word encoder enhanced by attention. Then abstract meaning representations are used to create hyperbolic graph representations of related social media posts. Evaluation: The authors compare their method to both general neural network baselines (e.g. BERT) and taskspecific baselines (e.g. CNNbased approaches). They find statistically significant improvements on most tasks tested. Ablation Study: The authors examine the impact of using social media posts and the Fourier coattention mechanism. Strengths: Consistent performance improvements across tasks especially with a large number of posts. Can incorporate multiple social media posts. Comprehensive ablation study demonstrates the importance of method components. Improved interpretability of sentence importance. Weaknesses: The paper does not specify how social media posts were encoded for pretrained language models like BERT. Questions: How are candidates and their posts encoded Does Table 2 show average results across multiple runs with different seeds", "Paraphrase: Summary: This paper presents a novel approach using hyperbolic Fourier coattention to exploit collective knowledge in social text classification (e.g. fake news and hate speech detection). Previous research has demonstrated the value of user comments and reactions in enhancing text classification performance. The proposed method leverages a coattention mechanism to integrate this collective knowledge within the Abstract Meaning Representation (AMR) graph derived from comments and reactions. Notably it employs hyperbolic geometry operations (addition and multiplication) instead of their traditional counterparts in the attention and Fourier transforms. Experimental results indicate that the proposed method outperforms both specialized and general neural models. Strengths: 1. Impressive model performance 2. Effective capture of hierarchical structure in the AMR graph 3. Extensive evaluation Weakness: The comparison with taskspecific stateoftheart (SOTA) models is a concern because these SOTA models typically use preBERT architectures with weaker linguistic cue capture capabilities. Generic neural models outperform taskspecific SOTA models making it unclear whether the proposed Fourier Hyperbolic CoAttention mechanism offers significant improvements over existing dataspecific SOTA techniques. Suggestions: To address this concern the authors should provide evidence that the proposed method outperforms taskspecific SOTA models that incorporate BERT or present visualizations of the learned embeddings to demonstrate the models effective capture of hierarchical structure.", "Paraphrase: This paper aims to improve social text classification by utilizing the insights contained in comments and replies on social media platforms. To facilitate this it introduces a novel approach called Hyphen which combines hyperbolic graph representation learning with a Fourier coattention mechanism. Comprehensive experimentation across 10 datasets demonstrates the effectiveness of Hyphen. Strengths: Leverages social media discourse for improved text classification. Extensive evaluation on 10 datasets and 4 tasks. Weaknesses: Insufficient documentation of experimental setup (e.g. dataset splits). Justification for choosing hyperbolic Fourier coattention for social text classification is lacking. Absence of societal impact discussions. Questions: 1. What dataset splits were employed across the 10 datasets 2. Were the baseline results reported in Table 2 obtained from direct citations or reproduced by the authors 3. Can the authors provide a more detailed justification for the use of hyperbolic Fourier coattention in enhancing social text classification 4. Can the authors consider incorporating newer language models (beyond BERT and RoBERTa) as baselines for comparison 5. Were crowdsourced annotations used for the PolitiFact dataset If so please provide additional information as required by the referenced checklist."], "tUH1Or4xblM": ["Paraphrased Statement: This paper presents a novel method for creating motion segmentation from motion fields using a combination of a UNet and a Transformer neural network. The UNet generates segmentation masks for each time snapshot of the motion field while the Transformer assigns a depth ordering to the masks assuming that each motion segment has a distinct depth. Synthetic flow data is used to train the model without manual labeling. It has been tested on existing datasets and new data showing promising results. Strengths and Weaknesses: The paper has a logical structure and clear writing. The approach combines a UNet and Transformer effectively. The evaluation is thorough and demonstrates good performance. The model is limited to a fixed number of layers (queries) currently set to three. It may not fully address occlusion boundaries or handle high numbers of motion segments. The influence of optical flow method choice and the assumption of three layers require further clarification. The models behavior under specific scenarios such as frontoparallel motion or camouflaged objects is not explicitly discussed. Additional Questions: How would the method perform in scenes with multiple people moving in the same direction Is the method sensitive to the choice of optical flow algorithm How was the assumption of three layers determined How can the method handle cases where appearance and flow information conflict such as camouflaged objects", "Summary Paraphrase: This study examines the challenge of segmenting multiple moving objects in a video using optical flow data. The proposed method combines three key elements: 1. Optical flow inputs allow for training with simulated videos enabling generalization to realworld scenarios. 2. A depthordered layered representation efficiently handles object occlusions. 3. A modelbased representation helps improve segmentation results. Compared to methods using nonlayered representations the proposed approach demonstrates superior performance. Strengths and Weaknesses Paraphrase: Strengths: Originality in combining objectcentric layered and model representations for video object segmentation. Experiments are conducted on realworld videos providing more convincing results than prior work on simulated datasets. Clear and easytounderstand methodology. Weaknesses: Some important details are deferred to supplementary material which could be better presented within the main text. Testtime adaptation significantly enhances performance but details should be included in the main text. Question Paraphrase: Can the proposed layered representation method handle cases where different objects have complex mutual occlusions such as where part of object A occludes object B and part of object B occludes object A If not are there solutions within the layered representation framework for these cases", "Paraphrased Summary: Researchers have developed a new method for segmenting videos into distinct objects even when parts of those objects are hidden (amodal segmentation). Using a neural network inspired by transformers (a type of AI architecture) the model predicts the shape and depth of each object despite being trained on a synthetic dataset. When tested on standard video segmentation benchmarks the model performs well without additional training and outshines previous unsupervised methods and even rivals supervised methods specifically trained for these benchmarks. Strengths and Weaknesses: This approach to amodal segmentation is unique and may yield more complete and useful object representations than traditional methods that only consider visible object parts. However the models superiority over other methods is unclear as it was trained on a different dataset and not directly compared under the same conditions. A fairer comparison would train all models on the same synthetic dataset and evaluate them on the benchmarks. Questions: Clarification on \"Unsupervised\" Model: The model is trained with ground truth supervision which contradicts the label of \"unsupervised.\" It should be clarified that the model performs \"unsupervised transfer.\" Computational Details: The calculation of the models segmentation process needs further clarification. Specifically the initial accumulated opacity layer (\u03b10) should likely be zeros as stated and the addition of modal segmentation masks to obtain opacity layers may result in negative weights which can be problematic. Its unclear if the researchers took steps to address this issue.", "Question: Does the papers claim of being an unsupervised method for video object segmentation contradict the use of synthetic supervision Paraphrased Discussion: The paper presents a model for video object segmentation without direct human supervision. However it employs synthetic supervision by training on synthetically generated data. This raises a question about whether the model can be truly considered unsupervised. The author argues that the use of synthetic supervision is justified because it allows the model to scale in performance with the volume of synthetic data. In principle if the model can outperform supervised methods that use limited supervision it would demonstrate the benefits of synthetic data and support the claim of unsupervised learning. Additionally the model introduces the novel concept of producing amodal segmentation maps as an intermediate step. This allows the model to segment objects even when they are partially occluded. However the evaluation of this amodal output is limited. The author suggests that future work should explore the models performance on dedicated amodal segmentation tasks."], "l7aekTjF6CO": ["Summary The research investigates the process of random citizen selection (sortition) in a democratic context. Two key criteria are fairness (equal participation opportunities) and representation (reflecting the full populations diversity). The authors introduce a representation metric based on the distance between an individual and their qclosest panel member. Using this metric they analyze the relationship between fairness and representation. They provide theoretical results demonstrating that fairness and representation can be mutually exclusive. However they present an algorithm that balances both criteria. The study includes experimental evaluations on datasets showcasing the effectiveness of the proposed approach. Strengths Clear and wellwritten presentation Novel representation metric based on distance analysis Straightforward and intuitive theoretical results Convincing experimental evaluations Weaknesses Algorithm 1 requires minimization over all possible subsets which may be computationally intractable. The authors need to provide a more efficient method. Minor Comments The paper falls outside the usual scope of NeurIPS (machine learning) but sortition is an important democratic issue. Provide additional references on representation constraints in candidate selection. Questions Main Weakness: How can Algorithm 1 be computed efficiently for larger values of k (e.g. k100 as in citizens assemblies) Approximate Runtimes: What are the approximate runtimes of the algorithms", "Paraphrased Statement: Summary: This study evaluates the fairness and representation of sortition a random method for selecting groups. Sortition typically involves randomly selecting k participants with equal probability. However the study investigates whether alternative selection methods could lead to more representative panels with minimal compromise on fairness. Fairness is defined as the goal of selecting each participant with a probability of at least kn while representation is the ability to select a panel where nonmembers are similar to members. The study utilizes the \"qsocial cost\" metric which is the sum of distances between each individual and their qth closest panel member. The study finds that for q \u2265 k2 uniform random selection provides optimal representation. For smaller q achieving good representation with minimal fairness loss is challenging. However the study develops an algorithm that modifies a low social cost panel to improve representation. Experiments demonstrate the effectiveness of the algorithms. Examples are provided for both uniform selection and RandomReplace showing varying qsocial costs for different k and q values. Strengths and Weaknesses: The study introduces qsocial cost as a suitable metric for evaluating panel quality. Measuring metrics beyond fairness is valuable in sortition. The paper is generally wellwritten but further intuitive explanations of representation and fairness would enhance clarity. The proofs are clear and seem valid. Uniform selection is a simple and useful algorithm contributing to theoretical understanding. RandomReplace is intriguing but may remain theoretical due to the complexity of finding optimal panels in realworld settings. Discussing the significance of optimal panels would be valuable. The experiment section is informative and provides visualization of qsocial cost changes. Clarification of some experimental details would improve comprehension. Minor Issues: Line 127: Correction: \"denotes the distance between i and j.\" Line 209: Definition of P should be included earlier. Line 218: \"The algorithm reaches this line when it considers...\" Line 224: Consistency in terminology would be ideal (e.g. \"agent\"). Questions: 1. Is RandomReplace intended to balance fairness and representation compared to the optimal panel If so is this evident in the experiments 2. Are there multiple perfectly fair algorithms in Section 3 If so are they equivalent to uniform selection", "Paraphrase: The authors investigate the representativeness of panels selected through sortition which involves randomly selecting individuals from a population. They assess representation based on the similarity between panel members and individuals in the population. Each person is assumed to prefer representation by the most similar panel member. The \"qcost\" for an individual is the similarity distance between that individual and the qth closest panel member. The panels overall cost is the sum of the qcosts for all individuals. Representation is defined as the worstcase ratio between the lowest achievable panel cost and the expected cost (due to the randomized selection process). The authors demonstrate that when q is greater than half the panel size high representation is feasible otherwise it is not. Strengths and Weaknesses: Strengths: Rigorous and conclusive theoretical results. Weaknesses: Modest incremental value over prior research. Unclear practical significance of experiments. Plots lack standard deviation information. Minor grammatical errors. Questions: Why is it necessary to measure similarity distances rather than assuming a binary \"represent\" or \"not represent\" relationship"], "mCzMqeWSFJ": ["Paraphrased Summary: This research proposes a new neural network design that can learn representations of 3D graphs primarily for applications related to molecules. The network efficiently exchanges information throughout the graph using a \"message passing\" system that operates within 1hop neighborhoods of each node. This approach ensures that different conformations (shapes) of the same molecule lead to distinct representations. Strengths and Weaknesses: The proposed method demonstrates sound methodology and provides valuable insights for researchers in the field. The networks performance is evaluated using extensive public datasets demonstrating its effectiveness compared to existing approaches. However the papers writing could be improved for clarity. Questions: What is the reference for SchNet which is mentioned on page 2 line 83 How is the distance for an edge generated (page 3 line 89) What does \"SE(3)\" refer to and where is it defined (page 3 line 89) What specific differences in structures are included in the analysis on page 3 line 91 What is the reference for DimeNet mentioned on page 3 line 94 How large is the local neighborhood for which SphereNet guarantees completeness (page 3 line 95) How is the singleatom unit shown in Figure 3 applied to the entire molecule An example using a conformer from Figure 2(b) would be helpful. How are atoms in a molecule processed: one by one or concatenated for further processing The properties listed in Tables 4 and 5 require full names and references for their definitions.", "Paraphrased Statement: This paper presents ComENet a framework for representation learning on 3D molecular graphs. ComENet utilizes message passing to capture local information within molecules and incorporates rotation angles to account for global structural information. The model offers computational advantages over existing methods and demonstrates effectiveness in experimental evaluations. Strengths: Provides a theoretical foundation for capturing global completeness overcoming the reliance on intuition. Experimental results support the accuracy and efficacy of the proposed framework. Weaknesses: Several areas require clarification: Define the variables n and k used in O(nk2). Explain the notation m x d and define m. Provide details on the functions R() and SE(3). Define the term \"strongly connected\" in Proposition 1. Specify that v represents the node feature vector. Define the notation Lij j\\in Ni. Typographical error: \"achieves\" should be \"achieve.\" Uncertainty in understanding the calculation of rotation angles based on Figure 1(b): Clarify why specific planes are selected for angle calculation. Explain the rational behind the choice of planes used in angle calculation. Questions: The author is requested to address the concerns identified in the weaknesses section.", "Paraphrase: Summary This paper presents a new messaging system for 3D representations of molecules thats more complete (actually onetoone) and better in other ways including: Works within a small local area Less complex calculations Trains and predicts faster Tests show that our approach works just as well as others on many different sets of data and is clearly more efficient. Strengths Focuses on a completeness issue that previous work has missed. Efficient and complete messaging system for molecular graphs. Comprehensive analysis and testing on various tasks and metrics. Solid experimental results support the effectiveness of the system. Weaknesses The paper emphasizes completeness and efficiency but experiments seem to focus more on the latter. For completeness experiments could specifically target tasks like identifying molecular conformers which is key to the argument. Questions Can we assume that baseline methods have the same network architectures as stated in line 320 Could the messagepassing paradigm be compared to other approaches (e.g. SphereNet) with identical network architectures Providing details on baseline and ComENet network sizes could strengthen the findings. Suggestions Explain the messaging scheme in Section 2.5 before discussing completeness in Section 2.22.4. Summarize the final formulation and notations for clarity.", "Paraphrased Statement: Summary: The authors introduce ComENet a novel network for predicting molecular properties using 3D information. It employs a new message passing mechanism (Equation 1) and a unique network architecture (Figure 3). Strengths and Weaknesses: Originality: While the overall framework resembles existing models like PaiNN and DimeNet the message passing approach in Equation (1) is novel. Significance: The accuracy reported in Tables 3 and 4 is inferior to baseline models. ComENet was not compared to stateoftheart algorithms on the OC20 dataset. It is unclear whether Equation (1) can be directly integrated with existing backbones like DimeNet for improved efficiency. Clarification: Section 2 should clearly outline the key concepts and their contributions to the models performance. Questions: See \"Strengths And Weaknesses\" section for specific questions."], "q4IG88RJiMv": ["Paraphrased Summary: This paper suggests a new approach to approximate the Area Under the Receiver Operating Characteristic Curve (AUROC) using the arc length of the curve. It proposes optimizing the arc length based on theoretical justifications linking it to AUROC. Strengths: Introduces a novel and intriguing concept. Provides some examples to illustrate the idea. Weaknesses: Optimizing arc length may not fully align with optimizing AUROC itself as demonstrated in the provided examples. Despite its novelty as an alternative to traditional AUROC optimization methods the paper lacks justifications for its potential advantages over existing approaches. The experimental evaluation is limited to CIFAR10 logistic regression and a pairwise squared loss optimization method without considering deep learning experiments or comparisons with other wellestablished optimization techniques. The paper makes claims about the proposed methods computational efficiency without providing sufficient justifications. Questions: As highlighted in the \"Weaknesses\" section please address the following: The discrepancy between optimizing arc length and optimizing AUROC itself. The lack of justifications for the advantages of the proposed method over mainstream AUC optimization algorithms. The limited experimental evaluation and the need for more comprehensive testing including deep learning experiments and comparisons with other methods. The lack of supporting evidence for the computational efficiency claims.", "Paraphrase: Summary: This study establishes a connection between the Receiver Operating Characteristic (ROC) curve for testing two distributions p(\\boldsymbolx) and p(\\boldsymbolx) and the \"distance\" between them. Specifically the length of the optimal ROC curve corresponds to a type of fdivergence between p(\\boldsymbolx) and p(\\boldsymbolx). This fdivergence can be estimated using its variational formulation. The optimal solution to the equivalent variational problem is \\arctan[\\fracp(\\boldsymbolx)p(\\boldsymbolx)]. Therefore by solving the empirical version of this problem an estimate of \\arctan[\\fracp(\\boldsymbolx)p(\\boldsymbolx)] can be obtained. Strengths: The correspondence between the optimal ROC curve length and fdivergence is a novel finding. The paper is wellwritten and engaging. Weaknesses: The condition in Proposition 4 requiring that \\arctan[\\fracp(\\boldsymbolx)p(\\boldsymbolx)] belong to a specific function class \\mathcalH may be restrictive. It would be helpful to discuss the implications of approximation errors in \\arctan[\\fracp(\\boldsymbolx)p(\\boldsymbolx)]. The paper could benefit from additional numerical experiments such as comparisons with other methods of choosing score functions t(x) and results on different datasets. Questions: 1. Why is a second step needed in the twostep procedure mentioned in Line 264 The goal is already to estimate \\arctan[\\fracp(\\boldsymbolx)p(\\boldsymbolx)] which is accomplished in Step 1. 2. The author could clarify under what choice of g the optimal solution of the variational problem (Line 141) becomes a monotone transform of the likelihood function. 3. There may be a typographical error in Eq. (1) which should potentially read [\\partial\\tau\\tildeF(\\tau)]2.", "Paraphrased Statement: Summary This paper explores the connection between fdivergence and the arc length of Receiver Operating Characteristic (ROC) curves. Beginning with the observation that ROC curves are commonly used to compare distributions the authors demonstrate that the arc length of an ROC curve can be considered a type of fdivergence. They derive an algorithm to estimate the arc length of an ROC curve and present experiments showing promising results for a score function obtained using a twostep procedure. Strengths Clear presentation of the research topic Logical and stepbystep approach to answering the question about ROC curve comparison Weaknesses Insufficient exploration of the relationship between the arc length of ROC curves and other fdivergences such as KL divergence and total variation distance Lack of comparison with other fdivergences in the experimental analysis Questions for the Author Can you provide more insights into the relationship between the arc length of ROC curves and other fdivergences Can you elaborate on why the experimental analysis did not include comparisons with other fdivergences", "Paraphrase: Strengths: Establishes a novel connection between the Receiver Operating Characteristic (ROC) curve and variational fdivergences represented as densityratio estimators between positive and negative distributions. Proposes an estimator based on the arclength of the optimal AUC curve rather than the standard AUC. Provides convergence guarantees for the estimated arctangent of the logdensity ratio under certain conditions. Weaknesses: Questions about the originality of the theoretical results. Lack of information on computational time for the proposed method. Technical and notationheavy explanations particularly in sections 4.1 and 5 which make them difficult to understand. Vague abstract. Questions: How do the theoretical convergence results compare to previous work on divergence functionals and likelihood ratio estimation Minor Points: Clarification needed on the term \"surface area\" (line 40). Explicit mention of thresholds for false positive rate (FPR) and true positive rate (TPR) in line 7374 would be helpful. Correction of typo \"maximual\" to \"maximal\" (line 40)."], "nJt27NQffr": ["Summary This paper introduces Maximum Entropy Encoding (MEC) a selfsupervised learning method that uses the principle of maximum entropy to extract unbiased representations from image datasets (tested on ImageNet). By combining maximum entropy with contrastive learnings augmentationinvariance objective the authors define a novel loss function with a log determinant term approximated using a Taylor series. Notably the resulting loss relates to existing methods like SimSiam and Barlow Twins through different orders of approximation. Evaluation on ImageNet shows MEC outperforms or matches existing objectives in pretraining tasks including linear evaluation semisupervised classification transfer learning for object detection and segmentation and video tracking. Strengths Originality: MEC unifies previous contrastive learning methods under a single theoretical framework enabling future adaptations and explorations. Quality: The experiments are clear and follow standard settings with results consistently matching or exceeding existing benchmarks. Clarity: The paper is wellwritten and easy to understand leveraging pseudocode for reproducibility. Significance: The mathematical foundation linking a diverse range of contrastive objectives provides a convergence point for the field and opens avenues for future research. Weaknesses Originality: While MEC consolidates existing work it does not introduce a fundamentally different ultimate objective. Quality: Pretraining experiments on datasets other than ImageNet would enhance the papers scope. Questions Can the authors clarify the source of improvements over cited benchmarks Are there any results for pretraining on datasets beyond ImageNet Could MEC benefit selfsupervised learning on smaller datasets", "Paraphrase: This paper presents a jointembedding objective known as Maximum Entropy Coding (MEC) which is related to MCR2. MEC directly maximizes information content by minimizing the encoding length. It combines batchwise (SimSiam) and featurewise (BarlowTwins) objectives implemented as a Taylor series expansion. Despite strong experimental performance (combined with techniques like exponential moving average and asymmetric networks) the paper acknowledges that this performance largely stems from combining existing techniques. Strengths: Theoretical basis in the principle of maximum entropy. Strong empirical performance on a range of tasks including ImageNet linear probe semisupervised classification transfer learning and object detection. Provision of experimental details pseudocode and hyperparameters. Weaknesses: Significant performance gain is primarily attributed to a mixture of design components raising questions about the specific contribution of MEC. The papers main contribution is unclear: The principle of maximum entropy has been explored in other methods like MCR2 and BarlowTwins. The higherorder approximation provides only a marginal performance improvement. The practical advantage of the mixture model is not sufficiently demonstrated. The authors emphasize comparisons with fundamental frameworks but neglect similar mixture models in their field.", "Paraphrase: This research introduces Maximum Entropy Coding (MEC) a new approach inspired by the principle of maximum entropy from information theory. MEC is designed to learn generalizable representations even without labeled data. The method employs minimal lossy coding and a Taylor series approximation to make maximum entropy estimation practical. Experiments demonstrate that MEC consistently surpasses existing selfsupervised learning (SSL) methods on various downstream tasks. Additionally MEC shows robustness to different hyperparameters (such as smaller batch sizes) and architectures (such as Vision Transformers). Strengths: Simplicity and scalability High performance across various downstream tasks Robustness to hyperparameters and architectures Weaknesses: It is not clear what specific information MEC learns that existing SSL methods cannot. The paper mentions that existing SSL methods introduce \"biases\" that MEC does not but does not provide specific details or empirical evidence to support this claim. It is not explained why and how MEC outperforms other SSL methods. Minor Concerns: Small top margins in the papers layout Narrow caption spaces Recommendation: The empirical results supporting MEC are strong but some explanations are lacking. Therefore the reviewer recommends \"Weak Accept\" with revisions to address the concerns."], "sof8l4cki9": ["Summary This paper demonstrates theoretically that the Stochastic Gradient Descent (SGD) algorithm for scaleinvariant loss functions converges rapidly within O(1\u03bb\u03b7) steps using a novel time rescaling limiting Stochastic Differential Equation (SDE). SGD without weight decay (WD) has a much slower convergence rate in the limiting SDE highlighting the advantages of WD for generalization performance. The paper also provides empirical evidence supporting the rapid convergence observed in asymptotic analysis. Strengths: Proof of the fast equilibrium conjecture explaining the behavior of SGD in deep learning. Introduction of a new time rescaling SDE. Clear and welljustified results. Theoretical significance in understanding SGD behavior. Weaknesses: Insufficient explanation of the assumptions used in the proof. Limited exploration of the implications of the mixing property. Discretization analysis needs further development. Typos in the paper should be corrected. Questions: Can the assumptions used in the proof be easily satisfied in practical applications Can the mixing property lead to new insights beyond the proof What are the discretization errors involved in the analysis", "Paraphrase: Main Results: 1. SGD combined with weight decay (SGDWD) converges in distribution to a random variable within a specific number of steps that depend on the step size weight decay parameter and iteration budget. 2. When the step size and weight decay parameter are inversely proportional SGD without weight decay has a slower convergence rate but the same limiting behavior as SGD with weight decay. 3. Under certain assumptions about the loss function and noise SGDWD partially supports the \"faster equilibrium\" conjecture. Strengths and Concerns: The paper is generally wellwritten. The interplay between normalization gradient noise and weight decay in SGD is a significant research question. The authors provide theoretical results on the convergence dynamics of SGD with weight decay and empirical evidence supporting their findings. Specific Concerns: In the experiments on a toy example: The authors mention an equation that is not explained earlier in the text. It is unclear how to guarantee that no gradient vector is aligned with the optimal solution. In the CIFAR10 experiments: The algorithm appears unstable with significant fluctuations in accuracy. The initial step size may be too large. It is not clear how the authors determined the number of epochs for training (100). The authors claim that time scaling can be used to adaptively adjust the step size but this requires further verification. Questions: The concerns raised in the above sections require clarification.", "Paraphrased Statement: The paper makes several significant contributions: 1. It defines a new Stochastic Differential Equation (SDE) describing SGD with Weight Decay (SGDWD) with appropriate time scaling making it amenable to prior work by Katzenberger. 2. It demonstrates that SGD without Weight Decay for stochastic scaleinvariant loss has identical limiting dynamics as SGDWD but experiences exponential slowdown. 3. Assuming minimizers form a compact manifold and noise is nondegenerate in the manifolds tangent space the authors prove convergence of the limiting diffusion process to a stationary distribution for any initialization. 4. Empirical experiments support these theoretical findings. Strengths: Characterization of SGDWDs limiting dynamics with a novel time scaling approach. Alignment of SGDWD dynamics with prior work by Katzenberger. Weaknesses: The time scaling approach may be the sole distinctive contribution. Other results largely follow from previous work by Li et al. Implications for MLDL Models: The papers findings provide theoretical insights into the dynamics of SGD and SGDWD during training. This knowledge can inform algorithm design and optimization strategies for Machine Learning and Deep Learning (MLDL) models.", "Paraphrase: Summary: This study investigates the mixing time of models where parameters follow a stochastic differential equation (SDE) that approximates stochastic gradient descent (SGD) with weight decay. The paper demonstrates that under certain conditions the parameters converge to a unique distribution at a rate proportional to \\Theta( \\fracTln( \\lambda\\eta) \\eta \\lambda). Strengths: The paper proves that under specific assumptions SGD iterates converge in distribution to a solution of an SDE approximating SGD with weight decay. The technical approach utilizes time rescaling and a scaling factor that closely resembles the parameter normalization factor. Time rescaling helps apply the Katzenberger theorem ensuring that the negative gradient loss dominates the SDE. A corollary suggests that scaleinvariant models without weight decay converge much slower exponentially. Weaknesses: The analysis relies on technical assumptions such as constant noise covariance trace asymptotic results and smoothness assumptions. The strong assumptions are necessary given the complexity of the problem. Questions: Could similar results hold if the noise covariance trace is bounded but not constant Could comparable theorems apply to models with varying homogeneity degrees instead of scale invariance"], "v9pljSdlUNP": ["Paraphrase: Summary: The paper explores Stochastic Backpropagation (SBP) for image classification and object detection. Training memory usage is significantly reduced with minimal accuracy loss. Comprehensive derivations and experiments support the authors findings. Strengths and Weaknesses: Strengths: Practical approach that conserves memory without compromising model performance. Easy to implement for users with memory constraints. Weaknesses: Novelty: The concept of SBP is not new and the proposed method builds upon an existing CVPR paper. Technical Contribution: The paper lacks a significant technical challenge that it tackles and solves. Writing: The writing is difficult to follow and contains unclear sections (e.g. \"slightly loss\"). Questions: What are the specific differences between the proposed method and the original CVPR papers SBP approach What was the most technically challenging problem faced and solved by the paper", "Paraphrase: Summary Stochastic Backpropagation (SBP) is a memorysaving training technique for video models. The authors conducted a comprehensive study of SBP and applied it to train deep neural networks for image classification and object detection. Unlike previous SBP approaches for video models which dropped frames during backpropagation while keeping all frames in forward propagation this approach drops feature maps in earlier layers during backpropagation but retains all feature maps in forward propagation. Similar to dropout this strategy is applied selectively during backpropagation. SBP is investigated for pointwise convolution convolution and transformer layers. The behavior of the SBP approximation varies depending on the operator. For pointwise convolution the SBP approximation results in zero gradients for dropped indices while gradients for retained indices are exact compared to standard gradient descent (SGD). For convolution layers with kernel sizes greater than 1 the SBP approximation is inexact and nonzero regardless of which indices are dropped or retained except in certain special cases. In addition to studying SBP the authors experimented with different design choices including the number of layers to apply SBP to the keep ratio and the dropping strategy. Experiments revealed that applying SBP to 23 layers with a keep ratio of 0.5 using gridwise sampling provided substantial GPU memory savings without a significant drop in accuracy. Moreover the gradients exhibited a high correlation with SGD gradients in terms of cosine similarity. The generalizability of SBP was evaluated on the ImageNet image classification benchmark and the COCO object detection benchmark. SBP reduced GPU memory usage by 40 with a less than 1 drop in accuracy. Strengths and Weaknesses: SBP is a memoryefficient training method. It generalizes well to image classification and object detection tasks. It is relatively simple and efficient to implement. SBP provides significant GPU savings and faster training speeds with minimal accuracy loss. Unlike exact training methods like gradient accumulation SBP introduces approximations that may result in some loss of accuracy. Fixed grid sampling for backpropagation potentially biases feature importance. While cosine similarity suggests that SBP approximated gradients are correlated with SGD gradients it is unclear how well this translates to similar final weights or correct example identification in ImageNet and classification tasks.", "Summary Paraphrase: This paper explores the application of Stochastic Backpropagation (SBP) to object detection and image classification leveraging both convolutional neural network (ConvNeXt) and transformer (ViT) backbones. It extends the concept initially proposed in [4] for video inputs to a wider range of vision tasks. The paper formalizes SBP proposes efficient implementations and evaluates different design strategies to optimize its application. The effectiveness of SBP is demonstrated through quantitative analysis on the ImageNet and COCO datasets. Technical Contribution Paraphrase: This work makes a significant contribution by demonstrating the broader applicability of SBP to vision tasks. It improves memory efficiency during network training by randomly selecting subsets of layers for gradient calculations during backpropagation. The proposed efficient implementation in Algorithm 1 eliminates the need for value recomputation during backpropagation. The paper provides valuable insights into the impact of design choices through the analysis in Section 3. However the vast number of potential SBP configurations raises questions about the generalizability of these observations. Specific findings such as grid sampling outperforming random sampling for gradient selection (Figures 3 and 4) provide valuable guidance for future research. Presentation Paraphrase: The paper effectively introduces the problem and clearly outlines its objectives in expanding upon [4]. However the placement of Related Work as the last section (Section 5) could benefit from being moved earlier to provide context before the introduction of SBP. Some minor grammatical errors are present but the overall presentation is clear and engaging. Strengths and Weaknesses Paraphrase: SBP demonstrates remarkable performance in vision tasks reducing memory consumption by 54 (Tables 1 and 2) with minimal accuracy loss. The paper contributes a straightforward and efficient implementation suggesting the potential for widespread adoption of SBP. The indepth analysis of design choices provides a solid foundation for future research. The key limitation lies in the need to address the risk of vanishing gradients when discarding gradient information. Questions: 1. How well do the insights from the design experiments on a 4layer network generalize to deeper networks especially those with skip connections 2. Are specific precautions necessary when using SBP to prevent vanishing gradients considering the potential for catastrophic loss of information when discarding gradients", "Paraphrased Statement: Summary: The research presents a method called \"stochastic back propagation\" (SBP) for use in image classification and object detection. This method involves randomly dropping certain feature maps during the back propagation process leading to memory savings and faster training. Strengths: The study extends the application of SBP to image classification and object detection broadening its potential use cases. The researchers demonstrate the methods effectiveness with transformer and ConvNeXt architectures. The analysis provides valuable insights and demonstrates the methods utility. Weaknesses: The research contains minor typos and grammatical errors. A more thorough proofreading would enhance its clarity. Questions: The statement does not include any questions."], "skgJy0CjAO": ["Paraphrased Statement: Summary: This study proposes LogiGAN an adversarial pretraining architecture to enhance the logical reasoning capabilities of language models. LogiGAN employs a GeneratorVerifier framework where the Generator learns to predict logical statements masked within a given context. The Verifier evaluates the predicted statements for logical consistency. This approach addresses the nondifferentiability of sequential GANs. LogiGANs effectiveness is validated on 12 reasoningfocused datasets. Strengths and Weaknesses: Strengths: Clear motivations and organization Novel approach to improving logical reasoning in language models Superior performance on diverse reasoning datasets Informative ablation studies Weaknesses: Limited modeling of logical relations (ignoring negations conjunctions etc.) Manual identification of logical reasoning markers hindering generalization Evaluation results reported only on the development set reducing credibility Performance improvement on generation datasets less pronounced than on classification datasets Limited comparison with related work focusing on different relation types Lack of opensource implementation or code compromising reproducibility Questions: 1. How do you ensure that maskedout statements are factually verifiable statements with complete subjectpredicate structures 2. How do you guarantee that the Detection Heuristics accurately identify conclusions or premises For example \"so\" can be an adverb expressing degree rather than a logical connective. 3. What are the parameter statistics for LogiGAN This information is crucial for fair comparisons.", "Paraphrased Summary: This paper introduces LogiGAN a new generative adversarial network (GAN) approach for language models that integrates logical reasoning. The authors argue that: 1. Logical reasoning enhances language model capabilities. 2. Adversarial pretraining improves performance on NLP tasks that require reasoning. 3. The GeneratorVerifier mechanism allows for extensive pretraining with variable target lengths. These assertions are supported by experiments across 12 datasets including an ablation study. Paraphrased Strengths and Weaknesses: Strengths: Novel approach to GANs for language models demonstrating performance improvements. Comprehensive evaluation on 12 downstream tasks. Effective method for handling the nondifferentiability of SeqGANs. Weaknesses: The claim regarding largescale pretraining with arbitrary target lengths needs clarification. The paper lacks a comparison with stateoftheart models on the given tasks. The evidence for the orthogonality of linguistic and logic abilities is inconclusive as the masked sentence selection process may diminish both equally. Formatting errors in Figure 2 and Table 1. Incorrect paragraph heading punctuation. Paraphrased Questions: How do the qualities of generated examples compare to intrinsic logic ability What is the value of x in Equation 1 (should it be c)", "Summary Paraphrase: LogiGAN is a novel approach that overcomes the challenges of sequential GAN training by introducing a GeneratorVerifier consensus mechanism. This enables largescale pretraining with extended target lengths without differentiability issues. Through extensive experiments and component analysis LogiGANs effectiveness and the importance of its functional elements are demonstrated. LogiGANs performance is evaluated on 12 datasets highlighting the significance of logical reasoning in general problemsolving tasks and the efficacy of LogiGAN in improving language models logical capabilities. Strengths and Weaknesses Paraphrase: Strengths: Clear organization and logical flow Compelling motivation based on hypotheses about logics role in general reasoning tasks and the potential of PLMs logical abilities Robust experimental evaluation Weakness: Considerably close performance numbers in Table 1 raising concerns about statistical significance Lack of an average performance column in Table 1 Questions: 1. Are the results for the vanilla T5 base models reported in your own implementation or obtained from other sources", "Paraphrased Statement: Summary: This paper introduces LogiGAN an unsupervised adversarial pretraining framework that enhances the logical reasoning capabilities of pretrained models. The framework employs heuristic rules to identify logical indicators and incorporates a masked logical statement prediction task instead of traditional masked language modeling. Additionally an iterative generatorverifier adversarial training process is introduced to produce more logically consistent statements. Experiments across 12 datasets demonstrate LogiGANs superior performance. Strengths: 1. The motivation for improving logical reasoning in pretrained models is compelling and LogiGAN effectively addresses this need. 2. Experimental results show that LogiGAN outperforms T5 on various datasets. 3. The paper is clearly written and easy to understand. Weaknesses: 1. The novelty of LogiGAN is somewhat limited. Its primary contributions include the masked logical statement prediction task and the adversarial training framework. However these ideas have been explored to some extent in previous research. 2. The generation of pseudostatements for verifier training raises concerns. While selfsampling and BM25based retrieval are used its unclear why these methods provide highquality negative samples that are logically inconsistent. 3. Masking taskspecific texts in pretraining may hinder the generalization of pretrained models to broader NLP tasks. Conducting experiments on general benchmarks such as GLUE would provide a more comprehensive evaluation. Questions: Why do selfsampling and BM25 retrieval generate effective negative samples for verifier training How does masking taskspecific texts impact the generalization of pretrained models to other NLP tasks"], "uCBx_6Hc7cu": ["Paraphrased Statement: Summary: This research introduces a novel framework that combines associative memory models with variational inference principles. Key Idea: In standard predictive coding models we aim to discover the cause behind observed data which can be influenced by prior knowledge. When the prior is modeled as a collection of memories finding the cause that minimizes a specific mathematical metric can be interpreted as a memory retrieval process. Equivalence to Hopfield Networks: In a particular case when the prior has a Gaussian mixture distribution without a hierarchy the resulting model closely resembles a modern Hopfield network. Novel Update Rule: The authors present a simplified update rule based on variational inference which is similar to the update rule used in universal Hopfield networks. Semantic Space: Using semantic information in memory retrieval improves the performance of memory models. The covariance matrix of the prior distribution can also be optimized to enhance retrieval results. Findings: 1. Memory retrieval accuracy improves when memories are encoded in a semantic space. 2. Training the model on retrieval tasks enhances its performance. 3. Distancebased similarity metrics outperform dot products for memory retrieval. Strengths and Weaknesses: Strengths: Solid theoretical basis Improved performance over baseline models Weaknesses: Confusing writing in the methods section Lack of visual aids to explain model operations Proposed models perform worse than baselines on pixelspace storage and retrieval tasks Some results are not significantly better than competitors Questions and Suggestions: Provide sketches of the proposed models for clarity. Explain the stepbystep retrieval process for the VAEBPGMM model. Consider exploring the neural implementation of the proposed associative memory models.", "Summary This paper combines the Modern Continuous Hopfield Network (MCHN) with the latent space of a Variational Autoencoder (VAE) to explore the connection between predictive coding and the Free Energy Principle (FEP). It demonstrates these approaches on two baseline datasets. Strengths and Weaknesses Strengths: Demonstrates the integration of MCHN into VAEs latent space. Provides an analysis of the relationship between predictive coding FEP and GMM representations. Offers equations that link MCHN and FEP. Weaknesses: Lacks a comparison to a baseline VAE model. Assumes the importance of the relationship to FEP without providing strong justification. Fails to explain the biasing of the GMM prior towards patterns with larger Euclidean norms. Uses MOHN on the CLEVR dataset which may compromise vector similarity metrics. Does not clearly define \"representation\" as compressed latent embedding or raw image pixels. Limited performance analysis based on only 100 images. Lacks analysis on the impact of the number of memories stored. Needs to clarify the difference between VAEHopfield and VAEGMM. Does not cite relevant prior work (e.g. associative memory MCHNs SDM Attention Kanerva Machine) that address similar concepts. Assumes distancebased metrics are superior to dot products without justification. Does not discuss the limitations of MCHNs due to their lack of normalization. Fails to demonstrate clear benefits or implications of the work. Acknowledges the NTMs superiority but does not explore this further. Lacks analysis on memory capacity and normalization of the memories. Questions: Provide justification for the weighting of Gaussian components by the Euclidean norm of the memory. Conduct experiments on memory capacity and normalization of memories. Include a comparison to a baseline VAE model. Cite and discuss relevant prior work. Clarify the difference between VAEHopfield and VAEGMM. Explore the potential benefits and uniqueness of this work in the context of existing literature.", "Paraphrase: Summary: The paper establishes a connection between predictive coding (PC) and associative memory (e.g. the modern continuous Hopfield network). It demonstrates that associative memory retrieval can be reinterpreted as a type of probabilistic inference using hidden variables. The paper proposes several inference algorithms based on this framework. Strengths: It draws connections between distinct concepts studied in different fields namely variational inference predictive coding and associative memory. The link between PC and the modern continuous Hopfield network is novel. Weaknesses: The conditioning of latent variables on memory is not entirely new. Similar approaches have been explored in prior works such as Kanerva machines [12] and variational memory [3]. The text is unclear containing multiple versions of Equation 2 between the framework (Section 3) and methods (Section 4). The experiments are limited in scope. Questions: Can you clarify the papers contributions in the context of existing works (e.g. Kanerva machines and variational memory) as highlighted in the Strengths and Weaknesses section Please provide more details about the memory M. How does it relate to Mk in Equation 2 How are patterns stored in the memory References: [1] Wu Y. Wayne G. Graves A. Lillicrap T. (2018). The Kanerva Machine: A Generative Distributed Memory. In International Conference on Learning Representations. [2] Wu Y. Wayne G. Gregor K. Lillicrap T. (2018). Learning attractor dynamics for generative memory. Advances in Neural Information Processing Systems 31. [3] Le H. Tran T. Nguyen T. Venkatesh S. (2018). Variational memory encoderdecoder. Advances in neural information processing systems 31.", "Paraphrase: Summary: The authors investigate methods for integrating autoassociative memories with variational Bayesian (VB) approaches. Using recent advances in Hopfield networks they examine iterative memorybased inference through the lens of predictive coding. Specific Findings: A VB interpretation of autoassociative memories and the link between predictive coding and Memorybased Competitive Hebbian Networks (MCHNs) Four new models that combine autoassociative memory and iterative VB inference to varying degrees Strengths and Weaknesses: Weaknesses: Omissions of relevant prior research leading to unclear research questions and lack of apparent practical or neuroscientific motivations Deceptive clarity: Derivations assume fixed or negligible covariances which may not warrant a Bayesian framing Limited numerical evaluation including a lack of experiments using autoassociative memory for \"oneshot learning\" Relevant Literature: Ramapuram et al. (2021) Wu et al. (2018a 2018b) Questions: Please clarify the research motivation and justify the use of Bayesian terminology given the assumed covariances. Provide additional numerical evaluations to demonstrate the potential value of the proposed models."], "uxc8hDSs_xh": ["Summary Paraphrase: This study introduces a novel unsupervised learning model for solving the maximum clique problem. Additionally a greedy decoding method is proposed to convert the continuous output of the neural network into discrete values. The model utilizes the scattering transform to mitigate oversmoothing issues in Graph Neural Networks (GNNs). The proposed model demonstrates promising performance on established benchmarks and synthetic instances of varying complexity. Strengths and Weaknesses Paraphrase: Strengths: Simplicity and Clarity: The approach is straightforward and wellexplained in the paper. Benchmark Comparison: The model is evaluated on an experimental setup from the literature enabling direct comparisons with previous work. Competitive Results: The model achieves strong performance across multiple datasets. Weaknesses and Criticism: Insufficient Attribution: Parts of the models pipeline are not correctly attributed to existing work particularly the loss function. Overemphasis on Oversmoothing: While the model tackles oversmoothing other GNN architectures and skip connections can also address this issue. Incorrect Problem Terminology: The authors refer to the \"maximal clique\" problem which suggests ease of finding solutions but the intended problem is likely the NPhard \"maximum clique\" problem. Missing Experimentation: The paper lacks experiments comparing the model to other techniques on Xu instances and greedy heuristics. Limited Applicability: The model only addresses the maximum clique problem without significant innovation beyond previous approaches. Scalability Concerns: The paper does not address the scalability of the model for larger graphs. Questions: Data Generation: How were the instances in table 3 generated Was the RB model used or was the implementation created from scratch Architecture Relevance: Does the scattering architecture provide any specific benefits for the maximum clique problem or is its selection unrelated to this particular problem", "Paraphrased Statement: Summary The maximum clique problem is a challenging optimization problem with applications in various fields. Researchers aim to develop fast and efficient heuristics to solve it and machine learning has emerged as a potential tool. This study introduces a novel approach that employs a Graph Neural Network (GNN) trained to predict the likelihood of nodes belonging to a maximal clique. The GNNs predictions feed into a greedy algorithm that constructs large cliques. The proposed approach incorporates a geometric scattering transform which reduces smoothing among neighbors. Empirical results indicate improvements over alternative neural network methods and Gurobi with a time constraint. Strengths The method demonstrates speed advantages over competitors while maintaining comparable performance. The geometric scattering transform is a valuable innovation that could benefit other graphbased combinatorial problem heuristics. The unsupervised training approach allows for differentiable inference. Weaknesses The experimental benchmark datasets may be too simple making it difficult to differentiate methods. Bold highlighting of only two methods in Table 1 may obscure the fact that the proposed method is not the best on the most challenging dataset. The absence of comparison with \"normal\" maximal clique heuristics hinders the evaluation of problem difficulty. The decoders Tau parameter lacks justification and the goal of maximizing clique size suggests setting Tau to infinity. Gurobi time limits are not consistently respected raising concerns about result validity. Competing neural network approaches are absent from Table 3. Nonuniqueness of solutions in clique problems could lead to the GNN predicting an average of optimal solutions compromising clique maximality. The use of approximation ratio as a ranking criterion in Table 1 and time as a ranking criterion in Table 3 suggests selective criteria to favor the proposed method.", "Paraphrased Statement: The paper introduces a novel hybrid method that combines scattering and rulebased decoding to address the maximal clique problem in graphs. The method aims to alleviate oversmoothing in graph neural networks (GNNs) and outperforms existing approaches. While the paper is wellwritten and comprehensive it lacks sufficient motivation and could use more precise notation. Additionally the proof of Lemma 1 requires clarification and the results section could be better organized. One concern is that the approximation score only measures the size of the maximal clique and does not evaluate its accuracy against the ground truth. It would be beneficial to clarify why this measure was chosen instead of a metric like Jaccard overlap."], "lC5-Ty_0FiN": ["Paraphrased Statement: Summary: This research examines the structure of the last hidden layer in extensive neural networks. The authors discover that for networks that attain flawless training performance these representations have duplicate \"clones.\" They observe that removing neurons randomly from the last hidden layer impacts performance similarly to training a smaller network. This may clarify the innocuous overfitting seen in oversized neural networks. Strengths: The observation is unique and compelling. Some preliminary experiments including investigations of intermediate layers are presented. Weaknesses: The article lacks a discussion connecting the findings to pruning neural networks. Identifying and eliminating redundant neurons could lead to an efficient pruning technique not covered in the paper. The significance of the result is limited. The findings are confined to instances with zero training error and the use of weight decay. Observations may not apply if the network is stopped early or lacks regularization. Additionally the neural networks last layer must be exceptionally wide. Questions: 1. If redundant representations enhance generalization for wide neural networks the full network in Figure 1(b) should exhibit less error than the chunk network. However for large widths we observe the reverse. Either the explanation from the introduction is unclear or the conclusion is invalid. 2. Figure 1: Why is the error denoted as \"errorinf\" Why not display the error directly Also on what is the classification error based 3. What is meant by \"wc predicts the output with an accuracy that scales with wc(0.5)\" in finding number 1 Why is the slope of the error versus network size graph referred to as \"accuracy\" 4. Why is wc chosen at random Why not select neurons based on correlation values to ensure that correlated neurons are not grouped together while unconnected neurons are 5. The researchs main goal is to illuminate the biasvariance conundrum. However the specific link has not been established. How would your experiments affect the biasvariance curve 6. Does the observation regarding regularization only apply to weight decay or does it extend to other forms of regularization If the latter the findings scope would be significantly narrower. Additionally what is the rationale behind this", "Paraphrased Statement: This study investigated the statistical behavior of randomly selected neuron groups in the final layer of wide deep neural networks. It found that when these groups are large enough they perform similarly to a complete network of the same width suggesting they act as independent estimators of the same data features (\"clones\"). The study analyzes the properties of these clones. Strengths: The discovery of clones suggests that large neuron groups capture independent representations of the data. The hypothesis is supported by observations across multiple architectures and datasets. Thorough analysis confirms the hypothesis and describes clone properties. Weaknesses: The study used small datasets and networks. Clones may only emerge when models can closely approximate training data. Implications for largerscale settings remain unclear due to unknown clone properties. Minor: Figure 1b lacks a yaxis label. Question: Would the authors consider expanding their research to ImageNet datasets using models with increased redundancy in intermediate layers If so it would be intriguing to investigate the existence and properties of clones in such settings.", "Summary This paper investigates \"benign overfitting\" in convolutional neural networks (CNNs) by examining the representations in the final hidden layer. The authors observe and propose an explanation for the phenomenon that neurons tend to form groups carrying duplicate information when the network is wide enough and training error is zero. Strengths: Originality: The paper offers a novel perspective on the longstanding problem of why overfitting does not harm generalization in deep neural networks. Quality: The authors conduct extensive experiments and ablation studies on various datasets and network architectures. Clarity: The paper is wellwritten and clearly explains the background methods experiments and limitations. Significance: The problem addressed is crucial for understanding neural network training dynamics and designing wide and deep architectures. Weaknesses: 1. Architectural Limitations: The research focuses on CNNs. Its unclear if the findings generalize to Transformers or other networks with attention mechanisms. The dependence on limitedspan convolution units is questioned. 2. Training Error Uncertainty: The paper mentions the training error reaching zero but continuing training for the claimed results. It remains unclear if the error is actually zero and where the gradients originate from. The training curve or relevant statistics should be shown to understand any phase transitions or dynamical changes. If the error is zero parameter updates may solely result from weight decay which could introduce the phenomenon of duplicated neurons. 3. Regularization Considerations: The authors attribute the phenomenon to weight decay. Its unclear why standard regularizations (e.g. L2 norm or Gaussian priors) dont have similar effects. 4. Empirical Claim Uncertainty: The claim that error decays as W12 for chunks with w neurons requires more empirical support especially for complex datasets like CIFAR100 and ImageNet. 5. Proof of Distinct Clones: The authors claim that distinct clones differ only by uncorrelated random noise. Direct proof of this claim is lacking. Statistical analysis of neuron activations could verify this claim. 6. Other Issues: The reference to neural tangent kernels (NTK) is missing. Figures in the appendix should be referenced as \"Appendix Figures\" in the main text."], "ufRSbXtgbOo": ["Summary (Paraphrased): This research proposes MultiPfD as a decentralized optimization problem which can be solved using a multiagent approach. Enforcing consensus relaxation in this multiagent problem results in a less stringent condition for the existence of a MultiPS solution compared to the original singleagent case. The study investigates the DSGDGD scheme and proves its convergence towards the MultiPS solution. This scheme converges under the same condition as the existence of a MultiPS solution. Strengths and Weaknesses (Paraphrased) Originality: The proposed DSGDDG method differs from existing multiagent reinforcement learning algorithms by enabling convergence to a unique MultiPS solution. Quality and Clarity: The article is wellorganized and clearly written. Significance: The algorithm has practical applications in tasks requiring dynamic adaptation to environmental changes such as spam classification. Its effectiveness has been demonstrated in experiments with the spambasebased multiagent spam classification task. Question: The text does not mention whether the method can be applied to reinforcement learning to address the unique convergence problem of reinforcement learning algorithms. Thus this question cannot be answered from the provided statement.", "Paraphrase: Summary: This study examines a collaborative multiagent performance prediction problem where agents collectively aim to minimize the average of their individual loss functions by learning a shared decision parameter. The challenge lies in the dynamic nature of the local data distributions which are influenced by the agents decisions. Prior research has addressed similar problems in singleagent and competitive multiagent settings while this work focuses on a cooperative setting where agents share their decisions and strive for consensus. A decentralized learning algorithm is proposed along with its convergence analysis and numerical simulations to support the theoretical findings. Strengths: Clear and accessible writing style Engaging topic of performative prediction in machine learning Detailed explanations of problem setup algorithm and theoretical results Thorough comparison with existing work to highlight innovations and challenges Weaknesses: The multiagent performance prediction problem setup is novel and natural but its motivation is not fully convincing. The requirement for all agents to reach consensus on their local decisions seems artificial given that local data distributions depend solely on individual decisions. The use of a communication graph primarily for information exchange without any influence on local data distributions makes the multiagent problem appear localized. More realworld application examples are needed to justify the assumptions of the problem setup such as consensus and localized communication. Questions: The authors acknowledge the concern about the problem setups motivation. In their rebuttal they could elaborate on why specific assumptions like consensus and local communication are essential in practical application examples. Additional examples that demonstrate the applicability of the problem setup and the significance of its assumptions would strengthen the papers case.", "Paraphrased Statement: Summary: This paper explores the \"multiagent performative prediction\" (MultiPfD) problem which is formulated as a decentralized optimization task. The authors demonstrate that consensusseeking agents in MultiPfD can achieve unique stable solutions under less stringent conditions than in the singleagent case. They propose a decentralized \"DSGDGD\" scheme based on a greedy deployment approach and analyze its convergence properties showing that it converges to the stable solution with a linear speedup. Strengths and Weaknesses: Pros: Provides a framework for understanding MultiPfD with consensusseeking agents. Develops a practical DSGDGD scheme that converges to a stable solution. Demonstrates that consensusseeking agents relax the conditions for stability in MultiPfD. Cons: The proposed optimization methods lack novelty with limitations such as synchronous updates and strong convexity requirements. The main contribution lies in the formulation of the MultiPfD problem which is not a widely studied area. Questions: What is the current state of research in MultiPfD How does this paper advance the field and what are the key contributions From an optimization perspective are there any novel aspects to the proposed methods beyond the consensusseeking framework", "Paraphrased Statement: Summary: This study examines multiagent performative prediction (MultiPfD) where multiple agents collaborate to determine a joint decision from data that their choices can influence. The researchers present this as a decentralized optimization problem and establish a condition for a stable solution (MultiPS). They demonstrate that the consensus effect simplifies this requirement compared to singleagent cases. To address the problem the authors propose a greedy decentralized scheme (DSGDGD) that converges to the MultiPS solution. They analyze the convergence rate showing that heterogeneous user behavior and poor network connectivity can slow convergence but do not affect the asymptotic rate. Numerical experiments on synthetic and real data support the analysis. Strengths: Thorough theoretical analysis addressing key questions. Insightful discussion of contributing factors to theoretical results. Practical assumptions that do not overly restrict the setting. Clear presentation of results and proof strategy. Validation through numerical experiments. Weaknesses: Weak motivation for the multiagent setting. Limited discussion of assumptions on data distribution. Restricting assumption of strong convexity. Questions: Can the authors provide a stronger rationale for the consensus solution being desirable Can the results be extended to accommodate scenarios where the data distribution is influenced by both agent decisions and external factors How might the results be modified to handle weaker assumptions on loss function convexity"], "nOdfIbo3A-F": ["Summary Paraphrase: This paper proposes a networkbased method for modeling the dynamics of chains and ropes. Compared to unconstrained models it achieves better accuracy and energy behavior. Strengths and Weaknesses Paraphrase: While previous research has focused on applying graph neural networks (GNNs) to simpler systems this paper suggests that GNNs can also be used for more complex systems. However the paper does not address the challenges of simulating rigid bodies such as collision detection friction or 3D rotation. Additionally some of the benefits attributed to the proposed method such as length preservation and generalization are actually due to manually encoded constraints. The method itself only learns basic inertial dynamics which are relatively straightforward to infer from geometry. The main contribution of the paper is demonstrating the applicability of GNNs to chainlike systems. However it is important to note that the method is limited in its ability to handle the complexities of realworld rigid body simulation. Questions: Definitions for terms such as gamma and a(q) should be provided. A single round of neighborhood information collection may be sufficient for this method as kinetic and potential energy are local. The paper distinguishes between bars chains and ropes but in the context of rigid body approximation these are essentially the same (except for segment length differences). The reason for the poor performance of LGN (Hamiltonian GNN) in the appendix is not clear given its similar core method and expected energy preservation capabilities.", "Paraphrased Statement: Summary: In this study the authors introduce a Lagrangian Graph Neural Network (LGNN) for modeling rigid body dynamics represented as graphs. The LGNN uses two GNNs to estimate the potential and kinetic energies in the Lagrangian formulation. This approach is validated through 2D simulations of articulated bodies using inextensible chains and rods. Results demonstrate that the LGNN surpasses the baseline Graph Neural Simulator (GNS). The models adaptability is assessed on larger systems with novel topologies. Strengths: The application of GNNs to learn Lagrangian mechanics is novel presenting intriguing results as evidenced by the videos in the supplementary materials. The paper is wellstructured and clearly written. The background information provided facilitates understanding of the proposed concept. Weaknesses: The method is evaluated on simplified rigid body simulations (less than 20 links) that can be accurately solved using traditional numerical methods. The advantage of the proposed learningbased approach over traditional simulations remains unclear. The term \"rigid body dynamics\" is broad the simulations focus on articulated body dynamics representable as graphs. \"Zeroshot generalizability\" is mentioned without explanation or references. Figures 5 and 6 display error values for limited time intervals (0.1s and 0.3s) compared to the 1s simulations shown in the videos. Visual representations of the simulations would enhance comprehension. The appendix references a \"Lagrangian Graph Network (LGN)\" without clarifying its distinction from the proposed LGNN. Minors: Inconsistent use of punctuation marks for paragraph titles (e.g. \"loss function\" vs. \"5.1\").", "Paraphrase: Summary 1. A novel framework for Lagrangian Neural Networks (LNNs) using graph networks is proposed for modeling rigid body dynamics. 2. Comparisons with classical Lagrangian and Hamiltonian Neural Networks (HNNs) demonstrate the adaptability to various topologies and system sizes. Strengths and Weaknesses 1. Strengths: Clear contributions and framework explanation for rigid body modeling. Demonstrated capabilities through benchmarks and nondiagonal mass matrices. 2. Weaknesses: Lack of further explanations for unfamiliar concepts (e.g. line 100 operator). Missing derivation of lambda solution substitution in line 121. Overlapping information in Figures 2 and 3 could be streamlined. Text references in Figure 4 (\"ABC\") need clarification. Purple lines in Figure 4 should be explained in the caption. Hamiltonian extension appears oversimplified requiring further investigation. Lack of experiments in Section 4.2. Referenced appendix is not included in the manuscript. Questions 1. Line 100: Operator \\nabla\\dotqi \\dotqi is a secondorder derivative 2. Justification for choosing edges to represent rigid bodies and nodes to represent connections in Section 4.1. 3. Experiments conducted on Hamiltonian extensions 4. Reason for quasiconstant error in Figure 5 and if it represents initial error propagation. 5. Reason for smooth GNS results and chaotic LGNN results in Figure 6.", "Paraphrased Statement: A Lagrangian graph neural network is proposed to forecast the path of a rigid body represented as a graph with edges linking rigid parts (e.g. chain links) and nodes representing joints. Given the initial position and velocity of each joint the network consists of two simultaneously trained components. One component predicts kinetic energy and the other predicts potential energy. The Lagrangian is calculated by subtracting these values. The network is optimized by minimizing the disparity between predicted acceleration (derived from EulerLagrange equations) and the actual acceleration of the system. Strengths and Weaknesses: Originality: The approach appears to be novel. Quality: Technically sound with wellsupported claims. Clarity: Wellwritten and organized. Significance: Addresses the challenging problem of predicting plausible rigid body trajectories. Limitations: Assumes uniform mass and moment of inertia for all graph elements. Requires known system constraints. Questions: [55] Clarification on the notation in Equation 5 (kinetic energy formula). [208] Meaning of the symbol \u03c8 in Equation 9. [309] Comparison between learned and ground truth mass matrices. Comments: Relevant citations should be included. Improve the notation in Equation 5. Use a more appropriate loss notation. Fix typos and inconsistencies."], "tQRoZ9nRgM": ["Paraphrase: Summary This study examines a sequential jobworker matching problem where there is a limitless supply of workers with attributes drawn from a specific distribution. Unlike previous matching algorithms that considered limited markets and focused on competition and congestion this paper investigates the challenge of choice overload in large markets. The paper presents lower bounds on the regret of any algorithm for this problem and proposes a rateoptimal learning algorithm. Strengths 1. Dynamic matching with uncertainty is a novel research area. 2. The lower bound on regret aids in understanding the problems limitations. Weaknesses 1. Justification for the problem scenario is insufficient. While the paper acknowledges the difference between infinite and finite markets it does not provide a compelling reallife example to explain the relevance of the considered scenario. 2. Readability: The theoretical analysis and algorithms can be difficult to follow. 3. Conflicting Lower Bounds: Theorems 1 and 2 present contradictory lower bounds on regret which should be addressed to support the proposed algorithms optimality. 4. Algorithms and Results: The paper introduces Algorithm 2 (CABK) with a log(n) regret upper bound but fails to demonstrate its superiority in experiments. CABK (UCB) introduced in Algorithm 4 performs well in practice but lacks a strong theoretical guarantee (o(n)). 5. Positioning: The assumption of unlimited workers aligns the problem with countably manyarmed bandit problems suggesting that the paper would be more suitable for that literature. 6. Submission Length: Given the 9page Neurips limit a journal publication may be more appropriate for the papers depth. Questions See the \"Weaknesses\" section for further clarification.", "Paraphrased Statement This research explores a dynamic matching scenario involving a stream of jobs with varying types and a distribution of workers. At each moment a number of jobs emerge each with a specific type. The workers are randomly generated according to a specific distribution. The study introduces an algorithm that operates in this environment with a regret bounded by O(log(n)) where n denotes the number of rounds. Strengths and Weaknesses Originality: The research introduces a novel problem setting with potential realworld applications. Quality: The mathematical foundations of the research appear sound. Clarity: While the model is intricate further effort is needed to improve comprehension. The setting is complex and key details are scattered throughout the document. Significance: The research has some significance but more technical comparisons with related studies would enhance its impact. Questions 1. Clearer Problem Formulation: Specify whether the worker distribution is known or unknown and explicitly state the assumption about known Kj in Section 2. 2. Emphasis on Optimization Objective: Highlight the primary difference in objective between previous dynamic matching studies and this research: the shift from stabilityequilibrium to pure optimization. Additionally discuss the implications of linear function approximation in [1] and compare with the authors framework. 3. Comparison with Combinatorial Bandit Research: Expand the discussion to include comparisons with the extensive literature on matchingcombinatorial bandits citing relevant studies such as [36].", "Paraphrased Summary This research investigates learning in a large online marketplace where a platform matches jobs to workers whose abilities are initially unknown. The authors address the challenge of \"choice overload\" where the number of potential workers far exceeds the available jobs. To model this they formulate the problem as an infinite multiarmed bandit (MAB) problem treating jobs as arm pulls and workers as an infinite pool of arms. Their proposed algorithm optimizes learning in this environment achieving a logarithmic regret over time. Paraphrased Strengths The study examines a unique learning challenge in imbalanced markets. The authors derive a valuable technical result for infinite MABs minimizing the influence of the arms distribution. The results are presented clearly and coherently. Paraphrased Weaknesses The economic implications of the technical findings are not discussed indepth. The discrete cluster assumption appears crucial to the algorithms functionality. It is unclear if comparable results can be obtained without this assumption. Paraphrased Questions Can the results be extended to the situation where workers abilities are drawn from a continuous distribution How many reinitializations are required on average for the CABK algorithm", "Paraphrased Statement: This research explores a matching problem on a twosided platform involving \"jobs\" and \"workers.\" Workers skills are uncertain and the worker supply is unlimited representing the \"choice overload\" phenomenon. The platform does not know the distribution of worker skills or the distribution and average of match payoffs. The platform faces a sequential decisionmaking process: Jobs arrive over time. The platform decides whether to match a job to a worker (either an existing unmatched worker or a new worker). After the match a random payoff is received (unknown to the platform). The platform aims to design a policy that maximizes its expected total payoff. This is equivalent to minimizing the expected cumulative regret compared to the clairvoyant policy. Results: Theorem 1 provides lower bounds for the specific case of one job type and two skill levels considering \"admissible policies.\" Theorem 2 presents a lower bound for the same case but considering \"memoryless policies.\" Theorem 3 demonstrates an upper bound for the regret of the \"CABK\" algorithm for one job type. Theorem 4 combines CABK with another algorithm (\"MATCH\") to obtain a general upper bound. The Appendix includes additional theoretical results and a computational study. Strengths and Weaknesses: The paper is wellwritten and accessible with helpful discussions throughout. It considers a more general version of the CAB problem than previous research. It leaves open questions for future work particularly in methodology and modeling. Questions: How does the lower bound in Theorem 1 relate to existing lower bounds Is there a connection between Theorems 1 and 2 and how do they compare Could regret guarantees improve if the platform knows rewards How does the choice overload assumption affect the results Is the clairvoyant benchmark assumption realistic and how would regret change assuming limited access to optimal workers Minor Comments: Typographical errors in the Appendix need correction. The parameter K is assumed to be constant or much smaller than the horizon n."], "yam42JWePu": ["Paraphrased Summary: This paper focuses on improving visionlanguage alignment in AI models using a contrastive pretraining approach. Previous models like CLIP only aligned images and text globally neglecting specific features. While crossattention mechanisms offer partial alignment they lack explicit supervision for matching image patches to text phrases. To address this the paper introduces a new supervision mechanism based on Shapley values from game theory. It explicitly matches text phrases to image patches unlike previous approaches. To improve efficiency a separate estimator predicts Shapley values instead of directly calculating them. Experiments show that the proposed model effectively aligns text phrases to image regions without relying on pretrained detection models. It outperforms other models in zeroshot transfer tasks for retrieval detection and visual grounding. Ablation studies validate the contributions of the new losses and Shapley value estimation module. Strengths and Weaknesses: Strengths: Clear presentation of the approach and its theoretical basis Novel use of game theory to enhance visionlanguage alignment Significant improvement in zeroshot transfer tasks Weaknesses: Missing comparison to related works like RegionCLIP and XVLM Limited discussion of training efficiency and model limitations Ambiguity in defining the score function used in the game theory framework", "Paraphrased Summary: This study introduces LOUPE a framework for aligning vision and language through finegrained semantic interactions. Its effectiveness is demonstrated in tasks like imagetext retrieval object detection and visual grounding. Strengths and Weaknesses: Strengths: Clear motivation and novel finegrained semantic alignment approach (PhraseRegion Semantic Alignment). Intriguing hybrid Shapley interaction learning strategy. Weaknesses: Insufficient experiments inconsistent model structures for different methods in comparison tables. Unclear if vanilla DualEncoder can handle zeroshot classification and results on ImageNet. Lack of explanation for setting loss weights to 1:1:1 for LCMC LTSA and LFSA. Questions: Can more extensive experiments be conducted Can you provide more information on the 240M dataset including its composition and any use of public datasets like CC12M", "Paraphrased Summary: This study developed an advanced framework for aligning vision and language in pretraining using finegrained semantics. It emphasizes both visual tokenlevel and phraseregion level alignment. The supervision signal for region generation and visual enhancement is derived from assessing the Shapley interaction between visual regions and text. Furthermore regionphrase alignment is supervised by Shapley interaction as well. To optimize efficiency an uncertaintyaware learning module predicts Shapley interaction circumventing timeconsuming samplingbased estimation. Strengths: Innovative approach to exploiting local correlations between images and text in contrastive pretraining. Shapley interaction as supervision for region generation and visual backbone. LOUPE outperforms similar datascale methods in retrieval tasks with comprehensive ablation studies. Region generation model can serve as a zeroshot object detector. Weaknesses: Lack of evaluations on zeroshot image classification and linear probing to demonstrate the strength and robustness of the visual backbone. Increased training time (65). Private dataset usage raises concerns about fairness and reproducibility. PostRebuttal Updates: Weaknesses 1 and 2 have been addressed warranting a score adjustment.", "Paraphrased Statement: This research focuses on developing a method for training visionlanguage models to create a versatile backbone network for tasks like object detection and visual grounding. The proposed framework uses Shapley values a concept from game theory to align visual and textual tokens during training. Specifically the framework calculates Shapley values to identify interactions between image patches forming objects or regions and corresponding noun phrases representing object types in images. These values are used as pseudolabels for guiding the alignment between textual and visual representations. To reduce computational complexity the framework introduces an approximation technique for calculating Shapley values through sampling. The effectiveness of the method is demonstrated through evaluations on image retrieval object detection and visual grounding tasks using the MSCOCO and Flickr30k datasets. After Rebuttal: The authors response adequately addresses the concerns raised. The reviewer maintains their rating and recommends the paper for acceptance. Strengths: The concept of using Shapley interaction for semantic alignment and generation is innovative and novel. Game theory provides a promising framework for weakly supervised visualtextual alignment. The approximation mechanism effectively reduces training complexity. Comparisons with appropriate baselines demonstrate the methods effectiveness. Weaknesses: The technical explanation could be challenging for nonspecialized readers. A table summarizing key differences from other visionlanguage pretraining methods would be beneficial. The use of Shapley values as pseudolabels requires further clarification (e.g. how imagetext similarity is obtained). The benefits of using Shapley values over tokenwise alignment (e.g. FILIP) are not fully explained. The complexity of calculating Shapley interactions and estimating uncertainty is unclear. Questions: How does the model learn to approximate Shapley interactions and estimate uncertainty What is the input to the Shapley value approximation module How reliable is the uncertainty estimation"], "sipwrPCrIS": ["Paraphrase: Summary: This paper advances the use of dynamical mean field theory (DMFT) to study deep neural networks. The authors derive equations that govern the evolution of network features and gradients in the infinitewidth feature learning regime. While these equations are computationally expensive to solve the authors propose a samplingbased approach for numerical solutions. The theory predictions align well with simulations. The framework also encompasses various perturbationbased approaches to understanding neural networks. Overall the paper makes significant contributions to the NeurIPS community. Strengths: Extends previous DMFT work to multilayer networks. Provides selfconsistent equations. Captures existing approaches like finitesize corrections. Theory matches simulations well. Weaknesses: Rigor of assumptions is unclear. Presentation lacks accessibility for nonDMFT readers. Interpretability of the equations is limited. Computational complexity is high (cubic dependence on steps and samples). Questions: Can assumptions be formulated for theoretical soundness How do YangHu and EFFICIENT COMPUTATION OF DEEP NONLINEAR \u221e WIDTH NEURAL NETWORKS... compare in terms of equations efficiency and advantagesdisadvantages Can the proposed algorithm be scaled up to handle large datasets like CIFAR10", "The research introduces a novel method to simulate the behavior of extremely wide neural networks during gradient descent using a selfconsistent dynamic field theory. This method leverages techniques from dynamic kernels to describe network evolution. Key Findings: Analytical Solution: An analytical result is derived for linear networks. Approximate Sampling: A sampling method is proposed to approximate the dynamics for wider networks. CrossRegime Consistency: The proposed method provides consistent solutions across different network regimes compared to other approximations. Realistic Experimentation: The method remains valid even in realistic settings. Strengths: Clear Presentation: Wellstructured and easytounderstand writing. Novel Approach: Use of dynamic field theory to simulate infinitewidth networks. Effective Modeling: Captures the dynamics of neural networks compared to other methods. Future Optimization Potential: Acknowledged limitation in computational efficiency which can be addressed later. Questions for Future Research: Comparison with FiniteWidth Networks: How does the proposed method differ from NTK methods in practical performance particularly in feature learning", "Paraphrased Statement: This study investigates feature extraction in extremely wide neural networks trained using gradient descent. In this extreme case the distribution of activations and gradients in hidden units approaches independence and can be represented by order parameters using selfconsistent field theory. These order parameters comprise inner product kernels selfconsistently determined from these distributions. While these selfconsistent equations can be solved analytically for linear networks numerical methods are required for nonlinear networks. The theory closely aligns with simulations in networks of finite width. The limitations of the theory are explored indicating where it falters and comparing it to other analyses. Strengths: Contributes to the fundamental understanding of neural network feature extraction which remains a critical open question. Presents original contributions and compelling results while building upon existing work. Provides insights into the theory by discussing connections to previous research. Weaknesses: Relegates most calculations to supplementary material due to complexity. Does not clearly distinguish between proven results and formal calculations. Experiments are conducted on small datasets due to computational constraints. Limited interpretation of features learned in this extreme case. Minor Typos: Citation error on line 198. \"numerically method\" should be \"numerical method\" on line 249. Inconsistent usage of \"which\" and \"that\" throughout the text. Missing hyphens in compound adjectives (e.g. \"infinite width neural networks\" should be \"infinitewidth neural networks\"). \"Figures 5 and Figure 6\" should be \"Figures 5 and 6\" on line 165. Questions: Can the authors approximate the performance improvement gained by feature extraction in finitewidth networks compared to infinitewidth networks For linear networks is the primary barrier to scaling the experiments to larger datasets the resolution of the linear system in Equation 12 or other factors"], "ocg4JWjYZ96": ["Paraphrase: Summary: The authors introduce EIF an improved version of IF for DML. EIF differs from IF in two key ways: No Hessian Calculation: EIF does not require the timeconsuming computation of the Hessian matrix resulting in faster calculations. Tolerates Wide Parameter Range: EIF allows the possibility that the new parameters may substantially deviate from the original ones which is beneficial when a large portion of the dataset is reweighted. The authors demonstrate EIFs effectiveness through experiments on datasets with deliberately mislabeled points: Reduced Confusion: EIF significantly decreases the confusion of the most confused pairs compared to IF (Table 1). Point Identification: EIF effectively identifies mislabeled points (Figure 2). Relabeling Accuracy: EIFKNN can accurately relabel mislabeled points (Table 2). Computational Advantages: EIF offers computational benefits over IF (Table 3). Moreover EIF proves useful in benchmark tasks: Labeling Validation: People often agree with the models mistakes suggesting errors in data labeling (Table 4). Error Analysis: EIF analyzes various types of data labeling errors. Strengths and Weaknesses: Originality: IF has not been previously applied to DML and EIF is its novel extension. Quality: The methodology appears sound although the use of student volunteers for Table 4 is unusual. Clarity Issues: Clarify what \"L\" represents in Line 7071. Parse Equations (1) and (2) more clearly. The objective is to solve the second line constrained by the first. Correct \"lXY\" to \"RXY\" in Equation (2). Define \"delta d\" and \"delta L\" as the differences in metrics between new and original parameters. Explain how distance normalization is performed in Line 156. Describe how EIFKNN \"recommends a new label\" (possibly: EIF identifies a point as harmful and KNN assigns a different label). Significance: EIF addresses a crucial problem in DML. While computational advantages are evident EIFs superiority to IF in effectiveness is not fully clear (see Questions). The authors claims about DML dataset challenges lack novelty. Questions: Interpret the discrepancy between Table 1 (improved confusion reduction) and Figure 2 (EIFs advantage only evident in CUB200). Are influence scores sorted by absolute value Does merging similar classes simplify the problem rather than representing a dataset issue", "Paraphrase: Summary: The authors introduce a new method for measuring the impact of training samples on error rates in deep metric learning (DML). They also propose a framework for relabeling data based on this method. Additionally they present a faster version of the traditional approach to calculating influence functions which are typically slow to compute. Strengths: The work is novel and significant in the field of DML. It is one of the few studies to investigate model interpretability in DML through the analysis of training data. The authors address the computational expense of influence functions by developing a faster version using a simplified approximation. The paper is wellorganized and clearly written. The authors have expanded the references to address a previous weakness. The experiments cover a range of commonly used DML datasets providing empirical support for the results. Weaknesses: While the use of an approximation to speed up influence function calculation is not technically new it has a significant impact by reducing computation time significantly. The authors focus exclusively on DML which is understandable but could be strengthened by exploring the methods applicability to supervised learning scenarios. It is uncertain whether the choice of network architecture affects the quality of the influence values calculated by the new method. Questions: Does the selection of backbone architectures play a role in determining the quality of influence values", "Paraphrased Statement Summary This paper investigates why Deep Metric Learning (DML) models struggle to surpass traditional approaches on benchmark datasets. The authors find that trained models fail to recognize similar samples and make consistent generalization errors. They attribute this problem not to model shortcomings but to the presence of mislabeled data in these datasets. They propose an empirical influence function (EIF) to identify and quantify the contribution of these mislabeled examples to generalization error. They also develop a technique to relabel these problematic examples based on the EIF. Strengths and Weaknesses Strengths: Identifies mislabeled examples in large datasets. Provides a novel empirical influence function that addresses runtime and accuracy issues of existing influence functions. Contributes to the improvement of benchmark task performance. Weaknesses: Lack of clarification on the improvement of EIF over IF as the number of confusing pairs or confusion groups varies. Limited examples to support the effectiveness of retraining models with EIFidentified mislabeled data. Incomplete information on the identification method of potential mislabeled examples in the appendix. Lack of intuition on the accuracy improvement of EIF compared to IF despite the reduced computational cost. Limited field study to only EIF pairs inclusion of IF pairs data in Table 4 is desirable. Questions How does the improvement of EIF over IF change with the number of confusing pairs and confusion groups considered Can the authors provide concrete examples of retraining models with EIFidentified mislabeled data to demonstrate the improvement What is the rationale behind the dependence on the choice of Ntheta for the DML training experiment In Figure 2 why does the performance of EIF and IF saturate for the middle tranche of training examples before increasing rapidly at the end Why was a field study not conducted for IF pairs as well The writing could benefit from a thorough review to correct missing words or variables in sentences."], "tHK5ntjp-5K": ["Summary Paraphrase: Researchers have created a generative model for 3D point clouds that uses a diffusion model on latent space. The model consists of a variational encoder and decoder that translate between point clouds and latent representations. The latent space is divided into a global shape embedding and localized detail embeddings the latter represented as sparse point clouds with feature values. The model is trained initially as a VAE with a Gaussian priorposterior and the diffusion model is then fitted to the resulting posterior. It is evaluated on various tasks using the ShapeNet dataset including unconditional generation sampling variations on a coarse model interpolating shapes and generating meshes through integration with a differentiable Poisson solver. Strengths and Weaknesses Paraphrase: Strengths: The model is innovative and wellreasoned. Quantitative results demonstrate substantial improvements over current point cloud generation techniques. Qualitative results are impressive especially the generated meshes. The paper provides thorough explanations of model components. An ablation study supports design decisions. The writing is clear and accessible. Weaknesses: The technical contribution of the model itself is limited as latentspace diffusion models and point cloud diffusion models already exist. Experiments are primarily conducted on only three ShapeNet classes. The mapping from point clouds to meshes is trained separately which could be improved by joint endtoend training. Questions: What is the computational cost of generating a single chair or training the entire model Why is the paper presented using discretetime diffusion instead of continuoustime diffusion which the model actually uses", "Paraphrased Summary This paper focuses on generating and modifying 3D shapes. It employs a denoising diffusion model on point clouds akin to PVD and DPM. The claimed advancements over these methods include: Utilizing a hierarchical latent space with a global shape and point features. Integrating the network with a shapeaspoint network to generate a surface from the output point cloud. Reportedly the proposed approach outperforms PVD and DPM on metrics related to shape quality and generation guided by interpolation. Strengths and Weaknesses Strengths: Improved stateoftheart shape generation results. Comprehensive evaluation and clear method exposition. Weaknesses: Lack of explicit comparison with PVD and DPM hindering an understanding of novelty and performance differences. Ambiguous ablation studies (Tables 10 and 11) that make it unclear which architectural aspects contribute to performance gains. Minor performance improvement with additional latent point dimensions potentially indicating noise. Limited insight into the hierarchical architectures impact on performance. Questions Provide clearer ablation results to establish the source of performance improvement compared to PVD and DPM. Explain the significance of hierarchical latent space dimensions (Dh1) in enhancing performance. Determine if performance gains are primarily attributed to architectural changes or other factors such as enhanced training time.", "Paraphrase: This study introduces LION a hierarchical Latent Point Diffusion Model for 3D shape generation. Unlike previous approaches that perform denoising diffusion modeling (DDM) directly on point clouds LION employs a VAElike structure with a hierarchical latent space comprising a global shape latent space and a local pointstructured latent space. Additionally two hierarchical DDMs operate on these separate latent spaces. Comprehensive experiments show that LION surpasses existing models on various ShapeNet benchmarks. Furthermore the VAE structure allows for easy adaptation of LION to other relevant tasks by finetuning the encoder. When combined with a surface reconstruction model such as SAP LION can generate smooth 3D meshes. Strengths: LION employs DDMs on a hierarchical latent space rather than raw point clouds demonstrating effectiveness in shape generation and ablation studies. The VAE structure enables flexible transfer to other relevant tasks without retraining DDMs as evidenced by experiments on shape denoising and voxelguided synthesis. LION achieves stateoftheart results in both singleclass and manyclass 3D generation on ShapeNet benchmarks. Integration with SAP allows for the generation of smooth 3D meshes. Weaknesses: The idea of using DDMs on latent spaces with VAE integration is not entirely novel with similar approaches used in other domains but not cited in this manuscript. The manuscript presents selected results focusing on the metric 1NNA. Evaluation using other metrics (MMD and COV) reveals that LION may not always outperform baselines as indicated in supplementary materials. Question 1: For shape interpolation does LION interpolate in both latent spaces or just one Answer: This question cannot be answered from the provided text.", "Paraphrased Statement Summary This paper introduces a hierarchical architecture for generating point clouds. It comprises two diffusion models for shape latents and latent points. Unlike PVD the proposed architecture is hierarchical and enables the reconstruction of corresponding meshes using an existing method called SAP. The paper provides extensive qualitative and quantitative evaluations to demonstrate its effectiveness. Strengths Comprehensive evaluations show the effectiveness of the proposed method. The paper is wellwritten and accessible. It thoroughly discusses related research. It outperforms other stateoftheart methods in terms of generation quality. Weaknesses The proposed pipeline is not particularly innovative it combines PVCNN and a diffusion model. Mesh reconstruction is also achieved using an offtheshelf method. The primary contribution may be the hierarchical generation architecture. It lacks insights on the choice of backbone for feature extraction. While the paper highlights singleview reconstruction as a potential application it does not present any results for this task. The method can reconstruct surfaces so comparisons with stateoftheart implicit surfacebased methods (e.g. AutoSDF Deep marching tetrahedra) could be valuable. Quantitative results for unconditional generation in the singleclass setting show limited improvement. The reason for this is unclear. Questions Why was PVCNN chosen as the feature extraction module The paper should provide experimental results to justify the choice of backbone for feature extraction. The paper should include interpolation results from other baseline methods. The paper should compare the methods singleview reconstruction capabilities to existing RGB or RGBDbased methods. The paper should explore comparisons with implicit surfacebased methods."], "xs9Sia9J_O": ["Paraphrase: Summary: The study reexamines the principle of Individual Global Max (IGM) decomposition in cooperative multiagent reinforcement learning (MARL). It identifies a limitation of IGM and introduces an imitation learning strategy called IGM Decomposition with Imitation Learning (IGMDA) to overcome it. IGMDA addresses the issue of partial observation where local information may be insufficient for optimal decisionmaking. The approach involves training a centralized agent with global state knowledge and then using imitation learning to extract decentralized policies for agents with limited observations. Empirical results demonstrate that IGMDA surpasses other IGMbased algorithms in cases with zero sight view. Strengths: Challenges the IGM principle bringing novelty to the field. Recognizes the challenges of IGM decomposition due to partial observation. Proposes a teacherstudent model to leverage global information and address this issue. Weaknesses: 1. It does not consider the possibility of enriching agent observations through interagent communication which could complement the approach. 2. Proposition 5 suggests averaging policies for samples with the same local observation. However this may result in poor policies when the policies are significantly different potentially compromising the effectiveness of imitation learning. The authors should address this potential issue. Questions: In a scenario where agents need to identify a winning landmark among two options with equal probability and limited local observations IGM decomposition may not provide sufficient information for optimal decisionmaking. Can IGMDA effectively address this limitation", "Paraphrased Statement: Summary: This research examines errors in multiagent Qlearning particularly those caused by limited visibility. To address this issue it combines a DAgger module to derive a fullvisibility policy from a limitedvisibility policy. Strengths and Weaknesses: Strengths: Explores a relevant problem with an innovative solution. Weaknesses: Ambiguous notation and definitions for key concepts (such as error terms and Error(Q)). Poor writing quality inconsistent with a conference publication standard. Questions: 1. Major Concerns: Formal definition of error terms in Proposition 2. Absence of experimental evidence supporting error reduction (Eq. 7 to Eq. 9). Missing loss function for supervised imitation learning in the main text. Ambiguous wording in Proposition 5 (e.g. \"satisfy local observation\" \"optimal actionvalue after imitation learning\"). 2. Minor Concerns: Discount factor missing in Eq. 7. Incorrect use of onehot probability representation in Eq. 10.", "Summary This research examines the IGM decomposition framework used in decentralized partially observable Markov decision processes (CTDE). It identifies the problem of information loss during value decomposition and its accumulation during training. To address this issue the paper proposes a method that incorporates imitation learning to mitigate the discrepancies between global and local observations in the value decomposition process. The method utilizes an expert agent trained with global observations using offpolicy algorithms and a learner agent trained with supervised learning using local observations. By alternatingly training these agents the value decomposition loss of the learner agent is reduced. Strengths Clear organization and wellwritten presentation Detailed analysis and formal definitions Novel approach of integrating imitation learning into CTDE to reduce value decomposition errors Weaknesses Extensive analysis without substantial depth or innovation Insufficient experimental results particularly focusing on zerosight SMAC settings Questions about the suitability of imitating global state experts when learner agents need to act independently Comparison with existing works using CTCE teachers and CTDE students is missing Questions Can imitation learning effectively address the challenge of information loss in CTDE given that the differences between global and local observations are inherent to the partially observable environment How does this method compare to previous approaches that utilize teacherstudent architectures with CTCE and CTDE agents", "Summary This study examines Qvalue decomposition methods in multiagent reinforcement learning (MARL) from a novel perspective. Current research focuses on centralized training with decentralized execution (CTDE) to achieve decentralized decisionmaking while preventing nonstationary learning during training. However the authors identify a discrepancy between local observations and global information which leads to inaccurate Qvalue representations. This error can accumulate over time. To address this issue the authors introduce a supervised learning approach to derive correct Q values from global observations. This method has been shown to enhance the performance of various MARL algorithms. Strengths and Weaknesses Significance The paper highlights an important problem in CTDE MARL methods that has been overlooked in previous research. It provides valuable insights that can inspire future MARL research. Quality Concerns: 1. The experimental results (Figure 3) do not fully support the analysis in the paper. A smaller sight range may not necessarily hurt performance due to limited information about agents and enemies. The observed performance gap cannot be conclusively attributed to the issue discussed in the paper. 2. Some findings such as the reduced accuracy of local Q functions vs. global Q functions are not surprising and could have been anticipated. A more compelling contribution would be to bound the error resulting from this discrepancy. Questions: 1. Can the authors provide a bound for the discrepancy between local and global Q functions by quantifying the information loss in the local observations 2. Figure 3 needs to be replaced with a more informative plot that demonstrates the gap between Q functions derived from local and global information. 3. The paper should include results that show the impact of varying the sight range from 0 to 5 (or other values). A zero sight range may be too extreme to provide meaningful insights."], "lSfrwyww-FR": ["Summary Paraphrase: This study presents a novel approach to efficiently generate adversarial examples using a group of surrogate models. By utilizing these surrogates the method applies Projected Gradient Descent (PGD) based on their losses and adjusts their impact using weights. Through experiments it demonstrates the ability to swiftly find adversarial examples for various inputs significantly outperforming existing methods in terms of speed. Strengths and Weaknesses Paraphrase: The paper offers clear and logical organization. The concept is intuitive and feasible. However the novelty and depth of analysis appear somewhat limited which may restrict its potential for future research. To address this the study might benefit from more indepth theoretical or experimental analysis. Questions Paraphrase: 1. The authors should clarify the definition of the target label \"y\" in equation (6) to align with its usage in maximizing the loss. Similar clarification is needed for the PGD optimization expression. 2. The methods key advantage is leveraging multiple existing models to generate adversarial examples effective against all surrogates and likely against the targeted victim models. It achieves this by converting queries used in blackbox attacks into whitebox attacks via the surrogate models. Therefore the number of queries consumed by the surrogates should be thoroughly investigated and reported. 3. The authors employ coordinate gradient descent to update the weights \"w\" instead of using Stochastic Gradient Descent (SGD) over all elements of \"w.\" An explanation for this choice would be beneficial. 4. While the paper demonstrates the transferability of generated adversarial examples to object detection tasks it remains unclear whether the training data for the surrogates is the same as the data used in the attack. Clarification is needed on whether the attacked data is assumed to be unknown and distinct from the surrogate training data.", "Paraphrased Statement: Summary: This research presents a hybrid attack on deep learning models that combines transferbased and querybased methods. It reduces the number of queries needed while maintaining a high rate of successful targeted attacks. Key Features: Creates an ensemble of surrogate models weighted based on their ability to generate adversarial examples that transfer to the target model. Uses these adversarial examples to minimize the number of queries required for blackbox attacks. Demonstrates effectiveness on Imagenet models with varying architectures both targeted and untargeted attacks. Strengths: Optimizing over weights in the ensemble ensures limited queries by utilizing prior information. Empirical results show a low number of queries for highdimensional data. Clear and easy implementation with pretrained models and adversarial examples available. Weaknesses: Limited evaluation on a single dataset and nonrobust models. Only image classification and Google Cloud Vision API considered. Lack of comparison with similar hardlabel attacks like HopSkipJump. Questions: Does the whitebox version use gradients with respect to weight vector w and how are these gradients computed For hardlabel attacks is scorebased access to the surrogate model assumed What is the relationship between surrogate and victim model architectures", "Weaknesses 1. Limited Ensemble Weight Updates The author expresses concern that the average query times of the proposed method are around 12 indicating that the ensemble weights are updated infrequently. They question whether the effectiveness stems from the ensemble itself rather than the weight searching strategy. Response: The weight searching strategy plays a crucial role in determining the effectiveness of the ensemble. By updating the weights according to the feedback from the target models the ensemble can prioritize members that have higher success rates leading to more efficient and targeted queries. 2. Overfitting to Target Model Architecture The author notes that the architectures of the target models are included in the substitute ensemble. They suggest that this might make the ensemble search strategy more like guessing the architecture of the target model which is not a typical scenario in practice where the substitute ensemble may not include the target models architecture. Response: While the inclusion of target model architectures in the substitute ensemble could potentially limit its generalizability it is important to consider that the proposed method is designed for the specific scenario of blackbox adversarial attacks where the attacker has no knowledge of the target models internals. In such cases leveraging a substitute ensemble that includes various model architectures can provide a more comprehensive search space and improve attack performance.", "Summary This research investigates efficient blackbox attacks that aim to create adversarial examples with minimal queries to an unknown victim model. The authors propose using an ensemble of surrogate models to generate candidate perturbations which are then tested on the victim model. The feedback from the queries is used to adjust the weights of the surrogate models leading to better candidate perturbations for subsequent queries. Empirical results show the approachs effectiveness in producing successful attacks with limited queries. Strengths and Weaknesses Strengths: Clear and straightforward presentation Weaknesses: Limited Related Works Review: The paper lacks an extensive review of related research particularly those combining transfer and querybased attacks. Insufficient Baselines: The adopted baselines are unconvincing and more recent and stronger baselines should be included. Incomplete Evaluation: Experiments should assess the impact of each attack process component to demonstrate their importance. Targeted Attack Difficulty: The selected target classes may be too easy warranting a more comprehensive evaluation considering different target difficulties. Ensemble Diversity Assumption: The attack setup assumes diverse ensemble models which may not always be available in practice. Missing Direct Transfer Rate: In the attack results direct transfer rates should be reported to gauge the improvement brought by updating surrogate model weights. Questions The weaknesses section outlines suggestions for improving the paper including expanding the related works review incorporating stronger baselines and addressing the ensemble diversity assumption."], "mwIPkVDeFg": ["Summary This paper considers distributed (both centralized and decentralized) problems of minimizing smooth nonconvex functions (PL) in general and in the special case of overparameterized quadratic functions. The authors derive lower bounds on the amount of information transferred in terms of the problems dimension the number of agents the amount of data and the accuracy of the solution. They then propose an algorithm that achieves these lower bounds. The algorithm utilizes a specific quantization technique. In the general case (Section 4.1) the algorithm is similar to existing methods. In the overparameterized quadratic case it employs an additional transformation F and an inverse transform \\tilde F. The authors also present a decentralized version of the algorithm using a gossip protocol with compression. Strengths and Weaknesses Strengths: The paper is generally clear and easy to read. Weaknesses: The literature review could be more comprehensive particularly regarding the work of P. Richtarik in compressed optimization. The lower bounds do not explicitly consider the number of bits (B) used in the data representation although they are mentioned in the paper. The compression method from Definition 4 is not new. The upper bounds in the overparameterized case are not competitive compared to existing results. The overparameterization assumption (Definition 2) is specific and has not been widely studied in the literature. The decentralized results are not particularly novel or exciting. Questions: How important is the consideration of B in the lower bounds Can the authors provide more details on the significance and applicability of the overparameterization assumption What order of gain can be expected for specific problems under the overparameterization assumption How does the proposed decentralized algorithm compare to other gossip protocols with compression", "Paraphrase: Summary: This paper investigates the problem of optimizing an overparameterized model across multiple distributed nodes aiming to determine the minimum number of bits required to achieve zero loss. It provides lower bounds for both piecewise linear (PL) and overparameterized cases. The paper presents algorithms that can attain these lower bounds under certain conditions such as when the model is similar to linear regression and the quantization algorithm is carefully designed. Strengths: The paper addresses a gap in the study of bit complexity in distributed optimization as most previous works focused on communication rounds. The theoretical results establish optimal or nearoptimal rates for two cases and settings. The theoretical findings are robust and novel. Weaknesses: A discussion of Lipschitz constants is missing. The lack of consideration for Lipschitz constants may conceal an extra factor of N in the final bound which needs clarification. The paper primarily focuses on PL and overparameterized cases its applicability to general convex or nonconvex functions is unclear. Although the paper is theorydriven additional experimental results on various applications would enhance its practical significance. Questions: 1. How would the proposed bounds change if Lipschitz constants are included Would they become less precise if the global function represents the average of local functions 2. Can the tightness of the bounds be guaranteed when the proposed algorithms handle general convex or nonconvex functions", "Paraphrased Statement This paper investigates the communication efficiency (data transmitted) for distributed optimization problems where: Objective functions meet the PL condition (Property Lambda) Overparameterized generalized quadratic functions are utilized It establishes lower bounds for these problems and proposes firstorder algorithms employing compression and decompression techniques. Key Findings For PL objectives the proposed algorithm achieves optimal communication efficiency (within logarithmic terms of the problems dimension). For overparameterized quadratic objectives it delivers optimal communication efficiency (with logarithmic terms) with high probability. The algorithm can be decentralized using the CHOCO estimator. The study also examines a simplified feedforward neural network demonstrating its optimal communication efficiency in certain scenarios. Questions and Concerns 1. Is the additional logarithmic factor (\\log(D)) caused by unavoidable initialization issues 2. Can a stronger upper bound be derived for neural networks considering less stylized architectures 3. Can the overparameterized class be expanded to encapsulate overparameterized generalized linear models 4. Why is the second inequality in (2) essential for overparameterized settings 5. How does the quadratic setting differ from the analysis by Korhonen and Alistarh (2021) 6. While the study emphasizes bit communication complexity reducing communication rounds is also crucial in practice. How can the lower bound results be extended to intermittent communication settings 7. Can the proposed quantization schemes be integrated with local update algorithms commonly used for communication round reduction 8. Are there any realworld scenarios where bit complexity is more critical than communication rounds Can these scenarios be captured in experiments"], "monPF76G5Uv": ["Summary Paraphrase: This study presents GNAT (Globally Normalized Autoregressive Transducer) to tackle the label bias issue in realtime speech recognition. Strengths: 1. GNAT reduces the performance gap between realtime and nonrealtime recognition systems by resolving the label bias problem. 2. Unlike previous approaches that primarily adjust network structures GNAT offers a welldefined solution in both theory and implementation. 3. Realworld ASR experiments demonstrate that global normalization improves realtime recognition significantly bringing its WER within 50 of that of a nonrealtime system. 4. GNATs theoretical underpinnings are sound within the WFSA framework. Weaknesses: 1. The approach requires WFSA construction which can be resourceintensive and inflexible without a realtime compilation mechanism.", "Summary: The paper presents: 1. A unified theoretical framework (wFST) for various ASR architectures including RNNT CTC HAT LAS and their variants. This framework extends previous work but provides a more comprehensive treatment. 2. A solution to recover around 50 of the performance degradation that occurs when transitioning from offline models to causal encoders (with globally normalized variants). An algorithm for this implementation is provided in pseudocode in the appendix. The results are significant and applicable to the ASR community. Strengths and Weaknesses: Strengths: Relevance to the ASR community. Generalization of DL ASR models into the wFSAT formalism. Improved streaming models through globally normalized variants. Weaknesses: Limited scope to the ASR community. Lack of extensive experiments and analysis to fully understand the contributions of globally normalized RNNTlike models. Unclear extent of label bias problems in streaming ASR models and their potential solutions. Small vocabulary used (30 characters) raising questions about applicability to larger vocabularies. Questions: To enhance the value of the paper further exploration is suggested in the following areas: Experiments on performance changes with encoder lookahead. Analysis of different denominator sum approximation methods. Experiments and analysis on the conditions and impact of label bias. Evaluation of the approach on larger vocabularies. Comparison to similar work on K2 FSA.", "Summary The paper presents the Globally Normalized Autoregressive Transducer (GNAT) to combat the label bias issue in streaming speech recognition. It employs a novel algorithm based on the Weighted Finite State Automaton (WFSA) framework to precisely compute the denominator for sequencelevel normalization. The modular nature of WFSA enables the model to potentially integrate various neural speech recognition architectures. Experiments conducted on Librispeech demonstrate that GNAT reduces the word error rate (WER) gap between streaming and nonstreaming automatic speech recognition (ASR) by 50. Strengths Clear and wellorganized writing Introduction of a novel GNAT model based on solid WFSA theory Demonstration of the benefits of global normalization in overcoming label bias Experimental validation of improved performance in streaming ASR Weaknesses Feasibility for Large Units: GNAT may be limited to models with a small number of units and it is unclear if it can be applied to large vocabularies typically used in stateoftheart ASR systems. Imprecise Statements about Locally and Globally Normalized Models: The paper makes inaccurate claims about the equal expressiveness of locally and globally normalized models in nonstreaming settings. Insufficient Introduction to Prior Work on Globally Normalized Models: The papers discussion of existing globally normalized models is imprecise mischaracterizing some models and failing to establish clear distinctions. Incomplete Explanations of Label Bias in Streaming and NonStreaming Settings: The paper presents an incomplete understanding of label bias in streaming versus nonstreaming settings. Limited Discussion of Mgram Context Dependency and RNNT Baseline: The details of Mgram context dependency and the RNNT baseline in the experiments are insufficiently explained. Additional Concerns Line 594: The symbol \u03a3 is used without definition. The provided references [ac] are not complete.", "Summary of the Paraphrased Statement: This paper presents a unified model for speech recognition that combines two finitestate acceptors: one for context (similar to a language model) and one for aligning frames to labels. The proposed model addresses the issue of degraded performance in streaming speech recognition due to label bias. By applying global normalization during training the model alleviates this problem. Experimental results demonstrate the superiority of global normalization in streaming settings. The papers strength lies in providing a framework that can encompass various ASR models. It uses clear examples to illustrate the framework. However some sections are notationintensive reflecting the nature of describing finitestate automata (FSAs). A clarification could be added to acknowledge that the described FSAs represent paths of all possible lengths leading to a potentially exponential number of edges in the combined FSA. The paper argues that label bias arises from a mismatch between training and test conditions rather than from global or local normalization. It acknowledges that globally normalized models consider partial paths during training but questions whether locally normalized models trained in streaming mode would not also alleviate the issue by minimizing the negative logconditional likelihood loss. The paper briefly mentions that P(x y) is not defined which could be confusing. It should be noted that any locally normalized model is trivially globally normalized because Z(x) equals 1 in that case. This distinction between Bayesian networks and Markov random fields has been recognized in the community since the advent of discriminative training. The paper suggests that their algorithm cannot be directly implemented with existing toolkits but it acknowledges that this may not be entirely true in principle. Finally it should be clarified that the separation of the weight function from the automaton topology allows for a complex nonlinear weight function to model dependency among alignment states context states and output labels a feature that is not possible with composition cascades. The concept of \"structured composition\" was introduced in a previous work by Tang et al. (2015). The ground truth used in the experiments lacks any processing or tokenization which could be more accurately described as \"unprocessed transcript.\""], "qmm__jMjMlL": ["Paraphrase: Summary: This paper introduces an autoencoder that transforms objects into a form that respects symmetry enabling their efficient representation. The encoder extracts separate features for object appearance and pose while the decoder reconstructs the original image in a manner that preserves these symmetries. The method can learn this symmetryaware representation without supervision outperforming previous techniques. Strengths and Weaknesses: Originality: The paper demonstrates the potential of symmetryaware representations in unsupervised learning showing promising results on both synthetic and realworld datasets. However it lacks a comparison with the Canonical Capsules method which shares similar goals. Quality: The method is validated on multiple datasets and the claims are wellsupported. However results on the benchmark used for unsupervised object discovery are missing. Additionally the paper could include an estimation of object canonicalization to better understand the disentangled representations. Clarity: The paper is generally clear but it could provide more details on the encoders implementation to facilitate reproducibility. Significance: The results highlight the effectiveness of symmetryaware representations in unsupervised settings. The paper demonstrates good performance on multiobject images suggesting the potential for these representations in scene analysis tasks. However more discussion and analysis of multiobject experiments (e.g. detection performance and canonicalization) would be valuable. Questions: The paper could benefit from comparing its performance to Canonical Capsules and including more detailed results and analysis of multiobject scenarios.", "Paraphrased Statement: This study introduces a Translation and Rotation Group Equivariant Variational Autoencoder (TARGETVAE) model that reportedly generates semantic object representations invariant to pose and location. The authors claim this is achieved by decomposing the image generative factors into semantic rotation and translation components. Experiments on MNIST and a protein dataset (EMPIAR10025) demonstrate the models ability to accurately estimate rotation and translation. Strengths: Group equivariant convolution is appropriately utilized. Results indicate superior rotation modeling compared to spatial VAE. Weaknesses: The papers claims appear exaggerated. Experiments are limited to MNIST and EMPIAR10025 with insufficient baselines. The term \"unsupervised object\" or \"semantic\" representation learning in the context of protein data on these basic datasets may be misleading. Figure 1 requires additional context and clarity. The motivation for disentangling rotation and translation for generative models is not adequately discussed. The results do not showcase generated images but only translationrotation estimation (which could also be achieved by a vanilla spatial transformer network).", "Paraphrase: Summary The paper introduces TARGETVAE a method for learning representations that separate the appearance position and rotation of an object in an image. While the model focuses on singleobject images early findings indicate its potential for multiobject images. TARGETVAE is a VAE with encoder and decoder enhancements that set it apart from the baseline SpatialVAE. Notably it incorporates a group convolution network in the encoder. The methods key advantage lies in improved disentanglement of object rotation. Strengths and Weaknesses Strengths: 1. Innovative encoder design for enhanced disentanglement of object rotation. 2. Accurate rotation inference in singleobject images outperforming the baseline. 3. Use of a separate latent for rotation reducing the burden on the appearance latent. 4. Improved clustering performance for MNIST digits based on appearance latent. 5. Potential for extending to other factors of variation through appropriate group convolutions. Weaknesses: 1. Disentanglement driven by explicit rotation transformation in the decoder potentially limiting extension to other factors such as color. 2. Complexity of model design and presentation. 3. Overstatement regarding unsupervised object detection and representation as training images contained a single object. Questions: 1. How is the soft onehot vector from GumbelSoftmax used 2. Further clarification on the prior applied to rotation (\\theta). 3. Is there a typo in L180 where q(t r) should be q(t ry) 4. Correction in L137: \\thetaoffset belongs to the angle set. 5. Why not introduce a \\Delta x for position allowing for correction similar to \\Delta \\theta for rotation"], "rA2tItoRUth": ["Paraphrase: This paper introduces the LanguageGuided Denoising Network (LGDN) for videolanguage modeling which addresses noise in video frames. LGDN dynamically removes frames that do not match the language context selecting only a few key frames for crossmodal alignment. Experiments on various datasets demonstrate that LGDN achieves significant improvements over existing methods. Strengths and Weaknesses: Strengths: Clear and concise structure Practical focus on noise removal in videotext retrieval Simple and effective salient frame selection mechanism Strong experimental results Weaknesses: Insufficient analysis and ablation studies to justify the network design and frame selection mechanism Incomplete comparison of the salient frame proposal (SFP) mechanism with alternative sampling methods Key experimental findings (e.g. SFP effects) are relegated to supplemental materials Questions: 1. Is \\mathcal LMVCL necessary for the LGDN model Could MILNCE be used to compute \\mathcal LMVCL 2. While the authors claim superior results over existing methods Table 8 indicates inferior performance to Clip4CLIP. This discrepancy needs clarification. 3. Given the significant impact of CC12M pretraining it would be valuable to compare LGDN and other methods under the same pretraining conditions.", "Paraphrase: Summary: This paper presents LGDN which uses a selftaught mechanism to identify key frames for training the models fusion layers. The vision and language encoders still use all frames and text for training but their goal is to learn representations and calculate relevance scores for frame selection. This contrastive learning mechanism helps remove distracting information improving performance and reducing computational costs. Strengths and Weaknesses: Pros: LGDN effectively selects frames from cluttered video enhancing both accuracy and efficiency. It addresses the issue of noisy video data potentially opening up new approaches for multimodal model construction. Cons: The empirical results could be more convincing with a comparison of model capacities between LGDN and other methods. While LGDN claims to mitigate noise it still relies heavily on noisy data in the early training stages. The redundancy issue within video data may not be fully resolved and the sampling process may introduce additional concerns. Questions: 1. How does LGDN compare to ClipBERT and other methods in terms of memory efficiency and speed given that LGDN only performs frame sampling before the fusion layers 2. Does the salient frame selection mechanism in LGDN help address frame redundancy or could it potentially result in the selection of highly similar frames 3. To ensure a fair comparison it would be helpful to know the model sizes of the baselines used in the study. Can the performance gain be attributed to the larger size of LGDNs vision and language encoders 4. Can you explain the concept of global and local alignments used during inference Why does global inference yield lower performance 5. Does the salience sampling approach disrupt the temporal dependency of the video data which is crucial for many video tasks Can LGDN be applied to tasks that heavily rely on temporal information", "Paraphrased Summary: This study addresses a crucial issue in videolanguage modeling: previous assumptions that video frames and their text descriptions are always semantically related. However this assumption doesnt always hold true in realworld scenarios for two reasons: 1. Videolevel text descriptions may not cover all video information. 2. Raw videos often contain distracting or irrelevant information not reflected in the text. Previous models that relied on correlation assumptions and selfattention mechanisms failed to address these noise issues leading to misaligned frames and hindering crossmodal alignment. This study proposes LGDN an endtoend languageguided denoising network for videolanguage modeling. LGDNs key features include: A Salient Frame Proposal (SFP) mechanism dynamically removes irrelevant frames and samples salient frames to optimize videolanguage modeling. Crossmodal interactions at three levels: salient frame matching at the tokenlevel framelevel momentum contrastive learning (MFCL) and videolevel momentum contrastive learning (MVCL). Experimental results demonstrate that LGDN outperforms current stateoftheart models on several public textvideo retrieval and VQA datasets. Ablation studies highlight the importance of addressing noise issues. Strengths: Addresses a crucial limitation in videolanguage modeling. Proposes innovative SFP mechanism and multilevel crossmodal interactions. Comprehensive evaluations with strong performance gains and comparisons to existing models. Ablation studies validate contributions of proposed components. Visualization results illustrate the significance of noise removal and SFP mechanism. Detailed experimental results and appendix provide support for reproducibility. Weaknesses: Lack of a \"Limitations\" section discussing potential negative societal impacts. No exploration of alternative experimental setups with increased computation resources."], "vNrSXIFJ9wz": ["Paraphrase: Summary: This paper addresses the issue of selecting clients in Vertical Federated Learning PrivacyPreserving Setting (VFLPS) where each client represents a specific feature subset and has varying importance for the overall model. The approach uses mutual information (MI) to determine client importance. The Fagin algorithm and a batched group testing method are employed to enhance efficiency. The framework guarantees privacy and evaluation results demonstrate its effectiveness with minimal sampling overhead. Strengths: Explores the novel problem of clientfeature subset sampling in VFLPS. Introduces practical techniques (Fagin group testing and batching) to make MIbased sampling feasible. Conducts comprehensive empirical tests across various datasets and models. Prioritizes privacy throughout the process. Weaknesses: Assumes a shared data space across clients which may not always be practical. The effectiveness of the group testing scheme depends on the grouping approach. Random grouping may lead to inefficiencies. Uses informal terminology and symbols (e.g. \"participants\" \"NNk yq\" \"Alltrain\"). Questions: Could the kNNbased MI estimator be biased towards smaller local feature subsets with Euclidean distance Does the framework remain secure if the leader participant attempts to reconstruct nonlocal features How are groups for testing generated and what determines the number of groups (T)", "Paraphrased Statement: The study investigates participant selection in vertical federated learning. It proposes a framework based on mutual information estimation called VFMINE. To enhance VFMINE a group testingbased participant selection method called VFPS is developed. Empirical evaluations demonstrate the effectiveness of VFPS. Strengths: Innovative approach to improving vertical federated learning by selecting participants. Novel optimization technique inspired by Fagins algorithm. Thorough analysis of security and positive experimental results. Weaknesses: Lack of clarity in certain aspects of the algorithms. Questions: The paper presents VFPS as a static method. However it remains uncertain whether the optimal participant selection remains consistent throughout the training process.", "Paraphrased Summary The authors introduce a novel approach to enhance federated learning (FL) efficiency in the vertical setting. They propose a method to select a small group of clients whose data is highly informative. This is achieved using: 1. Mutual information to assess the information content of features. 2. A group testing strategy. Empirical evaluations demonstrate that their method achieves comparable or superior accuracy to traditional FL training with a significant reduction in computation time. Strengths and Weaknesses Strengths: The paper presents the algorithms components with clear reasoning. The innovative use of mutual information in homomorphic encryption is both intriguing and practical. Evaluations on diverse datasets show the methods effectiveness compared to existing approaches. Weaknesses: The group testing method for client selection may be computationally expensive. Additional justification for the impact of test count on topk client selection is requested. The study focuses on scenarios with a limited number of clients (5). It is unclear if the method scales to larger federated networks. The evaluations employ a logistic regression model. The authors are asked to evaluate VFPS on more complex ML tasks with nonconvex loss functions."], "lNokkSaUbfV": ["Paraphrased Summary: This study introduces an adaptation of the Masked Autoencoder (MAE) from computer vision to a decision transformer for demonstration modeling in reinforcement learning (RL). After unsupervised pretraining the model can be applied to various downstream tasks including goalreaching and finetuning. Experiments were conducted using the Mojuco domain in the DeepMind Suite and the authors analyzed the effects of masking ratio and scalability. Strengths: Clear and concise presentation Novel application of MAE to RL demonstration modeling Promising results demonstrating the effectiveness of the approach Weaknesses: No comparison with existing methods like PromptDT Questions: Would be valuable to compare with PromptDT for a more comprehensive evaluation Could the approach be further validated in a robotics domain like Metaworld", "Paraphrased Statement: Summary: This research leverages masked autoencoding to train a selfsupervised transformer model that can encode and decode sequences of intermixed states and actions from sampled trajectories. This model can then be utilized for goalreaching tasks (openloop and closedloop) skill prompting and offline reinforcement learning. Evaluations on the DM control suite show competitive performance compared to imitation learning and autoregressive models. Strengths: The approach is straightforward and the downstream application of the trained model is inventive. Skill prompting addresses a significant inference challenge. The masking strategies are insightful and applicable to this domain. Experiments are welldesigned with online performance evaluation and appropriate baselines that isolate the effects of masking. The paper is wellwritten and clearly presented. Weaknesses: The application may be limited to robotic environments and its extension to pixelbased domains is uncertain. The impact of the datagenerating policy on model performance is not explored. Questions: For goalreaching tasks how is the fixed trajectory length handled in practice Do the models capabilities extend to trajectory lengths it has not encountered during training In the multigoal setting how does the model perform when future goals are not known in advance For offline RL is only the encoder finetuned or both the encoder and decoder", "Paraphrased Statement: This paper presents a novel approach to pretraining RL models enabling them to be directly used for Goal Reaching and Skill Prompting tasks. The model can also be finetuned for Offline RL. Unlike previous methods which pretrain using an autoregressive objective this paper proposes the use of a masked autoencoder (MAE) objective. The bidirectional transformer model is pretraining through the reconstruction of masked state and action tokens. The paper demonstrates how to deploy the model for three types of downstream tasks using specific masking strategies. While the method is an extension of MAE pretraining which has shown promising results in CV and NLP tasks this paper provides extensive comparative evaluations against baselines. Notably offline RL results on the Walker domain are comparable to the stateoftheart ExoRL approach. The paper also presents an ablation study to support design choices like the mixed mask ratio strategy. Strengths: Investigates the use of MAE for unsupervised pretraining in RL. Provides practical implementations and comparisons against autoregressive baselines. Experiments demonstrate the superiority of MaskDP over baselines. Clear and wellwritten paper. Weaknesses: MaskDP still relies on expert data not \"unlabeled\" data as in CV and NLP. Comparison to stateoftheart results on Goal Reaching tasks is lacking. Reasons for only comparing Walker domain to SoTA results are unclear. Questions: Consider using an alternative term for \"zeroshot\" performance as goal reaching tasks are inherent to the pretraining masked autoencoding task."], "kyY4w4IgtM8": ["Paraphrase: Summary: This research addresses heterogeneous metalearning where datasets may have varying numbers and types of features. The approach focuses on using textual feature descriptions that make the model invariant to feature permutations and feature count. This allows the use of the same model across all tasks as new features can be encoded through a sentence encoder. The proposed method outperforms alternatives on tested datasets. Strengths: Solid approach based on leveraging feature and task descriptions Thorough comparison with baselines Consistent gains in both evaluated tasks Weaknesses: Limited technical novelty as the proposed method primarily adds a feature encoder to an existing baseline (\"MDK B\") Lack of exploration with different sentence encoders to evaluate the effectiveness of simple word embedding approaches Absence of a baseline that finetunes the model on downstream tasks Questions: Clarification on the relationship between \"Ours T\" and \"Ours F\" in Table 4 Potential performance improvement by utilizing instance embeddings for prediction using a finetuned function approximator or fewshot finetuning on the support set", "Paraphrase: Summary: This research aims to enhance the generalization capabilities of models in metalearning settings by utilizing BERT embeddings of textbased feature descriptions as input features. The proposed approach outperforms or matches baseline metalearning techniques on two datasets. Strengths and Weaknesses: Strengths: The concept is novel and straightforward. The paper is wellwritten and easy to grasp. Positive empirical results are presented. Weaknesses: Limited empirical evaluation on only two small datasets leaving the practical generalizability of the method uncertain. Would benefit from testing on larger datasets. Questions: For categorical features that lack textual descriptions is there a systematic way to generate them How sensitive is the models performance to the selection of feature descriptions given that they are manually created", "Paraphrase: Summary: This study introduces a novel method for metalearning that develops connections between supervised learning tasks with distinct feature spaces using natural language descriptions of the features. It leverages a pretrained language model (e.g. BERT) which captures various forms of knowledge to enhance generalization performance on unfamiliar tasks with minimal labeled data. The proposed method has demonstrated superior performance over existing metalearning techniques on realworld datasets. Strengths: 1. Presents a unique and challenging problem where the same task has consistent feature spaces while different tasks have varied feature descriptions and spaces. 2. Conducts thorough experiments and ablation studies to assess the methods effectiveness. 3. Offers clear and comprehensive documentation. Weaknesses: 1. Despite a compelling premise certain aspects require further explanation. For instance it remains unclear whether metatraining and metatest datasets share common feature description or type sets. This raises concerns about the models ability to generalize to unseen tasks with substantially different feature descriptions or types. 2. The authors acknowledge limitations in their method including its dependence on the quantity of metatraining datasets. Additionally it remains uncertain if the method extends to datasets with numerical features or discrete labels. Evaluating the methods capabilities on a dataset conforming to these characteristics would be beneficial. 3. Implementing a baseline neural network with untrainable feature description could help determine the significance of metalearning despite the use of feature description. Questions: Do the metatraining and metatest datasets share common feature description or type sets", "Paraphrase: Summary This research presents a metalearning approach that employs natural language descriptions for feature characterization. These natural language feature descriptions can be converted using pretrained models allowing for feature transferability across tasks. The approach was evaluated using statistical datasets containing categorical features and numerical labels which were divided into metatraining metavalidation and metatesting sets. Experiments demonstrated that the method adapts swiftly to new tasks with minimal taskspecific examples. Strengths 1. The concept of using natural language to describe features in metalearning appears innovative and sensible facilitating the sharing of such features. 2. The problem definition and method description are presented clearly. Weaknesses 1. While the idea is generalizable the evaluation setting appears constrained. The datasets employed are not widely used in metalearning research. The author should justify their selection and consider broader testing on other datasets. 2. The tasks within the datasets and their interconnectedness are unclear. It would be beneficial to provide examples to help readers comprehend the complexity of generalizing across these tasks. 3. Results indicate that much of the improvement stems from utilizing pretrained models (NN versus proposed method) raising questions about the contributions of other method components. 4. Technically the author averages feature embeddings (equation 3) which raises concerns about the representativeness of the resulting vector as the number of features grows. While this may not be an issue for datasets with a limited number of features it may impact the methods application in realworld machine learning scenarios with numerous features. Questions Refer to the weaknesses section for specific questions."], "utahaTbcHdP": ["Paraphrased Statement: Neural Collapse Analysis for Imbalanced Data Classification: This paper investigates neural collapse in classification tasks with imbalanced data classes. The authors developed a novel concept called SimplexEncodedLabels Interpolation (SELI) a unique characterization of neural collapse. Strengths: Clear and concise writing style Novel characterization (SELI) that remains consistent regardless of class imbalance levels Extensive analysis of regularizations impact on geometric structure Weaknesses: Lack of clarity on how the geometric structure description contributes to understanding generalization challenges in imbalanced learning Limited insights on mitigating class imbalance issues Additional Question: A comparison between the proposed SELI and the ETF geometry (Figure 1) would be valuable for understanding their respective roles.", "Paraphrase: The research investigates the influence of unbalanced data on the neural collapse (NC) of deep neural networks (DNNs) using the unconstrained features model (UFM). It analyzes the structure of minima in the UFM within a maxmargin optimization framework and links it to minima in the crossentropy (CE) loss with vanishing regularization. The study demonstrates that the findings are complementary to prior observations of \"minority collapse\" (representing extreme cases of regularization levels) and provides empirical evidence supporting the theory in practical DNN NC. The contributions are deemed significant. Strengths: Novel and substantial results. Clear and wellorganized presentation. Weaknesses: The results may be sensitive to different regularization hyperparameters for W and H in the UFM model. The interpretation of the UFM as a twolayer linear network is problematic due to its limitations and inaccurate representation of dimensions. The statement about \"repeated columns\" in H requires clarification and mathematical support. The alignment of W with feature means without global mean reduction should be explained. The omission of relevant related work by Tirer and Bruna (2022) on the extended UFM merits consideration for reference.", "Paraphrase: Study: This research investigates neural collapse in classification tasks with imbalanced training data under the unconstrained feature model (UFM). Using crossentropy loss and vanishing regularization it demonstrates that embeddings and classifiers always interpolate a label matrix encoded by a simplex and their geometries depend on the matrixs singular value decomposition (SVD) factors. This behavior holds regardless of class imbalances. Experiments: Experiments on synthetic and real datasets confirm convergence to the simplexencoded label interpolation (SELI) geometry. The convergence rate is influenced by the imbalance ratio. Strengths: Examines the problem of neural collapse in imbalanced data and UFM. Identifies the SELI geometry as a general phenomenon in such scenarios. Supports findings with experiments. Weaknesses: Presentation and organization may be improved for clarity. Lack of practical guidance for training models with imbalanced data. Omission of relevant recent research on neural collapse. Limited explanation of the benefits of understanding SELI. Questions: Can the authors provide clearer insights into the SELI geometry Is there empirical evidence to support neural collapse with vanishing regularizations in UFMSVM formulations Can the authors further explain the significance of SELI for practical applications", "Paraphrased Summary: Researchers examine the geometric properties of embeddings and weights learned by neural networks on simplified models called unconstrained feature models (UFMs). They focus on imbalanced datasets unlike previous studies that assumed balanced ones and develop a new geometric characterization that categorizes the behavior of embeddings and weights in these imbalanced scenarios. These findings provide insights into how neural networks operate in more realistic settings. Strengths and Weaknesses: The papers novel approach extends existing work on neural collapse to imbalanced datasets addressing a significant gap. It introduces SELI a generalized description of geometric patterns and establishes its connection to the established concept of ETF. The authors provide a detailed analysis of how regularization affects UFM minimizers offering potential insights into neural networks. The classification of UFM minimizers geometries under imbalanced conditions expands on previous minority collapse results. The paper is wellwritten and organized. However the paper would benefit from additional empirical experiments on more diverse neural networks and datasets to enhance the practical relevance of the results. Question for Authors: It would be insightful to hear from the authors about any challenges faced in generalizing the results beyond the specific imbalanced datasets considered in the paper.", "Summary The research examines the \"Neural Collapse\" (NC) phenomenon in data with class imbalances using the Unconstrained Feature Model (UFM) a theoretical model for NC. The paper proposes SimplexEncodedLabels Interpolation (SELI) geometry as a generalization of the equiangular tight frame (ETF) for NC in balanced data. For UFSVM the paper proves that all solutions follow the SELI geometry with major classes assigned larger norms. For UFM with ridgeregularized crossentropy loss the paper shows that no finite regularization leads to SELI geometry. Instead solutions converge towards the SEL matrix as regularization diminishes. The paper also provides experimental evidence supporting SELI as a better characterization than ETF for classimbalanced data. Experiments indicate that convergence worsens with increasing imbalance levels. Strengths and Weaknesses Strengths: Addresses class imbalances extending NC theory. Clear and wellorganized presentation. SELI geometry justified as an optimal solution for UFSVM. Connections drawn to UFM with ridgeregularized CE model. Weaknesses: UFM is an oversimplified model so SELI geometry may not apply to real neural networks. Main technical contribution (Theorem 1) relies on previous work. Lack of empirical results demonstrating SELI geometry in realworld datasets. Questions: How does minority fraction (\\rho) affect SELI geometry or convergence speed Does imbalance level influence the depth required for NC observation in deep networks"], "pBz3h8VibKY": ["Paraphrased Statement: This research presents a refinement of the InteractionGrounded Learning (IGL) framework which enables an agent to learn from a limited context action space and stepbystep feedback that is linked to a hidden reward function. Strengths: Clearly identified a critical flaw in the original IGL formulation which assumed that feedback was conditionally independent of both context and language given hidden rewards. Introduced a modified formulation with less restrictive assumptions conditioning feedback on both hidden rewards and executed actions. Developed a learning algorithm to derive effective policies under the new assumptions. Weaknesses: Evaluation focused on synthetic tasks rather than realworld scenarios such as multimodal interactive feedback or braincomputer interfaces which motivated the research. Specific examples or explanations would enhance the understanding of the revised framework and its potential applications. Questions: The notation L(\\pi\\psi) could be clarified as it appears to indicate a loss but is actually used to maximize the difference in values. Consider using J(\\pi\\psi) instead. The source of augmented data for marginal distributions \\mu(x) and \\mu(y) is unclear especially in the context of online interaction data.", "Paraphrase: This study examines the interactiongrounded learning (IGL) problem where an agent performs action a in context x and receives feedback y and an unobserved reward. IGL aims to learn a policy that optimizes reward without explicit observation by mapping y to reward space and mapping x and a to a value function. A significant challenge arises when y includes information about a causing IGL to fail. This paper introduces a contrastive learning approach that enables IGL to succeed even with information about a in y. Experimental results on RL benchmarks demonstrate improvements over IGL approaching the performance of contextual bandits (which assume full reward access). Strengths: Clear and easy to follow explanation Intuitive informationtheoretic arguments Potential broad impact of main contributions Weaknesses: Assumptions and limitations in realworld applications are not fully discussed Figure 2s format may be confusing Questions about Assumptions: Does the algorithm assume a discrete action space or can it handle continuous action spaces Is y assumed to be deterministic given (x a) Is there an assumption that an optimal mapping from (y a) to reward exists Does the algorithm require executing all actions in all states in the batch data Is the actionexclusive feedback in Table 1 the same as the reward image in Figure 2 Are the images y in 5.1 randomly selected from the dataset", "Paraphrased Statement: Summary: This paper explores learning policies without environmental rewards but with access to feedback. Previous research (Xie et al. 2021) excluded actions from the feedback. This paper eliminates this assumption and introduces a novel contrastive method that can learn policies by incorporating actions into the feedback. Empirical evaluations demonstrate the ability of this approach to learn from feedback containing agent actions. Strengths: Advances the field by relaxing assumptions and making algorithms more applicable to realworld scenarios. Explores learning from nonscalar rewards and feedback. Includes appropriate ablations and model comparisons. Weaknesses: The papers writing could be improved. The introduction lacks a clear motivation for the problem. Section 3 is difficult to understand due to excessive notation and limited intuition. Empirical evaluations are inadequately described requiring reference to another paper. Evaluations focus on simplistic policies and representations which may limit their practical applicability. The feedback signal for MNIST classification is not explained. Evaluations are not directly connected to potential realworld applications. The paper does not address the impact of noisy feedback. It is unclear why a contrastive bandit setting is superior to an MDP formulation for the proposed tasks. Questions: How does this work relate to interaction learning in robotics humanrobot interaction and humanintheloop learning"], "qqIrESv4f_L": ["Paraphrase: Summary This paper presents a new method called INSPNet for processing images directly by using their intensity normalized representations (INRs) without first decoding them. INSPNet accomplishes this by employing highorder differential operators. The researchers demonstrate that any continuous convolution filter can essentially be approximated using a combination of these operators. They validate the effectiveness of their approach through experiments on a variety of image processing and recognition tasks. Strengths The concept of INSPNet is original as it directly operates on INRs without decoding. The paper provides theoretical support for the approximation of convolution filters using highorder differential operators. The methods performance is empirically validated across various vision tasks demonstrating its effectiveness. The paper is wellstructured and straightforward to follow. Weaknesses Despite its strengths INSPNet struggles to surpass the performance of traditional convolution operations in some tasks as evident from experimental results. The paper lacks explanations for using Depthwise CNNs as a baseline for image classification instead of standard CNNs. The paper has some minor typos such as \"sought to to\" in line 77. Questions Why is INSPNets performance on INRs not superior to that of convolution on raw pixels in tasks such as edge detection Why was Depthwise CNN used as the baseline for image classification rather than typical CNN architectures", "Summary This paper introduces a technique for performing operations on continuous representations of data defined on discrete grids called \"implicit neural representations\" (INRs). This allows for operations to be applied directly to INRs without the need for decoding them into discrete form. The authors demonstrate the effectiveness of this approach on image processing tasks such as denoising deblurring and classification. Strengths and Weaknesses Intuitive and demonstrated on a range of image processing tasks. Paper needs clarification and improvement in writing and notation. Quality of solutions using INR operations is not particularly impressive. Questions Equation (1): Did the authors intend to use \"arg min\" instead of \"arg max\" Notation: Clarify the use of \"m\" (dimension of discrete input) and \"M\" (number of discrete measurements of the continuous function). Operators: Explain the distinction between parametric functions learned from data and original definitions of operators \"from scratch\". Results: Provide numerical comparisons over entire datasets not just individual examples. Supervised Learning: Define labels and supervised learning setting. Differential Operators: Explain the claimed wavelet basis properties. Convolutions: Clarify the claim that operators can simulate exact convolutions. Errors: Correct errors in the writing.", "Summary Motivated by the success of implicit neural representations (INRs) in image representation this paper introduces a novel approach for modifying trained INRs \"without decoding.\" The proposed method trains an additional neural network that operates on highorder spatial derivatives of the original INR. This additional network takes as input the INR prediction and its highorder derivatives and its objective varies based on the specific task including denoising deblurring inpainting and classification. Strengths Wellmotivated by the potential benefits of extending INRs for downstream tasks. Covers a wide range of image processing applications. Weaknesses Experimental Design: Despite the title suggesting a general framework for signal processing and geometric processing the paper focuses solely on image processing. Lacks experiments on geometric processing and signal processing of time series signals. Validation Rigor: Provides quantitative metrics for only a limited number of selected images raising concerns about cherrypicking. Does not compare the results to the decoded image from the original INR making it unclear how much the proposed network improves over fitting the INR directly to the corrupted input. Does not provide a clear justification for the choice of network architecture and hyperparameters. Literature Review: Incorrectly interprets related work and ignores highly relevant papers. Misrepresents positional encoding and ignores the introduction of Gaussian mapping in FFM. Questions Why does the title suggest general signal processing and geometric processing while the paper focuses solely on images What happened to the geometric processing and 3D shape shown in Figure 1 For Figures 47 why are the initial decoded images from fitting the INR on corrupted images not shown How much does the extra network contribute to improving image quality Is the INSPNet with the \"A\" kernel essentially a fully connected MLP How does stacking multiple INSPNets differ from simply using a convolutional neural network (CNN) What is meant by \"modify an INR without explicit decoding\" Doesnt the proposed method require evaluating the INR on every pixel which is a form of decoding Why is the proposed INSPConvNet considered convolutional rather than multiple MLPs How much slower is the inference speed with the proposed method due to computing highorder derivatives", "Paraphrased Statement: Summary: This paper introduces a method for processing signals represented as neural fields directly without needing to sample the underlying multilayer perceptron (MLP). It proposes expressing operators on the neural field as functions of the first and higherorder derivatives of the MLP. While the output is not a new MLP it retains the resolutioninvariant nature of the neural field representation. Strengths: Originality: The approach is novel as no prior work has attempted to directly compute convolutional operators on neural fields without sampling. It also innovatively approximates shiftequivariant and certain groupinvariant kernels as linear combinations of derivatives of a neural field. Applications: The authors demonstrate various applications including image processing and a basic convolutional neural network. These applications are novel and provide practical examples of the methods utility. Quality: The paper is wellwritten and informative. The figures clearly illustrate the key contributions. The experiments are relevant and showcase the methods capabilities. Weaknesses: Practical Significance: Currently the approach has limited practical significance due to the requirement for prefitted neural fields and the computational cost of evaluating the derivative networks. Questions: The paper does not address the effective receptive field of the parameterized operators. The authors could provide insights on lifting the limitation of requiring prefitting of neural fields. Minor Comments: Line 49: The sentence \"are loaded from the INR being processing...\" is grammatically incorrect. Line 51: \"even though we are not able to (perform) surgery\" could be rephrased for clarity."], "w6tBOjPCrIO": ["Summary The paper proposes a data augmentation approach for reinforcement learning (RL) called MoCoDa. Inspired by causal reasoning and factored Markov decision processes MoCoDa exploits local factorization in the stateaction space to generate transitions for policy learning. The authors provide a theoretical analysis of sample complexity for the local factored model and demonstrate the effectiveness of the approach through empirical results on continuous control tasks. Strengths Technical soundness: MoCoDa is a novel and theoretically motivated approach that leverages the structure of causal graphs to improve sample efficiency in RL. Empirical verification: The paper provides empirical evidence that MoCoDa can improve outofdistribution generalization on novel tasks. Clear presentation: The paper is wellorganized and easy to follow with clear explanations of the approach and its benefits. Weaknesses Missing details on causality: The paper does not provide sufficient details on how counterfactual reasoning and causal graph identifiability are addressed in the approach. Lack of comparisons: The paper does not compare MoCoDa to other similar approaches that aim to learn factored masks for RL. Minor typos: There are a few minor typos in the paper. Presentation suggestions: Provide a brief introduction to CoDA upon which MoCoDa is built. Move some of the content on deriving masks to the main paper for improved readability. Add more related work on causality and causal RL data augmentation. Questions Can MoCoDa be applied to more complex outofdistribution generalization scenarios Can the approach be extended to learn nonstationary graphs to handle long horizon predictions How can the entangled longhorizon structure be leveraged in MoCoDa", "Paraphrase: Summary: This study introduces MOCODA a new modelbased framework that extends the CODA framework. MOCODA enhances the training data for an offline Reinforcement Learning (RL) agent using samples generated from the parent distribution and factored dynamics. It aims to tackle zeroshot outofdistribution (OOD) tasks. MOCODA is tested on continuous control tasks from the 2D Navigation and HookSweep2 environments. Strengths: Uses locally factored dynamics models to generalize to unseen state space areas. Reduces generalization error on specific simple tasks. Weaknesses: Lack of comparison with CODA despite using SAC as a modelfree baseline. It is unclear why MOCODA works even with overlapping parent sets. Pseudocode for the entire framework would improve clarity. Absence of results demonstrating the importance of using locally factored models on HookSweep2 tasks. Lack of clarity regarding causal assumptions. Questions: Why is there no CODA baseline Are there cases where parent sets overlap and MOCODA performs well Additional comments: Improving the papers clarity with a pseudocode. Providing analysis of the parent sets and parent distribution. Listing specific causal assumptions made.", "Paraphrase: This paper demonstrates that a model of dynamics with factored local components enhances training efficiency and the ability of a model to generalize beyond its training data when learning transitions in a dynamic system. It also introduces a method for augmenting counterfactual data in modelbased reinforcement learning allowing offline agents to tackle tasks outside their training distribution. Strengths: 1. Clear and concise presentation. 2. First to show that factored dynamics models improve generalization beyond the training data. 3. Novel counterfactual data generation method. 4. Strong experimental results. Weaknesses: 1. Relies on a known factored dynamics model which is often not available in practice. 2. Only tested in a simple environment more complex scenarios with higherdimensional state spaces are needed.", "Paraphrase: The research presented in this paper involves creating a dynamic model with local factors and the original distribution of experience data. Both are used to generate additional data which is then added to the original data to enhance the agents efficiency (reduce sample complexity) and generalization abilities beyond the initial data distribution. The paper also includes theoretical explanations for the effectiveness of this approach and proposes some modifications to the MoCoDa model to overcome its limitations. Strengths and Weaknesses: Extends the work of ref. [34] and offers theoretical insights into the role of dynamic models in improving generalization and reducing sample complexity. Figure 4 illustrates how the factorized causal model addresses potential data biases that could hinder generalization. Demonstrates the effectiveness of MoCoDa in enabling generalization outside the original data distribution and across different architectures and algorithms. The paper presents a wellimplemented and clearly written approach that would be of interest to the conference audience. Questions: Lines 8284 mention an assumption that no two graphical models share the same structure. The authors could provide further explanation about the implications (potential benefits and limitations) of this assumption and its significance for understanding the proposed framework."], "x_HUcWi1aF1": ["Paraphrase: This study focuses on offline reinforcement learning for multiple agents. However it has some limitations: The complexity of the problem grows exponentially with the number of agents in most cases. This is because each agent has to consider a large number of possible actions and to ensure that every action is considered a huge number of samples is needed. Finding the optimal solution in this setting is computationally difficult. Hence it may not be practical to find the ideal solution.", "Paraphrased Statement: Summary: This study examines offline reinforcement learning for multiple agents. It introduces the strategywise concentration principle an alternative to the pointwise principle currently employed. Using this new principle the authors present innovative algorithms with strategywise bonuses for both cooperative and competitive games. These algorithms boast improved sample complexities compared to existing methods and their complexity does not depend on the size of the joint action space. Additionally the algorithms can utilize predefined strategy classes as input and generate strategies that closely approximate the optimal strategy within that class. Strengths and Weaknesses: This paper proposes novel algorithms with strategywise bonuses that significantly improve sample complexities. This is a significant contribution to the theoretical study of offline Markov games as the new complexity result does not scale with the joint action space size. The writing is generally clear and the paper is wellstructured. However the paper does not explicitly discuss limitations despite the authors indicating its inclusion in the checklist. It would enhance the work by including a concise summary of limitations in the conclusion or checklist. Question: Regarding the strategywise concentration method can the strategywise bonus be incorporated into the online Markov game setting", "Paraphrase: This research combines the fields of multiagent reinforcement learning (MARL) and offline RL where the joint action space expands dramatically with the number of agents. The authors propose a method to tackle this challenge and find a Nash equilibrium strategy in offline MARL with unilateral coverage. They construct a confidence interval for each stateaction pair by estimating each strategy separately avoiding the exponential dependence of the joint action space. Additionally they develop frameworks for twoplayer zerosum Markov games and multiplayer generalsum Markov games in the tabular case. Strengths: Advances understanding of offline MARL algorithms in tabular settings. Key contribution: \"strategywise bonus\" that estimates state value functions for a strategy and adds a bonus. Weaknesses: Estimating state value functions can be challenging for complex strategies with unstructured class space. For twoplayer zerosum Markov games the proposed framework relies on a computationally inefficient surrogate function minimization. The assumption of independent data tuples in the dataset does not reflect realistic scenarios. Questions: Is there a principled approach to structure the strategy class especially for efficient computation in multiplayer generalsum Markov games What are the potential downsides or limitations of the strategywise bonus approach compared to pointwise estimations"], "nyn2ewuF-g9": ["Paraphrased Summary: The paper examines a type of machine learning called collaborative learning where data from various parties is combined to create a more accurate machine learning model. It focuses on fairness ensuring that each party receives rewards proportional to their contribution to the models performance (as measured by the Shapley value). The paper introduces desirable attributes for the exchange of payments between parties and derives a fair allocation scheme from these properties. This method can be applied to situations with or without financial constraints. An evaluation using segments from the MNIST dataset is presented. Strengths and Weaknesses: Strengths: Clear and concise writing Interesting fairness properties for distributing rewards Handles both budgeted and unbudgeted scenarios Comparison of two allocation schemes providing clarity on their strengths and limitations Weaknesses: Unclear contribution of the paper Similarity between the proposed properties and traditional axioms of cooperative game theory Typos in the text Questions: How does this work differ from previous research (e.g. [14]) in employing Shapley values to assess party contributions Why should fairness considerations lead to a reduction in a partys model performance under budget constraints", "Summary Paraphrase: The study introduces a novel problem where data owners contribute data to build a collaborative machine learning (ML) model that outperforms individual models. The key innovation is a solution that allocates monetary rewards and model benefits to each contributor based on Conditional Shapley values. Strengths Paraphrase: The proposed solution simultaneously handles monetary and model rewards a unique approach not found in existing literature. Ideas are generally clear except for the concept of \"payoff flows.\" Weaknesses Paraphrase: The introduction of payoff flows and Conditional Shapley values may be confusing for readers until Section 4. The incorporation of budget constraints into Conditional Shapley values is only explained later in Section 4 which could benefit from earlier clarification. Questions Paraphrase: Can the proposed framework be applied if a party has performance requirements instead of monetary budget constraints Can the method accommodate situations where a party requires the model performance to be within a certain threshold to receive a discounted payoff The formulation assumes the monotonicity of the value function. Is this assumption necessary to ensure that cooperation is beneficial and is it consistent with Shapley value computation", "Paraphrased Statement: Summary: This paper examines situations where there is a tradeoff between rewards for model performance and rewards for contributions in collaborative machine learning. Based on the observation that differences in party contributions should be considered in reward allocation a new concept called \"conditional Shapley value\" is introduced. It is shown that the resulting payoff function is unique based on specific desirable properties. The study proposes an allocation based on this payoff function and explores settings with and without budget constraints. Empirical evaluations demonstrate the effectiveness of the proposed allocation scheme. Strengths: Considers a general setting where varying payments are made based on party contributions. Develops a unique payoff function by characterizing desirable properties. Satisfies \"replication robustness\" a concept previously studied in the literature. Includes experiments on various datasets. Weaknesses: Does not compare to existing benchmarks in either theoretical statements or empirical results. Lacks a contribution section in the introduction. Payoff function descriptions are difficult to understand. No running time analysis. Additional Weaknesses (Added After Reviewer Rebuttal): Similar results have been published in 2019 with a different terminology (\"Fair Allocation\"). Concepts in the paper are presented without proper references despite being widely used in mechanism design and beyond. The conditional Shapley value appears similar to the Shapley value. Properties introduced in the paper (balance linear value dummy party) have already been proven for the Shapley value making the theoretical contributions incremental. The paper lacks comprehensive references to existing collaborative learning allocation rules. References are informal and incomplete. Question: The reviewer questions the underlying assumption that the party with more contributions should receive higher payoff rewards. They suggest that this is not a standard fairness notion.", "Paraphrase: The paper focuses on collaborative machine learning setups where multiple companies combine their data to produce more effective models than they could individually. The researchers examine the tradeoffs between model quality and vendor profitability specifically for vendors with limited resources. They propose a method for dividing rewards fairly balancing model performance and vendor profits. This method can be derived from principles of desirable properties for vendor rewards or based on the flow of rewards between parties. The authors propose guidelines for adjusting reward distribution to balance model and vendor rewards. They demonstrate the effectiveness of their method in various collaborative machine learning scenarios. Strengths and Weaknesses: Despite the intriguing subject matter the papers bar for originality is set higher due to the existence of similar research. The papers strength lies in its use of cooperative game theory concepts like the Shapley value to this domain. However the authors proposed guidelines for reward distribution require further justification and comparison with alternative power index principles. An indepth introduction to cooperative game theory is recommended. The paper lacks an analysis of computational complexity and alternative computational methods for calculating Shapley values. It also fails to address the potential for data tampering or data poisoning by a malicious vendor and the implications of computational constraints beyond budget limitations. Questions and Future Directions: What safeguards prevent data tampering or poisoning Could other mechanism design principles such as auctions or VickeryClarkeGroves mechanisms be applied in these scenarios Are there additional metrics or future work that could demonstrate a more equitable outcome", "Paraphrased Statement: Summary: The paper analyzes how to fairly distribute the costs of developing a machine learning model among multiple collaborating parties who pooled their data. The authors define several fairness criteria (including a linearity condition) and propose a unique allocation method that satisfies these criteria. They then extend the method to situations where parties have limited budgets balancing their financial constraints while maintaining model performance. Strengths: The collaborative machine learning setting is significant and practical. The fairness axioms such as budget balance are reasonable. The proposed allocation method is unique and theoretically sound. Weaknesses: Limited empirical comparisons with other approaches. The uniqueness of the method is largely due to the linearity axiom which connects it to the Shapley value. Additional Comments: The authors did not provide a comprehensive analysis of payoff allocation methods that do not rely on the linearity criterion. This could be explored in future research. The paper contributes to the growing literature on payoff allocation in collaborative settings. Questions: Can the authors elaborate on the potential advantages or disadvantages of payoff allocation methods that do not satisfy the linearity property"], "pCrB8orUkSq": ["Paraphrase: Summary This paper introduces a dataset of singleview videos for evaluating nonrigid novel view synthesis methods. It also suggests a factor to gauge the multiview effect and two evaluation metrics. Strengths The paper is wellstructured and accessible. It provides datasets with evaluation metrics. Several nonrigid novel view synthesis methods are assessed using the proposed datasets. Weaknesses 1. Multicamera setups are inherently more reliable than singlecamera setups for performance analysis. However the focus of this work is on evaluating methods rather than training models so this should not be considered a flaw in existing methods. The importance of the proposed approach in terms of data capture remains unclear. 2. The concept of MaskedPSNR is not particularly original. While it may be reasonable to mask regions that are not visible in training data when evaluating visible areas it could also be desirable to assess methods ability to predict and synthesize unobserved regions. Therefore the use of a mask does not appear to be groundbreaking. 3. Although Nerfies trains models using images from multiple cameras it can also do so with a single camera. 4. The proposed factor for the multiview effect is a simple technique and there are many other potential variations. While this factor is accurate its significance in this context is questionable. Post Rebuttal The authors acknowledge the reviewers concerns and agree to consider the feedback from Reviewer vDbH in their final submission to better position the work. Questions If the authors propose a novel method it would be appropriate to evaluate it using the proposed datasets and metrics. However as it stands the contribution may not be substantial enough for presentation at a conference like NeurIPS.", "Summary Paraphrase: This paper provides a comprehensive review of current methods for recovering dynamic 3D scenes from monocular videos focusing on Nerfies HyperNeRF and NSFF. It analyzes camera trajectories and introduces an Effective Multiview Factors (EMF) metric to measure the availability of multiview cues in dynamic scenes. The authors also note that existing datasets often have ample multiview cues. To address this they introduce a new dataset captured using an iPhone that exhibits limited multiview cues. They also introduce additional metrics such as Masked PSNR and PCKT to ensure fair evaluation. The new benchmark poses challenges for existing dynamic 3D capture approaches. Strengths: EMF metric effectively quantifies multiview cues highlighting the difficulty of different datasets with varying camera trajectories. Introduction of PCKT correspondence which provides a more direct measure of model understanding of the 3D world than PSNR. Welldocumented supplementary webpage. Weaknesses: The paper primarily focuses on introducing a new benchmark. It may be more appropriate for a benchmark track. Additional Comments: Page 8 footnote mentions better performance of a specific code base without proper referencing. Question: How were the keypoints for annotation selected in the training sequences", "Summary Paraphrase: This study identified the issue of overrepresented slowmoving objects in video datasets when using a fastmoving camera. To address this a new metric called Effective Multiview Actors (EMF) was developed to measure the level of multiview information in an image sequence. The researchers created a new dataset with low EMF proposing that it would be more suitable for assessing dynamic 3D scene synthesis methods. They compared four prominent algorithms on the new dataset and observed performance differences that were not apparent with previous datasets. Strengths and Weaknesses Strengths: The study analyzes the shortcomings of existing datasets and introduces a novel metric (EMF) to quantify their difficulty. Weaknesses: While the research emphasizes the importance of building 3D representations from lowEMF data realworld videos often contain significant highEMF portions. The difficulty of a dataset may stem from factors beyond EMF such as object shape complexity and surface properties. Question: When evaluating algorithms on sequences with varying EMF values will higher EMF sequences always exhibit lower loss compared to lower EMF sequences on existing methods", "Summary This study evaluates metrics for assessing novel view synthesis from monocular videos. The proposed Effective Multiview Factors (EMF) gauge multiview signals in evaluations accompanied by a new dataset that aims to reduce such signals. Two novel metrics Masked PSNR and Correspondence measure view synthesis and motion quality. However improvements to stateoftheart methods still face challenges in cases with motion and limited multiview signals. Strengths EMF: Facilitates the evaluation of multiviewness in captures making the task easier when there is sufficient multiview signal. Masked PSNR and Correspondence: Tailored metrics for dynamic scenes that assess image quality and motion accuracy. Weaknesses Correspondence: Limited generalizability as it is only applicable to methods that explicitly model motion. Camera angular velocity: Methods like Nerfies and HyperNeRF exhibit high velocity due to frame switching between cameras. Reordering frames could reduce velocity and potentially affect performance. No view direction or appearance encoding: These factors are essential for new view synthesis but are turned off during training potentially impacting the PSNRM score. Questions How does EMF perform if frames from only one camera are used in Nerfies Why are view direction and appearance encoding disabled during training for PSNRM evaluation"], "lYHUY4H7fs": ["Paraphrase: Summary This research explores a model for teaching policies in a fair way considering multiple agents with personal reward functions in a shared environment. The goal is to instruct each agent to follow a target policy while minimizing the cost of such incentives. The concept of \"envyfreeness\" is introduced ensuring that no agent prefers another agents incentives. The authors prove that an envyfree solution exists without penalty incentives but not necessarily with only penalty incentives. They also establish an upper bound on the cost of fairness which increases linearly with the discount factor and the number of agents. Concrete Formulation Each agent faces an MDP with shared transition dynamics and individual reward functions and discount factors. The principal aims to teach a target policy to each agent by adjusting their reward functions with \"incentives\" (costs or penalties for specific actions). The goal is to minimize the expected cumulative payments to all agents while achieving the desired fairness objectives. Definitions of EnvyFreeness Three definitions of envyfreeness are proposed: EF SEF and CF. EF requires that each agent receives more reward under their own incentive scheme than under any other agents scheme. Main Results 1. Envyfree solutions exist and can be efficiently computed using a linear program. However if incentives are restricted to be nonnegative a solution may not exist unless the discount factors are equal. 2. Explicit bounds on the price of fairness are derived for each envyfreeness definition. Strengths Introduction of envyfreeness to multiagent MDP policy design Detailed theoretical results on existence computation and cost of envyfree incentives Connection to existing literature on reward shaping and fair resource allocation Weaknesses Strong assumptions for computing envyfree incentives (exact knowledge of rewards and transitions) Lack of practical motivating examples Questions Can the techniques be extended to a learning framework where the MDP components and incentives are learned jointly Are there \"instancespecific\" losses for different cost measures besides the worstcase guarantees Minor Comments Related work section is comprehensive. Line 143 should mention the aggregation of cost measures across agents. The notation \"OPT\" in Equation 4 could be improved.", "Paraphrased Summary: This study explores how a teacher can impart a desired policy to multiple agents while adhering to fairness constraints. The focus lies on envyfree fairness where each agent prefers their own adjustments over others. Contributions include: Establishing the existence of fair solutions when adjustments can be negative. Demonstrating that nonnegative adjustments may not always yield fair solutions except when agents share a common discount factor. Proposing approaches to achieve optimal fair solutions through constraints. Analyzing the tradeoffs between fairness and efficiency in various settings. Strengths and Weaknesses: Strengths: Novelty: Combines policy teaching and fairness in a unique setting. Quality: Rigorous proofs and awareness of potential social impact. Clarity: Wellwritten overall despite some formatting inconsistencies. Weaknesses: Clarity: Inconsistent capitalization in definitions. Significance: The problem setting and algorithms lack clear practical use cases. Questions: What are realworld scenarios that can be effectively translated into policy teaching with fairness constraints What motivated the combination of fairness and policy teaching in this research", "Summary The authors propose a framework that combines fair division theory with reinforcement learning (RL) to address the problem of policy teaching. They introduce \"envyfreeness\" constraints to policy teaching through reward adjustments for various RL agents acting independently in a singleagent Markov decision process. The authors demonstrate that envyfree adjustment schemes exist if negative reward adjustments are permitted. They prove that finding such schemes is computationally tractable as it involves a convex optimization problem with linear constraints. Additionally they establish bounds for the \"cost of fairness\" which parallels the \"price of anarchy\" concept in game theory. Strengths and Weaknesses Strengths: Clear presentation with welldefined notations and rigorous proofs Significant Theorem 4.2 with implications for reward shaping in both singleagent and multiagent settings Motivating and informative examples with intuitive illustrations Weaknesses: Limited motivation for the works relevance Incomplete connection to inverse RL despite its relevance Limited discussion of future research directions Minor grammatical errors and potential feasibility concerns for solving the convex optimization problem in Section 5 Questions 1. What are the primary motivations for this work and its potential impact on the field 2. Is the convex optimization problem in Section 5 solvable for realistic RL problems (e.g. gridworlds or extensiveform games) 3. What factors contribute to the choice of the cumulative measure of cost and how might it impact the results if the teacher and students were cooptimized", "Paraphrased Summary The study investigates the challenge of instructing agents to adhere to a specific policy while making sure the incentives are equitable (envyfree). Definitions for envyfree (EF) instruction and fairness in this setting are provided. Subsequently the study establishes positive and negative criteria for the feasibility of EF policies. The practicality of identifying costeffective solutions under two common cost functions is examined along with the introduction of a \"price of fairness\" metric\u2014the extra cost incurred due to EF constraints. Strengths and Weaknesses Clear writing welldefined contributions and thorough explanations. Useful characteristics of newly introduced ideas are established providing insights into EF teaching policies. Positive consideration of positive reward incentives. Limitation of agent operation in isolated environments is acknowledged as a necessary but unfortunate constraint. Practical examples that justify the model would enhance motivation. Analysis of teaching agents to follow varying policies is suggested. The presented EF definition is considered weak the Strong EF (SEF) definition is suggested to better capture equity. Questions The focus on EF strategies with nonnegative incentives and unequal agent discount factors could be reduced to emphasize equally significant contributions in Theorem 4.3 and Section 6."], "yI7i9yc3Upr": ["Paraphrased Summary: This study introduces a method for converting sequencelevel requirements into tokenlevel instructions during text generation with the goal of enhancing controllability. A neural model is utilized to approximate the tokenlevel guidance by utilizing samples drawn from the base model. Both theoretical analysis and practical results confirm the effectiveness of this approach across various text generation tasks. Strengths and Weaknesses: Pros: Clear and concise presentation Innovative method backed by theoretical analysis Comprehensive experimentation on various tasks demonstrating performance gains Cons: The NADO model introduces new hyperparameters that require tuning. The authors could provide more extensive analysis of the hyperparameters sensitivity. Questions: See comments above for potential questions.", "Paraphrase: Summary: This study introduces a method for controlling autoregressive text generation by breaking down control signals into tokenlevel constraints. This approach is formulated as an optimization problem using posterior regularization and approximated by a neural network. Experiments demonstrate its effectiveness in lexically controlled generation and machine translation tasks with formality control. Strengths: Novel method for controlling autoregressive generation. Distinguishes itself from existing approaches. Provides theoretical guarantees. Demonstrated effectiveness in experiments particularly for formality control in machine translation. Weaknesses: Missing details: Number of samples required for approximation (Section 3.3). Quality and diversity requirements of the base model. Performance relationship with model scale. Handling constraints that are distant from base distribution. Behavior and learning with partial constraint satisfaction. Lack of linguistic explanation: Examples of intermediate sample sequences and their evolution towards constraint satisfaction. Comparison with other models in terms of generated sequence characteristics. Absence of human evaluation: Classifierbased evaluation could be unreliable due to spurious correlations. Recommendations: Address missing details. Provide linguistic explanations and examples. Include human evaluation or generated examples for reviewer judgment.", "Summary Task: Developing a versatile and efficient framework for controlling text generation models based on input from a guiding oracle. Background: Previous methods like PPLM GeDI and FUDGE also used auxiliary models to guide base models. However they either modified the base model distribution without theoretical guarantees or required labeled data for training. Main Contributions: Proposing a decomposition of a sequencelevel oracle function into tokenlevel guidance for controlling the base model during text generation. Deriving an optimal solution to incorporate the tokenlevel guidance without external labeled data or tokenlevel guidance. Conducting a theoretical analysis to demonstrate how the quality of guidance approximation affects generation results. Main Results: Experiments show that the proposed framework effectively guides the base model towards the specified oracle while maintaining high generation quality in applications like text generation with lexical constraints and machine translation with formality control. Strengths and Weaknesses: Strengths: High generation quality despite being a postprocessing method. Efficient inference time. No need for additional labeled data due to training on examples sampled from the base model. Weakness: Typo in Line 41: \"aims\u2192aim\""], "rwdpFgfVpvN": ["Paraphrased Statement: This study focuses on a variation of the Online Convex Optimization (OCO) problem where both a loss function (ft) and a constraint function (gt) are revealed after selecting an action xt at each round t within a time horizon T. The goal is to minimize the overall loss (\u2211t1T ft(xt)) and the constraint violation (\u2211t1T max(gt(xt) 0)). The paper proposes RECOO a single algorithm that achieves stateoftheart bounds for static and dynamic regrets as well as constraint violation for convex and strongly convex losses. Numerical experiments confirm the theoretical findings. Strengths: Clear and wellwritten presentation. Comprehensive literature review and comparisons. Novel use of a timevarying minimum penalty price in the update for Q(t) leading to new proof techniques for bounding constraint violation. Weaknesses: Experiments conducted only on synthetic datasets realworld applications mentioned in the introduction should be included. The claim of achieving the \"best of two worlds\" is misleading as it refers to adversarial and fixed constraints not adversarial and stochastic constraints. Clarification is needed on Slaters condition its role in previous works and how the proposed framework differs. The definition of path length PT should be provided earlier in the paper. The natural static benchmark should be modified further and the choice should be justified. RECOO is similar to algorithms in [18] and [30] comparisons and highlights of new ideas and proof techniques should be included. Questions: Can the experttracking technique in [33] be applied to adversarial constraints to obtain optimal O(\u221aPT T) regret bounds The appendix could contain the proofs to allow additional experiments with realworld datasets. Convexity of loss functions is missing in the statements of Theorem 1 and Theorem 3.", "Paraphrased Statement: Online convex optimization (OCO) is a crucial aspect of online learning theory. In practical scenarios optimization often requires satisfying constraints. This paper investigates the constrained OCO problem under fixed and adversarial conditions where the loss function is either convex or strongly convex. Unlike previous studies our analysis considers the \"hard constraint violation\" assumption where the average constraint violation (CV) is calculated only based on positive violations resulting in a minimum where the solution may become infeasible after extensive iterations. We also define two types of regret: static and dynamic. Under the hard constraint assumption we either match or improve upon existing results for CV in all the considered settings. Strengths: Clarity: The paper is wellwritten providing clear explanations of different settings and emphasizing the significance of the hard constraint setting. The proofs are presented in an understandable manner. Weaknesses: Lack of Technical Explanation: The paper does not adequately explain how it achieves the improved CV bound compared to existing methods. It should highlight the specific technical approach that led to this improvement. Originality: While considering hard constraints in OCO is novel it is not entirely original as the techniques used have been previously employed. Significance: The results are relevant to realworld problems and CV bound improvements are demonstrated in some settings. Limitations: Lack of Experimental Validation: The experimental results presented are only for toy examples. To showcase the practical significance of the method results on realworld datasets would be valuable. Questions: The experimental results in Figure 1 show a constant CV bound for existing methods contradicting the sublinear bound claimed in Tables 1 and 2. What accounts for this discrepancy", "Paraphrased Summary: This research investigates the online convex optimization problem with strict constraints. Unlike previous settings violations from individual iterations cannot be offset by others. The authors introduce RECOO an algorithm tailored for both fixed and adversarial constraint environments. RECOO demonstrates superior performance minimizing either the regret or constraint violations. The cornerstone of RECOO is a rectifying mechanism for constraint functions. The work also introduces a dynamic regret setting and analyzes RECOOs performance within it. Experimental findings support the theoretical findings. Strengths and Weaknesses: Strengths: Clear presentation Novel and significant theoretical results Comprehensive experimental analysis Weaknesses: Summary could include additional baseline algorithms (e.g. online SGD with constraint projection in the fixed constraint setting) Questions: No specific questions", "Paraphrase: The work focuses on an online convex optimization problem where the goal is to minimize regret while also avoiding violations of hard constraints (restrictions that cannot be compensated for later). Setting 1: Fixed Known Constraints Achieves regret bound of O(\u221aT) and constraint violation of O(1). Setting 2: Adversarial Constraints Achieves regret bound of O(\u221aT) and constraint violation of O(T(34)). Setting 3: Strictly Convex Functions Provides improved bounds. Strengths and Weaknesses Addresses an important problem with rigorous analysis. Introduces novel techniques for better regret and violation bounds. Concern: The significance of the results for the fixed constraints setting is questionable as the problem can be solved trivially by restricting the decision space and using standard regret minimization techniques. The algorithm does not appear to be efficient as it requires complex computation. The approach may be similar to standard online convex optimization problems with relaxed constraints. Questions: Is there a connection between the proposed method and classical online convex optimization problems How does the tradeoff between performance and regret relate to the choice of the parameter \u03b5"], "q-FRENiEP_d": ["Paraphrased Summary: This research introduces a new \"saliency guided mixup\" technique for point clouds. It identifies salient points in two point clouds and assigns blending weights using an RBF kernel centered on those points. This method expands upon PointMixup and outperforms current methods. Strengths: Incorporating saliency guidance into point cloud mixup aligns with successful applications in imagebased techniques. Selecting remote query points prevents overlap while preserving essential local features. The experimental evaluations appear thorough. Weaknesses: None identified. Questions: Why are not all robustness tests from RSMix included in Table 3 How does the weight decay factor (\\sigma) impact SageMix performance beyond the qualitative evaluation in Figure 3 A reference to an unreviewed arXiv paper (PointCutMix Regularization Strategy for Point Cloud Classification) is missing.", "Paraphrase: Summary: This paper describes a method (SageMix) for increasing the data in 3D point clouds. SageMix prioritizes important local details and produces continuous samples with gradual changes in mixing ratios. Saliency is calculated based on its impact on the task which is measured using loss gradients. SageMix outperforms similar Mixup methods consistently as shown by experiments. Strengths: 1. Novel saliencyguided sequential sampling 2. Ablation studies demonstrating the methods impact 3. Wellorganized paper Weaknesses: 1. Lack of indepth analysis on 2D vs. 3D mixup methods in the introduction 2. Limited novelty in the shapepreserving continuous mixup component 3. Limited experimental results with comparisons only to PointMixup and RSMix and no qualitative results provided Questions: 1. Challenges in extending mixup from 2D to 3D 2. Impact of different saliency computation methods 3. Suggestion to expand the evaluation to provide a more thorough assessment", "Summary SageMix is a novel data augmentation technique specifically designed for point clouds. Inspired by Mixup SageMix combines two point clouds by preserving prominent local features. The authors demonstrate the effectiveness of this approach through experiments. Strengths Clear and wellwritten paper Augmentation improves network robustness (Table 3) Weaknesses Experimental results show small differences between techniques with no error margins provided (Table 2) Lack of standard deviation measures makes it difficult to determine statistical significance Experiments are primarily focused on classification with limited details on partsegmentation It would be beneficial to demonstrate the techniques applicability beyond classification tasks Questions Can the authors provide mean and standard deviation values for the experimental results Can the paper include segmentation experiments comparable to Table 2 Overall The paper introduces a promising data augmentation method but the lack of significant experimental findings and detailed exploration in the partsegmentation domain leaves the reviewer with uncertainty. The rating will be revised based on the authors response to the concerns raised."], "sr0289wAUa": ["Paraphrased Statement: Researchers introduce a novel approach to enhance robot learning by leveraging offline demonstration data. They propose a method to: 1. Identify fundamental skills from offline demonstrations using a latent space. 2. Utilize these skills to accelerate learning new tasks including those that require combining skills sequentially or concurrently. To extract skills they train a VAE on action sequences to create a latent space and then determine skill priors by minimizing a distance measure. When learning a new task the policy is constrained to remain close to a combination of skill priors through another distance measure. The weights of this combination are adjusted dynamically to optimize task performance. The method is evaluated in simulated navigation tasks where it outperforms several baselines including learning from scratch and simply combining demonstrations with reinforcement learning. However the researchers acknowledge potential challenges in training stability due to the numerous components involved and the limited scope of experiments. They welcome further exploration and provide questions for future investigation: Detailed hyperparameter sensitivity analysis Evaluation in nonnavigation domains Comparison with additional relevant work such as PARROT", "Paraphrased Summary The paper addresses the challenge of combining skills learned from multiple offline datasets. The proposed method Aspire acquires numerous priors from these datasets and dynamically adjusts their influence during online training for tasks requiring multiple skill applications. Aspire outperforms existing compositional policy methods on three Maze tasks that involve navigating and pushing while avoiding obstacles. Strengths Clearly defined problem and motivation Concise method description Demonstrates Aspires effectiveness in composing diverse skills Novel algorithm and loss functions for policy regularization by composed primitives Weaknesses Explanation of the approach could be improved Clarification needed on offline vs. online learning Skill prior generators rationale is not clear Need for skill prior generator questioned by potential alternative Counterintuitive result of push priors dominance Questions Is G(\u03bc) learned offline and \u03c9(\u03c3) learned online What does \u03c9 condition G(\u03bc) on in Equation 6 Are all baselines trained with equal parameters How does Aspire handle situations where the push agent is unable to complete the task as mentioned in the text despite the push priors high weight Why is the push prior significantly more prominent than the navigation prior", "Paraphrase: Summary: This study expands on previous work in skill priorregularized reinforcement learning by developing a distinct skill prior for each skill. With K skill labels from offline data we create K skill policies using Variational Autoencoders (VAEs) along with K skill priors that guide the agent on whether a particular skill is appropriate in a given state. To combine the K skill priors into a single composite skill prior for subsequent reinforcement learning we employ a weighting function that optimizes the estimated Qvalue based on the composite skill prior. This allows us to simultaneously compose multiple skills such as navigating towards a goal while avoiding obstacles. Experiments show that the proposed method can effectively combine up to three skill priors and tackle tasks requiring the execution of both individual and composite skills. Strengths: The visualizations in Figures 3 and 4 clearly demonstrate how the adaptive weight module regulates the downstream task policy. The novel method of weighting multiple skill priors is supported by strong quantitative and qualitative results. Weaknesses: The claim in Line 2832 about the limitation of single skill priors in handling composition lacks clarity. Figure 1 does not provide relevant information in this context. The use of the term \"the learned policy\" is confusing in some instances. The requirement of task labels for trajectories may limit the applicability of the proposed method which was a key advantage of the prior SPiRL approach. A comparison between uniform weights for multiple skill priors and the proposed adaptive weight module would be valuable. It is unclear if the MCP baseline is equivalent to using the proposed composite skill prior as a policy. If not this baseline should be included. Questions: Paper citations should be updated to use appropriate references (e.g. from conference or journal sources). Numerous typos and grammatical errors need to be corrected and the overall writing quality should be improved.", "Paraphrase: Summary: This paper introduces a hierarchical Reinforcement Learning technique. The toplevel controller selects skills that enhance rewards while adhering to a weighted combination of all skill priors. The skills and priors come from fitting one prior to each identified dataset using a Variational Autoencoder (VAE). A key aspect is the adaptive skill prior which learns to assign weights to skill priors based on their critical value. The method performs well in three simulation environments exhibiting high sample efficiency and performance. Strengths: The proposed approach has welldefined components and a logical concept. The Adaptive Weight Module allows for optimal weight allocation among priors to create skills with high values. The visual representation of weights during a rollout is userfriendly. Weaknesses: The test settings concentrate on a single domain. Specifically the point mass and ant settings vary primarily in lowlevel dynamics which mainly affects the skill learning phase (not the works main focus). As a result the conclusions drawn appear to be somewhat limited. Questions: With skill encoder and decoder trained on a single primitive does the skill latent space (z) become overly tailored to the initial task"], "yNPsd3oG_s": ["Paraphrased Summary: The paper demonstrates that backdoored Deep Neural Networks (DNNs) create a linear decision region under certain conditions. The authors posit that compromised DNNs exhibit increased linearity due to many neurons activating within a specific part of the activation function. Based on this insight they propose a novel trainingstage defense (NONE) that detects and removes potentially poisoned samples repairing affected neurons. NONE has been tested on various datasets including MNIST GTSRB CIFAR10 ImageNet10 and TrojAI. Strengths: 1. Timely and relevant topic for NeurIPS attendees. 2. Opensource code availability is commendable. 3. Theoretical support for the defense method is provided. 4. Examines resistance to potential adaptive attacks. 5. Clear concept although certain details need clarification. Weaknesses: 1. Inadequate detail and justification for certain claims: \"An adaptive attack with slow poisoning can bypass such defenses\" without defining \"slow poisoning\" and supporting evidence. \"These methods fail to defend against the natural Trojan\" without supporting experiments. 2. Missing assumptions in Theorem 3.3: It is assumed that the Trojan is complete and accurate. 3. Lack of specifics in certain appendix statements. 4. Omission of a relevant related work and baseline defense [1]. 5. Poor organization in Section 4: Long paragraphs hinder readability. Missing details on sample separation Fishers linear discriminant analysis and Jenks natural breaks optimization. 6. Unclear evaluation of defense against hidden Trojan backdoor attacks. 7. Limited comparison of NONE with other defenses under natural Trojan attacks. 8. Insufficient exploration of the defenses internal mechanism: Precision and recall of the poisoned sample detection stage are not provided. Minor Issues: 1. Typographical error in Line 91 (Page 2). 2. \"R\" should be \"Rl\" in Line 143 (Page 4). 3. Verify and cite official versions of all references (e.g. [36]).", "Paraphrase: Summary: This study found that neurons involved in backdoors in Deep Neural Networks (DNNs) form a hyperplane surface that serves as the decision boundary for the backdoor target class. Based on this insight the authors developed NONE (NONLinEarity) a robust training algorithm that recognizes and eliminates linear decision boundaries filters out potentially poisoned inputs and resets impacted neurons to enforce nonlinear decision regions. Strengths and Weaknesses: The paper is wellstructured and straightforward to follow. Questions: Observation of Hyperplane Surface: The finding that backdoor attacks exhibit shortcut or linearlyseparable characteristics is not entirely new as noted in previous studies. The authors should clarify that this observation is not solely attributed to their research. Specific Characteristics of Hyperplane: The authors claim that a single hyperplane in the input space corresponds to all poisoning samples due to the piecewise linear function (ReLU). They should provide evidence to support this argument and explain why only one hyperplane exists. Generalizability of Linear Separability: It remains unclear whether linear separability can be induced by specific model architectures input dimensionality or other activation functions. Adaptability of Adaptive Attacks: The authors demonstrate that traditional backdoor attack techniques like BadNets create linear separability in DNNs. However exploring the feasibility of \"adaptive\" attacks that blur the distinction between clean and backdoor features within separating hyperplanes is worthwhile. Neuron Reset Timing: The reason for resetting neurons during training instead of after training in the backdoor removal step should be explained. Computational Complexity: The authors should compare the computational complexity of their approach with other techniques.", "Paraphrase: Summary: The paper presents a defense against backdoor attacks in neural networks. It detects and suppresses malicious neurons that create shortcuts between classes. The defense has improved effectiveness compared to existing defenses with a low drop in clean accuracy. Strengths: Novel approach High effectiveness Ablation studies demonstrate robustness Opensource code available Weaknesses: Informal definitions of symbols in the algorithm Lack of paragraph breaks for readability Missing citations for datasets Questions: Runtime performance of the NONE defense Scalability to larger models and datasets (e.g. ImageNet) Number of inputs removed by NONE on CIFAR10 Precision and recall of the removal process Evaluation of NONE against backdoor attacks targeting the training code"], "lfe1CdzuXBJ": ["Paraphrased Statement This paper explores a new fairnessfocused bandit problem in which each option represents a specific group. The fairness metric used is group meritocratic fairness where individuals are chosen based on their relative ranking within their own group. To achieve this the bandit algorithm must learn the cumulative distribution function (CDF) of rewards. The paper proposes a linear bandit algorithm called FairGreedy and analyzes its performance in terms of fair pseudoregret. Strengths S1: Presents a novel bandit problem with the challenge of estimating relative rank requiring CDF estimation. S2: Provides a technically sound theoretical analysis clearly outlining the steps leading to the main result. S3: Introduces a valuable meritocratic fairness criterion and extends it to sequential decisionmaking which is relevant for hiring processes. Weaknesses W1: The formal setting is unrealistic as each option corresponds to a sensitive group leading to questionable scenarios. W2: Lacks theoretical guarantees for standard regret making it difficult to assess the fairness cost. W3: Does not provide empirical comparisons with existing fair bandit algorithms hindering evaluation of performance. Questions Provide a specific scenario that accurately maps to the formal problem studied in the paper. Address concerns about the lack of theoretical guarantees for standard regret. Discuss how the proposed algorithms compare to existing fair bandit algorithms in empirical evaluations.", "Paraphrased Statement: Summary: This study examines fairness for groups in linear contextual bandit algorithms. It introduces a notion of fairness that evaluates rewards for individuals relative to others in their group. The study also proposes a fairnessrelated regret metric. Additionally it presents the FairGreedy policy which demonstrably achieves sublinear regret. Numerical simulations are provided. Strengths: Practical problem with realworld applications. Comprehensive review of related work. Technically challenging solution. Weaknesses: Writing could be refined. The regret definition and fairness concept may be overly aligned potentially reducing the fairness constraint during learning. Questions: 1. Problem Formulation: The formulation in Section 3 is unclear. Line 125: The authors should explain why the standard objective is unfair. Line 127: The authors should define the random variable Xa and clarify what \"i.i.d. copies\" means. Line 130: The distribution of arm a should be defined. 2. Regret Definition: The authors claim in Remark 3.1 that their regret definition is similar to the standard pseudoregret definition but this is not evident from the provided formulation. More discussion or proof would be beneficial. Furthermore the interplay between fairness and regret minimization can vary depending on their definitions so the statement in Line 148 is incomplete.", "Paraphrased Statement: This research focuses on selecting candidates equitably while ensuring that the rights of underrepresented groups are protected. It introduces the concept of Group Meritocratic Fairness which prioritizes candidates with the highest relative standing within their underrepresented group. The authors propose the FairGreedy policy which ensures that the selected pool maintains demographic parity (a fairness measure). They unlock regret analysis by leveraging techniques from classical bandit problems allowing them to calculate the regret of the selected pool relative to the best possible pool. Strengths: Clarity and ease of understanding Novel approach to integrating fairness metrics into bandit algorithms Weaknesses: Lack of comparison with existing fairnessaware bandit algorithms Questions: How can exploration be optimized to maximize Group Meritocratic Fairness The greedy approach typically does not require deep exploration strategies but insights on using estimate uncertainty to improve exploration efficiency would be valuable.", "Summary Paraphrase: The research investigates a scenario where different groups of candidates are represented by distinct options. The goal is to find a balance between maximizing performance by selecting highranking candidates and promoting fairness. Fairness in this context ensures that candidates with higher relative ranks within their groups are chosen over those with lower ranks. Strengths and Weaknesses Paraphrase: Strengths: The algorithm achieves low regret (sublinear \\sqrt(T)) in a fair manner. This means the selected candidates have consistently high ranks within their groups ensuring performance and fairness. The algorithm ensures fairness across groups by uniformly sampling from all options. This prevents overselection from specific groups. The technical foundation of the algorithm hinges on the fact that rank distribution is uniform across groups allowing for fair sampling of highranking candidates without hindering exploration of all options. Weaknesses: The definition of fairness could be expanded to incorporate comparisons across different groups. Currently it only considers relative ranks within each group. The algorithms regret bound has a K3 dependency which could potentially be improved by exploiting knowledge gained during exploration. Assumption iv in the paper requires further clarification and examples of distributions that satisfy or violate this assumption. Questions: Can the authors provide a more intuitive explanation of Assumption iv Can the authors discuss the implications of relaxing the assumption that \\mu (the reward mapping) is the same across all groups Can the authors explore the possibility of implementing exploitation during the exploration phase potentially reducing the K dependencies in the regret bound"], "lAN7mytwrIy": ["Paraphrased Summary This study presents a selfsupervised multiview stereo (MVS) framework that leverages a partaware approach. It consists of: Part representation learning: Geometric features are encoded to generate a piecewisesmooth depth map. Partaware matching: Depth hypotheses are propagated using a patchmatching algorithm. Strengths Achieves competitive performance even outperforming some supervised methods. Improves upon previous selfsupervised and supervised methods in certain datasets. Does not require a fixed number of segmentation labels or a ground truth segmentation map. Novel and incorporates geometrically reasonable principles. Visualization demonstrates the effectiveness of the part representation learning. Weaknesses Evaluation metric and units are absent in Table 3 making comparisons difficult. Running time is inferior to costvolumebased methods. The framework relies heavily on threshold parameters which could make it sensitive to parameter settings. Additionally details on parameter robustness are lacking. Questions To what extent is the proposed method resilient to variations in threshold parameters Are the threshold values the same across all datasets or optimized for each dataset individually What is the typical number of seed points used", "Paraphrased Statement: Summary: The paper suggests a method for enhancing pixelbypixel multiview correspondence matching using geometric relationships. It introduces an \"elastic part representation\" to capture surface connectivity obtained by feeding the input image into a convolutional neural network. The authors then propose selfsupervised contrastive learning to promote similarity in the part representation on the same physical surface based on the estimated depth and normal map. Strengths: Novel elastic part representation and effectiveness proven through ablation study. Comprehensive experiments demonstrating significant improvements over other selfsupervised methods. Weaknesses: Similarity to the PatchmatchNet method particularly in the \"propagation\" concept. Absence of detailed discussion on differences with PatchmatchNet. Inconsistent use of symbols: \"zp\" refers to both \"perpixel elastic part representation\" and \"feature on pixel local p.\" Redundancy in Equations (1) and (6). Lack of specification on the final loss function used in training. Disparity in reported Gipuma values for the DTU dataset compared to the original paper. Absence of quantitative evaluation for COLMAP on the TT dataset despite its inclusion in Figure 4. Undefined abbreviations (e.g. NCC). Questions: 1. Explain the similarities and differences between \"propagation\" and \"adaptive propagation\" and why PatchmatchNets \"adaptive propagation\" was not cited as a suitable reference. 2. Clarify the meaning and usage of \"zp\" throughout the text. 3. Provide the precise formulation of the loss function employed for CNN training.", "Paraphrased Summary: This paper introduces a technique called elastic part representation into a selfsupervised MultiView Stereo (MVS) framework to improve the accuracy of dense correspondences. The elastic part representation identifies pixels with similar visual features to guide depth propagation. It is trained using contrastive learning after an initial unsupervised learning stage. The proposed approach shows promising results on benchmark datasets. Strengths: 1. Emphasizes selfsupervised learning for MVS using contrastive learning to identify pixels belonging to the same surface. 2. Demonstrates successful results on various benchmark datasets. 3. Includes detailed ablation studies to support the effectiveness of the elastic part representation. Weaknesses: 1. Figure 1 does not accurately represent the multistage nature of the algorithm specifically the reliance on an initial depth estimate from an unsupervised learning framework before applying the elastic part representation. 2. The papers arguments regarding the sensitivity of pixelwise photometric regularization to textureless regions and illumination changes are unclear. It is not evident how the elastic part representation can address these issues. 3. The depth estimation process is not described in detail and seems to be based on traditional optimization rather than learned reasoning. Questions: 1. How is the initial depth map obtained for contrastive learning Is this process repeated as the depth map improves 2. How are the depth hypotheses chosen Are they related to the initial depth estimation without the elastic part representation 3. Please clarify how the energy function in line 171 is optimized. Does this process involve any network learning 4. Does the runtime in Table 5 include the calculation of the feature distance confidence map and normal map", "Paraphrase: Summary This research introduces ElasticMVS a novel selfsupervised method for multiview stereo (MVS). Unlike existing techniques that rely solely on local image consistency ElasticMVS employs geometric relationships derived from images to guide pixellevel correspondence. The method consists of: A new \"elastic part representation\" that captures spatial proximity and surface connections trained using a contrastive learning approach. A selfsupervised MVS framework (ElasticMVS) that estimates depth per view by propagating and evaluating partaware information. Experiments demonstrate that ElasticMVS outperforms both supervised and selfsupervised methods significantly. Strengths Addresses the limitations of local photometric consistency in selfsupervised MVS. Provides clear motivation and insights for the proposed approach. Offers extensive comparisons with baselines and detailed ablation studies. Achieves stateoftheart performance backed by qualitative and quantitative results. Weaknesses Question: Why learn the part representation (zp) when its supervision signal (3D distances between points) is available during inference Could simpler similarity functions be used instead Question: How does the part representation benefit propagation compared to geometricbased heuristics Question: How robust is the part representation to errors in the initial depth map Can it handle textureless regions Comment: The abstract overclaims the methods use of \"surface connectedness and occlusion boundaries.\" It primarily relies on geometric proximity for clustering and guidance. Question: What is the value of \u03b2 in equation (7) Minor Issue Line 94: \"sucn\" should be \"such\""], "zp_Cp38qJE0": ["Paraphrased Summary This research proposes using consistency regularization to enhance fairness in machine learning models under varying data distributions. Specifically it extends the LAFTR algorithm (an adversarial learning method for fairness) and FixMatch to enforce consistency constraints through selftraining. The consistency constraints are expanded to include \"intragroup expansion\" where transformations and augmentations are applied within each group and label. Experiments on both synthetic and real datasets demonstrate that this approach can sometimes improve fairness in the target domain compared to current methods. Strengths Tackles the critical issue of fairness under distribution shift with a novel approach using consistency regularization. Wellwritten paper with clear explanations of the method and related work. Weaknesses The proposed algorithm appears to perform effectively only on synthetic data (Figure 3). Real dataset experiments show modest results: On UTKFace to FairFace transfer (Table 1) the proposed methods (LaftrFixMatch and CFair) do not achieve the best fairness in terms of equalized odds. Other baselines exhibit better accuracy and lower equalized odds. While group accuracy variance decreases for the proposed approach its significance compared to fairness metrics is unclear. Additionally the high standard deviations raise questions about the statistical significance of the results. On the tabular dataset (Figure 4) only LAFTR is shown as a baseline so its difficult to assess the superiority of the proposed methods. The fairness improvements are also not clearly evident from the figures. Significance tests are not provided to support the results. The transformationaugmentation component is crucial but lacks detailed discussion. The authors mention excluding specific transformations in Section 6.3 but should analyze the fairness impact of each transformation more thoroughly. Robustnessfocused augmentations may not necessarily enhance fairness and a detailed study is needed. Minor Figure 3 (a) shows LAFTER instead of LAFTR. Questions How does Table 1 demonstrate the superiority of the proposed methods in target fairness The numbers suggest comparable performance with other baselines. Why are augmentations designed for robustness assumed to improve fairness The authors should investigate the potential negative effects of certain transformations.", "Summary: The author examines how model fairness is impacted under distribution shifts. They employ equalized odds fairness metrics and introduce various distribution shift types in synthetic image and tabular datasets. Additionally they present an algorithm for transferring fairness with a fair consistency regularization. Strengths and Weaknesses: The paper focuses on a timely topic but uses a single fairness metric without justifying its choice or discussing its societal impact. The text is difficult to comprehend due to unfamiliar mathematical formulations and complex notation. It relies heavily on an appendix and external sources without proper referencing. Assumption 1: This assumption enables the problem to be solved but in realworld scenarios it may not hold. Concept shift where the data generation process itself changes is common in tabular data. It challenges the distinction between covariate and concept shift making the assumption unrealistic. Question: Do the limitations imposed by previous authors on predicting model performance degradation under distribution shift still apply when assessing the transferability of fairness", "Paraphrased Statement: Summary: This paper investigates fairness preservation under distribution changes. It presents an indepth analysis of distribution shift classifying it into two types: subpopulation shift and domain shift. Employing selftraining the authors propose sufficient conditions for fairness transfer and develop an algorithm with fairness consistency regularization. Experiments demonstrate the methods effectiveness in maintaining fairness and accuracy under various distribution shifts. Overall the paper is technically sound and wellevaluated. Strengths: 1. Clear Analysis and Sufficient Condition: The authors provide a detailed analysis of fairness under distribution shift and establish necessary conditions for fairness transfer. 2. Simple and Effective Fair Consistency Regularization: The proposed regularization is easily implemented and supported by theoretical insights. Weaknesses: 1. TheoreticalPractical Gap: The theoretical result (Theorem 4.1) suggests minimizing worstgroup consistency loss for fairness transfer. However the proposed regularization uses a balanced consistency loss. The reason for this discrepancy and the effectiveness of the proposed regularization method remain unclear. 2. Marginal Improvement: Results show only a minor improvement in accuracyfairness tradeoff compared to the LaftrFixMatch baseline. It is difficult to assess the superiority of the proposed method without a broader analysis. 3. Limited Parameter Exploration: The Pareto frontier of different methods with variable hyperparameters is not presented making it challenging to evaluate the potential performance range."], "z9CkpUorPI": ["Paraphrase: This research introduces a scheduling optimizer for designing hardware circuits (HLS) using a graph neural network (GNN) model. To improve efficiency and output quality in HLS the researchers developed a scheduling approach that leverages the GNN models ability to process complex graphs. Strengths: Clear Explanation: The paper provides a straightforward example to explain the optimization algorithm aiding readers comprehension. Reduced Complexity: The method converts overall priority into a relative order simplifying calculations and reducing complexity. GNN Applicability: The GNN model aligns well with the problem enabling the prediction of operator priorities and accelerating the scheduling process. Weaknesses: Lack of Detail: Certain descriptions require elaboration such as clarifying the mutual exclusivity of operators in each step and providing a more precise execution description. Limited Experimental Analysis: While the accuracy of the proposed method is compared to the EDS method the analysis of the tradeoffs between improved accuracy and increased execution time using a GPU is missing. Questions: 1. The meaning of ASAPALAP and its role in priority determination require clarification. 2. The authors contradictory statements regarding priority value and scheduling actions (lines 138 and 140) need to be reconciled. 3. The description of the encoding operator and the benefits of normalization require further explanation. 4. The accuracy of Algorithm 1 is mentioned in line 25 but the algorithm only has 23 lines creating confusion. 5. The paper lacks an explanation of the final time intervals for different scheduling sequences in Figure 5 which would enhance the understanding of operator flexibility during scheduling.", "Paraphrased Statement: Summary: Researchers introduce NeuroSchedule a novel scheduling algorithm using Graph Neural Networks (GNNs) to optimize FPGA designs. NeuroSchedule is the first GNNbased scheduler in this field and leverages supervised learning trained on data annotated by an ILPbased scheduler. Compared to the ILP scheduler NeuroSchedule significantly reduces runtime while delivering nearoptimal solutions. It also outperforms a heuristic method in solution quality. Strengths: Inference is exceptionally fast 50000 times faster than the ILP scheduler. NeuroSchedule generates better solutions than the entropydirected method improving by an average of 6.10. Finetuning the pretrained model allows for effective adaptation to different settings. Rank loss (relative priorities) enhances the models optimality compared to traditional MSE loss. The experimental section is wellstructured. Weaknesses: The study could benefit from a more rigorous statistical analysis to validate the significance of NeuroSchedules performance superiority over the entropydirected method. A comparison with reinforcement learningbased scheduling methods would provide additional insights. A potential typo exists in one of the equations where \"max\" may need to be changed to \"min\" for accuracy. Questions: Should \"max\" in Line 41 be \"min\" because list scheduling prioritizes operations with higher priorities (smaller F values)", "Paraphrased Statement: Summary: This paper presents a method named NeuroSchedule which utilizes Graph Neural Networks (GNNs) for efficient and effective scheduling in HighLevel Synthesis (HLS). The paper formalizes the operation scheduling problem in HLS and integrates operation information into the GNN model to enhance schedule learning. Additionally the paper creates a dataset of ControlData Flow Graphs (CDFGs) with various scheduling configurations for model pretraining. By finetuning on randomly generated designs with diverse settings the model generalizes its predictions accurately. Strengths and Weaknesses: Strengths: 1. The paper proposes a standardized approach to address the complex scheduling issue in HLS potentially paving the way for wider application of ML techniques in hardware design. 2. The methodology and supplementary materials are elucidated clearly facilitating comprehension and potential replication. Weaknesses: 1. The performance improvements attributed to NeuroSchedule may not be substantial based on the provided evidence. While Table 1 indicates a 16.67 reduction in runtime latency it comes at the expense of a 5.7fold increase in optimization runtime. Furthermore existing entropydirected methods are already nearoptimal raising questions about the tradeoff between minor latency improvements and significantly longer scheduling times. 2. The scheduling assumptions appear simplistic considering primarily individual operation latency and dependencies. The paper should clarify how resource conflicts are resolved when hardware modules need to be reused such as in Figure 5.a where multiple additions may require the same adder unit. 3. The paper does not address the impact of different HLS paradigms on scheduling which can significantly influence performance by modifying underlying hardware elements like buffer memory size and bandwidths.", "Paraphrased Statement: Summary: This study introduces NeuroSchedule a graph neural network (GNN) scheduling technique for highlevel synthesis (HLS). NeuroSchedule employs a new priority function that predicts the readiness of an operation enabling list scheduling. To enhance NeuroSchedules versatility pretraining techniques are used allowing finetuning for specific tasks. NeuroSchedule closely approximates the optimal priority function derived from integer linear programming (ILP) with a significantly reduced computational cost (50000x less). This results in an average improvement in execution time of 6.10 compared to the EntropyDirected Scheduling (EDS) algorithm. Strengths and Weaknesses: Strengths: Novel and ingenious approach employing GNNs for HLS scheduling. NeuroSchedule achieves performance comparable to the optimal ILP algorithm despite its substantially lower computational overhead. Weaknesses: NeuroSchedules effectiveness is demonstrated only on smallscale kernel benchmarks with execution times typically less than 50 cycles. While NeuroSchedule shows an average improvement over EDS this excludes kernels where no improvement was observed resulting in a relatively modest average gain of 6.10. Additionally it takes about 6x longer to run than EDS. Questions: How would NeuroSchedule perform on more intricate and realistic applications that involve nondeterministic events like DRAM accesses cache conflicts and bank conflicts In Equation (2) why does ALAP include a second definition using ASAP() for vertices not connected to vi Please elaborate on this concept."], "tVbJdvMxK2-": ["Summary Paraphrase: This study introduces a method for optimizing memory allocation in Continual Learning (CL) systems. It simplifies the complex task of memorizing data by determining an optimal ratio for dividing the memory between baseline strategies: random population and classbalanced random population. The proposed algorithm is computationally efficient and outperforms the baselines. Strengths and Weaknesses Paraphrase (SWs): Strengths: Addresses a relevant research topic in CL. Proposes an innovative approach to memory population. Focuses on computationally efficient online CL scenarios. Weaknesses: W: Ambiguous methodology with undefined notations. W: Unclear placement of \"pseudo tasks\" in the training process. W: Lack of justification for choosing the proposed baselines. W: Confusion between the optimality of memory selection and computational feasibility. W: Lack of explanation for the selection of data to store in memory. W: Potentially misleading term \"pseudofuture tasks.\" W: Unclear fairness of comparisons to other methods. W: Implication of multiple model training in each task without clarification. W: Missing information on the time cost of other methods in Table 2.a. Questions: Q: Is \"g\" exclusively the optimization function Q: Does the model optimization use only memory M or does it also help identify memory elements Q: Why is Section 4 classified as an offline study despite the absence of offline assumptions Q: Has the proposal been compared to methods without data augmentation Q: How does the selection of \"aj\" evolve during different tasks", "Paraphrased Summary: This study explores strategies for optimizing the composition of experience replay buffers in continual learning where tasks change over time. The authors consider an offline setting where past experiences can be replayed to evaluate different buffer compositions. Specifically they analyze two existing strategies: ERRes (replay a fixed subset of examples) and ERRingFull (replay all examples). Since the offline setting is impractical the authors introduce \"pseudotasks\" to simulate future changes in data distribution. They evaluate three pseudotasks: permutation rotation and blurring. Empirical results show that permutation outperforms ERRes ERRingFull and a hybrid approach. The authors further investigate the accuracy of pseudotasks and their compatibility with other update methods such as DER. Strengths: Novel concept of using pseudotasks to simulate future learning dynamics Satisfactory empirical analysis showing consistent performance gains in different datasets and settings Clear and wellwritten presentation Weaknesses: Potential concerns about the accuracy of pseudotasks in approximating the true loss function Limited decision space in terms of buffer construction policies Exclusion of incremental class learning a challenging scenario where ER methods are particularly beneficial Questions: Theoretical or empirical justification for the effectiveness of the permutation pseudotask in approximating the global loss Consideration of expanding the policy space for buffer construction", "Paraphrased Statement: Summary: This research develops a process for building dynamic memory structures for episodic recall (ER) by solving a combinatorial optimization problem that seeks to reduce the loss on all encountered tasks. The authors propose \"Global Pseudotask Simulation\" for continuous online learning which involves permuting current tasks. Strengths and Weaknesses: Strengths: Clear explanation of the proposed method. Weaknesses: Lack of explanation as to why data augmentation of the current task can effectively represent future tasks. Overlooked research on data augmentation in the introduction. Unrealistic assumption that tasks have similar difficulties. Intractability of two out of three proposed offline continual learning strategies. Questions: Why is data augmentation of the current task a suitable substitute for actual future tasks Clarify ambiguous statements such as \"each memory Mi after task ti as a variable\" (Line 80)."], "lxsL16YeE2w": ["Paraphrase: This study offers a method for computer vision tasks with extensive output spaces superimposed on the image such as object detection and colorization. Large output spaces make it challenging to model these tasks in an autoregressive framework despite its success in natural language processing. The authors introduce a \"guiding code\" that represents the image overlay using a limited set of discrete tokens (256). This reduces the codes complexity compared to the [H x W x C] overlay. Training is divided into two stages. The model learns the code through a VQVAElike approach in the first stage where an oracle with access to labels summarizes them through the code. The base model reconstructs the labels using the code and input image. In the second stage the model learns to generate guiding codes from images imitating the oracle but only using the image. The paper demonstrates strong performance on various tasks including Panoptic Segmentation (COCO) NYU Depth and ImageNet colorization. Detailed analysis reveals that the autoregressive model is crucial and that an optimal code length exists. Strengths: Novel and innovative approach that unifies various image overlay tasks. Impressive results without substantial taskspecific optimization. Comprehensive analysis with helpful ablation studies. Weaknesses: Very limited discussion on social impact. Questions: Can the codes be visualized to understand the models functionality Could the approach generalize beyond supervised data sources due to its classagnostic nature", "Paraphrase: Summary: The paper introduces UViM a novel approach that combines a discrete code learned from ground truth (GT) with a language model for various vision tasks. The approach involves two steps: learning a code from GT to guide predictions and predicting the code from the input. The effectiveness of UViM is demonstrated through experiments on three dense vision tasks. Strengths: The concept is unique with discrete codes used as a bridge between input and GT rather than solely for generative tasks. The code is informative due to its access to GT enabling effective inference guidance. The discrete bottleneck ensures code compactness and abstraction making it suitable for language model learning. The autoregressive language model is adaptable allowing it to handle varying task structures. The paper is wellwritten and easy to understand. Weaknesses: While the approach appears promising the experiments may not fully support its effectiveness. A comparison with similar methods like PerceiverIO would strengthen the claim of a unified structure for vision tasks. The ablation study on the restricted oracle model may not be fair due to the significantly larger size of the LM compared to the baseline model. More experiments or a fairer comparison are needed to fully assess the methods superiority to baseline models.", "Summary (Paraphrased): This paper introduces UViM a unified vision modeling approach that aims to consolidate the fragmented and specialized architectures used in computer vision tasks. UViM involves a twostep process: Stage 1: Train a feedforward model that learns a discrete bottleneck representation of the target data (e.g. segmentation map or depth map) from the input image. Stage 2: Train a language model (encoderdecoder transformer) to learn the distribution of guiding codes for the target modality. UViM does not require taskspecific architecture modifications and instead relies on a generic input image to generate the target modality. The paper evaluates UViM on three tasks (colorization depth estimation and segmentation) and demonstrates reasonable performance. Strengths: Clear motivation for unifying vision modeling approaches Wellstructured and accessible writing Informative ablation studies Weaknesses: Requires paired training data which can be expensive to acquire Latent representation is modeled using an autoregressive language model which is prone to overfitting Performance may not match that of more specialized models without incorporating additional decoding strategies or architectural choices Mixed Comments: Adding a GAN loss to colorization task training could improve fidelity. Using 2D backbones for guiding code modeling may improve applicability to smaller datasets. Exploring crossdomain dependencies in the language model training could facilitate a \"true\" unification. Clarification is needed on the models ability to handle image resolutions beyond its training data and the influence of pretrained encoder scale. The model could be interpreted as a variant of \"ViTVQGAN\" with taskspecific reconstruction loss and conditioning on input images instead of labels."], "oNWqs_JRcDD": ["Summary This work introduces a novel search space for optimizing algorithms and a search method that combines rejection sampling with Monte Carlo search. The optimizer search is evaluated on tasks such as adversarial training of image classifiers training of Graph Neural Networks (GNNs) and finetuning of BERT models. Strengths: The search space is novel and improves efficiency compared to previous methods. The code and detailed experimental information are provided. The paper is clear and concise. The optimization procedure is evaluated on various deep learning tasks. Weaknesses: The search procedure includes hyperparameters (e.g. rejection sampling thresholds and depth) that require tuning but the selection of their values is not explained. The pipeline is not fully automated as it uses a separate grid search for learning rates for each task. The overall search budget is unclear and comparisons to running SGD on deep nets and using a regular hyperparameter tuner (e.g. ASHA) with the same budget would be helpful. Its uncertain if the search space can discover stateful optimizers like Adam if not explicitly included. The choice of tasks for evaluation (e.g. only five tasks from GLUE and one type of CNN training) raises questions about representativeness.", "Paraphrase: This paper introduces an approach called Learning to Optimize (L2O) that aims to automatically determine the optimal gradient update rule for a given task. Strengths: The paper addresses a significant problem. It provides a comprehensive discussion of related work. Weaknesses: The papers soundness and clarity are questionable. It is unclear when the optimization process occurs (e.g. online or offline). The integration of the MCT approach with the core optimization process is not adequately explained. The paper lacks evidence to support the claim that different tasks have different optimal optimizers (e.g. specific datasets learners and performance differences). It is unclear how the L2O and essential optimization phases are connected. It is not specified whether different update rules are selected based on the MCT approach for each iteration. The concept of \"generability\" is not defined.", "Paraphrase: The paper focuses on finding the best optimization settings for deep learning problems. It defines a set of mathematical operators and frames the optimization problem as searching for a sequence of these operators (like a tree). The authors use Monte Carlo sampling to find the best sequence. The proposed method outperforms other existing methods on a variety of tasks. Strengths and Weaknesses PostRebuttal: The authors have addressed my concerns adequately. I am satisfied with the revisions and increase my score to 6. The paper is wellwritten and clear. The problem and solution are welldefined. The experiments cover a wide range of tasks. Novelty Concerns: While the problem of automatic optimizer search is not new the proposed method is significantly more efficient than previous methods. However the main novelty of the paper seems to be the use of a more efficient data structure and Monte Carlo sampling. Questions: How is the probing vector selected for different tasks What guidelines do the authors have for selecting the set of operators for a general task How is score thresholding defined for different tasks Can the authors provide error bars for the experimental results to show the stability of the found optimizers"], "vfR3gtIFd8Y": ["Paraphrase: The authors present a Bayesian approach to the KarhunenLo\u00e9ve (KL) decomposed kernel BSSANOVA. Strengths and Weaknesses: Weaknesses: Unfamiliar methods for the GP community: A clearer introduction and standard notation are needed. Unclear notation: Clarify why curly theta is used instead of x for input locations. Undefined Bernoulli polynomial: Define the kth Bernoulli polynomial. Unspecified distribution shape: Specify the shape of \\mathcalB1. Confusing notation: Replace \"the full kernel for a system with n inputs is written \\delta \\sim MVN(0 \\Gamma)\" with more precise phrasing. Explain the origin of \\delta and why MVN is used instead of \\mathcalN. Unclear variable definitions: Define \\Gamma \\sigma02 and \\tau02 clearly. Undefined eigenvalue and eigenfunction sources: Provide the sources for the eigenvalues and eigenfunctions. Questions: See \"Strengths and Weaknesses\" section.", "Paraphrased Statement: Summary: The authors propose BSSANOVA a technique for feature selection in GPtype models handling large datasets. BSSANOVA employs a kernel with a basis decomposition that allows for precomputing basis functions reducing training complexity to O(NP) where P is the number of terms used. The novel contribution focuses on selecting nonzero components from this decomposition using forward selection and a natural order of complexity. Coefficients and an information criterion are determined through Gibbs sampling. Strengths and Weaknesses: BSSANOVA offers a new approach to fitting GPtype models to large datasets. The authors should clarify the impact of the proposed Gibbs sampler emphasizing its originality and potential effects on model fitting. If the sampler is not novel the use of an information criterion alone may not justify its acceptance. Questions: What is the worstcase complexity of BSSANOVA Does BSSANOVAs dynamic determination of P guarantee a complexity better than N2 Is the Gibbs sampling algorithm original or previously published Does assumption (8) significantly restrict model expressiveness Are there other feature selection methods for BSSANOVA Are there faster methods such as L1 penalties to find solutions", "Paraphrased Statement: This article presents a method for fast and accurate prediction on large tabular data using Gaussian process modeling. It involves a forward variable selection approach assisted by a Bayesian Information Criterion (BICAIC) within the ANOVA framework. Strengths: 1. The proposed method enables forward variable selection for GP modeling within the ANOVA regime. 2. It has been applied to time series regression with promising results. Weaknesses: 1. The advantages of using forward variable selection are not fully demonstrated. 2. The differences between the proposed method and existing approaches are not adequately discussed. 3. Numerical experiments are limited in showcasing the superiority of the proposed model. Questions: 1. Additive Gaussian processes within the ANOVA formulation for highdimensional inputs have been studied extensively and a recent paper by Lu et al. (2022) should be acknowledged and compared. 2. The use of BICAIC for truncation is not a novel concept. Its uniqueness in this context should be highlighted. 3. The benefits of employing ANOVAtype decomposition for GP should be clarified such as reduced computational costs and identification of important variables. 4. The proposed model should be benchmarked against other ANOVAtype GP competitors to provide more robust conclusions.", "Paraphrased Statement: Summary: This paper introduces a scalable Gaussian process (GP) that employs KarhunenLoeve expansions of kernels. It adopts a fully Bayesian approach using Gibbs sampling to determine kernel parameters and perform forward variable selection. Strengths: Variable selection using GPs is a significant challenge. Weaknesses: The paper lacks clarity and structure. An introductory section is missing which makes it difficult for readers to understand the problem being addressed. Insufficient descriptions and analyses are provided for the proposed method. Literature reviews and comparisons with alternative methods are omitted. Questions: How to scale the mean and covariance matrices in Equation (1011) which necessitate matrix inversion If these parameters are estimated based on 500 data points the results may only be valid locally."], "tIqzLFf3kk": ["Paraphrase: This study provides theoretical and experimental findings on the rank of the Jacobian matrix for deep neural networks. Strengths: The study investigates the impact of network depth on learned representations and geometric structure. It combines theoretical and empirical results. Weaknesses: The connection between theoretical and experimental findings could be stronger. The theoretical result on nonincreasing rank does not indicate a necessary decrease. The results lack specific predictions based on network structure (nonlinearities initialization). The experimental plots do not use a logarithmic scale making it difficult to determine if rank decay is exponential. The influence of pooling layers and width changes on rank decay is not discussed. Question: How challenging do the authors believe it would be to establish quantitative relationships between the predicted rank behavior and network architecture aspects", "Paraphrase: The paper investigates how the rank of a neural networks feature maps changes as the network becomes deeper. By using the abstract definition of rank as the rank of the Jacobian matrix the authors can analyze rank dynamics in general (independent of architecture). This leads to Theorem 1 which states that the rank of a neural network should not increase with increasing depth due to its compositional nature. The authors then examine conditions under which the rank strictly decreases (Theorem 3) and when it converges to specific constants (Theorem 45). They also explore how rank deficiency impacts the relationships between different output classes observing that some ImageNet classes (e.g. hamster) can be predicted with a linear combination of outputs for unrelated classes (e.g. broccoli and mouse trap). They attribute this to the low rank representations caused by very deep networks. Strengths: 1. Generality and Importance of Results: The theoretical results are highly general abstracting away from specific architectures and assuming only the compositional nature of layers. 2. Paper Organization: The paper clearly explains abstract concepts making the theory easy to understand initially. 3. Independence Deficit of Feature Manifolds: Section 5 provides practical applications of the theory and may inspire research on reducing undesirable correlations between different classes. Weaknesses: 1. Inconsistency with Residual Networks: While the authors suggest skip connections as a way to mitigate rank deficiency Figure 1 shows an exponential decay of rank even for architectures that use skip connections like ResNets MLPMixers and Transformers. 2. Presentation Style: The concepts of \"Structural Impetus\" and \"Implicit Impetus\" of rank diminishing could be better explained. Questions: 1. Can the authors provide a way to quantify the \"Structural Impetus\" for different architectures and normalization layers 2. Can their results be used to design architectures initializations and training procedures that preserve rank information better", "Paraphrased Summary: This study explores the relationship between the depth of a neural networks hidden layers and the rank of their representations. The findings reveal that the rank decreases monotonically as the layers become deeper. The paper introduces numerical metrics to quantify the rank focusing on the Jacobian rank (linear approximation of the mapping to hidden layers) and the dimensionality of hidden layer feature spaces. Additionally the study examines the tolerance of the final hidden layer to dimensionality reduction using PCA. By projecting onto fewer eigenvectors the researchers estimate the intrinsic dimensionality of the hidden layer based on the point at which a significant performance drop occurs. Finally the paper explores the possibility of using the logits of one category to classify another. For instance assigning a weight of 0.923 to the \"triumphal arch\" category logit allows accurate prediction of the \"junco\" category. Strengths: Original rank metrics and their justification Wellmotivated need for numerical rank measurement tools Weaknesses: Concept of rank diminution with layer depth is wellestablished Unjustified claims in certain sections (e.g. Gaussian elements in Jacobians) Lack of sufficient detail on most technical aspects (e.g. Theorem 4 and 5) Notation in Section 5 can be confusing Significance: Touches on potentially significant concepts (e.g. symmetryrank connections) Experiment on using categories as predictors could be of interest to the ML community Recommendation: Rephrasing the work to emphasize the significance of Theorem 4 and 5 and providing more clarity in certain sections would enhance its impact.", "Paraphrased Summary This paper shows that as deep neural networks (DNNs) get deeper the rank of the mapping from the input space to intermediate feature layers decreases. The paper provides a theoretical proof of this rank reduction and shows it experimentally across various DNN architectures. Additionally it demonstrates that the number of significant principal components in the final feature layer is much lower than its dimension leading to weak correlations between different categories. Strengths and Weaknesses Strengths: 1. The paper systematically analyzes the evolution of function rank throughout network layers and provides theoretical support for the observed rank reduction. 2. The finding that final feature manifolds exhibit independence deficit is intriguing and sheds light on DNNs vulnerability to adversarial attacks. Weaknesses: 1. The classification dimension estimated using the number of major PCA components is not an accurate measure of feature dimension. As a result the papers main results on rank reduction cannot fully explain the low classification dimension of final feature manifolds. The claim that rank deficiency causes independence deficit is misleading. 2. The definition of function rank and Lemma 1 assume that the Jacobian matrix of neural network functions has a constant rank across the input domain. This assumption is not generally valid and the paper should clarify when and how it applies to DNNs. Questions: 1. What is the \"rank of its Jacobi matrix Jf over its input domain X\" in the definition of function rank How does this relate to the rank of the Jacobian matrix at a specific point x in the input domain 2. Is there a theoretical explanation for the independence deficit observed in final feature manifolds"], "sQiEJLPt1Qh": ["Paraphrase: The authors investigate how ReLUactivated neural networks represent continuous piecewise linear (CPWL) functions. They demonstrate that for CPWL functions with q segments or k linear sections a ReLU network with specific layers maximum width w and hidden neuron count h can represent them. The boundaries in this study are superior to those in prior publications. Furthermore an algorithm for constructing these networks is provided. Strengths and Limitations Originality: Relevant studies are properly cited and the paper presents a high level of innovation. Accuracy: The proofs are thorough and the paper makes a significant contribution to machine learning theory by providing precise bounds on the hidden neurons required to represent CPWL functions. Quality: Technically sound. Clarity: Wellwritten and wellorganized for ease of comprehension. Significance: Important results as highlighted earlier. Questions: Exploring the results extension to more complex network architectures would be worthwhile.", "Paraphrased Statement: Summary: This paper presents tighter bounds for approximating piecewise linear functions using ReLU functions unlike previous work. These bounds depend solely on the number of linear segments making them advantageous when the dimensionality increases faster than the number of segments. The authors employ a novel representation for piecewise linear functions (described in Equation 13). Additionally they provide an algorithm for converting a piecewise linear function into a ReLU network. However this algorithm requires prior knowledge of the number of linear segments and relies on the assumption that the final function is a combination of smaller ReLU functions that approximate the individual linear segments. Strengths and Weaknesses: The paper is wellwritten and provides a thorough analysis of the problem. It effectively motivates the research and places it in context. The authors have done an excellent job in explaining their findings and comparing them to related work. A potential drawback is that Algorithm 1 assumes knowledge of the number of linear segments. The authors could explore ways to relax this assumption. Questions: For improved readability the paper could include a brief outline of what Algorithms 2 3 and 4 accomplish in the main text.", "Paraphrase: Summary: This paper examines neural networks with ReLU (rectified linear unit) activations. It establishes limits on the neuron count required to represent continuous piecewise linear functions (CPWLs) using these networks. These limits are based on the number of linear segments and distinct linear components in the CPWL. The paper demonstrates that quadratic bounds on the number of pieces suffice in contrast to previous work that suggested exponential bounds. Strengths: The paper significantly tightens previous bounds enabling more precise estimates of the requisite neuron count. Weaknesses: The practicality of Algorithm 1 is uncertain. The algorithms runtime and computational complexity are not specified. Clarity: Definition 4 should be revised to clarify that the term \"ReLU network\" refers specifically to \"ReLU Multi Layer Perceptron.\" It is confusing that the Broader Impact section suggests that the paper focuses solely on ReLU networks when in fact it also considers other neural network architectures such as residual networks and densely connected networks. Questions for the Rebuttal: Can Algorithm 1 be applied in practical settings What is the computational complexity of the algorithm Questions: Why is the invariance with respect to the input dimension considered more significant than the quadratic bounds \"q is always not less than k\" should be corrected to \"k \u2264 q.\""], "ocViyp73pFO": ["Paraphrase: Summary: This paper aims to decipher the impact of specific interventions on multiple variables simultaneously while accounting for hidden confounding factors using both observational and interventional data. The authors assume an additive noise model and propose two conditions under which disentanglement is feasible. Theorem 1 applies when interventions occur in a specific causal sequence allowing the isolation of the effect of the last intervention on the outcome. Theorem 2 applies when there are no causal relationships between the interventions. Furthermore the authors present an EM algorithm for disentangling effects. Strengths: The problem of disentangling intervention effects is intriguing. The paper is wellwritten. Weaknesses: The paper focuses on cases where disentanglement is possible rather than providing a complete understanding of when it is not. A general condition for the additive noise model would be valuable. The assumption that covariates are never caused by interventions (Line 121) should be explicitly stated. The formalization of the problem (Line 133) is not comprehensive enough to cover multiple intervention variables. The motivation for disentangling effects of multiple interventions is not sufficiently explained. Lee et al. (2020) have established nonparametric identifiability for arbitrary experiments which includes the manuscripts case as a specialization under specific assumptions. The practical significance of the manuscripts results needs to be clarified. Questions: The examples in the first paragraph (computational advertising and medical treatment) seem unconvincing as motivations for identifying individual intervention effects. In these scenarios the primary goal is often to maximize the overall effect of interventions not to disentangle their specific contributions.", "Paraphrased Statement: Assuming that certain independence conditions hold in a causal diagram this paper establishes identification proofs for nonlinear causal models with Gaussian noise. The proofs show that causal effects can be identified from a combination of observational data and intervention data on multiple variables. The paper also develops an algorithm to estimate these effects. Strengths and Weaknesses: Strengths: Identifies causal effects from limited data (observational and intervention). Considers a challenging problem in multitreatment effect analysis. Weaknesses: Relies on restrictive assumptions that may not hold in practice. Uses a nonrepresentative baseline in experiments. Does not fully address the challenge of disentangling single intervention effects. Questions: 1. Is it still challenging to disentangle single intervention effects under the assumed independence conditions 2. Can traditional methods be used for single treatment analysis in the presence of additive Gaussian noise 3. Does the paper require prior knowledge of the causal equations 4. Why does the paper identify causal effects from intervention distributions when they are already identifiable from observational distributions 5. Could other causal distribution modeling algorithms be considered as baselines 6. Can the causal effect of a single intervention be accurately estimated in the presence of unmeasured confounders", "Paraphrased Summary: This study explores identifying the impact of a specific intervention when multiple interventions are applied simultaneously. It examines cases where identifying the effect of a single intervention is possible including scenarios with nonlinear relationships unobserved confounding factors and interactions between interventions. The paper presents an algorithm that combines observational and interventional data to estimate the causal effect. Strengths: Extends previous research by allowing nonlinear dependencies in causal models. Considers unobserved confounders a significant challenge in causal analysis. Uses observational data to enhance interventional data analysis which is typically more widely available. Weaknesses: The identifiability conditions assume a multivariate normal distribution which may not always apply in practice (e.g. when variables are binary). Questions: Exploration of the necessity and sufficiency of the identifiability conditions would be valuable. Theoretical insights into the proposed algorithms operation would be beneficial. Guidance on verifying the identifiability conditions using realworld data would be helpful.", "Paraphrased Summary: This paper explores methods for separating the effects of individual interventions when multiple interventions are applied simultaneously using observational data. It demonstrates that identifying the effect of single interventions is usually impossible except in specific cases known as symmetric additive noise models (ANMs) where there are no direct causal relationships between treatments. The paper presents a practical approach to estimate the causal effect of single interventions in such models. Strengths and Weaknesses: Strengths: Clearly stated motivation and wellorganized structure Explores a novel research area Provides both theoretical identification criteria and practical estimation methods Weaknesses: Identification criteria rely on strong assumptions: No direct causal connections among treatments for ANMs Identifiability of causal effect for sink treatment variable in general ANMs It would be beneficial to explore weaker identifiability conditions and the identifiability of subsets of interventions. Questions: If only a subset of treatments is affected by hidden confounders what are the implications for identifiability Can identifiability conditions be relaxed in such cases"], "v7SFDrS44Cf": ["Paraphrase: This paper introduces a range of neural models for different types of submodular functions: monotone nonmonotone and monotone approximately submodular. These models can be trained using data that includes either set values or \"perimeterset highvalue subset\" observations. The model for monotone submodular functions combines a linear combination of the function from the previous step with a modular function followed by an increasing concave activation function. This model is extended to monotone approximately submodular functions using an activation function that meets a stricter condition than concavity. For nonmonotone submodular functions the model involves composing a single modular function with a concave activation function which is not necessarily increasing. The authors use neural networks to model these activation functions as opposed to previous work that used fixed functions. Additionally the authors present a greedy sampling method for learning set functions from perimeterset highvalue subset observations. This method is based on a game between an adversarial generator of permutations and the set function learner. The paper provides experimental results showing that their models perform better than other methods under both supervised learning settings. Strengths and Weaknesses: Strengths: Provides a practical approach for learning submodular and approximately submodular functions. Uses neural networks to model concave activation functions allowing for flexibility in learning. Wellwritten and clearly presents results. Weaknesses: Proposed models are limited to a specific form of monotone submodular functions. Nonmonotone submodular model is simple and may not capture complex functions. Empirical comparison with DSF baseline could include more concave function options. Related work is missing. Questions: For a fairer comparison with DSF consider using different concave function options. How are highvalue subsets selected in the Amazon baby registry dataset", "Paraphrase: This study introduces new types of submodular (and approximately submodular) functions that neural networks can parameterize efficiently. These functions are applied to learn submodular functions using examples of sets and value functions. A novel maxmin method is also proposed to aid in the selection of highvalue subsets from a submodular function using perimeterset highvalue subset examples. The min objective is implemented using a GumbelSinkhorn network to promote permutation invariance. Strengths and Weaknesses: Originality: Combines existing research (Deep Submodular Functions GumbelSinkhorn NNs Unconstrained Monotonic NNs) with novel ideas and methodologies. Appropriately references previous related works except for Reference [42] in the Appendix. Quality: Introduces enhancements: Efficient parameterization of (approximately) submodular functions. Maxmin formulation for selecting greedy sequences through adversarial permutations. Presents promising experimental findings but some experimental setup concerns exist. Experimental Concerns: Using a smaller batch size with set transformers could reduce GPU memory consumption. For the Amazon baby registry dataset other baselines such as set transformers could be used to predict the next item even without enforcing submodularity. The paper would benefit from additional performance evaluations such as training time and sample complexity. The appropriateness of NDCG as a metric is unclear as it depends on ordering yet the problem is designed to be permutation invariant. Significance: Presents insightful and potentially impactful concepts for the important problem of learning submodular functions from data. Would benefit from justifying the application of the planted \u03b1submodular set functions with realworld scenarios. Clarity: Wellorganized and generally easy to follow. Theoretical details are clear but experimental evaluations lack specifics. Questions: Dimensions of each FlexSubNet layer and parameter count comparison to baselines. Hyperparameter selection process and range of values considered. Values of \u03b1 for ground truth planted set functions (v) and (vi). Implementation details of the decoupling method in Table 2. Set size and prefix length when using the Amazon baby registry dataset.", "Paraphrase: Summary: This paper investigates ways to train neural networks to learn submodular functions especially in the presence of certain types of function access: 1. Value Oracle: An oracle that given a set S reveals the value of the function f(S). 2. PerimeterSet HighValue Subset Oracle: An oracle that given a set S finds the most valuable subset T within S (i.e.argmaxT\u2286S f(T)). The paper explores a specific class of submodular functions constructed from combinations of modular functions and proposes a neural network architecture that handles input sets permutation invariance and nonpositivity of secondorder derivatives. This approach extends to alphasubmodular and nonmonotone submodular functions. Experiments using synthetic and realworld data (Amazon baby registry dataset) show performance advantages over other neural network methods. Strengths and Weaknesses: The paper tackles an important problem and presents its work clearly. However discussions on the following aspects could enhance its context: Comparison with existing deep sets approaches that offer invariance for setbased datasets. Performance evaluation on more complex function instances. Learnability analysis (e.g. sample size training time) of the proposed functions. Questions: The characterization of alphasubmodular functions in Theorem 2 resembles concepts like onesided smoothness and metasubmodularity. Please clarify the connections. What is the exact objective function in the perimeterset highvalue subset oracle setting Comments: Its important to note that worstcase approximation of submodular functions using poly(n) value oracle queries is challenging. Discussion of theoretical approximation bounds in both the value oracle and demand oracle settings would be helpful."], "v2es9YoukWO": ["Summary Paraphrase: This paper tackles the challenging task of estimating optical flow by enhancing the popular RAFT and GMA architectures. To address the issue of occlusions it suggests employing a larger kernel size in convolutional neural networks. To mitigate the increased computational expense the authors introduce a \"super kernel block\" featuring depthwise and channelwise convolutions. This proposed approach achieves stateoftheart results. Strengths and Weaknesses Paraphrase: Strengths: Clear presentation of the contribution and baseline methods. Novel super kernel block with a straightforward design that can be applied to various networks. Comprehensive ablation study. Weaknesses: The concept of using a larger kernel in optical flow is not entirely novel. The super kernel block is only tested with the GMA architecture leaving questions about its effectiveness in other architectures. Questions Paraphrase: 1. Computational Cost Calculation: Provide a detailed explanation of how the cost is determined. 2. Cost Comparison: Specify the conditions under which SKFlow has a lower computational cost than the conventional kernel method. Explore the relationship between cost and inputoutput channels and kernel size. Quantify the cost ratio for the example provided (kernel size 15). 3. Depthwise Convolution Efficacy: Clarify how depthwise convolution contributes to improved performance when the auxiliary kernel size is 11. 4. Baseline Architecture Selection: Provide the rationale for choosing GMA as the baseline and disclose if other baselines were considered with the super kernel block. 5. Comparison to Traditional Large Kernels: Examine whether a traditional large kernel approach (e.g. kernel size 15) performs comparably to the super kernel block and quantify any difference in computational cost. 6. Occlusion Analysis Extension: Evaluate the occlusion analysis on the Clean and Final passes in addition to the Albedo pass.", "Summary The authors introduce an innovative solution to mitigate occlusion in optical flow estimation. They employ a \"superkernel block\" comprising a large and small kernel to effectively expand the networks receptive field. Empirical results validate that this approach enhances optical flow accuracy while minimizing computational overhead. Strengths Simple yet highly effective architecture for expanding the receptive field of optical flow estimation networks. Achieves stateoftheart accuracy on the Sintel benchmark. Comprehensive experimentation showcasing the methods efficacy across various setups and contrasting it with alternative designs. Broader applicability beyond motion estimation potentially benefiting numerous image processing tasks. Weaknesses None notable. Questions Discuss the advantages and disadvantages of the proposed approach compared to dilated convolution for receptive field expansion. Provide actual runtime measurements in addition to FLOPSMACs considering potential cache locality optimizations or memory issues. Suggestions Elaborate on the performance differences between funnel and conical kernel configurations offering insights into their behavior. Explore potential applications of the superkernel structure in other image processing tasks.", "Summary: This paper introduces a CNNbased approach for estimating optical flow focusing on addressing occluded areas. The method explores the relationship between the networks receptive fields and the ability to recover occluded motion. It utilizes super kernel blocks consisting of small depthwise convolutions followed by largescale convolutions to design the network. Experiments show that the method outperforms existing methods. Strengths: Consistently superior performance on Sintel and KITTI15 datasets establishing a stateoftheart ranking on Sintel. Clear writing with a comprehensive literature review and welldefined methodology. Weaknesses: Lack of Theoretical Justification: The paper lacks a theoretical explanation for the effectiveness of super kernels. Inconsistencies in Experimental Setup: The paper claims to train models on complex data but test them on intrinsic components which may not fully assess effectiveness. Table 1 however shows results using clean and final datasets. Incomplete Ablation Study: The authors should conduct an ablation study to confirm that the performance improvements are not solely due to increased parameters compared to baseline methods. Conflicting Claims: The paper highlights the conical architecture as crucial but the ablation study suggests that super kernel modules are more impactful. This needs further explanation. Typos: Several grammatical errors and missing punctuation in equations and headings. Questions: Performance on KITTI2015: The papers performance on KITTI2015 known for largescale motion and occlusions is comparable to RAFT. The authors should provide more analysis regarding this.", "Paraphrase: Summary: The authors suggest enlarging the networks receptive field to enhance optical flow estimation in obscured areas. The concept is simple but implementation faces obstacles. Small optical flow datasets make training deep networks impractical. Similarly increasing the kernel size directly is problematic due to high memory usage and optimization difficulties with large kernels. To overcome these challenges the authors introduce two architectural tricks. The first uses depthwise separable convolutions while the second adds a 1x1 convolutional layer before the large kernel convolution to simplify optimization. Comprehensive experiments show that this approach surpasses existing methods particularly in obscured regions. An extensive analysis confirms the significance of most architectural decisions for performance. Strengths: The idea of improving flow estimation in obscured regions by expanding the receptive field is solid. The architectural modifications to GMA to enable training with large kernels are wellfounded and effective. The approach excels over previous methods especially in obscured areas. An extensive ablation analysis is provided. The paper is wellwritten. Weaknesses: The novelty lies primarily in the architectural tricks for training GMA with large kernels on limited datasets. The text contains several grammatical errors. The ablation analysis misses some key points including the motivation and evaluation of the GRU to residual network change and the selection of hyperparameters (inference iterations and loss lambda). Questions: Can you provide a rationale and ablation for the GRU to residual network switch How many iterations are used during inference Explain the choice of the lambda parameter in the loss function."], "mjVZw5ADSbX": ["Paraphrased Statement: Summary: This research introduces an innovative algorithm for training generative text models that employ neural networks. Traditional training methods rely on the maximum likelihood estimation (MLE) objective which encounters the \"exposure bias\" problem. To address this the paper proposes a contrastive learning objective. The models encoderdecoder representations are subject to contrastive learning pushing input vectors closer to the intended output and away from incorrect ones. The algorithm incorporates three key strategies: 1. Utilizing modelgenerated hypotheses to form challenging negative samples. 2. Decomposing negative samples into (positivenegative) pairs based on an oracle scoring function (similar to BLEU). 3. Employing an additional reranking step during decoding that minimizes the cosine similarity between the learned contrastive representations. The authors evaluate their approach on 5 diverse text generation tasks (using over 10 datasets) and report an average improvement of 0.52 points across tasks compared to baselines based on automatic token overlap metrics. Strengths: 1. The novel method of integrating contrastive learning into text generation model training tackles the exposure bias problem effectively. 2. The comprehensive experimental setup encompasses 10 datasets across five practical text generation tasks demonstrating consistent improvements over competing baselines. 3. Justifications for design choices and model generations are provided in thorough ablation studies. 4. Limitations are acknowledged including increased training time and strategies for mitigation are discussed. Weaknesses: 1. Despite the extensive experiments human evaluation is lacking. Since the oracle scoring function is used in training it is crucial to assess whether it leads to overfitting to flawed metrics rather than genuinely improving generated text. Human evaluation particularly for tasks with different inputoutput spaces is recommended. 2. The approach is primarily tested on small transformer models (60M220M parameters) with PEGASUSlarge (560M) as an exception. It remains unclear whether the method scales effectively to larger models like T5large T53B and T511B. Scaling may impact both performance and efficiency. Minor Considerations: 1. The paper could benefit from discussing related works that use contrastive learning for text generation such as [8 9]. 2. Citing early works with similar ideas such as [1012] would enhance the context. 3. A discussion of GPU memory requirements for the approach and the impact of negative sample size and beam size during inference would be helpful.", "Paraphrased Summary: This paper introduces CoNT a new framework for generating text using neural networks. It includes three key contributions: 1. Additional Contrastive Examples: Using the beam search results of the model as additional contrastive examples for training. 2. Npairs Contrastive Loss: Constructing a contrastive loss that measures the relative difference between the ground truth and the constructed examples. 3. Learned Similarity Function: Modifying the hypothesis score by adding a learned similarity function. Experiments on various tasks demonstrate the effectiveness of the proposed methods. Strengths and Weaknesses: Strengths: Intuitive and effective approaches. Use of model predictions for generating highquality negative examples. Generalized approach applicable to multiple tasks. Comprehensive experiments. Clear and wellwritten presentation. Weaknesses: Results may not be entirely reliable as the cited baseline scores for text summarization are lower than expected. Misleading baselines labeled as \"T5xxx.\" Mathematical errors in certain equations: Eq 4: Likelihood defined as a product of decoding sequences (incorrect). Use of alpha and 1alpha to balance terms with different ranges. Questionable diversity of the beam search algorithm. Confusing notation and arrows in Figure 2. Questions: Have alternative sampling methods been considered for greater diversity in beam search Can the arrows and symbols in Figure 2 be explained more clearly", "Paraphrased Statement: This study introduces a sequencelevel contrastive loss to enhance text generation quality. The loss utilizes selfgenerated negative samples and pairwise ranking relationships. Additionally the proposed method employs a ranking score for result reranking during inference. Extensive experiments conducted on five text generation tasks demonstrate the effectiveness of the approach. Strengths: The proposed method addresses the issue of exposure bias through selfgenerated negatives. The loss function considers pairwise ranking relations to overcome limitations of previous contrastive learning approaches. The study provides detailed experiments with ablation studies and analysis. Weaknesses: The improvement over a basic contrastive loss (Naive CL) may appear incremental despite the use of wellestablished techniques applied to text generation. Questions: The study lacks a thorough explanation of the computation of sequencelevel representations (zx and zy). It is unclear how the contrastive loss at the sequencelevel is combined with the tokenlevel generative loss. The authors are asked whether they explored variations in computing sequencelevel representations such as using different pooling methods or more advanced designs.", "Paraphrased Summary: The CONT framework proposed by the authors enhances contrastive learning for neural text generation by addressing three key factors: Contrastive Example Creation: CONT develops a novel method for generating contrasting examples. Contrastive Loss Selection: CONT utilizes an optimized contrastive loss function. Decoding Strategy: CONT employs an improved decoding strategy that incorporates similarity constraints. Extensive experiments across five text generation tasks and ten benchmark datasets demonstrate the superiority of CONT compared to previous contrastive learningbased methods. Strengths: Accessible and effective framework for improving text generation performance. Addresses key limitations of contrastive learning. Proven effectiveness through comprehensive experimental evaluation. Weaknesses: The use of selfgenerated texts as negative samples and similarity constraints in decoding are not entirely novel concepts. The overall innovation of the method may be limited due to these similarities to existing techniques."], "vMQ1V_z0TxU": ["Paraphrase: Summary: This paper introduces a method for detecting outofdistribution (OOD) samples in data using hierarchical variational autoencoders (VAEs). The approach employs the likelihood ratio principle with several modifications. The paper demonstrates that mitigating posterior collapse in VAEs enhances OOD detection using the likelihood ratio. It also proposes a modified training objective that emphasizes the mutual information between the data and higherlevel latent variables preventing higherlevel latent variables from collapsing into prior distributions. Additionally a novel score is introduced eliminating the need for hyperparameter tuning in the likelihood ratio method. Strengths: 1. The method is wellmotivated demonstrating the impact of posterior collapse on OOD detection in hierarchical VAEs and proposing techniques to mitigate it. 2. The proposed method is straightforward yet effective alleviating posterior collapse by upweighting mutual information between data and latent variables. 3. The paper provides extensive and convincing experimental results. Limitations: 1. The paper lacks detailed information on the previous likelihood ratio method which would make it more selfcontained. 2. Section 3.4 of the paper requires clarification and some explanations from the appendix should be included in the main text. Questions: 1. How does the modified training objective affect VAE performance in other aspects such as reconstruction quality and sample diversity 2. Can posterior collapse be alleviated using simpler approaches such as downweighting the KL term or employing a warmup scheme and how would that impact OOD detection 3. In Section 3.4 why not automatically select the value of \"k\" with the largest R ratio instead of using a fixed value 4. In Figure 2 why do the partial reconstructions for p(xz3) and p(xz4) exhibit no variation despite the assumption that z3 has collapsed into the prior", "Paraphrase: This study explores the relationship between outofdistribution (OOD) performance in hierarchical variational autoencoders (HVAEs) and the phenomenon of \"posterior collapse.\" HVAEs have been observed to perform better on OOD samples when higherlevel latent variables in the hierarchy exhibit posterior collapse where they align with their conditional prior and become less informative about the input sample. To address this issue the study proposes promoting increased mutual information between the input and higherlevel latents. Additionally it develops an OOD score based on a weighted difference of the loglikelihood ratio between successive latent variable slices. Experiments show improved OOD accuracy on two benchmark datasets. The proposed OOD criterion proves to be more stable than layerspecific scores which require the layer index as a hyperparameter.", "Paraphrase: Summary: This paper introduces a hierarchical variational autoencoder (HVAE) for outofdistribution (OOD) detection. The authors explore the issue of posterior collapse in HVAE models and propose a solution to alleviate it by maximizing the mutual information between input and latent representations. An adaptive likelihood ratiobased metric is also presented for HVAE models to identify OOD samples. The approach is evaluated on benchmark datasets demonstrating superior performance compared to alternative methods. Strengths: 1. Thorough analysis of posterior collapse in HVAE models with an innovative solution to increase inputlatent mutual information. 2. Proposal of an adaptive likelihood ratiobased metric for OOD detection utilizing all HVAE layers. 3. Wellwritten and accessible presentation. 4. Comprehensive experimental validation of approach components. Weaknesses: 1. Comparison with alternative posterior collapse prevention approaches is lacking such as bidirectional inference and oversmoothing loss function modification. 2. Table 1 references to approaches are omitted. 3. Limited experimental comparisons. The authors should consider evaluating against recent stateoftheart methods such as Energybased OutofDistribution Detection On the Importance of Gradients for Detecting Distributional Shifts in the Wild and iDECODe: InDistribution Equivariance for Conformal OutofDistribution Detection. Questions: The authors should address the comments in the \"Weaknesses\" section particularly regarding the novelty of their proposed approach in relation to existing posterior collapse prevention methods.", "Summary: This papers goal is to address the issue of \"posterior collapse\" in hierarchical variational autoencoders (HVAE). It presents a theoretical explanation for why this occurs during training based on the evidence lower bound (ELBO) lower bound. It also explores how posterior collapse affects the HVAE models outofdistribution (OOD) detection performance. To address these issues the paper proposes enhancing the connection between the input and its multilayer stochastic latent representations using an informative HVAE training objective. Additionally it introduces an adaptive likelihood ratio score for detecting OOD inputs that improves the separation between indistribution and OOD inputs irrespective of the higherlevel latent layer representations used. Strengths: Addresses a major issue in HVAE models that can limit their OOD detection capabilities. Presents a theoretical explanation for posterior collapse and its impact on OOD detection. Proposes an informative HVAE training objective to mitigate posterior collapse. Introduces an adaptive likelihood ratio score for OOD detection that enhances separation. Extensive experimental evaluation comparing the proposed method with various OOD detection baselines. Weaknesses: Technical details are occasionally difficult to follow and lack sufficient discussion. The proposed adaptive likelihood ratio scores formulation lacks a clear explanation. The experiments use a limited number of indistribution datasets for evaluation. Several typos and grammatical issues could be addressed with proofreading. Questions and Comments: The paper claims to be unsupervised but its unclear whether utilizing class labels (if available) could enhance the proposed method. The paper omits several important prior works on OOD detection including nongenerative modelbased methods. An indepth discussion of the loglikelihood ratio score and its approximation is required. The derivation of the adaptive loglikelihood score requires further clarification and a more principled explanation. The FPR80 metric is not truly thresholdindependent. The absence of AUPRC results in Table 1 and incomplete results for some baselines in Table 1 raise concerns. Providing area under the ROC curves in Figure 3 would enhance the interpretation. Additional results including an ablation study of the adaptive loglikelihood score could strengthen the evaluation."], "ok-SB1kz67Z": ["Paraphrased Statement: The paper proposes a twostep approach for neural network (NN) classification to enhance its robustness. The first network is conventional while the second network utilizes the gradients of the first network as inputs. Despite the authors claims of novelty a similar twostage process was introduced in [1] in 2020. While there are technical differences between the two methods the cited work also includes aspects of explanation and uses the Kahneman reference. However the authors have not discussed [1] in their paper. While the theoretical concepts are sound there is a concern about the correctness of the proof. In the transition from Equation (15) to (16) the authors use an incorrect transformation: log(a0b0a1b1) is not equivalent to log(a0a1) log(b0b1). The papers results are limited to a single dataset (CIFAR10C) and one architecture. This raises doubts about the generalizability of the method especially since [1] showed improvements on the CIFAR10 test set using a different approach. The evaluation process lacks sufficient variety in datasets and architectures making it difficult to assess the true impact and effectiveness of the proposed method.", "Paraphrased Statement: This research introduces a new concept: using a simplistic threelayer Multilayer Perceptron (MLP) in place of traditional fully connected layers. This \"reflection stage\" trains the network to reconsider its predictions by considering alternative options. Strengths and Weaknesses: The proposed Introspection Networks offer a clever method of transforming N categories into N potential introspective queries and responses. However the networks robustness as discussed in Section 4 is not sufficiently explained. Questions: 1. How does the concept of extracting Introspective Features differ from conventional approaches using fully connected layers softmax and crossentropy loss Why is the proposed method more effective 2. Can you provide an explanation of the \\mathcalH(\\cot) function used in equation 7 3. What is the impact on computational complexity by replacing the final network layer with an Introspection Network", "Summary: The researchers propose a twostep method for neural network decisionmaking that incorporates introspection. The initial step involves processing input data through a standard feedforward network to generate logarithmic class probabilities (logits). Introspective features for each class are then derived from changes in network parameters (excluding the final layer) when the input is labeled with that class. A separate introspective network is then trained to align class predictions based on these introspective features with ground truth labels and the predictions from the first network. The researchers provide theoretical explanations for these features their sparsity and efficient extraction methods. Experiments demonstrate that introspective learning enhances neural network performance in terms of robustness calibration and active learning. Strengths: Novelty and Theoretical Grounding: Utilizing logit gradients to encode class similarity information and employing introspection to improve neural network performance are novel approaches with strong theoretical support. Improved Performance: Experiments show that introspection improves neural network robustness against distorted images and enhances calibration. Complementary to Existing Techniques: Introspection can be used alongside existing robustness techniques to improve model performance. Weaknesses: Specific Definition of Introspection: The paper uses a narrow definition of introspection focused on discriminative learning which may not fully capture the broader concept of introspection in human cognition. Calibration Error: The low calibration error of the introspective model may potentially be influenced by the use of an MLP architecture which has been shown to improve calibration through vectormatrix scaling. Questions: 1. Shouldnt a network with N classes have N2 possible introspective questions (considering pairs of classes i and j where i \u2260 j) 2. There is a typo in line 245: \"Robustness\" should be \"Robustness\".", "Summary Paraphrase: This paper proposes a new classification method with two stages. In the first stage data is processed through a regular neural network. The second stage called \"Introspection\" calculates features based on gradients from the penultimate layer of the first stage and uses these features to predict the correct class via another neural network. Specifically it creates features by combining gradients calculated for each possible class in the dataset. This approach achieves stateoftheart results on the Cifar10C and Cifar10 Cure datasets. Strengths: Novel approach to improving classification accuracy Theoretical justification for using gradientbased features Demonstrated effectiveness on Cifar10related datasets Weaknesses: Limited applicability to classification tasks with small class sizes and manageable penultimate layer dimensions Lack of results on datasets beyond Flowers 102 Caltech 101 and Cub200 Computationally expensive Introspection stage"], "m8YYs8nJF3T": ["Paraphrased Statement: Summary This paper introduces the Sliced Wasserstein process a stochastic process based on the empirical Wasserstein distance between subsets of empirical probability measures. The process converges to a Gaussian limit as shown by the stated uniform distributional limit theorem. Empirical results support this convergence. Strengths Builds on and adapts mathematical techniques from a previous study (reference [20]). Presents seemingly correct results (though proofs were not thoroughly reviewed). Neutral Numerical results are convincing but more examples could be provided possibly demonstrating the theorys validity (or limitations) for unbounded cases. Weaknesses Clarity could be improved with more space dedicated to defining mathematical objects and a brief sketch of the main theorems proof. Despite the checklist indicating otherwise the code is not included (it is reportedly provided in a footnote on page 6). The analysis is restricted to compactly supported probability measures P and Q with no explanation of the difficulties associated with unbounded cases. The submitted documents contain visible labels for equations and sections. Numerous typos exist (e.g. the supplemental material title does not match the papers title). The practical applications of the results are not explicitly stated. Questions Regarding the SW process it is unclear why the Gn process is defined based on an expectation that differs from the actual Wasserstein distance. The authors insights on this are requested. Expanding the analysis beyond the compact case at least to subGaussian measures is an area of interest. It is suggested that the authors utilize the remaining space to include proof sketches or conduct additional experiments to demonstrate the theorys applicability to unbounded cases. Depending on the quality and comprehensiveness of the responses the reviewers score may be adjusted accordingly.", "Summary Paraphrase: The authors provide a unified method to determine the limiting distribution of statistics based on empirical Wasserstein distances between projections of empirical distributions onto onedimensional subspaces. They use empirical process theory and the differentiability properties of certain functions. Strengths and Weaknesses Paraphrase: Strengths: Presents a comprehensive approach for proving distributional limits for distance measures using sliced distributions and 1D Wasserstein distance. Weaknesses: Limited novelty as similar results have been established in other works. Restrictive assumption that the support of projected measures is connected for all directions on the unit sphere limiting its applicability. The methodology does not significantly differ from previous approaches for classical optimal transport. Results are constrained to specific types of probability measures with compact support. The paper lacks a conclusion and future research directions. Additional Comments: Minor formatting issues and typos are present. The paper should clarify that the method is applicable to absolutely continuous probability measures only. The benefits and implications of generalizing beyond the Wasserstein distance are not fully explored. The appendix considers a case not covered in the main section and it would be helpful to explain its relation to the overall results.", "Paraphrase: Summary: Wasserstein distance is a powerful measure of similarity between probability distributions but it can be computationally expensive. Sliced Wasserstein distance (SWD) offers an alternative by approximating the Wasserstein distance using 1D projections of the data. This paper proves that under certain conditions the empirical SWD distance follows a Gaussian process distribution on the unit sphere. This result holds for various forms of SWD including vanilla SWD MaxSliced and Trimmed SWD. Strengths and Weaknesses: The paper is clear and presents a unified framework for proving the central limit theorem for SWD. The result is valuable for statistical applications involving SWD. Questions: Can the central limit theorem for discrete SWD be directly derived from that for Wasserstein distance Is the limit process for SWD uniform with respect to the number of random direction projections Can the convergence rate to zero be determined from Theorem 2.4", "Paraphrase: Summary: The paper establishes a Donskertype theorem for the Projected Wasserstein distance in the l\\infty norm over the projected directions ball. Using this theorem it derives limit theorems for popular Sliced Wasserstein distances. Simulations support the theoretical findings. Strengths: The results are significant for Optimal Transport theory and its applications. The paper is wellpresented and accessible. The proofs are rigorous and detailed. Questions: 1. Could the results be extended to distributions with arbitrary supports or are the compact support assumptions essential 2. It would be valuable to have simulations for the limiting distribution of the empirical process of MaxSliced Wasserstein distances even in low dimensions to provide intuition about this nonGaussian limit."], "zb-xfApk4ZK": ["Paraphrased Statement: This research introduces an approach for Markov Chain Monte Carlo (MCMC) sampling from unnormalized probability distributions. The method blends ideas from global proposal sampling and local MCMC updates. The approach leverages a global proposal to enhance exploration across different modes and a local kernel (such as MALA or HMC) with a Metropolis correction to refine sampling within each mode. This strategy combines the strengths of both global and local approaches. The paper provides theoretical proof that the method is geometrically ergodic even when importance weights are unbounded. This contrasts with other methods like iSIR that require finite weights. An adaptive variant of the method is also proposed which adapts the global proposal as a normalizing flow during sampling. The authors provide conditions for preserving ergodicity in the adaptive variant. Experiments demonstrate the methods effectiveness in toy examples standard benchmark distributions and highdimensional energybased model generative adversarial networks (EBMGANs). It generally performs well outperforming most baselines except for NUTs in high dimensions. Strengths: Simple and intuitive approach Strong theoretical foundations Clear and wellpresented results Scales effectively to highdimensional problems Weaknesses: Lack of comparison to similar methods in the EBM space Experimental results on image generation could be improved with more quantitative measures Importance of mixture training proposal distribution initialization and practical initialization procedures could be further explored Questions: Importance of using both forward and reverse KL for adaptive proposal training Sensitivity of the method to proposal distribution initialization Practical guidelines for proposal distribution initialization", "Paraphrase: The authors propose a hybrid MCMC method combining a global MCMC kernel (iterate sampling importance resampling) with a local MCMC kernel (MALA) for both exploration and exploitation. To enhance the proposal distribution in the global kernel they employ normalizing flow to learn a proposal closer to the target distribution. They demonstrate the theoretical and practical effectiveness of their proposed method. Strengths: Clear motivation and easytofollow presentation Technically sound methodology Theoretical and empirical analysis Weaknesses: Unclear originality compared to previous localglobal MCMC work Empirical results lack comparisons with previous localglobal methods limiting the demonstration of significant improvement Questions: Figure 1: Can the authors provide results comparing local MCMC (MALA) on a multimodal distribution where their method (Ex2MCMC) shows advantages Theorem 2: How does the mixing rate of Ex2MCMC compare to that of iSIR and MALA and under what conditions does Ex2MCMC offer faster sampling Related Work: A clearer explanation of how their method differs from previous localglobal MCMC methods including [1] would be helpful. Experiments: Including previous localglobal methods as baselines in the empirical evaluations would enhance the analysis.", "Paraphrase: This work introduces a new Markov chain Monte Carlo (MCMC) algorithm that combines two existing kernels: a \"local\" update (such as Metropolisadjusted Langevin) followed by a \"global\" iSIR update. The iSIR update proposes multiple candidate states from a distribution that is independent of the current chain state and accepts one of them as the new state. The paper proves that the algorithm converges geometrically under certain conditions. It also investigates adapting the iSIR proposal distribution to the target distribution using a normalizing flow and gradient steps to minimize a divergence measure. The authors show that under additional assumptions the adaptation process converges to zero indicating that it does not adversely affect the algorithms performance. Strengths: 1. Strong theoretical guarantees including for the adaptive algorithm. 2. Clear presentation. 3. Demonstrated effectiveness in low to moderate dimensions. Weaknesses: 1. The algorithm is not particularly novel as it combines existing kernels. 2. The analysis does not provide specific insights into the convergence rate compared to using the iSIR kernel alone. 3. The paper does not fully acknowledge the limitations of the algorithm in targeting multimodal distributions in higher dimensions. Additional Notes: The manuscript contains some typos and formatting issues. The authors choice to submit to this journal is not explicitly explained but it is possible that they sought a broader audience for the theoretical contributions."], "rQAJmrLmGC6": ["Paraphrased Summary: This research proposes a novel framework for identifying sounding objects in an audiovisual environment without relying on specialized sound localization data. It utilizes a combination of vision text and audio modalities to locate objects effectively. The framework consists of two streams: Localization Stream: Based on the DETR architecture it identifies objects from related text descriptions. Alignment Stream: Establishes a common feature space for audio and text enabling knowledge transfer between these modalities. This approach allows the framework to locate objects related to specific sounds without additional annotations. Experimental evaluations demonstrate its efficacy. Strengths: Novel knowledge transfer mechanism among visual text and audio modalities. Direct bounding box regression instead of heatmaps eliminating the need for additional human annotations. Wellwritten paper with clear technical details. Adaptive sampling strategy that optimizes knowledge transfer between text and audio. Promising results on singlesource sounding object localization. Weaknesses: Lack of experiments on multisource scenarios which would validate the effectiveness of the alignment stream in more complex environments. Difficulty in handling multibox scenarios further analysis is needed to determine the cause of this issue. Questions: Will the performance of the localization stream improve if text embeddings are provided as queries Can the framework be extended to multisource scenes with minor modifications", "Paraphrased Statement: This research investigates zeroresource sound object localization. The proposed Twostream Universal Referring localization Network (TURN) consists of localization and alignment streams. An adaptive sampling strategy addresses the domain gap. The models effectiveness is evaluated through ablation experiments. Strengths: TURN combines image grounding and audio retrieval domains for zeroresource sound object localization. A classifier estimates the domain gap of each sample guiding data sampling for training. Ample ablation experiments demonstrate the efficacy of each model component. Failure case analysis illustrates the models limitations. Weaknesses: The inconvenience of previous models generating heatmaps is not sufficiently explained. A method proposed in [12] for generating bounding boxes is not compared to TURN. The text alignment process between the two domains during training is not described. Adaptive sampling addresses domain gap but irrelevant text descriptions may coexist within batches. TURN can only output a single bounding box limiting its performance in mixed sound scenes. Questions: 1. Is each element in set V \\hatvi or vi 2. Why does equation (5) use a crossmodality approach in the intermediate domain 3. Is it fair to compare the IOU calculated using heatmaps and bounding boxes to the IOU calculated using bounding boxes against ground truth", "Paraphrased Statement: Summary: The proposed model trains a localization system for objects in audio recordings even without direct pairing of audio and visual data. It leverages knowledge from different datasets with paired audio and text or image and text to bridge the gap. The text representations from different datasets are aligned enabling the audio representation to be matched with the corresponding visual embedding. Codebooks learned through selfsupervised training of vqwav2vec are used to project text and audio representations into a shared space. The visual and text representations from the codebooks are combined and fed into a patch encoder for bounding box prediction. Strengths: Integrates ideas from different modalities to learn multimodal models with limited resources. Addresses sample weighting through an objective that balances domain similarity and discriminability. Surpasses supervised methods in performance on the VGSS dataset. Explores the choice of source and intermediate domains with an ablation study. Weaknesses: Low IOU score on MUSICSolo dataset remains unexplained. Doesnt include experiments combining annotated and unannotated data. Question: Request for results that combine annotated and unannotated data in the training process.", "Paraphrased Statement: Summary To pinpoint the source of a specific sound (sound localization) this paper presents the Twostream Universal Referring localization Network (TURN). It consists of two streams: the localization stream helps locate the target object in the image while the alignment stream aligns the audio signal with the textaudio semantic space. This mechanism employs retrieved codebooks from the textaudio space to activate the object detector in the localization stream. An optimized sampling method assists the training process. TURNs performance is comparable to existing methods. Strengths and Weaknesses Strengths: Utilizes a promising approach for sound localization without requiring specific training data. Clear organization and sufficient references. Weaknesses: Limited technical innovation as TURN primarily combines existing methods (DETR VQVAE and VQWAV2VEC) with an additional sampling strategy. Complicates the original task making it harder to obtain bounding box predictions. Requires additional data while ignoring available datasets for sound localization. Underwhelming performance gains. Questions: Why was the sound localization task treated as a combination of image grounding and audio retrieval This approach appears complex for the intended task. Previous studies suggest that heatmaps from sound localization can produce corresponding bounding boxes. Is the TURN model evaluated based on boxlevel predictions Clarify the concept of \"patch\" in line 168. Ensure consistency between loss terms in section 3.5 and notations in Figure 2. Include more comprehensive references especially related methods such as [A]."], "yhlMZ3iR7Pu": ["Paraphrased Statement: Summary This paper uses a diffusion model as a prior for modeling and emphasizes downstream task applications. Strengths Provides a formal method for estimating the posterior distribution using a pretrained diffusion model. Highlights the advantage of using a fixed diffusion SDE eliminating the need to retrain the score network and enables solving downstream tasks without model retraining. Offers a general framework for applying pretrained diffusion models to various tasks. Demonstrates the novelty of the approach by solving image segmentation and the traveling salesman problem using DDPM for the first time. Weaknesses The framework itself is not novel and similar approaches have been used previously. The abstract level of the framework limits its impact as it covers multiple tasks in one paper. It would be more effective to dedicate separate manuscripts to each task for greater clarity and focus.", "Summary: The authors propose a method to infer highdimensional data that fits both a prior distribution and an auxiliary constraint. The constraint is differentiable and can come from various sources. The authors show that their method performs well on tasks such as image segmentation and combinatorial search. Strengths: The idea is innovative as it allows the generation of samples that satisfy both a classifier and a prior. The proposed algorithm is novel and has not been seen before in the literature. The paper is wellwritten and easy to follow. Weaknesses: Quantitative evaluations are missing so its difficult to assess how the proposed method compares to existing approaches. The authors spend a significant amount of time discussing the traveling salesman problem which is not the focus of the paper. The authors present results on the FFHQ dataset but its unclear how their method compares to other popular GuidedDiffusion methods. Additional Questions: How does the proposed model outperform previous approaches What is the difference between the proposed method and popular GuidedDiffusion methods", "Paraphrase of Statement: This research presents a new approach to using pretrained denoising diffusion probabilistic models (DDPMs) as priors in various inference tasks. The authors achieved this by inverting the DDPM process taking input x and outputting a random noise \\epsilon. By comparing the difference between this inverse mapping and (typically Gaussian) noise the framework ensures that x remains within the DDPMs defined range. Furthermore the authors have created an optimization problem with a loss function comprised of the sum of loglikelihood (auxiliary constraint) and l2 loss. By iteratively searching for a solution that both satisfies the DDPMs range and fulfills the constraint the framework transforms DDPM into a \"plugandplay\" prior applicable to a range of inference tasks. Strengths: The authors propose a novel \"plugandplay\" priors scheme using DDPMs. Unlike existing priors based on GANs the proposed framework inversely maps the image x into the latent space making the iterate the actual image rather than a latent variable. The paper demonstrates the frameworks versatility across various tasks including segmentation face generation and traveling salesman problems. Weaknesses: The papers readability could be improved. The detailed derivations leading to Equation 711 are lacking. The novelty of the work is not explicitly stated. The frameworks name is confusing as \"plugandplay priors\" is a wellknown concept in image restoration. Questions: 1. Can you provide more detailed derivations for Equation 711 particularly regarding the reparameterization trick 2. In line 109 why would you minimize F with respect to \u03b7 (and \u03c8 if desired) instead of directly minimizing xt 3. In the segmentation task how were the DDPM networks finetuned Does \"scratch\" imply that they were pretrained", "Paraphrased Statement: The authors introduce a method for using denoising diffusion models (DDPMs) as priors for conditional sampling. Their approach incorporates auxiliary constraints (e.g. pretrained classifiers) into the reversed diffusion process. The benefits of this technique are demonstrated through applications in image generation semantic segmentation and the Traveling Salesman Problem (TSP). Experimental results indicate the practical applicability of the proposed model. Strengths and Weaknesses: The paper leverages existing research and proposes a useful method. It introduces a conditional generation model that can incorporate various pretrained DDPMs as priors. Numerical experiments support the efficacy of the approach for both imaging and nonimaging tasks. The writing of the methods section could be clearer. The novelty and contributions of the method are not fully explained due to limited literature review and comparisons. Questions: 1. Can you clarify the definitions and values of \u03b7 and \u03c8 and how they relate to the forward process in [Jonathan. et al 2020] Explain the meaning of q(xt1 xt \u03b7 \u03c8) in Eq. 9 and its relationship to q(xt1 xt x0). 2. How does this approach differ from existing scorebased conditional models like [R1 R2] that also incorporate pretrained priors 3. Explain the key difference between GANbased priors and diffusion model priors. Does the predefined time schedule have an impact on generation performance 4. Provide numerical comparisons to baseline methods for conditional image generation to demonstrate the performance of the proposed model compared to stateoftheart conditional generative models."], "q0XxMcbaZH9": ["Paraphrased Summary: The paper aims to improve the distinctiveness and similarity invariance of queries in querybased instance segmentation techniques. The method employs techniques from contrastive learning to train queries that can better discriminate between instances. This approach has consistently outperformed baseline methods such as CondInst SOLOv2 and Mask2Former. Strengths: Significant improvements over various methods including CondInst SOLOv2 SOTA and Mask2Former. Introduces a novel approach for improving querybased instance segmentation by enhancing the discriminatory capabilities of queries. Weaknesses: The paper contains numerous equations that can be challenging to understand. Lack of information on iteration time and GPU memory consumption in Table 3 (b) and (c). KNet is not mentioned or compared despite its superior speedaccuracy tradeoff. The description should be updated to clarify that the method improves kernelbased methods rather than querybased methods. The augmentation techniques such as color jittering and blur are not fully explored. Questions: 1. Why are color jittering and blur not suitable for use in contrastive learning for this task 2. Can the proposed method be extended to panoptic segmentation tasks or is it limited to instance segmentation", "Paraphrased Statement: This study suggests two approaches to enhance querybased instance segmentation networks. One method forces the segmenters to derive instancespecific queries by retrieving matching instances from the training data. The other method involves applying geometric transformations and promoting network predictions to be invariant to these transformations which is expected to enhance the robustness of instancequery matching. Experimental findings demonstrate the effectiveness of both techniques in improving instance segmentation performance. Strengths: Simple and practical concept Clear and readable writing Impressive experimental outcomes Weaknesses: Lack of clarity in the connection between the two proposed contributions Equivariancebased augmentation while generally beneficial for augmentation may not be particularly tailored to instance segmentation or the querybased segmentation framework and thus may not warrant recognition as a key contribution of this paper. Specific details regarding the crossentropy loss and focal loss functions used for \\mathcalLinter\\mask are missing. Questions: Further explanation of the relationship between the proposed techniques is requested. Justification for the inclusion of equivariancebased augmentation as a unique contribution of the paper is sought. The precise mathematical formulations for the crossentropy and focal loss functions used in \\mathcalLinter\\mask are requested.", "Paraphrase: This paper presents a novel training approach that utilizes query embedding learning. It introduces the principles of datasetlevel uniqueness and transform invariance. The approach has shown promising performance on benchmark datasets. Strengths: The framework offers a significant shift moving from training within scenes to query embedding differentiation between scenes. The inclusion of equivariance is justified and the results demonstrate its effectiveness. Weaknesses: The evaluation lacks depth as it is only conducted on the COCO dataset. Questions: In line 158 uniqueness and robustness are mentioned as crucial qualities. How do these align with the statement in the abstract (line 5) If equivariance is calculated on feature representations how does it differ from conventional concepts like invariance", "Paraphrased Statement: This research introduces a unique training approach for instance segmentation that can enhance any model. It examines two key concepts: 1. Datasetlevel Uniqueness: Each image in the training set contains distinct objects. 2. Transformation Equivariance: The model should respond consistently to transformations applied to the input image. By integrating these concepts with stateoftheart (SOTA) models the authors achieve a substantial improvement of 23 AP on the COCO dataset. The training process only slows down by 5 compared to standard training with no noticeable speed degradation at the instance level. Strengths: Novelty and motivation of the training paradigm. Clear explanations and orthogonal additions that facilitate experimentation. Contrary to expectations the training cost is only slightly higher than baseline. Wellstructured experimental section that compares with SOTA methods and demonstrates the effectiveness of the proposed approach. Robustness of the newly introduced hyperparameters. Impressive performance gains including SOTA results. Weaknesses: Evaluation is limited to a single dataset (COCO). Minor discrepancy in reported Mask2Former results. Questions: The authors should address the weaknesses mentioned above particularly the need for evaluation on additional datasets. They could further explain the discrepancy in Mask2Former results."], "m6DJxSuKuqF": ["Paraphrased Statement: Summary: The paper introduces the KeyPointGuided model by Relation preservation (KPGRL) which integrates annotation information into popular optimal transport (OT) models. The model shows strong performance when combined with OT models for heterogeneous domain adaptation. Strengths and Weaknesses: Strengths: Clear and wellillustrated presentation of KPGRLs advantages over traditional OT models. Weaknesses: Limited theoretical understanding of KPGRL. Lack of empirical evidence to demonstrate KPGRLs reliability as a divergence metric. Incomplete mathematical proofs for certain scenarios (e.g. equivalence with OT distance when points share the same ground space). Lack of comparison with alternative models such as the fused GW model which also incorporate prior information. Questions: How does KPGRL perform without the G component (i.e. using only Lkpg(\\pi) \\langle M \\pi \\rangleF) What is the impact of the choice of similarity measure (e.g. JS divergence) on KPGRLs performance Why is JS divergence preferred over other measures (e.g. KL divergence L2 L1 Wasserstein distance) How does KPGRL compare to the fused GW model mentioned in the paper What is the range of \\alpha in equations 11 and 12 Is it tuned during training Could the alignment in Figure 4 be made more visible How is the target data transformed in Experiment 5.2 for the GW model Could the Appendix provide more details on the barycentric mapping in [48]", "Paraphrase: Summary: This study addresses the challenge of incorrect matches in optimal transport models using annotated keypoints. The authors first restrict the problem by imposing a maskbased constraint to maintain the alignment of keypoint pairs. Next they introduce a novel cost matrix to preserve the relationship between each data point and the keypoints. Their method dubbed KeyPointGuided model by ReLation preservation (KPGRL) is compatible with the Sinkhorns algorithm and can be applied even in incomparable spaces. Additionally they extend KPGRL to the partial optimal transport setting. Experiments in heterogeneous domain adaptation demonstrate the effectiveness of their approach in both closed and openset scenarios. Strengths and Weaknesses: While the maskbased constraint is not entirely original KPGRL is a novel and promising solution. The theoretical results are largely extensions of previous work. KPGRL outperforms baseline methods in heterogeneous domain adaptation and ablation studies attest to its stability under varying hyperparameters. The paper is wellwritten and includes helpful illustrations. Questions: 1. Related Work: Clarify the differences between KPGRL and Masked Optimal Transport (Masked OT). Conduct a comparison with Masked OT and Masked GW to demonstrate KPGRLs advantages. 2. Relation Score: Explain the rationale behind the specific formulation of the relation score in Equation 7. Provide more details on the choice of relation score function. 3. Cost Function: Conduct an ablation study or provide an explanation for the choice of the JensenShannon distance as the cost function. 4. Reproducibility: Provide missing experimental details such as: Kernel SVM details Values of \u03b1 for KPGbased methods in Section 5.1 Hyperparameter values in Section 5.2 5. Experiments: Extend the sensitivity analysis of \u03b5 to a wider range of values. Clarify whether the common label set of the target domain includes or equals that of the source domain in the openset HDA scenario. Provide more information on the sensitivity of KPGRL to hyperparameter choices. Minor Issues: Missing entry in Figure 2 Outdated line numbers in the Checklist", "Paraphrased Statement: This paper presents a method to optimize transportation plans for domain adaptation by incorporating prior knowledge regarding \"keypoints\" into the transportation matrix. The relationships between keypoints guide the definition of a \"guiding\" matrix used as the cost matrix for the transportation problem. Variations include a GreedyWasserstein (GW) and a partially formulated approach. The paper focuses on the Heterogeneous Domain Adaptation (HDA) scenario where some target distribution points are labeled. Strengths include its originality and good performance in HDA. However there are weaknesses: Insufficient comparison to similar OT variants like hierarchical OT and TLB. Experimental setup may not fully reflect the proposed method. The choice and number of keypoints in the source domain may impact results. Its unclear how the method extends to deep methods using minibatches. Questions: Why should one prefer this method over hierarchical OT or TLB Can it be extended to cases with different numbers of keypoints in source and target distributions", "Summary Paraphrase: The paper proposes a semisupervised Optimal Transport (OT) formulation for tackling heterogeneous domain adaptation in image classification. Unlike supervised OT where all images are labeled this method only requires limited labeling of key points in both source and target domains. The model leverages these key points to create a \"key pointguided\" OT formulation incorporating a masked product and a relation matrix. The model is appended to the OT formulation as a weighted regularization term. The authors also extend the model to handle partial OT problems where only partial matches between domains are available. Experiments on the Office31 dataset demonstrate the proposed models superior performance compared to existing methods. Strengths and Weaknesses Paraphrase: Strengths: The formulation introduced is novel and has not been previously proposed. Theorem 1 provides a solid theoretical basis for the model. The model can be extended to partial OT problems ensuring its applicability in scenarios with incomplete matchings. Weaknesses: The memory requirements of the new formulation are substantial due to additional pairwise discrepancies which were not discussed by the authors. The distinction between \"key pointguided\" OT and semisupervised OT remains unclear as the authors have not thoroughly explained the motivation and implementation of their model. Questions: What is the fundamental difference between the proposed \"key pointguided\" OT and the semisupervised OT approach in [9] Why is it necessary for matched key points to have a onetoone mapping in the OT solution How was the weight of the regularization term (0.1 x maxIk\\CIks) determined Is it tunable Where do the Rk ius and Gkld(Rks Rlt) terms come from and why were these specific discrepancies chosen What is the computational cost and memory usage of the model during the Office31 experiments"], "qf12cWVSksq": ["Paraphrased Statement: This paper proposes the iFormer architecture which captures high and lowfrequency visual information separately for enhanced performance. The inception mixer module (i) divides input data along channels and employs parallel convolutionmaxpoolingselfattention paths to gather diverse frequencies. A frequency ramp structure adjusts channel partitioning across layers. Strengths: Clear motivation and straightforward implementation. Improved performance with reduced computational cost. Weaknesses: Lack of novelty and simplicity of implementation. Absence of results in large configurations. Comparison only under intermediate settings which are less relevant to the community. Missing results with a combination of attention and depthwise separable convolution (DWConv). Questions: Address the weaknesses listed above. Provide results in both small (TINY) and large (LARGE) configurations to demonstrate the potential of the method. Include results with the combination of attention and DWConv. Updated Verdict: Based on considering other reviewers comments and the authors responses the potential impact on the research community warrants an increase in the review score.", "Paraphrased Statement: Summary: This paper introduces the Inception Transformer (iFormer) which enhances the lowpass filtering capabilities of traditional transformers. The iFormer consists of: An Inception mixer that combines parallel convolutionmaxpooling and selfattention paths capturing high and lowfrequency information respectively. A frequency ramp structure that gradually shifts focus between high and lowfrequency information across layers. The iFormer achieves stateoftheart performance on image classification object detection and segmentation tasks. Strengths: It innovatively enhances transformer capabilities for highfrequency information capture. The Inception mixer and frequency ramp structure are intuitive and effective as supported by ablation studies. Despite its parallel structure technical differences in the Inception mixer (channel splitting and fusion module) provide clear benefits. The iFormer delivers high performance without compromising model efficiency. Weaknesses: A detailed analysis of high and lowfrequency information capture using Fourier analysis would be beneficial. An indepth exploration of the Inception mixers low and highfrequency roles (e.g. qualitative visualization) is lacking. The channel splitting scheme in the lowfrequency mixer requires further analysis and ablations. Questions: How much computational savings are achieved by using average pooling and upsampling in the lowfrequency mixer Does this affect attention performance What is the exact channel ratio adjustment policy across layers How do different policies compare How does the feature fusion module perform compared to direct concatenation in [47]", "Paraphrased Statement: This paper introduces a new series of visual transformer models known as Inception Transformer. Drawing inspiration from the frequency characteristics of local (convolution and pooling) and longrange (selfattention) operations a multibranch Inception Mixer block is proposed to capture both high and lowfrequency image information. To blend operations a channel splitting mechanism is employed at each stage. The authors hypothesize that lower network layers favor local highfrequency patterns while higher layers prefer global lowfrequency patterns. To cater to this a frequency ramp structure is implemented increasing attention dimensions and diminishing convolution dimensions as the network deepens. Extensive evaluations in image classification object detection instance segmentation and semantic segmentation demonstrate impressive performance. Strengths: Clear motivation and comprehensible writing Wellfounded incorporation of local and longrange operations Simple and efficient structure Promising empirical results Weaknesses: Limited discussion of prior multibranch network architectures Misleading ablation study as it adds components sequentially instead of removing or modifying them Lack of explanation on the channel ratio increase in the frequency ramp strategy Computational complexity and performance comparison to vanilla ViT for downsampled and upsampled selfattention are not addressed Questions: 1. Can the authors provide more detailed comparisons with previous multibranch network structures highlighting the unique aspects of Inception Transformer 2. How is the channel ratio CC determined in the frequency ramp strategy An ablation study on channel number selection would be beneficial. 3. What are the computational implications and performance impacts of downsampling and upsampling the selfattention mechanism compared to the standard ViT architecture", "Paraphrased Statement: Summary: This paper introduces the iFormer architecture designed to capture both high and lowfrequency information in visual data. The architecture addresses the tendency of Vision Transformers (ViTs) to primarily focus on low frequencies. It employs an Inception Token Mixer with high and lowfrequency mixers to extract the corresponding frequency components. Additionally a Frequency Ramp Structure is proposed to balance the different frequencies across layers. Experimental results in classification detection and segmentation tasks demonstrate the effectiveness of the iFormer. Strengths and Weaknesses: Strengths: Wellmotivated design based on Fourier analysis. Extensive experiments showing superior performance over existing Transformer architectures. Effective use of Inception Token Mixer and Frequency Ramp Structure. Clear and wellwritten presentation. Weaknesses: Symbols in Figure 3 need to be defined. The design of the Frequency Ramp Structure seems straightforward exploring alternative designs would be beneficial. Insights on potential future work related to this research would be valuable. Questions: Have you considered alternative balancing strategies in the Frequency Ramp Structure Can you provide any insights on possible future directions for research based on this work Are you planning to release the code for your research"], "pOEN7dDC0d": ["Paraphrase: This study examines whether the Hypothesis of Articulatory Setting (HAS) applies to emergent languages which are created among virtual agents in a simulated setting. HAS is a universal principle in natural languages that states that articulation boundaries can be deduced from phonemic statistical data independently of word meaning. The studys experiments yield mixed results: emergent languages exhibit some prerequisites for HAS but the boundaries they produce may not always be semantically meaningful. Strengths: Novelty: The study investigates a unique topic. Quality: The experiments are wellconceived and the background information is thoroughly explained. Clarity: The writing is clear and accessible. Significance: The study contributes to our understanding of differences between natural and emergent languages. Weaknesses: Scope: The study focuses solely on emergent languages in one specific setting (Lewiss signaling game). Narrowness: The scope of the study is limited but this does not detract from its merit. Questions: Can the results be generalized to emergent languages in other settings besides Lewiss signaling game Are there alternative methods to determine the semantic significance of word boundaries in emergent languages", "Paraphrase: The Harris hypothesis (HAS) proposes that word boundaries in languages can be identified by a nonmeaningful counting method. However whether this applies to emergent agent languages is unknown. This study investigates this by using HAS for unsupervised word segmentation in emergent languages. The results show that emergent languages partially follow HAS: (1) conditional entropy decreases and (2) branching entropy exhibits alternating increases and decreases. However the boundaries determined by HAS in emergent languages are not always meaningful unlike in natural languages. These findings indicate similarities and differences between natural and emergent languages in terms of word segmentation. Strengths: Clear and understandable research question Solid experiments that address the question Weaknesses: Limited experiment settings for emergent language emergence Lack of clarity on the generalizability of results Questions: Are the results applicable to all emergent languages What are the limitations of the work in terms of scope and methodology", "Paraphrased Summary: This research proposes a novel framework for assessing the similarity between artificial emergent languages and natural languages. It examines the predictions of the Harris articulation scheme (HAS) on messages generated by computer agents trained through a communication game. A simple word segmentation algorithm based on HAS assumptions was developed. Predictions regarding vocabulary structure and utterance patterns were made based on the games nature. Strengths and Weaknesses: The study demonstrates a gap between the structure of emergent languages and natural language but its significance is uncertain due to weak negative results. Improved significance could have been achieved through a more noticeable gap with human language positive results or indepth qualitative analysis. The inclusion of excessive details such as repeated figures and results hinders clarity. Questions: 1. What is the baseline comparison for these experiments Since segmentation algorithms based solely on entropy are inadequate for language induction the expected results even for emergent languages with a natural language structure are unclear. How would the analysis outputs differ if humans engaged in the communication game 2. What is the rationale behind the ZLA result A length penalty for the speaker offers an explanation but this justification may not apply to all systems."], "nyBJcnhjAoy": ["Paraphrased Summary: The authors use prototyping to understand how deep reinforcement learning agents work. Prototyping involves grouping agent behaviors and game states into \"prototypical states.\" ProtoX a selfsupervised neural network is used to achieve this. Prototypes are trained to represent similar game states based on visual and action similarities. ProtoX predicts the agents action based on the prototypical states it identifies. The method is demonstrated on Pong Seaquest and Super Mario Bros. levels. Results show that ProtoX accurately groups similar states and identifies transition points where the agent changes actions. In Seaquest prototypes match scenarios that activate them. Bad behaviors in the scenarios are identified as having similar actions to the correct actions. Strengths and Weaknesses: Strengths: Clear and concise explanation of the method. Introduction of a novel approach to interpretability in deep RL. Weaknesses: ProtoX only provides a global picture of the state not featurelevel insights. ProtoX groups states based on visual and action similarity not on the underlying decisionmaking process. The demonstration in Section 4.4 (Diagnosing Bad Behaviors) does not fully explain the reasons for agent mistakes. Questions: Can ProtoX be augmented to identify featurelevel prototypes to improve its explanations", "Paraphrased Statement: Summary: This paper presents a technique for explaining the behavior of blackbox reinforcement learning (RL) agents. The method involves identifying \"prototype\" states and determining which prototype corresponds to the agents current state. Strengths and Weaknesses: Strengths: The proposed method is believed to be a valuable tool for the RL community. The paper is wellorganized and visually informative. The method includes evaluations to measure effectiveness and fidelity. An easytoreproduce Jupyter notebook is provided. Weaknesses: It can be difficult to understand how the prototypes are automatically generated. Certain sections of the text are unclear. The qualitative analysis of the prototypes could be improved. The visualization of the most similar frames could be enhanced for clarity. Some figures require additional information within their captions. A summary of the papers contributions would be beneficial. Questions: What data is referred to as proprietary Were any tests conducted to assess the methods sensitivity to random seeds How sensitive is the method to the loss coefficients", "Summary: This research proposes a method to interpret reinforcement learning (RL) policies using prototypes. They build a model that maps states to an embedding space where states leading to similar actions and occurring at similar times are close together. This embedding is then used to extract prototypes that represent key states in the RL policy. By calculating the similarity between an input state and the prototypes the model predicts the action to be taken and explains why using the most relevant prototype. Strengths: Provides intuitive prototypes for interpreting RL agent behavior. Potentially useful for debugging and understanding RL agents. Weaknesses: Limited novelty as the approach combines existing techniques from ProtoPNet and Time Contrastive Networks. Poor performance of the GAIfO model raises concerns about implementation or baseline selection. Recommendations: Clarify the relationship between the proposed method and ProtoPNet. Consider automating the merging of prototypes. Add a competitive baseline to the \"sensitivity to flip points\" experiment. Rephrase the caption in Figure 8 to avoid anthropomorphizing the RL policy. Investigate improving GAIfOs performance or adding a more competitive baseline."], "wmwgLEPjL9": ["Paraphrased Statement: This study examines the multitask learning (MTL) framework where a single model is trained for multiple tasks. Most MTL approaches oppose the basic strategy of combining individual task losses (unitary scalarization) as the training goal. Instead they typically modify the gradients of each task to determine a unified update direction. However these more intricate techniques can lengthen training time and complicate implementation. The authors propose that by incorporating appropriate regularization and carefully adjusting the unitary scalarization baseline it can achieve performance comparable to current MTL methods. The paper raises legitimate concerns about evaluation and experimental practices in MTL research. Strengths: Clear and wellstructured paper. Introduces a novel perspective on MTL methods as regularization techniques. Conducts a thorough evaluation of recent MTL approaches and highlights evaluation and experimental issues. Provides a unique analysis of wellknown MTL methods from a regularization standpoint. Weaknesses: Most MTL methods use gradients of specific features rather than complete task gradients leading to minimal runtime increase. The justification for presenting MTL methods as regularizers seems weak and incomplete. The experimental setup lacks crucial comparisons and evaluations on more meaningful benchmarks: Fails to compare with recent strong MTL approaches (e.g. references [123]) that outperform unitary scalarization. Results from [12] indicate better performance on the Cityscapes benchmark for all methods suggesting potential underoptimization. Relative task improvement should be used as an evaluation metric as employed in prior MTL research. For Cityscapes the early stopping criteria and metric combination are unclear. A suitable approach is to use relative task improvement as the stopping criterion. The supervised learning benchmarks are inadequate: MultiMNIST is a simple dataset with only two tasks. CelebA exhibits rapid overfitting limiting its usefulness for MTL optimization analysis. Cityscapes has only two tasks that have been shown to complement each other. Experiments involving multiple potential conflicting tasks are lacking. The authors should consider incorporating more relevant and challenging MTL benchmarks such as QM9 (11 tasks) and NYUv2 (3 tasks). Questions: See above weaknesses section.", "Paraphrase: Summary: Researchers tested different optimizers made for multitask learning especially those using gradients for each task. They tested them on image classification and multitask RL benchmarks and found that a basic method (summing losses and adjusting regularization) performed well on its own. More advanced optimizers didnt offer much benefit compared to their extra computational cost. The researchers believe these advanced methods may essentially act like a form of early stopping. Strengths: Comprehensive evaluation of multitask learning methods with clear details Unexpected empirical results Potentially valuable reference experiments for multitask learning Wellwritten (except for the theoretical section) Weaknesses: Lack of theoretical rigor (authors dont fully explain why more complex methods might act like early stopping) Limited hyperparameter tuning (needed since this paper focuses on comparing methods) No new methods introduced (though not essential for a strong paper) Questions: The theoretical section lacks convincing arguments. Specifically is the hypothesis that advanced methods have more convergence points and thus act like early stopping valid Could the researchers provide empirical evidence to support their theoretical claims such as by showing that advanced methods converge to points different from those achieved by the basic method How can these results be translated into practical improvements for multitask learning models Could the authors provide a clearer explanation of their results and recommendations for the multitask learning community Figure 5 should be improved for accessibility especially for colorblind readers.", "Summary (Paraphrased) This work investigates the effectiveness of multitask optimization methods (STMOs) by reevaluating their performance against a previously neglected baseline approach called \"unitary scalarization\" (simply summing objectives). The authors argue that unitary scalarization has not received fair treatment in past STMO studies. Through extensive experiments on four multitask datasets the authors compare the distribution of performance across four common STMOs two random weighting schemes and unitary scalarization. Their results indicate that no single method consistently outperforms unitary scalarization across all tasks. While some STMOs may show advantages on specific tasks unitary scalarization remains within their standard deviation range. Moreover the benefits of STMOs are inconsistent across datasets. Additionally STMOs significantly increase training time with limited performance gains. The authors propose that most STMOs may be acting as regularizers which contributes to their observed performance. They also provide a theoretical analysis that connects STMOs to convergence sets suggesting that their ability to converge to diverse solutions resembles early stopping. Overall this work challenges the prevailing focus on conflictmitigating methods in STMOs and suggests that their benefits may largely stem from implicit regularization effects. These effects can be achieved more efficiently through unitary scalarization. Strengths Comprehensive experiments demonstrate that unitary scalarization can perform comparably to STMOs. Extensive dataset coverage provides a valuable reference for future MTL research. Theoretical analysis connects STMOs to regularization effects. Weaknesses Disconnect between theoretical analysis and empirical results. Limited contributions primarily consisting of experimental comparisons. Lack of focus on the connection between regularization and STMOs in the main text. Questions Whether the slower convergence of STMOs in CelebA is due to early stopping or intrinsic properties of the algorithms. Whether continued training would result in overfitting similar to unitary scalarization for STMOs."], "zVglD2W0EAS": ["Paraphrased Summary: This paper introduces a new drug recommendation model based on causal inference. This model considers three main aspects of electronic health records (EHRs). Two modeling techniques DrugReca and DrugReck are presented to manage patients with multiple visits and capture their health history more accurately. Additionally a \"controllable drugdrug interaction (DDI)\" concept with 2SAT is introduced to ensure safety by balancing recommendations and avoiding harmful interactions. The proposed models are wellstructured and extensive testing verifies their effectiveness. Strengths: 1. The model leverages causal inference to determine drug recommendations offering causal insights. 2. DrugReca and DrugReck capture patient health history through multiple visits. 3. Realworld data (MIMICIII and MIMICIV) is used to evaluate recommendation quality and DDI rates. Weaknesses: 1. Symptoms are extracted from discharge notes which may introduce confusion as medications may have already been administered. 2. Limited information is provided on how previous methods address limitations without considering the three key factors. 3. Clarification is needed on how experiments are conducted on MIMICIVrelated symptoms since MIMICIV may not have clinical notes. Questions: 1. How are experiments conducted on MIMICIVrelated symptoms without clinical notes 2. How do existing drug recommendation methods handle the three key factors considered in this study", "Original Statement This research focuses on developing innovative clinical models for drug recommendations. The authors introduce DrugRec a recommendation model based on causal inference. The model uses causal graphical models to address recommendation bias and deconfounders. Additionally drugdrug interactions (DDIs) are modeled as propositional satisfiability (SAT) problems which enhances coordination in medication recommendations. The model is tested using realworld clinical data. Strengths Utilizes realworld data from MIMIC. Weaknesses Assumptions related to time intervals between visits refill frequency dosage adjustments medication corrections and the target population (inpatient vs. outpatient) are not addressed. The use of ICD codes and natural language processing for symptom extraction is not fully described. Questions How does the model handle different time intervals between visits especially when using historical data Are frequent refills and dosage adjustments considered in the recommendations How are medication corrections addressed to ensure data quality What assumptions are made regarding the target population (inpatient vs. outpatient) How are ICD codes utilized and what rules are applied How are symptoms extracted from clinical notes and what types of notes are used", "Paraphrased Summary: The study aims to design a drug recommendation system using electronic health record (EHR) data. The proposed system addresses biased recommendations caused by unobserved confounders through a causal inference model with frontdoor adjustment. To account for multiple patient visits the model extends the causal graph to connect consecutive visits. The model also tackles drugdrug interactions (DDIs) by solving a propositional satisfiability (SAT) problem. Evaluation on MIMIC datasets demonstrates the models effectiveness. Strengths and Weaknesses: Strengths: Addresses both hidden confounders and DDIs Models the sequential nature of EHR data Weaknesses: Incremental novelty combining existing methods Could benefit from a simulation example to further assess hidden confounder handling Questions: What is the distribution of patient visit counts in the datasets How does the parameter \"k\" (number of recent visits) affect recommendations when stratifying the datasets by visit count Typos: L299: \"addtion\" should be \"addition\" L319: \"methrics\" should be \"metrics\"", "Summary The paper introduces DrugRec a causal model for drug recommendation. DrugRec leverages historical and clinical data to address three issues: reducing bias optimizing patient data usage and mitigating drugdrug interactions. Two DrugRec variants were evaluated against baseline models using the MIMIC dataset and they achieved superior performance. Strengths Clear and wellwritten paper. DrugRec is evaluated against relevant baselines. Justification provided for design choices. Ablation study demonstrates the significance of historical data and modeling techniques. Weaknesses and Questions The Related Works section lacks a detailed comparison to existing models. Why is M not connected to S in Figure 1 Why were k visits used in DrugReck instead of a fixed time period How were ICD coding negation scenarios handled Error analysis and discussion of model limitations are missing. The authors are asked to expand on potential limitations of DrugRec."], "r6_zHM2POTd": ["Paraphrased Statement: This paper proposes a theoretical framework that links the level of balance in a network (tightness of balance) to the efficiency of information encoding. It shows that tighter network balance enhances information transmission. The theoretical analysis is supported by numerical simulations. Additionally the paper demonstrates that learning the connection weights in a network that encodes onedimensional inputs increases the mean strength of local connections within the network. When generalized to multidimensional inputs the network forms subnetworks with strong inhibitory connections within each subnetwork and weak connections between subnetworks. Strengths: Significant theoretical advancement in understanding balanced networks. Wellconnected to existing literature. Applicability to various transfer functions. Clear and accessible explanations of mathematical concepts and derivations. Detailed numerical simulations with provided parameters for easy reproducibility. Provides ideas for future work on lowrank networks and biophysical constraints. Weaknesses: Ambiguity in the definition of \"depth of balance\" and its connection to the theoretical balance parameter. Misleading claim about \"feedforward excitatory projections and local recurrent inhibition\" being a general architecture for information encoding. Lack of experimental predictions and connection to experimental work. No provided code for reproducibility. Questions: Applicability of the \"depth of balance\" concept in other studies. Comparison of binary and spiking networks. Rationale for moving Figure 6 to the main text. Potential for investigating unsupervised learning rules for weight learning. Explanation of the \"finitesize effects\" mentioned in Section 8. Supplementary figure to support claims in the last paragraph of the discussion. Minor Issues: Typos and errors in equations and text. Incorrect formulation in Section 6. Usage of \"e.i.\" instead of \"i.e.\" twice. Missing parentheses and spaces in certain text sections. Inaccuracies in figure legends.", "Paraphrase: Summary: This research examines how nerve cell groups process timevarying signals using a combination of mathematical modeling simulations and neural network techniques. The key findings are: 1. A nonstationary dynamic meanfield theory suggests that a balanced interplay between neural activity (excitation) and inhibition in recurrent networks facilitates more accurate encoding of highfrequency signals. 2. Information theory analysis reveals that a balanced excitationinhibition (EI) ratio maximizes the correlation between the input and output signals. These theoretical findings are supported by simulations. Additionally autoencoder networks trained to encode timevarying signals exhibit strong local inhibition implying a stringent EI balance. Strengths and Weaknesses: Strengths: The comprehensive study addresses the research question from multiple angles including theoretical models and empirical simulations. The analytical and autoencoder approaches are both innovative. The paper offers clear explanations of why and how a balanced EI ratio improves signal encoding and transmission. Weaknesses: The network mainly uses nerve cell groups to convey input signals rather than engaging in significant information processing. The specific neural representations formed by these groups are unclear. The networks primary role appears to be noise reduction and stability maintenance which raises questions about its broader implications. Questions: Can the principles described in the paper extend to networks that perform more complex computations How do the findings relate to realworld scenarios and potential applications", "Paraphrase: Summary: The authors present a theory describing how recurrent neural networks (RNNs) can learn from nonstationary inputs. Their framework involves strong inhibitory connections that balance incoming information enhancing the communication (mutual information) between inputs and outputs. Contributions: A theory combining RNN equations meanfield approximation and information theory Demonstration of the connection between network balance and mutual information Numerical simulations to support the theory Results showing that training an RNN on nonstationary data leads to increasingly balanced networks Strengths: Interdisciplinary approach combining multiple theories Integration of theory simulation and conventional NN training Weaknesses: Inconsistent mathematical notation throughout the paper Structure that mixes theory and numerical results Typographical errors Unsupported mathematical claims Questions: Clarification on the relationship between mutual information and balance (e.g. the formula) Justification for extending Equation (10) to nonstationary processes Explanation of why the results are expected to apply beyond the specific Gaussian setup Reference to Figures 1 ABC and 3 D in the main text Suggestions for improving notational consistency Typographical Errors: \"how\" (line 196) \"I(t) is grows by b\" (line 207) \"e.i.\" (lines 225 252) \"sqrt2\" (line 226) \"mean squared loss\" (line 232) \"y \\hat x\" (line 233)", "Paraphrase of Statement The authors present an extended Dynamic Mean Field Theory (DMFT) for nonsteady inputs. They employ this theory to investigate how the equilibrium between excitation and inhibition in recurrent networks allows for efficient coding of timevarying inputs. Their findings suggest that networks with a tighter balance can track highfrequency input fluctuations more effectively by reducing their integration time constant. The authors validate their theory by training a simple RNN to replicate an input signal using backpropagation through time (BPTT). They find that their theoretical predictions align with the empirical results. Strengths and Weaknesses Strengths: Technical innovation: Extension of DMFT to nonstationary inputs Potential relevance to neuroscience: Maintaining external inputs in neural circuits Easier calculation of metrics like approximate mutual information Weaknesses: Limited empirical evaluation: The \"copy\" task is basic and can be solved by standard RNNs without advanced training techniques. Lack of demonstration of nontrivial applications: The authors claim the \"copy\" task is challenging but do not provide justification. Inadequate scope: Most neural circuits perform more complex computations than simple transmission. Incomplete clarification of certain aspects of the model (e.g. definition of external input scaling of synaptic interactions) Poor writing and grammatical errors Additional Questions Why is the timevarying input signal (x(t)) not included in the definition of I(t) How does the parameter \"b\" affect the balance of excitation and inhibition as stated in L88 but not reflected in the equations Can the authors elaborate on the scaling of synaptic interactions with network size (L29) Can the authors provide a dedicated \"Contributions\" section to highlight the uniqueness and technical novelty of their work Can the authors reformulate L96 to avoid referencing an undefined equation Can the authors provide additional data in Figure 3 such as the eigenvalue spectrum as a function of training epochs"], "wxWTyJtiJZ": ["Summary: The study presents methods for maximizing revenue in online shopping scenarios where customers purchase multiple products based on unknown behavior. These methods employ classic and contextaware techniques. Strengths and Weaknesses: The studys main limitation lies in its minimal comparison with existing research which undermines its key contributions. Previous research has addressed multiple purchases in offline settings (e.g. \"MultiPurchase Behavior Modeling and Optimization\" by Tulabandhula et al). In online noncontextual settings similar approaches have been used (e.g. \"Thompson sampling for combinatorial semibandits\" by Wang et al). A previous study by Cao et al. presents a model nearly identical to the one used in this study with comparable results for optimal solutions and regret bounds. Questions: What specific challenges are encountered during algorithm construction and analysis How do the methods developed in this study differ from existing approaches", "Paraphrased Statement: This paper explores the problem of ranking products for customers who can purchase multiple items and may exit after a certain number of purchases. Technical Challenges and Contributions Compared to Liang et al. (2021) this paper focuses on a more general setting that combines elements from several previous studies. It considers both known and unknown customer parameters in both noncontextual (shared parameters) and contextual (personalized parameters) environments. Known Parameters: This paper characterizes the optimal ranking policy in this more comprehensive setting. Unknown Parameters: It proposes two UCBlike algorithms with O(\\sqrtT) regrets for the noncontextual and contextual settings respectively. These contributions extend the understanding of product ranking algorithms in a more realistic setting that captures multiple purchase behaviors and customer departure tendencies.", "Paraphrased Summary: The authors investigate the challenge of ranking products realistically considering situations where customers may have multiple purchases continue exploring products after purchasing and have limited attention and budget constraints. They derive the optimal ranking policy and develop UCBlike algorithms with demonstrable regret bounds of \\tildeO(\\sqrtT) in both contextual and noncontextual settings. Additionally simulations and realworld data sets are used to validate the proposed approaches. Strengths and Weaknesses: The paper is wellwritten and accessible. It clearly presents related work and offers a comprehensive theoretical analysis with promising results. However there is a concern regarding the practical time complexity of the algorithms which is given as O(N Log N). Questions: 1. How can Algorithms 1 and 2 be implemented efficiently while maintaining a similar time complexity 2. Consider parallelization options to reduce computation time. Minor Typos: 1. Line 53: \"314 rankling\" should be \"314 ranking\" 2. Line 210: \"management\" is likely a typo", "Paraphrased Statement: Summary: This paper examines a purchasing scenario where customers can select multiple products within a fixed financial limit and time frame. The authors present a ranking strategy for the online retailer considering customers precise behaviors. Most prior research assumes that customers only buy a single product and end their browsing immediately. This study introduces a more realistic setting where customers browse sequentially and make purchases based on a utility threshold and a purchase budget. They propose UCBstyle algorithms for both noncontextual and contextual settings and show that these algorithms achieve nearoptimal regret bounds of \\tildeO(\\sqrtT). Strengths: Realworld setting with multiple purchases budgets and limited browsing time. Optimal ranking policy structure presented in Theorem 4.1. Comprehensive study covering both noncontextual and contextual versions. Wellwritten presentation. Weaknesses: Assumption of geometric distributions for viewing time and purchase budget. Uniform product cost assumption. Questions: 1. Purchase Probability Independence: The purchase probability defined in Eq. (1) assumes independence between products. Please clarify this assumption and discuss its practical relevance. 2. Technical Contribution of the Algorithm: Highlight the major technical contributions of the proposed online algorithm. Explain how it differs from existing work and which aspects of the singlepurchase problem are not applicable here. 3. Distribution Assumptions: Discuss the reliance of Theorem 4.1 on the geometric distribution assumptions. Can the results be extended to other distributions 4. Regret Bound Tightness: Prove that the regret upper bounds specified in Theorems 5 and 6 are tight."], "v6CqBssIwYw": ["Paraphrase: Summary: The authors present IBUG a simple method for generating probabilistic forecasts using gradientboosted regression trees. Despite its simplicity IBUG has proven to be effective as demonstrated by the authors extensive computational experiments. The accompanying code is userfriendly and welldesigned. The papers main contribution lies in its practical solution and its highquality implementation rather than its methodological novelty. Strengths and Weaknesses: Originality: IBUG lacks originality as it resembles the \"Affinity Score\" in Davies and Ghahramani (2014). However the authors have made significant engineering contributions that cannot be overlooked. Quality: The paper showcases impressive engineering efforts evaluating IBUG across multiple datasets performance metrics base models and output distributions. The code is wellwritten and easy to use. Clarity: The paper is wellwritten and understandable. The code is also clear and userfriendly. Significance: IBUG has potential significance due to its extensive engineering contributions. The opensource code under the Apache 2.0 license enables wider accessibility. However the authors should compare IBUG to more intricate probabilistic prediction approaches and clarify the sensitivity of their method to the parameter k (number of nearest neighbors).", "Paraphrased Statement: The paper introduces a novel method based on gradient boosting trees to estimate the conditional distribution P(yx) for regression tasks. The approach involves identifying similar instances that share leaves in multiple trees. These similar instances form a neighborhood that can be used to estimate the variance or full conditional distribution P(yx). To address potential errors in variance estimation a calibration procedure is proposed. The method was evaluated on 20 tabular datasets using two performance measures NLL and CRPS. Strengths: Clear and accessible writing style Addresses an important issue of uncertainty estimation in gradientboosted regression trees Comprehensive experimental evaluation Weaknesses: Lack of theoretical justification Proposed method is essentially heuristic Comparison with alternative similarity measures for nearest neighbor methods would be beneficial Questions: Potential limitations identified by the reviewer include the absence of theoretical justification and the lack of comparison with other similarity measures. The authors are invited to comment on these concerns.", "Paraphrased Statement: This study introduces a new method for estimating uncertainties in Gradient Boosted Decision Tree (GBDT) models. The approach utilizes the k nearest training data points to make probabilistic predictions. These nearest neighbors are selected based on instances that appear in the same leaf node as the given test data point in the ensemble of decision trees. Notably this method is applicable to any GBDT model posttraining. Strengths: Compatible with any GBDT model and can be implemented on existing models during inference. Experimental results demonstrate superior performance compared to current methods (NGBoost and PGBM) in terms of Root Mean Squared Error (RMSE) and probabilistic evaluation metrics. Weaknesses: The inference time may increase significantly. The paper does not compare the proposed method to the probabilistic prediction capability already implemented in CatBoost. Questions: 1. Can the proposed method be compared to CatBoosts RMSEWithUncertainty loss function which also predicts mean and variance 2. How were the model hyperparameters tuned Were different parameters used for different evaluation metrics 3. How does the proposed similarity calculation compare to a naive kNearest Neighbors (KNN) approach applied to the original features 4. Was feature normalization performed before applying KNN 5. The study mentions using onehot encoding for categorical variables. Can this encoding strategy affect the performance of certain GBDT libraries like CatBoost Minor Typos: Line 87: \"mean and variance is\" should be \"are\" Lines 141 and 189: Modify \"means\" to \"mean\" Line 252: Change \"Table\" to \"Figure\"", "Paraphrased Statement: This paper introduces a novel approach for estimating uncertainty in Gradient Boosting Regression Trees (GBRT). Unlike existing distancebased methods the proposed approach utilizes an affinity measure between test samples and training samples. By leveraging efficient sampling techniques it reduces computational complexity. Empirical evaluations demonstrate the superiority of the proposed \"ImportanceBased Uncertainty Estimation\" (IBUG) compared to recent GBRT uncertainty algorithms (NGBoost and PGBM). Strengths: The method aligns with distancebased conformal prediction while employing a unique affinity measure. The paper presents a novel concept and demonstrates its effectiveness. Comparisons to PGBM highlight the advantages of the IBUG approach. Weaknesses: Technical Concerns: The paper lacks clarity regarding the point prediction of IBUG. It only compares IBUG to a limited number of methods excluding potential alternatives. Evaluation Metrics: The paper does not consider widely used metrics like calibration error and sharpness for assessing probabilistic forecasts. Hyperparameter Tuning: Validation data is used to tune hyperparameters which may not be optimal. The appropriate value of k for the neighbor set remains unclear and varies across datasets. The impact of minimum leaf node size on affinity calculation and uncertainty estimation is not explored. Mathematical Expressions: Equations (1) (3) and (4) require corrections for clarity."], "z9cpLkoSNNh": ["Paraphrase: Summary Researchers have developed DPGrad a method for incremental continual learning where feature extractors are assumed to be linear. They have also established a lower bound on errors for a specific scenario involving nonlinear feature extractors. Using a simulated environment they have compared DPGrad to SGD and OGD. Strengths The study precisely defines the setting and assumptions before presenting DPGrad. It aims to enhance theoretical understanding of continual learning an underresearched area due to its complexity. Weaknesses The authors refer broadly to continual learning but their findings are specific to certain settings and do not apply to all variations. The definition of forgetting is specific to the assumption of high accuracy on the current task which may not be the case in general. The potential practical significance of the findings is unclear and the results provided do not offer clarity. The lower bound on error presented in Section 5 lacks practical context. Questions Can the authors offer more empirical data on DPGrads performance in realworld settings What are the effects of different environmental assumptions on the lower bound presented in Section 5", "Paraphrased Statement: Summary: Recent research in Continual Learning (CL) has focused primarily on empirical approaches like replay regularization. This work addresses the understudied theoretical aspects of CL for regression tasks with known task descriptions. Algorithm: The paper proposes DPGrad an algorithm that provably prevents catastrophic forgetting in linear regression models when optimal solutions can be expressed as linear mappings of inputs. However the algorithm fails to prevent forgetting in nonlinear settings. Strengths: The theoretical contributions are robust especially for linear models using squared loss. Weaknesses: The works practical applicability is limited due to assumptions such as task descriptors and a linear feature case. Questions: 1. Clarify the definition of V in Algorithm 1. 2. How does DPGrad perform when combined with experience replay 3. Can catastrophic forgetting occur in linear feature scenarios with a single head (i.e. a shared vi for all tasks) 4. How can the proposed method be extended to classification problems 5. Could the authors incorporate the \"Reconciling Metalearning and Continual Learning with Online Mixtures of Tasks\" paper into their related work", "Paraphrased Statement: Summary The authors propose a theoretical framework for studying continual learning. They focus on feature extraction where features are learned sequentially in different environments followed by taskspecific linear classifiers. They analyze two cases: linear and nonlinear features. For linear features they present an algorithm that guarantees convergence and equal performance on past and present tasks under specific assumptions. For nonlinear features they create a counterexample demonstrating the impossibility of achieving low error with high probability using any continual learning method. The algorithm for the linear case is validated through simulations using synthetic data comparing it to conventional methods. Strengths and Weaknesses Strengths Addresses a gap in understanding between theoretical and practical aspects of continual learning. Clear and wellstructured presentation easy to follow. Intuitive approach using feature extraction for supervised learning. Comprehensive analysis of the linear case with sound and insightful results. Weaknesses Unbalanced focus with disproportionate attention to the linear case compared to the nonlinear case. Extrapolation of results from the linear case to the nonlinear case may be limited. Experimental validation is limited to synthetic data with specific assumptions that may not reflect realworld applications. Separate training of feature extractor and linear classifier may impact results. Questions Clarify the differences between the proposed algorithm and existing gradient projection methods. Explore the case where feature extraction and linear classification are trained simultaneously aligning with practical neural network training. Analyze the nonlinear case with different activation functions and targets such as ReLU and piecewise linear functions to assess the potential impact on the findings.", "Q1: Reasons for Importance Explores the limitations of CL approaches specifically in the multiheaded architecture setting. Provides a novel gradientbased algorithm for linear feature extractors guaranteeing no catastrophic forgetting. Q2: Comparison to [1] The result for nonlinear features in this paper mirrors a similar conclusion in [1]. However this paper presents a more general framework and algorithm for the linear case. Q3: Line 338 Yes the correct letter is \"r3\" instead of \"n3.\""], "w1CF57sLstO": ["Paraphrase: Summary: This paper extends the generalization bound for MAML to more complex linear regression problems with overparameterization. It establishes both upper and lower bounds for the excess risk connecting it to data input task distribution outer loop length and inner loop learning rate. Experiments support the theoretical findings. Strengths and Weaknesses: Strengths: Extends existing bounds by relaxing assumptions. Wellwritten and accessible for nonexperts. Dense math but wellpresented. Weaknesses: Theoretical results not independently validated. Questions: Extension of the theory to metabatch settings. Rationale for iterative averaging instead of using the final model. Theoretical extension for longer inner loops.", "Paraphrase: The authors investigate the behavior of overparameterization in the MAML framework focusing on mixed linear regression settings commonly used in realworld applications. They derive formulas that bound the excess risk (the difference between the risk of MAML and the Bayes risk) from both above and below. They show that with a significant number of metatraining iterations the excess risk vanishes. They also analyze the effects of early stopping and the adaptation learning rate on excess risk. Strengths: Clear and wellorganized writing Use of realistic application settings Derivation of explicit formulas for excess risk bounds Novel analysis of training behavior using spectrum decomposition Weaknesses: Focus on linear regression which is not widely used in meta learning in practice Limited practical relevance due to the emphasis on theoretical properties rather than practical applications", "Paraphrase: Summary: This paper examines the verifiable generalization capacity of overparameterized MAML particularly focusing on the mixed linear regression model and a onestep metatraining process using stochastic gradient descent (SGD). The authors provide theoretical upper and lower bounds for the excess risk of metalearning trained with SGD. They also investigate the impact of task data distributions and adaptation learning rates. This paper contributes to understanding the effects of MAML trained with SGD. Strengths: Clear and concise presentation Complete proofs for main results Relevance to the machine learning community Weaknesses: Novelty: Previous work by Chen et al. (2020) and Fallah et al. (2021) have also provided generalization bounds for MAML trained with SGD with more general results. The authors should compare their work to these studies. Definition of task diversity rate: The definition of task diversity rate used in analyzing the impact of task diversity is not clearly defined in the paper. Significance: Many of the results are intuitive or already proven in the literature. The paper lacks new insights or practical guidance for algorithm improvement. Generalizability: It is unclear whether the results can be extended to other machine learning methods beyond linear regression. Questions for Clarification: Can the authors clarify the definition of task diversity rate Why do the authors believe their work is novel despite the existing generalizations of MAML trained with SGD Can the authors provide insights into the potential practical applications of their results"], "toR64fsPir": ["Paraphrased Summary This paper presents TLSM a new model for detecting communities in multilayer networks. TLSM leverages tensor representation to capture the diversity of network layers mapping nodes into a lowerdimensional space where nodes from the same community have similar representations. Additionally TLSM employs a regularized framework to simultaneously estimate the multilayer network structure and identify communities. The paper provides theoretical analysis demonstrating TLSMs ability to achieve asymptotic consistency in both network estimation and community detection. Strengths and Weaknesses Strengths: TLSM is a versatile framework that encompasses existing multilayer network generative models. TLSM combines network estimation and community detection through a penalized likelihood function. TLSM provides theoretical assurances regarding the accuracy of its network and community detection outcomes. Weaknesses: TLSM relies on projected gradient descent which may only yield local optima. TLSMs performance improvement on synthetic networks is not replicated to the same extent on realworld data. While TLSM is versatile its techniques are not particularly novel. Questions Could incorporating random walks between network layers enhance TLSMs integration of heterogeneous structures What is the computational complexity of TLSM and how does it compare to alternative methods", "Summary Paraphrase: The study investigates the creation of node embeddings for multilayered networks focusing on community detection and link prediction. The paper introduces the TLSM model which is based on a tensor decomposition framework. TLSM enables different embeddings for nodes even within the same community enforces specific identifiability properties and can handle sparse networks. Theoretical and empirical evidence supports the papers claims. Strengths: Addresses a crucial issue with practical applications. Includes a thorough consistency analysis. Wellwritten and clearly presented. Weaknesses: Limited discussion of related research. Unclear practicality of consistency assumptions. Small scale of datasets used. Lack of time complexity analysis. Limited empirical evaluation focusing primarily on community detection with a single link prediction experiment. Use of \"yet\" in line 58 and incorrect Greek letters in line 73. Questions for Authors: Why is the embedding dimension fixed at K Rationale for the small datasets used. Time complexity analysis of the model. Selection criteria for baseline models. Plans for expanding the experimental framework to include more extensive link prediction and node classification tasks.", "Paraphrase: This paper presents a new latent space model (TLSM) that generates node representations that capture the community structure of multilayer networks. Instead of relying solely on loglikelihood optimization TLSM incorporates a clusteringbased penalty to promote the preservation of community information. The model is optimized using projected gradient descent (PGD) and mathematical proofs demonstrate its convergence properties. Strengths: Flexibility: TLSM can handle various network models. Multitasking: TLSM simultaneously models the multilayer network structure and detects communities. Theoretical Foundation: Asymptotic consistency guarantees the models accuracy. Weaknesses: Lack of Novelty: Tensor decomposition and regularized likelihoods are not particularly innovative concepts. NonGlobal Optimization: PGD may not find the optimal solution requiring careful initialization. Limited Performance: Empirical results show minimal improvement over existing methods. Question: The time complexity of TLSM is not explicitly addressed in the paper. Providing further details on this aspect would be beneficial."], "oOte_397Q4P": ["Paraphrase: S3PET is a technique that automatically finds the ideal structure for language models allowing for efficient training with fewer parameters. Strengths: Unlike previous methods S3PET doesnt rely on predetermined substructures but instead searches for the optimal structure based on downstream performance. S3PET excels even with a minimal number of parameters (0.01) far less than comparable approaches. Weaknesses: Experiments are limited to GLUE and SuperGLUE classification tasks not covering generation tasks like translation or summarization. No analysis is provided on training data requirements and whether the approach performs well with limited data. Only the T5large model was used and results for larger models or decoderonly models are unknown. Theres no discussion on the training costs of S3PET in the main text although some information is provided in the appendix. Questions: The authors dont compare with prompt tuning baselines which they note take longer to converge. If any preliminary experiments were conducted it would be helpful to provide data on the convergence times.", "Paraphrased Statement: This paper proposes an automated system for finding parameterefficient PET models for finetuning. Extensive testing shows that the automated method outperforms manual and random methods while using fewer trainable parameters. Strengths: The combination of architecture search and PET model tuning is a novel idea. The experiments demonstrate the superiority of the proposed method over previous approaches. Weaknesses: The runtime of the finetuning process is not reported which could hinder evaluating the effectiveness of the proposed method. Question: Provide more information on the runtime of the finetuning process using the proposed method.", "Paraphrased Statement: Summary This research presents a novel neural architecture search (NAS) approach for optimizing \"adapter\" modules (PET) in parameterefficient tuning techniques. The proposed method employs a comprehensive search space and utilizes a binary differentiable NAS algorithm for architecture exploration. It introduces a \"shifted global sigmoid\" mechanism to manage the sparsity of PET structures. The resulting PET structures demonstrate competitive performance with only 15 trainable parameters outperforming manually designed approaches. Strengths Clear and straightforward presentation Novel and intriguing research problem (automatic PET structure design) Potentially useful unified search space Efficient sparsity control mechanism Extensive experimental evaluation Weaknesses 1. Search Cost vs. Application: The motivation behind PET methods is to minimize model adaptation costs. However the proposed PET search requires significant training time (58 times more) potentially contradicting the PET objective. The authors claim that the search cost is comparable to manual design but similar effort is still required for search space design and hyperparameter tuning. This high search cost may limit the practical applications of the S3PET approach. 2. Missing Literature: Several relevant works are not discussed including: Differentiable NAS techniques with binary gates (e.g. ProxylessNAS FBNet) Automatic model adaptation approaches (e.g. SpotTune AdaFilter) Questions 1. How do PET methods reduce model adaptation costs While optimizing a PET module with gradient descent requires calculating and storing gradients for subsequent network layers it does not inherently reduce the computational or memory costs of the entire model. 2. Minor Suggestions: Use \\eqref in LaTeX for equation references. Vary the introductory phrases in consecutive sentences (instead of using \"Moreover\" repeatedly).", "Summary Paraphrase: This research paper presents a technique for automated construction of ParameterEfficient Transformers (PETs) called S3PET with Neural Architecture Search (NAS). It uses a twolevel optimization process to search for PET structures and introduces a shifted global sigmoid method to control the number of trainable parameters. Experiments demonstrate that S3PET achieves superior performance with a limited number of trainable parameters. Strengths and Weaknesses Paraphrase: Strengths: Automatic search for PET structures that optimize parameter efficiency. Stateoftheart performance with low trainable parameters. Weaknesses: Lack of clarity in some descriptions related to NAS. Questions arise about: Determining the number of positions where the sum of pi is used for selecting parameter weights. The choice of dataset used for NAS and whether searches were conducted separately for GLUE and SuperGLUE. The time cost of the NAS step compared to the training step. The lack of significant performance improvement in some cases. The reason why the method guarantees transferability."], "nEJMdZd8cIi": ["Paraphrase: Summary: The authors introduce two new algorithms for training deep neural networks while keeping their weight matrices unitary meaning they preserve a special mathematical property. They then extend these algorithms to handle convolutional layers allowing for the use of unitary operations in image processing. Strengths: The authors algorithms exploit the lowrank structure of gradients resulting in significant speedups over previous unitary training methods. The lowrank approximation allows for ranks even smaller than the batch size further reducing computational cost. The extension to convolutional layers opens up new possibilities for controlling linear operator properties in image processing potentially improving generalization. Source code is available for recurrent networks facilitating widespread use. Weaknesses: The experimental evaluation focuses primarily on technical advantages rather than practical applications. Applications for unitary operators beyond recurrent neural network toy problems are not explored in detail.", "Paraphrase: This paper proposes two efficient update rules (PROJUNND and PROJUNNT) for neural networks that use unitary matrices. Unitary matrices are beneficial for preserving vector norms and have differentiable parametrizations. However their use is often limited by the computational cost of O(n3). To address this the paper exploits the hypothesis that the training gradient for unitary matrices can be approximated as a lowrank matrix. This allows for efficient O(kn2) gradient updates where k is a hyperparameter that controls the gradient rank. PROJUNND approximates the gradient performs a standard SGD step on the unitary matrix and projects the result back to the unitary group. PROJUNNT instead rotates the unitary matrix along a projection of the gradient on the unitary Lie algebra. Both methods have been tested and shown to achieve competitive results on various RNN benchmarks while significantly reducing runtime. They also show promise for use in convolutional neural networks. Strengths: Compelling results that could make URNNs more feasible Clear explanations Extensive experiments Weaknesses: Methods appear complex to implement. Full pseudocode algorithms including lowrank gradient approximation would be helpful. Code availability would be beneficial.", "Paraphrased Summary: This study focuses on speeding up the training of unitary neural networks by leveraging gradient sampling to uncover their lowrank structure. The authors propose two efficient gradient descent algorithms for updating unitary matrices\u2014one that directly projects updates onto the unitary manifold and another that projects the gradient onto the tangent space and rotates it before updating. Experiments demonstrate that these algorithms achieve performance comparable to existing unitary neural networks with nearoptimal runtime efficiency. Additionally the study includes preliminary findings on training orthogonal convolution layers. Strengths and Weaknesses: The paper is wellstructured and provides robust theoretical underpinnings which have been verified. It comprehensively compares the proposed algorithms with previous work. However the evaluation of novelty and experimental results is left to other reviewers due to the authors limited knowledge of related research. Questions: The authors propose two methods (projUNND and projUNNT). Beyond numerical discrepancies are there any fundamental differences between these methods It would be beneficial to compare their advantages and disadvantages.", "Paraphrased Statement: This study introduces projUNN a method for efficiently training neural networks with unitaryorthogonal parameter matrices. It involves: Approximating the gradient update with a lowrank matrix. Using Riemannian gradient descent via either retraction or exponential map computation. The methods strength lies in its efficiency as it can approximate gradients with high accuracy using lowrank matrices resulting in significant speed improvements. Potential drawbacks include the computational overhead of eigenvalue and QR decompositions. To improve understanding comparisons could be made between projUNN and baselines in terms of: Convergence rate (iterations) Time to achieve comparable loss Correction: Typo in equation on page 23 where \\tildeM should be replaced with U."], "lKFOwaYNQlb": ["Paraphrase: This research expands the influencebased abstraction (IBA) concept to multiagent reinforcement learning (MARL) scenarios. The approach decentralizes the global system simulator into individual local simulators each with its own set of influence variables representing interactions among simulators. Agents are trained separately for each local simulator in parallel and approximate influence predictors (AIPs) are regularly updated to account for changes in simultaneously trained policies. The proposed method called DIALS is theoretically justified by demonstrating that precise influence values are not essential for optimal policy generation. Experiments on two multiagent environments show that DIALS accelerates training and improves performance compared to training on the global simulator. The study also investigates the optimal frequency of AIP retraining suggesting that occasional retraining can enhance performance while excessive retraining can harm it. Strengths: Originality and novelty in extending IBA to MARL. Innovative technical contributions including parallel training of agents AIPs and a theorem justifying retraining decisions. Relevance and significance to the active field of MARL. Enhanced simplicity and potential for community adoption. Weaknesses: Potential limitations in applying DIALS due to the requirement for parallelizable environment simulators. Limited experimental evaluation with only two domains and one baseline. Unclear reasons for the inferior performance of the global simulator compared to DIALS.", "Paraphrase: Summary: The authors apply the local simulator approach to multiple agents showing improved performance in runtime and agent returns. Strengths: Runtime improvement Agent return enhancement Weaknesses: Loose theorem bounds with limited intuitive value Unclear caption in Figure 3 Undefined F in fPOSG definition Use of 4M agent in Figure 3 despite lower returns Potential confusion with existing DIAL method Correctness: Lines 116118 require proper citation for the claim on solving fPOSG through sequential POMDP iterations. Experiments: Lack of intuitive reward metrics (e.g. speed collected objects) Absence of handtuned baseline for performance comparison Questions: Can grouping agents into smaller sets (e.g. 2x2) improve results Nonimprovement with RNNs: Are RNNs overfitting to local simulators causing global simulator failures Necessity of limiting agent performance in local environments Availability of time index to influence estimators Number of rollouts in the PPO training batch", "Paraphrase: Summary: This work aims to break down large network system simulators into smaller ones specifically for training multiagent reinforcement learning. The proposed approach involves creating an enhanced local simulator with increased influence by training an influence predictor on data from the global simulator. This predictor is then applied to the local simulator. Strengths and Weaknesses: Factorization is a common strategy in MARL training but this work proposes a unique angle of factorizing the simulator to enhance scalability for agent count and environment size. The methods ability to mitigate nonstationarity issues deserves further exploration. The manuscript is wellwritten and accessible including preliminary theoretical analysis. Questions and Concerns: Factorization into smaller simulators may require multiple processes which could introduce overhead. An evaluation of this overhead would be helpful. Partitioning the large simulator into multiple multiagent simulators might improve training efficiency and resource utilization. This should be considered in the evaluation. While periodic updates of the influence predictor at a slower frequency appear to stabilize training it raises concerns about potential effects on exploration. This aspect should be explored further."], "p4xLHcTLRwh": ["Paraphrased Statement: Summary: This study uses transformer models to tackle the Learning with Errors (LWE) problem. By feeding tuples of datalabels drawn from an LWE distribution to the model the authors aim to extract the secret variable \"s\" from the model if it has learned something about the distribution. They propose methods to extract \"s\" after demonstrating the effectiveness of transformer models in modular arithmetic. The technique is applied to a specific LWE instance with lowdensity binary secrets in dimensions below cryptographic standards but still significant. The authors discuss the potential scalability of their approach to more realistic LWE instances for cryptographic purposes based on these experimental results. Strengths and Weaknesses: The paper is wellwritten and accessible effectively introducing the necessary cryptographic knowledge. The attack principles are straightforward and the problem parameters are clearly explained and analyzed. However the paper does not describe transformer models or their variants assuming familiarity with these concepts. The main concern is the significance of the results. While the authors acknowledge that the problem parameters are not yet cryptographically standard they propose future extensions to larger parameters. The papers relevance depends on the validity of this claim which may be affected by the intrinsic hardness of LWE (as suggested by the authors) or the limitations of gradient descent (discussed in the comment below). Comment on Gradient Descent Limitation: The comment raises the issue of gradient descents inability to efficiently solve certain LWE instances despite not being cryptographically hard. The authors are asked to consider the applicability of this limitation to their problem and include a discussion on its potential impact.", "Paraphrase: Summary: This paper uses transformer models to analyze the LWE (Learning with Errors) problem. They demonstrate that transformers can master modular arithmetic implying their potential to learn the LWE inputoutput mapping. The paper proposes two MLbased cryptanalysis strategies: learning the LWE function and recovering the secret using that function. Evaluations are conducted on moderatesized LWE instances. Strengths: The paper is wellwritten and provides a comprehensive introduction to the problem. SALSA a novel LWE attack is proposed. SALSA shows some theoretical justification. The study includes thorough ablation experiments to assess SALSAs performance at various parameter settings. Weaknesses: The connection between the modular arithmetic results and the LWE results needs clarification. SALSA is less interpretable than existing cryptanalysis techniques. SALSAs performance exhibits unexpected variations (e.g. Table 2). It is unclear if SALSA can handle higher Hamming weights and dimensions simultaneously. Questions: Does SALSA demonstrate traintest overlap when predicting \"ai\" vectors Is there any inherent limitation to the small Hamming weight limitation observed in experiments Does the training data expansion technique resemble sievingbased SVP algorithms Are there any examples seen multiple times within an epoch", "Paraphrase: The paper presents a machine learning method for solving modular arithmetic and attacking LWEbased encryption. However the examples solved are very simple. For instance the last example involves uncovering a 128bit secret where 125 bits are zero. Only about 341376 possible secrets meet this criterion which can likely be tested much faster than the 23 hours reported. This is not comparable to the Darmstadt challenge where the secret was randomly chosen from a much larger domain. The authors primarily demonstrate how the solution works for decreasing densities with higher dimensions which limits the secrets possible values. It is therefore exaggerated to claim that this approach could potentially attack realworld LWEbased encryption systems. Question Reformulation: Is the running time for recovering the secret primarily determined by the number of possible secrets", "Paraphrased Summary: SALSA attacks cryptosystems using Learning with Errors (LWE) by training a transformer model to perform modular arithmetic. Using this model the attack can differentiate LWE instances from random ones. The authors propose secret recovery algorithms \"direct secret recovery\" and \"distinguisher secret recovery\" with theoretical justification. Strengths: Novel approach using transformers for modular arithmetic. Secret recovery is possible even with low model accuracy. Helpful tables demonstrate the attacks effectiveness. Weaknesses: Requires extensive computational resources and long runtimes. Poor scalability with lattice dimension. Questions: Runtime information in Table 2 includes training time If not provide training time. Conduct baseline method comparisons on the same device for runtime comparison. PostRebuttal Comments: Concerns have been addressed through author responses. The reviewer maintains the original score."], "rUc8peDIM45": ["Paraphrase: Summary: The popular belief in deep learning suggests that the randomness introduced by stochastic gradient descent (SGD) favors finding \"gentle\" minima. Researchers have struggled to formalize this intuition mathematically. This paper presents a mechanism that allows SGD to rapidly escape minima where the second derivative is excessively large. This mechanism is different from the curvaturebased process that drives fullbatch gradient descent to escape minima where the spectral norm of the second derivative exceeds twice the step size. Strengths: The concept of an exponential escape driven solely by randomness is novel. The necessary condition for this phenomenon (relationship between noise and loss value) is proven to exist in specific network models. Weaknesses: The analysis only provides a sufficient condition for instability not a definitive condition. While stability is influenced by both fullbatch and noise components the analysis focuses solely on noise. The papers structure could be improved by presenting the escape analysis before the sufficient conditions. Questions: Why is the flatness of minima independent of model size Is the linear stability analysis valid only near local minima or can it be generalized Could the bound for real networks (VGG ResNet) be tightened by considering smaller batch sizes than 64 It is hypothesized that noisedriven escape may be more significant with smaller batch sizes.", "Summary Paraphrase: The authors explore the significance of noise in stochastic gradient descent (SGD) from geometric and scaling perspectives. They demonstrate that if a specific alignment property is met a global minimum is stable if and only if the Hessian norm at the optimum is bounded by a model and samplesizeindependent constant. Strengths Paraphrase: The paper is wellwritten and clearly explains assumptions and estimates. The noise model is explicitly presented and discussed unlike in many cases. The focus on the noise model both in terms of variance and geometry differentiates the paper from previous studies. Weaknesses Paraphrase: The paper relies on several stated results and approximations with the crux being the validity of equation (2) specifically that alpha beta or mu are close to 1. While Figure 1 suggests this Figure 5 does not clearly show this and the authors statement about alignment strength being independent of model size is not entirely convincing. Despite the papers clarity the stability argument for SGD is still questionable as real training dynamics in neural networks do not exhibit behavior similar to Figure 4. The authors could address this. Minor Typos: Line 65: \"lambdai\" instead of \"lambda1\" Line 227: \"In the current paper\" (missing punctuation) After Line 235: Confusion between \"v\" and \"nu\" Questions: The authors could provide quantitative estimates of how closely the approximations in equation (2) match the actual covariance. They could also experiment further with the influence of the loss and the independence between the gradients and the loss.", "Summary The paper explains why SGD (Stochastic Gradient Descent) often finds flat minima. It does this by connecting the noise in SGD to its stability. In particular it shows that the Hessian of an SGDaccessible minimum in overparameterized models with square loss is bounded by a term that depends only on the batch size and learning rate. This is proven for linear networks and RFMs. Strengths Relevant topic of study Extensive quantitative analysis on various models including RFMs linear models and deep neural networks Wellwritten and clear paper Questions Are the results in Figure 3 robust to different definitions of flatness Could Figure 5 include VGG11 in the right panel The \"notion of flatness\" in line 110 could be clarified. The role of the norm in the flatness definitions is also unclear. Sizeindependence plays an important role in the analysis. Minor \"initiated studied\" in line 96 should be \"initiated studies\""], "lbQTJN42uea": ["Paraphrased Summary: This research explores the use of fast sketching algorithms in Kronecker Product Regression to accelerate Alternating Least Squares (ALS) for computing Tucker Decomposition. It improves upon previous work by recognizing the usefulness of the matrix M (K\u1d40K \u03bbI) as a preconditioner for solving sketched ridge regression through Richardsons Iteration. Additionally a method is provided for efficiently computing (K\u1d40K \u03bbI)v for a vector v using fast matrix multiplication. While updating the core tensor in Tucker Decomposition requires solving a standard Kronecker Product Ridge Regression updating the factor matrix necessitates solving a more complex variant with a subspace constraint. The authors demonstrate how their techniques address this challenge and expedite all stages of the ALS algorithm for Tucker Decomposition. Strengths: Demonstrates the benefits of accelerating Kronecker Product Ridge regression for ALS in Tucker Decomposition. Enhances speed by recognizing that solving the sketched problem with Richardsons Iteration using (K\u1d40K \u03bbI) as a preconditioner accelerates computation. Weaknesses: Does not provide any novel contributions to sketching theory. The input sparsity time claims for Kronecker Product Ridge Regression remain unclear as the algorithm computes singular value decompositions of A1... AN which are not used in the preconditioner. The introduction could provide more context on Tucker Decomposition for readers who are unfamiliar with the terminology. Questions: Clarify the input sparsity claims in Theorem 1.1. Explain the usage of Theorem 4.5 in the FastKroneckerRegression algorithm. Describe how the term \u00d5(MM(P\u1d62 \u2208 TsubRsubsupnsupsubR\u03b5\u2081sub P\u1d62 \u2209 TsubRsubsupnsup)) arises in line 13. Discuss the potential impact of solving intermediate ALS problems with small approximations on overall convergence. Confirm if additional details are provided in the appendix to address any misunderstandings or missing information. Update after Author Responses: The authors responses have addressed most concerns. While the paper lacks contributions to sketching it meticulously applies existing techniques to Tucker Decomposition a significant problem. Theorem 1.1 is clarified to apply to the intermediate problems of ALS where sparsity might not be guaranteed. Theorem 4.5 is used to bound the error in the preconditioner. Line 13 reflects the estimation error in solving intermediate ALS problems. Experiments show that small approximations of intermediate problems do not significantly affect convergence.", "Paraphrase: The researchers have developed a novel algorithm that can solve Kronecker regression problems significantly faster than existing methods. Kronecker regression involves a specific type of data structure known as a Kronecker product. This problem is commonly encountered in computing the Tucker decomposition of tensors. The algorithm employs sampling techniques and iterative methods leading to improved computational efficiency. It can also handle data structures with a block design matrix. The algorithm has been tested on both artificial and realworld datasets demonstrating substantial performance advantages. Strengths: Clear and wellwritten paper Novel method with strong theoretical and empirical support Significant runtime improvement Low relative error on synthetic data Thorough review of related work and background information Weaknesses: Example applications on real data are not provided Clarification: \"nnz\" on page two refers to the number of nonzero elements in a matrix. Theorems are included within the introduction for readability. Question: The scalar \"s\" is not defined within the context provided.", "Paraphrase: Summary: The research addressed the core issue of Kronecker regression and developed the initial sketchbased algorithm to deliver a (1\u03b5)approximate solution in subquadratic time. The paper provided rigorous proofs and empirical evidence to support the approach. The blocksketching method used has potential for other applications. Strengths: The study tackles an important problem with an accelerated solution making it highly relevant. The technique employed is novel and intriguing. The mathematical analysis is sound and supported by empirical findings. Questions: Did the experiments utilize fast matrix multiplication techniques How does the runtime complexity in the experiments compare to the theoretical bounds stated in Theorem 1", "Paraphrased Statement: This paper introduces faster algorithms for solving the \\ell2 Kronecker product regression problem which involves finding the least squares solution to a regression problem where the design matrix is a combination of smaller matrices of dimensions In \\times Rn. These matrices have dimensions In \\times Rn n \\in [N]. The authors also extend these techniques to optimize algorithms for Kronecker ridge regression and expedite the Alternating Least Squares (ALS) algorithm used to calculate the Tucker decomposition of a tensor. Previous research (Diao et al. 2019) on Kronecker product regression focused on scenarios where Rn \\gg In for all n \\in [N]. This paper enhances the efficiency of the algorithm when Rn may be comparable to In. Experimental evidence supports the significance of addressing this case. Strengths and Weaknesses: Originality: The authors do not claim originality leaving it unclear how this work compares to other similar research. Quality: The paper provides solid technical content with wellsupported claims. Clarity: The paper is wellorganized and easy to understand. Significance: The addressed problems are important as Kronecker product regression finds applications in various fields. Questions and Editing Suggestions: Define \\omega (along with a suitable reference and approximate value) before using it on page 2. Change \"pseduoinverse\" to \"pseudoinverse\" on line 93 (260)."], "lmmKGi7zXn": ["Paraphrased Statement: Summary: This paper utilizes the Neural Tangent Kernel (NTK) to explore the capabilities of infinitewidth autoencoders in recommendation systems. The authors introduce a data distillation framework that generates a compact dataset that closely resembles the performance of algorithms trained on the original dataset using bilevel optimization. The effectiveness of the infinitewidth autoencoders and data distillation framework is demonstrated through experiments on four publicly available datasets. Strengths: 1. The paper presents a novel approach to enhance recommendation systems by creating a smaller yet informative dataset which may stimulate further research in this area. 2. The proposed method exhibits promising results in improving recommendation performance and is more resilient to noise than existing stateoftheart techniques. Weaknesses: 1. For improved clarity it is recommended to provide additional background information and context on NTK and Kernel Ridge Regression (KRR). These concepts are relatively new in recommendation systems and may be unfamiliar to readers. 2. To establish the generality of the data distillation framework (DistillCF) it would be beneficial to evaluate its performance with other collaborative filtering models such as autoencoderbased recommendation systems to demonstrate its adaptability. 3. The number of users varies significantly across datasets (e.g. 3000 vs. 476000). To facilitate comparisons in Table 1 it would be helpful to consider setting the user budget based on the percentage of user count. This would enable readers to better interpret the results. Questions: Please refer to the detailed comments provided in the original statement.", "Summary This paper proposes an autoencoder model that utilizes Neural Tangent Kernel (NTK) to estimate an infinitely wide neural network called \\inftyAE. While infinitely wide networks offer high expressiveness they face challenges with large datasets used in recommendation systems. To address this the paper introduces a learnable method for dataset summarization. Experimental results demonstrate the effectiveness and robustness of the proposed sampling method. Strengths: Employs NTK to obtain an infinitely wide network with superior expressiveness. The data summarization method is modelagnostic allowing it to predict useritem scores and optimize the prior matrix XS. The combination of \\inftyAE and DistillCF provides a simple and effective approach to create a compact model without compromising performance. Weaknesses: DistillCF is compatible with various neural models but the paper does not explore combining it with models like NeuMF EASE LightGCN etc. to demonstrate its general applicability. Some baseline models may not have been fully optimized. For instance EASEs regularization coefficient is crucial but its optimization is not discussed. The theoretical basis for the effectiveness of the method using only a small number of summarized data is not explained. The formulation of Gumbel softmax should be elaborated for clarity. Multiple hyperparameters require finetuning in the proposed methods. Questions: The statement \"the synthesized data was never be optimized (in the innerloop of DISTILLCF)\" is confusing. Does it refer to Xs not being optimized in equation 3 or EASE not being optimized (since the definition of \"inner loop\" suggests this) In Figure 3 why does the SVPCF method experience the most severe drop In Figure 4 HR10 is used as the vertical axis instead of the drop in HR10 used in Figure 3. It is not clear whether \\inftyAE is more robust than EASE as the drop in \\inftyAE may be higher than EASE. How are \"Head User\" and \"SVPCF\" methods used with \\inftyAE In continual learning the data from previous periods should be inaccessible so the \"joint\" method does not qualify as continual learning. The combination of MVAE and DistillCF in continual learning experiments should be clarified. In Figure 5 the ADER method performs worse than individual methods which is unexpected. In Figure 6 the performance on the ml1m dataset differs significantly from the other two datasets with deeper networks performing better. Appendix A Algorithm 2 should specify that the input \\hatXu represents the \"history vector of user to be predicted.\"", "This study introduces an autoencoder for recommendations that extends infinitely. The proposed model for recommendations has only one hyperparameter and a closedform solution. The simplicity of autoencoders is exploited in this paper to create DistillCF for large datasets. The proposed model outperforms its competitors as demonstrated by experiments with various datasets. Strengths: Novel and intriguing design Superior performance in comparison to various baselines Questions: Not defined", "Paraphrased Summary: The paper presents a novel recommendation system that combines the infinitewidth autoencoder (\u221eAE) with a technique called DISTILLCF to create \"summaries\" of the recommendation data. These summaries are then used for training the recommendation model. The system has shown promising results in both predictive accuracy and computational efficiency. However the papers writing style is dense and some concepts are not adequately explained or cited. Strengths: Novel and thoughtprovoking framework that combines stateoftheart concepts. Promising empirical findings for performance and efficiency. Extensive experimental studies. Weaknesses: Dense writing style can make it difficult to read and follow. Concepts are not properly introduced or cited requiring external references. Lack of discussion on the effectiveness of using data summaries in recommendation systems. Minor Comments and Questions: Contradictory terminology of \"infinitelywide bottleneck layers\" needs clarification. Regularization and its impact on the bottleneck approach require further discussion. Need for additional comments on handling novel users with no history. Potential overestimation of performance due to focus on head items. Clarification of the f function in equation (1) as a vector. Undefined term \"Dval.\" Recommendation to use \"\\log\" in mathematical formulas for the logistic function. Additional Recommendations: Improve writing clarity to make the paper more accessible. Define and cite key concepts clearly. Justify design choices and performance claims adequately. Explore the performance of the system on tail items. Investigate why a small number of synthetic instances provide good results. Verify the experimental code for potential bugs that may have influenced the results."], "oiztwzmM9l": ["Summary The paper introduces two novel Bayesian Optimization (BO) algorithms that utilize the neural tangent kernel Gaussian Process (NTKGP) as the surrogate model. To determine the next query neural Thompson Sampling is employed by sampling the neural networks initialization and optimizing it. The work demonstrates the sublinear regret of the proposed methods and provides promising experimental results. Strengths Originality in applying NTKGP to Bayesian Optimization. Development of a new BO framework with a nonstandard Gaussian Process surrogate and theoretical guarantees. Encouraging experimental results indicating potential superiority to standard BO methods in blackbox optimization scenarios. Complex and nontrivial proofs of the theorems. Weaknesses Theorem 1 (a) Proof based on finite inputs leaving a gap between theory and practice in generalizing to continuous input spaces. (b) Batch size (B) dependence: Regret upper bound is related to B not independent as claimed. Extreme values of B can affect the sublinear regret bound. (c) Use of a previous result for bounding \u03b3T despite different input domain conditions. (d) Redundant mathematical notation in proofs. Theorem 2 (a) Insufficient proof for the claim of asymptotic noregret for wide enough neural networks. (b c d e) Mathematical errors in equations. Experiments (a) Lack of explanation on setting the value of \u03c3 in the loss function. (b) LBFGSB optimization for discretecategorical inputs and mixed input domains. (c) Acquisition function used for neural Thompson Sampling in LBFGSB optimization. (d) Implementation details for GPTS and GPUCB. (e) Unfair comparison of STOBNTS (B 1) with sequential baselines under fixed iterations. Batched STOBNTS should be compared with batched baselines for a more equitable evaluation.", "Summary: The proposal is a new approach for Thompson sampling using the neural tangent kernel (NTK). It offers a novel method for sampling the posterior Gaussian process (GP) without matrix inversion utilizing techniques from existing research. The approach has two algorithmic variants both with proven regret bounds. Empirical evaluations demonstrate excellent performance compared to baselines. Strengths: Addresses a current issue. Clear except for specific sections and legend use. Substantial contribution in theoretical bounds. Encouraging empirical results. Weaknesses: Incremental approach compared to prior work. Certain sections and algorithm presentation are difficult to follow. QuestionsSuggestions: Line 111126 and Algorithm 1: Improve structure and order of presentation providing a brief NTK introduction based on [14]. Line 111: Define \"construct an NN f(x\\theta)...\". Include this step in Algorithm 1. Line 1922: Clarify the claim that higher B is better explaining its practical implications. Figure 23: Improve readability addressing split results and legend reuse. Experiments: Discuss the rationale behind selecting and adjusting parameters m and L. Minor: Maintain consistency in writing out small integers (e.g. 3 vs. three).", "Paraphrase: This study introduces a technique for Bayesian optimization using neural networks. The key concept is that training a large neural network (with a minor adjustment) using squared loss results in a sample from the posterior Gaussian process (GP) with the neural tangent kernel (NTK). This makes it equivalent to Thompson sampling. Based on this insight the algorithm is straightforward: train a neural network on the observed data and use it as the acquisition function which is maximized to choose the next point for observation. Strengths: Simpler than previous neural bandit approaches which can be complex and computationally demanding Theoretical guarantees: cumulative regret grows sublinearly when the true function follows a GP with the NTK kernel Experimental evidence suggests that neural networkbased methods perform equally or better than GPbased approaches in lowdimensional problems Neural networkbased approaches scale well to highdimensional problems (e.g. images) while GPbased approaches struggle Weaknesses: Assumes the unknown function is drawn from a GP with the NTK kernel which may not always hold true Questions: How does algorithm performance change if the unknown function follows a different kernel such as the squared exponential kernel", "Paraphrased Statement: The proposed algorithms SampleThenOptimize Batch Neural Thompson Sampling (STOBNTS) and STOBNTSLinear combine Thompson Sampling and Neural Network (NN) surrogate models for Bayesian optimization. Strengths: Novel concept of using NNs as surrogate models in Bayesian optimization. Rigorous theoretical analysis to understand the algorithms convergence properties. Extensive realworld experiments demonstrating the algorithms effectiveness. Weaknesses: The writing needs improvement for clarity. The hyperparameter tuning of NN surrogate models requires further exploration. The computational cost of training NN surrogate models for each iteration may be significant. Questions: How are the NN surrogate model hyperparameters chosen What is the time complexity of the proposed algorithms compared to existing baselines"], "rJjJda5q0E": ["Paraphrase: Previously the information ratio was employed to limit regret in Thompson sampling for multiarmed bandits. However this study demonstrates that the information ratio is ineffective in establishing bounds for contextual bandits. The authors introduce a novel concept called lifted information ratio which captures information acquired about parameters rather than optimal actions. This ratio enables them to derive regret bounds for Thompson Sampling. Using these bounds they derive regret bounds for various cases. Strengths: The lifted information ratio is a meaningful and intuitive metric that the authors explain effectively. They establish strict regret bounds using the new metric. Weaknesses: The authors acknowledge the absence of practical algorithms and defer that aspect to future research. Providing more discourse on this would improve the paper. Questions: The paper would benefit from additional discussion on the tractability of the algorithms involved.", "Paraphrased Statement: Researchers investigated the performance of Thompson Sampling (TS) in situations with adversarial contexts and binary losses. They proposed a modified definition of \"lifted information ratio\" based on the informationtheoretic framework developed by Russo and Van Roy. This approach enabled them to establish regret bounds similar to those in noncontextual settings. As a result their proposed method provides stateoftheart regret guarantees for diverse scenarios within this framework. Notably they derived a regret bound that is independent of the link functions slope for K actions with Lipschitz logits. Strengths: The concept of \"lifted information ratio\" addresses the issue of contextdependent information where the relevance of optimal actions in one context may not carry over to another. This adaptation of the informationtheoretic framework is novel in the context of Bayesian regret bounds with adversarial contexts providing potential applications in other settings. The proposed approach delivers competitive regret guarantees in several practical scenarios with adversarial contexts. It draws a connection to the \"decoupling coefficient\" introduced by Zhang raising intriguing questions. Weaknesses: Substituting the mutual information between optimal action and loss with that between true parameter and loss may lead to less favorable scaling with respect to the number of actions. However Lemma 3 shows that this is not the case suggesting further explanation. While the intuition behind the \"lifted information ratio\" approach is clear a more rigorous explanation of why the previous information ratio fails in the contextual case would be beneficial. Zhang suggested that TS might not be optimal in certain situations introducing a bias towards higher rewards in the \"Feel Good TS\" approach. The connection to the current work implies that TS may be sufficient at least in Gaussian scenarios. It remains to be determined whether this holds for more general situations. Exploring an approach that allows judicious choice between using mutual information with optimal action versus true parameter in TS analysis could enhance the approachs versatility.", "Paraphrased Summary: The paper analyzes contextual bandit problems with parameterized contexts using the RussoVan Roy information ratio framework. The key observation is that information gain can be studied on the \"context\" level rather than the \"action\" level. This approach provides new regret bounds for logistic bandits. The analysis is simplified and removes problemdependent constants found in earlier studies. The \"lifted\" information ratio is proposed as a potentially suitable method for studying contextual bandits with parameters. Strengths and Weaknesses: The results are significant and the writing is clear. The main idea may not be as novel as the authors believe. The \"lifted\" information ratio appears similar to the \"learning target\" concept from previous research. Questions: The paper should be revised to acknowledge and compare with existing work on learning targets. The authors should clarify their statement about polynomial dependence on the number of contexts. It would be useful to discuss the necessity of the adversary not knowing \u03b8. The Bayesian telescoping argument in the paper may be overcomplicated the entropy chain rule could suffice. Other Comments: Equation (1) has a missing comma. The focus on countable priors simplifies the analysis making it more accessible."], "pluyPFTiTeJ": ["Paraphrase: Summary This paper proposes that optimizing the empirical risk and penalty simultaneously in penaltybased domain generalization (DG) methods can actually worsen empirical risk harming performance on both seen and unseen data distributions. The paper presents a new approach that minimizes the penalty under the constraint of empirical risk optimality drawing inspiration from ratedistortion theory. Experiments demonstrate that the resulting optimizer (SDG) improves generalization performance on unseen data. Strengths Originality Theoretical and technical soundness Clear organization Interesting findings on the generalization gap and indistribution performance in the WILDS benchmarks with SDG significantly enhancing the performance of specific DG methods. Weaknesses Limited evaluation of SDG with additional experiments using more penaltybased DG methods needed to fully demonstrate its effectiveness. The claim that SDG improves empirical risk without compromising generalization ability is not fully supported by the results as it increases the generalization gap on certain datasets while decreasing it on others. The explanation for FISHs strong performance on FMoW despite its low accuracy is not entirely convincing. Questions The main concern lies in the evaluation of SDGs performance. Please refer to the \"Weaknesses\" section for more details.", "Paraphrased Statement: Summary: This study claims that penaltybased domain generalization (DG) methods often underperform because joint optimization difficulties hinder the minimization of empirical risk (indistribution performance). To resolve this the authors propose a DG technique that: 1. Assumes excess risk is not essential for generalization implying that indomain performance can be maintained without compromising outofdomain performance. 2. Eliminates excess empirical risk by recasting penaltybased DG as minimizing the penalty while maintaining optimal empirical risk. 3. Utilizes concepts from ratedistortion theory to solve the proposed optimization problem. 4. Experiments on the WILDS and DomainBed datasets show the methods efficacy. Strengths: Clear writing: The papers clarity organization and writing quality are commendable particularly in sections 2.2 2.3 and 2.4. Novel perspective and solution: The approach of attributing the failures of penaltybased DG methods to excess risk is novel. The proposal to formulate DG with a constraint on empirical risk instead of the penalty is also innovative. Weaknesses: Selective experimental results: The reported experiments appear handpicked with certain methods evaluated on only a subset of datasets. This raises concerns about the validity of the results. Central assumption is questionable: The assumption that indomain performance does not need to be sacrificed for outofdomain performance is often violated in practice particularly when spurious correlations exist in the training data. The authors justification for this assumption is debatable. Questions: Why are certain methods excluded from evaluation on specific datasets such as ERM on DomainBed or FISH on all WILDS datasets Could the addition of a separate penaltyregularization term in Eq. 6 be absorbed into the abstract Penalty(\u22c5) term Is this addition only for theoretical derivations or does it have empirical significance", "Paraphrase: Summary: This paper claims that adding a penalty term to the empirical risk in domain generalization methods does not reduce the empirical risk and harms performance on both the original and unseen domains. It introduces an optimization algorithm that aims to minimize the penalty while ensuring the optimality of the empirical risk. Strengths: The paper proposes a novel approach for modifying gradients during training for penaltybased domain generalization methods. It is wellwritten and organized. Weaknesses: The proposed method assumes the existence of a universal model that can solve all domains (seen and unseen) potentially limiting its practical applications. The simplification of the gradient space for updating may be oversimplified and more sophisticated options should be considered. Questions: Should \\hatL in Equations (4) (5) and (6) be replaced with L as L is already defined as the empirical risk Why is the penalty minimized at \\thetat Gt in Equations (5) and (6) instead of \\thetat \\eta Gt It would be beneficial to explicitly state in the paper that the constraint is replaced with a stronger constraint of uniform bounds over domains (as mentioned in Line 602 of the appendix)."], "luGXvawYWJ": ["Paraphrased Summary: This article introduces a new method (HABA) for compressing datasets through factorization treating it as a basis problem. It also includes adversarial constraints to enhance image diversity and incorporate additional distinguishable information. Strengths: Clear and accessible writing Original concept Positive experimental outcomes Weaknesses: The reviewer questions whether the compressed datasets can be utilized for knowledge distillation methods. The impact of model size on performance is unclear. Specifically it is uncertain whether compression will result in a larger performance difference for ResNet18 compared to ResNet101. Questions: The reviewer requests clarification on the following: Can compressed datasets be used for knowledge distillation How does model size affect the compression effectiveness", "Summary This study investigates strategies to parameterize synthetic training datasets in the Dataset Distillation (DD) method. The authors introduce a reparametrization using \"basis\" latents (zi) and \"hallucinators\" (Gi) to capture relationships within the dataset. This reparametrization can be applied to any DD method. When combined with MTT (a stateoftheart DD approach) the proposed technique yields improved distilled datasets with increased dataset sizes but similar parameter counts. Strengths and Weaknesses Strengths: Proposed reparametrization enhances DD performance with fewer parameters. Increased dataset size significantly contributes to performance improvements. Weaknesses: The method essentially reduces dataset parameter count while increasing dataset size which may not be a desirable tradeoff in DD. The rationale for parameter count as a primary metric in DD is not fully justified. Performance comparisons should also be conducted at equal dataset sizes to isolate the effects of reparametrization. The claimed motivation for the reparametrization is not supported by empirical evidence. The motivating example in the introduction is misleading. Questions and Suggestions: Why should parameter count be prioritized in DD compared to traditional metrics like dataset size Provide comparisons with standard image compression techniques. Conduct experiments at equal dataset sizes to assess the true impact of reparametrization. Clarify the term \"relations\" and provide evidence to support the hypothesis that the proposed parameterization improves inductive bias for DD. Explain the choice of using images as \"basis\" rather than generic latent vectors. Clearly state the need for increased dataset size to achieve optimal performance avoiding potential reader confusion. Address the potential misattribution of the task by referring to it as Dataset Distillation rather than Dataset Condensation. Correct grammatical errors and inconsistencies throughout the paper such as the misuse of \"Constrain\" and \"Constraint.\"", "Section 1: Paraphrased Summary This paper presents a novel data parameterization technique HaBa for compressing datasets. HaBa splits data into hallucination networks and bases leveraging the relationships between condensed data points. Experimental results demonstrate HaBas superior compression rate and crossarchitecture generalization performance. Section 2: Strengths and Weaknesses Strengths: Addresses a key issue in dataset condensation: data parameterization. Proposes a novel method with promising results. Weaknesses: Could benefit from additional ablation studies and experiments for stronger evidence. Section 3: Questions 3.1 Basis and Hallucinator Encoder function in Hallucinators: It remains unclear if the encoder is essential. Removal of the encoder could impact model performance. Style vector optimization: Exploring multiple style vector pairs could potentially increase model efficiency. Hallucinator capacity: Reasons for using a single ConvReLu block in Hallucinators should be explained. Effects of employing more layers should be explored. 3.2 Adversarial Contrastive Constraint Objectives disparity: Explanation for using a contrastive objective for the feature extractor and cosine similarity for the hallucinator is needed. Unified objective: Potential benefits of using a single objective (e.g. cosine objective) for both components should be examined. 4.1 Datasets and Implementing Details Optimization time: Comparison of training times between HaBa and baselines would be valuable. Classifier bias: Effects of inappropriate hallucinatorbasis combinations on downstream classifiers should be discussed. Hyperparameter summary: An appendix table summarizing hyperparameters used in experiments would be helpful. 4.3 Ablation Studies Crossarchitecture performance: Explanation for HaBas improved crossarchitecture performance is lacking. Insights and design choices underlying this property should be provided. Crossdomain performance: Renaming the title to \"Robustness to Corruption\" is suggested. Clarification of how \"diverse training data\" contributes to improved performance is needed. Misc Data distribution recovery: Arguments about \"recovering the original data distribution\" should be revised to focus on capturing discriminative features instead. Supervision rigor: Explanation of what constitutes a \"rigorous supervision signal\" for synthetic samples should be provided. Typo and figuretable placement: Typo and disrupted reading flow due to figure and table placement should be addressed. Table confusion: Splitting Table 4 into separate tables or moving one to the appendix would enhance clarity."], "lDohSFOHr0": ["Paraphrased Statement: Summary: This study introduces a novel semisupervised learning problem where not all classes have labeled data making it distinct from previous SSL research. SSLs ability to categorize unseen classes holds practical value as collecting labels for all classes can be challenging. The goal is to optimize performance in unseen classes while ensuring safety in labeled classes. The method employs an unseen class discovery loss that utilizes pairwise similarity and an adaptive threshold unsupervised loss to balance learning between seen and unseen classes. Experimental results showcase its effectiveness. Strengths: Explores a novel SSL problem with practical relevance potentially inspiring future research. Proposes two new SSL objectives for unseen class classification and safety maintenance which are easily integrated with existing SSL methods. Wellorganized and clear presentation. Weaknesses: Requires prior knowledge of the number of unseen classes which may not always be available. The impact of the hyperparameters controlling the tradeoff between objectives needs to be addressed. Questions: How can the reliance on prior knowledge of the number of unseen classes be reduced How do the hyperparameters \\lambda1 and \\lambda2 affect performance and how can they be optimally set in realworld applications", "Paraphrased Statement: Summary: This research explores a type of semisupervised learning where unlabeled data includes classes that have not been previously encountered. The papers goal is to achieve high accuracy in classifying both known and unknown classes unlike prior openset semisupervised learning (SSL) techniques (that reject all unknown classes) and novelty class detection (NCD) approaches (that only classify unknown classes). This is a crucial issue that has received insufficient attention. The authors propose a novel SSL method called SUSSL to address this problem. SUSSL employs pairwise similarity to identify unknown classes while preventing a drop in performance on known classes through an adjustable threshold. The approach achieves performance gains in both known and unknown class classification as demonstrated by experimental results. Strengths: 1. The research tackles a significant and understudied problem of SSL with unlabeled data containing unknown classes. SUSSL aims to enhance performance in both known and unknown classes expanding the potential applications of SSL. 2. The proposed method is welldefined and technically sound. It incorporates a new loss function for classifying similar samples into the same unknown classes and an adaptive threshold to protect performance in known classes. These techniques effectively mitigate the relevant challenges. 3. Extensive experiments demonstrate SUSSLs effectiveness. It outperforms comparable SSL openset SSL and NCD methods on three benchmarks. Ablation studies confirm the value of its component techniques. Weaknesses: 1. Terminology could be enhanced (e.g. standardizing references to \"unseen classes\"). 2. Exploring the impact of unknown class ratios on performance (e.g. how performance changes with varying proportions of unknown classes in unlabeled data) would be beneficial. Questions: 1. How does the performance of SUSSL vary with different ratios of unlabeled data belonging to unknown classes", "Paraphrased Statement: Summary: This study presents an enhanced SSL approach capable of classifying both known and previously unseen classes. The authors assume that the unlabeled dataset may contain classes not present in the labeled dataset making this a relevant problem to investigate. To tackle this they introduce techniques for unseen class classification based on pairwise similarity and pseudolabel selection using an adaptive threshold. The methods effectiveness is demonstrated through experimental evaluations on various datasets. Strengths: 1. Novelty: The approach focuses on the unique problem of unseen classes in unlabeled data aiming to classify them automatically. 2. Technical Soundness: The pairwise similarity objective and adaptive thresholds provide effective mechanisms for discovering new patterns and balancing the learning of known and unseen classes. 3. Convincing Results: The evaluations show performance improvements over existing methods highlighting the benefits of these techniques. Weaknesses: 1. Dependence on FixMatch: The unsupervised component seems to rely heavily on the FixMatch algorithm. It remains unclear if the proposed techniques are applicable to other SSL methods. 2. Typos and Grammar: The paper could benefit from additional proofreading to correct any languagerelated errors. Questions: 1. Can the proposed approach be extended to other SSL methods such as MixMatch or ReMixMatch 2. Why do the NCD methods sometimes perform worse than the baseline SSL method on unseen classes 3. Could the performance on standard SSL tasks be improved by replacing the fixed threshold in FixMatch with the proposed adaptive threshold", "Paraphrase: This study introduces SUSSL a novel SSL technique that automatically categorizes unseen classes while ensuring the safety of viewed classes. Strengths: Clarity and coherence of writing Clear structure and understandable figures Captivating and intuitive visualizations Weaknesses: 1. Limited Novelty and Motivation: The adaptive threshold a key contribution lacks significant novelty and clear motivation. Related SSL methods with similar concepts (e.g. [1] [2]) should be discussed in more detail. 2. Similarity to FewShot Learning: SUSSLs handling of unseen classes resembles fewshot learning. This connection should be explored and addressed. 3. Insufficient Analysis of Adaptive Threshold: Ablation studies and analyses of the adaptive threshold would provide valuable insights (e.g. advantages in specific SSL scenarios). Specific threshold changes and visual representations before and after using the adaptive threshold should be presented. 4. Dataset Selection: The use of Imagenet100 raises questions about the relevance of largescale dataset validation on the full Imagenet. 5. Framework Complexity: Figure 2 presents multiple components making it difficult to comprehend the overall framework. Additional explanations would be helpful. 6. Limited Experimental Evaluation: Comparison with more recent stateoftheart SSL techniques is lacking. Only ORCA is included in the presented table."], "maSvlkPHc-k": ["Paraphrase Summary This research examines how imputing missing features using the average (mean aggregation) impacts fairness between marginalized and dominant groups represented in graphs. It concludes that this imputation method can worsen fairness. To address this issue a corrected imputation algorithm is proposed which takes into account feature fairness during updates. Strengths 1. The study identifies two mechanisms through which unfairness propagates from feature imputation to model predictions: from input to output and through iterative mean aggregation imputation. 2. A compensation algorithm is introduced to rectify discrimination across groups during each iteration demonstrably achieving epsilonfairness. 3. Empirical results reveal the tradeoff between model accuracy (utility) and fairness as expected. Weaknesses 1. The fair graph feature imputation algorithms mechanisms are not readily comprehensible. 2. Theorem 2s contraction coefficient (alpha) is difficult to interpret due to the absence of an explicit form for matrix T. This makes it challenging to assess its impact. 3. Theorem 2 suggests that discrimination risk is limited by initial differences within each group. This implies that diversity within marginalized or dominant groups could potentially harm fairness. 4. Theorem 3 implies that epsilonfair computation does not affect the convergence rate of the imputation process which is determined solely by the eigenvalues of the graph topology. This could suggest that epsilonfair computation may be as effective as standard mean aggregation feature imputation with sufficient data.", "Paraphrased Statement: Summary: The researchers investigate fairness issues in feature imputation within groups where differences between minority and majority groups are typically larger than within the majority group. They present experiments on both synthetic and realworld datasets that balance fairness and utility. Strengths and Weaknesses: The paper is wellwritten with both theoretical insights and comprehensive experiments. The authors candidly discuss the limitations of their method on realworld datasets. The problem is novel as fairness in graphbased structures has received limited theoretical attention despite its practical significance. The authors introduce a discrimination measure that is applicable beyond specific models or graphs and aligns more closely with statistical parity commonly used in fairness assessments. While the main theorems and proofs have been reviewed the supplementary material has not. Questions: (A) If group membership of applicants is unknown how does the algorithm function Is group information solely used for discrimination risk assessment or also for feature imputation (B) How are correlated features handled in the imputation process given that occupation title is linked to salary", "Paraphrased Statement: Summary: This research explores challenges related to fairness in machine learning models that operate on graph data with missing values. The focus is on fairness between two groups: a marginalized group (Q) and a dominant group (R). The paper aims to establish theoretical connections between the Total Variation Distance (TVD) and a measure called Discrimination Risk (DR). DR quantifies the difference between the mean imputed feature values for marginalized and dominant groups. The paper then proposes an algorithm for iteratively imputing missing data while controlling DR. The performance of this algorithm is evaluated on synthetic and realworld datasets. Strengths and Weaknesses: Strengths: Tackles an important issue of fairness in graph data imputation. Proposes a novel algorithm to control discrimination risk. Provides a sound evaluation protocol. Weaknesses: The concept of Discrimination Risk (DR) is not clearly defined and its significance is unclear. The theoretical results depend on restrictive assumptions about the data distribution. The papers presentation lacks consistency and some sections are difficult to follow. Questions: Why does the paper use the term \"a\" in line 172 of p. 4 How can the TVD inequalities in Lemma 1 be interpreted given the different supports of the lefthand and righthand sides", "Paraphrase: Summary Researchers investigated how filling in missing features in graphs affects the fairness of machine learning models. They defined \"discrimination risk\" (the difference in features between groups) and proved that \"statistical parity\" (fairness measure) is limited by discrimination risk. They created a general imputation framework that includes existing methods and identified scenarios where it increases discrimination risk. They also proposed an \"\u03b5fair\" imputation method to minimize discrimination risk. While their method improved fairness on synthetic data it had no effect on realworld credit data. Strengths Analyzed the impact of graph feature imputation on model fairness a common problem in practice. Theoretically and empirically demonstrated how graph properties relate to discrimination risk providing insight into the impact of imputation methods. Proposed a solution to control discrimination risk in a general imputation framework. Weaknesses Did not explain when statistical parity is preferred over predictive parity and why it was used in this study. Analysis was limited to two groups unclear if results generalize to multiple groups. The impact of graph properties on the \u03b5fair imputation method was not fully explained. Limited empirical evidence supporting the efficacy of the proposed solution. Complex sentence structure made some passages difficult to understand. Questions In what scenarios is statistical parity preferable to predictive parity How generalizable are the results to multiple groups How do graph properties affect the \u03b5fair imputation method Can the proposed solutions effectiveness be further demonstrated on realworld data", "Paraphrased Statement: Summary: This research investigates the estimation error of fairness in situations involving missing data. It introduces \"discrimination risk\" as a metric for fairness assessment. The study also includes numerical simulations to confirm the theoretical findings. Strengths and Weaknesses: Weaknesses: 1. While the proposed solution claims to reduce discrimination risk and enhance fairness it may neglect the preservation of the underlying data distribution potentially impacting model effectiveness. 2. The theoretical findings may provide insights into fairness estimation behavior but their practical impact remains uncertain. 3. In Theorem 3 the inconsistency between RK (a scalar) and ZU (a vector or matrix) for the comparison in the PW and PB constraints requires clarification. 4. The study focuses on missing data in graph data but its applicability to other missing data contexts (MCAR MAR MNAR) is unclear. Questions: 1. Can the proposed solution guarantee preservation of the underlying data distribution after imputation 2. What is the evidence supporting the practical impact of the theoretical findings 3. Can you explain the inconsistencies in the constraints for RK and ZU in Theorem 3 4. Are the findings applicable to all three missing data contexts (MCAR MAR MNAR)", "Paraphrased Summary: This paper examines the potential for unfair bias introduced by imputing missing feature values in graphs. A metric for assessing this bias is developed based on the disparity in expected feature values between marginalized and dominant groups. The authors propose a generalized method for mean aggregation feature imputation and analyze its effects theoretically and experimentally. They use special cases of this method and introduce a modified version to address bias within an acceptable margin. Theoretical evidence supports their claims and simulations using stochastic block models provide additional support. However results on realworld data do not fully align with expectations necessitating further research. Strengths and Weaknesses: Originality: Strength: Highlights the potential for discrimination in graph neural networks due to missing feature values. Weakness: Does not convincingly demonstrate that this issue exists in realworld scenarios or that their method effectively addresses it. Quality and Clarity: Strength: Wellwritten and wellcited. Mathematical explanations are clear. Weakness: Some claims become difficult to follow later in the paper and readers are frequently directed to the appendix for further details. Significance: Strength: The problem of discrimination in machine learning applications is important and wellmotivated. Weakness: Results on realworld data are inconclusive suggesting that the issue may not be prevalent or their method may not be as effective as expected. Questions: Does paper [15] address node feature imputation How does the Mean Aggregation Feature method incorporate Graph Regularisation Which maximum deviation is considered in Theorem 2: from the group mean or between feature values Why do the example datasets lack missing features How realistic is the simulation of missing features by randomly removing them"], "kuJQ_NwJO8_": ["Paraphrased Summary: This research presents a novel approach to dialogue generation utilizing knowledge graphs (KGs). The method involves selecting triplets constructing and retrieving subgraphs encoding graphs and employing a basic encoderdecoder framework. Contrastive learning enhances the alignment between graphs and text during generation. A KQA metric enables the assessment of generated dialogues factual accuracy. Experimental results on a benchmark dataset demonstrate the effectiveness of the proposed approach. Strengths: Introduces an innovative scheme for subgraph construction and retrieval. The graph encoding considers triplet order and inverse relations. Employs a flexible evaluation metric for dialogue generation. Weaknesses: Lacks comparisons to stateoftheart methods in triplet selection. Fails to evaluate different graphbased neural networks for encoding. The definition of p(Zx) in the paper may not represent a probability. Absence of empirical comparisons with other triplet ranking methods or approaches for assessing dialogue generation performance. Limited to a single dataset raising concerns about the approachs generalizability. Grammatical errors require thorough proofreading.", "Paraphrased Statement: Summary: KnowledgeAugmented Dialogue Response Generation with SURGE SURGE is a method for generating consistent dialogue responses by incorporating knowledge from knowledge graphs. It includes three key components: Contextual Knowledge Retriever: Extracts relevant knowledge from a graph based on the dialogue context. Invariant Graph Encoding: Encodes contextual and knowledge information jointly in a way that is invariant to the order of facts or entities. Contrastive Learning Objective: Encourages model encodings to match their decoded output when given the same knowledge graph and to differ when given different graphs. When evaluated on the OpenDialKG dataset SURGE outperforms other methods in generating responses that are consistent with the context and knowledge. Human evaluations and ablation studies support the methods effectiveness. Strengths and Weaknesses: Originality: Novel combination of existing techniques (e.g. retrieval augmentation contrastive learning) Invariant graph encoding is a unique contribution. Quality: Wellsupported claims and sound technical approach Significant performance improvements over baselines Ablation studies validate the proposed methods. Clarity: Wellwritten paper with clear explanations of methods and rationale Compact figures and tables may hinder readability. Significance: Provides reusable methods for incorporating knowledge graphs into dialogue systems. Highlights the benefits of a welldesigned pipeline for knowledgeaugmented response generation. Questions: Why are knowledge graphs more efficient than unstructured knowledge sources Is it possible for the encoder in a transformer to be permutation insensitive In what specific situations is contrastive learning more beneficial than semisupervised learning for SURGE", "Paraphrased Statement: Summary: The research aims to prevent the generation of factually incorrect responses by incorporating knowledge from a knowledge graph (KG) into conversations. It selectively retrieves only relevant sections of the KG to avoid unnecessary processing and proposes a graph encoder that maintains certain characteristics of the graph during encoding. Strengths: The proposed graph encoding method achieves two types of invariance: permutation invariance and relation inversion invariance. Weaknesses: 1. Insufficient baseline comparison: The study fails to compare or discuss some established and contemporary methods including [1] [2] and [3]. 2. Lack of ablation study: The effectiveness of the new graph encoder is uncertain due to the absence of an ablation experiment."], "rZalM6vZ2J": ["Paraphrased Statement Summary This paper investigates the PAC (probably approximately correct) learning task while preserving differential privacy. It introduces private algorithms that achieve the optimal sample size requirements. Strengths Clear presentation and motivations for techniques. Demonstration of an asymptotically optimal sample complexity for the DP PCA algorithm. Presentation of concrete examples for the lower bound particularly highlighting the impact of concentration assumptions. Weaknesses Ambiguities in the algorithm and proof. Questions 1. Why does Algorithm 3 apply subsampling to split data Is instead of the input Ai Is it due to the focus on asymptotic properties and the irrelevance of using batches of size nlogn for logn iterations 2. Can the Gaussian tail assumption (A4) be relaxed utilizing heavytailed mean estimation as in Algorithm 5 to implement Algorithm 3 While A4 modifications may result in a weaker dependence on data variance some bound may still be derived even if not optimal. 3. Can the lower bound construction techniques be adapted to scenarios where only the second or fourth moment of the tail is assumed Specifically for heavytailed data can the authors provide insights on the utility bound", "Paraphrased Statement: Principal Component Analysis (PCA) is a statistical tool used in various applications. Differential Privacy (DP) is a mathematical concept that ensures privacy in data sharing. This research introduces DPPCA an algorithm that combines the benefits of PCA with DP. It demonstrates that DPPCA achieves a bounded error for data similar to Gaussian distributions. Strengths: DPPCA privatizes Ojas PCA algorithm and achieves nearoptimal error for strictly Gaussian data. It addresses two limitations of DP: the restricted range of \\epsilon and excessive noise. DPPCA relies on minibatch stochastic gradient descent (SGD) and private mean estimation to overcome these challenges. Weakness: The paper lacks a comprehensive analysis of the algorithms weaknesses.", "Paraphrase: This paper explores Principal Component Analysis (PCA) with Differential Privacy (DP) constraints. The authors propose a novel singlepass algorithm that significantly improves sample complexity compared to previous approaches. This algorithm leverages a minibatch SGD approach that combines private top eigenvalue and mean estimation. Strengths: The authors address limitations in existing PCA with DP approaches. They provide both upper and lower bounds for their algorithm demonstrating its nearoptimality under certain assumptions. The paper is wellstructured and provides clear explanations. Assumptions and justifications are wellgrounded. Weaknesses: There is a potential error in the proof for the lower bound (Theorem 5.3) which may impact the claimed nearoptimality. The paper focuses on computing only the first principal component while other methods can handle multiple principal components. There is a typo in Theorem 2.2 and another typo in Line 178. There is a discrepancy between the sample complexity terms in Theorems 6.1 and 5.1. Questions: The rate in Theorem 6.1 includes a term (dlog(1\u03b4)) thats not present in the rate in Theorem 5.1. Is this a typo"], "pIYYJflkhZ": ["Paraphrase: Summary: This study examines anomaly detection (AD) in noisy environments. The authors present SoftCore a new AD technique that improves model performance and resilience to noise. The model uses a selective sampling strategy to adjust the weights of coreset samples and eliminate noise from affected images. Strengths: Clear and concise presentation. Practical relevance due to the investigation of noises impact on AD. Welldesigned experiments that demonstrate the impact of noise on AD. Verified effectiveness of the proposed method. Weaknesses: Grammatical errors: Line 156: \"W(i)\" should be \"W(i)\" Line 268: \"LOF achieve\" should be \"LOF achieved\" Notation could be improved particularly the use of \"\\sumhw\" for sample variance in Eqs. (2) and (3). Insufficient explanation of the critical threshold \"\u03c4\". Questions: Please address the noted weaknesses particularly with regard to notation and the critical threshold \"\u03c4\".", "Paraphrased Statement: This paper introduces a memorybased unsupervised anomaly detection method that leverages patchlevel data denoising. The method employs noise discriminators to identify and remove outlier noise within patches before constructing the coreset memory bank. Compared to existing methods the proposed algorithm demonstrates improved performance. Strengths: The patchlevel denoising strategy for the coreset memory bank aligns with common practices for noise removal. The method establishes a practical test environment for unsupervised anomaly detection. The experimental results provide adequate support for the effectiveness of the method. Weaknesses: The evaluation lacks testing on realworld noisy datasets which undermines the applicability of the method. The study does not include a computational analysis to assess the efficiency of the algorithm. Questions for the Authors: 1. Can you provide insights into how your method would perform on realworld noisy datasets 2. Could you include a computational analysis in your future work to quantify the efficiency of the algorithm", "Paraphrase: The authors introduce SoftCore an unsupervised anomaly detection model. It identifies outliers in training data creating an outlierfree \"memory bank\" used as a benchmark for normality. Novel anomalies in test data are detected by comparing them to this reference. The model uses Local Outlier Factor (LOF) for training data contamination detection. Experiments on the MVTec dataset demonstrate SoftCores superiority over baselines particularly in noisy training conditions.", "Paraphrase Summary The authors propose a new setting for unsupervised anomaly detection by adding noisy data to the training set. They introduce a denoising mechanism and explore three sampling strategies to distinguish between noisy and normal data. The results in this new setting show improvements. Strengths and Weaknesses Strengths: Novel task of adding noisy data to simulate realworld scenarios. Clear motivation and proposal. Extensive experiments to validate the methods. Weaknesses: Lack of significant novelty the method resembles PatchCore with a denoising step added to the memory bank. Confusing Overlap experiment setting unfair for methods to train and test on the same data. Minimal performance decline on stateoftheart methods in the No overlap setting and the comparison between the proposed method (98.6) and PatchCore (98.4) fails to demonstrate the benefits of the new approach. Questions 1. Further explanation of the weaknesses mentioned is needed. 2. Figure 2 requires clarification including definitions for N H W C and a correction to the misleading arrow indicating outlier factors of all selected patches that includes noisy data. 3. Analysis of the neighbors influence on local outlier factors is lacking. 4. English statement improvement is suggested such as replacing \"too long a training stage limits its usage.\" with a more precise phrasing on page 3 line 94."], "xz-2eyIh7u": ["Paraphrase: Collaborative Bandit Problem with Adversaries: This paper addresses a scenario where multiple agents collaborate to solve a bandit problem but a fraction of them (\u03b1) may behave arbitrarily. The authors propose the RCLB algorithm which achieves regret bounds for wellbehaved agents of the order O(\u03b1\u221aM\u221aT) where M is the total number of agents and T is the number of rounds. The paper also demonstrates that a regret bound of O(\u03b1\u221adT) is unavoidable regardless of M indicating that RCLB is nearly optimal. The authors extend these results to generalized linear bandits (GLM) and contextual linear bandits (CLB) achieving nearoptimal guarantees. Strengths: First to consider adversaries in collaborative bandit problems demonstrating the possibility of achieving O(\u221aMdT) regret despite their presence. RCLB does not add significant complexity compared to existing algorithms. The authors prove that the regret bound is worstcase optimal. The algorithm is the first to provide nearoptimal guarantees for GLM and CLB with adversaries. Weaknesses: RCLB is computationally inefficient due to its reliance on phased elimination. The noise model is limited to independent Gaussians. Questions: Can RCLB be made more computationally efficient Can it handle more general noise models such as conditionally 1subGaussian or bounded noises", "Paraphrased Statement: The paper explores a collaborative linear bandit problem with adversarial agents where a fraction of agents (\u03b1) can behave adversely. The challenge lies in balancing collaboration which can reduce sample requirements with the potential negative impact of adversarial behavior. The paper introduces a novel phased elimination algorithm that provides tight regret guarantees in this setting. Results are also extended to related settings of generalized linear bandits and contextual bandits. Strengths and Weaknesses: Strengths: Introduces a unique adversarial collaboration setting. Presents a novel phased elimination algorithm with strong theoretical performance. Extends results to other bandit settings. Weaknesses: Assumption of a clear distinction between adversarial and nonadversarial agents may not fully reflect practical scenarios. Lacks simulations which would provide a more intuitive comparison with noncollaborative approaches. Limitations of \u03b1: The algorithm assumes \u03b1 0.5 as it relies on median of means techniques that require a majority of nonadversarial agents. If \u03b1 \u2265 0.5 the bound on regret (1\u03b1)M\u221aT still holds indicating the robustness of the algorithm to a certain level of adversarial behavior. Suggestions for Improvement: Consider removing Corollary 1 and using the space to provide more intuition on constructing robust arm payoff estimates and confidence intervals. Improve the introductory paragraph to clarify the collaborative nature of the problem.", "Paraphrase: Summary This research addresses a collaborative linear bandit problem involving adversarial agents. The authors introduce new robust algorithms for both the linear bandit and general linear bandit problems ensuring a regret bound of (\\alpha1\\sqrtM)\\sqrtdT for each good agent. Furthermore they establish a theoretical lower bound for adversarial regret demonstrating the inevitability of \\alpha \\sqrtT regret. Strengths and Weaknesses Strengths: 1. The paper provides theoretical proofs for the algorithms ensuring nearoptimal regret bounds without adversarial agents. The lower bound suggests that the adversarialdependent term \\alpha \\sqrtdT is close to optimal in terms of T and \\alpha. 2. The authors present a theoretical guarantee for the lower regret bound in this context. Weaknesses: 1. The algorithms require Gaussian stochastic noise which is more restrictive than the subGaussian noise considered in previous linear bandit research. 2. For the contextual linear bandit and general linear bandit settings the algorithms assume that all agents share the same feature vectors which is impractical in realworld scenarios. Questions: 1. The authors reference recent progress in regret guarantees for adversarial corruption bandit problems but do not discuss more recent works involving function approximation techniques. He et al. [2022] has achieved nearoptimal regret bounds in this area which should be acknowledged. 2. Since all theorems focus on regret for individual good agents considering total regret in equation (11) may not be essential.", "Paraphrased Statement: This study introduces a type of collaborative linear bandit where a portion of agents may act maliciously in a centralized system. A phased elimination algorithm is proposed which estimates the malicious agents corruption fraction accurately and eliminates them while minimizing the cumulative loss for the group. The algorithm achieves an upper bound on regret and a lower bound analysis shows that the unavoidable term in the regret bound is dependent on the fraction of malicious agents and the number of agents involved. Extensions to generalized linear bandits and contextual bandits are also explored. Strengths: Introduces a novel problem in collaborative bandit learning with adversarial agents reflecting realworld scenarios. Proposes a Robust Collaborative Phased Elimination algorithm which handles both statistical uncertainty and adversarial behavior through robust confidence intervals. Provides regret analysis and a lower bound demonstrating the optimality of the algorithm. Extends the algorithm to generalized linear bandits and contextual bandits. Weaknesses: Some sentences may be lengthy or complex. The \"Related Work\" section is extensive and could be condensed for clarity. The assumption of known corruption fraction may not always hold in realworld applications. The reason for approximating Goptimal design when it can be solved efficiently is unclear."], "wN1CBFFx7JF": ["Paraphrase: Summary: This study derives bounds on the generalization performance of (sparse) feedforward neural networks with ReLU activation functions. The model was initially introduced by SchmidtHieber (2020) but this study assumes a nonindependent and identically distributed (noni.i.d.) dataset. Specifically the data sequences are considered to be stationary and exponentially \\alphamixing processes which encompass time series models like autoregressive models. The results indicate that a multiplicative term of order \\log3 n is added to the generalization bound presented in SchmidtHiebers work (for i.i.d. datasets). This is attributed to the increased difficulty in prediction due to correlations among samples in the dataset. The study provides numerical results and an application to a macroeconomic dataset to support its theoretical findings. While the main framework of this work (including proofs) originates from SchmidtHiebers study it incorporates modifications in Bernstein inequalities to accommodate the dependence among samples. More precisely the Bernstein inequality for i.i.d. processes used by SchmidtHieber is replaced with two versions in this study: 1. Bernstein inequality for \\alphamixing processes (cf. line 68) 2. Bernstein inequality for martingale difference sequences (cf. line 26) These modifications are based on the assumptions in this study and algebraic manipulations. Lemma 3 is a significant contribution of this work. Strengths and Weaknesses: This work combines established techniques with novel elements involving concentration inequalities. The strengths of this study include the originality of its findings and contributions in utilizing new Bernstein inequalities from a technical perspective. However the weakness lies in its reliance on assumptions 13 which may limit its applicability. Additionally the deep neural (worstcase) generalization networks with temporally dependent observations including the Autoregressive model have been addressed in existing research. These related results should be cited. Questions: 1. Explain in detail the derivation of (2k1)var(Yn0) in line 23. 2. Verify if the sum in Lemma 2 should be 2\\sumn\\geq i0cov(Yn0Yni) since the expression in line 22 only holds for i\\leqn. Examine if these adjustments affect the results. 3. In Lemma 3 the \\mathcalN(\\delta\\mathcalF\\lVert\\cdot\\rVert\\infty) should be defined as the \\deltacover of \\mathcalF under the \\lVert\\cdot\\rVert\\infty norm. 4. In the proof of (II) in Lemma 3 proof change \\hatf to \\tildef. 5. Define the \\sigmaalgebra \\mathcalFt in line 89 as \\mathcalFt\\sigma((\\mathbfXi)i\\leq t) instead of \\mathcalFt\\sigma((\\mathbfXi\\epsiloni)i\\leq t). Assuming that (\\epsiloni i\\in[n]) are independent in Assumption 1 the inequalities between lines 92 and 93 become more easily verifiable. Additionally Assumption 1 should also include \\mathbbE[\\epsiloni]0 to ensure that expressions in the proof hold (e.g. \\mathbbE[\\epsiloni f0(\\mathbfXi)]0 in line 83).", "Paraphrase: This research study explores the theoretical foundations of deep neural networkbased nonparametric regression techniques for analyzing data with dependencies. It establishes nonasymptotic bounds for prediction errors demonstrating the accuracy of these techniques. Through numerical studies and realworld applications the validity of the theoretical findings is verified. Weakness: While the study offers upper bounds for the DNN estimator it appears to be an extension of results derived for independent and identically distributed (I.I.D.) observations to the case of dependent data. Questions: To apply the DNN estimator optimization is required under the constraints of maximizing the weight vector norm \\f\\\\infty\\leq F and limiting the weights by a tuning parameter s denoted as \\0\\vi\\0\\leq s. It is unclear how this optimization should be performed in practice.", "Paraphrase: Summary: This study derives nonasymptotic bounds on the prediction error of using feedforward neural networks (FFNNs) with ReLU activation for time series data exhibiting temporal dependence. These bounds assume uniform bounds on network parameters and a model capable of representing finite autoregressive (AR) processes. Experiments demonstrate the performance of the FFNN structure and compare it to linear regression. Strengths and Weaknesses: The paper provides valuable theoretical bounds for FFNNs in the context of temporally dependent data. The code is provided for reproducibility. However the bounds may not be optimal as experiments show minimal difference between temporally dependent and independent cases. Questions: How can the bounds be tightened for better accuracy In the linear AR examples wouldnt linear regression be sufficient to learn the true model If so why is the approximation error still high in Figure 2"], "pZtdVOQuA3": ["Paraphrased Statement: A sampling method called Monte Carlo is used to improve the efficiency of rendering density fields represented using the Neural Radiance Field (NeRF) approach. Heres how it works: Step 1: Query the density field densely to generate an inverse Cumulative Distribution Function (CDF). Step 2: Use the inverse CDF to reduce the number of subsequent samples required for the color network. This method was tested against the original NeRF algorithm and achieved: Faster rendering speeds: Around 20 times faster. Slightly lower image quality: A 0.7dB decrease in Peak SignaltoNoise Ratio (PSNR). Strengths: Tackles the important issue of improving NeRF rendering efficiency. Method is wellconceived and reduces the sampling rate of the color network significantly. Weaknesses: Still requires dense sampling of the density network limiting the overall efficiency gain. Missing citations to related works that address similar challenges. Experiments are limited with only the Lego scene tested. Typos need to be corrected. Questions: Could using a smaller density network further improve efficiency Including missing references and expanding the experiment section would strengthen the paper.", "Summary This paper presents a modified volume sampling method for faster rendering of neural radiance fields (NeRFs). The technique involves reparametrizing volume sampling to enable importance sampling when density values are known at sparse points. This approach reduces variance and accelerates NeRF rendering. The paper demonstrates lower variance and increased speed compared to uniform sampling as well as novel view synthesis with varying sample counts. Strengths and Weaknesses Strengths: Addresses an important problem in neural rendering: accelerating NeRF rendering. Weaknesses: Presentation: Typos and inconsistent notation hinder clarity. Evaluation: The method is not properly evaluated against the original NeRF implementation with importance sampling. The proposed method modifies the original NeRF architecture reducing the quality of the density field and making the comparison unfair. Evaluations are limited to a single scene with marginal improvements. Efficiency: The efficiency of the proposed method compared to the hierarchical volume sampling of the original NeRF paper is not clearly demonstrated. Both methods require a coarse sampling stage and reducing the computational cost for the proposed method can also be applied to the original NeRF. Questions What specific advantages make the proposed method more efficient than the hierarchical volume sampling of the original NeRF paper Why is the modified NeRF architecture used in the evaluation and how does it affect the comparison", "Paraphrase: Summary: This proposal introduces a differentiated importance sampling technique to approximate the rendering equation used in NeRFbased methods. The approach leverages the reparameterization method to derive the sampling strategy. The gradient descent is also derived using the same method. This improved sampling method significantly reduces sampling variance compared to previous sampling methods used in NeRFbased approaches under similar computational constraints. Strengths: The presented importance sampling method for Monte Carlo estimation is theoretically sound based on the reparameterization method. The authors demonstrate how to make the importance sampling along the ray differentiable by utilizing reparameterization. This allows the sampling method to be integrated into the neural rendering pipeline and facilitates endtoend training of the implicit function. The proposed sampling method exhibits significantly lower sampling variance compared to existing sampling methods in NeRFbased techniques. The density field is approximated using closedform splines enhancing the efficiency of light attenuation calculations along the ray compared to previous methods that required additional integration along the ray. Weaknesses: Despite the theoretical soundness and promise of the sampling method the paper lacks experimental validation of its effectiveness. The method is only evaluated on a single scene (Lego) and compared solely to the baseline NeRF. Alternative methods that accelerate NeRF without compromising performance (e.g. Plenoxel NeRF) have not been considered for comparison. It is unclear how the proposed method compares to these approaches and whether it can be integrated for further speed enhancement. The paper lacks experiments examining the tradeoff between speed and reconstruction quality. It is uncertain how the reconstruction quality degrades with reduced sampling (i.e. increased speed) using the proposed method. Related to the missing speedquality tradeoff Table 2 shows that the estimation time per iteration increases with the number of points in the splines exceeding the baseline at around 16 points. The paper does not include a comparison of reconstruction quality at approximately 16 sampling points with the baseline. The proposed method lacks comparison to other stateoftheart (SOTA) NeRF techniques in terms of reconstruction quality computational efficiency and memory consumption. Paper Refinement: The paper contains several question marks due to reference errors. Questions: Regarding spline construction (L128L130) how accurate will the spline approximation be if the underlying density field is not piecewise linear or contains sharp spikes along the ray (e.g. thin structures) How are the sampled grid points t0...tm (L131L133) obtained initially Are they predefined If so how is it ensured that they cover all nonempty space across different views and scenes"], "wiBEFdAvl8L": ["Paraphrased Statement: This paper introduces GLIPv2 a new VisualLinguistic (VL) model that builds on the earlier GLIP v1 model. The focus of GLIPv2 is to combine visual localization tasks (such as object detection) with VL understanding tasks (like visual question answering). To achieve this three key pretraining tasks are employed: 1. Phrase grounding where the model aligns image regions with text tokens. 2. Masked language modeling where tokens in a sentence are randomly masked and the model is trained to predict them. 3. Masked language modeling where tokens in an image are randomly masked and the model is trained to predict them. Unlike other VL models GLIPv2 does not use taskspecific classification heads during pretraining. In finetuning for downstream tasks GLIPv2 exhibits competitive performance. Additionally it can be utilized in zeroshot and prompttuned settings. Strengths: Openvocabulary capability thanks to the \"classificationtomatching\" approach used by CLIP. This allows GLIPv2 to handle novel and outofdomain visual classes. Impressive results in zeroshot and prompttuning experiments showcasing the models ability to generalize. Weaknesses: GLIPv2 relies on GLIP (v1) for bounding box generation in pretraining data rather than an external object detector. This poses limitations and raises questions about data bias. The text transformer in GLIPv2 may be sourced from the pretrained CLIP model giving an unfair advantage due to prior visual supervision. Questions: The dependence on pretrained CLIP weights for the text transformer warrants further clarification. The authors thoughts on using another large multimodal model for pretraining data generation would be valuable. Suggestions: Condensing the prior work section would enhance readability and make more room for discussing GLIPv2s contributions. Redundant statements in Section 3.1 could be removed for clarity. Addressing minor typos throughout the paper would improve the overall presentation.", "Paraphrase: Summary: This research introduces \"GLIPv2\" a framework that combines several localization and visionlanguage (VL) tasks. Pretraining on localization data using this framework improves downstream model performance on all tasks and achieves stateoftheart results on many of them. Additionally the framework incorporates an intersample contrastive loss that boosts performance. Strengths: Enhances performance on various localization and VL tasks surpassing previous benchmarks. Wellwritten and organized presentation. Demonstrates both finetuning with taskspecific heads and prompt tuning with shared weights for multiple tasks. Conducts thorough ablation studies on pretraining objectives and dataset combinations. Weaknesses: Novelty concerns due to similarities with existing loss terms. The use of additional localization data generated by GLIP potentially influencing performance comparisons. The potential use of a teacherstudent setup between GLIP and GLIPv2 models. Unclear methodology for the interimage regionword loss. The possible presence of traintest overlap in Table 2. Questions: Is the MLM objective trained alongside other objectives in Table 4 (Row 6) or is there an additional pretraining step What is the impact of the new loss on training time Did the authors compare the frameworks performance on vision classification tasks given their discussion of its differences from CLIP", "Paraphrase: This paper proposes an enhanced GLIPv2 model that combines visionlanguage grounding tasks. The model unifies localization and understanding tasks by framing them through the lens of grounding. It also introduces a novel \"inter imagetext token loss\" to enhance performance. Strengths: The proposed losses and framework are innovative and crucial for visionlanguage research. The unified model can perform both grounding and understanding tasks making understanding tasks more grounded and interpretable. The method outperforms its predecessor GLIP in both grounding and understanding tasks. Weaknesses: The methods changes are primarily focused on the inter loss which has a limited contribution to performance. The performance improvements over GLIP are modest. Questions: The paper claims GLIPv2 simplifies pretraining by eliminating a localization model but doesnt it still rely on pretrained detector labels for localization The paper excludes positive matches between groundingtype texts and images. Were there any experiments exploring the inclusion of these positive matches Style and Writing Comments: The grammar and sentence structure are inconsistent throughout the paper. Some line breaks and headings could be improved for clarity. Vocabulary choices should be more formal such as \"classifier\" instead of \"classier.\""], "rG7HZZtIc-": ["Paraphrase: Summary: This paper offers a set of methods for separating moving objects from still backgrounds using only singlecamera videos effectively restoring the background and removing shadows. Solving the issue of interference from moving objects in 3D vision has been a persistent challenge. The papers approach which combines a skewed entropy loss with decoupled NeRF and a separate shadow field is notable for its simplicity and effectiveness. Strengths and Weaknesses: Strengths: The skewed entropy loss successfully distinguishes between static and moving components. The shadow field differentiates between timevarying shadows and static appearance. The proposed method significantly outperforms existing techniques in decoupling performance and NeRF quality. Questions: The skewness parameter (k) is crucial as it affects the proportion of \"dynamic\" content but it is sensitive to parameters warranting potential for automated tuning. The concept and mechanism of the skewed entropy loss resemble that of the beta distribution introduced in [1] highlighting the need for a comprehensive comparison and discussion. Key related literature on NeRF in dynamic settings such as [1] and [2] is omitted from the paper. The notion of a shadow NeRF has been previously explored in [3] which should be included in the literature review.", "Paraphrased Summary: This paper introduces a new selfsupervised method for separating moving objects from a static background. The method uses several techniques to address challenges in training including skewed entropy regularization static regularization and shadow ratio integration. Strengths: The proposed techniques are innovative and valuable to the research community. The visual results are impressive. The manuscript is wellwritten. The task of isolating dynamic objects from static backgrounds is important and the proposed approach provides highquality solutions. Weaknesses: The method requires scenespecific tuning of hyperparameters which may limit its robustness. The sensitivity of the proposed approach to hyperparameter changes is unclear. The supplementary material includes 9 sets of hyperparameters for 19 scenes showing the need for perscene tuning. Quantitative results with a single set of hyperparameters would be beneficial to demonstrate the robustness of the approach. Questions: The paper should provide a more thorough analysis of the hyperparameter sensitivity to assess the robustness of the proposed approach.", "Paraphrase: Summary: This study introduces D2NeRF a selfsupervised technique that uses a monocular video to create a 3D scene representation that separates moving objects (including their shadows) from a static background. It also presents a new loss function to ensure proper separation of static and dynamic events. Furthermore the study introduces a shadow field network to identify and isolate moving shadows. A new dataset has been developed that includes various moving objects and shadows. Extensive testing shows that the proposed method surpasses cuttingedge approaches in tasks such as separating dynamic and static 3D objects removing occlusions and shadows and segmenting images of moving objects. Strengths: Novel dataset for decomposing static and dynamic scenes Selfsupervised method for learning 3D scene representations that isolate moving objects including shadows New loss function for accurately separating static and dynamic phenomena Shadow field network for detecting and isolating moving shadows Stateoftheart performance in tasks such as decoupling dynamic and static 3D objects removing occlusions and shadows and segmenting images of moving objects Weaknesses: Requires precise camera calibration May struggle with highfrequency radiance changes Lacks comparison with other motion decoupling methods (e.g. STNeRF NSFF DynNeRF SIMONe STaR) Supplementary video shows instances of incorrect static and dynamic field decomposition May lack generalization due to training and testing on the same dataset Questions: Please include comparison results with other existing methods. Consider generalizing the method by testing it on different datasets.", "Paraphrased Statement: Summary: This paper devises a method that separates and isolates moving objects from a single camera video footage while retrieving the stationary setting. It enhances NeRF (Neural Radiance Fields) and its extension Hyper NeRF by refining the handling of shaded portions and introducing a loss function that promotes accurate distinction between dynamic and static regions. The method exhibits promising results in motion segmentation and shadow removal when compared to other NeRFbased techniques. Strengths and Weaknesses: This paper efficiently adapts NeRF for reconstructing scenes from monocular videos with moving foregrounds. It is wellwritten and provides substantial evidence to support its claims. The modifications made for handling shadows and foregroundbackground separation are inventive and appear effective. It has significant value for individuals researching similar issues and merits publication. Questions: 1. Line 226 states that no masks were used during realworld image registration using COLMAP to demonstrate the capability of fully selfsupervised scene decoupling. However in the reviewers experience COLMAP often estimates camera poses incorrectly when the foreground is sizable without masks leading to inaccurate static background reconstruction. The current claim seems contradictory given that the proposed method reportedly does not update camera poses during optimization. 2. Visualizations of the depth map for the reconstructed foreground and background would be beneficial. 3. The results for HyperNeRF appear significantly inferior to the proposed method even in dynamic areas. Given that the proposed method is not substantially different from HyperNeRF in dynamic regions the current results are somewhat surprising. The authors could provide additional insights into potential reasons for this discrepancy."], "mux7gn3g_3": ["Paraphrase: Summary: The authors review the pretrainandfinetune framework for reinforcement learning generalization and compare it to several MetaRL algorithms. The comparison is conducted on three visionbased RL tasks. The results show that pretraining and finetuning can often achieve similar or better performance than MetaRL algorithms. Strengths: Proposes a more practical evaluation benchmark for MetaRL research. Conducts comprehensive experiments comparing numerous algorithms and baselines. Presents clear and accessible writing. Weaknesses: Lacks detailed ablation studies to determine why MetaRL algorithms are less effective. Uses inappropriate baseline extensions for some MetaRL algorithms particularly Qvaluebased methods. Questions: Why were Qvaluebased MetaRL algorithms included despite most MetaRL research focusing on policybased algorithms How were the task settings chosen and why is the emphasis on observation space generalization rather than MDP generalization Could contextbased MetaRL algorithms be improved with more complex network structures or additional parameters Why was MELD not included in the comparison of visionbased algorithms Is the performance of the pretraining process itself reported", "Paraphrased Summary: This study evaluates popular metalearning techniques (Reptile Pearl RL2) against standard multitask pretraining with finetuning on three visionbased benchmarks (Procgen RLBench Atari). The study focuses on generalization to completely novel tasks not just variations of known tasks. Results indicate that multitask pretraining with finetuning performs as well as or better than metalearning approaches on all tasks. Thus the study suggests multitask pretraining with finetuning as a strong baseline for such tasks. Strengths: 1. Clear and wellwritten presentation. 2. Comprehensive experimental setup with diverse visionbased RL tasks. Weaknesses: 1. Lack of results for variations of the same task which could provide insights into the performance differences between metalearning and multitask pretraining. 2. Absence of additional baseline comparisons such as finetuning Pearl or RL2 during adaptation which could offer a fairer basis for evaluation. Questions: The reviewer recommends that the authors address the aforementioned weaknesses to enhance the studys value.", "Summary This paper evaluates the effectiveness of metalearning in reinforcement learning by comparing it to multitask learning with finetuning. Experiments using visionbased benchmarks (Procgen RLBench Atari) demonstrate that multitask learning with finetuning often outperforms metalearning approaches despite being simpler and less computationally expensive. Strengths Metareinforcement learning methods are fairly compared against simple multitask pretraining. Findings indicate that metareinforcement learning might not be the most optimal approach to transfer knowledge from previous tasks. Weaknesses Experiments were limited to visionbased benchmarks potentially limiting the generalizability of conclusions. The number of tasks in the experiment settings may not have been sufficient to fully assess the capabilities of metareinforcement learning. No concrete suggestions for improving metareinforcement learning methods are provided. Questions Could these findings extend to nonvisionbased benchmarks Is image representation learning a significant bottleneck in these benchmarks Why does Reptile underperform in Procgen experiments despite using finetuning during adaptation similar to multitask pretraining"], "s0AgNH86p8": ["Paraphrased Statement: TransBoost a new transductive learning loss function is proposed in the Summary Paper. It enhances performance in transductive image classification in domains with substantial data and test sets. TransBoost draws inspiration from TSVM and introduces a regularized term that distinguishes unlabeled samples based on similar class probabilities even when they should not. Comprehensive experiments reveal TransBoosts superiority across diverse architectures datasets and settings. Strengths: Clear and accessible writing style Innovative and novel concept of TransBoost Bridging the performance gap with tFSL methods Solid algorithmic choices and insightful motivations Extensive experiments demonstrating TransBoosts efficacy Weaknesses: TransBoost may be more beneficial for specific architectures Some architectures exhibit greater improvements than others suggesting TransBoosts suitability for particular models Grouping similar samples in the loss function may yield benefits but not to the same extent Transductive learning may involve sensitive optimization manifolds that require careful design Suggestions: Explore why TransBoost benefits Transformerbased architectures more than others Investigate the potential drawbacks of grouping similar samples in the loss function Elaborate on the implications of TransBoosts varying performance across architectures", "Paraphrase: Summary: This research explores deep transductive learning where training occurs with both labeled and unlabeled data. The objective is to enhance accuracy on the unlabeled test set. TransBoost is presented as a method to finetune deep learning models using the provided data. Comprehensive experiments support the efficacy of TransBoost. Strengths: Clear and structured writing Novel application of machine learning concepts to deep learning Thorough verification through experimentation Weaknesses: Deep transductive learning faces challenges with overfitting to the test set. TransBoost while avoiding the use of true test labels relies on pseudo labels derived from image similarities. This may lead to excessive performance gains in deep models. Questions: Concerns regarding overfitting in deep transductive learning Suggestion for defining a more specific task in this domain Reference to research in testtime training that utilizes selfsupervision", "Paraphrase: Summary: This paper introduces TransBoost a method for unsupervised deep learning. TransBoost incorporates the similarity and certainty of data pairs into its loss function aligning with the principle of maximizing the margin. The authors evaluate its efficacy with various pretrained neural network datasets and baseline models demonstrating its superior performance over inductive classification and existing SSL and tFSL methods. Strengths: Extensive experimentation on multiple neural network architectures and datasets shows significant improvements. Weaknesses: Submission format is lacking (no line numbers). Certain sections are difficult to comprehend. Clarity Concerns: In Section 4 Paragraph 3 the statement about incentivizing test sample pairs to differ in empirical class probabilities while respecting f\\thetas prior knowledge is unclear. Toy Example Queries: Section 4.2: (a) How is p(xf) computed using SVM (b) Despite TransBoost loss being lower in TSVM does its decision boundary effectively separate test instances Typographical Errors: Section 5.5: \"separating the different\" should be \"separating the difference\". Figure 2: \"... example indicating ...\" should be \"... example indicates ...\"."], "qm5LpHyyOUO": ["Paraphrase with Emphasis on Key Points: The study explores the hypothesis that a hybrid convolutiontransformer network can learn superior image representations using masked inputs compared to standard Vision Transformers (ViTs). However the masking approach proposed in previous research can be computationally costly when applied to hybrid models. This work introduces a novel multiscale blockwise masking strategy employing masked convolutions to efficiently train a hybrid model. Experiments demonstrate the effectiveness of the proposed technique across various vision tasks including classification object detection segmentation and video understanding. Strengths and Weaknesses: Originality: The multiscale hybrid convolutiontransformer encoder is a novel approach that combines the strengths of convolutions and transformers. Simplicity: The proposed framework is straightforward and effectively combines the two architectures. Hierarchical Representations: The masking scheme progressively upsamples masks to generate hierarchical representations resembling Feature Pyramid Networks. Limitations: The experiments primarily focus on ViTBscale models and runtime comparisons with existing techniques are limited. Questions: 1. How do the results scale for larger models such as ViTL and ViTH 2. Why are there limited data points for VideoConvMAE and VideoConvMAEmultiscale in Figure 3 3. Why were CoAtNet results excluded from the experiments", "Paraphrase: This study focuses on the challenges of using MAE training with layers of convolutions. To overcome this ConvMAE introduces masked convolutions in the early stages of the convolution layers. These convolutions are applied to masked feature maps preventing information leakage. This modified training method allows ViT models with early convolution layers to benefit from MAE training leading to improved transfer learning performance compared to standard ViT models. ConvMAE achieves impressive results on ImageNet and MSCOCO datasets. Strengths: Innovative strategy enabling MAE training for models with convolutional layers. Strong performance on various transfer learning applications. Weaknesses: The use of masked feature maps may introduce bias or artifacts in training due to the mask edges. Further research is needed to address this boundary issue.", "Paraphrased Statement: This paper introduces a new selfsupervised learning framework that combines hybrid convolutiontransformer architectures and masked convolution within masked autoencoders. This approach aims to optimize computational efficiency and minimize the gap between pretraining and finetuning. The frameworks effectiveness is demonstrated through experiments in various computer vision tasks. Strengths: Clear and concise writing with sufficient technical details Wellmotivated and straightforward method Innovative components to overcome computational challenges and pretrainingfinetuning disparity Applicability to both image classification and object detection Weaknesses: Similar performance to existing hybrid convolutiontransformer architectures in MAE Vague explanation of key idea and differences from previous work Unclear implementation of blockwise masking with masked convolutions Varying training epochs for different methods unknown if the proposed method maintains superiority with equal training PostRebuttal Thoughts: Concerns have been adequately addressed Increased rating and recommended acceptance", "Paraphrased Statement: This research presents a selftraining system that uses a combination of convolutional and transformer layers to extract image features at multiple scales. To prevent information leakage in convolutional layers we employ masked convolutions and to boost efficiency we blockwisely mask data. Our model excels in both image classification and object detection tasks. Strengths: Integration of the selfsupervised MAE approach with a hybrid convolutiontransformer architecture Improved model performance in classification and dense prediction tasks compared to prior selfsupervised models Questions: How does the duration of the pretraining phase affect the backbone models performance What is the performance penalty of shortening the training duration such as reducing it from 1600 to 800 epochs", "Paraphrased Statement: Summary: The paper introduces ConvMAE a hybrid architecture combining convolutional and transformer layers designed for pretraining with Masked Autoencoders (MAE). While MAE was initially developed for transformers its application to convolutional networks proved challenging due to the absence of masked tokens in the backbone encoder. ConvMAE overcomes this limitation by employing convolutions for the early stages and transformers for later stages. Masking is performed blockwise and masked convolutions prevent potential cheating. Extensive experiments demonstrate ConvMAEs effectiveness in image classification object detection semantic segmentation and video classification. Ablation studies provide insights into the architectures design. Strengths: Advances selfsupervised learning particularly masked autoencoding for images. Clear and wellpresented paper with informative illustrations. Comprehensive experiments including various downstream transfer tasks. Thorough ablation analysis covering key aspects of the architecture. Weaknesses: The name \"ConvMAE\" may overemphasize the role of convolutions given that the majority of the architecture still consists of transformer blocks. This raises concerns about the papers originality compared to similar approaches. The paper focuses primarily on the base model size and does not explore the scalability of ConvMAE to larger models. Minor typos and inconsistencies require proofreading such as missing definition of FOV and incorrect mask ratio for MAE. Questions: The paper should address the concerns raised about the overemphasis on convolutions and the originality of the approach. Further investigations into the scalability of ConvMAE with larger models would strengthen the papers claims."], "lKULHf7oFDo": ["Paraphrased Statement: The research introduces \"corestable fairness\" a novel concept for fairness in federated learning (FL). This concept ensures that no group of participants can significantly improve their results by forming an alliance. The paper proposes \"CoreFed\" a new FL protocol inspired by this concept. CoreFed is proven to achieve corestable fairness when participants loss functions are convex and approximates fairness when they are nonconvex. Empirical comparisons with FedAvg show that CoreFed enhances corestable fairness while preserving accuracy. Strengths: Theoretically sound with supporting mathematical proofs. Introduces a novel fairness concept in FL. Weaknesses: Limited empirical evaluation as it compares only to FedAvg and omits recent advancements. Experiments use a small number of participants (3). Lacks experiments with lowquality data from certain participants despite promising robustness against such scenarios. Minor Comments: Duplicate references in the introduction. Questions: 1. Clarify the privacy implications of corestable fairness. 2. Discuss the communication cost implications of CoreFed as it requires sharing local losses with the server. Assess its scalability in practical applications.", "Paraphrased Statement: This paper examines the concept of core stability in federated learning where no subgroup of participants can significantly benefit from establishing a new coalition. It demonstrates the following key findings: 1. If utility functions are continuous and the set of solutions maximizing any weighted combination of utilities is convex core stable solutions exist. 2. Maximizing the sum of the logarithms of individual utilities (where utilities are concave) produces a core stable solution. This objective can be efficiently optimized using Stochastic Gradient Descent (SGD). Notably the gradient update process resembles that of the FedAvg algorithm. 3. The paper extends core stability to include approximate core stability in the nonconvex case. Strengths and Weaknesses: Strengths: The paper provides a novel and insightful analysis of core stability in federated learning. Clear presentation of results supports the existence of core stable solutions. The proposed CoreFed algorithm is ingenious leveraging techniques similar to FedAvg. Weakness: The definition of core stability is specific to the scenario where players form new coalitions only when their utility increases by a factor of n times the current utility. This definition may be less applicable in cases where n is small. Questions: Can the authors provide further insight into the rationale for using the multiplicative constant n in the definition of core stability Intuitively players would be incentivized to form coalitions if their utilities decrease by a smaller factor than (1\u03b1) for all members of the subgroup. Are there any technical barriers to extending the results to this alternative definition of core stability", "Paraphrased Statement: This paper proposes a definition for fairness in federated learning based on social choice theory called \"corestability.\" Corestability prevents any group of agents from forming their model and achieving a better outcome than they would by participating in the federated model. The definition satisfies proportionality and Pareto optimality principles. Strengths: Novel approach using corestability in federated learning. Clear and sound theoretical analysis. Weaknesses: Some unnecessary prose and minor grammatical issues. Limited empirical results on diverse models. Significance: Potential for impact on fair federated learning. Further exploration of corestabilitys importance in this context is recommended. Questions: Why has corestability not been used in the past What impact do the authors anticipate it will have on federated learning fairness"], "yJEUDfzsTX7": ["Paraphrased Summary: This paper presents an optimistic algorithm for a general type of RiskSensitive Reinforcement Learning (RL) where the quantile values of the return distribution are weighted based on a weight function G. The algorithm is characterized by a regret bound that is linearly dependent on the Lipschitz constant of the weight function. Strengths: Novel analysis of a general class of risksensitive RL problems. Clear and logical organization with main results in the main text and supporting material in the appendix. Wellwritten and thorough presentation. Weaknesses: Lack of motivation for studying this particular class of problems. Queries: Line 113114: Should the equation read max\u03c0\u2208\u03a0 Ind\u03a6 \\mathcalM(\u03c0) on the left side Line 193: Should the equation read \u03b5R(k) on the right side", "Paraphrased Summary: This study introduces a new method for analyzing regret bounds in risksensitive reinforcement learning using an optimistic Markov decision process (MDP) formulation. This approach extends the applicability of regret bound analysis to a wider range of problem formulations than current techniques. Despite not receiving a detailed review of related work the regret bound analysis in the paper is considered reasonable for the conditional valueatrisk problem formulation. Overall the paper provides a clear analysis with wellexplained intuition. It would benefit from a more thorough discussion of related work. Highlighted Strengths and Weaknesses: Strengths: Provides indepth regret bound analysis for risksensitive reinforcement learning. Uses a clear formulation and explains the underlying concepts. Weaknesses: The claim of the first regret bound may be exaggerated. The papers structure could be improved to enhance clarity (e.g. placing lemmas at the end can be confusing). Questions: 1. The regret bound analysis relies on the Lipschitz constant LG. What happens if LG is not Lipschitz 2. Are there any additional assumptions about the MDP such as stationarity or nonstationarity", "Summary Paraphrase: This research proposes an exploratory learning method for riskaware reinforcement learning. The method offers verifiable guarantees for its performance in terms of regret. Strengths: The paper provides a universal approach for risksensitive reinforcement learning with provable regret bounds (specifically for the general formulation given in Eq. (1)). The papers crucial contribution is the result in Lemma 5.1 which transforms the complex risksensitive objective function into a manageable form. This allows for estimating the difference in objective values based on differences in quantile functions of the cumulative rewards. The paper introduces a novel UCB algorithm that differs from traditional OFU algorithms. This approach directly selects a model by deducting a penalty term (related to visitation counts) from the estimated model. This may reduce computational complexity. Weaknesses: The proofs are concise the paper could provide intuitive explanations and clarify intermediate steps. The paper does not sufficiently explain why a different UCB approach is used and its advantagesdisadvantages compared to existing methods. The computational feasibility of the generalized form is unclear the paper does not provide details on computing the oracle policy for the CVaR setting or a standard algorithm for the general setting. Despite proposing a new algorithm and focusing on a tabular setting the paper does not include experimental results to demonstrate the algorithms performance. Questions: Can you provide more intuitive explanations and clarify the intermediate steps in the proofs Why is a different UCB approach used and what are its advantagesdisadvantages How is the oracle policy computed in the CVaR setting for the generalized form Is there a standard algorithm for the general setting Why does the paper not include experimental results given that it proposes a new algorithm and deals with a tabular setting", "Paraphrase: The paper investigates risksensitive reinforcement learning (RL) and provides regret bounds for a range of risksensitive objectives including Conditional ValueatRisk (CVaR). The approach involves a new CVaR objective and an optimistic Markov Decision Process (MDP). Strengths: The study of risksensitive RL is a significant and valuable area of research. Weaknesses: The algorithmic contributions of the paper are not particularly innovative. A more detailed comparison to recent advancements would be beneficial. Questions: Clarify how the proposed formulation relates to existing settings. Explain how the proposed objective generalizes existing objectives. Provide a thorough comparison to existing works particularly highlighting the dependence on various parameters. Emphasize the novelty of the results which primarily focus on equivalence and transformation rather than algorithmic development."], "toleacrf7Hv": ["Summary Paraphrase: This paper introduces a way to select the best convex potential function for the Wasserstein2 optimal transport problem by comparing its empirical semidual error to other potential functions. The paper shows that the potential with the lowest empirical semidual error will have a gradient that is close to the optimal transport map. The paper provides some examples to illustrate the usefulness of this criterion and also discusses the potential relationship between optimal transport and domain adaptation. Strengths and Weaknesses Paraphrase: Strengths: Introduces a novel approach to model selection in optimal transport. Provides a rigorous theoretical foundation for the selection criterion. Raises important questions about the use of optimal transport in domain adaptation. Weaknesses: The papers presentation could be improved for clarity. The main contribution is not sufficiently novel as it follows from existing theoretical results. The experimental evaluation is limited and not fully convincing. The relevance of the domain adaptation experiment is questionable.", "Paraphrased Statement: This paper presents a new method to evaluate the accuracy of estimated optimal transport (OT) maps between two measures even when the true map is unknown. The method involves using the semidual formulation of the OT problem and finding the objective function that minimizes the squared error between the estimated and true transport plans. This objective function is shown to be highly correlated with the L2 distance between the two maps when certain conditions are met such as a Lipschitz gradient for the estimated potential and strong convexity for the true potential. The paper demonstrates that the map minimizing the semidual objective is related to the distance between the true map and the closest map within a factor dependent on the regularity of the estimated map. If the regularity conditions are not satisfied the authors propose regularizing the potentials to ensure that the semidual objective is computable. The criterion can be used to tune hyperparameters in OT algorithms (including Sinkhorn iterations) or to evaluate the correlation between OT performance and downstream task performance (e.g. domain adaptation). While the authors find that performance on a domain adaptation task does not correlate with the quality of the OT map they emphasize the potential usefulness of the criterion for comparing algorithms and assessing the applicability of OT to specific problems. Strengths: Simple and conceptually sound approach for evaluating OT map accuracy. Wellpresented theoretical development and illustrative empirical results. Applicability to practical cases where the semidual formulation would normally be infinite. Insights into the role of regularity in OT for domain adaptation. Weaknesses: Some areas of the presentation could use clearer motivation and context. The domain adaptation results should be further validated on different datasets. Exploration of the validity of the theoretical bounds in practical cases would be beneficial. Explanation of the smoothness and strong convexity of the Brenier potentials and their behavior with continuous measures would be helpful. The counter examples could benefit from more context and clarification.", "Paraphrase: Given two measures and a set of potential estimates from Optimal Transport theory this study presents a method for identifying the potential whose derivative is closest to the Monge map. Under assumptions of convexity smoothness and Bernsteintype inequalities the method provides error bounds (Lemma 1 and Proposition 2). Experiments and applications demonstrate the effectiveness and validity of the method. Strengths: 1. The idea of selecting the potential based on the dual formulation is intuitive and supported by theory and experiments. 2. The paper is wellstructured and the experiments are thorough. Weaknesses: 1. There are typos throughout the paper (e.g. in Figure 1 caption). 2. Some arguments and theoretical results may be incorrect (see Questions). 3. Sections 4 and 5.1 can be merged to improve readability. 4. The explanation of \"selfconcordant potentials\" and the method for finding the conjugate could be improved. Questions: 1. The calculation in Proposition 1 may be incorrect because the constraints in the OT dual formula only need to be satisfied within the support of the measures. 2. The Domain Adaptation experiment should be replicated for greater reliability. It is unclear how the distribution of training and test data is ensured to be the same.", "Paraphrased Statement: This research employs the Lebesgue transform to establish a quantitative criterion for selecting convex potentials using the semidual Brenier objective. This criterion helps in optimizing parameters and selecting models for entropic regularization of OTbased neural networks (ICNNs) and smooth and strongly convex nearest Brenier (SSNB) models. The authors also use this criterion to analyze the application of OT in domain adaptation. Strengths: Proposes a quantitative criterion based on the semidual Brenier objective. Provides theoretical analysis and experimental validation of the methods effectiveness. Demonstrates that the map minimizing the semidual objective may not always achieve optimal label transfer. Weaknesses: The authors acknowledge that the choice of network structure can affect ICNN performance but this claim requires further experimental verification. Question: To ensure accurate final model selection its important to ensure optimal ICNN performance. The authors suggest exploring this aspect through further experiments."], "rTTh1RIn6E": ["Paraphrased Summary Introduction: This research introduces Conditionali a new method for detecting outofdistribution (OOD) data using independent component analysis (ICA) methods. This method leverages probabilistic formulations and employs the HilbertSchmidt Independence Criterion providing a practical solution for reducing interdependencies between variables. Approach: Conditionali leverages the maximum mean discrepancy (MMD) to quantify the dependency between indistribution (ID) classconditional distributions and OOD distributions. The primary equations guiding this approach are presented in Eqs. 3 and 4. Theorem 1 demonstrates the advantages of the independence assumption for OOD detection. Experimental Findings: Experimental evaluations show that Conditionali outperforms classic methods in OOD detection tasks. Strengths: Superior performance compared to established methods Use of innovative ICA techniques Weaknesses: Poor writing and confusing notations requiring substantial effort to decipher (e.g. \"ptheta\" denoting a mixture of ID and OOD distributions) Assumption in Eq. 11 that the number of OOD data matches that of ID data (a major concern) Unclear method for training the model without OOD data (line reference needed) Additional Recommendations: Comparison with the stateoftheart method Gradenorm (a relevant reference on detecting distributional shifts) Similarity concerns with the HOOD paper as both employ similar ideas Questions: Request for clarification on complex notations and definitions (e.g. \"ptheta\") Confirmation of the assumption in Eq. 11 regarding matching OOD and ID data sizes Guidance on training the model in the absence of OOD data", "Paraphrase: Summary The paper presents a new training approach to enhance the ability of a classification model to detect data that is not from the expected distribution (OOD). This method is based on the assumption that the connection (mutual information) between OOD and indistribution (ID) data should be minimal. Specifically the approach involves enforcing class conditional independence between ID and OOD features leveraging Maximum Mean Discrepancy (MMD) to ensure this condition. The method includes two variations: one requires unlabeled OOD data while the other generates OOD data through robust data augmentation. Strengths and Weaknesses Strengths: The paper is wellwritten addressing similarities to existing work and providing detailed implementation details crucial for the methods success. The method is both theoretically motivated and empirically validated on substantial datasets involving both computer vision and natural language processing tasks. The paper includes an ablation study examining various method aspects such as the number of classes for enforcing independence during training and the hyperparameter for adjusting the loss component contribution for enforcing independence. Weaknesses: Like all methods requiring OOD data during training the proposed method incurs overhead in terms of storing samplinggenerating OOD data and additional computation (e.g. Gaussian Kernel). The methods performance on a specific type of OOD data may depend heavily on the type of OOD exposure or augmentation employed during training. It is unclear how the proposed composite loss function would impact the models discriminative ability (accuracy) on indistribution data. Questions: 1. Could the authors provide insights into the accuracy of this method in comparison to existing approaches 2. Could the authors discuss the level of sensitivity of the method to variations in training OOD data or data augmentation strategies", "Paraphrased Summary: This research presents ConditionalI a conditional independence model for detecting outofdistribution (OOD) data. It introduces an integrated loss function to minimize dependence between the features of inliers (samples belonging to known classes) and outliers (samples from unknown classes). This allows for the separation of OOD samples based on an independence measure. Theoretical guarantees are provided for the conditional independence assumption. Experimental results demonstrate the methods efficacy in computer vision and natural language processing tasks both with and without OOD training data. Strengths: Explicit independence assumption: Inliers extract limited predictive information from outliers during training. Conditional independence extension: Each class produces independent features from outliers. Theoretical support and analyses for the independence assumption. Weaknesses: Limited innovation: The model shares similarities with the probabilistic formulation and HSIC approach used in [2]. Lack of qualitative experiments to demonstrate the conditional independence model: Illustrative results could showcase the separability of inlier and outlier features. The validity of the HSICcondi metric should be tested. Unconvincing experimental results: Missing comparative results with methods like DIN Mahalanobis distance and Energy. HOOD results are not included in unseen OOD training data experiments. Conditionaligenerative and HOOD results are missing for NLP OOD detection tasks. Incomplete evaluation of the memory bank architecture: No quantitative results are provided. Questions: 1. Is there empirical evidence to support that conditional independence within a class is more effective than restricting independence between inliers and outliers 2. Why are two different methods used to generate fake outlier samples in the Image OOD detection tasks 3. Where are the HOOD results for the unseen OOD training data setting 4. What is the computational cost savings achieved by using the memory bank architecture", "Paraphrase: This research introduces a new method for detecting samples that lie outside the distribution of data (outofdistribution OOD) by leveraging known OOD examples. The method involves using a classifier with an encoder that generates features and a linear classification layer. The goal is to make the features of indistribution (ID) and OOD samples within each class independent of each other. To achieve this the authors employ the HilbertSchmidt Independence Criterion (HSIC) to measure the independence between ID and OOD features. The classifier is then trained using a combined loss function that incorporates both the classification objective and the independence criterion. To optimize computational efficiency the authors utilize a buffer to store and reuse old features for HSIC calculations avoiding the need to recompute features for all ID samples every time. The proposed method termed \"Conditionali\" is evaluated on benchmark datasets for OOD detection in image classification and natural language processing demonstrating superior performance compared to existing methods. Strengths: Conditionali exhibits strong performance consistently outperforming other OOD detection approaches. It shares similarities with the HOOD method but introduces key differences that contribute to its improved performance. Weaknesses: There is no guarantee that decorrelating ID and known OOD features will lead to better discrimination between ID and unknown OOD data. The tuning of hyperparameters (\u03bb and \u03c4) for each method is not explicitly discussed. Some assumptions in the papers theorem may not be fully justified. The authors claim that decorrelation can improve generalization to unseen OODs is not supported by theoretical guarantees. Questions: How were \u03bb and \u03c4 determined Was a separate validation OOD dataset used for hyperparameter tuning What specific assumptions in Theorem 1 are being questioned What is meant by \"different generative process\" and how can it be defined Have the authors evaluated using just the test scoring method (Equation 9) on a classifier trained with crossentropy Have they attempted outlier exposure with generated OOD data Were overlapping images present in different datasets What were the specific settings used for computing the values in Tables 4 and 5 and Figure 1"], "zofwPmKL-DO": ["Paraphrased Statement: Summary: The paper introduces two major contributions: Integrating quantum gradient estimators into Langevintype algorithms for sampling from target distributions. Employing quantum techniques to estimate the normalizing constant in multilevel Monte Carlo (MLMC) methods. It is a groundbreaking application of quantum estimation in Markov chain Monte Carlo (MCMC) sampling. Strengths: Low computational cost for each iteration using existing gradient estimation algorithms. Weaknesses: Lack of clarity and detail in the presentation. The methods for quantum computations and estimation are insufficiently defined. The main sampling algorithms are omitted. Questions: The paper lacks rigorous proofs and detailed algorithms making it difficult to comprehend and verify claims. More elaborate descriptions and proofs of the main assertions are necessary.", "Paraphrased Statement: Summary: This document explores quantum computing methods for evaluating log concave probability distributions. It calculates the convergence rates in both total variation and W2 norm forms. Strengths and Weaknesses: The presentation is clear and engaging. The findings are scientifically sound and logical. However the methods employed for convergence analysis are conventional. The contribution lies in applying quantum principles to the analysis. Despite this Table 1 suggests that quantum variations of algorithms like ULA and MALA do not significantly improve performance compared to traditional methods raising questions about the advantages of quantum computing in this area. Questions: None reported.", "Paraphrased Statement: Summary: The authors evaluate the challenges of sampling and estimating normalizing constants for logconcave distributions. They establish query complexity bounds for quantum algorithms leveraging existing bounds for classical algorithms as a starting point. Their results demonstrate the potential for quantum speedups. Quantum Speedups: 1. Sampling: Quantum algorithms employing underdamped Langevin diffusion achieve the same query complexity as classical algorithms but using zerothorder queries instead of firstorder (gradient) queries. Quantum Markov chain Monte Carlo (MCMC) algorithms have a faster mixing time than classical MCMC with specific time bounds for warm and cold starts. 2. Normalizing Constant Estimation: Quantum algorithms can accelerate the estimation of normalizing constants achieving query complexities of \\(\\tildeO(\\kappa12d32\\epsilon)\\) (with MCMC) or \\(\\tildeO((\\kappa76d76\\kappa12d43)\\epsilon)\\) (using a different sampler). This improves the classical dependence on \\(\\epsilon2\\). Quantum Techniques: Key quantum techniques used include Jordans algorithm for gradient estimation a general squareroot speedup for quantum algorithms based on reversible Markov chains and a quantum algorithm for mean estimation with query complexity dependence on \\(\\epsilon\\). Strengths and Concerns: Strengths: Pioneers the application of quantum algorithms to gradientbased MCMC algorithms. Establishes quantum analogs of established Langevinbased algorithms. Demonstrates signature \"squareroot\" speedups in certain situations. Concerns: Mathematical details of quantum objects could be more precise for nonexperts. Exposition on quantum formalism could be expanded. Questions: The definition of quantum states over \\(\\mathbbRd\\) and their relationship to probability distributions needs clarification. The feasibility of preparing these states in practical quantum systems is not fully addressed. Minor Notes: Line 51: Remove \"logconcave.\" Line 52: Add \"with distribution\" \\(\\tilde\\rho\\). Equation (A.3) is missing a negative sign. Line 637: Missing factor of \\(2x2(2\\sigmai2)\\). Equation (643): Should be \\(\\le\\). Line 666: Change \"speedup\" to \"sped up.\" Line 704: Change \"satisfying\" to \"satisfies.\" Algorithm 6.7: Consider moving \\(\\tildeg\\) calculation to the beginning of the loop. Algorithm 8 line 6: Missing tilde (). Line 797: \"Uncompute\" requires clarification. Equation (849): \\(T\\) is not defined. Equation (855): Should be \\(O(\\beta \\epsilon)\\). Line 867: Change \"Given a state\" to \"Let...\" Line 929: Change \"speedup\" to \"reduce.\" Line 974: Change \"much\" to \"more.\""], "uRTW_PgXvc7": ["Summary This paper presents a technique that transfers knowledge from an image model to video tasks while using minimal parameters. The \"Spatiotemporal Adapter\" (STAdapter) is introduced to finetune a pretrained image model for video recognition tasks without modifying the original models parameters. Strengths Relevant and timely topic due to the increasing size of video models Impressive results with a very small number of trainable parameters Comprehensive ablation studies Weaknesses Limited technical contribution as adapters and parameterefficient modules have been used in other domains Validation of STAdapters effectiveness not extended to a broader range of backbones Absence of experiments or insights exploring alternative design choices for STAdapter Questions Why were 3D depthwise convolutions chosen over other operators for STAdapter Could the proposed technique be further optimized by incorporating more designrelated experiments Is it possible to provide a comparison of both GFLOPs and trainable parameters to better assess the efficiency of the approach", "Weaknesses: 1. Runtime savings: Authors response: The proposed approach does not lead to significant runtime savings during training compared to fully finetuning. However the reduced parameter count enables training on GPUs with less memory making SOTA video models accessible to researchers with constrained resources. 2. Other backbones: Authors response: The proposed approach is not limited to ViTbased models and can be applied to other popular architectures such as MViT and Swin. Preliminary results with MViT show promising performance. Integrating the approach with largescale pretrained Swin models is an ongoing effort.", "Summary: The paper aims to efficiently train deep learning models for video tasks using parameters from pretrained image models. The authors introduce a \"SpatioTemporal Adapter\" (STadapter) that incorporates temporal and spatial information with minimal additional parameters. They demonstrate the effectiveness of their approach on Kinetics400 and SomethingSomethingv2 video datasets. Strengths: Clear motivation and wellpresented paper. Weaknesses: Lack of comparison with prior work: Its not clear how the STadapter improves over previous methods with similar depthwise 3D convolutional layers. Unclear experimental setup: The paper reports validation set results instead of test set results which may lead to biased estimates. Misleading parameter comparison: Table 1 only shows the number of finetuned parameters not the total number of parameters making comparisons unfair. Incomplete comparison to stateoftheart: The paper does not provide results that demonstrate the sole contribution of the STadapter compared to existing methods. Confusing notation: The inconsistent use of N in the equations may lead to misunderstandings. Questions: Validation set results: Can the authors provide test set results or explain how early stopping on the validation set affects the reported results Total parameter comparison: Can the authors provide a column showing the total number of parameters for each method Inference time: How does the addition of the STadapter affect inference time compared to methods without extra parameters Fairer comparison: Can the authors present results for ViTL models pretrained on IM21k with and without the STadapter to isolate its impact Notation clarification: Can the authors explain the inconsistency in using N in equations to represent temporal information", "Summary This study introduces a SpatioTemporal Adapter (STAdapter) for efficient finetuning in video tasks. Despite using fewer trainable parameters STAdapter achieves performance comparable or even better than the traditional full finetuning approach on K400 and SSv2 datasets. Pros: Introduces an Adapter for SpatioTemporal modeling with a 2D pretrained model. STAdapter demonstrates effectiveness in improving performance with fewer parameters. Achieves stateoftheart results on K400 and SSv2 using a CLIP pretrained model. Cons: Lack of comparisons: The STAdapter structure differs from common practices for extending 2D backbones to Spatiotemporal models. It would be beneficial to isolate the impact of the new structure and learning strategy. Variation in performance: The performance of CLIP and ImageNet21K pretrained models differs significantly. An ablation study using ImageNet21K or the SSv2 dataset could provide additional insights. Missing results: The results for ImageNet22K pretrained models are not included in the study."], "rP9xfRSF4F": ["Paraphrase: Summary: This groundbreaking paper introduces and investigates the problem of optimally timed intervention (OPI) an optimization challenge that balances timely intervention with minimizing the likelihood of missing a critical event. The authors establish the equivalence of OPI to optimal stopping in the underlying survival process. They derive analytical criteria for calculating the optimal intervention time based on the conditional survival function and demonstrate that the optimal policy is typically timevarying rendering static threshold policies inferior. Additionally they propose a deep recurrent survival analysis (DDRSA) architecture and present experimental results on realworld datasets. Strengths: Introduces the OPI problem and its connections to survival analysis and optimal stopping theory. Derives an analytical formula for determining the optimal intervention time. Proposes a DDRSAbased method for implementing an OPI policy. Demonstrates promising empirical performance compared to baseline approaches. Presents clear and accessible writing. Weaknesses: Experiments focus solely on RNN models leaving the flexibility of the framework unexplored. The OPI policy is based on the true hazard rate function which is not available in practice. Questions: Can the OPI policy be applied to models beyond RNNs Is it theoretically possible to assess the impact of hazard rate function estimation errors on the performance of the OPI policy", "Paraphrased Summary This research investigates a problem known as the optimally timed intervention (OTI) problem which involves identifying the best time to stop or intervene in a dynamical system to prevent a critical event from occurring. The authors develop a mathematical framework to minimize both the expected time until the critical event (lost when intervention occurs too early) and the fixed cost of the critical event happening (when intervention occurs too late). They demonstrate that existing approaches (windowbased intervention and timetoevent) are suboptimal for OTI and propose a novel method resembling timetoevent with dynamically adjusted thresholds. They prove the optimality of this new approach and validate its superiority through experiments on two datasets. Strengths and Weaknesses Strengths: Originality: The proposed approach may introduce a unique solution to the OTI problem. Weaknesses: Technical Quality: Loose and Ambiguous Definitions: The mathematical definitions are sometimes imprecise affecting clarity and casting doubt on the rigor of the findings. Missing Reasoning: The authors omit explanations of the optimal intervention the meaning of stationary processes and policies and the practicality of considering nonhomogeneous processes. Presentation: Unclear Notations and Definitions: The papers readability suffers from inconsistent notations and ambiguous definitions. Redundant and Complex Approach: The proposed method is presented twice using equivalent concepts (hazard rate and survival function) leading to unnecessary complexity. Lack of a Conclusion: The paper lacks a conclusion section hindering the assessment of its significance. Significance: Potential Importance: The analysis and approach have the potential to be valuable to the OTI community. Technical and Presentation Limitations: The poor technical quality and presentation hinder the proper evaluation of the significance. Questions for the Authors How does the optimal intervention translate into an optimal policy What is meant by a stationary process and a stationary policy Given that any Markov process can become homogeneous what is the purpose of considering nonhomogeneous processes", "Paraphrase: Summary: This study examines the issue of deciding when to intervene and halt a random process before it reaches a crucial event. A new problem setting known as Optimally Timed Intervention (OTI) is introduced where the goal is to minimize the expected time between intervention and the event occurrence (L) subject to a low probability of missing the event. The study devises an optimal policy using survival and hazard processes. It applies these theoretical insights to develop an RNN approach for predicting these processes. The method is tested using realworld data and appears to perform slightly better than a baseline and significantly better than a simplistic approach. Strengths: Presents a novel problem setting (OTI) with potential practical applications. Develops a comprehensive theoretical framework for computing optimal policies based on survival and hazard rates. Demonstrates promising experimental results suggesting the practical relevance of the proposed theory. Weaknesses: Does not explicitly define the relationship between the processs covariates and the event time L. The transition from the OTI theory to the experiments lacks formalization and could be improved. The hyperparameter C\u03b1 implicitly controls the probability of missing the event but it is not clear how to specify a desired probability and determine the corresponding expected intervention time. Questions: Can the original problem be reformulated as a standard optimal stopping problem If so why not apply existing tools instead of the proposed survival and hazard processes Is the use of survival and hazard processes truly novel or has this methodology been employed in previous research", "Paraphrased Statement: This research investigates a method for determining the best time to intervene in the event of a failure. This problem is framed as a series of randomly generated features. A failure occurs at some point and the goal is to intervene optimally by implementing a strategy that either permits the process to continue or decides to intervene at a specified time. Optimal intervention is defined as intervening without intervening too early (before the failure occurs) while ensuring a low probability of late intervention (intervening after the failure). Two processes are introduced: a survival process which models the probability of failure occurring after a given time and a hazard process which models the probability of failure occurring at a specific future time given the current time. The optimal intervention problem is then formulated as an optimal stopping problem on the survivalhazard process. Assuming that features evolve based on Markovian dynamics the optimal stopping problem can be solved. Experiments are conducted on two datasets comparing the proposed method with two baseline approaches. The prediction model used is a Recurrent Neural Network (RNN) that converts a history of features into a vector representation. This representation is then used by a decoder to generate a sequence of vectors that are directly converted to hazard rates. The results indicate that the new approach generally outperforms the baselines on both datasets. Strengths and Weaknesses: Strengths: Relevant to machine learning with practical applications Novel and wellmotivated solution Clear and wellwritten Weaknesses: Insufficient experimental details (e.g. RNN model training and prediction process) Lack of computational aspects (e.g. execution time) Insufficient discussion of Theorem 3 Questions: 1. How is the DRS loss calculated 2. How is the hyperparameter C selected in practice 3. Why is the (L\u03c4) loss used instead of alternatives 4. Why does TTE appear to perform better than OTI in certain scenarios Minor Presentation Issues: Spelling error in \"corrolary\" on Line 294 Extra space in \"Allmodels\" on Line 320 Subcaptions of Figure 1 appear to be incorrect"], "qPb0m0NXt4j": ["Paraphrase: The researchers have developed a reinforcement learning game to address the challenge of visual communication using sketches. Two agents (a sender and a receiver) participate in the game and attempt to agree on the meaning of a sketch. The game considers three important factors to provide insights into visual communication. Experiments were conducted using a portion of the Sketchy dataset to evaluate the proposed method. Promising results have been obtained. Strengths: Innovative approach to tackle the issue of developing graphical conventions using a visual communication game based on sketches. First study to model the evolution of graphical conventions through a Pictionarylike game. Clear technical contributions including a novel strategy to guide the abstraction process. Measurement of emergent graphical conventions using concepts like iconicity symbolicity and semanticity. Weaknesses: The quality of the target sketch produced from the original image (IS) may affect the effectiveness of communication. Limited dataset used for training and testing consisting of only 40 classes with 10 images per class. Questions: 1. Have the researchers explored using alternative sketching methods to determine the impact of sketch quality 2. Can the agent remove existing strokes from the canvas during gameplay 3. A human study on sketch evolution could validate the quality of sketches produced by the sender agent. 4. Is it possible to determine the most informative strokes from the final sketch that best represent the object in the original image", "Summary This paper investigates the use of sketches in emergent communication games that rely on visual cues. Previous research has explored graphical communication but this work focuses on studying the evolution of communication conventions in a multistep game. Three properties (iconicity symbolicity and semanticity) are defined to analyze the sketches that emerge. The results show that agents can generally learn successful communication channels. Strengths Explores the novel modality of sketches in emergent communication. Studies the evolution of sketches in a cooperative multistep game. Proposes three evaluation metrics for assessing evolved sketches. Weaknesses Does not fully consider recent advances in emergent graphical communication and differentiable drawing. Comparison to similar models is limited and the games are relatively simple with only two distractor images. Communication performance is lower than other approaches. The new measures are mostly evaluated qualitatively. Human performance is not considered. Additional Remarks The paper acknowledges that sketching has been explored previously. Section 2 on \"Learning to Sketch\" should be updated to reflect the current state of the art. Section 5.2.5 could be expanded to highlight additional contributions. Questions Why is the number of context images limited to three What causes the significant drop in performance for unseen images How low is the communication accuracy in certain settings and why Are communication conventions still valid when agents are tested with images from different subsets Can the semanticity measure be quantified and how does it relate to design choices in other works", "Summary This research aims to replicate communication methods that existed before language specifically the use of sketches. A sender attempts to convey an image through a series of minimalist sketches while a receiver tries to interpret them. The study introduces a new training objective based on a multiagent reinforcement learning (MARL) approach and defines three elements crucial to communication through sketches: iconicity symbolicity and semanticity. The results support the researchers hypotheses about the evolution of communication. Strengths The studys novel focus on using sketches for emergent communication sets it apart from current literature. The introduced measures of iconicity symbolicity and semanticity provide a solid basis for assessing the effectiveness of communication in these games. Weaknesses The novelty of the proposed training objective and its relationship to previous work is unclear. The results on the evolution of semantically similar classes of sketches are difficult to verify and seem to be influenced by the training objective. The disjoint observation spaces between the sender and receiver could benefit from a clearer explanation. The quality of the distractor images may impact the results leading to the question of whether emergent communication occurs due to communication or sender drawing abilities. The assessment of iconicity symbolicity and semanticity seems automated for some but not all aspects raising questions about the accuracy of the evaluations. Questions How were the learning rate schedule and other parameters determined Were they optimized through a grid search or other methods", "Paraphrased Summary This paper explores the development of languagebased communication through sketching. A sender sketches an image by predicting strokes while a receiver selects the corresponding image from a pool based on stroke sequences. A reinforcement learning algorithm trains the sender and receiver optimizing guessing accuracy. The authors find that early decisionmaking is advantageous for instances the receiver was trained on. The pretrained sender is effective and the receiver adjusts to its protocol. The sender learns to draw with fewer steps and the sequential nature of the game provides feedback on information acquisition. The paper also evaluates the iconicity symbolism and semanticity of drawings. Case studies reveal that sketches simplify preserving key concepts. Strengths and Weaknesses Strengths: 1. The study investigates the intriguing concept of communication through drawing. 2. The methodology and evaluation are sound as an initial step. Weaknesses: 1. The sender is pretrained potentially limiting insights into how it learns to draw effectively. 2. The evaluation of iconicity does not align with its definition."], "y--ZUTfbNB": ["Paraphrased Statement: Summary: Matrixvector multiplication is crucial in machine learning and faster algorithms are sought after. This paper introduces significant speedups for matrices that are \"distance matrices\" which arise from a set of points where each matrix entry represents the distance between two points. This assumption is reasonable in many applications. The complexity of matrixvector multiplication varies with the distance metric and the dimensionality of the points. For the \\(\\ell1\\) distance in \\(d\\)dimensional space the paper presents an algorithm that computes the matrixvector product in \\(O(nd)\\) time improving upon the worstcase bound of \\(O(n2)\\). This algorithm is straightforward. The paper also mentions efficient algorithms for other distance metrics but does not provide details. For the \\(\\ell\\infty\\) distance in \\(d\\log n\\)dimensional space it establishes a conditional lower bound of \\(n2\\) time and presents promising approximation algorithms. Both the upper and lower bounds are significant but rely on standard techniques in nearest neighbor algorithms. Strengths and Weaknesses: Despite the importance of the results the techniques are not highly innovative but offer valuable optimizations. The main weakness is that the paper lacks groundbreaking techniques. Questions: Is this paper known to you Are there similar results or techniques in \"Algorithms and Hardness for Linear Algebra on Geometric Graphs\" by Alman et al.", "Paraphrased Summary: The paper presents three major findings that are clearly stated on pages 2 and 3: Efficient computation of A.z in O(ndp) or O(nd log(n)) for 1 \u2264 p \u2264 \u221e Efficient computation of A.B in O(n2dp) for 1 \u2264 p \u2264 \u221e Computation of the distance matrix A itself for n points in d dimensions in O(n2 log(n)) Strengths and Weaknesses: The paper is an indepth study of numerical linear algebra presenting significant results with a complex proof methodology. The efficacy of the findings is demonstrated through experiments. However the paper has limited relevance to machine learning and focuses more on numerical linear algebra. It could benefit from a more logical presentation in a single publication (e.g. a numerical algebra journal) to receive valuable feedback from experts in matrix algebra. Questions: Section B which is fundamental to the paper is confusingly separated from the main text solely due to page limits.", "Paraphrased Statement: The authors have created efficient algorithms for computing matrixvector products using pairwise distance matrices (not necessarily metric). These algorithms enable fast matrix multiplication when one matrix is a distance matrix. Additionally they present lower bounds for the \\ell\\infty distance problem that show it is computationally challenging along with fast approximate algorithms. A third contribution is discussed in the appendix. They provide experimental results to demonstrate the practical benefits of one of their algorithms. Strengths and Weaknesses: The paper is wellorganized and clear. The arguments in the main body are sound. One potential weakness is that the algorithms for \\ell\\infty distances particularly \\ell\\infty are not particularly novel. However the reduction for OVP (Optimal Vertex Partitioning) is innovative and nontrivial. Minor Issues: In the discussion of the binary case in Section 3.1 the variable t appears without context. Some typos have been identified: Theorem 1.1: z should be in Rn. Line 162: \"The sampling probabilities only need\" should be \"The sampling probabilities need only\". Line 166: \"T is the time requires to perform a matrixvector query.\" should be \"T is the time required to perform a matrixvector query.\" Line 271: \"This motives\" should be \"This motivates\". Question: The author expresses surprise that the algorithm for \\ell\\infty had not been developed previously. They speculate that this may be due to the perception that such algorithms are inherently difficult to devise."], "oNnv9XjClGK": ["Paraphrase: Summary: Researchers present a novel approach to computing a policys advantage function that bypasses the need to estimate the Q or value function. They compare this method against the widely used GAE technique on various tasks and find superior performance. Strengths: Clear and wellwritten paper that effectively conveys complex concepts. Comprehensive experimental evaluation demonstrating significant improvement over GAE when employed in conjunction with various environments. Connections to causal relationships and the citation of relevant work enhance the papers value. Reinforcement of the value of decisionmaking based on expected returns. Weaknesses: Some sections may initially suggest that greedy decisionmaking is preferable which could benefit from clarification. The authors could emphasize the advantages of advantage functions (e.g. their relevance to PPO) and explain their suitability for local decisionmaking. Questions: 1. Despite the stated goal of directly estimating the advantage function the method appears to involve fitting a value function as per Theorem 2 and beyond. Why does the proposed method avoid the problems associated with valuebased methods 2. The paper mentions the need for sampling entire trajectories for DAE rather than minibatches. Has a wallclock time comparison been made between DAE and GAE 3. The challenges of estimating the sum of advantages for normalization resemble the difficulties faced in estimating the partition function in energybased exponential family models. Could techniques from that field provide a solution 4. The DAEWide results show remarkable improvement. Could the authors provide insights into why this might be the case", "Summary The paper investigates a direct method for learning the advantage function without first obtaining the Qfunction. The authors introduce the concept of \\pinormalized functions which have an expectation of zero under a specified policy \\pi. The advantage function is an example of a \\pinormalized function. Shifting the reward by a \\pinormalized function does not alter the expected return. This property is exploited to minimize the variance of the return by decoupling it from the policy. A key result (Theorem 1) shows that the optimal \\pinormalized function to subtract from the reward is the advantage function of the current policy for onpolicy learning. This leads to a learning framework with a loss function derived from the minimization result in Theorem 1. The framework is extended to Nstep learning using the value function. Empirical results support the effectiveness of the method. Strengths Novel and practical approach to learning the advantage function directly Intriguing minimization formulation from Theorem 1 Decoupling of learning from the current policy as demonstrated by experiments Weaknesses Complexity of implementation due to multiple steps dispersed throughout the paper Lack of a coherent algorithm block Unclear and condensed presentation of the second part of the theory Unclear emphasis on \"causal\" concepts Questions Definition of reward corresponds to expected reward Verwenden von PPO zur Demonstration der geringeren Varianz des Vorteils gegen\u00fcber Q kann unzureichend sein Der Begriff \\pinormalized kann irref\u00fchrend sein. Vorschlag f\u00fcr einen alternativen Begriff: \\pineutralized oder \\pinullified NichtNullPolitik ist eine fehlende Annahme im Beweis von Theorem 1 Die Verwendung der ValueFunktion f\u00fcr den NStepFall widerspricht dem Ziel der Entkopplung des Vorteilslernens vom Wertelernen Der letzte Teil der Theorie erfordert mehr Erkl\u00e4rungen und Erweiterungen einschlie\u00dflich der Definition von Vtarget und der Abh\u00e4ngigkeit vom Lernen von V", "Summary: This paper introduces Direct Advantage Estimation (DAE) an algorithm that estimates the advantage function directly unlike conventional methods that estimate stateaction values. The authors draw connections to causality and demonstrate the algorithms performance through experiments on synthetic MiniAtari and ALE environments comparing it to PPO with GAE. Strengths: The proposed method has a strong theoretical basis. It is straightforward to implement. Experimental results indicate improved performance over the baseline. The paper is generally easy to understand. Weaknesses: The paper presents various results and interpretations but some seem unnecessary for explaining the method. The connection to causality is weak and the synthetic example used for motivation is overly simplistic. The experiments lack a fair comparison with relevant baselines: Nstep return method ablation Directly estimating the advantage via QV or AQV Minor issues: Equation 4 should use \"sample\" instead of the approximate symbol. The use of \"IMPALA\" to refer only to the architecture can be misleading. Questions: How does DAE compare in terms of wallclock time to PPO given its trajectorybased sampling approach Does DAE maintain lower variance in more complex environments where achieving zero variance is not as straightforward Which definition of the stationary distribution (discounted or undiscounted) is used in equation 3941 What exactly is \\hatA in Algorithm 1: the estimated advantage function or the nstep TD version with an advantage function", "Paraphrased Statement: This paper explores the relationship between causality and the advantage function in reinforcement learning. The advantage function can be interpreted as an estimate of how an action causally impacts expected return. Additionally the paper presents a theorem that demonstrates that the advantage function can be seen as the solution to a problem aiming to minimize variance while adhering to constraints. This finding facilitates a new objective for learning the advantage function bypassing the need to estimate Q and V value functions. Empirical results suggest that this approach outperforms generalized advantage estimation (GAE) in several standard benchmarks. Strengths and Weaknesses as Paraphrased: Strengths: The paper establishes a connection between advantage functions and causal effects. It introduces a novel objective for estimating the advantage function. The paper provides clear explanations and welldesigned experiments. Weaknesses: Questions Raised: The paper does not account for confidence intervals when comparing algorithms. The connection between the advantage function and causality is not thoroughly explored. The paper does not explain why the advantage function tends to be more stable. It is unclear how the advantage function compares to the solution of the unconstrained version of the minimization problem."], "qqHMvHbfu6": ["Paraphrased Statement: This paper analyzes the goal of Lewis communication games where two players (a speaker and a listener) interact to convey information. The goal is twofold: 1. Mutual Information Component: Encourage the speaker to create clear messages that accurately reflect the intended meaning. 2. Adaptation Component: Motivate the listener to adjust their perception to match the speakers message. The authors propose that overfitting in the adaptation game leads to communication problems such as low accuracy. To test this hypothesis they introduce a probe that evaluates the adaptation term. By adjusting the balance between the adaptation and mutual information components they find that: The adaptation term overfits while the mutual information term does not. Limiting the adaptation component through alternating training improves emergent language quality. Strengths: Elegant theoretical framework with significant empirical implications. Clear writing and persuasive results. Potential to shape the field of emergent communication. Weaknesses: Presentation could be improved in the cameraready version. Conflicting terminology in the weighting scheme discussion. Questions: Can the weighting scheme be described as using a probe to estimate the mutual information term and approximating the adaptation term as the difference between the total loss and the mutual information term Why does the adaptation term overfit compared to the mutual information term Can the listener not compensate for ambiguous messages from the speaker by adjusting their distribution", "Paraphrased Summary: Researchers have developed a method to analyze how communication in games specifically Lewis signaling games generalizes to new situations. These games involve a speaker who receives information (like an image) and sends a signal to a listener. The listener then makes an action and both agents are rewarded based on the outcome. The researchers have found that the communication loss of these agents can be divided into two parts: an \"information\" term that measures the ambiguity of the communication protocol and a \"coadaptation\" term that represents the difference between the listeners actual actions and its optimal actions. This division helps identify the source of communication issues especially when agents encounter new situations that were not part of their training. To address these issues the researchers propose reducing the importance of the coadaptation term in the loss function. They tested different training strategies including using partial listener training and early stopping to achieve this goal. Experiments using the EGG toolkit and various object attributes and datasets support their hypothesis about the importance of the coadaptation term in evaluating unseen objects. Strengths and Weaknesses Strengths: The loss decomposition approach is innovative. The paper is wellwritten clear and organized. The experimental results are convincing and demonstrate the proposed methods effectiveness. The extended experiments on more complex tasks further validate the hypothesis. Weaknesses: No specific weaknesses are noted.", "Paraphrase: Summary: Teaching agents to play communication games using the Lewis signaling framework remains a popular approach for studying the emergence of communication. In this study researchers propose dividing the standard Lewis game loss into two components: one that promotes objectspecific messaging and another that encourages mutual understanding between sender and receiver. Main Findings: The researchers explore the impact of artificially controlling the coadaptation term (second component) on the resulting languages learned by agents. They discover that the loss term often overfits when training agents using the traditional method which hinders the compositionality of the languages. Strengths: Clear and concise presentation of the research. The proposed loss decomposition provides a valuable framework for understanding communication dynamics. The experiments demonstrate the detrimental effect of coadaptation overfitting on compositionality. Weaknesses: The contribution may not be significant enough for publication at NeurIPS given its niche focus and overlap with existing EC literature. The paper primarily focuses on optimizing the loss function rather than investigating the emergence of communication through population dynamics. The early stopping regularization method used appears somewhat artificial and not reflective of realistic language development. Questions: Does the relationship between coadaptation overfitting and compositionality persist in different tasks with varying complexity How does the authors approach differ from previous work in neural iterated learning and ease of teaching which have also identified coadaptation as a factor affecting compositionality"], "odOQU9PYrkD": ["Paraphrased Statement: This paper introduces a new approach to address uncertainty in stochastic segmentation networks (SSNs). It utilizes a lowrank multivariate Gaussian distribution to capture model uncertainty in SSNs. The authors have also developed a tool to analyze factor models in SSNs employing rotation criteria to create simple and distinct control variables. In experiments the proposed method surpasses existing methods on multiple datasets. Strengths and Weaknesses: This novel method combines SSNs to address segmentation uncertainty using reduced latent factor variables. The paper provides clear explanations of the proposed model and includes comprehensive experiments in supplementary material. Possible Weaknesses: Limited explanation of the \"significant\" effect on output segmentations. Insufficient detail regarding rotation criteria. Potential for more information on the finegrained sampling interface. High computational cost as noted in the supplementary material. Questions Raised: How to determine the optimal number of factors from Figure 1 How to identify the \"significant\" effect and characterize relevant factors", "Paraphrase: This paper introduces a method to enhance stochastic segmentation networks (SSNs) by adding latent factors that control noise components in a single covariance matrix. By rotating these factors and enforcing sparsity the model becomes more efficient and potentially easier to interpret. The paper provides mathematical details and evaluates different factor rotation approaches. The results show that the method successfully produces uncertainty factors that can be manipulated individually. Strengths: Extends SSNs to offer independent control over uncertainty components. Addresses interpretability by introducing sparsity and uncertainty. Weaknesses: The authors fail to demonstrate the interpretability and usefulness of the created tool. Examples provided in the paper and video are not intuitive or helpful. Questions: Can the authors provide an example where the methods uncertainty display is clearly beneficial Can they report the computational overhead of the proposed modification compared to the original SSN", "Summary Paraphrase: This article addresses segmentation uncertainty by utilizing stateoftheart Superpixel Segmentation Networks (SSNs) as factor models. It proposes a method to derive flow probabilities for these factors enabling visualization and quantification of uncertainty. These flow probabilities are used to guide \"minimal rotations\" of factors through orthogonal transformations. The technique allows for finegrained uncertainty maps facilitating improved segmentation results and potential assistance for experts using segmentation tools. Strengths and Weaknesses Paraphrase: The reviewer finds the work to be solid and intriguing. The flow probabilities derived from rotated factors provide intuitive uncertainty visualization and the accompanying code allows users to modify segmentations based on these probabilities. The methods potential utility for assisting domain experts in fast segmentation tasks is highlighted. However the reviewer expresses uncertainty regarding its ability to discard entire segmented zones in scenarios with more than two or three classes. Originality Paraphrase: The paper is considered somewhat original as it combines existing concepts from SSN factor analysis and flow probabilities. However the reviewer emphasizes the valuable perspective gained by viewing SSNs as factor models considering it more significant than absolute originality. Quality Paraphrase: The reviewer finds the writing and illustrations in the paper to be of high quality. The included code repository is appreciated but it requires improved documentation and testing for ease of use. The reviewer recommends citing all necessary packages within the main document. Clarity Paraphrase: While the experiments are visually appealing and easy to comprehend the reviewer finds the article challenging to understand initially. A figure illustrating the overall procedure connections between concepts and intuitions behind Proposition 1 is suggested to enhance clarity. Questions Paraphrase: 1. Why do rotated factors exhibit significant differences across datasets Is the method capable of detecting uncertainty for attributed classes beyond borders and if not what are the limitations of the model 2. In the separation criterion introduced in 5.1.2 are loadings or rotated loadings used for the evaluation"], "pgF-N1YORd": ["Paraphrase: This study investigates factors that enhance learning transfer in continual learning focusing on the CW benchmark and the SAC algorithm. Key Findings: Transferring only the critic is more beneficial than transferring both actor and exploration components. The cumulative benefits of transferring multiple components are generally equivalent to the sum of individual component benefits. Regularizing the actor loss with behavioral cloning (using expert data from previous tasks) is advantageous while regularizing the critic loss is not recommended. Utilizing a pretrained policy for exploration in new tasks is effective. Proposed Method: Based on these findings the paper introduces ClonEXSAC a modified version of SAC which exhibits superior performance on CW10 and CW20 benchmarks. Strengths: The study provides thorough experimental analysis testing hypotheses on both pairoftask scenarios and CW10CW20 benchmarks. The proposed ClonEXSAC method follows logically from the analysis and achieves stateoftheart performance on CW10CW20 benchmarks. Weaknesses: The study primarily focuses on SAC and CW benchmarks potentially limiting the generalizability of the findings. However the authors acknowledge these limitations.", "Paraphrased Statement This paper examines how to enhance knowledge transfer in continual reinforcement learning (CRL) using the Soft ActorCritic (SAC) algorithm. The study investigates three knowledge transfer methods in a twotask scenario and combines SAC with various continual learning (CL) algorithms in the CRL setting. It evaluates different exploration strategies data rehearsal and critic regularization techniques. The findings are combined to develop a SAC agent that outperforms existing methods. While the paper focuses on SAC the authors acknowledge that the findings may not generalize to other RL algorithms. The evaluation is conducted on the Continual World benchmark. Strengths and Weaknesses Strengths: Tackles the important topic of knowledge transfer in CL Clear and wellwritten presentation Weaknesses: Lacks significant originality primarily evaluating combinations of existing ideas Contributions are not explicitly stated in the introduction Generalizability of findings to other RL algorithms is not discussed Questions: q1. Contributions of the Paper Examines knowledge transfer methods in CRL using SAC Evaluates various CL algorithms in combination with SAC Explores exploration strategies data rehearsal and critic regularization techniques Develops a SAC agent with enhanced knowledge transfer capabilities q2. Transferable Knowledge Between Actors The paper does not specifically discuss the transfer of knowledge between actors. However it is possible that the evaluation methods such as data rehearsal and critic regularization may contribute to the transfer of knowledge between different actors or tasks handled by the SAC agent.", "Summary: The study analyzes key design choices in SAC (Soft ActorCritic) and their impact on transfer learning in continuous tasks using the Continual World benchmark. They apply their findings to SAC to achieve top performance on Continual World while using a minimal set of modifications. Strengths: Thorough experimentation and analysis: Comprehensive experiments with intuitive summaries and highlevel trends. Writing clarity: Wellwritten with a logical flow for easy reading. Contributions: Valuable insights into continual learning in RL providing useful knowledge for the research community. Weaknesses: Environment dependency: Results are limited to the Continual World environment suite raising questions about their applicability to other tasks. Lack of failure case analysis: More detailed analysis of specific failure cases in the main paper would enhance understanding. Minor Concerns: Grammatical error: \"analyses\" should be \"analyzes\" in line 81. Potential limitation of feature transfer: Lack of diverse data in tasktotask transfer experiments may contribute to deterioration in forward transfer. Inclusion of a figure showcasing the Continual World benchmark would provide context for the results. Question: The authors are asked to speculate on the transferability of their findings to different observation spaces such as imagebased RL."], "qHs3qeaQjgl": ["Paraphrased Summary: This study explores a method for comparing probability distributions over a domain under the assumption that we can: 1. Evaluate the probability at any specific point (called \"DUAL access\") 2. Generate samples from the distribution given a subset of two points (called \"PCOND access\") The method considers a nonstandard definition of closeness where it accepts when the probability density functions (PDFs) of the two distributions are within a certain range and rejects when their total variation (TV) distance is large. The algorithm developed requires only a number of queries proportional to n eta2 sqrt(n) eta4 where n is the domain size and eta is a parameter controlling the closeness tolerance. This complexity avoids the exponential growth seen in traditional closeness testers based solely on sampling. The algorithm outperforms an existing baseline called \"Barbarik2\" in both theoretical and practical evaluations. Concerns: However there is a concern that a previous study by Canonne and Rubinfeld already provided a simple and optimal tester for this problem using DUAL and sample access alone with a query complexity of 1(eta2 epsilon2). This tester may be more efficient than the one presented here as it does not require PCOND access and has no linear dependence on n. Additional Questions: 1. Weighted dDNNFs were mentioned as an example of a distribution where DUAL access is possible. Can you provide more details on how they are used in the experiments 2. How was PCOND and DUAL access simulated in the experiments 3. The guarantees of Barbarik2 and Pacoco are not directly comparable because Barbarik2 only assumes conditional sampling access.", "This paper proposes a probabilistic approach to determine the proximity of samples to highdimensional distributions. Using a framework introduced by Chakraborty and Meel the method rejects samples with large Total Variation (TV) distance and accepts those with small multiplicative distance. The paper focuses on distributions defined on ndimensional hypercubes and utilizes COND PCOND and DUAL oracles to provide the sampler with information beyond pointwise probabilities. These oracles have been used to demonstrate improved sampling efficiency in other studies but their practical implementation is not discussed in this paper. Compared to previous methods that required an exponential number of samples with respect to dimension the new algorithm achieves a linear sample complexity with formal guarantees and empirical validation. Empirical results demonstrate significant improvement in sample efficiency across various problem settings. Strengths: Formal proof guarantees with exponential speedup. Empirical evidence of speedup upon implementation. Weaknesses: The paper lacks an evaluation of performance in terms of wallclock runtime. It is unclear if the theoretical bounds for the previous method are tight or if it may perform well with fewer samples.", "Paraphrased Statement: Summary: Determining whether two distributions \\(P\\) and \\(Q\\) are close or far apart (in terms of total variation distance) is a common problem in testing distributions. A key setting is when the distributions are supported on the \\(n\\)dimensional hypercube. Algorithm: Pacoco a new algorithm offers a nearly linear sample complexity of \\(\\tildeO(\\sqrtn\\logn)(\\eta 11.6\\epsilon)n\\eta2)\\) to distinguish close and far distributions. Pacoco uses conditional sampling queries (COND and DUAL) that condition on subsets of the support or query probabilities of specific elements. Comparison to Barbarik2: Barbarik2 another recent conditional sampling algorithm has exponential sample complexity. Pacoco outperforms Barbarik2 in experiments on various distributions even though Pacoco uses the potentially stronger DUAL oracle. Strengths: Nearlinear sample complexity in \\(n\\). Conditional sampling oracle model is based on previous work. Experiments support the claimed sample complexity and indicate reasonable constants. Weaknesses: Incompatibility of model assumptions for algorithms and bounds. Only a specific set of parameters (\\(\\epsilon\\) \\(\\eta\\) \\(\\delta\\)) is tested without justification. Lack of running time comparison with Barbarik2. Questions: Clarification on the exact conditional sampling oracle models assumed by Barbarik2 Pacoco and the lower bounds. Explanation for the steep ascent in Pacocos samples for large instances on wSTS."], "r4RRwBCPDv5": ["Paraphrase: This paper analyzes the \"double descent\" phenomenon using the theory of VapnikChervonenkis (VC) generalization bounds. The analysis demonstrates that double descent can be fully explained by classical VC theory. Both theoretical and experimental findings support this conclusion. Strengths: The approach is theoretically sound and wellgrounded. It provides an explanation for double descent using a wellestablished theory. Weaknesses: It does not introduce any new theoretical concepts. The experimental results are limited in scope. Questions: It is not always true that the weight norm decreases with increasing parameters. How can this be addressed Can similar results be obtained using Rademacher or Local Rademacher Complexity", "Paraphrase: The authors offer a new explanation for \"double descent\" a phenomenon where classifier performance initially improves with increased model capacity then declines. They argue that a VCtheoretic analysis can explain this behavior unlike previous claims. Their analysis incorporates two modifications to existing approaches: Using relative deviation VC bounds (which adapt between slow and fast learning rates) instead of slow rate uniform deviation bounds. Employing normbased bounds on the VC dimension for larger models (e.g. VC \u2264 min(dimension of W2 1) for linear classifiers). They compare their VC bounds behavior to training and testing errors in double descent scenarios such as linear classification using random Fourier or ReLU features and onehiddenlayer ReLU networks. Strengths: Introduces new VC bounds in this context. Suggests that double descent results from minimizing a norm rather than an error after reaching the interpolation threshold. Weaknesses: Clarity: Grammatical errors and lack of explanations. Rigor: Ad hoc assumptions such as approximating the VC dimension of a shallow neural network by the final layer norm. Modification of VC bound constants for practical applications without theoretical justification. Rescaling z values to [1 1] instead of [0 1] for equation (3) may be incorrect.", "Paraphrase: Summary: The paper attempts to assess whether relative VapnikChervonenkis (VC) bounds hold true for the double descent phenomenon. Random features are generated and a regularized least squares technique is used in feature space. The resulting weights are analyzed using relative VC bounds. The obtained results indicate the double descent effect particularly for the unregularized case. Similar experiments are conducted for neural networks with one hidden layer. Strengths and Weaknesses: Unfortunately the paper contains a fundamental misconception regarding VC bounds. VC bounds require a fixed class of hypotheses independent of the data samples observed later. It is improper to utilize a VC dimension estimate obtained after training (via weight norms) and the discussion on page 3 is thus flawed. To apply VC bounds one must prove beforehand that the algorithms weight vector norm does not exceed a predetermined threshold. This threshold rather than the observed norm should be used in the bounds. The authors idea of using relative bounds instead of uniform bounds is also misguided since both types depend on hn where h is an upper bound of the VC dimension and n is the sample size. The primary distinction between these bounds is that uniform bounds depend on hn quadratically while relative bounds depend linearly in the ideal case. Questions: It is essential to address the identified critiques. Otherwise it remains unclear whether the papers conclusions are valid."], "vExdPu73R2z": ["Paraphrased Statement This research presents a Robust Referring Video Object Segmentation (Robust RVOS) task that utilizes unpaired video and text inputs. A pipeline is proposed to optimize the target referring and dual expression reconstruction tasks. A relational cycle consistency constraint is introduced to improve semantic alignment across modalities. The method surpasses existing benchmarks on three datasets. Strengths The paper introduces a new Robust RVOS dataset to assess the robustness of RVOS models. Relational cycle consistency is employed to maintain the feature space structure rather than only pointwise consistency showing superior results. The proposed approach outperforms previous benchmarks on three datasets and ablation studies confirm the efficacy of each component. The papers writing is generally clear. Weaknesses The main novelty of Robust RVOS is questionable as it does not address the challenge of unpaired video and text inputs that other RVOS methods already handle. While the paper introduces a new task and dataset ablation studies are conducted based on existing datasets potentially underrepresenting the need for separate modules in Robust RVOS. The text reconstruction approach which is commonly used in VideoLanguage tasks may not be uniquely applicable to Robust RVOS. Questions 1. Formula 8 introduces l(A\u0302) as a measure of semantic consensus between reconstructed and original text features but its calculation should be clarified. 2. The paper does not specify how the model is evaluated on the Robust RVOS dataset. Is it trained on RefYoutubeVOS and directly tested on Robust RVOS 3. Ablation experiments on RefYoutubeVOS which does not require separate modules due to paired inputs may not fully validate the effectiveness of the modules in Robust RVOS. 4. The source of negative sample videos in the dataset should be disclosed. If they are from RefYoutubeVOS each video may correspond to a positive text hindering the assessment of robustness. Videos with no corresponding text in the dataset would better test the models robustness.", "Paraphrased Statement: Summary: Researchers present a new approach to video object segmentation using referring expressions. This approach recognizes that a given expression may not refer to any object in the video. To address this they developed a method based on relational cycle consistency. The proposed method includes an additional head to predict whether the expression matches an object in the video. Its effectiveness was tested on two industrystandard benchmarks. Strengths: 1. The paper is clear and easy to follow with precise definitions of symbols and notations. 2. The authors introduce a method that can handle expressions not associated with any video objects using a simple head likely a fully connected layer. 3. The method was assessed on two standard benchmarks demonstrating strong performance on both datasets. Weaknesses: 1. While previous methods dont explicitly address the possibility of expressions that dont correspond to any video objects adding a background class to ReferFormer [1] or similar methods could potentially address this. 2. Researchers have utilized cycle consistency (text reconstruction) in image referring expression segmentation. As a result this study primarily expands existing ideas to video. Questions: 1. What is the range of value A can take 2. Under what conditions does the indicator function 1(A) equal 1", "Paraphrased Statement: This study introduces an improved method for referring video object segmentation (RVOS). It highlights a flaw in existing RVOS datasets: the assumption that the described object is present in the video. This assumption is problematic in realworld textvideo query scenarios. To address this issue the paper proposes two strategies: 1. Creating a new dataset R2YouTubeVOS which includes both positive and negative videotext alignment pairs to assess a models false positive rate. 2. Developing a method that enforces semantic compatibility between visual and textual features. Experimental results demonstrate that the proposed method outperforms current approaches on RefYouTubeVOS and RefDAVIS datasets and has significantly fewer false positives on R2YouTubeVOS. Strengths: Acknowledges and addresses a significant limitation in current RVOS datasets. Introduces a novel method for enforcing semantic consistency. Provides rigorous experimental evaluations. Weaknesses: Insufficient details about the R2YouTubeVOS dataset specifically the number of negative textvideo pairs. Questions: Can you clarify the meaning of \"constrain all negative textvideo pair unrelated\" in lines 242243 Suggestions: Expand the background section with relevant literature on visionlanguage representation learning. Replace \"medium\" with \"proxy\" in line 58. Label fm as an intermediate feature rather than a medium in line 112. Correct the typo \"textural\" to \"textual\" in line 109. Separate the caption for Figure 2c into two sentences. Mention the use of Swin or VideoSwin as the backbone when discussing the window size in line 259. Include a reference to Figure 4 in the text. Modify \"temporalconsistent\" to \"temporally consistent\" in line 320. Provide a more thorough discussion of quantitative results highlighting the differences between the proposed method and the ReferFormer architecture."], "lXuZaxEaI7": ["Paraphrase: Summary: In reinforcement learning this paper suggests a novel concept called batch size invariance and a method to achieve it by separating the target policy from the behavior policy. The authors use an exponentially weighted moving average of the target policy to obtain batch size invariance and demonstrate improvements in several experiments. Strengths and Weaknesses: Originality: The proposed method is innovative and uses an exponentially weighted moving average for batch size invariance which is a unique approach. Clarity: The paper is wellwritten and easy to understand. The method is straightforward and can be implemented without difficulty. Significance: The authors argue that large batch sizes provide lower variance estimates of policy gradients but are often limited by computational resources. They claim that their method addresses this issue but there are simpler alternatives like gradient accumulation. Experiments: The experiments (Fig. 1) indicate that the most effective behavior policy is the most recent policy which is typical for onpolicy methods. Motivation: The authors should provide a stronger rationale for their method. Questions: 1. What is the fundamental difference between gradient accumulation and the proposed method 2. Can you provide intuitive examples where batch size invariance is crucial", "Paraphrased Statement: This study explores how to make PPO and PPG reinforcement learning algorithms insensitive to batch size. They propose adjusting hyperparameters to compensate for changes in batch size. The researchers introduce a distinction between the old policys two roles in PPO and PPG and decouple these roles. This allows them to develop batchsizeinvariant variations of PPO and PPG. They show that these variations mostly maintain batchsize invariance across the ProcGen task suite. Moreover the new variants outperform the standard versions of PPO and PPG. Strengths and Weaknesses: The paper is wellwritten and presents the concept of batch size invariance policy decoupling and hyperparameter adjustments clearly. The experiments support the authors claims that their adjustments make PPO and PPG batchsize invariant and the improved performance of the new variants is a valuable contribution. However the studys lack of clear motivation for batch size invariance could be improved. The authors mention some potential benefits but providing realworld scenarios where batchsize invariance would be advantageous would strengthen the argument. Additionally while the study explores the effectiveness of the adjustments in reducing gradient variance experimental evidence supporting this claim would bolster the papers findings. Similarly evaluating the generalization performance of the proposed methods on unseen ProcGen levels would provide valuable insights.", "Paraphrase: Summary: The researchers present a method to make reinforcement learning algorithms like PPO immune to changes in batch size. This means that by adjusting certain settings the algorithms performance can be maintained even when the batch size is modified. Strengths and Weaknesses: Strengths: The paper is wellwritten and easy to follow. Equations are clearly labeled. Visual aids (e.g. Figure 3) are helpful. The authors acknowledge that the improvement in performance with their method is small particularly when using the EWMA technique. Weaknesses: The purpose of batch size invariance is not fully clarified. While the paper mentions a study with a similar approach it does not provide a comparison. The claim that Figure 1(c) \"holds back learning\" is questionable because the performance does not appear to be hindered. Figure 4 seems to have only two sets of bars instead of the expected four. Questions: Why is batch size invariance a desirable property How does the proposed method compare to the previously published approach mentioned in line 76 Why shouldnt Figure 4 contain four sets of bars Suggestions: Include a literature review for context. Highlight the potential benefits of the proposed method in a setting where the error due to coupled objectives is significant and decoupling can mitigate this error. Explore the applicability of this decoupling concept to actorcritic algorithms and its potential for improving both actor and critic performance. Additional Note: The appendix was included with the main paper.", "Paraphrased Statement: This research aims to make policy optimization algorithms like PPO batchinvariant. Changing the batch size (the number of samples collected per update) significantly affects policy learning. Batch invariance allows for larger batch sizes without requiring more computational resources. In traditional gradient descent batch invariance is achieved by adjusting learning rates. However this method compromises the policys learning speed due to the influence of both learning rate and KL constraints. This paper separates the behavior policy from the proximal policy arguing that they dont need to be the same. While the behavior policy used to gather data should be accurate its age is less important. In contrast the proximal policy used in the KL constraint affects the policys rate of change. The paper proposes a decoupled clipped objective that enables the use of an exponentially weighted moving average (EWMA) of the policy network as the proximal policy ensuring invariance for variable policy update sizes. This method combined with additional batch invariance techniques in gradient descent maintains performance across different batch sizes. Strengths: Emphasizes the importance of batch size in policy learning which is often overlooked in publications. Practical advice for beginners on tuning PPO parameters. Novel decoupled objective that provides a new perspective on PPO constraints and objectives. Comprehensive recipe for achieving batch invariance in PPO with practical modifications. Weaknesses: Experiments only conducted on procgen raising questions about performance on continuous control tasks."], "tfkeJG9yAX": ["Paraphrased Summary: This research provides a method to estimate the lower bounds for approximation in the Lp norm. This method builds on a probability theory that links the packing number and shattering dimension of a function set. Using this theory approximation in the Lp norm can be derived easily. By utilizing existing knowledge about the pseudodimension of neural networks the authors extend the Lp norm approximation result to neural network settings. Additionally they apply this result to H\u00f6lderball and monotonic function scenarios to highlight similarities and distinctions between approximation in the sup and Lp norms. Strengths: Studying approximation in the Lp norm is valuable theoretically. While the approximation result is expected based on previous work it complements our understanding of approximation. The paper is wellwritten and accessible. Weaknesses: The practical implications of moving from the sup norm to the Lp norm are unclear. Questions: Could the authors clarify the potential practical applications of the Lp norm approximation result in comparison to the sup norm approximation results", "Paraphrase: This study establishes a new minimum threshold for the worstcase error in approximating functions from a set F using functions from a set G. This threshold is then applied to the special case where G consists of piecewisepolynomial feedforward neural networks. The approximation error is measured using the Lp(\\mu) norm where p can be any value greater than or equal to 1. Strengths and Weaknesses: Strengths: The paper is original clearly written and wellstructured. The authors provide detailed proofs of their claims. Weaknesses: The practical significance of the results is unclear. The findings are highly theoretical and may not have direct applications in realworld scenarios. Questions: Can the lower bounds established in the paper be used in practical applications Line 132 should be corrected to replace \"finite\" with \"compact\" (or an equivalent term).", "Paraphrase Summary This paper investigates the approximation of functions in a family F by functions in another family G. Specifically it examines the lower bound of the best approximation error for realvalued functions: \\supf \\in F\\infg \\in G \\Vert f g\\VertLp(X\\mu) where X is a subset of \\mathbbRn and \\mu is a probability measure on X. The paper establishes a lower bound for bounded functions based on an inequality constraint involving the packing number of F and the fatshattering dimension of G. This result is then used to derive lower bounds for approximating H\u00f6ldercontinuous functions and multivariate monotonic functions by piecewise polynomial feedforward neural networks. The technical approach relies heavily on a previous result by Mendelson. However the application of this result to solve an open problem is significant as it was previously believed to require methods beyond VC dimension theory. Proving the lower bound of the packing number for different function families is also nontrivial. Strengths Overcomes previous limitations by considering approximation in the Lp norm which was an open problem. Employs VC dimension theory to solve a problem that was previously thought to be beyond its reach. Weaknesses The lower bound is complex and its tightness is unknown. The core technical result is largely based on previous work. Questions How tight are the derived lower bounds Can the smoothness of functions be defined more explicitly Why is the term \"inequation\" used instead of \"inequality\" Can the sentence referring to the measure \\lambda be omitted Should the term \"cancels\" be replaced with \"vanishes\"", "Paraphrased Statement: Summary: This study examines the minimum possible approximation error for functions f when approximated by functions gw parameterized by w. The study focuses on approximation error under the Lp norm which measures the difference between the two functions. The paper provides results for general cases and specific cases where gw represents a feedforward neural network f represents Holder balls or monotonic functions and matching upper bounds are derived for these specific cases to demonstrate the effectiveness of the lower bounds. Strengths: Clear presentation of results and contributions Exceptional writing quality and accessibility Weaknesses: Some technical details lack sufficient explanation Questions: 1. Why are monotonic functions considered particularly significant for approximation performance 2. How does the value of p influence the approximation error bounds 3. Why are piecewise polynomial activation functions essential for the analysis How do other activation functions such as sigmoid affect the estimation of approximation error"], "zSkYVeX7bC4": ["Summary Paraphrase: This study evaluates the performance of large language models (LLMs) on extended sequences compared to their training data known as length generalization. The authors use two custom synthetic datasets: Parity Task: Predicts if a binary sequence contains an odd or even number of 1s. Variable Assignment Task: Predicts variable values based on Python snippets. Three LLM settings are examined: Finetuning Fewshot Prompting Scratchpad Prompting (augmenting inputpredictions with intermediate values) The results show that finetuning and prompting alone fail at length generalization while scratchpad prompting (with or without finetuning) is effective. This finding highlights the value of scratchpad prompting as a simple and efficient technique. Strengths and Weaknesses Strengths: Controlled experiments for testing length generalization. Comprehensive experiments in various settings. Demonstration of the benefits of scratchpad prompting over finetuning and fewshot prompting. Weaknesses: The parity tasks justification is weak and may be faulty as knowing the historys parity eliminates the need for the history. The experiments focus on a specific mode of length generalization related to comprehending long sequences rather than generating them. The paper lacks experimental details (e.g. dataset sizes epochs finetuned finetuning method). Related work on synthetic tasks bias and counting behavior in neural networks is missing. Questions: Have the authors considered the interplay between finetuning and catastrophic forgetting Could prompt tuning offer benefits over finetuning or fewshot prompting Does the parity task solely measure counting behavior and how do prior works on this topic relate to the findings Were alternative synthetic tasks (e.g. ListOps Zaremba and Sutskever tasks) considered and why not", "Paraphrased Summary: This study examines how large language models (LLMs) handle length generalization during finetuning and incontext learning. To eliminate superficial clues in test data that LLMs might exploit for accurate predictions without true generalization the study employs two controlled synthetic tasks: parity and variable assignment. By analyzing finetuning and incontext learning on these controlled tasks the study finds: Finetuning fails at length generation regardless of model size. In the parity task with fixed input bitstring length but varying bit 1 count transformers prefer parallel over sequential strategies but still fail to generalize to longer bit 1 count. Contrary to previous work indistribution loss is not an indicator of outofdistribution generalization. In addition to finetuning scratchpad finetuning and prompting are also studied. Scratchpad prompting outperforms finetuning and scratchpad finetuning although combining finetuning and scratchpad prompting only improves performance in the parity task. Strengths and Weaknesses: Strengths: The study uses controlled experiments with synthetic tasks to assess length generalization of LLMs in finetuning and prompting settings. Meaningful conclusions are drawn by controlling sequence length and task parameters. Three popular LLM applications (finetuning scratchpad finetuning and prompting) are evaluated. Weaknesses: The length generalization studied is linear as task information can be stored incrementally in a context window. The influence of output constraints on 0s and 1s remains unexplored. Question: Regarding Footnote 2 the study did not constrain model outputs to 0s and 1s. Exploring the potential impact of output constraints on the findings would provide insights for downstream LLM applications that allow for such constraints (e.g. constrained decoding in machine translation).", "Paraphrased Statement: Transformers receive various types of input. Longer inputs tend to necessitate more reasoning steps. The authors examine how input length affects model performance by evaluating multiple strategies using artificial tasks. They demonstrate that transformers struggle to extrapolate to longer (and sometimes shorter) input lengths. Strengths: Explores the ability of transformers to extrapolate using scratchpad prompting and finetuning for longer sequences. Provides insightful analysis on why models fail in certain scenarios. Weakness: Experiments are limited to artificial tasks. Questions: Different prompting methods may yield varying results. Could using a different parity prompt improve performance Are there realistic tasks that could be used to further assess the findings", "Paraphrased Statement: Summary: This study demonstrates that large language models (LLMs) struggle to extend their abilities to longer sequences when trained on tasks of a specific length. Extending sequence length introduces challenges related to extrapolation and data scarcity. The study proposes a method that combines finetuning scratchpad and fewshot techniques which can partially mitigate this issue for models up to 1 billion parameters. Strengths: Addresses a crucial problem in LLM deployment and usability. Explores the scalability of techniques moving beyond a single model size. Examines model \"shortcuts\" to uncover limitations. Weaknesses: Figures should provide accuracy over multiple sequences rather than single sequences. Lack of a clear test set and average accuracy across sequence lengths. Accuracy fluctuations in Figure 5 raise concerns about undertraining. Inconsistent terminology between \"chain of thought\" and \"scratchpad.\" Noise in Figure 6 (right) may hinder publication. Suggestions: Consider plotting logprobability on a loglog axis to reveal scaling properties. Reverse plot order for vanilla finetuning and scratchpad finetuning. Provide examples and empirical results for each technique in Table 1. Relocate Figure 2 to supplementary material and promote results plots. Provide an example for the \"Instance Length as Number of Steps in a Markov Process\" section. Questions: Clarify the \"Finetuning prompting scratchpad\" method. Explain \"thresholding the output\" in the \"shortcut\" solution. Confirm that finetuned models did not output nonzero or nonone tokens due to model issues. Provide data to support the claim of poor length generalization for 100B model scales."], "wUUutywJY6": ["Paraphrased Summary: This paper focuses on disambiguation in partial label learning where labels are incomplete and the distribution of classes is highly skewed (longtailed). The authors present SoLar a framework based on Optimal Transport that refines disambiguated labels to align with the class prior distribution. Experiments demonstrate its effectiveness on various datasets compared to existing methods. Strengths: Clear and concise presentation. Technically sound proposed method. Weaknesses: Motivation: Claim that existing PLL methods assume classbalanced data is inaccurate. Many SOTA PLL methods can handle imbalanced classes. Only one specific pseudolabelingbased PLL method (PRODEN) is presented as evidence of performance degradation in the longtailed setting which is not convincing. Questions: The authors should compare their method to more SOTA PLL methods to prove their claim of performance degradation in the longtailed setting. Experiments should be conducted on benchmark PLL datasets (e.g. lostBird Song Soccer Player Yahoo News) to further support their motivation.", "Paraphrased Statement: This research tackles the challenge of learning from partially labeled data where examples are assigned a set of possible labels. The focus is on addressing two key issues: Problem 1: Existing PLL methods struggle to identify true labels from candidate labels because their pseudolabels are biased towards common labels. To counter this an optimal transportbased approach is proposed that aligns pseudolabels with class frequency. Problem 2: Estimating class prior for PLL training is difficult without external data. This work introduces a novel class prior estimation algorithm and sample selection mechanism for practical usage. Strengths: Addresses the imbalance in realworld PLL datasets. Proposes an OTbased framework to mitigate bias in pseudolabeling. Introduces a new class prior estimation technique. Provides theoretical \u043e\u0431\u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0435 for the proposed OT objective. Demonstrates strong performance in experiments. Weaknesses: Evaluation is limited to CIFAR datasets. Candidate label counts are not explored as an alternative class prior estimation method. Typos and figure caption errors exist.", "Paraphrased Statement: Summary: This paper introduces a novel approach called Solar for imbalanced partial label learning where each data point has multiple possible labels and the underlying label distribution is uneven. Solar refines pseudolabels by enforcing two constraints: allocating labels within candidates and matching class priors. It also presents a movingaveragebased algorithm for generating class priors for these constraints. Experiments on CIFARLT and CUB200 datasets demonstrate Solars effectiveness in enhancing accuracy with few labeled samples. Strengths: 1. Solar tackles the understudied problem of longtailed learning in partial label learning. It analyzes the limitations of existing PLL methods and proposes an innovative framework that can advance research in this area. 2. Solar is backed by a rigorous theoretical framework. Its objective function aims to converge to the true classifier and the paper provides detailed explanations of the SinkhornKnopp algorithm used for optimization. 3. Comprehensive experiments validate the effectiveness of Solars label refinement procedure. The ablation studies and provided source code ensure reproducibility. Weaknesses: 1. In the experiments the PRODEN method also employs mixup and consistency training for fair comparisons. Evaluating Solar against other competitive baselines that do not utilize these techniques would provide insights into the impact of these methods. 2. The paper does not fully explain why the proposed sample selection mechanism helps maintain the label distribution. 3. The appendix introduces a relaxed version of the SinkhornKnopp algorithm. It is unclear how this relaxation affects convergence and whether Solar consistently utilizes this relaxed version. 4. The paper does not discuss how the gamma parameter in the SinkhornKnopp algorithm influences performance. 5. Figure 1 shows a class distribution estimate for PRODEN but it is unclear how this was computed."], "pDUYkwrx__w": ["Paraphrased Statement: Researchers have demonstrated that Noisy Stochastic Gradient Descent (SGD) offers a finite privacy loss in scenarios involving convex optimization with smooth Lipschitz functions defined over a restricted domain. The proof strategy utilizes both privacy amplification through random sampling and iterative amplification. Additionally the authors present a lower bound confirming that the established upper bound is optimal within a constant factor. Strengths and Weaknesses: Strengths: The paper is clearly written and wellorganized. It provides a thorough background on the problem and related research. The result is significant as it challenges existing assumptions about privacy loss growing indefinitely. The generalization to smooth nonstrongly convex functions is a notable contribution. Weaknesses: No significant weaknesses identified.", "Paraphrased Statement In this work we demonstrate that running the DPSGD algorithm on a convex function within a bounded domain ensures convergence towards a finite privacy cost unlike classical composition approaches that exhibit unbounded growth. Similar to previous research by Chourasia et al. (2021) we assume privacy of the algorithms internal state (intermediate parameter values) while allowing the disclosure of the final result. Our novel analysis involves dividing the parameter history into two segments and evaluating the privacy cost separately for each segment. This approach relies on a new and enhanced privacy amplification by iteration (PABI) method. We also establish a lower bound for the privacy guarantee. Strengths and Weaknesses Our main result (the upper bound) is significant extending the findings of Chourasia et al. (2021) by considering subsampled GD (computationally preferred) and replacing the assumption of strong convexity with general convexity. The paper demonstrates considerable original work to address the research question. The proof of the main theorem (Thm. 3.1) incorporates various novel techniques and results including the improved PABI result. However the papers assumption of a bounded search domain limits its applicability although this assumption may be reasonable for some practical scenarios. The presentation of the paper is lacking the supplemental material provides a more comprehensive version but the main paper is overly condensed and requires further refinement by moving less critical content to the appendix. The abrupt ending without discussion or conclusions is a consequence of space constraints. Questions Despite claiming that the noise is realized with probability 1 \u2212 bn the paper suggests that the probability is bn. The derivation of the inequality on page 14 of the supplement is unclear appearing to exclude the first term of the summand. It is uncertain whether adaptive stepsize schemes like Adam can be incorporated into this analysis without introducing sidechannel privacy vulnerabilities through the use of previous gradients.", "Paraphrased Statement: Summary: This study introduces a method for estimating the asymptotic privacy limits of Noisy Stochastic Gradient Descent (SGLD) for convex functions when the number of iterations (T) is large. Initially privacy declines linearly as T increases. However after a \"burnin\" period privacy reaches a constant level and continued SGD iterations do not further compromise privacy. The analysis employs techniques based on Privacy Amplification by Iteration (PABI). A lower bound construction based on random walk trace analysis is presented to demonstrate the tightness of the proposed bounds. Strengths and Weaknesses: Pros: Tackles a fundamental issue in NoisySGD analysis and offers a novel asymptotic perspective on privacy bounds. Introduces new techniques for addressing the problem. Wellstructured with key points and proof sketches outlined in the main text and detailed proofs provided in the supplement. Provides thorough overviews of existing NoisySGD analysis. Cons: Focuses solely on convex functions avoiding the more complex nonconvex cases (such as deep networks). Lacks numerical experiments to validate the proposed framework. Questions: Can the framework be extended to bounded nonconvex functions with regularization (as in Li et al. 2019 and Chourasia et al. 2021) How can the bounds aid in cost minimization and privacy mechanism design How is D determined in realworld scenarios Does D depend on the dimension and convergence of the convex problem Does convergence affect privacy levels Suggestions: Provide simple numerical studies to demonstrate the efficacy of the proposed framework.", "Paraphrased Statement: Summary: This study examines the privacy implications of noisy projected stochastic gradient descent (NPSGD). The authors demonstrate that after a certain number of iterations NPSGD stops further compromising privacy if only the final iteration is released. This finding holds potential for private convex optimization applications. Strengths: Provides a novel privacy analysis for NPSGD in optimizing convex Lipschitzsmooth objectives with bounded parameters. Establishes that NPSGDs privacy leakage ceases after a specific number of iterations. Weaknesses: The practical implications of this analysis for NPSGDs utility are unclear. Limited in applicability to NPSGD it does not cover commonly used methods such as stochastic gradient clipping with added noise. Questions: How can the new analysis enhance our understanding of NPSGDs utility Can this analysis extend to more practical SGD variations such as clipped stochastic gradients with noise and provide insights for practitioners"], "vt516zga8m": ["Paraphrase: Largescale recommendation systems with multiple recommendation towers are widely used in the industry. However expanding datasets pose challenges in model training sometimes requiring random sampling of data which is inefficient. This paper introduces a new training process that employs inbatch importance sampling in distributed systems for training largescale recommender systems. The proposed approach includes: Importance sampling Cache augmentation The authors evaluate their methods against baselines and analyze parameters to guide users. Strengths: Addresses a crucial but often overlooked aspect of model training. Demonstrates that optimization of the entire training process including systems and algorithms can significantly improve model performance and complexity. Provides detailed technical explanations experiments and analysis. Weaknesses: Potential weaknesses in the literature review and baseline comparisons. Limited discussion on potential biases and societal impacts of these techniques. Future work suggestions could include candidate generation and retrieval for inference stages. Questions for Authors: How should readers choose which techniques to apply under different conditions What are the authors recommendations on integrating these techniques into existing training pipelines", "Paraphrased Statement: Summary: This paper analyzes the limitations of current negative sampling methods for retrieval tasks in general recommendation systems. It proposes an improved method that samples negative examples tailored to each userquery combination and demonstrates its potential for providing a more accurate approximation of the gradient during model training. Experimental results support the benefits of the proposed method. Strengths: 1. The research area is significant with negative sampling being crucial for training largescale retrieval systems. The authors effectively identify the shortcomings of existing methods. 2. The \"bias reducing theorem\" provides a theoretical foundation for the proposed method. Weaknesses: 1. While the paper mentions \"Theorem 3.1\" a finite batch size theorem would strengthen the results by addressing practical limitations. 2. The experimental results lack information about dataset sizes which raises questions about whether the proposed method is truly necessary or if naive global negative sampling could suffice. Questions: 1. The papers objective needs clarification: is it primarily theoretical innovation or practical algorithm development The ambiguity hinders the evaluation of its contribution. 2. The paper emphasizes hardware constraints that motivated the new method. It would be helpful to elaborate on these constraints and whether the method is only suitable for specific hardware architectures. 3. The bias reducing theorem has theoretical implications but its impact under typical batch sizes in realworld scenarios needs to be discussed. 4. The use of a cache to optimize representation retrieval is not adequately explained. It would be beneficial to clarify its role and how it contributes to the algorithms efficiency.", "Paraphrase: Summary This research develops a negative sampling technique within a batch that aims to enhance the effectiveness and efficiency of contrastive item recommendation. Strengths and Weaknesses Strengths The method is wellgrounded in theory and presented clearly. Comprehensive experiments on large datasets validate its performance. Weaknesses The method may overlook false negative samples when sampling from the entire softmax distribution. It primarily focuses on improving sampling efficiency from the softmax distribution. It lacks comparison with alternative negative sampling approaches that do not rely on importance sampling from the softmax distribution (e.g. DNS lambdaFM PRIS AOBPR). Questions 1. Provide a comparison of baseline methods like DNS lambdaFM and PRIS. 2. Explain how the proposed method addresses the issue of false negative samples. 3. Discuss the superiority of the proposed method over other sampling techniques that do not aim to sample from a softmax distribution in the introduction section.", "Paraphrased Statement: Summary: This research investigates a key issue in training recommender retrieval models using negative sampling within a batch. Existing methods face two main limitations: sampling bias and queryindependent negative selection. The proposed solution CacheAugmented Inbatch Importance Resampling (\u03c7IR) addresses these limitations by employing a querydependent resampling technique and incorporating hard negative samples. Empirical results demonstrate significant performance improvements over existing methods. Strengths: Importance: Addresses a prevalent problem in realworld recommender systems. Novelty: Introduces a novel sampling method that combines importance resampling and hard sample caching. Soundness: Provides theoretical analysis and empirical evidence to support the proposed design. Evaluation: Conducts extensive experiments on realworld datasets with notable performance gains. Presentation: Clearly presented and engaging for the NeurIPS community. Weaknesses: Data presentation: Table 2 contains excessive information highlighting best and secondbest results could improve readability. Analysis: More thorough exploration of factors such as batch size and popularity distribution would enhance understanding. Typos: Minor corrections needed (e.g. \"Assum that\" in Line 162 and Figure 3(c) ylabel). Questions: 1. Are there any space complexity considerations for the proposed method 2. To mitigate time complexity concerns could the cache update mechanism be performed less frequently and the potential impact on recommendation quality be analyzed"], "se2oxj-6Nz": ["Summary Paraphrase: Paper Overview: This paper presents a novel framework for image restoration that aims to enhance object detection performance. It proposes a finetuning pipeline that combines image restoration and object detection into a single task. The paper focuses on adapting restoration algorithms to generate highquality visual inputs thereby improving detection accuracy. Strengths: 1. The paper explores the relationship between image restoration and highlevel computer vision tasks like detection an understudied area. 2. It takes a unique approach of adapting restoration algorithms instead of modifying object detectors aiming to improve both restoration and detection. 3. The method proposed uses an ADAMbased adversarial example generation algorithm with theoretical support. 4. Extensive experiments demonstrate the frameworks superiority in both image quality and object detection accuracy. 5. The paper is wellwritten and clearly presents the method. Weaknesses: 1. Inconsistent notations for the restoration function (D(x) and D[R(x)]) in the text and equations. 2. Ambiguous use of the symbol \\hatx in Equation 6 which represents both a fixed value and a variable. 3. Misspellings in the captions of Tables 2 and 3 (\"detection\" is written as \"ddetection\"). 4. It is not specified how the initial parameters of the adversarial example generation algorithm (attack iterations step size and perturbation tolerance) are determined.", "Paraphrased Statement: Summary: This paper presents a unique training approach to enhance object detection in challenging environments like haze or low light. It involves training an image restoration model using pseudo ground truths generated through an adversarial attack (specifically an Adamlike targeted attack). The resulting model outperforms existing methods in object detection tasks in degraded scenes. Strengths and Weaknesses: Originality: The proposed approach is novel and departs from conventional methodologies. Quality: While the approach shows promise there are some limitations to address: The model performs poorly when trained on pseudo samples from the training set and evaluated on the test set. This may be due to minimal image differences after the attack making learning difficult. TOG a benchmark method achieves similar performance which suggests a limited technical contribution from this work. A baseline method that optimizes both image restoration and object detection simultaneously should be compared for a more comprehensive evaluation. Clarification: Some unclear aspects of the paper include: The meaning of \"Y\" in Tables 2 and 3 The definition of \"detection\" in Table 2s caption The reference for the 15 decrease in YOLOv3 mAP on unclean images (lines 227229) The purpose and procedures of the experiment in Section 4.2.1 Questions: Please refer to the \"Strengths and Weaknesses\" section for further inquiries.", "Paraphrase: The proposed method enhances object detection performance without compromising visual quality during image restoration. It does this by employing a target adversarial attack that targets the object detection task to improve detection and the restoration task optimization to maintain image quality. Strengths: Simple technique that improves object detection performance without affecting restoration performance. Adversarial samples are generated using secondorder gradients and cumulative momentum resulting in sharper images. Weaknesses: Lower performance compared to more complex methods in specific scenarios (e.g. Tables 2 and 3). Lack of zoomed visualizations for clarity. Insufficient explanation of how image properties are maintained during adversarial attack generation raising concerns about the naturalness of the adversarial images. Applicability to images with multiple degradations requires further exploration. Questions: Why is the performance of the proposed method lower in specific cases (Tables 2 and 3) Can the authors provide zoomed visualizations to aid understanding What mechanisms ensure the naturalness and plausibility of adversarial attack images Is the proposed method applicable to images with multiple degradations"], "yJE7iQSAep": ["Paraphrased Statement: This paper presents indepth analyses of variants of the S4 model focusing on the use of a specific matrix structure and parametrization as well as empirical studies on their impact. The authors demonstrate the effectiveness of a diagonal approximation of a particular matrix and provide explanations for its success. Strengths and Weaknesses: The papers strengths include comprehensive and wellorganized experiments and interesting discussions related to theoretical results. The target audience is limited and evaluation is based solely on classification or regression performance. The absence of qualitative or direct model output analyses is a potential limitation. Questions: 1. Figures 1 and 2: Specify the content of the graphs using axis labels and legends for better clarity. 2. Application to SequencetoLabel Problems: Clarify how the proposed sequencetosequence models are applied to sequencetolabel problems. 3. Theorem 2 Proof: Provide an intuitive interpretation for the intermediate result where u(t) is equated to BTx(t) for large N and the HiPPOLegS B matrix.", "Paraphrased Statement: Summary Paper: Simplifies Diagonal State Space Models (SSMs) by: Analyzing Eigenvalues of Model Matrices Suggesting Initializations for Stability Introducing Normalization for Variance Preservation Conducted Experiments: Validate Simplifications Show No Performance Degradation Key Takeaway: Diagonalization alone is insufficient for effective SSMs proper parameter initialization is crucial. Strengths: Clear Presentation Mathematical Basis for Simplifications Convincing Empirical Results Weaknesses: Limited Discussion of Limitations Evaluation on SmallScale Tasks No Proven Advantages over S4 Algorithm Questions: Elaborate on why S4 requires a correction term instead of diagonalization. Provide a clearer explanation of the transition from Conjecture 3 to S4DInv. Specify the S4D variant used in Table 1. Assess the robustness of S4D to variations in optimization hyperparameters. Explain practical reasons for preferring S4D over S4. Explore speed improvements and performance gaps between S4D and S4.", "Paraphrase: Introduction: Researchers have introduced a promising statespace modeling technique called HiPPo which uses linear systems defined in continuous time to capture longterm dependencies. HiPPo involves specific matrix structures called structured statespace architectures (S4) and diagonal statespace architectures (DSS). However practical applications of these architectures may encounter numerical issues due to poor initialization or parameterization. Contributions: This paper analyzes the diagonal approximation to S4 and proposes improved initialization methods that enhance numerical stability. The authors also provide theoretical support for these methods. They demonstrate the practical value of their analysis through an ablation study. Strengths: Addresses numerical concerns often overlooked in related research emphasizing parameterization for matrix stability. Introduces memorysaving techniques for storing matrices. Prescribes effective initialization methods with theoretical justification. Provides useful ablation studies to demonstrate the practical benefits of their analysis. Weaknesses: Inconsistencies in notation leading to confusion about variable dimensions. Lack of a proper conclusion summarizing the main findings. Insufficient emphasis on original contributions within the main text. Questions: Are the dimensions of the variables specified in the statement incorrect Is AD in Eq. (5) always the matrix of 1s on the diagonal What does DSS stand for in the context of the paper Is barC in Eq. (6) simply the original C row vector Should the order of B and C in Eq. (7) be reversed Can you provide a reference for the equation using the Vandermonde matrix What is meant by \"fully expressive\" when referring to randomly initialized matrices in Section 4 below Proposition 1", "Paraphrased Statement: Summary: Researchers studied the effectiveness of diagonalized variants of the S4 statespace model and developed S4D an improved approach based on existing diagonal statespace models (DSS). They demonstrated that the diagonal update in S4D mimics the dynamics of S4s diagonal plus lowrank update as the latent dimension increases. By optimizing initialization and parameters they enhanced DSS models in the S4D family. They supported their theoretical findings with ablations and showed that S4D achieves comparable performance to the base S4 model. Strengths and Weaknesses: The paper is wellwritten and combines theoretical and empirical results. Detailed ablations provide strong evidence for the theoretical conclusions. The paper clarifies why diagonal SSMs are effective unlike related methods like linear RNNs. The kernel computation method using a Vandermonde matrix could be improved with the parallel scan implementation which offers advantages such as variable sampling rates and unbounded sequence length. The \"three lines of code\" mentioned in the abstract should be included in the main text. Questions: Why do expressive parameterizations sometimes fail to optimize well Have the authors explored the interpretability of latent states in the diagonalized basis"], "mTXQIpXPDbh": ["Summary Back Razor is an algorithm for transfer learning on edge devices that saves memory by using sparse activations during neural network training. The algorithm has been tested on four image classification datasets and two architectures (CNN and ViT) and the results show that it can significantly reduce the memory usage required for training. Strengths The algorithm is simple to understand and implement and it can be used with any type of neural network. The experimental results show that the algorithm can reduce the memory usage required for training by up to 90. Weaknesses The algorithm has only been tested on a limited number of datasets and architectures. The theoretical analysis of the algorithm is incomplete. The writing in the paper could be improved. Questions How does the algorithm perform on other types of tasks such as segmentation or language understanding How does the algorithm perform with different batch sizes and optimizers What is the actual ondevice memory usage of the algorithm", "Summary To reduce memory consumption in transfer learning models the authors propose pruning activations only during backpropagation instead of both forward and backpropagation. Experiments on CNN and ViT architectures demonstrate that the method achieves comparable accuracy with more aggressive memory reduction compared to existing techniques. Strengths Clear rationale for focusing on compressing activations for backpropagation (Sections 1 and 3.1). Wellpresented techniques with theoretical analysis on convergence (Section 3.2 and 3.3). Convincing experimental results on CNN and ViT including theoretical and practical memory usage comparisons (Tables 2 and 3). Weaknesses Peak memory usage is not reported in the experiments leaving it unclear if the method reduces memory overhead in all scenarios. The unstructural sparsification strategy used can result in a discrepancy between theoretical and practical memory usage (Tables 2 and 3). Gradient checkpointing a traditional method related to the proposed technique is not included in the comparisons. Questions 1. Can peak memory usage during training be provided 2. Have structural sparsification strategies been explored 3. Can the method be qualitatively or quantitatively compared to gradient checkpointing", "Summary This work introduces Back Razor an efficient activation pruning technique solely during backpropagation. It provides theoretical evidence suggesting Back Razors convergence rate may match SGD and demonstrates its efficacy on CNN and Vision Transformer models. Strengths Clear organization and wellexplained concept Extensive experiments support Back Razors effectiveness Weaknesses Grammatical errors and awkward phrasing Lack of experiments on larger models (e.g. ViTL16) to fully assess Back Razors performance on pretrained models Questions In Figure 3 it would be beneficial to include plots showing both the memory footprint and accuracy of training when comparing pruning in both forward and backward passes versus only backward pruning.", "Paraphrased Summary This study investigates a scenario where users desire to locally train and finetune pretrained models. It introduces Black Razor an innovative approach that minimizes the memory consumption of neural networks during finetuning by compressing activations during backpropagation. Black Razor successfully achieves 96 sparsity resulting in a 9.2fold reduction in memory without compromising accuracy. Additionally the paper provides a theoretical analysis of Black Razors convergence. Strengths and Weaknesses Strengths: Effective in significantly reducing training memory without substantial accuracy loss. Simple and applicable to various models. Weaknesses: Novelty concerns: The method involves pruning activations during backpropagation which seems more like a technical trick than a foundational learning theory. Validation on additional downstream tasks would enhance the papers robustness. The theoretical results require clarification particularly regarding the apparent contradiction in the timing of activation pruning in Algorithm 1 and Lemma 1. Questions: Clarification on the discrepancy between \"Storeage\" in Figure 2 and the intended spelling \"Storage\" Explanation of the substantial memory difference between activations in forward and backward passes Rationale for the memory reduction exceeding 2x when pruning activations in backpropagation Exploration of Black Razors performance on language transformer models"], "oW4Zz0zlbFF": ["Paraphrase: Summary: The paper establishes generalization bounds for overparameterized linear models in meta learning for three methods: empirical risk minimization (ERM) modelagnostic meta learning (MAML) and implicit modelagnostic meta learning (iMAML). In meta learning there are multiple tasks each with its own labeled data. The goal is to find a shared parameter vector that fits all tasks. ERM minimizes the loss over all tasks. MAML and iMAML minimize regularized squared loss. The paper considers the \"overparameterized regime\" where the model has more parameters than data. The generalization bounds explain the phenomenon of \"benign overfitting\" in meta learning. Results: The generalization bounds follow the work of Bartlett et al. but include an additional term that captures crosstask data heterogeneity which refers to the maximum difference between the data covariance matrices of different tasks. This term is unique to meta learning. Methods: The methods generally follow Bartlett et al. but handle crosstask data heterogeneity. Strengths: The paper provides the first generalization bounds for meta learning under overparameterized linear models. The proof handles crosstask data heterogeneity. The paper is wellwritten. Weaknesses: The paper assumes an overparameterized model for multitask learning. The data heterogeneity term is not discussed in detail.", "Paraphrase: Summary: This paper explores whether using overparameterized nested metalearning models results in overfitting. It begins by analyzing the performance of simplified linear models in a metaregression scenario with a minimum norm solution. By breaking down excess risk into specific components the paper provides a bound for this risk. This bound leads to a condition for \"benign overfitting\" in metalearning which is supported by numerical simulations. Strengths: Extends the benign overfitting analysis from linear regression to metalinear regression. Insights into how data heterogeneity and model adaptation influence overfitting in metalearning. Experimentally verifies the theoretical findings. Clear organization and presentation of assumptions and key differences. Weaknesses: The benign overfitting condition in (13) lacks practical guidance for determining whether specific settings are benign. The use of \"nested metalearning\" is ambiguous as metalearning inherently involves nesting. The theoretical analysis and practical metalearning methods differ in terms of data model complexity and prediction tasks. Questions: How does using multiple gradient steps in MAML adaptation affect the analysis What settings violate the benign overfitting condition Why does increased data heterogeneity make it harder to satisfy the benign overfitting condition for MAML and iMAML How can the derived results connect to practical metalearning scenarios How can these results guide the design of more effective metalearning settings", "Paraphrase: This paper examines why overfitting is not necessarily detrimental in nested metalearning for meta linear regression. The analysis covers both MAML and iMAML methods. Numerical simulations support the theoretical findings. Strengths: The paper explores an intriguing phenomenon in meta linear regression. Weaknesses: The authors need to emphasize that the analysis only applies to meta linear regression which simplifies the study. The term \"nested metalearning\" is unnecessary because most metalearning approaches use a bilevel structure. The paper should address the gap between the analysis and practical MAMLiMAML methods which are used for fewshot learning with new categories. The authors should contrast the analysis techniques employed for benign overfitting with those presented in [1]. Questions: The authors should provide empirical evidence of benign overfitting in metalearning using various methods (even if they dont completely match the papers assumptions). The studys findings indicate benign overfitting in meta linear regression but its unclear if this phenomenon extends to other metalearning tasks. The authors should give concrete examples where the number of metatraining data points is less than the models parameters a condition that is not always present in metalearning.", "Paraphrase: Summary: This research examines the generalization performance of overparameterized metalearning models with a nested structure such as modelagnostic metalearning (MAML) and implicit MAML (iMAML). Assuming certain theoretical conditions the authors establish an upper bound for the additional risk associated with MAML and iMAML in overparameterized scenarios. They also determine the criteria for benign overfitting in nested metalearning and identify hyperparameters for MAML and iMAML that promote benign overfitting. Strengths and Weaknesses: The paper effectively explains the reasons for investigating this topic and clearly outlines the authors contributions. It also includes a proof outline for their primary finding enhancing the papers clarity. However the discussion of the experimental results could be improved. By providing more details and including numerical results in a table the authors could better demonstrate the significance of their theoretical findings. Question: Is there a simple explanation for why the crosstask variance upper bound is primarily dependent on 1M asymptotically"], "zJNqte0b-xn": ["Paraphrase: Summary: The paper presents an improved algorithm for solving minmax optimization problems with strong convexity and strong concavity on Riemannian manifolds. The algorithm guarantees fast convergence and has a stochastic variant. Strengths: Clear writing and solid technical analysis Addresses a conjecture from previous work Provides experimental results Weaknesses: Some proofs are difficult to follow The mathematical complexity may not suit all readers The practical applications of strong convexity and strong concavity on Riemannian manifolds are limited. Specifically robust PCA cannot be directly applied because it is only locally convex and concave and maintaining these properties throughout the algorithm is challenging. Questions: How can the analysis be extended to cover local strong convexity and strong concavity Can the authors identify a different optimization problem that is more suited to the theoretical analysis presented There may be an error in the second part of Definition 2.2 where \"convexity\" should be replaced with \"concavity.\"", "Paraphrase: The paper demonstrates that an existing algorithm from [1] (Sions theorem and Zhang et als algorithms) has a linear convergence rate for the final iteration in geodesic settings that are strongly convex and strongly concave. The paper also provides guarantees under the assumption of a noisy gradient oracle although these guarantees depend on 1\u03b5 and apply to nonstrongly convex and nonsmooth cases. These guarantees appear to be comparable to the best Euclidean guarantees aside from curvature parameters. Strengths: Detailed analysis of the performance of Riemannian extragradient algorithms in various scenarios. The original technical contribution is the linear convergence proof in the strongly convex strongly concave setting. Excellent writing and clarity. Generalizes a previous result in [1] on Sions theorem to geodesic settings. Weaknesses: Lack of examples for interesting Riemannian convexconcave problems. The robust PCA experimental example requires Riemannian geometry for its formulation which may not be a suitable example for all readers. The paper does not adequately explain the previous difficulties in proving linear convergence and how the new proof addresses them. Additional Questions: 1. The figure in Section 3.1 is difficult to read. 2. Can you provide a reference where RPCA (robust principal component analysis) is defined and used 3. It may be important to note that the condition number of gconvex functions depends on the radius in curvature 1 sometimes making the linear convergence rate irrelevant. 4. How were the difficulties in previous attempts to prove the linear convergence guarantee overcome", "Paraphrased Statement This paper explores optimization techniques for complex problems involving minimizing and maximizing a function on curved surfaces known as \"Riemannian manifolds.\" These problems are common in areas like optimal transportation and adversarial learning. The study focuses on comparing Riemannian optimization methods to their simpler Euclidean counterparts. It investigates whether theres a fundamental difference in their performance. The authors demonstrate that a specific Riemannian method (RCEG) performs as well as its Euclidean counterpart in certain types of problems where the surface is both strongly convex and concave. Strengths and Weaknesses Strengths: Addresses a pressing issue in machine learning applications. Provides a thorough review of previous work and clearly explains its contributions. Weaknesses: May have limitations which are not specified in the given information.", "Paraphrased Summary: This research paper presents a comprehensive analysis of Riemannian adaptations of Euclidean firstorder optimization techniques for solving optimization problems on manifolds. The paper demonstrates that the Riemannian Corrected Extragradient (RCEG) method exhibits linear convergence in the last iterate for problems with geodesic strong convexity and concavity. This result matches the performance of Euclidean methods and extends to stochastic and nonsmooth settings. Strengths and Weaknesses: The paper presents novel and significant findings. It establishes the linear convergence rate of the RCEG method in the geodesically convexconcave case. The authors also extend their analysis to stochastic and nonsmooth scenarios where RCEG and Riemannian Gradient AscentDescent (RGDA) achieve nearoptimal convergence rates relative to the manifolds curvature. Questions: 1. The first figure in Section 3 needs improvement in clarity and labeling. 2. The experimental section would benefit from additional comparative algorithms to provide a more comprehensive evaluation.", "Summary Paraphrase: The authors investigate the intricate nature of minmax problems on curved surfaces called Riemannian manifolds. They demonstrate that the last iteration of a specific algorithm (RCEG) converges linearly. This is a novel finding that hasnt been previously reported. They also explore variations of the algorithm involving randomness and functions that lack smoothness. Strengths and Weaknesses Paraphrase: Strengths: Focus on a compelling topic Wellwritten presentation Weaknesses: Typos and inaccuracies Excessive focus on assumptions already covered in previous work Lack of intuitive explanations for the new findings Questions: Which types of nonconvex constraints appear in Generative Adversarial Networks (GANs) Clarification of the term \"convex\" in the statement \"deriving ...for the convex case\" Explanation of acronyms in the Section 1.1 table Definition of manifold diameter in Proposition 2.1 Correction of a potential error in Definition 2.2 Concerns about the unicity of geodesics assumption: Does it exclude manifolds of positive curvature How does it interact with the sectional curvature assumption Clarification of which curvature (M or N) is referenced in Theorem 3.1"], "pFqgUJxXXz": ["Paraphrased Statement: Summary: This paper describes a method called \"task augmentation\" for improving metalearning models by generating new tasks with specific characteristics: Task aware: Consider the current tasks characteristics. Task imaginary: Generate tasks that are not part of the training data. Adaptive: Adjust the task generation process based on the models performance. Main Contribution: The paper introduces a novel task generation method that incorporates these properties to enhance the models performance. Strengths: Realistic problem: Task augmentation can mitigate the issue of \"metaoverfitting\" and extend the models capabilities. Wellmotivated method: The method is sound and clearly explained. Weaknesses: Limited empirical results: While the method shows improvement in some cases the overall results are not particularly strong. Crossdomain tasks: The paper lacks a detailed discussion on the impact of task augmentation on crossdomain tasks which are important in realworld scenarios. Computational cost: The paper does not provide information on the cost of generating tasks. Theoretical analysis: The theoretical results seem less relevant and could be replaced with more empirical insights. Questions: Please provide additional information on the following: Generation cost and how task augmentation is incorporated during metatraining. Impact of task augmentation on crossdomain tasks. Confidence intervals for the classification results. Computational cost of the method.", "Paraphrase: This study introduces an adversarial task upsampling method that enhances the tasks used for metatraining. This method trains a network to minimize the distance between generated tasks and real tasks using the Earth Movers Distance (EMD) loss. It also incorporates an adversarial loss based on the gradient similarity between support and query sets to create challenging metatraining tasks. Experimental results demonstrate improved performance on fewshot regression and classification tasks. Strengths: The method matches the distribution of generated tasks to real tasks via EMD loss minimization. Adversarial training generates more difficult tasks. It achieves significant improvements on sinusoidal regression tasks. Weaknesses: In fewshot image classification real tasks are constructed using mixup which is known to enhance performance. The necessity of this mixup step is unclear. Its unclear if the methods improvement is due to mixup or the upsampling network. A nonmixup analysis is needed. The classification accuracy improvement over other methods is small and may not be statistically significant. Adversarial loss provides only marginal improvement in fewshot classification despite the prominence of \"adversarial\" in the title. The method has not been tested on standard FSL datasets like miniImagenet or MetaDataset where most toptier fewshot learning methods are evaluated.", "Paraphrase: Summary: This study introduces a method to expand tasks in a metalearning environment using an adversarial task upsampling network. This network generates coarse tasks and refines them into multiple samples close to the coarse task. It is trained using an adversarial loss to produce tasks that enhance the performance of the metalearner. The networks effectiveness is demonstrated on a sine regression task and image classification datasets. Strengths: Task augmentation is an established method for preventing overfitting in metalearning. This study presents a novel and adaptive task augmentation technique. The method has been tested on various datasets and benchmarks showing consistent improvements. The study includes adequate ablations and experiments to understand the models behavior. The paper is wellwritten and easy to comprehend. Weaknesses: The problem and solution are not particularly original. The theoretical analysis does not fully align with the algorithm. The manuscript does not clarify how labels are generated in the upsampling process for classification tasks. The role of gs() in the classification upsampling network is unclear. Table 6 could provide more information on the choice of a crossdomain setting with more samples in the target task. It would be helpful to see how the models perform when the domains have a similar number of samples (1shot setting).", "Paraphrase: Summary The paper proposes a new technique for enhancing task augmentation called Task Proposal Network (TPN). TPN is trained to generate task distributions that match the real task distribution and challenge the capabilities of the metalearning algorithm. This approach leads to improved generalization performance for both regression and classification tasks compared to existing task augmentation methods. Strengths The TPN method consistently outperforms other task augmentation methods in both classification and regression domains. The introduction of TPN introduces properties to the output task distribution that are beneficial for task generalization such as taskawareness taskimagination and modeladaptiveness. Weaknesses The effectiveness of TPN on regression tasks has only been evaluated on the sinusoidal regression task which while challenging is lowdimensional and artificial. Evaluations on higherdimensional regression problems are needed to strengthen the claims of superiority over other methods on regression tasks. The theoretical analysis regarding the relationship between Earth Movers Distance (EMD) loss and task awareness is unclear. It appears to assume a specific solution and then upperbounds it with EMD loss but the justification for this is not evident. The paper lacks detailed explanation on how EMD is computed between two tasks. This makes it difficult to understand how the task upsampling network is optimized particularly in terms of how task labels are handled. Reference Yao et al. (2022). \"Metalearning with Fewer Tasks through Task Interpolation.\" ICLR2022. Yao et al. (2021). \"Improving Generalization in Metalearning via Task Augmentation.\" ICML2021. PostRebuttal The authors have addressed the weaknesses raised by the reviewer. Specifically they have: Clarified the theoretical analysis and provided a more rigorous justification for the relationship between EMD loss and task awareness. Provided additional results on a regression task involving pose estimation demonstrating the consistency of performance improvement in regression tasks."], "nJJjv0JDJju": ["Paraphrased Statement: The authors propose enhancing a diffusionbased reconstruction scheme by incorporating the gradient of a loglikelihood term into its updates. This term draws upon the relationship between the intermediate representation (xi) and the original input (x0). To provide context the authors highlight the existence of other methods that leverage pretrained priors to solve various reconstruction tasks in an unsupervised manner. They emphasize the need to discuss these approaches in the introduction. Strengths and Weaknesses: 1. Introduction: Lack of introduction to related methods such as GANbased reconstruction and the PlugandPlay (PP) denoiser approach which also use pretrained priors. 2. Continuous Formulation: Redundant inclusion of the continuous formulation of the diffusion process in the introduction. 3. Notation Error: In Eq. 15 \"y\" should be \"y0\" and the argument of \\hatx0 should be xi. 4. Manifold Constraint: Improved presentation needed for the \"manifold constraint.\" 5. Ablation Study: The effect of different alpha values should be explored through ablation studies and discussed in the experiments. 6. Mismatch: Potential inconsistency between Eq. 15 and the algorithms used in the experiments should be verified. 7. Related Work: Citation of existing literature that may have used loglikelihood functions in iterative diffusion reconstruction schemes. 8. Weak Competitors: The competitors in the experiments appear weak. Inclusion of strong nonnaive GANbased reconstruction methods is suggested. 9. Iterations and Runtime: Discussion of the number of iterations and runtime of the different methods in the experiments section is necessary. References: [a] Bora et al. \"Compressed Sensing Using Generative Models\" 2017. [b] Hussein et al. \"ImageAdaptive GAN Based Reconstruction\" 2020. [c] Venkatakrishnan et al. \"PlugandPlay Priors for Model Based Reconstruction\" 2013. [d] Zhang et al. \"Learning Deep CNN Denoiser Prior for Image Restoration\" 2017. [e] Tirer and Giryes \"Image Restoration by Iterative Denoising and Backward Projections\" 2018.", "Summary (Paraphrased): The authors present a novel technique for conditional sampling using diffusion models. They combine scorematching techniques with a stochastic contraction and a modified score function. The modified score function considers the conditional distribution under a manifold constraint. This technique complements the unconditional score by locally projecting onto the data manifold and moving on its tangent space. Evaluations demonstrate promising results for inpainting colorization and CT scan reconstruction. Strengths: Theoretically sound and easily implemented methodology. Yields good results. Weaknesses: Lack of comparison with recent conditional sampling methods. Similar correction already introduced in previous work limiting novelty. Unclear whether the proposed method samples from the correct posterior distribution. Visual quality may not be sufficient for applications like CT reconstruction where uncertainty quantification is important. Questions: How to characterize the invariant measure of the proposed diffusion model Why is retraining not necessary if the score is amortized with respect to y Derivation of (14) from (13) and (6) needs clarification. Assumption 1 differs from the typical manifold assumption and its implications should be stated clearly. Effects of the projection step Axi b in (14) should be investigated. Provide details about experimental process including hyperparameter tuning. Comparison with DDRM may be biased due to differing number of sampling steps.", "Paraphrased Statement: Summary This study proposes a method for using diffusion models as prior probability distributions in inverse problem solving. The approach involves adding a constraint to the posterior distribution that ensures the generated sample belongs to the manifold of natural images. This constraint is inspired by the Tweedie formula which establishes a connection between the noisy data distributions score function and the optimal meansquare error estimator used for denoising. Experimental results on various imaging inverse problems demonstrate that the proposed method outperforms existing techniques in terms of FID LPIPS PSNR and SSIM metrics. Strengths and Weaknesses Strengths The paper presents a novel adaptation of standard sampling methods based on the Tweedie formula. The paper is wellwritten and clearly explains the motivation behind the idea. The proposed method achieves superior performance compared to existing approaches across different imaging inverse problem scenarios. Weaknesses The papers exposition could be improved for clarity. While the paper frequently references noise2score the primary motivation is the Tweedie formula. Better introduction of the Tweedie formula and related research is necessary. The motivation for the added constraint (Section 3.1) is not fully clear particularly the derivation of Equation (14). The process of obtaining W for each application is not explained. The theoretical findings could be better linked to the proposed algorithm. Overall Evaluation and Questions The paper presents a promising idea with strong experimental results. However the motivation and exposition need further clarification. Specific Questions: 1. Can you provide more context on the relationship between the Tweedie formula and Noise2Score 2. Explain the motivation and derivation of Equation (14). 3. Describe how W is determined for each application.", "Summary The paper proposes a straightforward improvement to current diffusion solvers for inverse problems by incorporating an additional correction term into the diffusion process. This term in theory represents the portion of the current estimation error that lies on the manifold of the target image. Strengths and Weaknesses Strengths: The proposed approach is elegant and computationally efficient. The empirical results if accurate demonstrate significant advancements in signal recovery. Weaknesses: Theoretical: The papers main theorem makes unstated assumptions that may not hold in practice. The theorem depends on a proposition that assumes the diffusion score function is a global minimum which is unlikely in a diffusion model. The manifold constraint gradient used in the approach also relies on this assumption. Empirical: The visual quality of competing methods (e.g. RePaint and LaMa) in the papers experiments may not accurately reflect their performance as reported in their respective works. The experimental setup has changed from previous works and it is unclear if the authors adequately optimized the competing models for this new setup. Questions: What are the assumptions made in the main theorem and why are they believed to hold Can the theoretical concerns be addressed through modification or refinement of the approach Are the empirical results accurate and reliable and have potential biases been adequately addressed", "Paraphrased Statement: This paper explores diffusion models for solving inverse problems. However these models often require multiple iterations of forward and reverse diffusion to achieve accurate results. The authors propose addressing this limitation by considering the manifold structure of the data. They show that the diffusion process can drift away from the manifold because the score function contributes only to the normal direction. To compensate they introduce a constraint on the reverse diffusion equation using the gradient of the L2 residual. This constraint ensures that the diffusion remains on the data manifold. The authors provide theoretical proofs and geometric interpretations to support their claims. Strengths: Clear and wellwritten Novel and sound combination of Noise2Score and diffusion models Theoretical support with proofs and geometric insights Outperforms stateoftheart diffusion models and supervised learning baselines Weaknesses: Lack of discussion on failure cases or limitations Minor grammatical and formatting issues"], "t3X5yMI_4G2": ["This paper proposes an alternative to traditional reinforcement learning called Reincarnating Reinforcement Learning (RRL). Unlike standard RL which requires algorithms to learn from scratch RRL allows algorithms to leverage past policies and data to accelerate learning. This extends beyond existing offline reinforcement learning methods by enabling both online learning and the potential to access the original policy not just data gathered from it. The paper introduces a specific version of RRL called PolicytoValue Reinforcement Learning (PVRL) designed to enhance the efficiency of valuebased online RL algorithms (like DQN) using a past policy and its accompanying data. The authors present Qdagger a modification to deep Qlearning methods that combines nstep returns with a Daggerstyle loss for both offline training and online training from a pretrained policy to optimize learning speed. The results demonstrate that Qdagger outperforms various strong baselines across multiple benchmarks in Atari Learning Environment Breakout and Humanoidrun. Strengths: Clear and wellwritten paper that presents a novel problem setting. Thorough investigation of an understudied yet highly relevant problem setting as most realworld RL applications involve access to prior data and policies. Robust demonstration of the proposed PVRL algorithms superiority in benchmark tests. Weaknesses: The definition of RRL could benefit from clarification as it encompasses both offline RL and PVRL. Its important to consider the reproducibility and comparability of future research in this area. Releasing data gathered from the models used in this study would enable fair comparisons with new methods. To avoid the need for extensive retraining the authors suggest transferring knowledge from a previously trained agent to a new agent. However its worth noting that this may introduce additional computational costs if the original training was particularly lengthy. Recommendations: Modify the abstract and introduction to present RRL as a class of problems with PVRL as one instance. Describe additional significant problem instances within RRL. Reframe PVRL as a class of solutions to the PDRRL problem. Address reproducibility concerns by releasing data from the models used in the study. In conclusion the paper presents a valuable contribution to the field of reinforcement learning but could benefit from addressing the identified weaknesses to enhance clarity and reproducibility.", "Paraphrased Statement: Summary This paper introduces a novel approach for transferring reinforcement learning (RL) agents from pretrained policies that have different architectures. It proposes that RL research should embrace the \"pretrainandfinetune\" approach commonly used in computer vision and natural language processing rather than training everything from scratch. Method (QDagger) The paper presents QDagger a method that enables the transfer of pretrained policies with varying architectures. QDagger learns value functions from a suboptimal \"teacher\" policy and gradually decreases its reliance on the teacher. Evaluation QDagger is compared to several baseline methods on Atari and Balloon learning tasks. The results demonstrate its effectiveness. Strengths The idea of reincarnating RL policies is intriguing and relevant. The proposed method is straightforward and performs well. Extensive experiments provide strong evidence. Weaknesses The method heavily relies on the availability of massive offline datasets (1 million samples). This may be a limitation as replay buffers are typically implemented as fixedsize queues in memory. It remains unclear whether the proposed method can generalize to scenarios where target datasets are smaller than pretraining datasets which is a common benefit of pretraining and finetuning in other domains. The experimental setup for the Balloon Learning Environment lacks selfcontainedness making it difficult to understand the details. The significance of using nstep returns in the experiments is unclear. The observed performance degradation of TD3 in the humanoid run task raises questions about the methods robustness. Questions How does the proposed methods dependency on offline datasets affect its practicality Can the method be extended to work without requiring offline dataset pretraining for student models Can more details be provided on the Balloon Learning Environment setup and the use of Perciatelli What lessons can be drawn from the nstep return experiments Why does TD3 experience performance degradation in the humanoid run task and could this be addressed by using SAC instead", "Paraphrased Statement: Summary: The authors propose \"reincarnating RL\" as an alternative to \"learning tabular rasa.\" \"Reincarnating RL\" involves using previously learned knowledge to improve the training of new agents. They introduce QDagger an algorithm that addresses \"policytovalue reincarnating RL\" (PVRL). QDagger demonstrates strong performance compared to baselines. Key Contributions: 1. Promotion of \"reincarnating RL\" as an alternative approach to \"learning tabular rasa.\" 2. QDagger as a solution for PVRL. Strengths and Weaknesses: Originality: On \"Reincarnating RL\": Interesting concept that has not been explicitly discussed much before. On \"QDagger\": The specific setting and algorithm details are novel. Quality: On \"Reincarnating RL\": Coherent argument for democratizing RL research and improving efficiency. On \"QDagger\": Wellconducted experiments. Clarity: The paper is wellwritten and clear. Significance: On \"Reincarnating RL\": Possible benefits for realworld RL adoption. On \"QDagger\": Achieves impressive results in tested settings. Questions: 1. What practical applications of \"realworld RL adoption\" are envisioned 2. How can practitioners leverage findings from \"reincarnating RL\" papers for new tasks", "Summary (Paraphrase): The authors study reincarnated reinforcement learning (RL) which uses previous policies to train new agents instead of starting from scratch. They define the reincarnation setting and show that existing methods fail in this context. Their proposed nstep QDagger approach outperforms previous methods. Strengths (Paraphrase): nstep QDagger achieves strong performance matching or surpassing a fully trained DQN with fewer steps in Atari and outperforming baseline agents in humanoid running and BLE. The paper provides comprehensive baselines and hyperparameter sweeps demonstrating the pitfalls of previous agents. The paper is generally easy to understand although the evaluation sections bulleted list could be improved. Weaknesses (Paraphrase): While the paper formalizes reincarnation it may be addressing a wellknown process in the RL community. For many RL tasks training times are relatively short and with improved engineering they can be reduced further potentially limiting the impact of reincarnation. Reincarnation may limit diversity in policy training. It is unclear what teacher is used to train reincarnated agents in BLE and it is possible that reincarnated agents benefit from bootstrapping off a strong policy. Minor Issues: The claim that JSRL does not improve performance compared to DQN may be exaggerated given that it only hurts performance when using a single step. The references to Figure 1 Panel 3 and Figure A. 13A. 14 in the text could be improved. Questions: Can the authors justify the use of reincarnation in common RL settings How should evaluations be reported in the context of reincarnation considering its dependence on a pretrained teacher"], "wlEOsQ917F": ["Summary The paper introduces a new algorithm for solving stochastic bilevel optimization problems which are problems where the objective function depends on both a decision variable `x` and the optimal solution of another subproblem parametrized by `x`. The algorithm is based on the observation that the gradient of the upperlevel objective can be written as a sum of two terms one involving the gradient of the lowerlevel objective and the other involving its Hessian. Instead of directly inverting the Hessian the algorithm updates an auxiliary variable `v` that satisfies a certain minimization problem. This allows for the use of variance reduction techniques like SGD or SAGA which can improve efficiency. Strengths New and conceptually simple framework. Matrix inversion free making it easy to implement. Wellwritten and organized paper. Weaknesses Theorems discuss iteration complexity rather than sample complexity which is more relevant for largescale problems where the number of data points is very large. Sample complexity in the appendix for SAGA is the same as for the full batch deterministic version suggesting that SAGA does not provide any theoretical advantage. Questions Major: Why does the algorithm not benefit from SAGA variance reduction in terms of sample complexity Minor: What does \"global\" mean in the title Minor: The table in the appendix missing a closing parenthesis for SABA.", "Paraphrased Statement: Summary: This research presents an easytounderstand framework for addressing bilevel optimization issues. The framework uses only three impartial estimations per iteration. The authors offer theoretical confirmation of its performance in both the SGD (stochastic gradient descent) and variance reduction variants based on SAGA. Experimental results demonstrate its superior effectiveness. Strengths: 1. The simplicity and clarity of the proposed framework make it userfriendly. By breaking down bilevel optimization into three unbiased estimations the theory becomes more straightforward and accessible. 2. The authors provide numerous theoretical guarantees substantiating the research findings effectively. To the best of my knowledge the studys convergence analysis establishes an optimal convergence rate under specific conditions. Weaknesses: 1. While the framework employs a novel approach to approximating the Hessianvector product (equation (5)) this concept has been explored previously as noted by the authors (line 92). 2. The convergence analysis compares algorithmic complexities but the experiments assess performance solely in terms of running time. 3. The experimental evaluations are limited to a single dataset for each application. Questions: Could you elaborate on how the framework simplifies bilevel optimization What are the key implications of the theoretical guarantees on realworld applications Are there any other methods available for estimating the Hessianvector product and how do they compare to the proposed approach Why were multiple datasets not used for experimentation", "Paraphrased Statement: This research focuses on analyzing fully singleloop stochastic algorithms for bilevel optimization problems where the inner problem exhibits strong convexity and both the inner and outer objectives are smooth and represented by finite sums. The research presents a unified framework that iteratively updates the outer variable (x) inner variable (z) and linear system solution variable (v) used in the bilevel gradient expression. Two algorithms SOBA and SABA are proposed. SOBA similar to singlelevel SGD utilizes decreasing step sizes for the inner and outer variables. SABA on the other hand applies variance reduction along with constant step sizes akin to SAGA. The research establishes convergence rates of O(T2.5) and O(1T) for SOBA and SABA respectively where T denotes the number of iterations. Additionally SABA exhibits linear convergence under the PL condition. Experimental evaluations on image classification and data cleaning tasks demonstrate that SABA outperforms other bilevel methods. Strengths: Provides a clean framework for analyzing fully singleloop algorithms. Exploits finite sum assumptions to achieve rates comparable to singlelevel optimization. Presents a thorough experimental comparison showcasing the effectiveness of SABA. Clear and wellwritten. Weaknesses: Theoretical analysis of SOBA may be suboptimal. Experimental scale is relatively small. Discrepancy between the proposed algorithm and its practical implementation which uses a rolling average for variance reduction. Lacks experiments under the PL condition and does not provide detailed insights into the performance variations with respect to singlesample gradients and Hessianvector products."], "qfC1uDXfDJo": ["Summary Paraphrase: This paper explores mathematical properties of twolayer regression networks with ReLU activations and squared loss. Extending prior work it analyzes local minima based on loss function symmetries when the hidden layer has more neurons than the input dimension. Novel techniques allow for classifying local minima by symmetry estimating their loss and Hessian spectrum and understanding how certain minima become saddles with increasing neuron count. Strengths Paraphrase: Combines novel techniques from various mathematical fields. Extends previous work to a new setting requiring innovative approaches. Provides sharp analytical estimates for loss and Hessian leading to insights into minima and saddle transitions based on symmetry. Establishes the frequency of specific local minima configurations. Includes detailed proofs and supplementary calculations. Weaknesses Paraphrase: Assumes a teacherstudent setting with specific target network parameters limiting applicability to practical networks. Focuses on a specialized Gaussian input distribution potentially limiting generalization to realworld scenarios. Classifies local minima into symmetry groups based on input and hidden neuron permutations whose relevance to practical scenarios is unclear. Lacks a comprehensive comparison to existing results on spurious local minima in networks. Insufficiently precise use of terminology and implicit assumptions hinder accessibility. Additional Comments Paraphrase: Appreciation for detailed calculations and arguments. Suggestions for improving clarity and terminology. Questions about the rationale for studying input neuron permutations and the comprehensiveness of the theory. Potential for translating results into specific weight configurations for easier interpretation.", "Paraphrased Statement: Summary: This article examines the optimization of twolayer neural networks with the objective of minimizing squared error. By utilizing algebraic geometry techniques it analyzes the networks structure to expose how superfluous local minima are eliminated by overfitting. Strengths: Presents a robust theoretical framework for understanding local minima in twolayer ReLU networks. Introduces innovative tools from algebraic geometry and representation theory to address this problem. Provides insights into the behavior of local minima and Hessian spectra contributing to the theoretical foundation of twolayer networks. Weaknesses: Lacks selfsufficiency making comprehension challenging. Notation inconsistencies and undefined terms hinder clarity. Optimization variable conflicts and unclear theorem contexts raise questions. Questions: Can these findings be extended to realworld optimization scenarios involving limited data Can the theorems be generalized to target networks with different neuron counts How might these results be affected by overfitting beyond the specified parameterization regime", "Paraphrase: Summary: This paper explores the structure of critical points in overparameterized twolayer ReLU networks focusing on how local minima can transform into saddle points. By exploiting the symmetry of the loss function and considering specific types of critical points it demonstrates that adding neurons can convert nonglobal minima into saddle points shedding light on the behavior of overparameterized models. Strengths: The paper provides a wellwritten analysis of the effect of overparameterization on the transformation of different symmetry types of minima into saddle points. The FPS method for analyzing the Hessian spectrum proves valuable in characterizing the Hessian at local minima. The arguments regarding the change in critical point nature due to added neurons are insightful. The paper explores how different minima require varying numbers of additional neurons to become saddles. Weaknesses: The paper assumes familiarity with previous research and tools making it less accessible to readers new to the subject. Some proofs are difficult to follow without prior knowledge. Questions: Why is the loss function invariant under row permutations given that the weights of the second layer should play a role What are the limitations of applying these methods to kdimensional cases How is it established that loss at type II minima decays as O(1d) Is Definition 2 applicable to all k and d or only specific cases Is there evidence to suggest that the two families of critical points are representative of other symmetric minima Why is it reasonable to compare loss at initialization to loss at minima Where in the paper is the assumption of high symmetry in V made What is the significance of the subscripts 1 2 and 3 in the gradient of the loss function", "Paraphrased Statement Extension and Analysis of Network Landscapes with Mild Overparameterization The authors enhance the theory presented in [6] by investigating mild overparameterization scenarios where the student network has one or two additional neurons compared to the teacher network (using a ReLU activation function). For specific families of local minima they provide analytical descriptions that encompass the loss values and Hessians at these minima. Strengths and Weaknesses Strengths: Extends the theory of [6] on network landscapes with local minima. Weaknesses: The papers contributions are considered somewhat limited reducing its appeal. The analysis techniques are complex and nonstandard making it difficult to validate the proofs. The paper lacks experimental evaluations to support claims about the desirability of Type II minima. Clarity and Significance: The language used in the paper presents a barrier to comprehension. Table I is poorly explained with its caption directing readers to later sections rather than providing a clear description. The significance lies in the attempt to classify critical points in neural network landscapes potentially aiding in understanding training challenges. However the scope is limited and it is uncertain if dynamics lead to these points. Questions and Typos: Questions: Are there local minima of the form \u0394Sdp \u00d7 \u0394Sp with p2 Is it true that the twolayer setup can be analyzed similarly to the onelayer setup in the paper Typos: Line 6061: \"adding one neuron results in these minima...\" (unclear sentence) Line 101: Citation needed Line 152: Unclear definition of inclusion Line 289: \"so turn minima\" (should be \"to turn minima\") Line 367: Meaning of \"noninteger values of d\" requires clarification"], "stAKQ6vnFti": ["Paraphrased Statement: This research presents a framework (CLLR) that uses contrastive learning (CL) to create embeddings. It offers a lowdimensional projection of the feature space employed in CL. Additionally it provides a theoretical boundary for CLLR and experimental studies using the learned lowdimensional embedding. Strengths and Weaknesses: Originality: While the approach is somewhat innovative it combines established techniques to develop a practical framework for generating lowdimensional features via CL. Quality: The proposed framework is wellanalyzed with theoretical bounds and empirical tests covering various tasks. Ablation studies support the frameworks effectiveness. However the question remains: why cant lowdimensional embeddings be learned directly through CL The proposed regularizations could be applied directly to the featureproducing matrix. Clarity: The study follows a clear and structured approach. Significance: The concept of lowdimensional embeddings obtained through CL is valuable. This work makes a meaningful contribution in that direction. Questions: Why cant lowdimensional embeddings be learned directly through CL with the proposed regularizations In the experimental tests were the models trained using both CL and taskspecific loss functions (in a multitask learning setting)", "Paraphrased Statement This paper introduces a new technique for improving contrastive representation learning. The method includes an additional layer that transforms representations into a lowerdimensional space. The projection layer is trained to reduce reconstruction loss with added sparse regularization. This projected lowerdimensional representation is more effective in various tasks. Strengths: Contrastive representation learning is a significant field in machine learning. The proposed method is innovative. The experiments are extensive and the results are impressive. The method improves performance consistently across tasks including image classification text classification and reinforcement learning. Confidence intervals from repeated trials are provided. The empirical findings are valuable. Weaknesses: The paper needs to better explain contrastive learning including its definition and what it encompasses. Section 2.1s argument for the motivation is not convincing. Theorem 1 is only partially relevant as classification relies on distance ordering rather than exact values. There is no direct evidence that high dimensionality negatively impacts representation quality. Questions: Is the representation space hyperspherical Does the method apply to specific objective functions or \"contrastive learning\" types Can the method be applied to noncontrastive classifiers What are the contributions of reconstruction loss and sparse regularization individually", "This paper explores the problem of highdimensional representations in current contrastive learning (CL) techniques. CL aims to maximize the discriminability of data points in highdimensional space potentially leading to sparse data distribution. In this situation traditional learning algorithms struggle to capture pairwise instance similarities effectively. The authors propose a sparse regularized projection layer to address this issue. This layer reconstructs learned features in a lowdimensional (sparse) projection space while preserving instance discrimination in the original highdimensional space. The authors also introduce generic sparselowrank regularized loss functions with corresponding optimization algorithms. Theoretical analyses (Theorem 1 2 and 3) support the rationale and efficacy of the proposed method. Experiments across various domains including image text and reinforcement datasets demonstrate its superiority. Strengths: 1. The paper is wellwritten and easy to follow with clear illustrations to support the concept and effectiveness of the proposed method. 2. The issue of highdimensional contrastive embedding is significant and addressed effectively by the proposed method. 3. The proposed idea is original concise and backed by theoretical analysis providing a lower bound for distance contrast to distinguish pairwise instance similarities and dissimilarities. 4. The method is implemented in a general way and shows consistent improvements in extensive experiments across multiple domains. Weaknesses: 1. The authors could provide more context for the preliminary equation used in Theorem 1 and explain its relevance to the distance contrast conclusion. 2. The experiments focus on regular positivenegative CL baselines. It would be valuable to demonstrate the methods performance in negativefree CL approaches which have shown promising results. Overall the paper explores an important issue in CL and proposes a novel solution supported by theories and experiments. The quality meets the acceptance criteria for NeurIPS and the authors are encouraged to address the concerns and questions raised in the review.", "Paraphrase: Summary: This study focuses on using contrastive embedding in lowdimensional spaces. The authors observe that applying contrastive embedding to highdimensional spaces leads to the \"curse of dimensionality\" hindering the identification of underlying similarities between paired instances. Thus they present a novel formulation for lowdimensional contrastive embedding supported by convergence analysis and a lower bound of the minmax distance ratio. The experimental results demonstrate the effectiveness of their proposed algorithm. Strengths: 1. Strong motivation: The authors clearly highlight the challenges of contrastive embedding in highdimensional spaces and propose a solution to overcome them providing a wellreasoned foundation for their research. 2. Clear presentation: The paper is wellstructured with a welldefined problem setting and a detailed description of the proposed algorithm. The theoretical analysis is presented in a logical and concise manner. Weaknesses: 1. Ablation study: Although the paper acknowledges the potential limitations of contrastive embedding in highdimensional spaces the ablation study in Table 1 does not include extreme cases to fully demonstrate the poor performance that can occur in such scenarios. 2. Motivation example: While the inclusion of section 2.1 provides context for the motivation a more concrete example would enhance the impact of this section by showcasing the specific shortcomings of highdimensional contrastive embedding. 3. Novelty and contribution: The integration of lowdimensional mapping into contrastive embedding is a valuable step but the papers novelty and contribution may not meet the high bar set for NeurIPS publication. Questions: The authors are encouraged to address the weaknesses and comments raised in this review to strengthen the papers overall quality. Specifically it would be beneficial to include more extreme cases in the ablation study provide a more detailed example in section 2.1 and consider how the papers contributions align with the expectations of the NeurIPS conference."], "o4uFFg9_TpV": ["Paraphrased Statement: This research introduces a unified approach for several image processing tasks including colorization edge detection inpainting segmentation and style transfer. It views these tasks as a form of inpainting. By doing so the authors achieve significant results. Additionally they have created a dataset for training models. Strengths: The authors novel approach of tackling image tasks as inpainting is innovative. The paper is wellwritten and the results are impressive. Weaknesses: The paper could benefit from additional details on the methodology and implementation. The authors have not compared their approach to established methods in each image task domain such as Mask RCNN Mask2Former and others. Adding these comparisons would strengthen the paper. Questions: How are MAE and VQGAN combined During training was VQGAN trained prior to using MAE for reconstructing visual tokens During inference do you encode the combined image using VQGAN and then use MAE to fill missing parts (visual tokens) Why is MAE used for filling in missing areas instead of the transformer from VQGAN How are probabilities assigned to visual tokens Is the prediction of zk related to zk1 Does the prediction of each zk rely only on x and m In the equation above L119 is m a mask for the area needing to be filled", "Paraphrase: Summary: The study presents a dataset of images from computer vision and machine learning papers focusing on gridstyle illustrations. This dataset enables training an image inpainting model on numerous inputoutput examples (combined into one image). The trained model can tackle various tasks by using \"prompting\" where an image is created as a grid of inputoutput examples and the final output is concealed for inpainting. The models prediction is interpreted as the task prediction. The study extensively assesses different model architectures training sets prompting strategies and tasks demonstrating that the final model outperforms models trained on ImageNet in various settings. Strengths and Weaknesses: Strengths Provides a straightforward method for training an inpainting model for visual prompting. The trained model exhibits reasonable performance in various tasks. The study investigates several aspects of the model including architectures training datasets prompting schemes and tasks to determine the optimal configuration. The proposed dataset consisting of figures from arXiv papers has potential value for the research community. Weaknesses Fewshot Learning: The paper claims that the trained models are capable of fewshot learning but this assertion relies on a broad interpretation of the term. True fewshot learning involves training a model on a set of familiar classes then evaluating it on annotated examples of a novel class to determine its ability to recognize that class. However in this study the model is not tested on unseen categories. Dataset: The paper lacks sufficient details about the collected dataset making it difficult to evaluate the findings or the dataset itself. It is unclear if proper consideration was given to copyright and attribution. Additionally the dataset contains validation and testing images from popular datasets which could bias model performance when evaluated on downstream task performance. Learning Signal: The dataset primarily contains figures comparing methods to the ground truth or other methods. This could lead the model to predict inaccurate results based solely on the fact that papers often present predictions. Further investigation is needed to determine the impact of this factor.", "Paraphrased Summary Research Goal: Explore the idea of \"natural language prompting\" for vision tasks by introducing \"visual prompting\" through image inpainting. Method: Create a \"gridlike image\" prompt that combines task examples (target image) and a query image. Use an inpainting model to fill in the masked region of the prompt aligning it with the example images. Train the inpainting model on a specialized dataset (Figures) to perform image manipulation tasks. Strengths: Demonstrates that incontext learning (prompting) is effective in vision tasks. Simple and flexible approach that covers a wide range of vision tasks. Adopts a \"purely vision\" perspective avoiding language pretraining and language prompts. Weaknesses: Requires a specific dataset (Figures) tailored to the prompting task limiting scalability. Pretraining and downstream tasks use different masking strategies creating a domain gap. Lacks comparisons to baseline methods making it difficult to assess performance. Contradicts the value of prompting by requiring pretraining a new model. Questions: How does the method compare to established baselines (finetuning and zeroshot learning) What is the practical utility of the prompting method given its reliance on specialized training Would reforming existing datasets into a gridlike structure improve pretraining effectiveness and reduce domain gap How does performance compare to training from scratch on the same dataset with a taskspecific objective", "Paraphrased Summary This paper introduces a groundbreaking method for utilizing pretrained visual models for diverse downstream tasks through \"prompting.\" Its essentially a oneshot or fewshot learning approach but without the need for extensive finetuning. Similar to natural language processing (NLP) systems the method involves pretraining the visual model on an image inpainting task. To prompt the model for a downstream task the research team combines exemplar pair images with the input image to create a larger \"visual prompt image.\" Crucially the paper highlights the importance of the dataset used for pretraining the visual model. The authors developed a dataset consisting of images of paper figures from arXiv papers. Empirical results demonstrate the effectiveness of the proposed approach for various downstream tasks including foreground segmentation object detection and colorization. Strengths and Weaknesses Strengths: Novel and innovative approach Simplicity and applicability to visual models Creative use of arXiv paper figures for pretraining Wellwritten and organized paper Promise of data and code availability Weaknesses: 1. Lack of comparison with a oneshot baseline 2. Limited details on prompt engineering 3. Unclear information on VQGAN training details 4. Missing standard deviation reporting for quantitative results 5. Sensitivity analysis of the approach to the prompted image"], "mT18WLu9J_": ["Paraphrased Statement: Summary This paper demonstrates that poisoning a training dataset can enhance the privacy vulnerability (in terms of membership inference attacks) of other training points. It presents the dirtylabel and cleanlabel attacks and tests them in a transfer learning context. Both attacks markedly increase the success rate (AUC) of membership inference attacks. Strengths and Weaknesses The paper introduces a novel threat model where poisoning a model can compromise the privacy of other training points beyond just evading the model itself. A similar issue has been explored in a concurrent study [34] while a previous work [18] investigated weaker forms of data leakage related to global dataset characteristics. The papers objective is to improve membership inference success rates overall and evaluates attack performance using the AUC a measure of averagecase success. It could be beneficial to supplement this with worstcase metrics as suggested by recent research. As illustrated in Figure 2 the poisoning attack appears to enhance the AUC while also significantly increasing the true positive rate (TPR) at low false positive rates (FPRs) a critical aspect for privacy attacks. The primary criticism is that the paper exclusively focuses on the transfer learning setting. Even the dirtylabel attack which could theoretically be employed in other contexts is only tested in transfer learning. The abstract and introduction should acknowledge this limitation. Surprisingly the results show a substantial gap between the performance of dirtylabel and cleanlabel attacks in the transfer learning setting. This disparity is not fully explained especially considering that the dirtylabel attack involves a small perturbation (eps16255) which should enable nearperfect matching of features to other data points. Figure 5 suggests that the attacks operate in distinct ways with the dirtylabel attack consistently assigning the same \"dirty\" label while the cleanlabel attack does not. Confirmation of this is requested. If true it is unclear why this occurs particularly given the lack of sufficient data points from the same class in Dshadow. Regarding the threat model it would be valuable to analyze the interplay between the assumptions underlying poisoning attacks and membership inference attacks. Notably membership inference attacks require the attacker to obtain some data samples from the targeted distribution implying that the existence of Dshadow is inherently plausible. In practice the attacker would likely encounter a Dshadow that overlaps with Dclean making the membership inference problem more complex. Questions In the dirtylabel attack is the same incorrect class used for each poisoning instance or is it randomly selected Explain the significant difference in performance between the dirtylabel and cleanlabel attacks. Can you demonstrate the feasibility of the dirtylabel attack in an endtoend training scenario Evaluate the effectiveness of the membership inference attacks at low false positive rates.", "Paraphrased Summary: This research investigates the issue of membership exposure where an attacker attempts to determine whether a specific sample was part of a training dataset. To enhance membership identification the authors propose using data poisoning techniques such as label flipping and cleanlabel attacks. Experimental results demonstrate the efficacy of these attacks. Strengths: Novel Approach: The use of data poisoning to amplify membership exposure is a novel concept. Weaknesses: Unclear Rationale: The effectiveness of the proposed attacks is not fully explained particularly regarding the relationship between data poisoning and membership exposure amplification. Limited Practical Insights: The proposed methods do not offer insights into addressing the membership inference problem or the objective in Equation (2). Confusing Experimental Section: The experimental setup is unclear with the stated goal being to identify membership but the reported metric (MI AUC) measuring class accuracy. High Baseline AUC: The baseline MI AUC values are exceptionally high indicating that the severity of the addressed problem may be overstated or illdefined. Questions: Classification Metric Relevance: The authors use of crossentropy loss as a classification metric in Equation (2 is questionable as it does not effectively determine membership. Subset Split Assumption: The even split of the dataset into subsets may oversimplify the problem and facilitate poisoning. Poisoning Budget Comparison: The poisoning budget appears to be smaller than the size of the shadow dataset raising questions about the necessity of such a large shadow dataset.", "Paraphrase Summary This paper explores data poisoning attacks that heighten the susceptibility of machine learning models to membership inference attacks (inferring membership of data points in the training set). The study examines two types of attacks: \"dirty label\" (or label flipping) and \"clean label\" attacks. Using image classification datasets the paper demonstrates that dirty label attacks can significantly increase the success rate of attacks compared to uncompromised models with minimal impact on model accuracy. Strengths and Weaknesses Strengths: Investigation of an insightful problem. Clear and accessible writing. Weaknesses: Prior research has already established the claimed contributions. Inadequate comparison to literature on privacy risks. Deficient evaluation. Detailed Comments The evaluation of privacy risks under data poisoning attacks is a compelling concept especially as machine learning models train on increasing amounts of data from diverse sources. However data poisoning to escalate privacy risks has already been explored and the purported contributions made in this paper have been previously demonstrated. Moreover the paper employs questionable metrics for measuring membership inference risks. Poor Comparison to Prior Work The paper fails to adequately compare its findings with existing research. While the combination of data poisoning and membership inference attacks is novel the paper overlooks crucial research questions such as the underlying mechanisms of poisoning the vulnerability of specific training samples and the implications of increasing AUC. Weak Evaluation The interpretation of the evaluation results is unclear. The paper claims that clean label poisoning attacks are an \"approximate\" version of dirty label attacks but the visualizations suggest distinct impacts on models. The method for quantifying distributional differences between training and shadow data is vague. The connection between the presented attacks and adversarial training is not welldefined. Questions 1. Scientific comparison of contributions to prior work. 2. Clarification of contributions in the context of existing research questions. 3. Interpretation of evaluation results (as outlined in detailed comments).", "Paraphrase: Summary: This study introduces a method to amplify membership inference attacks by poisoning a machine learning model with samples from a specific class. Two attack types are introduced: \"dirtylabel\" (altering training data labels) and \"cleanlabel\" (slightly modifying training data). These attacks focus on increasing the influence of members of the targeted class to enhance inference attacks. Strengths and Weaknesses: Strengths: The combination of poisoning and membership inference is explored demonstrating significant implications. The proposed attacks are simple and effective. A thorough ablation study reveals attack characteristics. Weaknesses: The claim of concurrency with [34] is questionable since it was published on ArXiv for a considerable time before the NeurIPS submission and involves prominent researchers. The membership inference attack used is relatively weak and a more current approach may reduce poisoning effectiveness. While the attacks appear stealthy in input space OOD detection algorithms that operate in latent space may still detect them. The paper does not consider potential defenses against the attacks such as outlier removal differential privacy regularization or data augmentation. Questions: 1. Is test accuracy measured only for classes other than the target It seems that the attacks would significantly degrade accuracy for the target class reducing stealth."], "wYGIxXZ_sZx": ["Paraphrased Statement: Summary: This study presents an example demonstrating the limitations of using the primal risk metric to assess generalization in minimax optimization problems. To address this issue a new metric called the \"primal gap\" is introduced. The study uses this new metric to compare the generalization abilities of GDA and GDMax algorithms. Lastly the study establishes generalization bounds for primal and primaldual risks without relying on strong concavity assumptions. Strengths: The introduction is wellwritten. The development of generalization bounds without strong concavity assumptions is a significant contribution. Weaknesses: The study does not discuss whether Theorem 1(d) from Li and Liu (2022) examines a more comprehensive stability measure of the primal gap. A clear explanation of why the primal gap is a superior metric is missing. Questions: Why is good generalization behavior for minimax problems measured by the primal function Is this a necessary and unique approach Could the authors provide a more detailed explanation of why the primal gap is a suitable metric", "Paraphrased Statement: This paper introduces the \"primal gap\" metric to evaluate the generalization of minimax learning algorithms. It starts by highlighting the limitations of previous metrics (primal risk and primaldual risk) when applied to minimax problems. The paper demonstrates that the primal gap metric is particularly effective for minimax problems. It derives generalization bounds and applies them as special cases to primal risk and primaldual risk for nonconvexconcave settings providing improved bounds there. The paper also compares gradient descentascent and gradient descentmax algorithms. Strengths: Proposes a novel primal gap metric for minimax learning. Provides clear and concise explanations of results. Weaknesses: The primal gap metric closely resembles existing metrics for minimax problems. The example in Section 3.4 raises concerns about the effectiveness of the primal gap metric as it shows a significant gap between upper and lower bounds. Optimal dependence on the hyperparameter \\epsilon is not established and it is unclear if current results under weaker convexity constraints are optimal. The dependence on parameters Cp and Ce is not fully addressed and the example in Section 3.4 suggests that it may introduce additional factors. Questions: Is the dependence on \\epsilon optimal especially in the low \\epsilon regime where there is a gap between current results and previous ones under stronger convexity constraints How do Cp and Ce affect the dependence on \\epsilon and n Will the inclusion of Cp and Ce throughout the paper replace dependence on kappa and introduce terms related to Cp and Ce Can further comparisons be drawn between the proposed results and existing work", "Paraphrase: This study explores how optimization algorithms affect generalization in nonconvex and concave or nonconvex and nonconcave stochastic minimax problems. The authors provide examples to argue that generalization error for the primal risk may be insufficient for predicting actual generalization error due to differences in objective values between theoretical and empirically optimal minimax learners. To analyze generalization error for the primal risk the authors use the concept of \\epsilonalgorithmic stability. For nonconvex and concave problems they establish a bound with sublinear dependence on \\epsilon overcoming challenges such as the lack of Lipschitz continuity and uniqueness. They employ a virtual gradient ascent algorithm to bridge this gap. Furthermore the authors examine the generalization properties of Gradient Descent Ascent (GDA) and Gradient Descent Max (GDMax). They present examples and estimates to demonstrate that GDA generalizes better than GDMax. Strengths: Extends recent advancements by establishing generalization bounds for stable algorithms in terms of primal risk for nonconvex and concave problems and primaldual risk for convex and concave problems. Provides theoretical examples to clarify the lower bound and parameter estimation for generalization in minimax problems. Weaknesses: Writing clarity could be improved particularly in Table 1 references terminology and organization. Theorem 3s assumptions lack clarity and references to other literature are inconsistent. Equation (9) has an apparent discrepancy between the positive lower bound and the suggested upper bound of 0. Experimental results on MNIST exhibit poor performance for GDMax raising questions about parameter tuning.", "Paraphrased Summary: The paper investigates metrics that indicate a solution to a minimax optimization problem using a dataset will also perform well on average loss. It demonstrates that \"generalization error in primal risk\" being small is insufficient. The paper proposes a new metric \"generalization in the primal gap\" as a better indicator. The authors then analyze two popular minimax optimization algorithms GDA and GDMax and establish conditions under which one outperforms the other. Strengths: Addresses the important topic of understanding the generalization of stochastic minimax optimization. The problem is wellmotivated and has the potential to improve practical applications. Weaknesses: The authors suggest the insufficiency of the primaldual gap as a metric but a clear counterexample is not provided. It is unclear if cases exist where the proposed metric \"generalization in the primal gap\" is small while the traditional \"generalization error in primal risk\" is not. The comparison between GDA and GDMax is solely based on comparing upper bounds lacking lower bounds raising concerns about the reliability of the comparison. The proposed metrics \"generalization in the primal gap\" and \"generalization error in primal risk\" are not described in detail leaving it unclear if they can be computed in practice. Questions: Are \"generalization in the primal gap\" and \"generalization error in primal risk\" metrics that can be computed in advance or using data Does the paper imply that stable algorithms with structural properties guarantee the generalization of minimax learners If so why are the intermediate quantities \"generalization in the primal gap\" and \"generalization error in primal risk\" needed"], "qj-_HnxQxB": ["Paraphrased Statement Summary This article introduces FINE a method for enhancing generalization to unfamiliar situations using analogy making and indirection in the function space. FINE draws analogies between known inputoutput pairs to determine the linking function. Then using indirection it approximates the function by combining a set of functions stored in memory. For assessing outofdistribution generalization the paper creates a visual reasoning dataset based on CIFAR100 and Omniglot. Each sample includes an inputoutput pair serving as a transformation cue and four alternatives. FINE outperforms comparable models in terms of accuracy and sample efficiency for various functions and function combinations. Ablation studies emphasize the significance of layer count and memory size. Strengths Draws inspiration from human intelligence features combining them uniquely. Proposes a formal framework that leverages this technique to address the issue of outofdistribution generalization in neural networks. Demonstrates the models ability to abstract image transformations in the latent space allowing it to generalize transformations to novel images. Introduces a unique use case for visual reasoning showcasing the new frameworks benefits. Compares the method to various relevant baselines. Provides clear and comprehensive writing. Offers code for reproducing results. Acknowledges limitations of the approach. Weaknesses To learn the task the model must identify and construct the transformation. Therefore outofdistribution samples should exhibit different functions not just different input images. A fair comparison with baselines requires adapting all models to this framework. The paper lacks essential details about training setups and hyperparameters used for training these models. The paper does not explore additional applications for the framework. While the use of NICE layers is intuitively motivated it is not essential for FINE. The datasets design which incorporates reversible transformations motivates its use. To ensure a fair comparison it would be appropriate to present results for all models without NICE layers. The rationale for using two NICE layers instead of one is not fully explained. The paper does not clarify the choice of using yt vectors as output from \u03b3(y). The first two paragraphs of subsection 3.1 consider different versions of Hypothesis H1 and H2 which could be simplified by removing the hypotheses. FINEs advantage becomes clearer when the task requires learning multiple functions using a single backbone. Questions Can the paper clarify that the outofdistribution generalization claim pertains to input images undergoing transformations not the transformations learned by the model Can the paper include evaluations for unseen transformations to strengthen the papers claims Can the paper describe the training setups for baseline models in the supplementary material Can the paper provide results comparing all models without NICE layers Can the framework be adapted and tested for outofdistribution generalization in other tasks such as image classification Can the first two paragraphs of section 3.1 be revised for clarity removing hypotheses and focusing on FINEs advantage in learning multiple functions with a single backbone", "Paraphrase: This paper focuses on extending the adaptability of deep learning models beyond their original distribution for IQ visual tasks that involve identifying and applying geometric transformations to images. To achieve this outofdistribution (OOD) generalization in functional spaces the paper introduces a new approach that combines a neural architecture with an augmented memory. Strengths: Unlike existing methods that operate in data spaces this approach operates in functional spaces providing a more versatile OOD generalization mechanism. The model can adjust its backbone networks parameters dynamically based on inputoutput queries retrieving them from a memory. Weaknesses: The approach may require a substantial memory to store the trainable weights for complex backbone networks. The papers evaluation is limited to geometric transformations. It remains unclear if the approach can handle other types of transformations. Questions: Can the proposed method be applied to transformations beyond geometric ones", "Summary Paraphrase: This paper proposes a novel method called Functional Indirection (FINE) for optimizing neural networks to improve their generalization abilities in abstract reasoning tasks. FINE introduces a mechanism that selects weights for a neural network dynamically based on the inputoutput pair allowing it to adapt to new data and solve problems that share similar underlying rules. This process is constrained by a predefined memory of weights ensuring that the network can only select from a limited set of functions. FINE is similar to hypernetworks but with the added constraint of using a function composer to determine the optimal arrangement of weights. The performance of FINE is evaluated on a newly introduced abstract reasoning dataset derived from Omniglot and CIFAR100. Strengths: The proposed FINE method is a novel approach to functional indirection in neural networks. The paper is wellstructured and easy to understand. FINE achieves strong performance in OOD generalization across unseen categories in the used datasets. Weaknesses: The evaluation focuses on unseen characters rather than unseen rules which limits the assessment of its ability to handle more complex abstract reasoning tasks. The method implicitly selects weights based on inputoutput transformations similar to the explicit expert mixture approach described in a previous paper. A comparison with this approach would be beneficial. The limited basis of network weights may constrain the models generalization to only observed rules and their combinations. It is unclear how the model would perform with unseen rules.", "Paraphrase: Summary: Generalizing to unfamiliar data (outofdistribution or OOD) is a major challenge in artificial intelligence. This study tackles this issue for IQlike tasks by introducing FINE (Functional Indirection Neural Estimator). FINE uses the concept of indirection to connect two different representations and bridge the gap between them. The proposed architecture effectively handles IQ tasks involving images that differ from the training set but follow similar rules. Strengths: The question of OOD generalization is crucial for understanding intelligence. The indirection idea is innovative and FINE offers an effective implementation. Figure 4 demonstrates interesting clusters in weight space raising questions about other models behavior. Weaknesses: The definition of OOD is ambiguous as the study only tests on different classes within the same datasets which may not represent true OOD scenarios. The term \"generalization\" is limited to testing on slightly different images not novel rules which is the true challenge in IQ tests. Questions: What quantitative evidence supports the claim that humans can generalize to OOD Is there any evidence that models actually recognize objects and infer relationships or is this mere anthropomorphization The conclusion assumes that FINE discovers relational patterns and solves new tasks but the study does not provide evidence for this. Beyond slightly different images where does the study demonstrate \"new tasks\" or the discovery of relational patterns"], "zt4xNo0lF8W": ["Paraphrase: Summary: The authors propose a different approach to FewShot Segmentation (FSS) using a twostage training process. In stage 1 a network is trained to generate potential masks for the query image. In stage 2 another network is trained to generate features for the query and support sets which are then pooled using the support masks and candidate query masks. The best matching candidate masks are found through correlation and a MultiLayer Perceptron (MLP). The authors argue that separating segmentation and matching tasks improves learning and present strong experimental results to support their claim. Strengths: Novel approach Simple architecture (though 2stage training increases complexity) Weaknesses: Concerns about class leakage from the test set into the first stage of training Questions: Configuration and code: Does the POS network have a separate model for each fold to avoid class leakage POS loss during stage 1: Is the segmentation loss based on all classes in the image or only the target class POS generalization: How do the N proposals avoid overfitting to base classes when the queries are learnable Motivation for 2stage training: Why would endtoend training degrade the quality of mask proposals Anonymization: The provided code contains references to author home folders and should be more thoroughly anonymized.", "Paraphrased Summary: This paper introduces a fewshot semantic segmentation network inspired by Mask2Former. It treats the segmentation task as a mask classification issue employing a new masklevel matching mechanism instead of pixellevel matching in existing fewshot methods. The authors propose a featurealignment block leveraging an attention mechanism to independently align support and query features while crossaligning them. Experiments on two datasets demonstrate stateoftheart performance. Strengths: Clear and structured organization Substantial performance improvements over current methods Weaknesses: Primary novelty lies in the Feature Alignment Block (FAB) with incremental contributions from other components. The \"fewtofew\" matching concept is not novel having been used in fewshot instance segmentation. Mask2Former architecture may overfit small datasets limiting its suitability for fewshot settings. Related work lacks analysis and comparison. Questions: Training time memory consumption and Mask Matching Module implementation details Upsampling mechanism for the final mask Selfalignment module and its potential replacement with selfattention Explain and visualize the MLP in Equation (4) Rationale for excluding dice loss in Equation (173) Explanation for constraining only positive support samples (Equation 180) How to determine the lowest IoU (e.g. for masks with no overlap) Additional visualizations of failure cases Remove redundant closing parenthesis in Equation (4)", "Paraphrased Statement This paper introduces a novel twostage framework for fewshot segmentation that separates the matching and segmentation processes. Despite comprehensive experiments and ablation studies showing its effectiveness on the COCO and Pascal datasets there are several areas where the paper needs improvement. Weaknesses: 1. Redundant and unclear writing with ambiguous explanations and unclear logic. 2. Numerous grammatical errors that require correction. 3. Limited innovation and novelty. 4. Inadequate performance on the Pascal dataset compared to older methods. Questions: 1. The statement in line 40 oversimplifies the methods for modeling the relationship between query and support feature maps implying that attention or 4D convolutions are the only options. 2. The explanation for poor performance on the Pascal dataset is not convincing as the datasets limited size alone does not fully account for it. 3. Visual illustrations would enhance the clarity of the network structure described in section 2.2. 4. The claimed complexity advantages of the algorithm need to be supported by comparative experiments and explanations. 5. The phrase \"the last three layers\" in line 115 requires more detail about the layers used. 6. The novelty of the approach is questionable as it borrows heavily from MaskFormer. 7. The effects of various components on model efficiency should be discussed including the overall efficiency in terms of frames per second and model size. 8. Numerous grammatical errors and ambiguous expressions need to be corrected.", "Paraphrased Statement Summary This study addresses fewshot segmentation using a novel Mask Matching Transformer (MMFormer). MMFormer consists of two components: Proposal Decomposition: Divides query images into segmentation proposals using a classagnostic segmenter. Guided Merge: Combines similar proposals into final masks guided by support images. With a ResNet50 backbone MMFormer demonstrates promising results on the COCO and PASCAL benchmarks. Strengths Clear and wellorganized presentation. Thorough experimentation exploring the effectiveness of MMFormers components. Weaknesses Object Segmenter Dependence: MMFormer relies on Mask2Former to generate segmentation proposals but in fewshot learning the number of meaningful object parts is unknown. How does MMFormer learn these segments without guidance Transformer Parameters: MMFormer uses transformers for decoding which can require many parameters. Providing a breakdown of the parameters could clarify the impact of transformer size on performance. PASCAL Performance: MMFormers performance on PASCAL is inferior to other stateoftheart methods. An analysis of this difference could reveal insights into MMFormers characteristics. Questions Does the number of meaningful object proposals influence MMFormers performance How do the identified weaknesses affect MMFormers overall effectiveness"], "uloenYmLCAo": ["Summary The paper introduces a new model that incorporates a blockwise outer recurrent mechanism into a Transformer architecture. This recurrent mechanism processes groups of tokens (blocks) simultaneously updating a hidden state matrix after each block rather than after each individual token. The hidden state is accessed by the Transformer using cross attention and the model additionally employs slidingwindow attention like the Longformer transformer. It inherits features from TransformerXL and Longformer such as accessing previous sequences and hidden states through a cache. Training and Implementation During training sequences of tokens are divided into blocks and the recurrent state is updated after each block. This is orthogonal to processing tokens within each block. The hidden state accessed for any token is the latest hidden state available. The recurrent state update involves selfattention on the hidden state itself and crossattention on tokenwise features of the current block using a gating mechanism instead of residual connections. The model also includes variations with simplified updates. The overall model resembles Longformer with one Longformer layer replaced by a blockrecurrent Transformer layer. An additional \"Feedback\" variant modifies other Longformer layers to include crossattention to the last recurrent state. The slidingwindow attention is also implemented in a blockwise manner for efficiency. Results and Discussion The model achieves stateoftheart performance on the PG19 arXiv and GitHub datasets. The authors highlight the combination of recurrency and selfattention as a key factor in its effectiveness. Weaknesses The model definition lacks clarity in certain sections. The analysis of model aspects is brief and raises unanswered questions. The experimental comparison to the Memorizing Transformer raises concerns about fairness and the exclusion of standard language modeling benchmarks limits comparisons. The code has yet to be released which would aid in clarifying model details. Questions and Suggestions Clarify model definition and overall architecture. Describe how the recurrent cell is integrated into the model. Explain the relationship between block size and context length. Analyze the usefulness of the recurrent state and selfattention within the recurrent state update. Compare the model to alternative recurrent architectures like Memory Networks and Neural Turing Machines. Provide a clearer explanation of training and inference implementation. Release the code for better understanding and reproducibility.", "Paraphrase: Summary: This study presents an intriguing new RNN architecture for transformers. The authors have added transition states to connect transformers resulting in improved stateoftheart (SOTA) results. Strengths: Achieved new SOTA by combining multiple transformers with minor modifications. Overcame transformer limitations by incorporating an RNN structure for language modeling. Weaknesses: The networks effectiveness may be limited to very long sequences. Questions: Is it possible to utilize this network in other NLP applications", "Paraphrase: This study introduces the BlockRecurrent Transformer a novel architectural design. Sequences are segmented into blocks with each block processed by a transformer layer. These blocks are linked through a recurrent layer. The researchers use block ID encoding similar to position encoding to capture sequential relationships. The study examines the impact of various gate mechanisms in the recurrent layer through ablation studies. Strengths and Weaknesses: The paper is wellwritten and the rationale is evident. The BlockRecurrent Transformer surpasses the TransformerXL a benchmark language model in terms of the efficiencyaccuracy balance. Efficiency is measured by the number of parameters and runtime while accuracy is assessed by language modeling perplexity. Extending the proposed approach to bidirectional recurrent layers or justifying the use of a single recurrent layer would strengthen the findings. Furthermore quantifying the complexity of the model based on parameters (N) word length (W) and sequence length (S) would provide additional insights."], "kQgLvIFLyIu": ["Paraphrased Statement: This study introduces the linesets kmedian problem which aims to find a set of k centers that minimize the sum of Euclidean distances from each set of lines to its nearest center. The authors present an algorithm that based on a conversion to a colored pointsets clustering issue creates a smallsized coreset for the linesets kmedian problem. Strengths and Weaknesses: The paper introduces a new challenge the linesets kmedian problem which extends line clustering. The algorithm for creating a coreset is complex and uses a conversion to a clustering problem for colored point sets. However the papers writing and significance raise concerns. The linesets kmedian problem is not widely used in practice. The authors discuss applications in computer vision but they seem artificial. The problem is more theoretical than practical. The technical writing is difficult to comprehend. Definition 3.2 is introduced after the term \"ordered set\" has been used. The algorithms are not explained or described which hinders understanding. The main coreset algorithm is surprisingly placed in the appendix. It is not clear how the algorithms work or why they are correct. The authors could improve understanding by providing insights. Additional Questions: Can the authors explain the rationale behind the algorithms For example what is the concept behind Lemma 3.4 How can we deduce the coreset size in Theorem 3.5 from Lemma 3.4", "Paraphrased Statement: This research introduces the concept of kclustering linesets where each \"data point\" represents a collection of 1D lines in Euclidean space. The objective is to identify a set of k points in Euclidean space such that the total distance from each lineset to the nearest line in the center set is minimized. While this problem may appear abstract it has practical applications including clustering points with missing values. Specifically clustering linesets can be used to identify missing features in data points where the missing coordinate values form a line and each lineset represents a different missing categorical feature. The studys primary focus is building a coreset for the lineset clustering problem. A coreset is a smaller set of data points that accurately represents the entire datasets clustering cost to within a tolerable error margin. The main result is an efficient algorithm to construct coresets with a size proportional to the number of lines in each lineset the number of linesets and the desired error bound. This result extends prior work on coresets for point clustering and clustering with single missing values. To handle linesets with multiple missing values the authors introduce an auxiliary problem called \"clustering with colored sets.\" This approach ensures that only one line from a lineset is selected as the closest to the center set. The paper includes experiments comparing the proposed coresets to uniform sampling on synthetic and realworld datasets. The results demonstrate that the coresets achieve lower error rates with smaller set sizes although the advantage over uniform sampling may be modest in some practical applications. Strengths and Weaknesses: The paper provides a valuable extension of previous work on coresets for clustering. While the theoretical results are strong the empirical results are somewhat inconclusive leaving room for further investigation. The paper is wellwritten and provides a comprehensive overview of related work. Questions: 1. The absolute advantage of the proposed coresets over uniform sampling could be more clearly demonstrated in the experiments. 2. The paper briefly mentions the relationship between colored clustering and fair clustering but a more formal connection would be helpful. Minor Comments: 1. Line 211: \"LangbergFeldman\" should be \"FeldmanLangberg.\" 2. Line 246: \"lth\" should be \"lth.\" 3. Line 4 of Algorithm 1: Consider using \"\\mid\" for clarity in the set notation. 4. Line 260: \"that are both satisfy\" should be \"that both satisfy.\"", "Paraphrased Statement: This paper explores the task of grouping together multiple sets of lines. Specifically it aims to find a set of k cluster centers that minimize the squared distance between each line and the closest point on the closest cluster center within each set. This task extends the existing kmeans problem for line clustering. Strengths and Weaknesses: The paper is wellwritten and presents an intriguing problem. However the motivation for studying it is questionable. The provided example involving missing values seems arbitrary and the objective functions dependence on multiple minimization steps raises concerns about its practical applicability. The reduction from lines to colored points is interesting. However the analysis behind this reduction is mostly omitted from the main paper. The general result is derived using standard coreset techniques but the coreset size comparison with existing kmeans for lines approaches is missing. The experiments are simplistic comparing against random sampling only. It would be more informative to include comparisons with other methods and explore the impact of the number of lines. Additionally the authors may consider incorporating work on projective clustering into their analysis. Suggestions: Remove the experiments and focus on theoretical analysis. Emphasize the reduction to colored points and provide more details on its analysis."], "uuaMrewU9Kk": ["Summary The authors introduce a multitask offline RL method called Skill Regularized Task Decomposition (SRTD). They use autoencoders to embed skills (action sequences) and tasks (transition and reward models) into a shared latent space prioritizing highreward transitions. Artificial demonstrations are generated using the autoencoders to enhance the training dataset. In experiments involving different offline data compositions SRTD outperforms existing methods (TD3BC PCGrad SoftMod) in Metaworlds 10task setting. Strengths Compares favorably against strong baselines (TD3BC PCGrad SoftMod). Evaluates performance under various offline data compositions. Separates SRTD into its two components (SRTD SRTDID) to assess the impact of each. Weaknesses RL agent training details are provided only in the appendix. Motivation for choosing the MT10 task set is lacking. Explanation of why TD3BC and PCGrad explore orthogonality of tasks while SoftMod exploits commonality is insufficient. Questions Clarification on how TD3BC baseline was trained (with or without task knowledge).", "Paraphrased Summary: This study presents a novel multitask reinforcement learning approach that leverages skills to extract knowledge from multiple offline datasets gathered using behavior policies of varying quality. The method employs a technique that decomposes tasks into subtasks using Wasserstein autoencoders which represent both skills and tasks in the same latent space. A qualityweighted loss is applied to guide tasks towards decomposing into subtasks that align more closely with highquality skills. To enhance performance further the datasets are augmented with synthetic trajectories relevant to highquality skills for each task. Experiments on manipulation and drone navigation tasks demonstrate the algorithms effectiveness in surpassing existing stateoftheart algorithms. Strengths and Weaknesses: Strengths: Clear motivation for addressing multitask problems in offline RL. Novel and complex method. Wellwritten and easytounderstand presentation. Weaknesses: Some figures lack clarity and information (e.g. Figure 1 and the right side of Figure 2). Potential missing baselines in the experiment. Questions: Meaning of the red region in Figure 1. Benefits of using skill embedding for generating synthetic data. Comparison with a baseline using typical counterfactual data augmentation methods in Figure 4. Recommendations for additional baselines designed specifically for multitask offline RL problems.", "Paraphrased Statement: This research paper introduces a novel offline multitask reinforcement learning (RL) algorithm. It consists of a unique encoder that transforms subtrajectories and tasks into a shared space. This generative model allows the creation of hypothetical data enhancing training. The effectiveness of the proposed method was evaluated in simulated Metaworld environments (MT10) and a drone navigation task in the Airsim simulator. It outperformed previous approaches including TD3BC PCGrad and SoftMod. Strengths: Novel approach in encoding skills and tasks into a meaningful shared space. Demonstrates improved performance against relevant baselines even with varying dataset quality. Introduces a useful technique for generating highquality imaginary data. Weaknesses: Subtrajectory and skill embeddings have fixed time lengths requiring careful selection and sampling during training. Qualityaware skill regularization may not accurately assess subtrajectory quality in scenarios with mixed behaviors. Questions: Clarification on the distinction between TE and SRTDQ in the ablation study (i.e. which specific term is removed in SRTDQ). Explicit mention in the main text regarding the integration of learned embeddings into TD3BC training for offline RL optimization."], "qtZac7A3-F": ["Paraphrased Statement: Summary: This paper explores adversarial training in vision tasks by incorporating insights from natural language processing (NLP). It introduces a novel method named \"DAT\" (Discrete Adversarial Training). DAT converts images into discrete \"words\" (tokens) using a VQAGAN encoder. Adversarial perturbations are then applied to these tokens. The effectiveness of DAT is evaluated on: Robustness: ImagenetACV2R StylizedSketch Adversarial attacks: FGSM DamageNet Object detection: COCO Strengths: Crossdomain knowledge exchange from NLP to vision in the context of adversarial training Thorough evaluation across various benchmarks and architectures Clear presentation of methodology Weaknesses: Lack of evaluation on domain generalization benchmarks despite prior research on adversarial training for this purpose Unclear justification of certain assumptions in the methodology Absence of analysis on the impact of discrete perturbations on image quality Questions: How does DAT compare to existing domain generalization methods How does DAT compare to previous vision adversarial training methods (e.g. GUD MADA) What are the effects of bounding or limiting the magnitude of adversarial perturbations Does the assumption of ideal discretization (x\u02c6 \u2243 x) hold true for larger perturbations Cite the previous work that suggests different underlying distributions for adversarial and clean images. Suggestions: Include references to related adversarial training methods in NLP (e.g. FreeLB VILLA). Discuss other forms of adversarial training for vision tasks (e.g. AGAT which uses symbolic attributes). Correct typos and provide more context in figure captions. Clarify the use of \"robustness\" library to avoid confusion.", "Summary (Paraphrased) The paper presents a new training technique using discrete adversarial samples to improve the robustness and generalization of vision models. Inspired by adversarial training in natural language processing the authors propose that representing images in a discrete space enhances model robustness without significantly sacrificing generalization. They leverage a VQGAN model to generate discrete adversarial samples for adversarial training. The method demonstrates improved performance on various vision tasks including classification object detection and selfsupervised learning. Strengths and Weaknesses (Paraphrased) Originality: The concept of using discrete adversarial samples is novel and promising. Clarity: The paper is wellwritten and straightforward to understand. Clarity: However it lacks specific implementation details for the adversarial robustness experiments and contains some typos. Quality: While the theoretical intuition behind the method is limited the experimental section could be strengthened with more robust baselines. Significance: The claim of superior adversarial robustness is difficult to assess due to the absence of RobustBench baselines. However the method shows promise regarding model generalization. Questions: Can the authors provide robust accuracy values for epsilon 4255 on ImageNet (Linf) and for AutoAttack (epsilon 8255 on CIFAR) Are error bars possible for results on CIFAR to enable comparisons to other methods", "Paraphrase: Summary: This study presents a new approach called discrete adversarial training which enhances the capabilities of representation learning. Its effectiveness has been demonstrated through comprehensive tests covering image classification object detection and selfsupervised learning. Strengths and Weaknesses: Strengths: Clearly presented Convincing experimental data Weaknesses: 1. The rationale behind using discrete representations needs clarification. 2. Similarity between the proposed method and DrVit except for the optimization and quantization processes raises concerns about a biased comparison. 3. Inconsistencies in the winner between DrVit and AugRegVit across different metrics in Table 1 warrant further explanation. Questions: Provide justification for the use of discrete representations. Clarify if the proposed method and DrVit were trained using the same data loss function and architecture. If not provide results for the proposed method trained under these conditions. Address the changing winner between DrVit and AugRegVit in Table 1.", "Paraphrased Summary: This research introduces Discrete Adversarial Training (DAT) a technique for image tasks. DAT replaces continuous adversarial perturbations with discrete ones by combining image discretization with VQGAN. Inspired by NLP the study shows that discrete adversarial training improves generalization. Adversarial examples generated using this method are used to train various architectures for different tasks. Unlike conventional adversarial training DAT enhances network generalization across tasks distribution shifts and adversarial resistance. It notably improves image classification but has varying effects on object detection and semantic segmentation. Strengths: Clear writing and easy to comprehend Welldefined motivation Extensive experimental evaluation Enhanced network generalization and adversarial robustness on ResNet50 and ViT Ablation studies to analyze component impact Weaknesses: Assumptions simplify computations during gradient backpropagation through VQGAN Lack of theoretical support for assumptions heuristics are used Unclear how straightthrough gradient estimators are used to copy gradients from vq to v and from Q(x) to x No verification if adversarial inputs generated by VQGAN mislead the classifier F Unanswered Questions: Why is the constraint term unnecessary for bounding perpixel values of \u03b4 Which previous work supports the use of straightthrough gradient estimation from vq to v Can straightthrough gradient estimation be applied in practice (outside of idealized settings) Is there any crosschecking to ensure that adversarial inputs generated by VQGAN deceive the classifier F"], "wQ2QNNP8GtM": ["Paraphrased Statement: The study presents a novel image restoration model called Cross Aggregation Transformer (CAT) that employs a distinct selfattention mechanism utilizing horizontal and vertical rectangular windows. It also incorporates an axial shift operation to enhance window interactions. Furthermore a locality complementary module is proposed to leverage valuable properties of Convolutional Neural Networks (CNNs) within the Transformer architecture. The effectiveness of each component is demonstrated through ablation studies and comparisons with existing methods showing the models superiority in terms of performance and visual quality. Strengths and Weaknesses: Strengths: Simple and efficient CAT model for highquality image restoration. Clear and logical presentation of components including rectangular window selfattention axial shift operation and locality complementary module. Extensive ablation studies demonstrate the impact of each component. Favorable performance compared to recent methods such as SwinIR as evidenced by Table 2 and visual comparisons. Good overall organization and accessible code and pretrained models. Weaknesses: Lack of clarity in Figure 3s caption regarding the meanings of \"H W sl.\" While CAT achieves high performance it has a larger model size and floatingpoint operations (FLOPs) compared to SwinIR. Comparisons with SwinIR using comparable parameters and FLOPs would be valuable. Limited information about the specific model size and FLOPs for JPEG compression artifact reduction. Questions: Explain the differences between the key ideas of CAT (e.g. axialRwin) and CSWin. The authors suggest that CAT can be used for other image restoration tasks. Have they attempted such tasks or obtained preliminary results", "Paraphrased Statement: Summary: The authors introduce Cross Aggregation Transformer (CAT) for image restoration which effectively captures longdistance dependencies. CAT utilizes Rectangle Window SelfAttention (RwinSA) with axial shift and feature aggregation across multiple windows. Additionally a Locality Complementary Module (LCM) captures both local and global information. Extensive evaluations on image superresolution (SR) and JPEG artifact reduction demonstrate CATs superior performance. Strengths: 1. Wellorganized manuscript with clear illustrations and results. 2. Novel CAT architecture with three key components: RwinSA axial shift and LCM. 3. Ablation studies validate the impact of each component. 4. Comprehensive experiments on SR and JPEG artifact reduction highlighting CATs superior performance. 5. Variant models demonstrate the representational capabilities of CAT. 6. Convergence analyses in the supplementary material show CATs superiority over SwinIR. Weaknesses: 1. CATR and CATA have higher parameter counts despite showing better performance in Table 4. Table 2 in the supplementary material provides a more meaningful comparison that should be included in the main paper. 2. JPEG artifact reduction results do not show significant improvement over SwinIR. Further analysis would be beneficial. Questions: 1. What are the specific differences in the pipeline for JPEG artifact reduction compared to image SR 2. How does the axial shift in CAT differ from the shift operation in SwinIR 3. Can CAT be extended to other image restoration tasks such as denoising and deblurring If so what is its performance compared to current stateoftheart methods", "Paraphrase: This study introduces CAT an image restoration model. It incorporates RwinSA which uses parallel horizontal and vertical rectangle window multihead attention. CAT employs LCM which integrates CNN principles into the Transformer architecture to enhance selfattention. Ablation studies demonstrate LCMs improved performance with minimal computational overhead. Extensive evaluations show that CAT outperforms stateoftheart methods in image superresolution and JPEG artifact reduction tasks. Strengths and Weaknesses: Strengths: Proposes the CAT model with RwinSA and LCM. LCM enhances selfattention by incorporating CNN biases. Weaknesses: The contribution and novelty of RwinSA and LCM are not fully convincing as similar concepts have been previously introduced. The discussion of existing Transformer methods is insufficient. More recent methods should be included in the comparison to demonstrate CATs superiority. Questions: 1. Difference between shifted window and axialshift operation: While both operations use horizontal and vertical shifting the difference lies in the shape of the window. Shifted window operation [19] uses square windows while axialshift operation uses rectangular windows with one fixed dimension. 2. Originality of RwinSA: The use of rectangular windows in RwinSA is similar to the axial rectangle window division proposed in CSWin [10] which should be cited as a reference. 3. Additional comparison: The study should include more recent methods like Uformer and Restormer to establish CATs superiority. 4. Insufficient discussion of existing Transformer methods: The related work section should provide a more detailed analysis of existing Transformer methods particularly CSWin and Uformer. 5. Additional experiments: Experiments on restoring color JPEG images should be included to provide a more comprehensive evaluation.", "Paraphrased Statement: Summary: A novel Transformer architecture Cross Aggregation Transformer (CAT) is introduced for superior image restoration. It incorporates three key elements: window selfattention axial shift operation and locality complementary module (LCM). Ablation studies demonstrate the effectiveness of these components. Comparative evaluations show CATs advantage over existing Transformers. Strengths: Aggregates features across windows with expanded sensing area enhancing restoration capabilities. Axial shift operation increases window interactions. LCM utilizes translation invariance and locality allowing global and local feature capture. Extensive experiments and ablation studies validate component efficacy. Promising Transformer approach with provided source code and models for reproducibility and further exploration. Visualization results indicate CATs expanded receptive field supporting longrange dependency estimation. Clear and wellorganized presentation. Weaknesses: CATs parameter count and FLOPs exceed those of SwinIR. Future work could optimize model size and complexity. The distinction between axial shift operation and Swin Transformers shift option is not entirely clear. Questions: Further clarification is sought on the modest performance gains of CAT over SwinIR in JPEG artifact reduction despite evident visual differences. The authors claim CATs suitability for image restoration inquiries are made about its performance in other image restoration tasks and its potential to achieve stateoftheart results. A detailed comparison is requested between CAT and CSwin highlighting their respective differences."], "ttC9p-CtYT": ["Paraphrase: Summary: This paper analyzes a technique for solving optimal control problems using deep neural networks to approximate the value function. This approach results in a quadratic representation of the value function in terms of control effort enabling efficient optimization for deep Q learning. Mathematical theorems establish the conditions under which this method applies. Strengths and Weaknesses: A key strength lies in the empirical evaluation of the proposed method demonstrating that two variants (RBBNAF and GBBNAF) significantly outperform DDPG on various test problems. These methods show promise for solving optimal control problems within a specific class. However the method requires partial knowledge of the system (reward function and controlaffine terms in dynamics) limiting its application in pure reinforcement learning. Additionally as the underlying NAF algorithm is not novel the papers contribution is relatively modest. The class of applicable problems is important in practice but not universally applicable. The authors propose extending the method to more general problems by introducing a quadratic term in the cost function but this approach may not fully capture the generality of the original method. Questions: Is DDPG the most current baseline method Should PPO or SAC also be included in empirical comparisons On page 2 line 47 is \"quality index\" referring to the cost (reward) function A minor typo exists on page 8 line 199: \"greedy politics\" should be \"greedy policies.\"", "Paraphrase: Problem: The popular Summary Qlearning technique faces challenges in continuous action spaces. Approach: Normalized Advantage Functions (NAFs) solve this issue by assuming a quadratic relationship between the approximated Q function and the controls. This allows for efficient policy determination through argmax computation. Analysis: The study explores the suitability of the NAF approximation for systems with controllable dynamics quadratic costs and continuoustime formulation. It shows how to map the continuoustime problem to discretetime reinforcement learning and introduces a modified NAF approximation to bound the Qfunction approximation error. Results: The analysis provides performance bounds that hold for the modified NAF approximation but not for the original NAF approximation. Empirical evaluations show that the extended approximation family improves performance on benchmarks compared to the baseline NAF and a policy gradient method. Strengths: Addresses the important question of NAF approximation suitability. Provides approximation and performance bounds for a useful class of problems. Introduces variants that incorporate knowledge about rewards and dynamics. Presents experimental results demonstrating the benefits of the extended approximation family. Weaknesses: The addition of control constraints may change the original research question. Its unclear if the modified NAF approximation is crucial in the case of unconstrained controls. The benefits observed experimentally may be due to incorporating additional information rather than the new approximation family. Grammatical errors need to be corrected. Questions: Do Theorems 1 and 2 hold for the original NAF approximation in the absence of control constraints Are the observed benefits of the modified NAF family solely due to the new approximation or also due to additional information", "Paraphrased Statement: Summary: This data identifies a group of optimal control problems where a quadratic approximation of the problems Qfunction is accurate and yields nearly optimal policies. The proposed quadratic parameterization of the Qfunction is similar to the Normalized Advantage Functions (NAF) approach but with added generalization. The authors demonstrate the effectiveness of this approximation in various control problems. Strengths and Weaknesses: This paper offers a useful theoretical contribution by classifying RL tasks suitable for NAFstyle Qfunction approximations. The explanations and derivations are clear and the experiments are easy to follow. However the main contribution has a limited scope and its unclear if NAFstyle approaches are competitive with current continuous action space RL methods. The authors compare their approach to DDPG but not to more advanced baselines like SAC. The papers theoretical contribution also depends on the performance of NAFstyle algorithms. Another major shortcoming is the lack of analysis of problems not suitable for a quadratic Qfunction approximation. A clearer understanding of the applicability of NAF would be valuable. For instance its unclear how the suitability and performance of the NAF approximation change as the cost becomes less quadratic in terms of u(t). Presentation issues include: The motivation for Equation 19 and the three implementations in Section 5 could be improved. The rewardbased BNAF and gradientbased BNAF algorithms are introduced abruptly without sufficient motivation. Questions: 1. How does the performance of BNAF and its variants compare to more advanced RL algorithms (e.g. SAC) for continuous action spaces To what extent is BNAF compatible with implementation details that enhance performance in these baselines 2. How effectively does BNAF handle violations of the assumptions in Equation 6"], "x-i37an3uym": ["Paraphrase: Summary: Traditional auxiliary task learning assigns a single scalar to each auxiliary task to adapt it to the main task. This paper proposes learning a scalar per module for each auxiliary task giving finer control over adaptations. Experiments show this approach outperforms previous methods. Strengths: Clear writing Welldefined motivation Comprehensive experimental evaluation Weaknesses: Incremental approach building on existing ideas Lack of novelty in combining existing techniques Insufficient justification for using ParameterBlockModelL Questions: 1. Criteria for bolding numbers in tables: Is it simply highest average 2. Did all Table 12 experiments use BlockL 3. Guidelines for determining when to use ParameterBlockModelL 4. Effects of model size and development set size on results 5. Would exploring a broader range of hyperparameters (including learning rate) enhance all methods 6. Investigating the evolution of modulelevel weights over training time would provide insights.", "Paraphrase: This paper introduces a new learning framework that upgrades network settings at the module level. It does this through adjustable modulelevel significance. The framework is upgraded using bilevel optimization a standard technique for auxiliary learning with meta learning. This involves two steps: Inner loop: Updates network settings by reweighting gradients of auxiliary losses for each network module. It does this through a fixed importance mask on the training data. Outer loop: Upgrades the auxiliary importance mask based on primary task performance. It uses multiple inner rollout steps and assumes network weightings are stable in the inner loop. This saves training time and memory on development data. The framework stores the best network based on validation performance. Experiments on diverse datasets show that the framework outperforms existing auxiliary learning methods. Strengths and Weaknesses: Strengths: Clear writing visually appealing explanation of auxiliary importance. Weaknesses: Limited number of auxiliary tasks (usually one or two). Benefits of auxiliary tasks not clearly reflected in experiments. L2 regularization may not be an auxiliary task. Lack of a simple baseline (equal weighting) to demonstrate the advantages of proposed methods. Questions: Bounded or unbounded auxiliary importance mask Experiments with 3 or more auxiliary tasks Experiments with equal weighting baseline Are the datasets in Section 4.4 suspicious due to large performance gaps", "Paraphrased Statement: Summary: This study aims to enhance the efficiency of using auxiliary objectives to train deep neural networks. The researchers introduce an algorithm that simultaneously optimizes: The weight of each auxiliary loss on specific model components Model parameters based on a weighted sum of loss terms Tests on image classification and user rating prediction tasks demonstrate the proposed approachs effectiveness. Strengths: Wellwritten and clear presentation of the idea and technical aspects Potential impact on the research community due to the limited understanding of auxiliary loss effects on different model parts Interesting observations from experiments particularly loss weight visualizations Weaknesses: Limited scale experiments with potential for more promising results if applied to larger networks and tasks Lack of visualization supporting the explanation for loss weights learning to be 0 Absence of information on computational resources required by the proposed approach Questions: Can the algorithm be applied to more advanced networks and larger tasks Can the change in loss weights over time be visualized to support the explanation for loss weights learning to be 0 What are the computational constraints associated with the proposed approach", "Paraphrased Statement This research presents an optimization approach tailored to the prevalent \"learning with auxiliary losses\" paradigm considering that various auxiliary losses impact different modules of the model differently when it comes to improving the primary task. Inspired by multitask learning the authors propose an effective twolevel optimization method to optimize the significance of each loss to each module. Strengths The importance of moduleaware optimization in resolving modulelevel conflicts between the main task and ancillary losses is acknowledged. The novelty and efficiency of the proposed method are highlighted with a focus on its efficient hypergradient derivation and bestresponse approximation. Extensive experiments demonstrate the methods effectiveness across various datasets and scenarios. The analysis of the experiments provides valuable insights including the varying weights of different modules with their complexity. The paper is wellwritten and accessible showcasing the general applicability of the proposed approach. Weaknesses While the method is efficient compared to direct unrolling a more detailed analysis of the time complexity of both approaches would be beneficial. While the impact of module choice is discussed further guidance is needed on when to use finergrained module splits or parameterlevel splits with larger development sets. Questions 1. Do the proposed method and baselines use the same validation set or does the validation set for the baselines include the development set 2. What are the time complexities of the baselines and the proposed method"], "lCGDKJGHoUv": ["Paraphrased Summary: Researchers investigated the performance of Stochastic Gradient Descent (SGD) in stochastic convex optimization. They demonstrated that: Main Contributions: Constructed an example where onepass SGD exhibits a significant gap between empirical risk (training loss) and generalization error (performance on unseen data) indicating that SGD may not always minimize empirical loss. Showed that this issue is reduced when using SGD with replacement (sampling data points multiple times). Established upper and lower bounds for SGD without replacement in multiepoch scenarios. Strengths: Provides new insights into the behavior of SGD in optimizing both empirical and population risk. Challenges the common assumption that optimizing empirical loss leads to generalization. Highlights the potential for a new approach to analyzing statistical learning. Weaknesses: The constructed example may not represent realworld learning problems. The analysis focuses on averaged SGD solutions rather than the final iteration solution. Questions: Can the example be related to practical learning problems How would the results differ for the final iteration solution", "Paraphrase: The study demonstrates that within the stochastic convex optimization framework certain problem scenarios exist where the solution obtained through Stochastic Gradient Descent (SGD) exhibits significant gaps in both empirical risk and generalization error (approaching 1 in the without replacement scenario). Consequently SGD is not considered algorithmically stable. However this phenomenon is not observed in SGD with replacement. Strengths: The paper presents a novel and intriguing finding that has the potential to impact SGD generalization bounds. The analytical evidence provided is robust and rigorous. The analysis extends to the multiepoch regime. Weaknesses: The paper lacks substantive discussions and conclusions. Questions: Why are there no discussions or conclusions present Minor Issue: The word \"forgo\" in line 338 should be \"forego.\"", "Paraphrased statement: Summary This paper proposes a framework for stochastic convex optimization using one pass SGD. While the method achieves an O(1 \\sqrtn) population risk minimization rate it exhibits \\Omega(1) training and generalization errors. Strengths and Weaknesses The paper is wellstructured and clear. The analysis is meticulous and original. However the primary results in Section 3 may not be practical because: 1. One pass SGD is rarely used in practice. 2. The considered setting is specific (cherrypicked distribution and loss d \\geq 2n \\log n). Concerns The papers conclusions and interpretations raise concerns: 1. The claim that SGD cannot be interpreted as regularized empirical risk minimization is questionable as the implicit regularization viewpoint is important in practical settings (interpolation). 2. The argument that uniform convergence cannot explain SGDs generalization is limited to one pass SGD which is not used in practice. 3. The statement that SGD is not algorithmically stable is valid only for one pass SGD which is not realistic. Minor Comments 1. The population risk convergence rate could be specified rather than using a big O notation to avoid potential dimensiondependent hidden dependencies. 2. It is unclear why the argmin in line 183 is unique. If it is not it should be clarified which one is chosen. Questions None provided in the original statement.", "Paraphrase: The authors investigate Stochastic Gradient Descent (SGD) in two scenarios: with replacement and without replacement. They provide examples where SGD fails to converge even though the optimization objective is wellbehaved (e.g. convex and smooth). Critique: This research is deemed substantial but unsuitable for the NeurIPS community due to its highly theoretical nature. While the dataset and objective function used are simple it remains unclear how the findings relate to practical machine learning applications. This is compounded by the absence of experiments in the paper."], "tVHh_vD84EK": ["Paraphrase: AutoST1 and AutoST2 are newly proposed algorithms designed to create spatiotemporal forecasting systems. These algorithms search for the best architecture by choosing from a set of layers that include: Temporalfirst modeling layer (T2S) Spatialfirst modeling layer (S2T) Spatialtemporal synchronous layer (STS) These three layers have similar inputs and outputs but use different combinations of time series linear selfattention units and highorder mix graph convolution units. AutoST2 is the more effective algorithm building a sequential architecture with layers that can be any combination of T2S S2T or STS. Strengths: The concept of learning an architecture that prioritizes spatial or temporal relationships for forecasting is innovative. Figure 2 provides a clear visual representation of the authors approach. Weaknesses: The experiments focus exclusively on traffic forecasting datasets limiting the generalizability of the algorithms. The complexity of AutoST1 and especially AutoST2 is not discussed. The motivation for using adaptive diffusion convolution is not explained. The rationale for the specific architecture of T2S S2T and STS layers is not provided. Minor Issues: Some typographical errors and inconsistencies in terminology have been identified. Questions: What is the time complexity of AutoST2 particularly in terms of searching for optimal layers The dataset diversity is limited. Would the algorithms perform as well on more varied tasks such as pose forecasting", "Paraphrased Statement: The Summary Paper introduces UniST an innovative universal model for anticipating events in both space and time. UniST combines three approaches: spatial first temporal first and spatiotemporal synchronous. These methods are realized as separate layers within UniST: S2T Layer T2S Layer and STS Layer. The paper also presents a new strategy for optimizing the architecture of UniST using network architecture search. Strengths: 1. Clear and concise paper. 2. Detailed explanations and visual aids. 3. Extensive experiments showing significant improvement over existing methods. Weakness: 1. Table 1 needs a more detailed caption to clarify its contents.", "Summary: This paper examines architectural designs for spatiotemporal modeling in forecasting tasks. It proposes three types of layers (S2T T2S STS) to capture spatial temporal and spatiotemporal information. These layers are integrated into a unified encoderdecoder framework for spatiotemporal forecasting. An architecture search algorithm is employed to determine the optimal connection configuration of the modeling layers. The resulting model (AutoST) outperforms baseline methods and a simpler model (UniST) that uses only one type of modeling layer. Strengths: Wellmotivated investigation of different spatiotemporal modeling approaches. Clear ablation studies demonstrate the effectiveness of using automated architecture search to combine modeling layers. Weaknesses: Limited technical novelty most techniques are adapted from previous works with minor modifications. Unfair comparison with baseline methods due to differences in model complexity and optimization details. Lack of a baseline model that combines all three modeling layers without using architecture search. Minor Issues: Missing comma in the last equation of Eqn. 1. Equation in L204 does not match the text describing the final output calculation."], "u6MpfQPx9ck": ["Summary Paraphrase The authors propose \"Max Probability Cover\" as the optimal strategy for 1NN (1Nearest Neighbor) models that minimizes an upper bound on their generalization error. However since data distributions are unknown in practice they use empirical distributions which transforms Max Probability Cover into \"Max Coverage.\" To overcome the computational complexity of Max Coverage they present \"ProbCover\" a greedy algorithm for Max Coverage. For 1NN models Max Probability Cover and Coreset methods have a \"dual\" relationship: Max Probability Cover aims to maximize coverage for a fixed parameter (\u03b4) while Coreset minimizes \u03b4 for a fixed coverage of 1. In lowbudget scenarios where the budget is insufficient to cover the unlabeled set ProbCover outperforms existing AL (Active Learning) methods that are mostly tailored for highbudget situations. This superiority is demonstrated through comparisons with 9 baseline AL models in 3 different frameworks. Strengths and Weaknesses Strengths: Addresses the problem of lowbudget regimes in AL. Establishes a theoretical relationship between Max Probability Cover and Coreset in 1NN models. Considers hyperparameter tuning (\u03b4) in a reasonable manner. Weaknesses: Theoretical analysis is based on 1NN models which may not be applicable to practical scenarios. There is a limitation in extending theoretical results to general architectures due to the gap between 1NN models and deep neural networks. Questions 1. The authors state that their approach relies on the existence of a good embedding space. However concerns are raised regarding the quality of representations provided by ResNet18 for highdimensional input images. The authors are requested to clarify this aspect. 2. The authors specify that their method is intended for lowbudget regimes. As the budget increases over time the covered region expands. At some point the problem may transition from a lowbudget regime to a highbudget regime. The authors are asked to suggest an efficient way to detect this transition.", "Paraphrase: Summary: This paper presents an active learning approach for lowbudget scenarios. It formulates the task as a Max Probability Coverage problem and uses a greedy algorithm to select instances for annotation. The method shows improvements over baselines in both supervised and semisupervised settings across various datasets. Strengths and Weaknesses: Strengths: The objective of maximizing probability coverage is logical with theoretical support. The paper draws an interesting parallel between Max Probability Cover and Coreset as dual problems. The approach is practical and provides valuable insights for model design. It outperforms existing methods in lowbudget active learning. Weaknesses: The greedy algorithm may not guarantee the optimality of Max Coverage potentially affecting the sampling quality. The paper lacks references and comparisons to related work such as \"Low Budget Active Learning via Wasserstein Distance: An Integer Programming Approach.\" Questions: Why is Max Coverage objective a submodular monotone function (Reference needed) Can alternative methods be explored to achieve optimal Max Coverage given the limitations of the greedy algorithm", "Paraphrase: This research presents a novel active learning technique designed for scenarios with limited budget constraints. The approach focuses on maximizing probabilistic coverage. Its effectiveness is demonstrated across both low and highbudget settings. Additionally the method is evaluated in fully semisupervised environments. Strengths: The problem addressed is highly relevant and fundamental to the research field. The lowbudget regime presents challenges that are encountered in realworld scenarios. The experiments conducted are comprehensive and welldesigned. Weaknesses: The training details are not provided making it challenging to replicate the method. The experiments are limited to a relatively small dataset. The comparison with TypiClust is somewhat confusing. Questions: 1. Data Selection Process: How does the proposed method select data Does it utilize data point features or solely the RGB image What specific data points were used in the experiment 2. Cost of Graph Building and Data Selection: Can you provide insights into the time and computational costs associated with graph building and data selection 3. Experiments on Larger Datasets: Please consider conducting experiments on a larger dataset (e.g. ImageNet) or on different tasks (e.g. object detection on COCO) to demonstrate the methods effectiveness in more realistic applications. 4. Comparison with TypiClust: The performance of TypiClust reported in this paper is lower than that reported in the original TypiClust paper. Can you clarify the differences between these experiments and explain the apparent discrepancy"], "wmsw0bihpZF": ["Paraphrased Statement: This article introduces the LOC framework for improving data collection strategies in machine learning models while factoring in uncertainty in data requirements. The framework can be expanded to scenarios involving multiple data sources. Experiments demonstrate that LOC effectively reduces the likelihood of falling short of targeted performance. Strengths: Clear and accessible writing Innovative concept Applicability to multiple data scenarios Weaknesses: Missing baseline comparison with Mahmood et al.s [2] correction factor for powerlaw regression Potential for LOC to overestimate data requirements in certain instances (e.g. CIFAR100 and BDD100K) Inconsistencies in descriptions (e.g. inaccuracy in Figure 2 caption incorrect problem formulation in line 149) Questions: 1. Provide results for baseline methods mentioned in Weakness 1 (powerlaw regression with correction factor algebraic root regression). 2. Clarify the distinction between \"optimal data collection problem\" in this work and the data collection problem in Mahmood et al. [2]. 3. Guide on selecting an optimal cost value considering its significant impact on cost ratios (e.g. Table 2).", "Paraphrased Statement: Summary: Its crucial to label data for machine learning but it can be costly. Overlabeling wastes resources while underlabeling reduces model performance. This research presents a method for determining the optimal amount of data to label to achieve a specific accuracy goal. The framework can also account for varying costs associated with labeling different data items. Strengths: Addresses a significant underresearched problem Clear and wellwritten Effective approach that outperforms alternative methods Weaknesses: Limited discussion on the implications of the results Unclear presentation of key metrics Unknown accuracy of the estimated cumulative density function (F(q)) in practice Questions: How accurately can F(q) be estimated Is the procedure applicable when training multiple models on subsets of the data Why are multiple data collection rounds necessary with the CDF What does the bootstrap resampling process entail Is the final model trained with all collected data to verify accuracy against V How does D vary with T and V What are the implications of cost ratios that are not significantly different from labeling the entire dataset Why do failure rates differ across tasks for the same T What are the outliers removed in the 99th percentile filtering", "Paraphrase: Summary: The paper extends a method for determining the optimal data collection size for a given task and model. It adds a bootstrap sampling step to improve performance enabling the method to handle multivariate tasks. Contributions include: A new algorithm for data collection planning Extension to multivariate tasks Improved performance compared to the original algorithm Strengths: The problem is relevant and gaining interest. The proposed method is technically sound. Experiments show improvements over the baseline method. The paper is generally wellorganized. Weaknesses: The Related Work section overlaps significantly with the baseline paper potentially violating NeuRIPS guidelines. The motivation for the new algorithm is unclear. The bootstrap sampling step can be timeconsuming and introduce unnecessary complexity. The experiments are limited with only one regression function and two values of a hyperparameter explored. Minor typographical error (line 139). It is unclear why one would use such a framework when support and validation sets are already available for sample size estimation. Questions: Are the results sensitive to the regression function used What is the impact of varying the hyperparameter K"], "rUb6iKYrgXQ": ["Paraphrase of Statement: Summary: This study introduces Conditional Robust Optimization (CRO) an approach for solving robust optimization problems with contextual information. The uncertainty set incorporates the latest side information from a set of covariates. The conditional uncertainty is defined using an integrated framework (IDCC) that simultaneously partitions the covariate space and creates regionspecific deep uncertainty sets for the random vector perturbing the CRO problem. The authors evaluate IDCC on US stock market data and demonstrate that it significantly reduces outofsample Value at Risk (VaR) compared to noncontextual RO methods at high protection levels. Strengths and Weaknesses: Strengths: CRO is a novel method that utilizes contextual information. It reduces outofsample VaR compared to noncontextual optimization. IDCC corrects an earlier error in the DDDRO approach. IDCC jointly performs deep clustering and uncertainty set design ensuring accurate uncertainty estimation based on covariate variation. It handles partial clustering situations through a parameterized random assignment policy. DDDRO and IDCC can identify nonconvex uncertainty sets using deep neural networks. IDCC concentrates the uncertainty set on highmass regions resulting in a less conservative optimization problem than DDDRO and ellipsoids. Weaknesses: Additional real or synthetic datasets would provide better generalization. Including IDCC and DCC algorithm descriptions would enhance understanding. Exploring alternative clustering methods like DBSCAN and fuzzy approaches could improve scalability and partial clustering. Runtime information for the experiments would facilitate method comparisons. The algorithms utility beyond time series data and the potential for generalization are not explicitly discussed. Questions: Can the proposed algorithms be extended to other domains such as computer vision Are there limitations to the proposed algorithm that require further investigation", "Paraphrased Statement: This research explores a challenging optimization problem that involves incorporating given variables (covariates) and developing datadriven solutions. It introduces a unified approach that combines deep learning for grouping data points (deep clustering) and generating uncertainty sets (deep uncertainty set design) based on the covariates. By linking the variation in the covariates to the variation in a set of parameters the approach aims to provide similar guarantees in both spaces. Additionally the paper establishes the relationship between a specific type of robust optimization (conditional robust optimization) and a separate optimization problem (CVO problem). Realworld data from the US stock market and simulations support the proposed solutions. Strengths: Introduces a novel conditional robust optimization framework for addressing riskaverse optimization problems considering context (covariates). Offers a comprehensive solution that combines deep learning techniques for data grouping and uncertainty set design. Provides theoretical connections between conditional robust optimization and another optimization problem. Demonstrates the effectiveness of the approach through simulations and realworld scenarios. Weaknesses: None mentioned.", "Paraphrase: The study explores risk aversion in a datadriven problem with contextaware data using robust optimization. It extends previous work using deep neural networks for unconditional uncertainty sets. Conditional uncertainty sets leveraging contextual data are introduced. Building these sets for a new observation involves: 1. Assigning it to a data cluster. 2. Defining a local unconditional uncertainty set for the cluster. Two clustering methods based on total variation minimization are proposed. The second method generalizes the first allowing tradeoffs between feature and disturbance space variations. The methods performance is demonstrated in a portfolio optimization problem showing improved risk reduction for high risk aversion and balanced cluster sizes. Strengths and Weaknesses: Strengths: Novel approach to a relevant problem Method leverages concepts from diverse fields Clear methodology and sound presentation Weaknesses and Questions: Exposition: Q1.1: Intuitively minimizing total variations when building uncertainty sets promotes robustness by considering more extreme scenarios. Q1.2: The approach may be limited to linear uncertainty in the cost function. Can it be extended to nonlinear functions or uncertainties in constraints Q1.3: Uncertainty sets are fixed within clusters. Can this be improved to incorporate more contextual data Q1.4: The critique of DDDROs assumption of normalized projections needs clarification. Q1.5: Explain the intuition behind the DCC approachs first drawback. Q1.6: The statement about IDCC converging to DCC needs more precision it appears that \u03b1S 1 makes them identical. Q1.7: The cluster assignment uses a random policy. Could this be replaced with a more deterministic approach Numerical Study: Q2.1: Why are benchmarks from recent literature outside of DDDRO not included Q2.2: Are 10 replications sufficient to detect statistical differences between methods What do the error bars represent Q2.3: Clarify the number and nature of the majority and minority clusters. Q2.4: Is the parameter \u03b5 used in both uncertainty set construction and VaR evaluation Could its value impact the tradeoff between average costs and VaR Minor Comments: Footnote [1] not found. Define \u03b1K and \u03b1S more clearly. Correct references to Algorithms 2 and 3."], "yjybfsIUdNu": ["Paraphrased Summary: The paper focuses on multifidelity Reinforcement Learning (RL) where multiple environments with varying levels of accuracy are available. The proposed approach leverages the correlation between the reward variables in two environments (exact and approximate) to reduce variance in value function estimation and sample complexity. Theoretical analysis supports the accuracy of the value function and greedy policy. Experimental results demonstrate the effectiveness of the approach on synthetic domains and neural architecture search (NAS). Strengths: Addresses the relevant topic of multifidelity RL. Novel approach based on control variate techniques. Theoretical analysis provides insights into the approachs advantages. Weaknesses: Relies on the assumption of correlated reward variables which can be restrictive in practice. Requires estimation of the true lowfidelity value function introducing further uncertainty. Theoretical analysis does not fully account for uncertainties in covariance and correlation coefficient estimation. Questions: Can the authors clarify the implications of the correlation assumption and provide examples of where it applies Is the assumption of a finite state and action space crucial or can the approach be extended to continuous spaces Is the generative model for the lowfidelity environment essential for value function estimation Can the authors elaborate on the practical considerations for mapping highfidelity states to lowfidelity states in realworld applications", "Paraphrased Summary This paper introduces a novel multifidelity reinforcement learning (RL) methodology (MFMCRL) that utilizes lowfidelity data to reduce variance in value function estimates. Unlike existing MFRL techniques MFMCRL employs control variate theory to harness observed correlation between high and lowvalue returns to reduce variance. This approach yields theoretical improvements in policy evaluation and improvement. Empirical results demonstrate its effectiveness in synthetic and realworld hyperparameter tuning scenarios. Strengths and Weaknesses The paper presents a clever idea and wellscoped approach. However there are areas where terminology could be more precise and some disadvantages need to be acknowledged compared to prior MFRL work. Specific Concerns 1. Independence Assumption and Covariance Construction: The assumption of independence between high and lowvalue returns when their indices dont match (i \u2260 j) is unclear. A concrete example illustrating this would be helpful. 2. Episodic Domain Restriction: The analysis is limited to episodic domains with terminal states. The necessity of this assumption should be clarified and potential extensions to nonepisodic domains should be discussed. 3. Efficient Return Estimation: MFMCRL uses inefficient MC returns while prior MFRL methods utilized bootstrapping. The authors should explain why bootstrapping is unsuitable in this framework. 4. Need for Pseudocode: The pseudocode for MFMCRL should be included in the main paper as the current description is incomplete. 5. Potential Disadvantages: Compared to previous MFRL methods MFMCRL assumes only two environments (low and high) and uses less efficient valuebased RL with MC returns. The authors should acknowledge these potential drawbacks. Questions 1. Can the authors clarify the i \u2260 j covariance construction and its rationale 2. Why is the analysis restricted to episodic domains 3. Can MFMCRL handle multiple lowfidelity environments 4. How does MFMCRL compare to prior MFRL methods in terms of sample efficiency when sampling from the highest fidelity environment", "Summary The authors focus on enhancing the performance of reinforcement learning algorithms in the presence of multifidelity data by employing control variates. Strengths Clearly written paper Exploration of an important area Wellpresented experiments with discussion of strengths and weaknesses Weaknesses 1. Lack of Literature Review The paper overlooks the literature on variance reduction in classic RL and asymmetric RL. Assumptions and discussions surrounding policy iteration need to be elaborated in the main text or appendix. Inclusion of additional baseline algorithms in the NAS example is necessary. 2. Transparency The authors do not fully disclose the types of MDPs compatible with the algorithm leaving it unclear if it applies beyond lowdimensional stateaction spaces and toy problems. Questions Preliminaries and Related Work The paper lacks citations for basic statements regarding optimal policies. A reference like (1) should be included. The concept of baselines in RL should be acknowledged with references to relevant literature such as (2) and (34). The specific policy search algorithm used in [1] should be explicitly stated. The meaning of \"close in terms of the true value function\" in relation to lowfidelity and highfidelity models needs clarification. The connection to asymmetric learning algorithms (56) should be discussed to enhance the analysis. Multifidelity Estimation in RL The potential for larger state spaces and slower learning due to the manytoone mapping of states from highfidelity to lowfidelity environments should be addressed. The correlation between Ghi and Glo is only present in the first step of the lowfidelity simulator rollout with the correlation diminishing for subsequent steps due to the discount factor. The assumption of being able to exactly estimate Q in the lowfidelity simulator seems unrealistic unless this notation is misunderstood. The authors should include a reference for the policy improvement theorem mentioned in 3.3.2. The statement about equal exploration of all actions at the target state seems to contradict the earlier claim that the algorithm addresses exploration issues in systems with high correlation in expected reward ahead between high and lowfidelity simulators. Numerical Experiments The paper lacks comparisons with other common baselines for NAS such as nonvariancereduced algorithms. The relative clock times of NAS with and without controlvariate schemes should be presented. The plot in figure 3 appears incorrect as the standard deviation does not fully overlap the mean for the nonvariancereduced algorithm. The authors should ensure they have reported the correct information."], "uRSvcqwOm0": ["Paraphrased Summary This research introduces a parameterized method for causal discovery with categorical data a less explored area compared to continuous data analysis. The proposed method known as \"classification with optimal label permutation (COLP)\" draws inspiration from ordinal regression. While ordinal regression is limited to variables with a natural ordering COLP extends its applicability to categorical variables lacking inherent order. The causal model associated with COLP identifies an optimal permutation and COLP parameters conforming to a designated causal direction while the opposing anticausal direction remains unrepresentable by any COLP model. The paper demonstrates this concept via identifiability proofs. The estimation process employs basic maximum likelihood estimation (MLE) techniques to determine the optimal COLP model. Experimental evaluations using synthetic and realworld data demonstrate the superiority of COLP over existing methods. Strengths and Weaknesses Strengths: Intriguing intuition: The illustrative example clarifies the methods principles. Comprehensive experimentation: COLPs performance is assessed on both synthetic and realworld datasets exhibiting its advantages over HCR and CE. Weaknesses: Assumptions not clearly defined: The paper lacks a mathematical formulation of the parametric assumptions underlying COLP and ordinal regression. Theoretical issues: The identifiability section appears poorly explained with questionable basic examples and results. Experimental results: While extensive the realdata analysis lacks quantitative metrics and focuses only on specific edges raising concerns about cherrypicking. Questions 1. Assumptions: What are the underlying assumptions of COLP and ordinal regression How do they compare to existing methods Can they be verified from data 2. Identifiability: The identifiability results seem flawed. Can the authors clarify 3. Real data experiments: What criteria were used to select the evaluated edges Are there metrics that assess overall performance across all edges in the real datasets What is meant by \"outofsample prediction\"", "Summary The authors introduce a new method for determining the pairwise orientation and proximity of categorical variables with two categories. Strengths The technique is grounded in a reliable concept. Creating a dataset of categorical pairwise causal data is a valuable concept. The theory and algorithm for the COLP algorithm appear reasonable. Weaknesses The paper overlooks the established Markov Equivalence Class (MEC) approach to causal discovery. The authors did not demonstrate how the \"natural orderings\" identified by COLP correspond to intuitive orderings despite acknowledging this in the experimental section. They also overlook strategies that combine MEC with pairwise approaches to orient edges. It is unclear to what extent the COLP method can correctly identify nonorientable pairwise edges. Questions Could the authors incorporate MEC methods into their analysis Can they clarify the identifiability criteria for binary variables Can COLP detect the nonexistence of pairwise edges and indicate \"undirected\" when a direction cannot be inferred", "Summary (Paraphrase): This research introduces a new method called \"classification with the optimal label permutation\" (COLP) for uncovering causal relationships from data consisting of categories. The central concept is that identifying the correct permutation enables the identification of ordinal regression. Strengths: COLP can deduce causal connections from categorical data. COLP is almost certainly identifiable. Weaknesses: COLPs novelty is limited as it closely resembles [1]. COLP heavily depends on label permutation in ordinal regression. An ablation study is required to demonstrate its superiority in the absence of label permutation. The proposed greedy algorithm for determining permutations may be inefficient for data with numerous categories. Its performance and accuracy in determining permutations under various category counts should be investigated. [1] Ni Y. Mallick B. (2022). Ordinal Causal Discovery. The 38th Conference on Uncertainty in Artificial Intelligence.", "Paraphrased Statement: Summary: The authors present a method for determining causeandeffect relationships when both the cause and effect variables are in categorical form. They use an ordinal regression model and search for a more simplified model. The simplification is driven by the belief that specific sequences of input data fit better with the outcome data even in the absence of inherent ordering in the data. Consequently the authors propose the COLP (classification with optimal label permutation) model which seeks the permutation of inputs that yields the best maximum likelihood fit. Strengths and Weaknesses: The proposed method is clear and seemingly valid. The experimental evaluation includes simulated and realworld data and the findings are persuasive. The COLPgreedy approach performs well compared to the computationally intensive COLPexhaustive approach. However the COLP method assumes that a specific ordering of categories makes more sense than others and its difficult to assess the frequency of this assumption holding true. The method is limited by the assumption of no hidden confounders which the authors acknowledge. The clarity of the presentation could be improved. Detailed Comments: p1 l27: Change \"the Y admit\" to \"Y admits\". p1 l31: Rewrite as \"or\" instead of \"and\". p2 l39: Remove \"natural orderings. But...\" and replace with \"... natural orderings but...\". p2 l68: Omit \\gammaL from the list of thresholds as it is later excluded (p3 l109). p2 l75: Remove the dangling left parenthesis at the sentences end. p3 l85: Change \"contraint\" to \"constraint\". p3 l108: Add \"the\" before \"causal model\". p3 l111 l117: Change \"casual\" to \"causal\". p3 l132: Add \"a\" after \"such\". p3 l138: Change \"does\" to \"do\". p3 l141: Change \"confounder\" to \"confounders\". p6 Figure 2 caption: Change \"COLPExhausitve\" to \"COLPExhaustive\". p8 l233: Change \"pair\" to \"pairs\". Questions: Which anticausal model from Figure 1 is chosen if it is not COLPbased Consider adding horizontal jitter to Figure 2 to enhance the visibility of COLPExhaustive (especially in Scenarios 1 and 3). Why would error bars hinder clarity in Figure 3 Instead of one figure could there be three separate ones for each scenario given the available space In Subsection 3.1.3 what is the strength of the confounder In Subsection 3.2.1 what does RR stand for"], "vQzDYi4dPwM": ["Summary (Paraphrase): Authors investigate neural networks with four hidden layers and threshold activation functions for both interpolation of monotone data and approximation of monotone functions. The number of neurons required for approximation is estimated. Strengths and Weaknesses (Paraphrase): The study demonstrates the significance of network depth in monotone function approximation. While insightful the findings are not practical due to the discontinuity of the threshold activation function and the potential loss of monotonicity in realworld data due to noise. Questions (Paraphrase): 1. The authors should address how their study can accommodate noise in data. 2. Can similar results be obtained using continuous activation functions", "Paraphrase: Summary: If the input of a function increases or decreases the output will also increase or decrease respectively. Monotone ReLU neural networks maintain this structure in data by using only positive weights. Researchers have investigated these monotone neural networks focusing on two aspects: 1. Universality: Can a monotone network with a certain number of layers approximate any monotone dataset (The paper shows that L 4 layers are sufficient but L 2 is not.) 2. Approximation Power: Do monotone networks require a significantly larger number of neurons than standard networks (The paper demonstrates that for specific functions the difference can surpass any polynomial in the number of input neurons.) Strengths and Weaknesses: The paper provides strong approximation theory results for monotone neural networks. However there is a significant gap between the number of neurons required for optimal approximation. The paper only addresses universal approximation but training challenges are not discussed. This suggests that monotone neural networks may not be widely adopted. Prior Research: The problem of monotone approximation has been studied in approximation theory for decades. Researchers have developed techniques such as monotone tensor product regression splines triangulationbased monotone splines and kernel regression. Writing: The paper could benefit from further polishing. Examples of grammatical errors include: \"modeling monotonic relationship\" should be \"modeling the monotonic relationship\" \"requires much larger size\" should be \"requires a much larger size\" \"may result with negative\" should be \"may result in negative\" Questions: The researchers have shown that approximating Lipschitz functions with monotone neural networks requires exponentially more neurons than standard networks. Do they believe this exponential blowup is necessary The paper claims that another study [8] constructed neural networks with comparable neurons. However the paper referenced suggests that O( (Leps)d ) neurons may be sufficient. Can the researchers explain this discrepancy", "Paraphrased Statement: Summary: This research aims to create efficient neural networks that can accurately learn and interpolate monotone datasets (datasets where the output increases or decreases as the input increases). They compare ReLU and threshold activation functions showing that threshold activation is better suited for this task. Results: 2layer monotone neural networks with threshold activation cannot interpolate monotone datasets. 4layer monotone neural networks with threshold activation can interpolate monotone datasets with n points in ddimensional space using O(nd) neurons. Constantdepth monotone neural networks with threshold activation can approximate any monotone function on the [01]d domain. This improves upon previous results that required depth linear in the input dimension. Limitations: The network size required for approximation may increase rapidly if the network parameters are constrained to be positive. Strengths and Weaknesses: The paper provides theoretical proof that 4layer monotone neural networks with threshold activation can interpolate monotone datasets. This leads to the existence of constantdepth networks that can approximate monotone functions. However the tradeoff is an increased network size. Questions: Are these results dependent on the specific choice of activation function If the threshold function is approximated by a sigmoid function in practice would the results remain valid"], "u4ihlSG240n": ["Summary The paper introduces a unified transformer model for ImageLanguage (IL) and VideoLanguage (VL) tasks by leveraging pretraining on both IL and VL data. The model is pretrained separately on IL and VL data allowing for a decoupled approach. Additionally a novel loss function integrates imagetext videotext imagelabel and videolabel information. The models performance is evaluated on multiple benchmarks. Strengths Addresses an important problem. Proposes an effective system with promising results. Wellwritten and clear presentation. Weaknesses Ablation study could be more comprehensive. Missing Citations Antoine et al. \"Thinking Fast and Slow: Efficient TexttoVisual Retrieval with Transformers\" Croitoru et al. \"TeachText: CrossModal Generalized Distillation for TextVideo Retrieval\" Bogolin et al. \"Cross Modal Retrieval with Querybank Normalization\" Questions 1. Data Usage: Provide details on the data used for experiments in Table 9 including the number of samples and use of label annotations. 2. Zeroshot or Finetuning: Clarify if the Table 9 experiments represent a zeroshot setup or finetuning on downstream tasks. If finetuning include results without pretraining. 3. Performance Analysis: Explain why Joint pretraining performs worse than Imageonly pretraining and how img2vid improves performance while Joint does not. Consider including img2vid results in the main paper. 4. Decoupled Learning: Emphasize the significant performance gain from decoupled learning. 5. Pretraining Data Details: Clarify the meaning of \"from the tables.\" Confirm if 14M indicates pretraining on 14M textimage and textvideo pairs. Include results with only 14M samples to assess the impact of video data on performance.", "Paraphrase: Summary Researchers developed a new foundational model for tasks involving images and videos (imagelanguage and videolanguage). The model consists of: A single shared transformer encoder that processes both image and video inputs. A twostep pretraining process that trains the model separately for imagelanguage and videolanguage tasks. A unified loss function that combines supervision from different data sources. The authors claim that their model outperforms existing approaches on a variety of tasks. Strengths The models design addresses a relevant research area (foundation models for imagelanguage and videolanguage tasks). Weaknesses The twostep pretraining process is complex and could be replaced with a more efficient endtoend approach. The evaluation lacks comparisons to several important baseline models such as FLAVA OFA and SimVLM. The authors do not report results on important datasets like ImageNet. The absence of these key comparisons makes the evaluation incomplete. Questions Why is SimVLMs performance reported in one table but not another despite its relevance to both VQA and captioning tasks The authors incorrectly claim that FLAVA is concurrent work (published at the same time). FLAVA was published earlier and should be included in the evaluation.", "Paraphrased Summary: OmniVL: A VisionLanguage Foundation Model OmniVL is a new foundation model that combines imagelanguage and videolanguage pretraining within a single framework. It enables evaluation of tasks based on visiononly multimodal retrieval and multimodal generation (e.g. captioning and VQA). OmniVL achieves competitive or stateoftheart performance on numerous benchmark datasets. Strengths: Unifies objectives task formulations and input data types across multiple modalities. Contributes to the development of multimodal foundation models. Incorporates video data in addition to images. Shows strong results in comprehensive experiments. Decoupled pretraining significantly improves performance. Weaknesses: UniVLC (Unified VisionLanguage Contrastive) loss may not be entirely novel. Question: The reason for OmniVLs poorer performance on the indomain subset of the NoCaps captioning task is not explained."], "yZ_JlZaOCzv": ["Paraphrased Summary: The authors present an adversarial attack targeting AlphaZero. It includes value attacks to mislead the agents value estimate and policy attacks to manipulate its policy output. To improve efficiency they introduce a trick and a bound rule. Adding meaningless stones renders agents vulnerable to these \"adversarial examples\" across various datasets. Strengths: 1. First study on the adversarial vulnerability of Go agents. 2. Wellstructured and understandable paper. 3. Technically sound methodology. Weaknesses: 1. Concerns about the definition of adversarial examples in Go as it differs from traditional definitions emphasizing imperceptibility and robustness tradeoffs. The papers focus on meaningless stones and abnormal agent behavior may not align with classical adversarial examples. 2. Lack of appropriate baseline experiments to evaluate the attack methods performance against existing works on adversarial attacks in reinforcement learning.", "Paraphrased Statement: Summary: This study explores how vulnerable AlphaZero (AZ) agents are to deceptive attacks. It defines an adversarial example for AZ agents as a slightly altered state. The alteration adds a few meaningless actions which are deemed semantically similar to the original state. An adversarial attack on a state is considered successful if it causes the policyvalue neural network (PVNN) to make erroneous predictions. This leads to the agents underperforming in the future. The study proposes an efficient attack method by exploiting the fact that a meaningful action in a state is also meaningful in prior states reducing the search space for meaningless actions. Empirical results demonstrate that PVNNs are vulnerable to adversarial attacks across various AZ agents. Strengths: It is believed to be the first study examining the vulnerability of AZ agents to deception. The study is wellstructured and offers a helpful figure (Fig. 1) for understanding adversarial attacks against AZ agents. The experimental design is solid and the findings are compelling (e.g. results on various datasets and the sturdiness of PVMCTS). Weaknesses: Some criteria for determining state similarity appear to be contrived. The suggested adversarial scenario seems impractical. Despite claiming that the attacked agent will make significant errors (line 19) the study does not offer any quantitative data demonstrating how the proposed attack approach reduces the target agents performance. Grammatical errors include: achieve achieves (line 4) indicate indicates (line 97) denote denotes (line 177) Base Based (line 264) Questions: Why is the term \\max(V(s) 1V(\\mathcalT(s as))) included in the evaluation of ss value Is as the action chosen from the target agents previous strategy or the action chosen after the MCTS The suggested adversarial scenario appears to be unrealistic because it necessitates both players making meaningless moves which is unlikely in practice.", "Summary This study explores the vulnerability of AlphaZerolike algorithms in Go and NoGo to \"semantically invariant\" adversarial attacks. These attacks involve making moves that are legal but nonsensical aimed at disrupting the game without significantly affecting the players chances of winning. The authors define semantic invariance and employ human verifiers to confirm the meaninglessness of moves. They identify such moves within a restricted perturbation set excluding moves that have been meaningful in past turns or fail an attack success criterion. Experiments demonstrate the effectiveness of this method on various datasets and MCTS depths. Strengths and Weaknesses The paper addresses the novel problem of identifying adversarial moves in discrete turnbased games with limited computation. The method is shown to be effective in both Go and NoGo. It is unclear how the method compares to existing approaches and whether it offers significant computational advantages. The paper could be more concise and easy to follow. It is not explained why the authors focused on semantically invariant moves rather than general adversarial moves. Minor Comments The title should be more descriptive of the key findings. The authors should correct grammatical errors throughout the paper. A brief formal definition of Q(\\cdot) should be provided. Line 197 should be clarified regarding the constraints on meaningful moves. The authors should explain why they estimate approximately 150 meaningless moves per state.", "Summary This research aims to assess the resilience of AlphaZero agents trained for games like Go. The challenge lies in defining semantically invariant perturbations for discrete games like Go. To address this the authors propose using action perturbations rather than manipulating states directly. They develop two attack methods: 1STEP and 2STEP which define qualifying perturbed states based on the target agent and a weaker verifier. The authors then propose an efficient method for finding adversarial examples by assuming that a meaningful action for one state remains meaningful for preceding states. Results demonstrate the effectiveness of the proposed attacks against several trained AZ agents under various scenarios. Strengths and Weaknesses Originality: The paper tackles a unique problem not addressed by existing attack methods. Quality: The evaluations use challenging AZ games and the attack success rates are high. Clarity: Certain descriptions in the paper are confusing. Mathematical terms are used without definitions (e.g. state space) and experiment details lack clarity such as the expertise level of human participants. Significance: The findings are relevant to the AZ model specifically as they differ from traditional attacks on continuous state spaces and involve significant computational challenges. The proposed acceleration technique significantly reduces attack computation. Questions 1. State space and L0 distance: Define the games state space and explain how the L0 distance between states is calculated. 2. Semantic equivalence verification: Describe how to verify if two states (s s) are semantically equivalent. Does this verification require human expertise in Go 3. Condition C3: Explain the difficulty of Condition C3 which requires the verifier to identify incorrect states. 4. Impact of single mistakes: Elaborate on why a single mistake can lead to an agent losing the game. Define what constitutes a \"mistake.\" 5. Human participation: Discuss the possibility of eliminating human participation in training the attacker."], "nOw2HiKmvk1": ["Paraphrase: Summary: This paper introduces a novel method for training unbiased classifiers without assigning labels to spurious attributes. Samples are given weights based on a consensus and the primary classifier and committee classifier collaborate. The committee classifier extracts knowledge from samples that are challenging for the primary classifier to categorize called \"distilled knowledge.\" This knowledge is then integrated into the loss function used to train the committee classifier. The selfsupervised representation is obtained using BYOL and serves as the backbone. Strengths and Weaknesses: Leveraging distilled knowledge from the primary classifier to train committee classifiers is a valuable concept. Utilizing a selfsupervised backbone is a wellconsidered approach. Random subsets of the training data may not always be dominated by biasinducing samples particularly in datasets with long tails. Figure 1 does not demonstrate that the committee classifier focuses more on biasconflicting samples. Enhanced enrichment may be due to other factors. Questions: How would representations learned by BYOL on a limited dataset compare to those learned by DINO (base or small VIT) pretrained on ImageNet How are hyperparameters such as learning rate batch size committee size and lambda optimized for each dataset Given that LBC outperforms SSLERM on CelebA but is comparable on ImageNet is this because bias is less prevalent in ImageNets textureheavy images compared to CelebA How can we ensure that the improvement observed in the CelebA dataset is not due to random fluctuations that affect smaller datasets more The sentence \"Therefore we use HairColor as the target attribute and Gender as a spurious attribute the same as HeavyMakeup\" is confusing. The proposed approach should not rely on explicit attribute labels to induce bias. Its unclear how these attributes were selected and how their information was used during training. Corrections: Line 160: Replace \"We identity\" with \"We identify.\"", "Summary The paper presents a new bias mitigation method called LBC that addresses the limitations of previous singlebiased classifier approaches like LfF. LBC overcomes this by training a committee of biased classifiers. Additionally it leverages knowledge distillation from the main classifier. The authors also explore the benefits of selfsupervised representation in bias mitigation. Experiments demonstrate the effectiveness of the proposed method across multiple datasets. Strengths Wellwritten and easy to follow Addresses a clear limitation of LfFs singlebiased classifier Introduces a novel approach with a committee of biased classifiers Explores the role of selfsupervised representation in bias mitigation Weaknesses Missing citations and discussions on other debiasing methods without spurious attribute labels Potential inconsistencies in results for ERM and LfF between tables Ambiguity regarding the split used for CelebA results Lack of clarity on the release of a new split for the BAR dataset Questions Please clarify the differences in ERM and LfF results between Table 1 and Table 2. Which split is used to report the results on CelebA Will the new split of the BAR dataset be released for reproducibility Suggestions Cite and discuss the following debiasing methods without spurious attribute labels: PGI (Ahmed et al. 2021) ARL (Lahoti et al. 2020) George (Sohoni et al. 2020) SD (Pezeshki et al. 2021)", "Summary This study addresses the challenge of classifying biased data when no annotations for the biasing attributes are available. The authors propose a method LBC that leverages an ensemble of classifiers trained on different data subsets (bagging) to identify samples that introduce bias. They generate sample weights for the main classifier based on these ensemble disagreements. Additionally they employ selfsupervision to enhance robustness and complement the reweighting strategy. Strengths Novel use of ensemble disagreements to mitigate bias Welldefined problem setup and motivation for the ensemble approach Simple intuitive and effective method Extensive evaluation on diverse datasets and with various baselines Clear presentation and detailed explanations Weaknesses Claim of being the first to demonstrate the link between selfsupervision and bias reduction is not fully justified Lack of reference to Figure 4 in the main text Absence of discussion on related ensemblebased debiasing methods Lack of theoretical guarantees for the effectiveness of the bootstrapping approach Undefined term \"enrichment\" in Figure 1 Misc. Issues Inconsistent citations for Liu et al. (Just Train Twice) Typographical error on line 225 Ambiguous phrasing in \"e.g. most dog images are appeared \u2018on grass\u2019 the others are appeared on 6 context\" Questions Why is there no stopgradient on the main classifier g Is bagging essential or could random initializations provide sufficient diversity Why does performance decrease beyond 40 ensemble members instead of plateauing How significant is the warmup period and how many iterations are typically needed for stable performance", "Summary: A method is proposed to enhance classifier accuracy and resilience against false correlations. It utilizes a biased ensemble learning approach to identify underrepresented (biasconflicting) examples in the training data. These examples are given higher weights leading to a final classifier that is less biased. Additionally the method involves: Pretraining representations using selfsupervised learning and building shallow models on top of them. Iteratively updating the biased ensemble and the final classifier transferring knowledge between them. The approach demonstrates high performance on several benchmarks including CelebA ImageNet9 ImageNetA BAR and NICO. Strengths and Weaknesses: Originality: The proposed method is novel combining known techniques in a unique way. It is the first to introduce a biased bootstrapped ensemble for biasconflicting example identification. Quality: Concern: The hyperparameter tuning procedure is not fully described including how values were chosen and the impact on biasconflicting example performance. Observation: Selfsupervised learning pretraining improves bias robustness but requires fair comparison with methods that also utilize it. Question: The significance of bootstrapping and the interaction between ensemble size and training data split size need further exploration. Ablation studies: Additional experiments are desired to assess the impact of individual components such as knowledge distillation and the biased ensemble. Generalizability: Evaluation on other data modalities such as NLP tasks would provide insights into the methods wider applicability. Clarity: The paper is clear and easy to follow. It would improve the flow to describe the datasets before discussing the method. Significance: The method significantly improves bias robustness performance on benchmarks. Further evaluation and refinement could enhance its impact within the bias robustness community."], "o8H6h13Avjy": ["Summary Paraphrase: This study introduces a method for enhancing model extraction attacks without assuming prior knowledge of the victims training dataset. The method leverages membership inference attacks (MIAs) to identify members of the victims training set within an adversary data pool. These identified members are then utilized to boost the effectiveness of model extraction. Strengths and Weaknesses Paraphrase: Strengths: Enables model extraction attacks without requiring knowledge of the victims training data. Implements a novel approach using MIAs to enhance model extraction. Weaknesses: The method relies on MIA to infer victim training data which may be unreliable if the adversary data pool does not contain such data. The assumption that the adversary pool contains a significant portion (13) of the victims training data is unrealistic in practical scenarios. The study only compares the proposed method against ActiveThief and fails to evaluate it against other types of model stealing attacks (e.g. PRADA KnockoffNet) in terms of query efficiency.", "Paraphrase: Summary: This study proposes a novel attack method called Poolbased Active Model Extraction (PAME) that aims to recover a model by using a data pool that may not align with the training data the target model was trained on. The attack employs an iterative framework that uses membership inference (MI) to identify training samples within the data pool. This knowledge is then utilized to refine the extracted model through active learning. This process improves both model fidelity and MI performance. Strengths and Weaknesses: Introduces a novel approach to reinforce ME and MI attacks. Adapts shadow model MI attacks to a setting with query budget constraints and different data distributions between the pool and\u8bad\u7ec3\u96c6. Experiments demonstrate MExMIs superiority over existing PAME attacks in terms of model fidelity and MI performance. Concerns: Limited evaluation of attack resilience against defenses for ME and MI. Unclear role of MI in MExMI as it claims not to require training samples in the data pool but still benefits from them. Questions: What is the definition of the \"ideal ME attack\" used in the experiments How can MExMI outperform it What is the mechanism underlying the performance improvement of MExMI when the data pool lacks training samples Is it due to MI or other factors", "Paraphrased Statement: Summary: This study demonstrates that mutual exclusivity (ME) and mutual inclusion (MI) can enhance each other through a repetitive chain reaction. This interaction can substantially increase the precision of ME attacks while reducing query expenses and improving MI. Strengths: Clear paper organization Wellwritten English Weaknesses: Related work should be presented at the outset. Methodology and algorithms are not described. Text in figures is too small. Questions: Why are there numerous references in the paper", "Paraphrased Statement: This paper suggests a new system that combines model extraction (ME) and membership inference attack (MI) methods. These attacks enhance each other during the attack process resulting in stronger ME and MI attacks. Empirical evidence demonstrates that this combined approach improves the accuracy of ME attacks and maintains comparable performance to existing MI attacks that require more queries. Strengths and Weaknesses: The concept of combining ME and MI attacks is innovative and appealing. Empirical findings largely support the claims made in the paper. The paper is wellwritten and straightforward. Concerns: 1. It would be helpful to define a threshold for significant fidelity improvements in practical settings. For instance the paper should explain the significance of the reported fidelity scores in Table 1. 2. The claim that training a shadow MI model requires the same dataset size as the victims training set is questionable. The method can still be applied even if the shadow models training data is a subset of the victims. 3. Its challenging to compare the baseline MI attack [38] and the MExMI approach fairly. Terms like \"on par\" are ambiguous when using individual metrics like precision and recall. Unified metrics like F1 score AUC score or MI accuracy would provide a clearer comparison. Its important to note that the MExMI approach is not expected to outperform existing MI attacks that demand substantially more resources. 4. The baseline methods for both ME and MI attacks might not be comprehensive enough. For example the MI attack [40] is not included for comparison. A thorough comparison against various existing baselines followed by an ablation study on the impact of each MExMI component would be beneficial. Additional Question: Considering the rapid advancements in the field of ME and MI attacks do the baseline methods used in this paper accurately represent the current stateoftheart The selected baselines might be outdated given the high activity in this research area."], "ylila4AYSpV": ["Paraphrase: Summary: This study examines the internet as an auction platform where advertisers possess a large collection of ads with varying values and space requirements. The authors employ the Myersonian approach to analyze allocation rules that exhibit monotonicity both in terms of bid amounts and the set of ads available. They propose a new efficient and monotone allocation rule that approximates the optimal solution within a factor of three. Strengths: 1. The problem under investigation is highly relevant. 2. The study offers theoretical proof for the effectiveness of the proposed rule. 3. Empirical validation of the rule is provided. Weaknesses: 1. The reviewer is unfamiliar with the subject matter and has difficulty understanding the claim in the abstract that despite the proposed allocation rule paired with GSP not being truthful its Price of Anarchy (PoA) is bounded. This seems to suggest a contradiction since the paper presents a monotone allocation rule and proves that such rules paired with appropriate payment rules lead to truthful auctions. It is unclear how these claims reconcile. 2. The uploaded PDF document becomes blurry when zoomed in making it challenging to read for those unfamiliar with the field. Questions: 1. Can the reviewer align their score with other experts in the field if they are confident in the accuracy of the studys claims 2. Could the authors clarify the apparent contradiction regarding the truthfulness of the proposed rule", "Paraphrased Summary The authors present a complex online advertising auction problem that can be categorized as a variant of the MULTICHOICE KNAPSACK problem. If advertisements are allocated based on the \"bang per buck\" principle the allocation may not always be consistent (monotone). It is proven that no deterministic allocation rule that maintains monotonicity can approximate the optimal welfare within a factor of 2. The studys key contribution is a greedy allocation mechanism that approximates at least onethird of the optimal welfare. Strengths and Weaknesses The study addresses multiple variations of the rich ads problem. It solves an unsolved problem posed in [DSYZ10]. Both the positive and negative results are substantial. The proposed mechanism is practical and performs well in experiments. The paper is wellwritten and comprehensive. Questions The reviewer has no questions regarding the study.", "Paraphrased Statement: Summary: The fundamental concept of search advertising auctions is wellestablished but implementing this model for complex ad formats that display multiple ad variations in various sizes and embellishments has proven challenging. This combinatorial issue poses computational and motivational obstacles. This article introduces a straightforward greedy approach based on the optimal fractional algorithm. This strategy forms the basis of a truthful mechanism with verifiable guarantees and strong practical outcomes. Strengths: This paper offers a comprehensive solution to the complex ad problem which has received limited attention in the literature. The main theoretical result (Theorem 3) is convincing and supported by additional results that enhance the articles depth. The analysis extends beyond theoretical bounds to include practical heuristic enhancements. Empirical findings demonstrate the approachs superiority over the theoretically intractable and much slower VCG algorithm. Weaknesses: The articles main section appears somewhat constrained. The paper balances page limitations well but some details are relegated to the appendix. Reproducibility is constrained due to the sensitive nature of advertising data. However some studies have defined models for generating synthetic data for experiments increasing reproducibility. A significant omission is the absence of reserve pricing which is essential in practice. The appendix acknowledges this but a discussion of reserve pricing application would have been desirable. Questions: Given the intractability of VCG its experimental performance (0.03 msec) is surprisingly quick. Is the data used atypical If not why is this VCG performance considered inadequate", "Paraphrased Statement: Summary This paper explores \"Rich Advertising Auctions\" where advertisers can choose to display additional content with their ads making the setting complex. The goal is to design truthful mechanisms that maximize social welfare. While the VCG mechanism achieves both goals its NPcompleteness limits its practicality. Results The paper proves that no deterministic monotone rule can approximate welfare within a factor better than 2 and proposes a greedy allocation rule that guarantees a 3approximation. This rule combines greedy and randomized elements. The Generalized Second Price (GSP) payment rule is not truthful but the price of anarchy is bounded under the assumption of no overbidding. Empirical Evaluation The authors evaluate their mechanism on realworld data. Strengths Results are largely correct Clear and accessible writing Footnotes provide valuable information Weaknesses Some proofs could be moved to a dedicated section for clarity (e.g. definitions of GSP and price of anarchy) Results are not entirely novel and rely on standard techniques May not be considered groundbreaking for the NeurIPS conference Questions Clarification on the application of \"Myersonion payment function\" in the experiments"], "sFQJ0IOkHF": ["Paraphrase This research introduces a method to enhance the overall performance of ensemble techniques within the CASH framework. The method recognizes that a diverse group of base learners generally leads to better ensemble results. The method searches for combinations that not only meet quality criteria but are also distinct from previously evaluated combinations. A surrogate model is utilized to quantify diversity between pairs of configurations. Search objectives for quality and diversity are merged using a weighted ranking system. Experiments demonstrate improved performance compared to established baselines. Strengths Novel approach considering both diversity and performance in CASH Clear and wellstructured presentation Informative experimental results Weaknesses Limited dataset evaluation (fewer than 20k rows per dataset) Lack of comparison with a strong baseline such as AutoGluon Tabular which employs default hyperparameters and multilayer stacking", "Paraphrased Statement Summary: The authors enhance CASH (Configuration Analysis and Selection for Hyperparameter Optimization) by incorporating diversity considerations into optimization leading to better ensembles during posthoc ensemble learning. They modify the Bayesian optimization acquisition function to favor diverse predictors. The method is evaluated against standard CASH automated ensemble learning and AutoML methods on 15 datasets. Strengths and Weaknesses: Strengths: Novel idea with academic and practical relevance Clear method description Weaknesses: Limited practical impact due to small improvement over random search with posthoc ensemble learning Lack of statistical significance testing Insufficient ablation studies to explore the importance of diversity and hyperparameter sensitivity Limited comparison against a simple baseline strategy that promotes diversity in the initial set of models Additional Points: Figure 6 demonstrates increased diversity in the candidate pool but does not assess the impact of diverse models on ensemble performance. Sensitivity analysis is insufficient to determine hyperparameter sensitivity across different datasets. The use of ensembles while potentially effective may face practical limitations (interpretability inference time maintenance). Questions: Is the improvement statistically significant How important are the most diverse models in the final ensemble", "Summary This research aims to enhance the performance of automated machine learning (AutoML) solutions by incorporating diversity among base models in ensemble generation. The authors introduce a notion of diversity and modify a combined algorithm selection and hyperparameter optimization (CASH) solver to consider diversity while maintaining predictive performance. The proposed Diversityaware Bayesian Optimization (DivBO) scheme is shown to generate diverse base models resulting in improved ensemble performance. Strengths Novelty in directly incorporating desired diversity in CASH optimization. Clear recognition of the challenges in balancing diversity and predictive performance. Welldefined diversity acquisition function that weighs diversity and predictive performance effectively. Strong experimental results demonstrating DivBOs ability to enhance ensemble performance. Weaknesses Lack of discussion on why the specific diversity definition leads to improved performance. Unclear whether diversity plays a role in ensemble selection given the greedy selection method used. Limited significance of DivBOs improvement over some baselines requires further analysis. Uncertain contribution of diversity to ensemble performance due to potential exclusion of diverse configurations from the ensemble. Questions What are the performance differences between DivBO with and without the weight schedule Why is a mean over minimumofpertheta samples used instead of a minimum of pertheta sample means in the diversity acquisition function Are DivBO and BO truly identical when w0 is fixed throughout the optimization What are the reasons for the limited correlation between true and predicted diversity and the potential challenges in improving it How are class probabilities generated for discriminative models that do not inherently produce them (e.g. SVC) Why do the different schemes in Figure 6 have varying numbers of base learners potentially affecting the comparisons", "Paraphrased Statement: Summary: This method optimizes ensemble construction by combining a diversity surrogate function with a performance measure and maximizing the result. It improves upon existing ensemble methods as shown by experimental results. Strengths: Enhances hyperparameter optimization for ensemble learning. Wellwritten and structured. Thorough experiments. Weaknesses: Incremental contributions may not be significant for publication at Neurips. Small performance improvement over existing methods (RBES). Questions: Is the performance improvement substantial enough for publication at Neurips The results may show overfitting since validation errors are used. Should testing errors be used instead How many points are sampled per objective function Consider modifying the section titles to reflect the evaluation focus (\"Evaluation of DiVBO Diversity Surrogate\" and \"Evaluation of DiVBO\")."], "kK200QKfvjB": ["Paraphrase: This research investigates the minimum loss of neural networks (DNNs) with L2 regularization and full connectivity. It demonstrates that the loss function based on the network parameters can be rewritten as a function of the layerwise activations of the training data resulting in a problem with attractive and repulsive forces. Furthermore for nonlinearities that exhibit positive homogeneity the loss can be further transformed into a function of the covariances of the hidden representations. This takes the form of a partially convex optimization problem on a convex cone. Strengths: The novel reformulations of the loss function for L2regularized DNNs provide new insights into the behavior of deep networks. The second reformulation suggests implications for the sparsity of local minima in homogeneous DNNs. Weaknesses: The theoretical results are limited to the minima of L2regularized DNNs assuming that such minima can be achieved and perfectly interpolate the training data. Questions: Typographical errors: Line 124: \\mathcalLr\\lambda should be \\mathcalL\\lambda Line 130: \\mathcalL\\lambda(Z1 \\dots ZL) should be \\mathcalLr\\lambda(Z1 \\dots ZL)", "Paraphrase: Summary: This paper presents two alternative ways of understanding L2regularized deep neural networks (DNNs) in terms of representation costs. The first approach describes feature learning as a combination of attraction and repulsion forces linking DNN training with optimization over a translated convex cone. The second approach analyzes covariance learning to explain how L2 regularization promotes sparsity in DNNs with uniform nonlinearities. Strengths: 1. The reformulations provide valuable insights into L2 regularizations effects in DNN training. 2. The second reformulation demonstrates an interesting sparsity result for homogeneous DNNs (Proposition 7). Weaknesses: 1. The paper lacks discussion of the implications of the insights gained. 2. The first reformulation suggests a potential block coordinate descent algorithm but it remains unclear how to handle interpolation requirements. 3. While Proposition 5 establishes equivalence with a partially convex problem its implications and benefits are not explored. Questions: 1. For the second reformulation when can we expect to observe the sparsity plateau 2. Can we gain insights into how early the sparsity plateau might occur particularly since it has been observed in many datasets", "Paraphrased Summary: Researchers investigated training deep neural networks with weight matrix \\(\\ell2\\) regularization. They reformulated the cost function in two ways to gain insights into the learned features at optimal solutions. Reformulation 1: Introduced orthogonal projection to express network weights in terms of preactivations. Interpreted the resulting problem as local interactions between neighboring preactivations within layers. Reformulation 2: Replaced preactivations with covariance matrices. Interpreted the resulting cost as rank constraints enforcing sparsity of optimal features when layer widths exceed sample size. Strengths: The first reformulation provides a novel framework for understanding network weight optimizers. The focus on deep networks is significant as theoretical works often limit analysis to shallow networks. Weaknesses: The reformulations are not computationally viable due to scaling issues with data sample size. The correspondence between the objective landscapes of the original and reformulated problems is unclear. The analysis assumes no specific structural assumptions on input or target data. Minor Issues: Typographical errors in Proposition 1 proof. Unclear interpretation of fractional powers in Proposition 3. Questions: Justification for \\(\\Psi\\) being normpreserving. Correspondence between minimizers in Proposition 1 without normpreserving justification. Interpretation of plots in Figure 1. Theoretical implications for less overparameterized networks."], "vjKIKdXijK": ["Summary: The authors propose a new method for verifying the convexity of functions based on computational graphs. Currently convexity is often checked manually or using tools like CVX which have limitations. The authors approach involves verifying positive definiteness of Hessians using simple algebraic rules and it can encompass and improve upon CVX. Strengths: It provides a useful tool for complex modeling where convexity can be a criterion for model selection. It eliminates the need for manual additions to base functions in CVX. It offers a promising path for developing a comprehensive framework for proving function convexity. Weaknesses: The approach lacks a formal framework making it difficult to build upon. Some definitions are unclear such as \"normalized expression DAG\". The authors could provide more background and pseudocode to aid in understanding and implementation. Questions: What is a normalized expression DAG What is a normalized vectorized form Where is the language for representing multivariate functions formally described What does the factor line mean Could the authors provide a concrete example motivated by real applications", "Summary: This paper describes a new method for verifying the convexity of functions using the Hessian approach. This method complements existing disciplined convex programming approaches and can handle nondifferentiable functions. Strengths: Clearly written and wellpresented Implemented using a formal language that supports a wide range of problems Anonymized implementation demonstrates the approachs potential Uses computational graphs and rule sets for solid implementation Weaknesses: Reviewer is not an expert in computational frameworks for convex optimization and cannot fully assess the papers technical merits.", "Paraphrased Statement: This research introduces a method called the \"Hessian approach\" to verify the convexity of functions that change continuously. It claims to enhance the popular \"disciplined convex programming (DCP)\" approach. Examples demonstrate the effectiveness of this new approach even surpassing DCP in certain cases (e.g. Section 7). Strengths: The paper is wellwritten and presents its core concept clearly. The analysis is sound and highlights the approachs unique features. An anonymized implementation is available online. Weaknesses: The computational efficiency of the Hessian approach is unclear. Computing and verifying the positive definiteness of the Hessian for the entire domain can be computationally expensive. The implementation appears suitable only for lowdimensional or simplified problems. It is not explicitly stated why the Hessian approach is superior to the wellestablished DCP approach. DCP is more flexible requires less computation (without complex Hessian computations) and can solve convex optimization problems using conic formulations. The Hessian approach seems limited to a broader notion of convexity verification. Proposition 1 translates DCP rules for differential functions into an alternative form. However the differences in computational graphs (DAGs) compared to DCP are not explained clearly. It is unclear how DAGs would handle a logarithmic determinant function (e.g. f(X) log det(X)). Questions: Provide more details on the computational properties of the Hessian approach."], "pELM0QgWIjn": ["Paraphrased Statement: This study applies a greedy quasiNewton technique to address saddle point issues. The suggested techniques rely on approximating the indefinite Hessian matrix squared. They exhibit local superlinear convergence. Strengths and Weaknesses: The study is organized well. The key to the suggested method is emphasized as the estimation of the squared Hessian. More details on memoryefficient storage of the approximate Hessian would be appreciated. However if all elements can fit in memory and are calculable the proposed algorithm is theoretically and practically efficient. The proposed algorithms demonstrate strong performance in the experiments presented. Questions: The authors mention two stages of convergence behavior. It would be helpful if this characteristic could be explored further in the experimental section.", "Paraphrase: Summary: This paper introduces a new algorithm for solving saddle point problems that are either strongly convex or strongly concave. The algorithm updates the square of the Hessian matrix rather than the Hessian itself. This approach allows for a faster convergence rate compared to classical firstorder methods. Strengths and Weaknesses: Strength: The algorithm avoids the difficulties of working with an indefinite Hessian which is common in saddle point problems. Weakness: The papers presentation is dense and lacks motivation for the introduction of notations and lemmas. It could be improved by clearly presenting the main results and then providing justifications for the necessary lemmas. Questions: What are the novel contributions of this paper compared to previous work What are the key technical challenges that the paper addresses beyond the issue of an indefinite Hessian", "Paraphrased Statement: Summary: This study focuses on using quasiNewton methods to solve saddle point problems with strong convexity and strong concavity. The authors establish the first explicit local superlinear convergence rates for quasiNewton methods using Broyden updates. They leverage a technique that approximates the square of the indefinite Hessian matrix rather than the Hessian itself. They also achieve nonasymptotic convergence rates for the BFGS and SR1 methods. Numerical experiments support the theoretical findings. Strengths: Originality: This paper offers the first explicit local superlinear convergence rates for quasiNewton algorithms applied to saddle point problems. Quality: The authors provide thorough proofs and consistent empirical results that validate the theoretical conclusions. Clarity: The paper is wellstructured and easy to follow. Weaknesses: Significance: The papers contributions are incremental building upon previous work. It extends existing superlinear convergence results for quasiNewton methods in general convex optimization to the setting of strongly convexstrongly concave saddle point problems. Minor Weaknesses: The authors should use t as the time index instead of k to avoid confusion with the condition number notation. They should use d for dimension instead of n as n is often used to denote the number of functions in the objective function. It would be clearer to use a different constant for the Hessian Lipschitz parameter than L2. Question: The authors do not present specific results for the DFP method. Is this because its theoretical performance is inferior to that of BFGS rendering its inclusion unnecessary", "Paraphrased Statement: The researchers offer improvements to QuasiNewton techniques for solving strongly convexstrongly concave saddlepoint problems with Lipschitz gradients and Hessians. The key innovation is modifying the matrix used for inversion. Unlike QuasiNewton methods for minimization which approximate the Hessian inverse the proposed methods estimate the Hessian squared. This modification enhances the methods analyzability. Similar to minimization [references] the researchers establish twoperiod local convergence results (one with linear convergence one with superlinear convergence) for the general random Broyden family with improved rates for BFGS and SR1. These rates resemble those known for minimization except for the substitution of \\kappa (condition number) with \\kappa2. This deterioration is anticipated due to the methods use of the approximate Hessian square inverse. Strengths: Novelty and Significance: The findings are novel with only two prior studies proposing QuasiNewton methods for minmax problems (for nonlinear equations) using distinct settings and concepts. The research is applicable to the machine learning field and the findings are substantial. Clarity: The results are presented clearly and the detailed proofs are simple to follow. Weaknesses: Condition Number Dependence: The derived results exhibit a squared dependence on the condition number \\kappa. While the proposed approach of approximating the Hessian matrix squared is intriguing a discussion on mitigating this dependence would be valuable. Incomplete Comparison with [46] and [23]: Although the authors compare results and assumptions with concurrent works [23 46] they omit an explicit rate comparison. Such a comparison should be included in the text. Additionally comparing the proposed methods with the method from [23] in experiments would be beneficial. Questions: How do the initial conditions from Theorem 3.17 differ from those known for similar methods in minimization How do they compare to those in [23] and [46] Line 582: Change \"object\" to \"objective.\" Line 587: Change \"continuous\" to \"continuity\" and remove \"that.\" Line 626: The inequality should be \\frac\\betaM \\leq \\frac14L. This is sufficient because (56) lacks the factor \\mu before \\frac\\beta2M\\rhok2. Inequality (63): How was the last inequality derived"], "sde_7ZzGXOE": ["Summary Recent advances in reliable AI highlight the importance of designing intelligent machine learning systems that can recognize their limitations. This paper focuses on OOD (outofdistribution) detection a critical aspect for reliable AI systems addressing the challenge of identifying data that the system should not know. The authors investigate the learnability of OOD detection in a practical scenario where OOD data is unseen during training. Theoretical Contributions: Demonstrates that OOD detection is not generally learnable but identifies a necessary condition for learnability leading to numerous sufficient and necessary conditions. Proves that in certain scenarios involving finite indistribution domains OOD detection is learnable a significant advancement for practical applications. Practical Significance: Provides guidance for practitioners by establishing the boundaries of OOD detection learnability indicating when such methods are applicable. Offers a theoretical basis for designing OOD detection methods addressing concerns about the reliability of existing approaches. Strengths: Pioneer research on the learnability of OOD detection filling a major gap in the field. Practical theorems applicable to realworld scenarios with finite indistribution data. Provides a necessary condition that inspires further research in OOD detection. Distinguishes OOD detection from related concepts like PQ learning and classification with reject option. Weaknesses: The difference between OOD detection and PQ learning should be discussed further. A deeper comparison of OOD detection with classification with reject option would be beneficial. Clarification is needed regarding Figure 1 specifically the solid and dashed lines and their relationship to overlap and learnability. Brief proofs or explanations would enhance understanding of the theorems. Redundant citations can be eliminated. Densitybased space and its applications (theorems 9 and 11) should be elaborated upon. The mathematical expression in Definition 1 should be clarified and its equivalence to the standard PAC learnability definition should be briefly demonstrated. Questions: Please revise your paper in accordance with the questions raised in weaknesses 1 2 3 4 7 8.", "Paraphrased Statement: Summary: This study investigates the theoretical basis for learning outofdistribution (OOD) detection. Using PAC learning theory the paper proves the impossibility of learning OOD detection under certain conditions and identifies scenarios where it is PAClearnable. The paper applies the theory to realworld cases using factorial convolutional networks (FCNNs) and OOD scores as examples. Despite the abundance of empirical methods proposed for OOD detection theoretical exploration has been limited. This paper addresses this gap by providing the first comprehensive theoretical investigation of OOD detection. Strengths and Weaknesses: Strengths: Clear and wellorganized presentation Accurate proofs Pioneering exploration of OOD detection theory contributing to the field. Theory offers practical insights and can guide OOD detection algorithm design. Weaknesses: Refinements in notation and explanations could improve clarity. Typographical errors in Section 2. Expansion of experiments to demonstrate the theorems validity would be beneficial. Practical implications may be limited. Questions: What does \"\" represent in the equation (line 82): DX (1\\piout) DX1 \\piout DXO", "Summary In outofdistribution (OOD) detection the goal is to classify test samples as OOD if they originate from outside a known joint distribution (ID). However if a test sample comes from the ID the classifier must still correctly predict its label. This paper explores the agnostic PAC learnability of OOD detection in various scenarios. Key concepts are defined as extensions of supervised learning learnability. Due to the imbalance problem in OOD detection the authors propose focusing on agnostic PAC learnability in \"priorunknown\" spaces. The paper establishes a necessary condition (Condition 1) that cannot be met in total or separate spaces. This proves that OOD detection is not learnable in these settings. However the paper demonstrates that separate space detection can be learnable if the hypothesis space includes almost all classifiers. For finite ID distributions a new condition (Condition 3) is sufficient and necessary for learnability. In densitybased spaces OOD detection is learnable under a realizability assumption. The paper analyzes practical hypothesis spaces such as FCNNbased and scorebased models. It shows that OOD detection is learnable in FCNNbased or scorebased spaces only if the feature space is finite. In summary the paper provides insights into when and how OOD detection can be applied in realworld scenarios and offers guidance for designing OOD detection algorithms. Strengths Relevance to the field Sound proofs Practical assumptions that can be satisfied by many practical cases Theoretical insights into the conditions under which OOD detection works Weaknesses Long and complex appendix Slight discrepancy between Theorem 4 descriptions in the main text and appendix Removal of Condition 2 from Theorems 7 and 10 could be considered", "Summary (Paraphrase): This research addresses the theoretical foundations of outofdistribution (OOD) detection a classification task where test data may originate from classes not seen during training. The goal is to correctly label data from known classes or identify OOD data belonging to unseen classes. The authors present a series of theorems that define conditions for OOD detection in various practical scenarios. While these results suggest that finding a universal OOD detection algorithm is unlikely they still provide guidance for developing algorithms for specific scenarios. Strengths: Provides a rigorous theoretical foundation for OOD detection a crucial machine learning task. Wellwritten and accessible despite its technical nature with proofs provided in supplemental material. Considers scenarios that are both practical and relevant to realworld OOD detection methods. Weaknesses: Focus on negative results (impossibility of OOD detection in general cases) without providing concrete algorithms. Questions: Line 118: The authors state that Eq. (2) implies the standard PAClearnability by Markovs inequality. Can they provide a proof or reference for the converse Line 260261: The authors mention \"finite ID datasets\" in the context of algorithm design. Would they clarify whether this refers to finite sample datasets or a finite variety of datasets Theorem 3: The conditions seem similar to the realizability assumption and compatibility condition in Theorems 8 and 9. Should these conditions be considered beneficial or detrimental Terminology: The paper uses the term \"domains\" to refer to distributions. Is this a common convention and if so can the authors provide a rationale for the choice"], "osPA8Bs4MJB": ["Summary The paper introduces a novel method for DeepFake detection. Instead of embedding the spatiotemporal data with 3D convolutions and then using Transformers it directly splits the image into patches and adds the temporal dimension. Unlike prior methods temporal modeling occurs directly over patches using Transformers earlier in the pipeline. The method LTTD uses a combination of 3D convolutions and Transformers. It is supervised with crossentropy loss and a similarity loss to enforce consistency among real patches. Experimental results on recent DeepFake detection benchmarks show promising performance along with ablation studies to validate design choices. Strengths Innovative method that diverges from the stateoftheart by directly modeling patch sequences with Transformers early in the pipeline. Experimental validation provides encouraging results and demonstrates robustness to common perturbations. Wellsupported claims in the introduction match the experimental evidence particularly in terms of robustness. Weaknesses The average improvement over the competing method is marginal (89.6 vs. 91.8) and may not be statistically significant. The overcomplexity of the method raises concerns about reproducibility and reimplementation. Despite the methods aim to model temporal discrepancies with patches it still utilizes 3D convolutions which may not fully account for facial deformations. The methods face cropping and alignment procedure lacks clarity. The notation is often complex and difficult to follow making the article challenging to read. The paper could explore the impact of varying temporal dimensions and sampling strategies on performance. Certain typos and odd word usage could be corrected. Justification of Rating Overall the paper presents a good system paper for DeepFake detection with marginal improvement over the stateoftheart. The claims are supported by experimental results but concerns remain about the methods complexity and reproducibility. While it offers valuable insights it falls short of delivering truly groundbreaking ideas. Therefore a borderline reject score is recommended before rebuttal. Questions Provide an average in Table 3 for improved clarity. Disclose details on training and testing time. Specify the parameters of the 3D convolution (kernel size in spatial and temporal dimensions). Clarify the rationale behind the loss function and how it enforces real patch similarity. Explain the process of face region cropping and alignment in detail.", "Paraphrased Summary: The proposed method combines 3D convolutional operations and a vision transformer to identify forged artifacts in Deepfake videos. Experimental results demonstrate the methods excellent generalization performance across various Deepfake datasets. Strengths and Weaknesses: Strengths: Exceptional overall performance Weaknesses: Manuscript writing requires improvement Questions: 1. The authors assert that their model avoids semantic modeling of facial features but lack supporting evidence. 2. Equations 4 and 8 use inconsistent formats and unclear notation hindering comprehension. 3. Equation 12 has a possible error where m should be equal to Flatten(m\u03b2). 4. Temporal dimension T is set to 16 without justification or discussion of frame selection strategy. 5. The proposed method has not been evaluated for generalizability against the method presented in Shiohara and Yamasaki (2022).", "Summary: This paper introduces the Local Temporalaware Transformerbased Deepfake Detection (LTTD) framework designed to detect deepfakes with high generalizability across different methods and robustness against subsequent image processing. The framework employs lowlevel temporal learning and prevents dependency on global semantic cues enhancing its reliability. Strengths: Addresses the important challenges of deepfake detection: generalizability and robustness. Focuses on lowlevel temporal learning to extract subtle temporal variations. Conducted comprehensive experiments to evaluate effectiveness. Weaknesses: Random face region cropping and clip range determination could impact practicality in realworld scenarios with varying video content. Details on clip selection are not fully disclosed potentially affecting the results. Limited information on computing complexity and inference efficiency. Ground truth similarity matrix and mask sequence generation methods are not clearly explained. Questions: How would the random face cropping and clip range determination affect the detection accuracy in realworld videos What is the optimal temporal dimension for the method How computationally efficient is the LTTD framework during inference How are the ground truth similarity matrix and mask sequence generated especially for fake clips", "Summary Paraphrase: This study presents the Local and Temporalaware Transformerbased Deepfake Detection (LTTD) framework which extracts temporal cues from localized sequences. It employs a Local Sequence Transformer (LST) to model temporal consistency within limited spatial regions capturing local and global features. The LTTD framework demonstrates exceptional adaptability and resilience. Strengths and Weaknesses: Strengths: Method is clearly explained. Generalizes well to unseen deepfakes and is robust against postprocessing. Weaknesses: Novelty is limited. Analyses are lacking. Questions: 1. Novelty: What motivates the LTTD framework Does it address specific shortcomings of earlier methods The authors should emphasize its innovative aspects and advantages. 2. Analyses: The Lowlevel Enhancement uses shallow 3D convolutions. Is their primary contribution to feature learning Can 2D convolutions achieve similar performance gains with reduced computational expense LTTD focuses on lowlevel temporal patterns in limited spatial regions. How are these regions defined Is restricting specific regions necessary What differences emerge between restricted and unrestricted regions Table 3 shows that ViT underperforms compared to the Xception baseline. Why is this the case How do ViTbased methods like FTCN and LTTD improve ViT performance in deepfake detection What are the differences in model complexity and GFLOPs between LipForensics FTCN and LTTD Localization: Can LTTD identify forged regions Visualization of such localization would be valuable.", "Paraphrased Summary The authors present a framework to enhance the generalization and robustness of deepfake detection. It incorporates local lowlevel and temporal information using a transformerbased model. Specifically the Local Sequence Transformer (LST) detects temporal inconsistencies at a low level while the CrossPatch Inconsistency loss (CPI) captures spatial inconsistencies. The Global Contrastive Classification (GCC) utilizes temporal tokens and additional transformer blocks for final classification. Extensive experiments demonstrate the frameworks superiority in generalization and robustness evaluations compared to previous methods. Strengths: Clear and accessible writing Effective integration of temporal and lowlevel features Novel LST module for identifying temporal discrepancies Welldesigned CPI module for capturing spatial inconsistencies Robust evaluation benchmarks Weaknesses: Potential redundancy of face alignment in the LST module Lack of exploration of interpolation methods in calculating simgt Absence of intradataset evaluation Limited robustness evaluation set Lack of ablation study for robustness evaluation Questions and Confusion: Binary labels vs. masks in supervision information Significance of the CPI module Potential benefits of exploring midlevel features Strengthening the robustness evaluation"], "kb33f8J83c": ["Summary The paper introduces a textguided image editing technique for StyleGAN an AI model for generating images. The approach employs an attention mechanism to enhance the alignment between the text understanding model (CLIP) and StyleGANs latent code space. By predicting the importance of different layers and channels for specific text attributes the model can modify images more precisely and disentangled. Strengths Introduces a clear and effective attentionbased approach for textguided StyleGAN editing. Achieves better results compared to existing methods enabling more precise and complex manipulations. Facilitates the use of a wider range of texts for editing including combinations of unseen texts. Weaknesses Clarity: The papers writing needs improvement for better readability especially in the abstract and introduction. Justification and Verification: The paper makes claims but lacks experimental support within the main text. The supplementary material provides some evidence but its not referenced in the core text. Minor Errors: The paper incorrectly states that StyleGAN inversion is performed in W space for editing when its actually done in W space for identity preservation. The authors imply that StyleCLIP requires a separate model for each text prompt which is only true for their specific latent mapper approach. Questions Generalization: Can the model change hair color using texts that were not used in training (e.g. from \"blond hair\" to \"red hair\") Hair Color Failure: Why does StyleCLIP struggle with hair color changes when other models perform well FineLayer Editing: Are finelayers disabled in the models training as this could explain the hair color issues", "Paraphrase: Summary: This paper introduces FFCLIP an image editing technique that uses text prompts to modify image semantics. To ensure alignment between text and image semantics in the latent space of StyleGAN a semantic modulation method is used. This method aligns visual representations with text embeddings through linear transformations with parameters derived from cross attention computations. This alignment allows a single model to edit images based on various text prompts with different meanings. Experiments demonstrate the models effectiveness in editing images from various datasets using a single model. Strengths and Weaknesses: Strengths: Effective alignment of text embeddings with the StyleGAN latent space. Enables diverse text semantics to be reflected in visual representations for manipulation. Semantic modulation aligns semantics first then injects text prompt semantics which is suitable for nondisentangled latent spaces like StyleGAN. Weaknesses: Comparison with similar methods (TediGAN StyleCLIP HairCLIP) could further highlight the contributions of FFCLIP. A discussion of diffusion models would emphasize the significance of editing StyleGANs latent space for future research. Questions: Explore the weaknesses mentioned above in the discussion section. Provide more indepth comparisons and analysis with previous methods and diffusion models.", "Summary Paraphrase: This study presents FFCLIP a novel architecture that combines CLIP and StyleGAN for image modification. FFCLIP uses semantic alignment and injection blocks to identify and embed textbased semantics into the latent space of the GAN. Crossattention ensures semantic alignment while an injection technique from HairCLIP is utilized to incorporate text semantics. Visual demonstrations and human evaluations show that FFCLIP outperforms StyleCLIP in image transformation quality. Strengths and Weaknesses Paraphrase: Strengths: Explores the innovative application of CLIP for textguided GAN image modification. Introduces a novel alignment and injection architecture for semantics embedding. Visual examples and human evaluations demonstrate the effectiveness of FFCLIP. Weaknesses: Similarity to existing CLIPbased image modification systems such as StyleCLIP. Insufficient evaluation robustness limiting the significance of the proposed improvements. Grammatical errors in the writing. Questions: Does the size of the CLIP model used impact the effectiveness of FFCLIP"], "q41xK9Bunq1": ["Summary: This paper introduces a new neural architecture called Neural Architecture Creator (NAC) that learns both the optimal connections (called signatures) and settings (called codes) for its modular building blocks (ModFCFFN). Compared to Perceiver IO NAC seems to have fewer parameters and handles data more efficiently. It performs well in adapting to new tasks with limited examples (CUBCIFAR) and generalizing to unseen data (TinyImageNet). It also produces similar connection patterns for similar input. Strengths: Unique architectural features (sparse graph priors) in ModFC compared to Perceiver IO Improved performance (linear scaling with input size and speed boost) over Neural Interpreters Clear and concise writing with helpful figures Weaknesses: Limited empirical validation: only tested in fewshot adaptation and OOD generalization on computer vision datasets needs a wider range of comparisons and baselines Insufficient evidence to claim NAC as a truly generalpurpose architecture: uses preprocessing rendering its ability to handle raw inputs questionable Questions: Can NAC work with raw inputs like Perceiver IO In Table 1 does \"Perceiver IO ModFC\" represent NAC without minimizing Eq 9 How does NACs sparse connectivity enhance its OOD generalization capabilities", "Paraphrased Statement: Summary: This paper introduces a novel neural architecture called Neural Attentive Circuits (NAC) which is designed for generalpurpose tasks. In NAC information flows through modules that perform specific computations based on given conditions. The circuit generator determines the routing of information and the conditions for computations while the circuit executor processes the information. NAC offers different variations for the circuit generator tailored to different applications. NACs performance is compared to PerceiverIO a generalpurpose architecture that served as a foundation for NACs development. While NAC may not surpass PerceiverIOs performance on the ImageNet validation set it excels in OOD (outofdistribution) generalization and fewshot adaptation. Pruning certain modules significantly reduces computational costs without compromising accuracy. Removing some components in the model pinpoints the significance of key design features such as conditioned computations modularity and sparsity priors. Additionally using data conditioning in the circuit generator enhances performance on visual reasoning tasks where similar instructions lead to similar connectivity patterns. Strengths: Proposes a novel approach to modularize neural networks. Provides a flexible and efficient architecture design that allows for adjustments in module connectivity and modulation. Reduces computational costs by pruning modules without compromising accuracy. Embedded priors enhance learning in datascarce conditions OOD robustness and fewshot adaptation. Module connectivity offers insights into how the model processes information. Weaknesses: Demonstrated usefulness only on visual data despite being labeled as generalpurpose. Performance on ImageNet validation set falls short of PerceiverIO. The module state (\\thetaiu) may be misleading as it may not represent the overall models state. The need to share module weights across layers may limit the models flexibility. Unclear how vectors in the latent array are processed by modules. Lack of details on the circuit generators architecture and how CLIP embeddings are integrated. If PIO and NAC achieve similar accuracy on NLVR2 NAC may not offer significant improvements. Questions: Provide further clarity on the models design and experiments. Explore using dynamic signatures and codes while sharing weights across layers. Investigate prior research on similar model designs.", "Summary This research introduces Neural Attentive Circuits (NAC) a model that learns the internal connectivity and parameters of modules simultaneously. This architecture builds upon PercieverIO which serves as a baseline for their experiments. They introduce a Sparse Kernel Matrix Dynamic Parameterization Attentive (SKMDPA) mechanism that uses the Concrete distribution (Gumbel Softmax Distribution) and two modules (ModFC and ModFFN) that modulate their computation based on an input code. These modules control the attention connectivity and their own calculations using the output of the Circuit Generator which can be inputconditional or unconditional. The inputunconditional version of their work is tested on fewshot image classification tasks using the CIFAR and CUB datasets surpassing the baseline PercieverIO network in performance. Ablation studies on the Tiny ImageNet dataset reveal that simply incorporating their modules and adaptable sparse connectivity significantly enhances outofdistribution results but has minimal impact on indistribution performance. Additionally adding various graphconnectivity priors to the learned connectivity further improves performance. They also explored a sampleconditioned circuit design generation approach trained on the Natural Language for Visual Reasoning for Real dataset using the CLIP encoder to preprocess both text and images. The circuit generator is only trained on the input text while only the image is provided to the network executor. They demonstrate using a tSNE visualization of the connectivity graphs produced by the circuit generator that there is a distinct grouping of graph connectivity based on the type of question posed. Strengths Proposes a novel architecture for jointly learning both the sparse connectivity between modules and the models parameters. Ablation studies show that the learned nonsampleconditioned sparse connectivity boosts outofdistribution accuracy on TinyImageNet while graph priors enhance both in and outofdistribution accuracy. Surpasses PercieverIO on fewshot adaptation. Demonstrates via visualization that the model can learn various paths for different problem types on the Natural Language for Visual Reasoning for Real dataset. Shows that the method can effectively sparsify the architecture achieving comparable performance to PercieverIO with 80 fewer circuits. This leads to significant realtime inference speedup. Weaknesses Fails to demonstrate the architecture as a generalpurpose tool after presenting it as such. The text modality results use CLIP embeddings. Weak empirical results. Lack of strong baselines for the nonsampledependent results. Closely resembles existing work on architecture search and sparsification without referencing these prior efforts. No comparisons on the same tasks as the original PercieverIO paper. The sparsification results appear promising but comparisons with alternative sparsification techniques or Neural Architecture Search methods are lacking. The unconditional circuit generator work overlaps significantly with prior research in NAS and sparsification and comparing it to these works would strengthen the study. The conditional circuit generator is quite innovative particularly when applied to multimodal data but the performance and characteristics of the network could be explored in greater depth. Elaboration is needed on the ModFNN properties: \"unlike in transformers each copy is conditioned by a unique learnable code vector and therefore performs a different computation. Consequently though the computation performed in each module is different the total number of parameters does not noticeably increase with the number of modules.\" The accuracy and performance could be discussed in section 4.2 along with baselines. Regarding the statement \"We observe that the Neural Attentive Circuit (NAC) is much more robust than Perceiver IO to sparsification at inference time\" please provide results with PercieverIO sparsification baselines. For Figure 5b please provide a baseline for the compute cost and inference speed of Perciever IO. It may be useful to compare with [5]. The results with the metanetwork inference such as in section 6.3 bear similarities to the work with an Unconditional Circuit Generator. Unrelated to the paper quality: Could this architecture be beneficial for continual learning Other methods in continual learning also aim to dynamically adjust the network to partition off tasks and prevent catastrophic forgetting. References: [1] Cai Shaofeng Yao Shu and Wei Wang. \"Dynamic routing networks.\" Proceedings of the IEEECVF Winter Conference on Applications of Computer Vision. 2021. [2] Xie Sirui et al. \"SNAS stochastic neural architecture search.\" International Conference on Learning Representations. 2018. [3] Correia Gon\u00e7alo M. Vlad Niculae and Andr\u00e9 FT Martins. \"Adaptively Sparse Transformers.\" Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP). 2019. [4] Yoon Jaehong et al. \"Lifelong Learning with Dynamically Expandable Networks.\" International Conference on Learning Representations. 2018. [5] Shaw Albert et al. \"Meta architecture search.\" Advances in Neural Information Processing Systems 32 (2019)."], "o762mMj4XK": ["Paraphrased Summary: This paper presents a method to improve the accuracy of neural ratio estimators by enforcing a \"balancing condition\" on the classifier to make it more conservative. Strengths: Ensures validity of posteriors derived from deep learning algorithms. Uses an additional balancing condition to regularize the model. Simple to implement. Experiments show that the method produces more conservative posteriors. The bias and variance of the model decrease as the sample size increases. The authors acknowledge the tradeoffs and limitations of the method. Weaknesses: The regularization term can lead to worse calibration especially with low sample sizes. While conservative posteriors are generally preferable overly conservative posteriors may hinder accurate parameter identification. The discussion around equation (6) lacks clarity regarding the difference between probability density functions. The model details could be expanded especially regarding the stochastic versus deterministic nature of the underlying processes. The authors do not acknowledge related work on model (over)confidence in statistics such as power posteriors. The claim that the method is an \"improved\" version of a previous approach is misleading it is a variant with different advantages and disadvantages. The specific reference for the MG1 model should be corrected.", "Paraphrase: Summary: Bayesian inference can be used to estimate parameters in simulationbased models even when evaluating the likelihood efficiently is not possible. Neural networkbased methods have been developed to estimate the posterior probability distribution likelihood or likelihood ratio enabling MCMC sampling for posterior inference. However these methods may produce inaccurate posteriors particularly with limited simulation data. Balanced NRE (BNRE): This study introduces BNRE a variant of neural likelihood ratio estimation (NRE) that aims to produce more conservative posterior estimates. BNRE trains a classifier to approximate the likelihoodtoevidence ratio similar to NRE. However it introduces the concept of a \"balanced classifier\" and modifies the loss function to promote balance. It is shown that the optimal Bayesian classifier is balanced and BNRE converges to NRE with unlimited data. Evaluation: Experiments using tractable benchmark problems demonstrate that BNRE tends to produce more conservative posteriors than NRE with limited simulation budgets resulting in larger posterior variance. The authors conclude that BNRE is a suitable method for obtaining conservative posterior estimates. Strengths and Weaknesses: Strengths: Addresses the important issue of reliability in SBI inference. Introduces the novel concept of balanced classifiers for NRE. Weaknesses: Limited experimental evaluation primarily using lowdimensional benchmark problems. Relies heavily on a single metric (expected posterior coverage) to evaluate performance. Does not compare BNRE to other SBI methods or evaluate its practical relevance. Questions and Suggestions: Conduct additional evaluations on more complex and realworld problems. Consider using more comprehensive metrics such as simulationbased calibration and local coverage tests. Compare BNRE to other SBI methods and provide guidance on its practical use.", "Paraphrased Statement: This research extends the existing NRE simulationbased inference method to address its tendency to overestimate uncertainty. The proposed improvement BNRE aims to enhance the reliability of posterior estimates. Empirical evaluations on benchmark problems demonstrate that BNRE significantly outperforms NRE in terms of reliability as indicated by various metrics. Strengths: Originality: This study is among the first to specifically address overconfidence in simulationbased inference algorithms. Quality: The paper is technically robust with empirical evidence supporting most claims. Clarity: The manuscript is wellwritten providing a clear understanding of the technical contributions and empirical results. Weaknesses: One claim regarding the optimal parameter value (\\lambda) requires further empirical support which could be provided in an appendix figure. Certain imprecise language may lead to confusion such as regarding the impact on the global optimum (page 4 line 119) and the use of \"increasingly\" (page 4 line 122). Some unclear phrasing such as \"Ideally\" (page 4 line 123) and the notation for the nominal parameter (page 7 line 186) could be improved for clarity. Significance: The technical development and improved performance of BNRE over NRE will be valuable to the machine learning community particularly those working with simulationbased inference. The proposed improvements may inspire enhancements to other simulationbased inference methods.", "Paraphrased Statement: The researchers propose an adjusted version of the NRE algorithm that reduces overconfidence in posterior estimation. They introduce a new class of \"conservative\" classifiers that outperform the optimal Bayes classifier on average. By penalizing overconfidence the proposed algorithm Balanced Neural Ratio Estimation (BNRE) improves the reliability of simulationbased inference with the NRE algorithm. Strengths and Weaknesses: Strengths: Clear and concise presentation Rigorous theoretical proofs Extensive empirical validation Weaknesses: Applicability limited to the NRE algorithm Lack of specific examples of overconfident simulationbased inference leading to errors Additional Comments: Extending the proposed logic to other SBI algorithms would enhance the value for the community. Providing realworld examples of overconfident SBI algorithms causing problems would strengthen the papers argument. Overall: The proposal presents a valuable concept that warrants further exploration. While its current focus on NRE limits its immediate impact its potential to improve simulationbased inference could prove significant."], "vriLTB2-O0G": ["Paraphrased Statement: Summary: This study suggests using a neural network based on Gaussian process surrogates for objectives to approximate the Pareto set in complex multiobjective optimization problems. The goal is to assist decisionmakers in navigating the estimated Pareto set and front. This model also aids in batch selection for sequential optimization using the expected hypervolume improvement (qEHVI) criterion. The method is compared to standard test functions and baselines demonstrating its effectiveness. Strengths: Integrates Pareto set structure into qEHVI optimization. Provides a practical tool for decisionmakers. Employs backpropagation for Pareto set model construction which is innovative. Delivers impressive performance in terms of speed and sample efficiency. Weaknesses: Similar Pareto set models exist in literature. Relies on qEHVI criterion. Behavior on disconnected Pareto fronts remains unexplored. Certain aspects require clarification (see below). Questions: Can you provide examples of learned Pareto fronts and sets in cases where they are disconnected In Appendix line 57 is the term \"Expected improvement\" still used In Appendix C5 please clarify the meaning of \"uncertainty.\"", "Paraphrased Statement: This study unveils a novel technique for discovering the complete Pareto set in complex multiobjective Bayesian optimization problems. It employs a model that correlates preferences with Pareto solutions using scalarization. Subsequently it leverages a batched acquisition search guided by the learned model. Strengths and Weaknesses: The proposed algorithm addresses a longstanding challenge in Bayesian optimization. However the limitations of the experimental setup raise concerns: Concerns: The test problems are excessively simple potentially undermining the algorithms effectiveness. The dimensionality of the test problems is limited to 6 raising questions about scalability. Questions: Why were the test problems chosen with such specific characteristics To enhance the evaluations robustness it would be beneficial to include experiments using more diverse test problems from established suites like ZDT and DTLZ.", "Paraphrased Statement: Summary: This research introduces a new approach for optimizing multiple objectives using Bayesian optimization (MOBO). The proposed method improves efficiency and execution time compared to existing methods. It takes into account the typical infinite nature of the Pareto set and uses a finite approximation. Strengths and Weaknesses: While multiobjective optimization is a significant problem numerous studies have addressed it making it challenging to make substantial advancements. The method incorporates: 1. Leveraging the connection between Pareto optimal points and an extended Tchebyshev scalarization. 2. Using a neural network to select potential design candidates believed to be Pareto optimal. The paper is generally clear but its unclear why a neural network is needed for candidate selection. While empirical results demonstrate the methods effectiveness they raise questions due to the lack of theoretical support and insufficient motivation for the methodology. Questions: Why is using a neural network policy for candidate generation advantageous Why not directly optimize the acquisition function under one or multiple scalarizations for batch selection Can an ablation study exclude the neural network and directly optimize the acquisition function How significant are the individual contributions of using Tchebyshev scalarization and the neural network for candidate prediction Why not maximize expected improvement or LCB of HVI instead of using HVI for batch selection Why is an introduction to Bayesian optimization missing What is the benefit of using HVI for batch selection compared to the proposed sequential optimization approach Why is qEHVI excluded from comparison Why is qNEHVI excluded from comparison", "Paraphrase: Summary: The paper presents a new algorithm for handling multiple objectives in Bayesian optimization where decisionmakers dont have specific preferences among those objectives. The proposed technique utilizes a batch acquisition function that aims to identify a \"Pareto manifold\" of solutions which represents a continuous set of tradeoffs between the objectives. Strengths: The problem scenario is significant as many realworld scenarios involve solutions that form a continuous Pareto frontier. The concept of generating a projection of the Pareto manifold based on the users preferences is innovative. The approach is wellorganized original and clearly explained in the paper. The study extensively evaluates the solution against various baselines using synthetic and realworld datasets. Weaknesses: The utility of accessing the Pareto manifold in the context of costly evaluations remains questionable. Determining user preferences using scalar values poses practical challenges without clear guidelines. The experimental comparison lacks a critical algorithm from the literature on preferences over objectives. The contribution of the proposed batch selection strategy is minor and incremental. Questions: While many applications have an infinite Pareto set obtaining an accurate approximation of the manifold in the input space may not be feasible or useful with expensive evaluations. Assessing the Pareto manifold without output values may not be sufficient given the risk associated with relying solely on input space predictions."], "yLilJ1vZgMe": ["Summary Paraphrase: The NTK and NNGP models offer valuable insights into the performance of neural networks under specific conditions. They can predict network behavior accurately with estimated uncertainty. These models rely on computations involving a kernel or covariance function that represents the correlation between activations in the network. Previous research has derived closedform expressions for kernels for specific activation functions. This paper introduces a method to approximate NNGP kernels using a polynomial series. Moreover a connection between the NTK and NNGP can be established through differentiation extending the results to the NNTK. Additionally the paper provides a random sketching method to efficiently approximate large kernel matrices for deep networks. Experiments demonstrate that these methods provide accurate approximations for known NNGPs and NTKs with significant speed improvements compared to exact kernel computation. Strengths: General applicability to a wide range of activation functions Thorough literature review Weaknesses: Some concerns with proofs and mathematical conditions The presentation of the NNGP definition could be improved The condition for swapping derivative and expectation in Theorem 3 is not fully clarified The differentiation under the integral step in Theorem 3 does not fully account for the behavior of nonsmooth activation functions like ReLU and ELU The results in Theorem 3 are similar to those in a previously published paper which should be acknowledged or integrated", "Paraphrased Summary: This study explores efficient methods for computing Neural Tangent Kernels (NTKs) with various activation functions. The authors extend the Hermite polynomial approximation technique and provide approximation bounds for qhomogeneous dual kernels. They also propose a sketching method for NTK approximation extending previous work to homogeneous kernels with rapidly convergent Taylor expansions. Strengths: Focuses on NTK with general activation functions advancing the understanding of neural networks. Proposes a method to compute the dual kernel of derivative activations without activation knowledge. Provides approximation bounds for finite Hermite polynomial approximation. Extends the sketching approximation technique to more general homogeneous kernels. Offers a wellorganized and clearly written paper. Weaknesses: May lose the global convergence property associated with NTK and ReLU. Fast sketching approximation may only apply to K\u03c3 homogeneous dual kernels and its computational complexity is still high. Experimental time comparisons and error analyses could be improved. Questions: Does the general activation NTK approximation guarantee global convergence How to recursively compute Ldepth NTK using Equation 5 How is the experimental time in Figure 1 calculated Could it be compared to other fast kernel approximation techniques", "Paraphrase: Summary: The Neural Tangent Kernel (NTK) framework helps comprehend theoretical aspects of neural networks near initialization particularly in the infinitewidth limit. While NTK computation is straightforward for ReLU activations in this limit its challenging for other activations. This paper proposes an approximate computation method based on functional analysis. Strengths: Wellstructured and logical presentation Sound theoretical results Proposed methods are supported theoretically and outperform traditional NTK computation methods in numerical experiments Clear and easytofollow proofs Weaknesses: Approximation rates are not provided for general network depths. Literature review is limited. Numerical results focus solely on CIFAR10. Lack of discussion on dependencies between NTK accuracy and depth. Questions: How does the proposed method compare to spherical harmonics decomposition for NTK computation How does the method differ from the finitewidth NTK computation proposed in [4] Can the condition on the second derivative in Theorem 3 be further relaxed", "Paraphrased Statement: Summary: This research enhances the theory of neural kernels with infinite width. Compared to previous studies its contributions include: 1. Calculation of neural network Gaussian process (NNGP) and neural tangent kernel (NTK) kernels for a broader range of activation functions. 2. Derivation of an explicit formula for the dual kernel of polynomial activation functions extending the assumption to x y in general ddimensional space. It also suggests approximating the dual kernel for nonpolynomial activations using truncated Hermite expansion. 3. Development of a technique to compute the dual kernel of the derivative without knowing the activation function. 4. Implementation of random sketching methods to expedite NTK approximation extending this approach from ReLU activation to more general scenarios. Strengths: The research expands prior findings on dual kernels offering a valuable reference for further investigation in this field. The theorems presented while incremental are significant. Questions: Due to unfamiliarity with neural kernels the reviewer is unable to provide specific suggestions for improvement."], "xK6wRfL2mv7": ["Paraphrased Statement: Summary: This research introduces two costeffective techniques (SAF and MESA) to optimize sharpness without incurring the doubled training cost associated with the standard Sharpness Aware Minimization (SAM) approach. Strengths: Addresses the practical challenge of improving training efficiency. Demonstrates comparable or superior performance to other methods on ImageNet when applied to ResNets. Weaknesses: MESAs claimed 15 cost increase is unclear since it involves two forward passes instead of one. SAF implies it is \"free\" but has a minor memory overhead. The relationship between SAF and MESA to SAM could be better explained especially the motivation for Equation 8 and the source of the KL term. The ResNet baseline for SAM is lower than reported in the original SAM paper. Its uncertain if the proposed methods are compatible with modern ResNet approaches that achieve higher accuracy. Questions: Why does the ResNet baseline for SAM differ from the original SAM paper How does MESA achieve only a 15 cost increase Are the methods presented additive to modern ResNet approaches (e.g. from the timm library) Why is GSAM significantly better than other methods for the ViT model", "Paraphrase: Summary: This research focuses on the computational cost of SharpnessAware Optimization (SAM) which costs twice as much as Stochastic Gradient Descent (SGD). The proposed algorithms address this issue: SAM for Free (SAF): Uses past optimization data to estimate sharpness loss reducing computational cost to nearSGD levels. It requires additional memory. MemoryEfficient SharpnessAware Training (MESA): Reduces memory usage significantly increasing computational cost slightly compared to SAF. SAF and MESA show comparable performance to SAM and SGD on CIFAR and ImageNet datasets confirming their effectiveness in reducing SAMs computational cost. Strengths: 1. SAF is derived from a wellreasoned analysis of SAM loss. 2. The authors address practical implementation challenges. 3. Empirical results show improved performance over SAM at a reduced computational cost. Weaknesses: 1. The use of KullbackLeibler (KL) divergence instead of crossentropy in the loss function may have negative consequences. An ablation study is suggested. Questions: 1. Is the reduction in computational cost from 2x to 1x SGD sufficiently meaningful 2. Does the use of KL divergence affect the performance or robustness of the optimizer", "Paraphrased Statement: Summary: This research introduces efficient methods for implementing sharpnessaware training including: SharpnessAware Training for Free (SAF): Requires storing predictions for previous checkpoints. MemoryEfficient SharpnessAware (MESA) training: Involves performing two forward propagations and one backpropagation for each optimization step. Despite requiring less computation than existing methods SAF and MESA achieve competitive accuracy on tasks like CIFAR10 and ImageNet1k. Strengths and Weaknesses: Originality: This paper presents a new method for estimating sharpness based on weight trajectories (Eq. 7). Using this method the authors propose timeefficient (SAF) and memoryefficient (MESA) implementations of sharpnessaware training. Clarity: Figure 3 clearly illustrates the key idea of the paper. However the equation in L164 and the second approximation in Eq. 7 require further clarification. The definition of \\Phit with respect to the optimization step is not clear. Significance: There is a discrepancy in reported SAM accuracies between this paper and previous studies. The authors should clarify this to ensure fair comparisons. Questions: 1. Interpretation of Loss Landscape Visualizations: How can we reconcile the observation that SAF and MESA find flatter local minima than SAM with the fact that SAF has better test accuracy than MESA 2. Scalability of SAF and MESA: Can SAF and MESA benefit from longer training without overfitting as observed for the original SAM optimizer", "Paraphrase: This study introduces a novel optimization method that enhances model sharpness without significant computational overhead. The authors suggest utilizing the disparity in training loss specifically the KLdivergence between output predictions as a proxy for sharpness. This approach introduces minimal computational expense while requiring only additional memory. Additionally an adjusted version of the method alleviates the memory constraints of the original approach. Experimental results demonstrate the effectiveness of the proposed methods on ImageNet and CIFAR datasets. Strengths: 1. Clear and wellstructured writing. 2. The drawback of the initial method regarding memory consumption is effectively resolved in the subsequent method. 3. The method is straightforward and efficient in implementation. 4. The experiments employ industryleading configurations. Weaknesses: 1. The method occasionally falls short of SAM in terms of accuracy but this is not a major drawback since the primary goal is to reduce computational overhead. 2. The experiments are limited to image classification tasks. 3. The method adds several hyperparameters. Questions: 1. Since the authors note that model outputs are unstable before epoch Estart does employing this method from the outset have any impact on performance Does this instability have an effect 2. How should the hyperparameters (tau E and lambda) for your method be chosen Have you examined a range of hyperparameters It is mentioned that a fixed value is used for these parameters. 3. In Figure 4b why does MESA exhibit higher sharpness at the start of training but lower sharpness at the end of training compared to SAF Additionally the visualization suggests that MESA has the flattest landscape."], "xubxAVbOsw": ["Paraphrase: This research criticizes recommendation algorithms using collaborative metric learning (CML) because they may not adequately consider users niche preferences due to their limited ability to represent preferences. To address this the paper introduces a novel and effective approach called DiversityPromoting CML (DPCML). The papers strengths include: Clear motivation and originality Strong theoretical analysis with novel insights on DPCMLs advantages Convincing empirical evaluations that demonstrate DPCMLs effectiveness Weaknesses include: Occasional typos and grammatical errors Lack of proof links for theorems and corollaries making it difficult for readers to follow Overall the paper is wellwritten and presents a compelling case for further research on DPCML. To improve the authors should correct the minor errors and provide proof links to enhance the papers rigor.", "Paraphrased Statement: Summary: This research introduces a method for recommending items that combines standard collaborative metric learning (CML) with multiple user representations. The technique introduces an additional loss function to promote diversity in user representations. The effectiveness of this approach is evaluated against matrix factorization (MF) and CMLbased methods using benchmark data. The paper also provides a theoretical foundation for the method with a generalization bound. Strengths: Item recommendation is a critical challenge and the experiments show that the proposed technique is effective in the studied scenarios. The research suggests that ideas developed for MFbased techniques may be valuable to explore within the CML framework. Weaknesses: The concept of using multiple user representations is not novel tracing back to at least [1]. The approach is similar to that presented in [2] which is not cited in this work. Demonstrating that a concept from MF works in CML is a restricted contribution. The diversitypromoting regularizer lacks sufficient exploration. The paper does not adequately explain why user representations should not be too different. Further exploration and commentary on the regularization strategy would be beneficial. The potential improvement of MF results using the proposed regularizer as in [1 2] is not investigated. Alternative regularizers were not considered. The proposed systems effectiveness in promoting diversity is not explored beyond the result in Figure 5. Providing additional evidence of increased recommendation diversity would strengthen the paper. The paper includes several new hyperparameters (C \u03b7 \u03b41 \u03b42) and the results appear highly dependent on their specific values (Figures 6 8 9). This is a significant limitation especially if similar tuning resources were not applied to baseline methods. The paper is somewhat difficult to read and disorganized. There are incomplete sentences and unreadable figures.", "Paraphrased Statement: Summary: This research focuses on recommendation systems that utilize Collaborative Metric Learning (CML). The authors observe that existing CMLbased approaches may have limited performance due to insufficient consideration of varied user interests. To address this they introduce a new algorithm called DiversityPromoting Collaborative Metric Learning (DPCML). DPCML assigns multiple representations to users allowing it to cater to both majority and minority interests. The authors mathematically prove the superiority of DPCML over existing CML methods and demonstrate its effectiveness in unseen data. Experiments confirm DPCMLs efficacy across various datasets. Strengths: Clearly stated motivations and contributions Novel method that leverages multiple user representations and a diversity control term Strong theoretical support including a generalization analysis of CMLbased algorithms Convincing experimental results Weaknesses: Some sections require reorganization for improved clarity (e.g. moving the proof of Corollary 1 to the main paper) Typos throughout the paper Lack of a concise explanation of the Joint Accessibility problem Questions: How can the paper be reorganized to enhance understanding Can the authors provide a more detailed explanation of the Joint Accessibility problem"], "xDaoT2zlJ0r": ["Paraphrased Summary: The goal of this paper is to develop a method for identifying and preserving quantities that remain constant (invariant) in a dynamical systems observations. The researchers propose simultaneously learning a model that approximates the systems behavior and a set of parameterized functions (first integrals) that are kept constant between prediction steps. Several technical approaches are used to ensure that the first integrals are respected. Strengths and Weaknesses: Strengths: Clear presentation of objectives and relevance to the field. Wellwritten and technically sound. Weaknesses: Loss objective not clearly stated in the main text. Absence of an algorithm section to detail the practical learning process for the first integrals. Lack of experiments with higherdimensional data. Questions: 1. Clarification on the optimization problem for the Lagrange multipliers (\u03bb) used in Section 3.1. 2. Explanation of why the continuous version of the equation is prone to numerical errors compared to the discrete version. 3. Method used to solve equation 3 at each iteration. 4. Clarification on why gradients are not computable. 5. Discussion of potential spikes in the learned first integrals and any strategies to mitigate this issue. 6. Influence of hyperparameters on the learning process particularly regarding the balance between learning rates for the first integral and dynamics models.", "Paraphrase: Summary: The authors propose to modify the Neural Ordinary Differential Equation (Neural ODE) model to preserve first integrals of physical systems. These integrals can be known or learned using a neural network. Strengths and Weaknesses: The authors introduce a novel method to enforce conservation constraints by incorporating Lagrange multipliers into the derivative function of the Neural ODE. The model exactly conserves first integrals a feature not found in previous models. The first integrals can be learned from data and correspond to known conservation laws (e.g. mass and energy). Preserving first integrals in learned physical simulations is a significant challenge and this work offers an elegant solution. Questions: Difference between cFINDE and dFINDE: cFINDE assumes continuous functions f(u) and V(u) but uses an integrator in practice. dFINDE uses an integrator for the base model \u03c8 and claims to differ only in how it calculates gradV(u). Preventing degenerate solutions in learned first integrals: The function f(u) is trained alongside the learned first integrals V(u). The bestcase scenario is that f(u) preserves some or all of the first integrals rendering V(u) unnecessary to avoid degenerate solutions. Comparison of integrators: The authors demonstrate that dFINDE with the DormandPrince method conserves energy better than the leapfrog integrator. However it is unclear how much this performance difference relies on the choice of integrator. Generalization to different systems: Have the authors tested the model on systems with different masses and total energy than the training set This would verify whether the learned constraints can generalize to different physical conditions.", "Paraphrased Statement Summary This study introduces a new neural model that uses data to predict the dynamics of a system by identifying numerous undiscovered conserved quantities (first integral). Numerical errors are eliminated by the discrete derivativebased formulation and the model is designed to adhere strictly to conservation laws. The proposed model can more accurately capture multiple conserved quantities and predict state dynamics than the baseline model as shown in experiments involving physical systems with such quantities. Strengths and Weaknesses Originality The idea of simultaneously inferring multiple unknown conserved quantities is novel and intriguing. Incorporating prior information about the first integral is rational. The use of discrete derivatives is a useful formulation. Quality The study is technically sound. The effectiveness of the proposed model is demonstrated in experiments with physical systems having various conserved quantities. Weaknesses The explanation of the learning algorithm is inadequate. Only comparisons to the baseline model are provided. Clarity The paper is wellwritten. Significance Datadriven approaches to modeling physical systems have gained popularity. This is significant because it offers the potential to create simulators from data for physical phenomena where mathematical models are unknown. Weaknesses The application is not outlined in detail. Questions Q1. Do you have a method to determine K when the number of conserved quantities (K) is unknown Q2. It would be beneficial to define the objective function or learning algorithm. Im curious if its appropriate to use a 1step error for training. In [R1 R2] timewindow learning is used which could be useful in this study. Q3. While the evaluation experiments are carefully conducted Im interested in seeing comparisons to other models (e.g. DGNet). Q4. How resilient is the proposed model to variations in the number of trajectories and time resolution in the training data Its possible that it performs well with sparse data due to the consideration of several conserved quantities. References: [R1] Kevin Course Trefor Evans and Prasanth Nair. Weak form generalized Hamiltonian learning. In Advances in Neural Information Processing Systems volume 33 pages 18716\u201318726 2020. [R2] Yaofeng Desmond Zhong Biswadip Dey and Amit Chakraborty. Symplectic ODENet Learn ing Hamiltonian dynamics with control. In International Conference on Learning Representa tions 2020.", "Paraphrased Statement: Summary: This paper investigates using a neural network to learn and approximate the first integrals (conserved quantities) of a dynamical system. It then utilizes these learned first integrals to predict the future evolution of the system by projecting the time trajectory onto a submanifold defined by the gradients of the first integrals. Numerical experiments demonstrate that the proposed method called FINDE outperforms baseline models in predicting future states of the system. Strengths: 1. FINDE addresses the important problem of understanding and predicting physical systems governed by symmetries or conservation laws. 2. The DiscreteTime Projection Method for preserving first integrals in numerical simulations appears to be innovative. 3. The evaluation results provide convincing evidence of the effectiveness of the method. Weaknesses: 1. The methodology section is difficult to comprehend and lacks intuitive explanations after introducing the method formally. 2. The contributions of the paper are intertwined making it challenging to distinguish between the ability to identify the correct first integrals and the accuracy of future predictions. 3. It is unclear whether FINDE guarantees the preservation of other types of first integrals beyond energy and total mass. Questions: 1. Can the authors provide a clearer explanation of how to implement the implicit FINDE method in an explicit manner 2. What is meant by the statement \"Not only that FINDE found and preserved the system energy and the total mass as first integrals\" Does this imply that FINDE cannot ensure the preservation of other types of first integrals"], "wO53HILzu65": ["Paraphrase: This study presents a largescale analysis of 18 algorithms on 85 datasets in the domain of recommendation systems. The key finding is that optimal performance depends heavily on the dataset and evaluation metric used. The authors propose RecZilla a model that can predict the best recommender algorithm and its optimal hyperparameters based on the characteristics of a new dataset. Strengths and Weaknesses: Strengths: Comprehensive and thorough experimentation. Evaluation across a wide range of algorithms and datasets. Opensource code for reproducibility. Weaknesses: Contribution: The studys conclusions align with previous research limiting its novel contributions. Similar ideas have been explored in other works such as [4]. Methodology: Lack of consideration for deep learningbased algorithms. Overemphasis on a single evaluation metric (Prec10) potentially introducing bias. Minor Issues: Typo in line 172. Missing related work [3]. Overall: While the experiments are impressive the papers contributions are considered incremental and not particularly novel. To enhance the impact the authors could emphasize the practical implications of their work for researchers and practitioners.", "Paraphrase: This paper comprehensively investigates the performance of various recommender system algorithms across 85 datasets and 315 performance metrics. The analysis demonstrates that algorithm performance significantly varies depending on the metric and characteristics of the dataset. To address this challenge the paper proposes an automated approach for selecting the optimal algorithm for a given dataset using the insights gained from the analysis. Strengths: The study is comprehensive evaluating algorithms across a wide range of datasets and metrics. The authors release the training data and pretrained models which can assist practitioners in selecting the best models for their specific needs. Weaknesses: The study focuses on traditional recommender system models such as clustering matrix factorization and linear models. More sophisticated models such as nonlinear graphbased models are not considered. Selecting the best algorithm from the models included in the study may provide a reliable baseline but may not be optimal for more advanced models. Questions: Do the authors plan to extend their analysis to include more sophisticated models in the future What are the challenges associated with applying these models within the current framework", "Paraphrase: This study evaluates the performance of 18 recommender system models on various datasets using 23 metrics. Itembased nearest neighbor models generally performed well but different models excelled in specific datasetmetric combinations. The authors applied the SatZilla algorithm selector to recommender system model selection and found it to outperform existing selectors. Strengths: Comprehensive metastudy on recommender systems Integration of the SatZilla algorithm selector for model selection Weaknesses: Lack of clarity on hyperparameter optimization Absence of neural network models in the evaluation Limited comparison with other model selectors in the main text Questions: Can any model perform well across all datasetmetric pairs Can we predict the bestperforming model for a given datasetmetric combination The authors metastudy provides valuable insights but could benefit from addressing these weak points.", "Paraphrased Statement: This research investigates the impact of different data sets algorithms and hyperparameters on the performance of recommender systems through extensive experimentation. It demonstrates that the choice of data set and algorithm significantly affects performance and that these factors are not universal but rather predictable. The paper proposes RecZilla a metalearning technique that predicts optimal algorithm and hyperparameters for novel data sets using metafeatures. RecZilla is shown to rapidly identify highperforming algorithms on unfamiliar data sets. Strengths: Addresses a key challenge in applying machine learning to industrial recommendation systems where selecting appropriate algorithms and hyperparameters has traditionally relied on human expertise. Supports claims with a substantial number of experiments demonstrating the generality and predictability of recommender systems. Provides opensource experimental code fostering transparency and reproducibility in empirical research. Weaknesses: While the experimental scope is broad the study focuses on older algorithms (mostly pre2015) and public data sets raising concerns about generalizability to modern neural networkbased methods and realworld user behavior patterns. The cost of training and updating the RecZilla model may be high especially with the addition of new data. Questions: 1. Why were most selected algorithms developed prior to 2015 Should more contemporary algorithms be considered for evaluation 2. How can the authors demonstrate the practical value of RecZilla in realworld recommender systems where data sets are typically larger and algorithms more complex"], "suHUJr7dV5n": ["Summary: This research examines Hessianfree bilevel optimization a cuttingedge topic in machine learning with applications like neural architecture search hyperparameter optimization and reinforcement learning. The authors analyze the convergence and efficiency of Hessianfree bilevel algorithms that leverage zerothorder estimation which has been successful in deep learning applications like ESMAML and HOZOG. The authors tackle a significant variance and convergence issue with current zerothorder methods and propose estimating only the hypergradients response Jacobian through direct forward propagation without backpropagation. They provide a convergence rate analysis for the resulting Hessianfree PZOBO method with partial hypergradient estimation and its stochastic variant PZOBOS. Strengths and Weaknesses: Strengths: Wellwritten and effectively motivated. Demonstrates the benefits of partial zerothorder hypergradient estimation for variance reduction and improved convergence. Proposes a simpletoimplement zerothorder response Jacobian estimator facilitating extension to the stochastic case. Provides the first convergence rate analysis for zerothorder bilevel optimization. Weaknesses: No exploration of using accelerated gradient methods (e.g. Nesterovs momentum) for improved convergence. Explanation lacking for the choice of a single random smoothing vector (Q1). Comparison of different Q values should be included. Missing units on the xaxis of Figure 2(a) 2(b) and 2(c). Questions: Can accelerated gradient methods further enhance convergence and complexity performance Why is the choice of Q1 considered optimal Provide an experiment comparing different Q values. Correct the xaxis units in Figure 2(a) 2(b) and 2(c) for consistency.", "Summary This paper introduces an algorithm and its stochastic extension for solving smooth bilevel optimization problems. It avoids computing the lower level problems Hessian by using zerothorder oracles. Unlike traditional approaches that approximate the entire gradient of the loss function using zerothorder methods this algorithm focuses on approximating the gradient of the solution mapping for the lower level problem. This approach reduces variance and improves the methods efficiency. Convergence guarantees are provided and numerical experiments demonstrate its competitive performance against other methods. Strengths and Weaknesses The paper is wellwritten and of high quality. The idea of using zerothorder approximations of the solution mapping Jacobian is novel and significant. However some parts of the paper could be clarified (as discussed in the questions and limitations sections). The experiments are limited to comparing against baselines and it would be beneficial to compare against other firstorder optimization methods (such as AmIGO or SABA). Questions The dimension of the hyperparameter vector is not explicitly stated in the experiments (e.g. p1 in 4.1). It is unclear whether the lower level problem in the application problem (line 265) is strongly convex and smooth. The assumption of twice differentiability of g (line 131) conflicts with the assumption of C1 differentiability.", "Paraphrase: Update (Final) Despite promises some concerns were not adequately addressed during discussions. Reducing overall rating from 6 to 5 and presentation rating from 3 to 2. Update Increased rating from 4 to 6 after authors effective response. Summary Zerothorder solver for bilevel optimization is proposed. Hessian matrix calculation is omitted by approximating the Jacobian matrix. Convergence analysis and performance guarantees are provided. Strengths Theoretical Analysis: Consideration of bias variance smoothness and boundedness for zerothorder approximation. Experimental Results: Effectiveness demonstrated through various deep learning tasks. Weaknesses Novelty Deficit: Zerothorder method is not novel in bilevel optimization as exemplified by DARTS. Lack of NAS Experiments: No evaluation on neural architecture search (NAS) for bilevel tasks where DARTS has similar approaches. Missing Variance Results: Stability and significance of results not demonstrated. Minor Issues: Inconsistent font sizes in figures. Conclusion Theoretical component is impressive but method lacks significant innovation.", "Paraphrased Statement: This research paper proposes a bilevel optimizer called PZOBO which utilizes a zerothorder Jacobian estimator for efficient and accurate hypergradient estimation. The paper provides theoretical analysis and experimental validation to demonstrate the effectiveness of PZOBO in solving bilevel optimization problems. Strengths: Addresses the challenging problem of bilevel optimization with an efficient and accurate solution. Provides theoretical guarantees for the convergence of PZOBO. Compares PZOBO to existing bilevel optimizers on various problems showing its superior performance particularly in highdimensional settings. Weaknesses: The paper lacks covariances in Figures 3(a) and 3(b) which would improve the visualization of performance variations. An analysis of the limitations of previous bilevel solvers and a discussion of how PZOBO addresses these limitations would enhance the evaluation of its novelty and impact. Figure 4 lacks subplot labels for (a) (b) and (c). Figures have inconsistent formatting such as the presence of shadows in Figures 2(d) and 2(f)."], "wZEfHUM5ri": ["Paraphrase: Summary: The paper suggests using Masked Autoencoders (MAE) for crossview completion as a preparatory task for 3Drelated monocular tasks (e.g. depth estimation pose estimation). The encoder encodes a masked input image and a reference image from a different viewpoint while the decoder uses crossview attention to reconstruct the input image. The approach demonstrates improved performance on monocular 3D tasks compared to MAE and DINO which do not involve crossview pretraining. Strengths: Proposes a novel method to extend MAE for crossview pretraining focusing on geometric tasks beyond semantics. Provides solid ablation studies that support the design choices. Empirical results show the effectiveness of the method on 3D monocular tasks. Weaknesses: 1. Assumption of Overlap: The method assumes a 50 overlap between the reference and target views. It is unclear how the performance changes with different overlap levels. 2. Synthetic Training Data: The model is trained on synthetic data which may limit its applicability and scalability to realworld scenarios. 3. Impact of Masked Ratio: The authors observe that the masked ratio significantly affects performance even with crossview attention. They do not fully explain this phenomenon. Questions: Regarding weaknesses 1 and 3 could the authors provide further insights or experimental results", "Paraphrased Statement: This study introduces CroCo a novel approach to pretraining transformers for lowlevel and 3D computer vision applications. CroCo is based on the Masked Autoencoder (MAE) concept but incorporates two overlapping views of the same scene rather than individual images. The key innovation lies in conditioning the prediction of masked patches in one image not only on the visible patches in that image but also on the complete second view of the scene. This enables the model to improve its understanding of spatial relationships between the two views and enhances its ability to predict the masked patches. CroCo is evaluated on a range of tasks including ImageNet classification depth estimation geometric tasks and camera pose estimation. On tasks requiring geometric understanding CroCo outperforms MAE consistently highlighting the benefits of its additional view conditioning. Strengths and Weaknesses: Originality: CroCo leverages the effectiveness of MAE for selfsupervised pretraining and extends it by utilizing multiple views of the same scene. This approach is particularly suitable for downstream tasks requiring geometric understanding. Weaknesses: Some highly relevant related works particularly those focused on selfsupervised learning with RGBD data have not been adequately cited. Quality: CroCo is technically sound wellmotivated and thoroughly evaluated. Clarity: The paper is written in a clear and concise manner with informative figures and tables. Significance: CroCo demonstrates the advantages of using multiple scene views for selfsupervised pretraining providing improved pretraining for geometric tasks specifically. Questions: 1. How can the limitation of requiring a simulator like Habitat be addressed and what impact does this have on downstream applications", "Paraphrased Statement: This paper proposes a new unsupervised representation learning task that leverages pairs of images showing the same scene from varying angles. This approach differs from traditional pretraining methods that use a single image. The proposed \"crossview completion pretext task\" encourages networks to focus on both visible and hidden content in images enhancing their pretraining effectiveness. Experimental results demonstrate the superiority of this pretext task compared to existing techniques. Strengths: The use of crossview completion as a pretext task is innovative and logical. The method achieves stateoftheart performance despite its simplicity making it easy to implement similarly to Masked Image Modeling (MIM). The novel attention blocks in the decoder designed specifically for crosscompletion tasks are noteworthy. The paper is wellorganized and accessible. Questions: Could the performance be evaluated using real crossview image pairs such as the DTU dataset to complement the synthetic image pairs used for pretraining The idea of using another image for MIMlike pretext tasks is intriguing. It would be interesting to consider generating synthetic crossview images using geometric matching for use with ImageNet. Additional comments or experiments regarding the use of the Habitat dataset for pretraining would be valuable. Varying the covisibility ratio (e.g. 10 or 90) could provide further insights into its effect on performance. An analysis of the impact of masking ratio (e.g. 75 vs. 90) across different datasets and decoder architectures would be beneficial. Comparing the performance of a pure decoder or the decoder from MAE [CVPR22] would be insightful. Exploring dynamic masking ratios as part of the masking schedule would be interesting. Minor Comments: The differences between crossview and multiview should be clarified as \"multiview\" may also be appropriate.", "Paraphrase: The paper proposes a novel model for pretraining 3D vision models for downstream tasks using Masked Image Modelling (MIM). The focus is on enhancing performance in monocular 3D vision tasks like depth estimation. Suggested Improvements: Abstract: Highlight the results achieved using the proposed model. Concisely describe the limitations of current methods (avoid repetition). Table 1: Correct row numbering for Croco normalization impact. Figures 5 and 6: Label the Yaxis for clarity. Appendix: Include a list of acronyms. Punctuation: Ensure correct spacing and punctuation in equations and text. Introduction: Remove \"instead\" in line 40. Improve the placement of Figures 2 and 3 providing the term \"Croco\" and intext citation. Use \"masked image\" instead of \"target image\" in line 52. Related Works: Summarize research gaps or outcomes at the end of the section. Crossview Completion Pretraining: Explain the selfattention and multihead attention block in detail as they play a crucial role in prediction. Clarify the difference between multihead crossattention and selfattention. Ensure consistency between the description in line 154 and Figure 4."], "tX_dIvk4j-s": ["Paraphrased Summary: The proposed method aims to enhance surface reconstruction quality by introducing two losses: 1. Viscosity Loss: This loss aims to smooth surface regions without points improving the surfaces smoothness. 2. Coarea Loss: This loss approximates the area of the surface at the zero level set contributing to the surface reconstructions accuracy. Experiments demonstrate the methods effectiveness particularly with sparse point inputs. Strengths and Weaknesses: Strengths: 1. Clarity and organization of the paper 2. Justification for the viscosity loss 3. Improved reconstruction results on sparse datasets Weaknesses: 1. Lack of neural network architecture contributions 2. Unclear motivation for the coarea loss (effectiveness shown only through ablation studies) 3. Insignificant improvements in quantitative benchmark results and ablation studies 4. Limited testing on point clouds with noise Questions: 1. Provide a clearer explanation of the motivation for the coarea loss. 2. Conduct experiments on point clouds generated by MultiView Stereo (MVS) which typically contain more noise than the data used in the paper.", "Summary Paraphrase: The paper presents a surface reconstruction approach that uses gridbased functions instead of current INR techniques to construct surfaces from point clouds based on signed distance functions (SDFs). It employs standard INR losses along with a novel prior loss comprising two components: Viscosity prior: Promotes smoothness in the SDF Coarea prior: Enforces minimal surface area Quantitative evaluations on the SRB dataset show comparable performance to stateoftheart methods while qualitative results on the Stanford 3D scanning repository demonstrate the techniques effectiveness. Strengths and Weaknesses Strengths: Recognizes the ongoing significance of generic direct priors for surface reconstruction introducing two novel priors. Provides mathematical explanations intuition and visualizations to illustrate the benefits of the prior components. Produces results with normals comparable to stateoftheart methods. Includes thorough ablation studies of the components. Presents a wellwritten paper. Weaknesses: Quantitative evaluations are limited to only 5 shapes. Despite the gridbased nature of the method which should result in faster optimization times there is no comparison to other methods in this regard. Storage requirements for the learned INR are not discussed. Related work omits references and discussions of previous research using Laplacian minimization as a prior such as DiGS and NSP. Without normals the method yields inferior results despite its intended contribution as an effective prior for SDFs. Questions: Why does PHASE perform better despite utilizing similar priors to this method Could it be attributed to the use of neural networks versus the networkfree approach in this work or potentially due to more effective direct priors The cited research might be of interest as it explores similar priors and achieves comparable or better performance. Could this suggest that direct priors are not the sole factor determining reconstruction quality", "Paraphrased Statement: Summary: This paper presents two novel geometric prior terms for aligning regular grids of Signed Distance Functions (SDFs) to point clouds. These priors include: Smoothing the field by simulating viscosity Minimizing the area of the zero level set Strengths: Technically sound Wellpresented with informative figures Effective ablation studies demonstrate the impact of each prior Weaknesses: Major: Experiments are limited to only five shapes Optimization strategy details (grid resolution gradient descent steps) are missing Normal consistency is not discussed Minor: Introduction and related work sections could be more informative and focused The use of a grid of SDFs is claimed to have advantages but these advantages are not specified The proposed loss terms could also be used with implicit networks Questions: How are gradients computed from the grid in equation (3) Why is Figure 2 considered to show a problem when the zero isosurface appears unchanged In Table 2 why does Daratesh perform better without additional loss terms How difficult is it to tune the weighting of the viscosity term in practice", "Paraphrased Summary: This research introduces a novel gridbased method for surface representation. It incorporates two new priors viscosity loss and Coarea loss to enhance surface reconstruction accuracy. The method outperforms existing neural networkbased Surface Distance Functions (SDFs) on the Surface Reconstruction Benchmark. Strengths and Weaknesses: Pros: Simplified training: Gridbased approach is less complex to train than neural networks. Improved surface extraction: Simplifies locating the zero level set surface representation. Novel priors: Viscosity loss and Coarea loss promote smoothness and control surface area. Cons: Lack of direct comparison: The study does not directly evaluate the performance of the proposed priors against neural networks. Questions: Separate grid optimization: Does the separate grid optimization process for each sample limit the method to solving a partial differential equation (PDE) rather than a learning problem Fixed resolution: How does the fixed grid resolution compare to the flexible resolution capabilities of neural networkbased methods"], "mq-8p5pUnEX": ["Paraphrase: Summary: The researchers have created a transformerbased system that processes input sequences in chunks of K elements. Each chunk is run through a set of transformer layers that focus on selfattention and crossattention. Afterward the outputs from each chunk are handled sequentially using another transformer with crossattention. The hidden state generated by this layer becomes input for the crossattention layers in the next chunk. The system is tested against transformers in image recognition selfsupervised learning reinforcement learning and language processing tasks showing improvement in all areas. Strengths and Weaknesses: Combining recurrent networks with attention and transformers isnt a new idea with prior works exploring similar concepts (for example [22] and [53]). The authors need to clarify how their solution is distinct and novel compared to these existing approaches. The paper lacks sufficient experimental details comparing the proposed architecture against transformers that may not represent the current stateoftheart solutions. Its unclear how beneficial the method is compared to more advanced transformers designed for similar tasks. While the generalization results for image resolutions of 64 to 128 pixels are positive they are not comprehensive enough (accuracy drops significantly for CIFAR100). Further analyses would be helpful in showcasing the methods value. The papers notation is occasionally confusing and consistency could be improved. The relevance of the work is recognized but its limited novelty reduces its significance. Questions for the Authors: How is the network trained considering its recurrent connections What is the network initialization specifically how is the initial I selected How can the input be segmented for chunking and why is the selected approach considered the most suitable How does chunking impact the networks quality and can it be universally applied What are the model sizes and are there any differences between using the recurrent model presented and other transformer models How does the method compare to previous works like [22] and [53] in the experiments conducted Is a noncausal mask utilized for language modeling within each chunk", "Summary This study explores the role of memory in deep learning models for sequences. Traditional recurrent neural networks (LSTMs GRUs) condense history into a single state vector while Transformers maintain a list of past state vectors providing higher fidelity but potentially limited compression. The proposed solution is a hybrid architecture with two processing streams: Slow Stream: Recurrent module that updates a set of state vectors every chunk size steps using crossattention. Fast Stream: Transformer encoder with crossattention blocks added. This architecture aims to balance compression with expressiveness achieving benefits in terms of both performance and computational efficiency. Strengths Interesting highlevel idea. Empirical results showing gains on visual representation learning offline reinforcement learning and language modeling. Provides both computational efficiency and performance improvements. Weaknesses Novelty: Architecture is an amalgamation of previous components lacking in uniqueness. Presentation: Writing and organization need improvement. Analysis: Empirical results lack depth and do not provide insights into the specific contributions of different components. Related Work: Does not comprehensively cite or discuss related fast and slow learning mechanisms. Experimental Design: Inconsistent reporting of evaluation metrics and number of seeds. Claim: Needs empirical evidence to support the superiority of sequential crossattention over parallel crossattention.", "Paraphrased Statement: Summary: The authors introduce Temporal Latent Bottleneck (TLB) a modified transformer architecture designed to achieve a more compact representation. TLB forces chunks of the input data through a narrow passageway limiting the attention span of the model to within each chunk. While regular transformers can access any part of a sequence TLB can only examine elements within the current chunk with information from previous chunks accessible solely via the bottleneck vector. The authors assert that TLB is applicable in any scenario where transformers are currently employed. They present evidence of its superiority over other transformer baselines in image recognition sequential decisionmaking and language modeling. Strengths and Weaknesses: The paper is wellwritten and technically sound and the experimental evaluations are thorough. However the originality is rated as low to moderate as while the authors present a substantial motivation for their architectural ideas their proposal is primarily a modification of the existing transformer architecture. Additionally there is limited support for the claims made in the introduction about sample complexity and topdown conditioning. Suggestions for Improvement: Tone down the grand claims in the introduction and focus on presenting TLB as a superior transformer architecture for various applications. Provide more empirical evidence to support the claims about sample complexity and topdown conditioning. Expand the description of the enwiki8 experiment or remove it altogether for clarity.", "Paraphrased Statement: Summary: This research introduces Temporal Latent Bottleneck (TLB) a method that enhances the compression capabilities of transformerbased models for sequential inputs without significantly sacrificing their expressiveness. TLB divides computation into two streams: a slow recurrent stream and a fast attentionbased stream. These streams interact via topdown and bottomup connections allowing the recurrent model to inform the transformer and vice versa. Experimental results across Vision Language and RL domains demonstrate consistent improvements in model performance. Strengths: Clear explanation of the proposed architecture Convincing comparisons with baseline methods Weaknesses: Hypothesis: The hypothesis that recurrent streams improve generalization by reducing irrelevant details is unconvincing and the paper lacks evidence to support it. Architecture choice: The use of a recurrent stream for the entire input conflicts with the motivation that transformers are better suited for longrange dependencies. A chunkbased approach that allows the recurrent stream to process shortrange dependencies and the transformer to focus on longerrange interactions would be more logical. Resource constraints: While the paper claims enhanced performance in resourceconstrained settings it lacks analysis of model performance as more data becomes available or as sequence length changes. Other comments: Excessive use of vertical space Baseline method in Figure 3 appears to have inadequate training Computation cost calculation omits additional terms for Ksized modules Lack of exploration with Swin Transformer Use of a bidirectional recurrent layer could improve performance Training and inference time comparisons with baseline methods are missing Questions: Mentioned in Weakness section"], "rY2wXCSruO": ["Paraphrased Statement: 1. This paper identifies and examines the drawbacks of using only one modality for feature aggregation in existing methods. 2. The paper presents a novel framework that allows for the bidirectional interaction and combination of features from different modalities. 3. The framework outperforms current stateoftheart approaches based on the nuScenes benchmark. Strengths: 1. The paper is wellstructured and easy to follow clearly explaining its motivations. 2. Comprehensive experiments provide compelling evidence of the frameworks effectiveness both in comparison to previous methods and through ablation studies. Weaknesses: 1. The paper lacks a comparison of the speed and memory requirements of different methods. For autonomous driving applications its crucial to know if the model can operate in real time. 2. While the bilateral framework is wellconceived visual representations of feature heat maps from both modalities would enhance understanding of how information from each source is selected and utilized. Questions: Authors are encouraged to include the following additional experiments in the paper: 1. A comparison of the speed and memory utilization of different methods. 2. Visual demonstrations (e.g. heatmaps of features from both modalities) to illustrate the benefits of the proposed bilateral framework.", "Paraphrased Summary: This research focuses on 3D object detection for autonomous driving using both camera images and LiDAR point clouds. The unique approach combines these two modalities (unlike previous methods that use one as secondary). The method achieves superior results on the nuScenes benchmark. Strengths and Weaknesses: The paper emphasizes the importance of combining camera and LiDAR information for performance. While the ablation study suggests the bilateral modality fusion contributes to performance gains its unclear if this is due to the fusion itself or improved backbones. The evaluation is limited to a single dataset and testing on additional benchmarks would demonstrate the models generalizability. Related work on depth fusion and transformerbased 3D object detection (e.g. ImVoteNet 3DETR Misra et al.) is not cited. Questions: 1. Which ablation experiments specifically demonstrate the importance of bilateral feature fusion for performance improvement 2. What are the models performance metrics on other datasets such as KITTI 3D object detection Minor Details: Inconsistency in notation (\u03d5 vs. \u03c6) in Figure 2(a) and (b). Typo: \"preform\" should be \"perform\" (line 59). Reference [5] and [6] are identical.", "Paraphrase: Summary: This paper presents DeepInteraction an innovative method that combines RGB and depth data for 3D object detection. Unlike previous approaches that simply append RGB features to the LiDAR point cloud DeepInteraction handles both data sources equally merging them within transformer layers. Extensive testing on the nuScenes dataset demonstrates DeepInteractions superior performance establishing a new benchmark. Strengths: DeepInteraction surpasses existing methods in terms of performance on the nuScenes benchmark. Regardless of whether testtime augmentation is used it consistently improves mAP by 2 points under the same evaluation conditions. The nuScenes experiments are comprehensive and detailed. DeepInteraction performs well with either PointPillars or VoxelNet as the 3D backbone although VoxelNet performs slightly better. Weaknesses: The paper does not address how the approach would fare on other autonomous driving datasets. While DeepInteraction excels at detecting small objects in LiDAR points (often only a few pixels in size) it is unclear if such scenarios are common in other benchmarks. The method may not be compatible with certain other benchmarks. Questions: How effective is DeepInteraction on other autonomous driving datasets Are there any scenarios where DeepInteraction cannot be applied to other benchmarks"], "m67FNFdgLO9": ["Paraphrased Statement: This research explores the issue of embedding dense facial features across different species. Due to limited data availability the methods used include: Combining knowledge from diverse models (CSE and StyleGAN) through distillation. Generating pseudopaired images using StyleGAN2s latent space exploration to uncover implicit connections between animal faces. Introducing a semantic matching loss to address the challenges posed by significant shape variations between species. The methods are applied to the MAFL and AP10K datasets to facilitate interspecies facial keypoint transfer. Additionally results demonstrate their use in manipulating interspecies facial images and dense keypoint transfer. Strengths and Weaknesses Strengths: Presents a crossdomain study of human and animal facial features. Utilizes domain adaptation to mitigate the need for extensive animal data. Proposes a knowledge distillation framework to leverage information from various models. Develops a method to learn continuous facial embeddings across species. Introduces a technique to generate paired data for semantic matching. Weaknesses: Limited contribution to the field focusing mainly on applications. Inconsistencies in baseline comparisons due to adaptation to the proposed task. Lack of convincing results for keypoint transfer and face manipulation applications. Incomplete reporting of errors in the paper.", "Paraphrased Statement This paper introduces an \"Interspecies face understanding\" task that predicts common spatial features in human and animal faces. To tackle this challenge it employs a multiteacher knowledge distillation framework. This framework leverages the benefits of two models: CSE for crossdomain feature extraction and StyleGAN2 for face embedding. Additionally it utilizes StyleGAN2s latent space to generate paired data for semantic matching. Strengths Explores an interspecies facial landmark detection model with potential applications. Combines orthogonal models for efficient crossdomain facial embedding. Weaknesses Interspecies Keypoint Transfer: Insufficient dataset and qualitative results. Concerns about landmark detection in various facial regions. Lack of comparison with existing methods. Ambiguous ablation study results. Domain Converter: No constraints to ensure spatial consistency between input and output. Training process for the Domain Converter is not specified. Encoder Training: Limited to synthetic images raising concerns about performance on realworld images. Questions 1. Why not compare on commonly used face alignment datasets and metrics 2. Can the method handle landmarks in mouth and face contour 3. Are the CSE results in Table 1 based on DIFE trained with LK1 If not how do they relate to the \"HumanDog\" column in Table 1 4. How is spatial consistency between input and output of the Domain Converter ensured How is the Domain Converter trained 5. How does the performance compare when testing on a synthetic dataset Do you have experience with this", "Paraphrased Statement: This study introduces a technique for generating facial embeddings that are shared across various animal species. The technique employs knowledge distillation to extract information from facial synthesis (StyleGAN2) and interspecies surface embedding (CSE) models. Strengths: The task is intriguing and the approach aligns well with the research goal. The models performance has been validated objectively and subjectively demonstrating improvement over baseline methods. Weaknesses: The model relies on pretrained StyleGAN2 and CSE models. The required data and annotations have increased compared to previous methods like DVE. The models embedding performance relies heavily on the capabilities of the pretrained models. Questions: Detailed information on the training data and pretrained models required is requested including: Images and annotations needed for targeting species not included in AP10K or AFHQ. The impact of pretrained model performance and training data quantity on the final embedding quality should be explored. An explanation is requested regarding the emphasis on the eyes in the generated images seen in Figure 7 and elsewhere.", "Summary The research introduces \"interspecies face embedding\" a new concept that extracts common facial features from multiple species including humans as a compact embedding. This process allows for the discovery of facial meanings in other species without relying on extensive annotations by leveraging knowledge learned from wellannotated human data. The algorithm uses a multiteacher knowledge distillation approach to guide the unsupervised learning process for this embedding. The authors demonstrate the effectiveness of their method by performing interspecies facial keypoint transfer experiments on the MAFL and AP10K datasets. Strengths Novelty of the research topic and problem addressed Technical soundness of the proposed interspecies face embedding learning method using multiteacher knowledge distillation Clear organization of the paper Weaknesses 1. Lack of Clarity: Justification for geometric consistency in learned W latent spaces of speciesspecific StyleGAN2 models is unclear. Details of pseudopaired data generation and the sharing of face geometry between original and generated images are not fully explained. Clarification is needed regarding speciesspecific training of StyleGAN2 models or the use of a universal model. 2. Limited Experimental Evaluation: Extension of keypoint transfer experiments to more landmarks is recommended. Inclusion of stronger baselines such as visual semantic correspondence methods would enhance the evaluation. Adding an experiment on interspecies face parsing could further demonstrate the methods capabilities. More visual results are needed to support the findings. 3. Relevance of Pretrained CSE Model: Clarification is needed on how the pretrained CSE model relates to face embedding learning and why it improves keypoint transfer performance. 4. Missing Limitations Discussion: Potential limitations such as the requirement for training speciesspecific StyleGAN2 models and domain converters should be addressed. Questions: See \"Weaknesses\" section for specific questions."], "r0bjBULkyz": ["Paraphrased Statement: Summary: This study introduces Active Bayesian Causal Inference (ABCI) a fully Bayesian active learning framework that integrates causal discovery and reasoning. It simultaneously estimates a posterior distribution over causal models and selects informative interventions to answer causal queries. The framework has been implemented for nonlinear additive Gaussian noise models using Gaussian processes. To manage the complexity of causal graphs and maintain a tractable posterior a continuous latent probabilistic graph representation is utilized. Strengths and Weaknesses: Strengths: Addresses a practical need: answering causal queries without prior knowledge of the causal graph. Novel and efficient approach that learns causal models and answers queries simultaneously. Weaknesses: Limited to nonlinear Gaussian models not applicable to general causal models. Information gainbased query selection is suboptimal and lacks approximation guarantees. Computationally intensive and may not scale well to larger graphs. Assumes causal sufficiency which may not hold in many realworld scenarios. May not be suitable for inferring complete causal graphs or for small intervention sets. Questions: Can ABCI be extended to model classes with discrete variables Are there alternative query selection strategies that offer better performance What is the computational complexity of ABCI How does ABCI compare to existing methods for causal graph learning under limited interventions Does ABCI learn the entire causal graph or just a subset before estimating the interventional distribution of interest", "Paraphrase: This paper presents a Bayesian approach to understanding causality combining causal discovery (identifying the causal graph and mechanisms) and inference (answering a question of interest). It employs Gaussian processes with additive noise for the mechanisms to facilitate analytical posteriors. Using the previously developed DiBS framework it optimizes over a smoothed version of the causal graph using a variational method. The paper proposes a method for handling various types of causal inference queries such as graph discovery edge identification and causal effect estimation given hypothetical interventions. An active learning algorithm is introduced that utilizes the estimated posteriors to estimate and maximize information gains for potential interventions. Strengths and Weaknesses: Strengths: Strong theoretical foundation based on Bayesian principles Addresses both causal discovery and inference acknowledging their interdependence Weaknesses: The additive noise and Gaussian process assumptions are convenient but could be improved in future work Computational complexity is not fully explored The studys use of small datasets (d \u2264 8 variables n \u2265 5 examples) raises concerns about scalability Questions: 1. Clarify the statement after Equation 3.3 to emphasize that q(M) concentrates mass on a single distribution unlike P(YM). 2. Explain the distinction between root and nonroot nodes and its significance. 3. Justify the factorization of p(f\u03c3G) for nonroot nodes but not for root nodes in Equation 4.4. 4. Explain why the reasoning in line 212 applies to nonroot nodes but not root nodes. 5. Determine if the computational bottleneck in Section 4.1 is solving for the Gaussian processes for all nodes after each graph sample G. 6. Assess the overall algorithm complexity with respect to graph size and number of examples identifying potential computational bottlenecks.", "Summary Paraphrase: ABC Framework for Bayesian Causal Inference: ABC introduces a framework that iteratively designs interventions to maximize information gain for a specific causal query. It maintains uncertainty by using latent particles representing causal graphs and computes information gain using a Gaussian processbased marginal likelihood. ABCs unique features include: Targeting specific causal queries rather than identifying causal models. Modeling linear nonGaussian causal models. Strengths and Weaknesses: Strengths: A practical endtoend solution for causal inference. Combines multiple existing ideas into a cohesive framework. Demonstrates improved performance over random intervention baselines. Weaknesses: Limited novelty in individual components or their integration. Concerns about the diversity and interpretability of the maintained uncertainty. Lack of exploration of alternative models or optimization methods. Questions: What are the key novel contributions and insights What are the reasons for choosing specific models and methods How sensitive is the framework to the number of latent particles Can alternative optimization methods be used for the utility function Are there practical benefits to using simplified utilities", "Paraphrase: Summary: This research presents a framework for inferring both a causal graph and a specific causal model from observational data. It proposes a Bayesian approach for joint inference which can also learn from new experimental data using active learning methods. The framework is evaluated through an experimental setup that demonstrates its effectiveness. Strengths: Clear problem definition with precise mathematical notation. Novel approach to inferring causal models from both observational and experimental data. Uses recent advances like the DiBS approach. Provides an opensource codebase for practical implementation. Wellwritten manuscript with consistent notation. Relevant and wellchosen realworld datasets for experimentation. Includes comparisons with multiple methods. Weaknesses: The experimental nature of the approach could benefit from a more detailed discussion of experimental setup and limitations. More extensive discussion of comparable stateoftheart methods would be helpful. The use of exogenous and endogenous variables for latent and observed variables may be confusing and a clearer explanation of the terminology would be beneficial. Questions: The commenter suggests that using exogenous and endogenous variables for latent and observed variables is confusing and suggests an alternative definition."], "zkQho-Jxky9": ["Paraphrase: Summary The paper emphasizes the importance of harm prevention for ethical and safe AI deployment. It claims that harm mitigation is inherently a hypothetical concept and that traditional machine learning methods fail to consider counterfactual reasoning resulting in harmful policies. The paper introduces a statistical definition of harm and develops counterfactual objective functions to minimize it. It applies the framework to drug dosage optimization demonstrating significantly reduced harm without compromising efficacy while standard algorithms yield substantial harm. Strengths and Weaknesses The paper excels in its writing and articulation. The research has significant societal implications. However the reliance on formal frameworks and models introduces potential limitations. Abstraction: The model simplifies complex realworld contexts for generalizability. It remains unclear how it can handle complex domains like hazardous environments with humanAI collaboration. Causal Constructs: The paper does not address how the framework handles human feedback or incorporates values and preferences into its objective functions. Related Works: The discussion on related works lacks an examination of why previous literature has not addressed harm definition and counterfactual reasoning. Questions Can the framework handle complex harmful situations Can it learn in dynamic environments with distribution shifts Proof for Claims: The assertion that factual objective functions lead to harmful policies needs clarification and evidence. The requirement for fixed harm aversion was relaxed but the rationale for this is not provided.", "Paraphrase: This research presents a rigorous statistical definition of \"counterfactual harm\" and quantifies both \"benefit\" and \"harm\" using a standard counterfactual comparative account (CCA) definition. Based on this mathematical formulation the authors develop a family of optimization functions for mitigating counterfactual harm. The paper offers robust theoretical analysis using structural causal modeling and provides an illustrative example to demonstrate the practicality of the proposed harm quantification. Strengths: The topic is relevant to fields like fairness AI and ethical AI. The quantification method is generalizable allowing for various downstream applications. The equations are clearly presented and easy to comprehend. The paper provides ample context and background information. The use of highlevel examples facilitates understanding of the core concepts. Weaknesses: The paper lacks clear structure and section titles. The analysis setup is specific potentially limiting the application of the quantification scheme to broader ML settings. The demonstration includes a simple example but lacks realworld experiments or comparisons hindering comprehensive evaluation. Questions: 1. How can the harm quantification definition be applied in general ML settings to mitigate model biases Is SCM an essential requirement for utilizing the proposed optimization functions 2. Can the harm quantification be connected to existing fairness measurements in a more specific fairness domain If so what key differences exist in the proposed quantification 3. Can validation experiments be designed to evaluate the performance of the proposed method on realworld data", "Paraphrased Statement: Summary: This paper explores the concept of defining harm as a hypothetical difference in utility between a users actions and a default option. The proposed harm definition is based on the expected difference in utility between the policy being implemented and the default action. Using this harm definition the authors introduce harm penalized utility (HPU) and discuss its implications for decisionmaking. They also examine the properties of the harm definition and provide a simulation example in the context of doseresponse suggesting that policies can be optimized to reduce harm based on userspecified parameters. Strengths: The harm definition is framed as a counterfactual rather than an intervention which aligns with causal inference research. Weaknesses: The reasoning behind defining harm and benefit separately is unclear. The terms P in HPU are not identifiable from factual outcomes or experimental data. The authors rely on an existing model rather than data which may lead to inaccurate estimates of harm. The practical definition of harm and benefit in medical research is unknown. Questions: In the trolley problem example the rationale for favoring Treatment 1 over 2 is explained. However why would HPU with lambda 4 result in no difference between switching lanes Why should noise be independent in the harm definition Is utility fixed for the entire population and how does it relate to the agents actions and user outcomes What is the practical significance of Y in defining harm and benefit when only X is observable", "Paraphrased Statement: This research presents a statistical concept of harm known as counterfactual harm (CH) using structural causal modeling (SCM). CH is based on the philosophical notion that an action causes harm if the recipient would have been better off without it. The research proposes a harm penalized utility (HPU) objective and compares optimal actions under this objective with others such as unpenalized utility and risk aversion. It demonstrates that avoiding harm requires causal and counterfactual reasoning and that actions optimized under other objectives can lead to harm. The research provides definitions and theorems to generalize this argument showing that nonCH objectives can cause harm when the data distribution changes. Strengths include the focus on practical problems and a direct approach to measuring what matters (utility). However a limitation is that relevant quantities may be unobservable and require assumptions such as the availability of an SCM. Questions: Q1: Necessity of counterfactuals Could the counterfactual distribution be replaced with an interventional distribution in the definition of CH Q2: Narrowness of SCM framework Does the SCM framework limit the definition of harm to specific variables ignoring more holistic considerations Q3: Claims of novelty Is the claim that this is the \"first\" statistical definition of harm justified without extensive literature review"], "ouXTjiP0ffV": ["Paraphrased Summary: Method: An unsupervised method is proposed for establishing shape correspondences between unseen shapes. It involves: 1. Training an unsupervised matching network (\"teacher\") on a set of shapes. 2. Training a supervised network (\"student\") using the teachers predictions as ground truth. 3. Using the student network to match shapes that were not part of the training set. Motivations: Two empirical observations motivated the method: 1. Neural Bias: Randomly initialized networks can extract features suitable for shape matching. 2. Neural Correspondence Prior: The \"Deep Image Prior\" concept applies to shape correspondence problems. Results: The proposed method achieves performance comparable to or better than stateoftheart methods on several datasets. Strengths: Extends the \"Deep Image Prior\" concept to shape matching. Wellwritten paper with a straightforward approach. Weaknesses: Neural Bias for Pointwise Features: It is unclear how the neural bias (untrained networks producing good matches) relates to the proposed method since both teacher and student networks are trained. NCP within a Shape Collection: The terminology \"teacher network\" and \"student network\" should be clarified for easier understanding. Correspondences on Point Clouds: The teacher networks performance is not explicitly shown making it difficult to assess the methods actual unsupervised component. Correspondences between Nonrigid Meshes: The proposed method only slightly outperforms unsupervised DiffusionNetbased methods suggesting limited novelty. Questions: Please refer to the \"Weaknesses\" section above for questions related to specific aspects of the paper.", "Summary The authors investigate the capabilities of random features extracted from 3D encoders. They demonstrate their effectiveness in various applications including: Point cloud matching Nonisometric shape matching on meshes Part segmentation transfer Keypoint transfer The analysis reveals that random features from certain architectures provide a strong initial prior that minimizes matching errors during training. This combined with a novel stopping criterion enables the refinement of accurate correspondences between shapes. Strengths Provides insights into features extracted from 3D operators particularly the strengths of random features. Introduces a neural correspondence prior and an early stopping mechanism based on cycle loss. Achieves stateoftheart results in point cloud matching nonisometric shape matching part segmentation transfer and 3D keypoint detection. Wellwritten and easy to understand. Weaknesses Raises several questions regarding the implications and limitations of the method. Some aspects of the second stage of the network architecture could be better explained. Theorem 1 may not be as significant as it could be and the assumptions of smooth encoders and injectivity should be addressed. The term \"noise\" is used somewhat vaguely and different types of noise should be clarified. The paper divides into two main messages which can make it difficult to follow. Despite these weaknesses the paper presents valuable insights and serves as a thoughtprovoking contribution to the field. Further work could focus on clarifying certain details addressing the questions raised and providing a more comprehensive analysis.", "Paraphrased Summary: This research presents a technique for predicting 3D correspondences based on the concept of deep priors. It utilizes two key ideas: 1. Randomly initialized diffusion network features initially outperform spectral and SHOT descriptors. When combined with functional maps they even surpass learned methods. 2. During training of a network to predict features that match noisy correspondences the quality of the features obtained early in training exceeds that of the noisy target. This leads to a method for predicting correspondences that involves extracting features from a randomly initialized network and training a second network to predict them. Training is stopped early based on a cycle consistency loss. Strengths and Weaknesses: Strengths: Novel use of neural priors for shape correspondence. Weaknesses: Absence of evaluation on FAUST dataset for comparison with other methods. Disorganized structure with important method details moved to Section 5. Unrelated and unclear Theorem 1. Excessive use of abbreviations. Illegible Figure 2 in print. Questions: FAUST evaluation requested.", "Paraphrased Summary: This paper presents a method for shape matching without supervision. It involves generating noisy maps from known methods then enhancing them by extracting pointwise features from the noisy maps. These features are used to create a new map that is more accurate than the original noisy maps. The methods effectiveness is supported by experimental results. Paraphrased Strengths and Weaknesses: The reviewer acknowledges the novelty and interest of the work. However there are some concerns: 1. The loss function used in the method may not be differentiable with respect to network parameters potentially hindering optimization. 2. The paper mentions the need for early stopping to train the pointwise feature extraction network. However it appears that the early stopping criterion is only defined in a later section. Paraphrased Minor Presentation Issues: 1. The notation used in Theorem 1 is unclear. It is unclear if N(M)x refers to the mapping N restricted to M applied to x or if it has a different meaning. 2. The fonts in the figures are small and hard to read. Paraphrased Questions: 1. Can the feature extraction and map matching process be repeated iteratively to improve accuracy further 2. What are the key differences between the NCP method (Section 4.1) and the test time denoising method (Section 4.2) Does Section 4.2 train an additional network or use the one trained in Section 4.1 3. Is the purpose of Section 4.2 to propose an early stopping criterion for NCP"], "lIeuKiTZsLY": ["Summary This research focuses on discovering causal relationships in linear models with unobserved (latent) variables. The authors propose an algorithm to identify the underlying causal graphs Markov equivalence class under specific graph structure assumptions. They show that the recovered graph is asymptotically accurate according to particular graphtheoretic operations. Strengths Technical novelty and significant contributions to latent causal graph discovery. Improved theoretical guarantees compared to previous works. Clear and wellpresented algorithms. Weaknesses Poor writing with misleading statements and definitions. Insufficient comparison to related work particularly [3]. Lack of explicit mention of the linear assumption in the introduction. Unclear definition of \"pure children\" and its application to complex graph structures. Difficult interpretation of the assumptions in Condition 1. Mildness of the graphtheoretic assumptions not adequately explained. Absence of standard error metrics like Structural Hamming Distance. Questions How are the latent graphs constructed in the synthetic examples What is the running time of the algorithm Is there a conceptual difference between this work and [2] Additional Notes The paper builds on existing ideas but introduces technical advancements in the algorithm. It is not clear why the authors claim that previous works do not handle hierarchical structures when [2] and [3] both do. The assumption of a hierarchical graph structure should be explicitly stated.", "Paraphrased Summary: This paper presents a novel approach for causal discovery in the presence of hidden (latent) confounders. The approach assumes a hierarchical causal structure and leverages rank deficiency constraints to infer both the causal graph and certain characteristics of the latent variables. Experimental results demonstrate the methods effectiveness. Paraphrased Strengths and Weaknesses: Strengths: Clear mathematical formulation Examples for reader understanding Sound theoretical analysis Weaknesses: Limited experimental evaluation with simple toy data Computational efficiency not extensively evaluated Paraphrased Questions: 1. How prevalent are hierarchical causal structures in realworld settings 2. Can Definition 2 be omitted without compromising the papers coherence 3. How can we ensure that Phase 2 correctly recovers the causal graph considering that not all possible atomic covers are examined 4. What mechanisms guarantee the identification of all colliders based on Lemma 6", "Paraphrase: The authors propose a hierarchical graph modeling technique that guarantees identifiability of latent structures if certain assumptions are met. Simulations demonstrate the superiority of their method. While the method is conceptually sound its practical utility and advantages compared to existing techniques are not clearly presented. Questions for Authors: In what specific scenarios is the proposed method advantageous for modeling observational data When would it be preferable to use this method for model selection compared to others What benefits does the linear hierarchical structure provide over other latent variable models Is the method computationally efficient compared to similar techniques Are there any drawbacks or tradeoffs associated with the increased identifiability guarantees Could the authors provide clearer explanations and definitions in the paper to improve its comprehension"], "mjUrg0uKpQ": ["Paraphrase: The paper proposes a technique called \"multimodal crossattention\" that aligns visual and text representations. This alignment is achieved through unsupervised training on imagedocument pairs. The model is then evaluated on a zeroshot learning task and subjected to an interpretability study. Strengths: The interpretability study shows that the model successfully aligns modalities. The model outperforms existing methods. The concept of \"Image2Document attention\" appears intuitive using an asymmetrical attention mechanism to align image and text representations. Weaknesses: The motivation for the research is briefly described late in the paper making it difficult to understand the practical applications. The training process is unclear including how outputs are classified how images are aligned with text and the source of class information. The processing of text data involves converting words into GloVe embeddings and then passing them through an MLP layer but the rationale for this approach is not explained. The model uses static word embeddings despite the possibility of distribution shifts in input representations. The choice of asymmetrical attention is not justified and other approaches like coattention or input merging could be explored. Table 1s caption lacks clarity regarding the source and representation of semantic embeddings. The papers presentation hampers understanding and reproduction of the models training process. Questions: 1. How is the training process conducted 2. How are datasets constructed particularly with respect to class information 3. What is the purpose of text processing with GloVe before using Transformers", "Paraphrased Statement Summary This study presents a more realistic approach to zeroshot image classification where each class is described by a single online document. To handle noisy and fragmented text documents the authors introduce a transformerbased framework for jointly encoding images and text mapping them to a shared embedding space. A crossmodality attention mechanism helps suppress noise. Comprehensive experiments demonstrate the effectiveness of the proposed framework. Strengths Proposes a transformerbased framework for ZSL. Introduces an \"ImagetoDocument\" (I2D) module for focusing on relevant image regions. Achieves favorable results on benchmark datasets. Interpretable results allow for understanding the learned text embeddings. Provides potential for further improvement in existing ZSL methods. Weaknesses Lacks detailed discussion on model complexity. Crossmodality attention while a contribution has limited novelty. Requires clarification on the fairness of using online documents compared to other ZSL frameworks. Questions for Authors Provide implementation details for compared methods using different noisy text sources. Explain the FLOPs (floatingpoint operations) of the proposed models relative to other SOTA models. Explore the possibility of feeding all tokens (visual and semantic) jointly to the transformer instead of using separate transformers and crossmodal attention.", "Paraphrased Summary: The authors suggest using attentionbased models for zeroshot learning where knowledge is gained from text documents and other sources (e.g. Wikipedia birdrelated websites). Their approach involves: Two Transformer models: one for images and one for text. Two components: I2D Attention: Merges image and text sequences using attention resulting in a \"local\" matching score. I2D Global: Uses classification tokens from both modalities to generate a \"global\" matching score. The model optimizes for both local and global matching but only the global score is used for classification. The authors evaluate their model on standard ZSL datasets (AWA2 CUB FLO). They find: Their learned text features (I2DEmb) outperform existing ones when used with other ZSL models. Their complete model performs well compared to other ZSL methods. The model provides interpretability through attention maps. Strengths: Simplicity and low hyperparameter requirement. Strong experimental results for both text features and the complete model. Comprehensive ablation studies and experiments. Interpretability through attention maps. Weaknesses: Possible suboptimal settings for comparison against other ZSL models. Lack of results using attribute data which may outperform text features. Ambiguity in attention direction. Limited qualitative evaluation and lack of failure case analysis. Questions: How are different input features (e.g. GloVe) used in the Document Transformer Are either the Image or Document Transformers finetuned How are different document text lengths handled computationally How does the model compare computationally to other ZSL methods How are soft attention maps produced Are there significant differences between local and global predictions What is the precise definition of \\hatx\\pa in Eq. 4 Suggestions: Include key ablation observations in the main paper. Terminology: Replace \"document embedding\" with \"encoding\" or \"representation.\"", "Paraphrased Summary: Researchers introduce a method for connecting an image with its text description capturing both visual and textual features. This technique assists in classifying images containing objects unknown during training using only their text descriptions (zeroshot learning). The proposed model consists of two components: ImageText Similarity Scoring: This stage calculates the similarity between image and text embeddings. Combined Transformer: This component aligns image features (queries) with text features (keys) using a transformer model. Experiments using standard zeroshot learning datasets demonstrate: The models accuracy surpasses existing methods. It can classify images with unseen objects based solely on text descriptions. The model performs effectively with basic text embeddings (Glove) questioning the necessity of more sophisticated embeddings like longformer. Strengths and Weaknesses: Strengths: Novel combination of visual queries and textual keys in a transformer model. Stateoftheart performance especially with simple embeddings. Accessible training requirements (single GPU). Weaknesses: The reviewer lacks expertise in zeroshot learning to raise further questions."], "rjbl59Qkf_": ["Paraphrase: This paper investigates how reweighting samples enhances the resilience of deep learning models (ERM) against data distribution shifts. It shows that constantly updating sample weights doesnt boost ERM performance in overparameterized settings. As similar findings exist for fixedweight methods this work mainly confirms that gradient descent in overparameterized settings converges to an interpolator irregardless of sample weights. Strengths: Provides a theoretical understanding of reweighting within a broader framework including iterative weight updates. Weaknesses: Since its known that reweighting is ineffective in overparameterized settings the generalized results dont offer significant new insights. Questions: Discuss the technical complexities of deriving theoretical results for iteratively updated sample weights to clarify the originality of this work. Add deeper analysis in the regularized case to illuminate how iteratively reweighted samples improve robustness. This would enhance the value of the study.", "Paraphrase: Summary: This study investigates the Generalized Reweighting (GRW) approach which adjusts loss terms during model training to enhance robustness to distribution changes. The authors demonstrate that for regression and classification tasks using linear models or wide neural networks GRW has minimal impact on optimization outcomes compared to the standard Empirical Risk Minimization (ERM) approach. This suggests that GRW despite its popularity does not provide advantages over ERM in these scenarios. Additionally the authors prove that even with regularization that sacrifices training loss by epsilon GRM only slightly modifies the minimizer compared to ERM (by the order of sqrt(epsilon)). Their findings indicate that alternative strategies to GRW may be necessary for developing models that are robust to distribution shifts. Strengths: Clear and intuitive explanations including for technical concepts. Theoretical support for empirical observations of GRWs shortcomings. Considers both linear models and wide neural networks. Corrects and expands on the analysis of Lee et al. (2019). Identifies potential alternative approaches for robust training. Weaknesses: Lacks explicit algorithms to overcome negative results. Asymptotic analysis of neural network behavior with infinite width may not align with practical usage. Interpretation of regularization results is challenging due to uncertainty about the necessary level of change for true robustness. Questions: Comparison with theoretical negative results presented in [HNSS18]. Author insights on the minimum difference in optimization outcomes from ERM required for distributionally robust training. Exploration of whether the regularization impact is insufficient for achieving robustness.", "Paraphrase: Summary: This study analyzes why generalized reweighting techniques fail to outperform empirical risk minimization (ERM) in distribution shift scenarios. It focuses on overparameterized linear models and wide neural networks optimized through gradient descent. The study demonstrates that generalized reweighting and ERM yield the same solution due to the implicit bias of gradient descent in overparameterized models. It suggests that these methods should perform similarly in practice. The analysis primarily leverages neural tangent kernel theory for wide neural networks. Strengths: Investigates the intriguing finding that reweighting methods often perform no better than ERM in distribution shift. Presents formal theoretical results to partially support its claims. Weaknesses: The scope is limited as prior studies have examined linear models. The analysis of wide neural networks while using neural tangent kernel theory has a disconnect from practical applications. The findings are not particularly surprising as the convergence of generalized reweighting and ERM to the same solution stems from the implicit bias of gradient descent in overparameterized models. The study could have emphasized this aspect more. Practical techniques like regularization and early stopping are overlooked. Questions: What unique properties does the interpolator that reweighting and ERM converge to possess How does distribution shift affect ERMs suitability What is the generalization performance of the learning algorithms considered in this study as this paper primarily focuses on optimization properties", "Paraphrased Statement: Summary: This study reveals that widely used methods like importance weighting and Distributional Robust Optimization (DRO) variants do not significantly enhance performance against dataset drift compared to Empirical Risk Minimization (ERM). The authors introduce Generalized Reweighting (GRW) which encompasses these methods as special cases. For overparameterized linear models or wide neural networks GRW performs similarly to ERM. Additionally small amounts of regularization that do not degrade training accuracy do not improve performance. Strengths and Weaknesses: Motivated by empirical observations the authors provide theoretical evidence that popular approaches for mitigating dataset drift fail to improve over ERM. They also propose potential strategies for addressing this issue. This work could shift the focus of distributionally robust approaches. Questions and Considerations: The theoretical analysis relies on linear models. To generalize to neural networks the study employs Neural Tangent Kernel (NTK) theory which approximates networks as linear models. However there are limitations to NTK theory: NTK networks require infinite width which is not practical. NTK networks are typically simple fullyconnected architectures. The optimization algorithm must be stochastic gradient descent (SGD). Assumption requires Lipschitz continuous firstorder activation functions which is not true for common activations like ReLU. These simplifying assumptions raise concerns about the validity of the theoretical analysis for realworld neural networks."], "kZnGYt-3f_X": ["Paraphrased Statement: Summary This research presents a distillation approach for bridging different image dimensions. The authors leverage knowledge from 3D convolutional neural networks (CNNs) to enhance 2D CNN performance. They map highdimensional teacher model representations into 1D curves using Hilbert curves which are continuous spacefilling curves. The student network is trained with these mapped representations. To optimize performance they introduce a variablelength version of the method that dynamically adjusts the curve stride. Experimental results show that their approach surpasses existing crossdimensional distillation methods on two datasets. Strengths and Weaknesses Positive Aspects: Novel and meaningful approach Wellwritten and easytounderstand paper Clear experimental evaluation demonstrating performance improvements Ablation studies provided Negative Aspects: Lack of comparisons to regular distillation from larger 2D networks Unclear impact of feature extraction layer selection and matching between layers No discussion on computational complexity of Hilbertbased matching generation Limited evaluation on only two small datasets Questions Can this method be compared to traditional distillation approaches from larger to smaller 2D networks How does the choice of layers impact distillation effectiveness What are the computational implications of Hilbertbased matching during training Is there potential for evaluation on larger and more diverse datasets", "Paraphrased Statement: Summary: This research introduces a new method using Hilbert curves to compress large convolutional neural networks (CNNs) into smaller more efficient models. The proposed method known as Variablelength Hilbert Distillation (VHD) can adapt to different activation maps generated by various inputs. Unlike traditional Hilbert curves which fix the mapping for a specific scale VHD offers flexibility. Experiments on two separate datasets show that VHD performs better than existing distillation techniques. Strengths: Novel Approach: VHD uses Hilbert curves for distillation allowing 3D networks to enhance the performance of 2D networks without requiring significant computational overhead. Flexibility: VHD extends the proposed Hilbert Distillation method to provide greater flexibility in handling activation feature maps and stride lengths. Improved Performance: Experimental results demonstrate that VHD outperforms existing methods. Weaknesses: Poor Writing and Presentation: The papers writing and presentation are substandard making it difficult to understand the proposed method and its results. Unclear Figures: Figures 35 lack essential legends and axes labels hindering comprehension. Writing Errors: Grammatical errors further detract from the clarity of the paper. Additional Questions (from Weakness Section): Experimental Setup Clarification: a) Is ActivityNet also randomly split b) Why are different traintest ratios used for the two datasets c) How were hyperparameters (alpha epochs batch size etc.) selected", "Paraphrased Statement: This study introduces a distillation technique that converts 3D neural networks into 2D networks using Hilbert curves. The 2D networks are encouraged to focus on important features by varying their length. This method was tested on two datasets alongside other knowledge distillation methods. It consistently outperformed the baselines on both datasets. Strengths: Novel use of Hilbert curves for crossdimensionality distillation. Thorough experimentation and comparison to existing methods. Significant performance improvements over baselines. Comprehensive ablation analyses. Weakness: None identified. Curiosity: The computational cost of constructing the Hilbert curve and its impact on model training.", "Paraphrase: Summary: The paper utilizes the Hilbert curve to investigate 3D knowledge distillation for video analysis and medical imaging. The authors present Hilbert Distillation (HD) to extract structural knowledge from 3D model intermediate features. They also develop Variablelength Hilbert Distillation (VHD) to adjust the Hilbert curves walking distance. The study demonstrates the methods effectiveness compared to existing techniques. Strengths and Weaknesses: Strengths: HD extracts structural information from 3D feature maps. VHD transfers knowledge efficiently through Hilbert curves. Favorable results compared to baselines. Weaknesses: Lack of emphasis on technical novelty. Unclear justification for Hilbert curves utility. Unverified claims regarding the fixed organ distribution in medical imaging (Question Q3). Insufficient stateoftheart comparisons. Questions: Q1: The papers originality may be questioned due to its reliance on previous works. Q2: Insufficient justification is provided for the assumption of fixed organ distribution in medical imaging. Q3: Visualization of selective feature map activation would enhance understanding. Q4: A more thorough explanation of Hilbert distillations mechanism is requested. Q5: Lack of comparison with contemporary knowledge distillation methods. Q6: Additional benchmarks are needed for evaluating the methods robustness."], "xaWO6bAY0xM": ["Paraphrased Statement: The paper investigates robust neural networks that have Lipschitz continuity. First it demonstrates that standard Lipschitz networks (feedforward networks with Lipschitz activations and unitnorm weight matrices) have limitations in approximating Boolean functions due to their robustness radius being bound by 12d. This limitation is then extended to general Lipschitz functions by showing that the kth largest component of a vector cannot be approximated by a standard Lipschitz network. Next the study examines nonstandard GroupSort and Linfinity architectures that have been identified as universal Lipschitz function approximators. The authors highlight that a simplified GroupSort architecture (MaxMin) is not a universal approximator and that Linfinity architectures can represent certain Boolean functions and order statistics. Inspired by these findings the paper introduces a new architecture called SortNet which encompasses both GroupSort and Linfinity networks. SortNet is a universal Lipschitz function approximator. A practical version of SortNet is also proposed using an unbiased estimator instead of sorting operations. This simplified SortNet retains its universal approximation capabilities. Numerical experiments demonstrate that the SortNet architecture performs consistently better or significantly better than Linfinity and GroupSort networks respectively.", "Paraphrase: The study presents a generalized version of recent variants of \\ell\\inftyLipschitz networks (guaranteeing expressivity) that are computationally efficient and robust. Despite the advantages of these networks previous designs have faced limitations in expressivity and training difficulties. The authors provide stronger negative results on approximating Lipschitz functions using standard Lipschitz neural networks. They also demonstrate the limited expressivity of MaxMin networks. The proposed architecture is computationally expensive but the authors introduce a stochastic approximation that reduces complexity. The approach is evaluated against stateoftheart baselines showing promising but inconclusive results. Notably it can be trained effectively on the TinyImagenet and Imagenet datasets which posed challenges for earlier methods. Strengths and Weaknesses: Originality: Combines GroupSort activation and \\ell\\inftydistance networks into a more expressive architecture. Proves that the more efficient MaxMin network has limited expressivity. Demonstrates the approximation capabilities of \\ell\\inftydistance networks for boolean functions. Quality: Wellstructured and easy to understand proofs. However the experimental results lack confidence intervals potentially overstating the significance of improvements over baselines. Significance: Addresses the longstanding problem of certified robustness and highlights the importance of understanding its limitations. Presents a general and expressive Lipschitz network architecture with practical training feasibility. Theoretical advantages of SortNet over \\ell\\inftydistance networks are unclear. Weaknesses: Missing references for computing Lipschitz constants. Ambiguous use of \"far from satisfactory\" in describing previous results.", "Paraphrase: This research introduces novel theoretical insights and a new architecture for designing neural networks that adhere to Lipschitz constraints (based on the infinity norm). The theoretical findings challenge previous approaches and support the effectiveness of the authors method. They also present a novel stochastic approximation technique to facilitate efficient model training without significantly compromising expressiveness. Overall the paper provides a detailed and wellwritten presentation with significant theoretical and empirical advancements. The theoretical results establish upper and lower bounds on approximation errors. They highlight the limitations of standard Lipschitz neural networks in achieving robustness with increasing input dimensions especially for simple Lipschitz functions. The paper also offers completeness results for distance nets and bounds for MaxMin networks that indicate the need for increasing depth with input dimension. Empirically the proposed SortNet architecture demonstrates excellent performance surpassing the accuracy of other certified methods. It achieves both clean accuracy and certified robustness with faster training and bound computation times. Strengths: Comprehensive theoretical analysis with strong upper and lower bounds Identification of order statistics as a key function class Innovative SortNet architecture with a clever stochastic approximation Thorough empirical evaluation with appropriate metrics Weaknesses: Limited to the infinity norm and fullyconnected networks The implications for other norms and convolutional layers are not addressed Empirical results may not resonate with the broader deep learning community Additional Questions: Explain the significance of \"Boolean circuit theory\" in understanding the difference in depth scaling between MaxMin and distance nets. Discuss the implications of fixing the weights in SortNet and the potential benefits or drawbacks of learning weights versus using order statistics alone.", "Paraphrased Summary: Problem and Motivation This study investigates the connection between certified robustness and Lipschitz continuity in the l\u221e setting. Unlike the l2 case Lipschitz networks havent performed as well for l\u221e. However recent research by Zhang et al. developed a 1Lipschitz network with strong robustness. This paper aims to understand this phenomenon. Methodology and Findings Methodology The authors explore this question using Boolean functions demonstrating limitations in training certifiably robust Lipschitz networks in the l\u221e setting. Understanding Performance of Lipschitz Networks The authors analyze how recent Lipschitz networks from Zhang et al. achieve good results. Designing Enhanced Lipschitz Networks Using the aforementioned insights the authors propose new networks. Experiments The proposed SortNet shows comparable performance to recent methods demonstrating its potential. Strengths and Weaknesses Strengths: Solid technical analysis using Boolean functions. Weaknesses: Dense and challengingtofollow writing in the methodology section. Questions about the relevance of certified robustness as it may not scale up to practical applications. Questions (Suggestions for Writing Improvement) Divide the paper into distinct sections with clear headings. Link theorems to the overall theme of the study and provide proper motivation for each theorem in the methodology section."], "mzze3bubjk": ["Paraphrase: Researchers have investigated the computational complexity of hypothesis testing in planted models where there is a \"null\" distribution and a \"planted\" distribution with a hidden signal. The authors introduce a new criterion called the FranzParisi (FP) criterion which relies on geometric properties of the distributions. They show that FP hardness is related to lowdegree (LD) hardness in Gaussian additive and sparse planted models. In Gaussian additive models FP hardness implies barriers that prevent Markov Chain Monte Carlo (MCMC) algorithms from fully exploring the configuration space efficiently. This means that lowdegree hardness implies a failure of MCMC methods. The researchers focus on the sparse linear regression model and derive new LD computational lower bounds using the FP criterion. Strengths and Weaknesses: Originality: Novel concepts and techniques including the FP criterion and its connection to LD hardness and MCMC hardness. Quality: Clear assumptions and seemingly correct proofs. Clarity: Wellwritten with detailed supplementary materials. Significance: Links algorithmic hardness to statistical physics properties of computational problems. Questions: The paper hints at connections between FP hardness and overlap gap properties and between hypothesis testing and estimation settings which could be explored further.", "Summary This work aims to prove that a class of statistical problems is hard for local Markov chain Monte Carlo (MCMC) algorithms. The authors introduce a new algebraic quantity called the FranzParisi (FP) criterion and establish its connections to hardness against MCMC algorithms. They also provide an application of the FP criterion in sparse regression. Technical Summary For hypothesis testing of Gaussian additive models with iid Gaussian null distribution and a planted spike prior the authors show that lowdegree hardness (LD) implies FP hardness. FP hardness in turn implies a freeenergy barrier in the solution landscape which leads to hardness against local MCMC algorithms. The main assumptions for these results are: Gaussian iid entries in the null distribution Symmetry properties of the signal prior The FP criterion is defined as a modified annealed potential that captures the overlap between the posterior distribution and the ground truth. Main Results For Gaussian additive models LD hardness implies FP hardness which implies hardness against local MCMC algorithms. The FP criterion can be used to obtain computational phase transitions for sparse regression in specific parameter regimes. Limitations The FP criterion fails to predict sharp thresholds for estimationrecovery problems compared to hypothesis testing. It does not work for boolean settings or planted subgraph problems. Strengths and Weaknesses Strengths: Conceptual interest and potential for further research. Bridges the gap between statistical physics and lowdegree tests. Recovers known thresholds for the spiked Wigner model of PCA. Weaknesses: Assumes strong symmetry properties. Requires significant effort to bound the FP criterion for other problems. May not be readily applicable to other problems due to technical complexity. May not have a good fit for the NeurIPS conference due to its technical nature and length.", "Paraphrased Statement: Summary: The paper introduces a \"FranzParisi criterion\" (FP) for assessing the computational complexity of certain hypothesis testing problems. This criterion involves evaluating the second moment of the likelihood ratio under specific conditions and examining the behavior of an annealed approximation to the FranzParisi potential near zero overlap. The authors hypothesize that if the FP criterion meets certain thresholds it suggests computational hardness for strong or weak detection in the testing problem. Supporting Evidence: The authors provide several rigorous results to support their claim: The FP criterion can be related to the lowdegree likelihood ratio in Gaussian additive models including spiked matrix and tensor problems with Gaussian noise. Under certain assumptions FP hardness implies a barrier in the posterior distribution of the planted signal leading to lower bounds on mixing times for MCMC algorithms with local moves. The equivalence between FP hardness and lowdegree hardness is shown for planted sparse models under a symmetry assumption establishing hardness in a sparse linear regression model. Additional Insights: The paper explores the connections between the annealed FranzParisi potential the overlap landscape and the lowdegree likelihood ratio. It provides new geometric and algebraic approaches for analyzing hardness in hypothesis testing problems. Limitations: While the paper offers valuable insights it remains unclear whether the FP criterion fully captures the \"physics intuition\" for hardness in all cases as it is based on an annealed potential rather than the true quenched potential and only considers local properties. Questions for Authors: Can more discussion from Section 1.3 of the supplement on the FranzParisi potential be incorporated into the main text to provide a clearer understanding of Remark 3.3 and Definition 3.2 Clarification on the typographical errors pointed out in the document."], "nrksGSRT7kX": ["Paraphrased Statement: The authors address the general Offline RL challenge with a modelbased approach. They propose RAMBO an algorithm that trains a model adversarially to minimize the performance estimate of the learned policy. This builds on their earlier theoretical work. Experimentation is limited to MuJoCo and AntMaze environments. RAMBO performs well in MuJoCo but falls short of modelfree baselines in AntMaze. Strengths: RAMBO is a novel and promising algorithm for modelbased Offline RL. Performs well in benchmark experiments. Clear presentation. Weaknesses: Formal theory lacks rigor. Theoretical claims may overstate capabilities. Lack of discussion on PAC bound constants. Confusing connection to literature in Equation 1 and 3. Imprecision in Proposition 2s formalization. Limited empirical validation. Absence of experiments on simpler and more complex environments. Questions: Why use MLE loss instead of TV loss in Equation 9 How does Problem 1 differ from pessimistic reward modification Minor Points: Consider \"nearoptimal\" instead of \"nearoptimality\" in Line 1. Use \"enforce conservatism\" instead of \"achieve conservatism\" in Line 6. Avoid using \"conservatism\" for general datasetbased regularization reserve it for policy regularization close to the behavioral policy in contrast to pessimism.", "Paraphrased Statement: This paper introduces RAMBO a novel offline modelbased algorithm that uses a twoplayer zerosum game to optimize models and policies. RAMBO trains models with an additional objective of minimizing the value function to ensure conservative policy optimization. The paper provides a theoretical performance guarantee and demonstrates lowerbounds for the learned value function. Experiments show that RAMBO performs competitively. Strengths: RAMBOs approach is innovative and addresses the distributionshift issue in offline modelbased RL. The theoretical analysis provides a strong foundation for the algorithm. Weaknesses: The computation of the gradient (Eq. 5) requires clarification especially when dealing with terminal states. RAMBO exhibits sensitivity to hyperparameters potentially affecting model optimization stability. The computational cost of the model training phase may be high. Empirical studies do not conclusively demonstrate that RAMBO learns a lower bound of the true value function. Questions: How is the gradient computed considering terminal states in Eq. 5 Can the sensitivity to hyperparameters lead to unstable model optimization What is the computational cost of RAMBO compared to existing methods Do empirical studies confirm that RAMBO learns a lower bound of the true value function", "Paraphrase: This paper solves the extrapolation error issue in offline RL by formulating the problem as a game between the agent and the environment. The environment is designed as a neural network thats trained with two objectives: maximum likelihood estimation (MLE) and a conservative objective focused on minimizing the agents state value. Previous work has shown that this setup provides performance guarantees (PAC). Experiments indicate some improvement on specific D4RL tasks. Strengths and Weaknesses: Strengths: Clear and wellorganized paper with a compelling argument for using adversarial training to reduce value overestimation. Weaknesses: The baseline methods (modelbased and modelfree) are outdated. The analysis of the proposed method (RAMBO) is too brief especially considering its different performance on mediumreplay and mediumexpert datasets. Questions: Was COMBO reimplemented and evaluated on version 2 of the dataset unlike the original paper How were the COMBO results in this paper obtained given their consistency with the original paper except for a few specific tasks", "Summary The article introduces RAMBO a novel approach that enhances offline reinforcement learning by modifying the learned models transition dynamics using adversarial methods. RAMBO is theoretically grounded with PAC (Probably Approximately Correct) guarantees. Experiments show promising performance compared to stateoftheart offline RL baselines on the D4RL benchmark. Strengths Clear and wellwritten presentation Simple and practical solution Weaknesses Similarity to COMBO another regularization method Lack of indepth comparison to clarify the distinctions and advantages of the two approaches Questions None identified by the reader."], "x2WTG5bV977": ["Summary Paraphrase: The study investigates a previous claim that a basic transfer learning approach surpasses advanced metalearning strategies in certain situations. It proposes that given equal conditions (e.g. architecture optimizer) MAML and transfer learning techniques exhibit similar performance when the variation in metatraining tasks is limited. Strengths Paraphrase: Addresses a critical question regarding the distinctions between transfer learning and metalearning techniques. The task diversity metric is not novel but its relevance to capturing task diversity requires further exploration. Weaknesses Paraphrase: Lacks insightful analysis and conclusive findings. Only MAML is considered in the comparison excluding other promising metalearning methods. Empirical results are limited to low task diversity conditions excluding scenarios with high task diversity. The practical implications of task diversity for method selection are not discussed. Questions Paraphrase: Why was only MAML selected for analysis when other metalearning methods exist Can task diversity serve as a criterion for choosing among various metalearners", "Paraphrase: Summary: This research aims to compare the effectiveness of transfer learning and metalearning (specifically MAML) through empirical evaluations. These learning approaches are examined based on: Dataset diversity: Amount of variation within the tasks used. Feature similarity: Resemblance among task features in different layers of the neural networks employed. Model size: Complexity of the neural networks used. Findings indicate that MAML and finetuning a pretrained model yield similar performance. Strengths: Compares metalearning and transfer learning providing insights into their respective advantages. Proposes a novel metric \"diversity coefficient\" to quantify dataset characteristics. Provides extensive literature review and analysis. Weaknesses: Lacks coherence in connecting key contributions. Diversity coefficient relies heavily on a specific probe network and may not capture outliers. Evaluation relies on classification accuracy from a limited number of tasks. Questions: Can dataset diversity be calculated informatically using entropy Could model complexity or calibration error provide more meaningful insights for comparing metalearning and transfer learning", "Paraphrase: This article proposes a new metric to measure the diversity of datasets used for fewshot learning. The authors evaluate two popular fewshot learning methods (MAML and transfer learning) based on various criteria using two different datasets. They demonstrate that transfer learning may underperform MAML when the dataset diversity is limited. Strengths: The article is accessible and straightforward. It presents a unique \"problemcentric\" approach to comparing MAML and transfer learning. The concept of dataset diversity is highlighted as a crucial factor in evaluating models supported by empirical findings. The proposed diversity coefficient is easily understandable. Weaknesses: The empirical evaluation could be more comprehensive. Certain sections lack clarity. Minor typos are present. Questions: 1. The empirical evaluation primarily focuses on classification tasks using ConvNet models. Including more tasks and diverse architectures (e.g. transformerbased) would strengthen the conclusions. Alternatively the scope of the conclusions should be limited. 2. The definition of tasks in Table 1 is unclear. Are the 500 fewshot tasks derived from the original dataset Do they include distinct classes How would changes in task definitions affect the diversity coefficient 3. Section numbers are missing in lines 292 and 396.", "Paraphrased Summary: This study challenges the common assumption that transfer learning outperforms metalearning in fewshot learning. It presents a new metric \"diversity coefficient\" to quantify task diversity using vector representations from Task2Vec. Experiments show that transfer learning and metalearning perform similarly on lowdiversity datasets. Strengths: Novel claim that challenges established knowledge. Welldesigned experiments support the claims. Weaknesses: Lack of a meaningful conclusion and explanation for the relationship between diversity and performance. Novelty of the diversity metric lies primarily in prior work on Task2Vec. Presentation quality needs improvement with typos missing references and lowquality figures. Questions: Is Figure 2 intended to demonstrate differences in model weights between MAML and USL How does diversity impact the performance of MAML and USL Why does USL outperform MAML on highdiversity datasets despite the claim of similar performance on lowdiversity ones Is the proposed diversity metric a reliable indicator of true task diversity Are there experimental justifications to support its validity"], "nZRTRevUO-": ["Paraphrased Statement: This paper addresses the challenges of solving highdimensional Bayesian optimization problems using latent space deep autoencoder (DAE) models focusing on structured input data. Previous approaches using DAE models have faced issues with high dimensionality even after projecting the input into a latent space. The novelty of this work lies in incorporating an efficient trust region into the Bayesian optimization step to effectively handle this highdimensionality issue an aspect that had not been explicitly addressed in earlier research. Empirical evaluations demonstrate that this approach significantly outperforms existing latent space Bayesian optimization algorithms on various complex realworld benchmarks. This paper emphasizes the importance of both improving optimization methodologies and deep generative models for effective latent space Bayesian optimization. Strengths and Weaknesses: Strengths: Addresses the mismatch between the trust region in latent space and the trust region in the original structured input space. Proposes an efficient formulation of trust regionbased local optimization for highdimensional latent space Bayesian optimization problems. Highlights the discrepancy between changes in latent space and changes in the original space and shows how trust regionbased Bayesian optimization does not explicitly account for this. Introduces joint inference allowing simultaneous optimization of the Gaussian process and the variational autoencoder parameters. Demonstrates significant performance improvements on LOLBO (SELFIES) and LOLBO (JTVAE) models. Weaknesses: Lacks clear novelty and uniqueness as it combines existing advancements in LSBO trust region BO and highdimensional BO. Requires a larger initial dataset for joint learning of multiple parameters and deep generative models but does not provide a comparative analysis with other methods with less initial supervised data. Simultaneous parameter updates require heuristics and the paper does not provide convergence guarantees. PostRebuttal: The authors response addresses concerns leading to an increased evaluation. Questions: 1. Please provide a comparison of the initial sample size (both unsupervised and supervised) used in this work with other methods. This is particularly relevant for cases with less supervised information. 2. Can you justify the increased sample requirement for joint inference and transformer architectures 3. Could the improvement over TuRBO potentially be attributed to the Sparse GP Augmentation aspect rather than the use of a single trust region", "Paraphrased Statement: Summary: This research explores a technique for optimizing \"black box\" functions with structured inputs. The approach leverages Bayesian optimization (BO) within a latent space. A deep autoencoder model is used to transform the structured inputs into continuous latent embeddings. However the high dimensionality of the latent space presents a challenge. To address this the paper presents a trust regionbased BO algorithm in the latent space. It assumes a mismatch between the trust region in the latent space and the corresponding region in the original structured space. A joint inference procedure is proposed considering both the Gaussian Process (GP) surrogate model and the deep autoencoder. The method is evaluated through experiments in molecular optimization and arithmetic expression tasks. Strengths and Weaknesses: The problem of optimizing structured input functions is crucial in fields such as molecular design and drug discovery. The concept of trust region BO for highdimensional spaces is not novel but the proposed method enhances the standard TurBO approach. The empirical analysis demonstrates the methods effectiveness. It outperforms baselines on various benchmarks. The discussion on objective metrics (e.g. logp) in molecular optimization is insightful. The literature review is extensive. The proposed method provides a practical solution for realworld applications. Questions: As mentioned above.", "Paraphrase: Summary: This paper introduces a novel fused autoencoder Virtual Graph Point (VGP) approach that significantly enhances Bayesian optimization performance for highdimensional spaces like molecule graphs. Strengths and Weaknesses: Originality: While not proposing entirely original ideas this paper effectively combines existing methods including trust region Bayesian optimization (TuRBO) and learned index point representations via Variational Autoencoders (VAE). It extends TuRBO to VGP by jointly training with a semisupervised objective. Improvements: The paper also incorporates representation enhancements through JTVAE and SELFIES which accelerate posterior evaluation and increase the likelihood of generating valid molecule representations. Quality: The paper is wellwritten and clearly defines the problem. It evaluates the approach against multiple Guacamol targets and benchmark datasets demonstrating its effectiveness beyond toy problems. An ablation study helps distinguish the contributions of joint training and representation changes. Significance: Bayesian optimization in highdimensional discrete spaces is a critical topic that remains unsolved. This paper contributes to this area with a promising approach. Questions: Regarding Equation 2 it appears that identical terms are present in the numerator and denominator. This may be an error and the correct terms should be p(q) and \\frac\\Gamma\\Phi. The paper does not compare SELFIES and JTVAE in terms of score achieved by iterations as it did for TuRBO and LOLBO. Adding a row for TuRBOJTVAE or LOLBOJTVAE to Table 1 could provide a more comprehensive baseline."], "r9b6T088_75": ["Paraphrase: This paper introduces an innovative architecture for supervised learning in spectral compressive imaging (SCI) based on the idea of unfolding. Unlike traditional methods it incorporates the acquisitions flaws (degradation and masking) explicitly into each iteration. The denoising component employs a Ushaped transformer with halfshuffle multihead selfattention allowing it to capture both local and nonlocal features. Strengths: Practical Significance: Significant improvements over existing methods. Practical Novelty: Introduces transformers as a denoising mechanism in SCI. Clear Writing: The concepts are presented concisely and effectively. Weaknesses: Limited Technical Novelty: The primary novelty lies in the use of the Ushaped transformer with halfshuffle multihead selfattention for denoising. However the concept of deep unfolding is wellestablished and transformers have been shown to outperform CNNs in denoising and lowlevel vision tasks. Questions: To strengthen the paper more ablations could be performed to justify the design choices of the Ushaped transformer. For instance the authors could consider comparing the performance of a standard multihead vision transformer to the Ushaped architecture.", "Paraphrase: Summary: This paper introduces a new Transformerbased algorithm called DAUHST for reconstructing hyperspectral images captured by the CASSI system. It consists of two parts: 1. Degradationaware unfolding framework: Analyzes the CASSI degradation pattern and adjusts the reconstruction process accordingly. 2. Halfshuffle Transformer: Captures both local and longrange dependencies in the image data. Strengths: First Transformerbased unfolding method for hyperspectral image reconstruction in snapshot compressive imaging. Novel approach with a degradationaware framework that adapts to the CASSI system. Halfshuffle Transformer combines the benefits of local and global Transformers efficiently. Outperforms existing methods with a significant margin in image quality. Comprehensive experiments and indepth analysis demonstrate the effectiveness of the proposed techniques. Clear and wellorganized presentation with reproducible code and models. Weaknesses: Visual comparisons with other methods are only included in the supplementary material. Reshape operation in Line 156 requires more explanation. Vectorization and shifting operations in Line 98 need further clarification. It would be beneficial to evaluate the compatibility of DAUHST with other CNNbased denoising networks. Questions: Why is BIRNATs FLOPS value unusually high in Table 1 Is it due to its RNN architecture", "Summary Paraphrase: This paper introduces a degradationaware unfolding framework (DAUF) that utilizes both the input image and a physical mask to estimate parameters. Each iteration stage consists of a linear projection and a denoising network operating sequentially under the MAP principle. The denoising network is based on a Transformer architecture with two branches: a local branch for capturing local dependencies and a nonlocal branch for capturing longrange dependencies. Strengths and Weaknesses Paraphrase: Strengths: Clear and logical presentation. Effective utilization of prior knowledge from the input image and mask. Local and longrange dependency extraction using a dualbranch attention mechanism. Significant performance improvements over existing deep unfolding methods. Weaknesses: Potential lack of novelty as the method combines elements from existing approaches [1 2 3]. Recommendation to include references to these inspiring papers if applicable. Questions: 1. Can both beta and alpha be learned in the denoising network instead of just alpha 2. Is HSMSA replaced by an identity operation in the first two subparts of the denoising network 3. Why is DAUHST1stg used for ablation studies without the selfattention mechanism 4. Is there a comparison of computational complexity for the different attention alternatives 5. Is the same HST used for different iteration stages Are there differences between the training and testing phases", "Summary This work proposes a hyperspectral image reconstruction method combining a deep unfolding process and a Halfshuffle Transformer (HST). The deep unfolding technique calculates image parameters providing guidance for iterative learning. The HST captures local information and longrange dependencies efficiently. The method has proven effective on various datasets and outperforms existing approaches. Key components of the method have been analyzed through ablation studies. Strengths The DAUHST method effectively combines transformers and deep unfolding for hyperspectral image restoration. Each component of the method contributes to its overall performance. Weaknesses Novelty: The combination of transformers and deep unfolding is not entirely new as a similar idea has been applied in hyperspectral image superresolution. Both deep unfolding and transformers have been used separately in hyperspectral image processing. Motivation: The motivation for the degradationaware unfolding framework is not clearly defined. The importance of \"CASSI degradation patterns\" and \"illposedness degree\" in the estimation process needs to be explained. Method Description: Details of parameter estimation in the parameter estimator are missing. The determination of iterationspecific parameters \u03bc and \u03c4 in lines 127129 is not explained. Analysis: An analysis of the impact of the number of stages (9 in this case) on performance is not included. Questions Please address the concerns raised in the \"Strengths and Weaknesses\" section."], "lblv6NGI7un": ["Paraphrased Statement: Summary and Approach: This study explores an efficient method for computing graph similarity. The key challenge lies in calculating graph edit distance which is computationally difficult. To address this recent approaches leverage graph networks for approximation. However the conventional nodelevel matching process in these networks can be timeconsuming. This work introduces a nodegraph level correspondence as a penalty term to improve the standard training framework. Results: Experiments demonstrate that the proposed penalty term (AReg) enhances both matching accuracy and inference speed outperforming various baseline methods. Strengths and Weaknesses: Strengths: Clear writing especially in the motivation section. Intuitive concept of AReg which captures interactions between nodes and graphs. Positive experimental results. Weaknesses: Unclear presentation of technical details: Line 40: Clarification needed on the role of crossgraph interactions. Line 209: How is the parameter lambda trained Limited contribution of AReg according to ablated results. Absence of sensitivity analysis on the lambda parameter. Lack of information on how lambda is learned. Incomplete presentation of ablated results which should be included in the main text.", "Paraphrased Statement: This study introduces ERIC a new framework for calculating graph similarity. ERICs key contributions include: 1. Technical Contributions: Analyzing GED (Graph Edit Distance) in embedding space and approximating groundtruth GEDs using \\gammai \\gammaj and \\\\gammai\\gammaj\\2. Incorporating Alignment Regularization (AReg) into model training based on the GED analysis. Employing a MultiScale GED Discriminator to ensure the effectiveness of the GNN encoder. 2. Framework Overview: ERIC comprises two submodules: AReg and MultiScale GED Discriminator. 3. Strengths: Clear and comprehensive presentation Novel and practical approximation of groundtruth GEDs The benefit of using multiple similarity discriminators Extensive experimentation and clear result presentation 4. Weaknesses: Limited use of similarity discriminators (NTN and l2 distance only) The approximation of GED using \\gammai \\gammaj and \\\\gammai\\gammaj\\2 may require further justification to establish its accuracy. 5. Questions: Explore additional types of similarity discriminators Analyze the approximation accuracy of the proposed method", "Paraphrase: This paper introduces a new method for calculating the similarity between graphs specifically focusing on Graph Edit Distance (GED). The main idea is based on necessary conditions that define the optimal mapping between nodes. These conditions (equations (3) and (4)) enable the computation of GED. To take advantage of these insights a neural model is proposed. This model significantly reduces the computational burden by switching from nodetonode similarity calculations (which scale quadratically with the number of nodes) to nodetograph similarity calculations (which scale linearly). Strengths: The method uses necessary conditions to reduce GED computation to nodetograph level unlike traditional neural methods that use nodetonode level computations. The model is technically sound and correct. Experiments demonstrate the methods effectiveness and efficiency achieving stateoftheart results with reduced running time. Ablation studies confirm the importance of each component in the model. The paper is wellorganized and easy to understand. Weaknesses: The motivation for the necessary conditions in Section 3 is not entirely clear. The derivation involves several steps and appears somewhat conceptual. Formal mathematical derivations to support the necessary conditions would enhance the papers credibility."], "nQcc_muJyFB": ["Paraphrased Statement: This work introduces a feature distillation method using multiple projectors differing from the traditional approach employing a single projector. It computes the average output of these projectors for knowledge distillation (KD). Two projector designs are explored and compared: a singlelayer projector and an MLP projector. Experiments on various datasets validate the methods effectiveness. Strengths: Enhancements in performance. Ease of implementation and reproducibility. Weaknesses: Limited novelty as singlelayer projectors are prevalent in KD and extending to multilayer is straightforward. Absence of references for comparison methods in tables making it challenging to assess the methods standing relative to others.", "Paraphrase: Summary: This paper introduces a feature matchingbased knowledge distillation technique. It employs a collection of feature projectors to enhance the alignment of features between the student and teacher networks. The authors demonstrate that even if the student and teacher networks have identical feature dimensions using such an ensemble of projectors can enhance distillation performance. They also offer some insights into how adding a projector can aid in the learning process of the student network. Various studentteacher network structure combinations were evaluated on benchmark datasets and the results were compared to those of other approaches. Strengths and Weaknesses: Strengths: Wellwritten paper with thorough experiments. Weaknesses: While the concept is straightforward the authors could investigate additional choices for projector ensembles such as ensembles with varying structural projectors. Questions: 1. Line 251: Table 1 is being referred to here. 2. Limitations in Projector Ensemble Training: The authors do not mention any particular limitations in the training of projector ensembles. They do not specify whether the training time is affected by the number of projectors or their depth.", "Paraphrased Statement: Summary: This paper relies heavily on experiments to show the effectiveness of feature projectors in feature distillation. Strengths: Demonstrates Superiority: The authors provide numerous experiments and comparisons to support the superiority of feature projectors in feature distillation. Weaknesses: 1. Insufficient Contributions: The papers experimental focus alone does not warrant acceptance at NeurIPS. 2. Organization and Notation Issues: The Method section is difficult to follow with unclear equations and notations (e.g. confusing \"sp\" and \"Wp\"). 3. Limited Analysis: The hypothesis (lines 125126) lacks depth and the authors should provide more comprehensive analysis of the feature projectors effectiveness. 4. Presentation Concerns: Tables would be more effective than figures for presenting experimental results (e.g. Figs. 3 and 4). 5. Missing Experiments: The authors should include experiments on CUB2002011 Cars196 and SOP datasets to comprehensively evaluate feature distillation. Questions: See weaknesses outlined above.", "Paraphrase: Summary: This study introduces a feature distillation technique using multiple projection models. These projections transform the student models features and a loss function is calculated between the transformed features and those of the teacher model. The authors analyze the benefits of projection and demonstrate the superiority of their method over existing ones. Strengths: Novel focus on projection models in feature distillation leading to a simple and applicable method. Clear writing and promising experimental results. Weaknesses: Overly simplistic ensemble strategy (simple addition of projected features). Limited discussion on projector behaviors and diversity despite claims of diversity being beneficial. Insufficient theoretical analysis: Unconvincing explanation of projector utility based on feature gradients. Lack of explanation for the superiority of multiple projectors under the feature gradient perspective. Questions: Visualization or experimental analysis of projector behaviors during and after training. Theoretical clarification on the benefits of using multiple projectors.", "Paraphrase: Summary This study introduces the novel concept of using projectors to enhance distillation performance when the feature dimensions of teacher and student networks align. The authors highlight the previously overlooked aspect of studentteacher projection and propose an effective ensemble method to further boost performance. Extensive experiments demonstrate the superiority of the proposed approach which offers a comprehensive understanding of the phenomenon in a concise and efficient manner. Strengths 1. The method is straightforward and practical. 2. Detailed experiments are provided along with code availability. 3. The paper is wellwritten and organized. Weaknesses 1. The novelty of the proposed method may be limited due to potential prior research using similar techniques. 2. The sensitivity of the ensemble size parameter requires further investigation for optimal practical implementation. 3. The ablation study lacks an analysis of the impact of different projector counts on distillation with varying feature dimensions. Questions 1. Have other researchers previously discovered or employed the use of projectors to improve distillation performance specifically in cases where student and teacher feature dimensions match 2. What is the rationale behind the initialization strategy for different projectors How does it influence the experimental outcomes"], "tWBMPooTayE": ["Summary Rewrite: The study presents a novel strategy for improving GAN training with limited data by focusing on the spectral qualities of real and generated photographs. The proposed FreGAN model converts intermediate characteristics within the generator and discriminator to the frequency domain using wavelets. These features are utilized as input for an extra discriminator branch (HFD) to heighten the discriminators awareness of image frequency ranges. An additional loss function (HFA) is used to supervise the generators frequency domain output. FreGAN outperforms existing fewshot GAN models in FID and KID scores when evaluated on various limitedsize datasets (645000 photos). Strengths and Weaknesses: Strengths: Originality: FreGAN addresses both improving GAN training with limited data and alleviating spectral disparities between real and generated images. Results: FreGAN demonstrates strong gains in FID and KID compared to FastGAN consistently across datasets resolutions and training image count. Weaknesses: Evaluation: Image diversity a crucial aspect of fewshot image synthesis was not evaluated. Metrics to quantify generated image diversity should be included to strengthen the evaluation. Claim: The paper posits that FreGAN reduces unhealthy competition between the generator and discriminator in low data settings. However the loss curves suggest otherwise indicating that the modified discriminator can more confidently differentiate real and fake images. Further analysis is required to support this claim. Clarity: The authors do not provide a clear justification for why the introduced techniques are specifically targeted for low data regimes. These techniques may also benefit GANs trained on larger datasets. Related Work: The paper lacks a thorough comparison to existing methods that address spectral biases in GANs. Such comparisons would enhance the studys significance. Evaluation: Quantitative assessment of the generated images spectral properties is missing. This aspect should be evaluated to fully assess the effectiveness of the proposed approach. Questions: Could FreGANs design prioritize memorization of training images over FastGAN due to the additional discriminator HFD and restrictive L1 loss HFA Can the authors address the remaining concerns regarding evaluation claim clarity related work and evaluation", "Summary Paraphrase: This study aims to enhance GAN stability for image synthesis in scenarios with limited data by utilizing wavelet information. The proposed method incorporates three techniques: 1. Frequency Skip Connections (FSC): Establishes direct pathways between different frequency bands in the generator and discriminator. 2. HighFrequency Discriminator (HFD): Introduces a separate discriminator specifically trained on highfrequency image components. 3. HighFrequency Alignment (HFA): Enforces alignment between the generator output and the original image at high frequencies. The proposed method demonstrates improved performance compared to baseline models on various datasets and metrics. Strengths and Weaknesses Paraphrase: Strengths: Exploiting frequency information is a promising approach for image synthesis with limited data as it provides additional guidance for the model. The method is extensively evaluated across multiple datasets and metrics. Ablation studies validate the effectiveness of the proposed design choices. Weaknesses: The paper lacks a comparison to SWAGAN a similar method that utilizes wavelet domain for image synthesis. This limits the assessment of the proposed methods novelty. The comparison to ProjectedGAN which improves upon FastGAN (the baseline used in this study) for small datasets is missing. The language used in the paper contains colloquialisms which could be improved. Questions: Provide justification for excluding SWAGAN and ProjectedGAN comparisons. Clarify what is meant by \"wavelet unpooling\" and provide a mathematical definition. Explain the color scheme used in Figure 3 and justify why the lowpass filter results in the observed color distortions. Consider including a spectral analysis of generated images to validate the methods impact on frequency awareness.", "Paraphrase: Summary: This paper introduces a powerful solution for the challenge of lowshot GAN training. It utilizes frequency data from generated images to enhance highfrequency details. The authors create FreGAN a model that combines wavelet transform a HighFrequency Discriminator (HFD) Frequency Skip Connections (FSC) and HighFrequency Alignment (HFA). They demonstrate the effectiveness of their approach through experiments on various datasets with varying sample sizes. The ablation study provides detailed insights into the functionality of each component. Strengths: 1. The method is innovative and focuses on improving highfrequency components in lowshot GAN training. 2. The comparisons showcase the models performance through both qualitative and quantitative evaluations. 3. The ablation study thoroughly examines the contributions of each component. Weaknesses: 1. Providing more visual examples or figures to illustrate the impact of each component would enhance the clarity. For instance how HFA operates or the function of FSC. 2. Some terminology clarifications are needed. Equations such as equation (7) should include definitions for symbols like \\mathcalG and \\mathcalT. Questions: 1. See weaknesses. 2. As a point of interest how would the authors incorporate their approach into StyleGAN Would the highfrequency information be obtained from the modulation signal or modulated feature maps"], "q6bZruC3dWJ": ["Paraphrased Summary The main contributions of this research are: 1. Identifying a Phenomenon: The study investigates why teachers with more knowledge can yield worse students in knowledge distillation. It concludes that this occurs due to the presence of \"undistillable classes\" in the teachers predictions. This phenomenon is observed across different KD methods datasets and teacherstudent architectures. 2. Proposed Solution: The paper introduces \"Teach Less Learn More\" (TLLM) a simple method that eliminates undistillable classes during student training. This approach leads to improved accuracy for students. Strengths and Weaknesses Strengths: 1. Clarity of writing and presentation particularly in Figure 5. 2. Demonstrated improvements in accuracy on the CIFAR100 dataset. Weaknesses: 1. Overly complex definition of \"undistillable classes.\" A simpler explanation could be: The teacher network is imperfect and makes errors. Classwise analysis identifies a subset of classes where the teachers predictions are highly incorrect (undistillable classes). TLLM avoids these classes by using groundtruth labels instead. 2. Limited evaluation on ImageNet dataset: Only KR method used as baseline. Table showing improvements of TLLM for ImageNet is missing. 3. Inaccurate baseline performance for MobileNetV2 on ImageNet. 4. Potential benefits of label smoothing for undistillable classes not explored. 5. CUB200 KD results not provided. 6. Incomplete understanding of undistillable classes and their impact on TLLMs effectiveness.", "Paraphrased Statement: Summary: This research investigates why larger teacher models often lead to poorer student performance in knowledge distillation (KD). By analyzing the data the study suggests that \"undistillable classes\" are the culprit behind this phenomenon. To address this issue the paper proposes a novel KD framework called Teach Less Learn More (TLLM). TLLM identifies and separates undistillable classes during training and allows the student model to learn these classes directly from the underlying data. Strengths: The datacentric approach provides a unique perspective on the \"larger teacher worse student\" problem improving understanding beyond earlier explanations based on capacity mismatch. The paper explores perclass accuracy in KD for the first time a concept that warrants further research. Comprehensive experiments validate the effectiveness of TLLM across various teacherstudent pairings advanced architectures (like ViTs) and both outputbased and featurebased KD methods. Weaknesses: The definition of \"undistillable class\" lacks mathematical precision. While the paper mentions classes with poor distillability a specific distillability measurement (c) is not provided making it difficult to fully grasp the implementation. The criteria for identifying undistillable classes (e.g. accuracy drop after KD) and selecting samples in Figure 1 are not clearly defined. A deeper analysis of undistillable classes is needed including their distribution and characteristics (e.g. are they challenging classes networkindependent).", "Paraphrased Statement Summary This paper suggests that removing certain \"nondistillable\" classes can enhance the performance of data distillation techniques. Strengths and Weaknesses Pros: 1. Clear and accessible writing 2. Simple and effective approach 3. Novel observation regarding nondistillable classes 4. Competitive experimental results Cons: 1. Limited exploration of the specific characteristics of nondistillable classes 2. Similarity to previous research on data distillation Questions for Further Research: Can the author identify why certain nondistillable classes are consistently observed across different experiments while others are not Within these consistent classes are the same instances consistently difficult to distill", "Summary Paraphrase: This study explores a novel perspective on the \"larger teacher worse student\" phenomenon in knowledge distillation (KD) by introducing a \"datacentric\" approach. They identify classes that hinder the KD process termed \"undistillable classes\" and propose Teach Less Learn More (TLLM) a framework that excludes these classes to enhance the performance of smaller student networks during distillation. To identify undistillable classes they propose two criteria: class accuracy and agreement prediction agreement. Strengths and Weaknesses: Strengths: Original and wellsupported observations. Simple and effective methodology. Demonstrated effectiveness across diverse datasets and network architectures. Weaknesses: Similarity in methodology to ESKD but lacks a direct comparison. TLLM requires significant computational resources to find undistillable classes. Undistillable classes are claimed to be \"universes\" without supporting evidence. Minor Corrections: Line 127: Replace \"j\" with \"M.\" Line 1635: Review the suitability of examples. Line 166: Define \"crystal.\" Figure 4: Clarify the meaning of the dashed line. Practical Application Questions: Investigate practical ways to apply TLLM to new datasets."], "mrt90D00aQX": ["Paraphrased Statement: Summary: The study presented a technique for feature representation learning to address federated learning in an outofdistribution (OOD) classification scenario. It introduced two regularization terms: L2norm and conditional mutual information. Experiments on four benchmark domain generalization datasets showed promising performance. Strengths: Tackles the intriguing issue of OOD federated learning. Proposes a straightforward and effective method. Weaknesses: The method does not directly advance federated learning (FL). Feature representation regularization is not new in domain generalization (DG) and can be applied individually per domain before FedAvg. The study did not explore FL in heterogeneous environments (nonIID) which is a widely studied area. It would be beneficial to evaluate the methods on more realistic datasets such as medical imaging which require privacy guarantees in FL settings. Questions: Since the focus is on feature representation learning it is crucial to compare the proposed method with DG baselines that do not require domain alignment and can be applied in an FL setting.", "Paraphrase: Summary: This study addresses domain generalization in federated learning by optimizing local objective functions and learning a datainvariant feature representation via conditional mutual information (\\ellCMI). A \\ell2 norm regularization is employed to control feature complexity. The algorithm is tested on multiple datasets and shows effectiveness in handling unseen domains comparable to centralized algorithms. Strengths and Weaknesses: Originality: The paper does not adequately cite relevant prior works on federated domain generalization including [11] and [12] which use similar methods including feature representation with norm regularization and domain alignment. This limits the papers originality. Quality: The paper is wellstructured and provides theoretical justification for the regularizers. However it lacks generalization or performance bounds for the proposed loss. Clarity: The paper is presented clearly with welldefined notations and concepts. Theoretical findings and numerical results are explained in detail. Significance: The paper addresses a key issue in federated learning namely domain shift between training and deployment phases. The numerical results indicate significant improvements across multiple datasets. Questions: 1. Could the authors expand on existing domain generalization methods in federated learning and compare their algorithm to [1.1] and [1.2] 2. Can the authors provide an algorithm block detailing the \\ellCMIi update process 3. The data split for training and testing is unclear. Is it one domain for testing and the others for training or 90 of data from all domains for training and 10 for testing", "Paraphrase: This paper introduces two simplified regularizers for improving the performance of federated learning models in the face of differing data distributions across clients. These regularizers leverage the common L2 norm and an approximation of conditional mutual information to enhance the alignment of data representations. The L2 norm regularizer encourages the alignment of latent distributions across clients similar to AlignFlow. The CMI regularizer promotes the alignment of conditional distributions by matching the clientspecific distributions to a global distribution. Empirical results demonstrate the effectiveness of both regularizers in enhancing domain generalization. Key Points: Strengths: Simple and straightforward algorithm Provides insightful connections to data alignment Demonstrated effectiveness on various datasets Weaknesses: Lack of evaluation on realworld distribution shifts Limited exploration of synchronization frequency Questions: How does equation (11) relate to the ELBO Why is the CMI regularizer discussed before the L2R How does the proposed approach relate to AlignFlow", "Paraphrased Statement: Summary This research introduces FedSR a framework for training neural networks that can learn simple representations in a federated setting. This enables the model to perform well on unseen client data (in the sense of domain generalization). The representation is a hidden layer in the network and is learned by minimizing a combination of: 1. Task prediction loss 2. Conditional mutual information (CMI) 3. L2 regularization (L2R) The authors provide theoretical evidence showing that these terms promote generalization performance which is supported by experiments on standard domain generalization benchmarks. Strengths Addresses an important but understudied problem in federated learning Clear and wellpresented Demonstrates strong empirical performance Weaknesses Experiment setup is not typical for federated learning Questions 1. Clarify that z is conditionally independent of y given x before Equation (4). 2. In the experiments is zs hidden dimension independent with separate variances or is a covariance matrix learned 3. The experiments use only a small number of clients. It would be more convincing to demonstrate FedSRs effectiveness with a larger number of clients sampled from an underlying distribution. 4. Investigating the quality of FedSRs representations (e.g. CMI L2R visualization) would provide insights into its performance advantage. Minor Points LHS of Equation (23) lacks a p in the log. Line 269: \"the variational distribution can match (what)\" is unclear."], "vphSm8QmLFm": ["Paraphrase: Summary: The authors present a flexible training method for recommender systems that can switch between synchronous (BSP) and asynchronous (Hop) training modes. This hybrid approach combines the advantages of both modes enabling continuous learning without losing accuracy or efficiency and it eliminates the need for hyperparameter tuning. Users can choose the preferred training mode based on their assessment of the systems workload. Strengths and Weaknesses (PostRebuttal Updates): Originality: The paper addresses a specific challenge in recommender system training and proposes a novel solution that has not been explored before. Quality: The paper provides a comprehensive overview of the proposed approach including insights and observations that guide the design and evaluation. Clarity: The paper is wellstructured and easy to follow except for a minor issue with Section 5.2 (Figure 6) that warrants clarification. Significance: The proposed method has the potential to improve resource utilization and cost savings for industrialscale recommender systems with variable workloads. Questions: Section 5.2: The text describing the performance of HopBS and BSP in Figure 6 appears to be incorrect. It should be clarified that HopBS outperforms BSP in (c) and the improvement in AUC for GBA is less than 0.2 in (a). The description of Figure 6 in the text erroneously refers to (de) instead of (df). Highlighting the bestperforming metrics in the tables would enhance readability.", "Paraphrased Statement: Summary: The document introduces a new algorithm for switching between distributed training modes (PS and AR) without compromising accuracy or performance. It eliminates the need for hyperparameter tuning. The algorithm is based on three critical observations affecting performance and accuracy. It employs a global batch aggregation method that emulates synchronous trainings global gradients to transition between PS and AR modes seamlessly maintaining accuracy. The paper presents a convergence analysis and evaluates the algorithms effectiveness on three tasks demonstrating superior performance and accuracy. Strengths: Provides an efficient way to switch between training modes without costly hyperparameter tuning. Offers theoretical proof of convergence. Outperforms existing recommender training techniques in various tasks. Weaknesses: Lacks guidance on the optimal time to initiate switching based on cluster status. Does not include ablation studies to assess the algorithms performance under different cluster conditions. Questions: How does the algorithms performance vary with varying numbers of workers Could the reliance on global batch aggregation introduce dependencies on the number of workers", "Paraphrased Statement: Summary: This research introduces the \"global batch gradient aggregation\" (GBA) technique which allows deep learningbased recommendation systems to seamlessly switch between synchronous and asynchronous training on parameter server architectures. The GBA technique mitigates the performance drop observed when switching between these training modes. Strengths: GBA enables efficient switching between synchronous and asynchronous training which is a novel finding. Weaknesses: The explanation for the observed performance drop lacks technical depth. The insights provided in Section 3.2 are more intuitive than analytical. Formal theoretical analysis would enhance the study. The performance improvement is relatively minor. Figure 6 shows a small performance gain with synchronous training achieving the best AUC faster than GBA. Questions: Figure 6 is difficult to interpret. Could the final AUC for each model and task be presented in a table"], "owDcdLGgEm": ["Paraphrase: Summary: This work aims to enhance 3D reconstruction from 2D cryoEM images using the MultiFrequency View Determination Method (MFVDM). First relative poses between image pairs are estimated using a group synchronization framework. Relative pose estimates are possible when viewing angles are nearly identical or differ by 180 degrees. The authors introduce a robust method for calculating viewing angle similarity and relative pose with respect to noise using a multifrequency approach. Finally individual image poses are estimated allowing for 3D object density calculation. These concepts are evaluated using synthetic vector datasets and cryoEM datasets. Strengths: The method can estimate relative poses when viewing angles differ by 180 degrees improving pose estimation quality and reducing connected components in the synchronization graph. Individual image poses are estimated avoiding the loss of resolution associated with image aggregation during 3D reconstruction. The analysis of synchronization symmetries could benefit other 3D reconstruction approaches in cryoEM. Weaknesses: Lack of proofofconcept on realworld data. The assumption of uniform viewing angle distribution may not hold true for cryoEM datasets. Questions: In Appendix C the proof of sufficiency for O(2) relative poses is shown. What part of the proof would fail if relative poses belonged to SO(2) (O(2) minus flips)", "Paraphrase: Summary: This paper discusses the symmetries in singleparticle cryoelectron microscopy (SPA cryoEM) and proposes a technique to determine particle orientations using these symmetries with Vector Diffusion Maps. This method could replace current 2D classification steps eliminating the need for clustering and averaging in the SPA cryoEM pipeline. The authors tested their approach on synthetic and experimental data and found accurate pose estimations. Strengths: Provides a formal description of symmetries in cryoEM. Shows promising results in pose estimation. Weaknesses: The papers formal approach may be challenging for nonmathematicians potentially limiting its accessibility to cryoEM practitioners. Questions: The method claims to allow the use of all images for 3D reconstruction increasing resolution. Could you provide evidence supporting this claim Are there any performance comparisons available between the proposed method and current 2D classification techniques", "Paraphrased Statement: Summary: This study explores symmetries in group synchronization for estimating the poses of images in cryoEM (cryoelectron microscopy). The findings indicate that relative poses in O(2) (the orthogonal group in 2 dimensions) but not SO(2) (the special orthogonal group in 2 dimensions) provide enough information to determine image poses in SO(3) (the special orthogonal group in 3 dimensions). This understanding is used to enhance the Multifrequency Vector Diffusion Map (MFVDM) algorithm transforming it from a 2D image classification tool to a method for absolute pose estimation applicable to ab initio 3D reconstruction. The study presents results on a synthetic cryoEM dataset with different noise levels. Strengths and Weaknesses: The study is wellwritten and presents a clear motivation. The authors do not provide indepth analysis of related vector diffusion map research but the idea is notable and the extended ability for absolute pose estimation is significant. The paper lacks certain details regarding the synthetic cryoEM dataset creation and experimental setup. Additionally a discussion of the algorithms computational complexity would be valuable. Questions: What are the time and memory requirements of the algorithm Can the authors provide specific errors (e.g. meanmedian error in degrees) between estimated and ground truth poses instead of relying on correlation measures Please elaborate on the dataset generation and experiment setup. In the supplementary material the authors mention updating poses from initial values through backpropagation. Could you provide more details (e.g. the balance between pose and model updates) and quantify the extent of pose refinement"], "qq84D17BPu": ["Paraphrase: This study aims to better understand how the dynamics of gradient descent can be modeled using a differential equation. Previous research (Gradient Flow) proposed an equation to describe this: ``` d\u03b8dt \u03b7\u2207\u03b8 L(\u03b8) ``` where \u03b7 is the learning rate L(\u03b8) is the loss function and \u2207\u03b8 L(\u03b8) is its gradient. However this equation only approximates gradient descent so there is a difference between its behavior and the actual gradient descent algorithm. This paper introduces a term to correct this model denoted as \u03be: ``` d\u03b8dt \u03b7\u2207\u03b8 L(\u03b8) \u03b7\u03be(\u03b8) ``` This corrective term is an integral that the paper approximates using a series solution. A recursive relationship is established to derive each subsequent term in the series leading to a new set of dynamics called the Equation of Motion (EoM). The paper also derives a limit for the learning rate which enables more accurate simulations of gradient descent using EoM with larger step sizes. These findings are supported by experiments conducted on scaleinvariant and translationinvariant layers. Strengths: Demonstrates impressive theoretical results Provides extensive experiments to corroborate the theory Clearly explains the motivation for the work Weaknesses: The writing style and presentation could be improved for clarity The papers heavy reliance on mathematics may make it less accessible Questions and Clarifications: Line 240 (Definitions): Should symmetry be defined as a parameter transformation that alters predictions rather than loss function Would it be possible to experiment with nonsymmetric layers such as standard linear layers to investigate the impact The majority of experiments used the first term in the expansion of \u03be. Could an ablation study be conducted to examine the effects of adding subsequent terms Besides the theoretical contributions does the authors anticipate any practical applications for this work", "Paraphrased Summary: This paper focuses on the difference between the practical implementation of gradient descent (GD) in discrete steps and its continuous formulation known as a gradient flow (GF). Using numerical analysis techniques the authors introduce a \"discrepancy error\" to quantify this difference. They propose a \"counter term\" to correct for this error allowing GF to better describe the actual trajectories of GD. While the counter term is complex it can be solved analytically assuming a power series solution. Applying their corrected GF to scale and translationinvariant layers the authors demonstrate its utility for analyzing learning dynamics in deep Neural Networks (NNs). Strengths: The counter term is novel and potentially valuable for understanding complex learning dynamics in NNs. It generalizes previous correction terms to higher orders recovering existing approaches as special cases. The technique is inspired by numerical analysis but its application to deep learning is a significant contribution. Weaknesses: The analysis applies only to fullbatch GD not minibatch stochastic GD. The authors do not provide empirical comparisons between their proposed counter term and lowerorder corrections. The methods applicability to GD with large learning rates is limited. The papers density and complexity make it challenging to understand. Questions: Can the authors provide experimental evidence to support the advantages of the proposed counter term Is it possible to improve the efficiency of the learning rate bound (13) for broader GD scenarios", "Paraphrased Statement: The authors propose an improved approximation called \"Equation of Motion\" for gradient descent (GD) in deep neural networks (DNNs). They introduce a \"counter term\" to the Gradient Flow (GF) algorithm which compensates for errors caused by discretization. The counter term is determined through backward error analysis and its inclusion reduces discretization errors in GD approximations of GF. The authors analyze the discretization error and derive an upper bound on the learning rate to minimize this error. They apply their Equation of Motion to translation and scaleinvariant layers and demonstrate that it provides better predictions of GD behavior compared to previous approaches. Strengths and Weaknesses: Strengths: Derivation of a general form for the counter term. Improved approximation of GD dynamics leading to better predictions. Characterization of learning dynamics in scale and translationinvariant layers. Weaknesses: Lack of significant novelty compared to existing work. Derivation of discretization error is assumed under certain conditions (zero counter term or firstorder term only). Upper bound on learning rate instead of precise formulation. Computational expense of higherorder counter terms. Lack of a clear sketch in the paper and appendix which could enhance readability.", "Paraphrased Statement: This study proposes a correction term to gradient descent (Eulers method) to reduce discretization error. By expanding this term as a Taylor series and including specific terms the discretization order can be adjusted. This refinement helps analyze how gradient descent behaves with specific constraints like scale and translation invariance. It introduces learning ratedependent correction terms that align with practical observations of gradient descent. Strengths and Weaknesses: Pros: Corrects the theoretical gradient flow formulation to better align with practice. Clear presentation and easytounderstand results. Cons: Underlying theory may not be entirely novel. The significance of higherorder terms in the derivation is not fully explored. Questions: How does the correction term relate to Eulers methods global truncation error Does the equation of motion capture potential inconsistencies in Eulers method when the step size exceeds certain bounds Can the correction term help adjust the stochastic gradient descent formulation to match practical observations Is experimental verification of Corollary 4.2 feasible"], "nzuuao_V-B_": ["Paraphrased Statement: This study examines the gradient inversion problem in federated learning (FL) where a malicious server attempts to uncover client data from shared model weights and gradients. Evaluation and Methodology: Evaluated popular gradient inversion algorithms (mainly [6] with an additional loss term) under different model variations (BN modes training epochs and architectures). Proposed an angular Lipschitz constant measure to assess reconstruction outcomes which exhibited a stronger correlation with attack loss and perceptual similarity than gradient norm. Strengths: Explores the impact of model variations on gradient inversion algorithms. Systematically evaluates optimizationbased gradient inversion approaches. Weaknesses: The main contribution (angular Lipschitz constant) appears secondary to the reevaluation of existing methods. Overlaps with previous work [10]. Reevaluation is conducted on limited model variations. Only considers CIFAR100 images not higherresolution images. Motivates the angular Lipschitz constant based on limited access to gradients but the interplay between model training and risk assessment is unclear. Questions: Were target labels and batch size known in the experiments How does the server alter BN modes without detection How is the angular Lipschitz constant computed Are learning rates sufficiently small to guarantee monotonic loss decrease What is the significance of \u03b4 in the theoretical result Which BN usage (loss term or network layers) has a greater impact on inversion performance", "Paraphrased Statement: Summary: This work focuses on gradient inversion attacks in federated learning and presents additional results on their effectiveness. It also proposes a new measure of vulnerability which is claimed to be useful for clientside defenses. Strengths: The work provides a comprehensive analysis of gradient inversion attacks. The concept of angular Lipschitz smoothness is introduced as a potential metric for vulnerability. Weaknesses: The cited related work is not fully accurate and some of its findings have been addressed in previous studies. The proposed blackbox setting is questionable since users would likely have access to global model parameters. The correlation between angular Lipschitz smoothness and attack success is not conclusive. The concept of safety in this work is flawed as it assumes that a single attack must work optimally in all scenarios. Questions: What is the intended purpose of the blackbox setting Can the proposed metric of angular smoothness withstand adaptive attacks", "Paraphrase: This study investigates privacy vulnerabilities in the blackbox federated learning (FL) environment specifically using gradient inversion techniques. It focuses on image classification tasks and provides empirical evidence of the privacy risks. Strengths: Clear purpose and significant relevance to FL privacy Thorough experimental analysis evaluating model characteristics vulnerable to privacy attacks The proposed angular Lipschitz constant provides a reliable measure of privacy risk with desirable properties Weaknesses: Limited experimentation with only a single dataset raising concerns about the generalizability of findings Conclusions rely heavily on empirical observations requiring additional support from more experiments", "Paraphrased Statement: Summary: This study examines gradient inversion attacks in varying model configurations including batch normalization modes skip connections and channel sizes. The findings indicate that these factors influence the effectiveness of these attacks. The authors introduce angular Lipschitz smoothness as a metric correlated with attack success. Strengths: Investigates gradient inversion attacks under different training settings and model architectures a novel approach. Introduces angular Lipschitz smoothness as a potential indicator of attack vulnerability. Presents a wellstructured paper. Weaknesses: Uncertain experiment results and ambiguous conclusions. Unreliable MSE values in batch normalization experiments casting doubt on the reported success. Inconclusive skip connection experiment results contradicting previous studies. Insufficient explanation for deteriorating reconstruction quality with increased channel size. Limited experimental details for angular Lipschitz smoothness experiments potentially leading to unfair optimization. Gradient inversion attack type used in the experiments is not specified. Questions: Clarification on the MSE values in batch normalization experiments. Explanation of the skip connection experiment results and alignment with prior research. Justification for the claim regarding channel size and reconstruction quality. Detailed experimental information for angular Lipschitz smoothness experiments. Specification of the gradient inversion attack method employed."], "tTWCQrgjuM": ["Paraphrased Summary: This paper presents a noiseaware statistical inference method for differentially private (DP) data. It addresses the challenge of capturing the noise introduced by DP into the posterior uncertainty of model parameters. The proposed method treats unobserved private data as an augmented latent variable in the model. Strengths: Solves the problem of latent sensitive data in noiseaware DP Bayesian inference. Introduces the concept of \"Record Additivity\" for DP mechanisms that enable efficient Gibbs sampling without computing the entire likelihood repeatedly. Demonstrates that the proposed method asymptotically targets the correct distribution and provides wellcalibrated posterior distributions. Weaknesses: The applicability of the method is limited to DP mechanisms satisfying Record Additivity. The extent of models that can be handled with this method is unclear. Questions: How does the method compare to existing noiseaware DP Bayesian inference solutions (e.g. Bernstein and Sheldon 2018 2019 Kulkarni et al. 2021) Can the method be extended to more complex models (e.g. logistic regression) and how would it compare to approximate solutions", "Paraphrased Summary: This study presents a general MCMC algorithm to approximate the joint probability distribution of model parameters and true (uncorrupted) data from a privatized corrupted dataset using the \\epsilondifferential privacy mechanism. The algorithm inspired by Bayesian inference approaches for measurement error data alternates between resampling the true data as a latent variable and resampling parameters given the true data. Strengths: Proposes a pointwise MetropolisHastings step for resampling true data with a nonvacuous acceptance probability under mild noise mechanism conditions. Demonstrates efficient computation with O(n) time complexity for this step. Establishes the algorithms ergodicity in this context. Substantiates theoretical claims with empirical case studies using Naive Bayes and Bayesian linear regression models. Weaknesses: Overlaps with established Bayesian measurement error modeling theory specifically \"data augmentation MCMC.\" Omits citation of relevant work on \"locally private Bayesian inference\" for \\epsilondifferential privacy. Majority of the paper focuses on exposition of known ideas rather than novel contributions. Additional Comments: The papers novelty lies in the MH algorithms generality across noise mechanisms and Proposition 3.1 which bounds its acceptance probability. However the emphasis on known theoretical concepts limits the papers significance. The authors should consider rewriting it to highlight the algorithms practical implications for differentially private Bayesian inference. Questions: The respondent clarifies that the papers originality lies in handling noise mechanisms where noise is added centrally not pointwise as in standard Bayesian measurement error models. They also suggest that Proposition 3.2 may follow from the pointwise resampling nature of the MH algorithm and ask for intuition on its novelty or difficulty.", "Paraphrased Summary This paper proposes a Markov chain Monte Carlo (MCMC) algorithm for handling private data by expanding the parameter space to include private information. The algorithm alternates updates on unobserved variables and parameters using a MetropolisHastings and Gibbs sampling approach. The paper discusses the acceptance rate and mixing properties of the algorithm in the context of privacy and provides simulations to support the claims. Strengths: Interesting problem Clear presentation Weaknesses: Limited experimental results that do not fully support the claims Potential issues with exploring state space in highdimensional settings Uncertain impact of privacy on algorithmic performance Lack of comparison to other MCMCbased privatization methods especially in terms of complexity and performance Lack of comparison to alternative methods such as \"Faster Differentially Private Samplers via R\u00e9nyi Divergence Analysis of Discretized Langevin MCMC\" Incremental contribution that aligns with existing literature Absence of justifications for the advantages of this method over others Lack of clarity on the impact of higher privacy on exploration Uncertain effects of truncation on privacy when k is not infinite Difficulty in comparing to highdimensional fast algorithms like \"On Connecting Stochastic Gradient MCMC and Differential Privacy\" by Li et al Ambiguous evaluation metric"], "q16HXpXtjJn": ["Paraphrased Statement: The study examines a problem called the infinitearmed bandit problem. While earlier research focused on identifying the \"best\" arm in this context this study focuses on estimating a specific function of the underlying distribution (F). They consider functions based on indicators such as mean median maximum and trimmed mean. The study covers both situations where arms are chosen in advance (offline) and situations where arms are chosen sequentially (online). A general approach for arbitrary indicatorbased functions is proposed with associated sample complexity and lower bound results for each scenario. Strengths: A unified algorithm and sample complexity result for all settings. Lower bounds for each setting. Mathematically elegant formulation and interesting investigation of indicatorbased functions. Weaknesses: Weak motivation for estimating a general function. Lack of specific examples or explanation of how the median or trimmed mean are relevant in practice. Unclear motivation for the offline setting. Limited algorithmic novelty despite the unifying algorithm. The gap between online and offline algorithms is not clearly demonstrated. The presentation of upper and lower bounds could be improved for clarity. The algorithm may require unrealistic knowledge of certain quantities in practice. Questions: Could the authors clarify the optimality of the algorithms in all settings Could the authors elaborate on the motivation for the study and provide concrete examples of its applications", "Paraphrased Summary: This research investigates estimating distribution functions (specifically mean median and trimmed mean) in the infinitearmed bandit problem. In the offline setting the study determines the optimal number of points (n) and samples per point (m) to minimize sample complexity (mn). In the online setting an algorithm is proposed to eliminate redundant points unrelated to the functional estimation potentially reducing sample complexity. The online algorithm achieves the same worstcase sample complexity as the offline algorithm but can improve sample complexity on average. The study establishes matching lower bounds for sample complexity in both online and offline settings. For mean and maximum estimation Wasserstein distances are used to bound KullbackLeibler (KL) divergence. For median estimation a thresholding phenomenon is identified demonstrating that KL divergence does not change smoothly with noise level. Strengths and Weaknesses: Originality: Novel work exploring sample complexity for different functions and settings. Technical Soundness: Strong technical basis. Clarity: Wellwritten and organized. Significance: Provides complete and solid results on sample complexity and highlights insights: Complexity varies based on the function (median is harder than mean but easier than trimmed mean). Offline and online settings differ in complexity for nonmean functions. Areas for Improvement: Acknowledge connection to literature in statistics. Explain why Table 1 does not include confidence level (\u03b4) dependence as it affects sample complexity. List related work in medianquantile estimation as the objective is not solely regret minimization or arm identification. Note that for online mean estimation discarding points is not necessary as all points contribute.", "Paraphrase: Summary This paper addresses the challenge of estimating distribution functionals (such as mean median and trimmed mean) in the \"infinite armed bandit\" problem. The authors: Formalized the distribution functional estimation problems. Developed metaalgorithms for both online and offline settings providing performance guarantees. Established lower bounds on sample complexity for offline and online settings. Strengths Provides a comprehensive theoretical analysis of distribution functional estimation including both upper and lower bounds. Weaknesses The online algorithm relies on the \"doubling trick\" which may not be practical in realworld applications. The paper lacks numerical experiments to demonstrate the practical effectiveness of the algorithms. Assessment The paper offers valuable theoretical insights into distribution functional estimation in the infinite armed bandit problem but its practical significance remains unclear due to the lack of numerical evaluation.", "Paraphrased Summary: This paper explores the challenge of estimating functions in an infinitearmed bandit setting where multiple actions yield different values. The authors present online and offline algorithms for estimating various functions (e.g. mean median maximum trimmed mean). Strengths: The sample complexity bounds are precise within logarithmic factors. A unified approach is proposed for online and offline settings revealing that they may have varying sample complexities. Weaknesses: The paper lacks sufficient motivation for learning nonmean functions in this setting. The writing could benefit from clearer definitions and formulations before presenting results. The algorithms are not particularly novel: the offline algorithm uses averaging with typical concentration properties while the online algorithm iteratively refines confidence intervals. Questions: Do the authors believe these ideas could be extended to classical bandit settings What factors might influence this"], "sBrS3M5lT2w": ["Paraphrase: Summary: The study investigates the stability and convergence of stochastic gradient descent (SGD) under specific assumptions. Notably the researchers focus on locally H\u00f6ldercontinuous functions (rather than global assumptions) and relax the assumption of bounded variance. Under these assumptions the researchers demonstrate that SGD iterates either converge (to a stationary point) or diverge to infinity. With an additional technical assumption they also prove convergence with probability one. Strengths and Weaknesses: The paper claims to use less restrictive assumptions than previous studies. While it does broaden the analysis of SGD to a broader function class the main concern lies in the assumption that most results hold for \u03b8 approaching infinity (though this assumption is not explicitly stated). The result of Theorem 1 appears intuitive due to the diminishing step size used. The researchers suggest that in such a case the iterates either remain unbounded or become contained within a bounded set. With diminishing step size and bounded variance the iterates are likely to eventually cease moving. They also believe it may be possible to establish a convergence rate for SGD to a stationary point which would depend on the radius of the bounded set (justifying Theorem 2). While the assumptions in the paper involve local H\u00f6lder smoothness and bounded variance the combination of these assumptions with \u03b8 approaching infinity is comparable to assuming global constants. Questions: The researchers are asked if they believe it is possible to eliminate the assumption that \u03b8 approaches infinity.", "Paraphrase: Summary: This study explores the convergence of stochastic gradient descent (SGD) for nonconvex stochastic optimization with smooth functions. By relaxing two common assumptions (global smoothness and bounded stochastic gradient variance) the authors introduce a weaker assumption: local H\u00f6lder continuity of the gradient and bounded second moment of the stochastic gradient. They demonstrate that SGD with a diminishing step size either converges to a stationary point or diverges. Additionally they show that the objective function remains bounded even when the iterates diverge. Strengths: Clear motivation and concise presentation Nontrivial technical contributions Weaknesses: Lack of insights into convergence rate Potentially overgeneralized assumptions in practical applications especially for modern neural network optimization problems Additional Comments: The study complements existing works by relaxing assumptions and proving convergence. Further analysis is required to fully understand the implications of these results.", "Paraphrased Summary: The research paper examines stochastic gradient descent across diverse applications by revising several problematic assumptions commonly used in prior analyses. These assumptions are replaced with more practical alternatives. The study demonstrates the drawbacks of the current assumptions and proves convergence and stability under the new assumptions although a convergence rate is not attainable. Strengths and Weaknesses: The papers strengths include its clarity and accessibility. The updated assumptions broaden its applicability beyond previous analyses that relied on flawed assumptions. The results are novel and important and the proposed methods for proving the theoretical findings are innovative. One weakness is that the counterexamples presented in the main text lack details and only refer to an appendix that contains the actual examples. Additionally the paper includes extensive introductory material and does not delve into the core analysis until page 6 with many results relegated to the appendix. Questions: 1. The paper mentions that a Ridge penalty is used in two counterexamples. Is it indispensable If so this could weaken the argument that previous assumptions were inadequate. 2. The authors claim that stochastic gradient descent (SGD) generally converges (line 92) which may surprise practitioners in deep learning where SGD can potentially diverge. Could the authors explain why this apparent discrepancy exists", "Paraphrased Statement: Summary: This paper examines how SGD behaves under more general noise models than previously studied. The authors first highlight three common ML optimization scenarios where SGDs convergence has not been proven. They then explore SGDs convergence under more relaxed conditions including removing the usual assumptions of Lipschitz continuous gradients and bounded variance. The results show that under these conditions SGD either converges to a fixed point (i.e. a stationary point) or diverges to infinity. Finally the paper provides a technical condition that if satisfied guarantees convergence to a stationary point. Strengths: Wellwritten and transparent about limitations Solid mathematical results Demonstrates the possibility of convergence results under general conditions Weaknesses: Difficulty in applying the results in practice as its impossible to predict whether SGD will converge or diverge before running the algorithm No discussion on practical applications or experimental results Question: The statement claims that SGD converges to a unique stationary point (if it doesnt diverge). However the reviewer wonders if its not possible for the iterates to move around a connected set of stationary points without converging to any specific one. For example in a function with a stationary point set [01] the reviewer suggests that the iterates could randomly fluctuate within this set without converging to a specific point within [01]. They question whether this is allowed or if theres a misunderstanding.", "Paraphrased Statement: This paper studies how SGD (Stochastic Gradient Descent) performs in practical settings where functions are nonconvex and feedback is noisy. Key contributions include: 1. Relaxing the assumption of global Lipschitz continuity to local \u03b1H\u00f6lder continuity (smoothing at small scales). 2. Relaxing the bounded variance assumption to allow for bounded (1\u03b1)th moments of stochastic gradients. Consequently the authors show that: SGD either converges to a stable point or diverges. Convergence can be ensured by an additional assumption that generalizes \"expected smoothness.\" Strengths: Practical relevance: Demonstrates invalidation of current assumptions for SGD convergence in realistic settings (e.g. linear regression neural networks). Rigorous analysis: Provides a comprehensive analysis of global convergence and a highlevel overview of the proof. Strong result: Theorem 1 implies global convergence in practical settings where model parameters are typically finite. Weaknesses: Proposition 1 limitation: Assumes an idealized scenario (\u03b8 \u2192 \u221e) that may not apply to typical deep learning model training. Questions: What evidence supports assumption 5 in the examples provided Provide clear definitions of \u03b1H\u00f6lder continuity and semicontinuous functions."], "wwW-1k1ljIg": ["Summary: This study investigates the behavior of poisoned and benign neurons in deep learning models. Based on their findings the authors propose two methods for defending against backdoor attacks that target only poisoned neurons or compromised models: DDE and MBNS respectively. The methods are tested on CIFAR10 and TinyImageNet datasets under various attacks. Strengths: 1. Relevant and significant to the NeurIPS audience. 2. Discusses resistance to potential adaptive attacks. 3. Easy to understand. 4. Evaluates the efficiency of different defenses. Weaknesses: 1. Statements like \"we claim that both the proposed indices can perfectly separate\" require further support or modification. 2. Missing some important related works such as preprocessingbased and poisonsuppressionbased backdoor defenses. 3. Concerns about the experiment setup: Hyperparameter selection for u. Lack of comparison with other pruningbased defenses in terms of pruning ratio. The proposed defenses are not tested against adaptive methods. Minor Comments: 1. \"we will\" should be \"we\" in Line 36 (Page 1). 2. Typos in Table 2: \"0.1\" should be \"0.10\" and \"64.2\" should be \"64.20\". 3. References should be updated to cite the official versions.", "Summary Paraphrase: In this research the authors introduce a defense strategy to protect Convolutional Neural Networks (CNNs) from backdoor attacks. They observe that the activation distributions of neurons in a backdoorinfected CNN vary between benign and backdoored samples. Using this observation they propose calculating the differential entropy to identify compromised neurons. The approach is evaluated on the CIFAR10 and TinyImageNet datasets in various scenarios including training and posttraining phases. The authors also test its resilience against two adaptive attacks. Their findings demonstrate the defenses effectiveness against backdoor attacks outperforming other approaches in robustness efficacy and efficiency. Strengths and Weaknesses Paraphrase: Strengths: 1. Clear and accessible presentation. 2. Thorough evaluation. 3. Consideration of adaptive attacks. Weaknesses: 1. Limited novelty as the concept of inspecting compromised neurons for backdoor defense is not new. 2. Lack of comparison with other defenses especially unlearningbased and trainingphase methods. 3. Insufficient evaluation in practical scenarios such as multiplelabel and multiplebackdoor scenarios. Questions: 1. Can the defense perform effectively in multiplelabel and other practical scenarios 2. What device was used to test the running time in the experiments", "Paraphrased Statement: Summary This research investigates techniques to improve the defense of models against backdoor attacks. The study reveals that only a specific group of neurons are responsible for malicious behaviors introduced by backdoors. Pruning these poisoned neurons effectively eliminates the backdoor behavior. The authors present a simple method to identify these neurons based on discrepancies in differential entropy and batchnorm statistics. This approach effectively detects backdoor neurons even with a limited number (10) of benign examples while requiring minimal additional computational overhead. Strengths and Weaknesses Strengths: Efficient and straightforward method Clear and easytounderstand presentation Thorough experimental evaluation demonstrating the effectiveness of the proposed approach Weaknesses: While the wording could be improved it does not significantly affect the papers message The authors acknowledge that while the method is effective against standard backdoor attacks its applicability in selfsupervised or unsupervised settings requires further investigation Overall Recommendation: The paper is highly recommended for acceptance due to its substantial contributions in improving defense mechanisms against backdoor attacks.", "Paraphrase: Summary: This paper presents a method to detect and remove neurons that are primarily affected by \"backdoor\" training samples. Backdoor training involves injecting hidden triggers into a dataset to manipulate model behavior for specific inputs. The paper asserts that clean (unadulterated) and backdoor samples have distinct feature distributions. The method proposed in the paper uses this observation to identify and eliminate the neurons that are most strongly influenced by backdoor samples. Empirical results demonstrate the methods effectiveness against certain backdoor attacks. Strengths: 1. The methods premise is logical as clean and backdoor samples are expected to have different feature statistics due to the models tendency to overfit superficial correlations in backdoor samples and learn semantic information in clean samples. 2. The paper is wellwritten and comprehensible with clear definitions of terms and notations. 3. The method can improve backdoor robustness even with limited availability of clean samples. Weaknesses: 1. Generalizability: The papers reported performance may not generalize to different settings. In practice the type of backdoor attack used is unknown. Defense methods should be robust against various attacks with the same hyperparameters. However this paper only mentions that one hyperparameter \\muk is \"usually set to 3.\" The authors should specify the \\muk values used in all cases and explore the impact of different hyperparameter settings on the methods performance. In addition the authors should consider more diverse attack settings such as varying poisoning ratios and different attack types. 2. Visualization in Figure 5(c): The visualization of the Blend attack appears unusual lacking the expected effects of the blending image. The authors should clarify the blending ratio and blend image pattern used. Questions: The title in the submitted PDF differs from that in OpenReview. Please clarify which title is correct."], "x26Mpsf45P3": ["Summary This research explores offline policy evaluation and optimization in the context of modelfree offline reinforcement learning with function approximation. It utilizes discounted reward settings. The primary contribution is a comprehensive framework for assessing offline policy evaluation via the weak convergence of the Bellman error to zero for a family of test functions. This enables the formulation of comprehensive policy evaluation and optimization bounds under relatively mild but reasonable conditions. The authors provide valuable interpretations of these bounds under stricter assumptions such as Bellman closure and importance sampling weights. Additionally the paper presents adaptations of previous results to the linear MDP setting. Strengths Originality: The concept of replacing stringent offline learning conditions with their weaker counterparts is innovative and the main theoretical finding is novel. Quality: The generality of Theorem 3 is commendable. Clarity and significance: The application of derived results in the linear setting provides a potential recipe for developing meaningful bounds in the function approximation setting with weaker conditions than in earlier studies. Weaknesses Computational efficiency: Key results require calculations of upper and lower empirical value functions for a given policy which involves optimization over the empirical constraint set. While this optimization can be performed efficiently for linear MDPs it becomes computationally expensive in general potentially requiring time that scales linearly with the size of the test function class. Given that in two examples (LSTD error test space and Bellman test space) the test function class complexity matches that of the Qfunction class it is unclear how to efficiently perform such computations beyond linear models. Presentation: The paper is technically demanding with the main results requiring considerable notation. However the notation is inadequately defined making it challenging to follow. The introduction of notation throughout the paper and across different sections makes it difficult to recall or comprehend. Additionally the interpretable bounds for OPC (offpolicy width) are not presented in the main text but hidden in the appendix which would have improved the presentation. Additional Comments The notation for C\\pin in the proof of C.1.1 is problematic it should include the dependence on \\rho to ensure the validity of the first inequality. The proof of Theorem 3 seems to revolve around using Proposition 4 and verifying Equation 37 but this is not explicitly stated or addressed. Without this connection Proposition 4 appears irrelevant to the paper. The paper would benefit from improved presentation and a discussion of computationally efficient algorithms beyond the linear setting. Questions Is weak realizability indeed a weaker condition than realizability It appears that it would equate to realizability if the test function class included indicators of all (sa) pairs. What is the measure \\mu used throughout the paper It is assumed to be the data collection measure but a formal definition is not provided outside of Assumption 1. How is the second equality in Equation 10 derived It seems that the measure d\\pi is replaced by \\mu which is nontrivial to establish. Where is b\\pi defined in Proposition 1 and what does it represent What is the definition of a prediction P\\pi(Q) in Definition 1 on page 19 Is it simply a mapping from the Qfunction class to itself What is K\\pi on line 750 In Lemma 6 \\lambda can become arbitrarily small is there no explicit dependence on the number of mixture components Where is Yn(Q\\pi) defined (display under line 1172)", "Paraphrased Statement: The authors present a new method for evaluating and optimizing offline policies that involves utilizing a \"test class\" (discriminator). The policy optimization process is conducted cautiously by establishing a \"version space.\" Computational efficiency is ensured for linear scenarios. Additionally they introduce a novel offpolicy coefficient that can yield existing or novel coefficients under various circumstances. Strengths: Unifies existing results under a common framework leading to new insights. Demonstrates promising results (e.g. a generalization of LSTD in Section 4.3). Develops a computationally efficient algorithm (Section 5.2) that overcomes the need for Bellman completeness. Weaknesses: Some of the concrete results derived from the framework are not entirely novel and require proper acknowledgment. Acknowledgment of prior work in specific areas (e.g. domainspecific results on page 2) is inadequate. Questions: Is it possible to extend the framework to incorporate nonlinear policy optimization methods (e.g. NPG) with general function approximation If so this would be significant as current approaches rely on Bellman closeness.", "Paraphrase: Summary: This study explores a method for approximating Bellman equations using a userdefined set of test functions and empirical Bellman errors to limit the value function. The framework allows for the construction of confidence intervals for offpolicy evaluation and pessimistic offline policy learning using a lower bound. Connections to concentrability coefficients from previous research are also established. Strengths and Weaknesses: Strengths: Original idea of constructing the empirical set and using it to create confidence intervals. Derivation of new theoretical results including offline policy improvement under weaker assumptions. Clear connections to concentrability coefficients from earlier work. Weaknesses: The concept of constructing the empirical set and its use for constructing confidence intervals has been explored previously. The lack of a conclusion section summarizing the work and discussing future directions. Questions: Can the Qfunction be used as the test function to construct an empirical set for the density ratio function How does this new principle relate to the principles discussed in the paper which use the density ratio as the test function References: [1] Jiang et al. Minimax value interval for offpolicy evaluation and policy optimization NeurIPS 2020. [2] Feng et al. Accountable OffPolicy Evaluation With Kernel Bellman Statistics ICML 2020. [3] Feng et al. Nonasymptotic Confidence Intervals of Offpolicy Evaluation Primal and Dual Bounds ICLR 2021. [4] Zhan et al. Offline Reinforcement Learning with Realizability and Singlepolicy Concentrability COLT 2022.", "Paraphrased Summary: This papers Bellman residual orthogonalization approach approximates Bellman equations by matching them across a customizable set of test functions. It applies this idea to create confidence intervals for OPE (offpolicy evaluation) and optimize policies in offline RL (reinforcement learning) when using function approximation. The method differs from IS (importance sampling) and regressionbased OPE approaches by utilizing weight learning techniques. Strengths and Weaknesses: Bellman residual orthogonalization aims to control Bellman error using weighted averages with weights determined by test functions. Careful selection of the test function space is crucial but challenging and the paper only provides general guidelines. The paper demonstrates the approachs use in OPE and policy optimization but comparisons with existing techniques are lacking. Questions: What are the specific benefits of Bellman residual orthogonalization methods over standard OPE and policy optimization approaches Can the orthogonalization principle be applied to solve other RL tasks besides OPE How can we develop principled methods for designing effective test function classes"], "o8nYuR8ekFm": ["Summary Paraphrase: The study proposes using model compression within a PACBayes bound to derive a \"better\" prior from data. This prior can then be used to make the bound nonvacuous. The method employs random subspace compression to project the highdimensional parameter vector into a lowerdimensional subspace. The projected model parameter is then quantized for further compression. Strengths and Weaknesses Paraphrase: Strengths: The study uniquely integrates model compression into a PACBayes bound to eliminate bound vacuity. The study presents adhoc techniques to enhance random subspace projection for better intrinsic dimension estimation. Occams razor is applied for model regularization through quantization. Weaknesses: The random subspace projection method despite efficiency gains appears as an empirical optimization technique. The evaluation is limited featuring a restricted number of baselines. Previous methods on PACBayes bound nonvacuity lack evaluations on datasets used in the study. An ablation study would clarify the effects of projection versus quantization. Questions Paraphrase: The studys contribution requires clarification particularly in differentiating its proposed methods from conventional random subspace projection and quantization. The evaluation should include a wider range of baselines. An ablation study would isolate the contributions of projection and quantization to the final results.", "Paraphrased Statement: The authors have introduced a novel method for compressing deep learning models. This method adjusts to the level of structure within the model and the training data. Inspired by the Occams razor principle the research provides an explanation for deep learning models generalization ability on structured datasets like CIFAR10. Strengths: The method is theoretically sound. The research topic is valuable. Weaknesses: Lack of comparative analysis with alternative methods. Missing results from competing methods in tables. Absence of statistical evidence supporting the superiority of the proposed approach (e.g. multiple runs with mean and standard deviation calculation). Limited focus on subspace dimension compression and Occams razor overlooking other parameter reduction techniques in the literature. Suggested Improvements: Provide missing values in tables or explain their absence. Conduct an experiment to verify the statistical effectiveness of the approach. Consider other parameter reduction methods and comment on their possible relevance. Additional Considerations: The provided links to papers offer valuable insights into alternative parameter reduction techniques. The graphs should be described more clearly.", "Paraphrased Summary: The paper introduces a novel compression approach for neural networks which significantly strengthens the generalization bounds derived from PACBayes theory. This approach combines existing bounds with a powerful compression scheme inspired by prior work on \"inherent dimension\" estimation. The compression scheme consists of: Limiting the parameter space to a random ddimensional subspace. Optimizing a ddimensional vector of learnable parameters using quantizationaware training and quantization. Employing arithmetic coding to retrieve the compressed code of the classifier. These components are combined to estimate the code length of the classifier which is then used to derive improved generalization bounds. The paper demonstrates the ability of this method to obtain dataindependent bounds that explain empirical performance on various datasets (MNIST FMNIST CIFAR10). It also highlights the approachs potential for explaining phenomena such as generalization of transfer learning and the impact of dataset structure. Strengths and Weaknesses: Strengths: Provides strong evidence supporting the conjecture that NN can be compressed significantly while maintaining accuracy. Obtains nonvacuous bounds that are closer to empirical observations than previous approaches. Highlights the limitations of datadependent bounds. Weaknesses: Many components of the approach are based on prior work reducing its novelty. The misrepresentation of the use of \"inherent dimension\" factorization in model compression. Questions for Authors: Are there additional benefits to using the Kroncker Projector beyond runtime Does the Kroncker Projector harm model compression in the \"from scratch\" scenario Could Figure 1 (right) include a plot for the training accuracy when using the dense projector Can the paper ensure that all citations are updated and correct", "Summary: This paper enhances compressionbased generalization bounds by using improved compression techniques. These tighter bounds are applicable to both supervised and transfer learning settings. The paper also provides empirical evaluations of the bounds behavior under various factors known to affect generalization. Strengths: The paper explores a promising approach that establishes a quantitative link between compression and generalization. The practical improvements achieved over prior work are significant. The discussion and empirical analysis in Section 6 are valuable and hold promise. Weaknesses: The approach lacks clarity and may lead to misunderstandings. The paper does not address the relationship between the generalization gap for uncompressed and compressed classifiers leaving open an important issue. It may not be methodologically fair to compare bounds on compressed models with those on uncompressed ones. The various tradeoffs associated with compression such as the actual generalization performance compared to uncompressed models and the computational costs are not thoroughly investigated. Questions: Can the authors clarify the relationship between the performances of compressed and uncompressed models and how they are accounted for in the analysis How do the generalization bounds evolve during training compared to the actual test error and what can this tell us about the role of the optimizer Miscellaneous: Line 99: \"Relative entropy\" and \"entropy\" should be explicitly named. Line 113: The KL bound should have h instead of h on the righthand side expressions. Figure 2 lacks (a) and (b) labels as mentioned in lines 290 and 296."], "kOIaB1hzaLe": ["Paraphrased Statement: Summary: Authors propose a general framework (NREA and NREB) with advantages over NREB. They provide optimal values for hyperparameters K and gamma based on a benchmark dataset. Strengths: Generalizing framework that advances the field. Weakness: Introduction of hyperparameter gamma whose optimal values may be difficult to determine with limited data. Questions: Clarification on the influence of hyperparameter k on Equation 2. Elaboration on the motivations for multiple k values and their benefits beyond simulation efficiency. Explanation of why NREA is considered a contrast given that it also includes a component drawn jointly with x. Computability or approximation of Zw in Equation 14 and the quality of such approximations. Convergence rate of Zw to 1 in datalimited scenarios (a general answer is acceptable).", "Paraphrase: Density ratio estimation can be done using methods that learn models for the posterior distribution. However when the posterior is expressed using a softmax function (as in InfoNCE) the resulting density ratio is biased. This bias affects downstream analyses. To address this a new loss function is proposed that eliminates the bias. Experiments show that it improves posterior estimation. Strengths: Highlights a critical flaw in the InfoNCE loss. Introduces a fix that improves posterior learning. Provides insight into optimal settings for density ratio estimation. Includes diagnostic tools to evaluate posterior estimation. Weaknesses: Simulations evaluate estimation performance by averaging across datasets. Reporting variance per dataset would be valuable. The impact of hyperparameters on convergence speed is not clear from the plots. Questions: Why doesnt learning the bias term (as proposed in [1]) solve the problem Is the normalized posterior required for diagnostics which is not obtained from the proposed method", "Paraphrase: This research proposes a technique called contrastive neural ratio estimation (NREC) to enhance the estimation of the likelihoodtoevidence ratio p(x theta)p(x) for simulationbased inference (SBI). The authors transform the parameter estimation problem into a classification problem by expanding the Kclass classification task (where the goal is to choose the best parameter thetak among K options) outlined by Durkan et al. with an additional (K1)th class consisting of samples generated independently from p(theta)p(x). They show that NREC is statistically consistent and outperforms existing methods on synthetic benchmarks. Strengths: NREC extends the \"trick\" from Durkan et al. for likelihood ratio estimation from binary classification to multiclass scenarios. The authors provide thorough experiments and empirical assessments using the SBI benchmarks suite. Weaknesses: The paper could include more selfcontained information with details currently in the supplementary moved to the main text such as related works and specifics of the SBI benchmark. It would be beneficial to provide a clearer explanation of why the equivalence of two ratios (mentioned in line 35) is important for simplifying posterior inference in SBI. The presentation lacks clarity with confusing figures and unexplained notation making the content harder to follow. The paper could provide a more comprehensive description of limitations and computational implications of setting up the classification problem for NREC particularly regarding the need to generate samples from p(x) and the prior p(theta). It would be helpful to assess if NREC remains more efficient than NREB considering these sampling procedures."], "pUPFRSxfACD": ["Paraphrase: Summary: Recent methods have tried to infer environment labels from data by learning invariant representations across multiple environments. However this paper shows that environment inference may not be generally possible. It demonstrates counterexamples where the data is compatible with multiple causal models having different invariant features. The paper argues for using auxiliary variables for environment inference instead. It provides theoretical evidence that auxiliary variables are sufficient for inference in specific settings and presents experimental results supporting this claim. Strengths and Weaknesses: The papers strengths include: A clear counterexample that limits the feasibility of environment inference. The use of auxiliary variables to overcome negative results. Experiments on realistic datasets. The main weakness is the lack of guidance on selecting appropriate auxiliary variables in practice. Questions: 1. Why is the assumption that invariants are functions of the environment variables necessary 2. Are the selected auxiliary variables similar to variables used to control confounding in causal inference Can ZIN (environment inference from invariant representations) be seen as an alternative to adjustment in this context Does ZIN fail when unblocked confounding is present", "Paraphrased Statement: This research explores the feasibility of using inexpensive auxiliary features to identify hidden environmental differences and assist in training models that are resistant to environmental variation. The proposed method ZIN is a minmax game where the learned environmental partition aims to separate samples into groups with varying distributions by maximizing performance differences across environments. The feature extractor and classifier aim to learn features that are invariant to environmental differences by minimizing these performance gaps. The proposed model is valuable when environmental labels are expensive to obtain but additional information related to environments is readily available. Strengths and Weaknesses: The key strength of this paper is the training of an endtoend model that leverages additional auxiliary information to address environmental inference. The methodology could be improved by reorganizing sections including missing baselines (e.g. Group DRO) and adding case studies with real data visualizations to illustrate the impact of hyperparameters like the number of inferred environments. Questions: The author claims that environment inference is inherently impossible without environmental labels or additional information. However this seems questionable as resamplingbased techniques like LrF contradict this claim. The effectiveness of the invariance penalty and the use of different classifiers for different estimated environments are not explained in detail. Group DRO and other generalizationenhancing techniques should be included as baselines for comparison. To fully demonstrate ZINs efficacy visualizations of inferred environments and the impact of the number of inferred environments should be shown for real data such as the distribution of genderrelated environments in CelebA. The choice of auxiliary variables should be clarified as they should strongly correlate with spurious variables to enable distinct distribution in different environments. For example the inclusion of the built year as auxiliary information for predicting house prices may raise concerns since it could be considered a causal factor.", "Paraphrase: Summary: The paper demonstrates mathematically that inferring the environment from diverse data alone is not possible necessitating additional information. It presents a framework that simultaneously learns environment divisions and stable representations using this extra information. Experiments show the frameworks efficiency. Strengths and Weaknesses: Strengths: 1. Theoretical Proof: The paper mathematically proves the impossibility of determining stable features from diverse data alone highlighting its significance. The illustrative example is clear and understandable. 2. Proposed Framework: The paper introduces a framework that combines environment partitioning and stable representation learning with a theoretical guarantee emphasizing its practicality. 3. Clarity of Writing: The writing is wellstructured and easy to comprehend. Weaknesses: 1. Auxiliary Information: The guarantee does not indicate how much information the extra information (Z) should provide to identify stable features. Assumptions and Conditions do not mention Z which should be clarified in the paper. Question: 1. Auxiliary Information Identification: Can you identify the extra information (Z) used in the synthetic dataset experiment in Section 7.1", "Paraphrase: Summary: The authors introduce a method for learning an invariant representation that concurrently learns the representation and partitions the environment based on auxiliary measurements (e.g. time or location). They provide conditions for their method to identify invariant features in both feature selection and linear feature learning settings. They also define necessary conditions for feature selection in this context. Experiments demonstrate its effectiveness with performance close to the optimal method (IRM) that relies on a predefined variable partition. Strengths: Clear writing and logical presentation of assumptions and concepts. Intuitive example illustrating the approach in the feature selection setting. Provides a valuable framework for understanding invariant feature learning and correcting misconceptions in the field. Helpful presentation of necessary conditions to guide future research. Effective experimental results show strong performance in identifying invariant features. Weaknesses: Some results are not novel for readers familiar with causality. Certain results appear selfevident. Questions: Explain the intuition behind Assumption 3. Clarify Corollary 1 regarding the meaning of \"Index\" and why h must be injective. Suggestions for Improvement: Define new quantities immediately after introduction to enhance clarity. Highlight the differences between Theorem 3 and existing IRM results. Clarify Assumption 3 regarding the potential reduction in the penalty term. Correct the typo in subscripting f with w after Equation (3). Use \"distinct\" instead of \"distant\" in Assumption 4."], "wk5zDkuSHq": ["Summary: The paper introduces a new algorithm for online agnostic multiclass boosting which is the first of its kind. It also includes algorithms for multiclass boosting in offline and online realizable settings where such algorithms already existed. Strengths: Wellwritten paper that tackles an important problem. Clear and concise proofs. Weaknesses: The definition of `ellti` in Theorem 1 is arbitrary but the proof remains valid. Online gradient descent as a learning algorithm could be replaced with Hedge for improved efficiency. The experimental results in Table 1 are inconclusive and the claim about the performance of the new algorithms compared to OnlineMBBM is not fully supported. Minor Comments: The notation for weak learner predictions (`calWi` instead of `calWti`) could be improved for consistency. Gamma a significant parameter in Algorithm 1 should be listed as an input. The definition of `calWS` as an \"independent sample...size m0\" is unclear. Questions: What is the rationale behind the arbitrary definition of `ellti` Is there an optimal choice How is `ellti` selected in the definition of weak learners Shouldnt it be passed from Algorithm 1", "Summary: This study proposes an extension of online agnostic boosting techniques to the multiclass setting by reducing it to OCO (OneAgainstOne). They extend the approach to statistical agnostic online realizable and statistical realizable multiclass boosting. They provide weak learning (WL) conditions and corresponding bounds for each condition. An empirical evaluation demonstrates the approachs efficacy. Strengths: Natural extension of existing boosting methods to the online multiclass setting. Rigorous analysis. Variety of WL conditions and bounds under a unified boosting framework. Weaknesses: Limited novelty as techniques are similar to Brukhim et al. (2020). Failure to address the difficulty of extending the framework to the multiclass setting. Strong WL assumptions requiring W to perform well without seeing the sequence of examples. Questions: Improve the clarity of Figure 1 by providing axis descriptions and explaining the interpretation of the plot. Explain the need for projecting to the line containing Deltak before projecting to Deltak itself. Relax the strong WL assumption regarding Ws performance without seeing the sequence of examples. Discuss the work in relation to other online boosting frameworks such as \"Online Gradient Boosting\" (2015) and \"Online Boosting with Bandit Feedback\" (2021). Provide formal proof for results mentioned in Appendix C to ensure clarity. Elaborate on the various WL conditions in Appendix C and discuss their relative strengths and weaknesses. Specify the source or include the details of the REWA algorithm mentioned in Appendix D. Provide an intuitive explanation for Lemma 14 considering its importance in the proof.", "Paraphrased Summary: This paper extends the findings of Brukhim et al. (2020) to multiclass problems and to both adversarial and realizable settings in online and batch scenarios. Paraphrased Strengths and Weaknesses: The presentation is wellwritten and the topic is engaging. The paper is dense with essential supplementary material needed for understanding. It relies heavily on Brukhim et al. (2020) for concept explanations. A more comprehensive journal paper format would allow for more detailed coverage and logical flow. The fractional relabelling modification is mentioned briefly in the results but deserves more attention due to its significance. The fractional relabelling approach approximates targeting the true distribution a weak learner capable of modeling the full distribution might be more efficient. Paraphrased Questions: Are the bounds tighter in nonstationary but nonadversarial settings Does Algorithm 1 simplify to the Brukhim et al. (2020) algorithm in the binary case Why does the agnostic algorithm outperform the realizable version despite the expected realizability of the datasets", "Summary (Paraphrased) This paper presents an algorithm for multiclass classification that extends an existing algorithm for binary classification. The algorithm uses online convex optimization (OCO) and weak online learners (AWOLs) to construct an online learner with a theoretical guarantee on performance. The algorithm is applicable to various settings but does not provide the best performance in all cases. The authors conduct experiments to compare their algorithm with other multiclass boosting algorithms finding that it is competitive. Strengths The problem is relevant and fills a gap in the literature. The paper is wellwritten and structured. The algorithm is not trivial to generalize to multiclass problems. The paper provides more intuition than the original work. The paper presents examples of AWOL construction. Experiments are conducted to evaluate the proposed algorithms. Weaknesses The fully adversarial setting may not be realistic. The condition on AWOLs is stronger than in the nonadaptive case. The regret bounds are not optimal for all settings. The experimental setup is limited. The complexity analysis of the algorithm is not provided. The experimental results are not analyzed in detail. Questions How does the condition on AWOLs in this work compare to the condition in the nonadaptive case Can the regret bound be extended to a simpler version of AWOLs that uses 01 loss as the gain function Additional Comments The paper would benefit from a brief overview of OCO. The proof of the main theorem could be moved to the supplement. The caption for Figure 1 should be corrected. The factor 2 in line 187 should be inside the parentheses."], "mn1MWh0iDCA": ["Summary: Experiments assess offline reinforcement learning (RL) algorithms based on three dimensions: 1. Representations: Measure feature similarity and effective rank of critic network representations to evaluate RL performance. 2. Value Functions: Rank actions based on learned Qvalues. Algorithms like TD3BC and IQL learn accurate value functions but have poor policy improvement. 3. Policies: Measure average policy improvement and propensity to take outofdistribution (OOD) actions. COMBO policy selects both optimal and suboptimal actions effectively. New Algorithm: RIQL: Motivated by observations on AWRs policy improvement. Introduces additional policy constraints to enable OOD action learning. Model Dynamics: Examines the impact of learned dynamics models on modelfree offline RL. Proposes an uncertaintybased sample selection method to enhance robustness to model noise. Strengths: Comprehensive experiments analyzing offline RL algorithms. Highlights the potential for poor representations and value functions in performant offline RL algorithms. Weaknesses: Evaluations limited to simple D4RL locomotion tasks which may not generalize to more complex tasks. RIQLs heuristicbased modification lacks theoretical justification. Questions: Can the authors consider more challenging tasks outside of D4RL locomotion Clarify the policy ranking metric description. Minor Typos: \"When does a learned dynamics model is helpful to a modelfree offline RL agent\" \"When is a learned dynamics model helpful to a modelfree offline RL agent\" \"rank the M action\" \"rank the M actions\" \"IQL uses expectile regression (ER) to for policy evaluation\" \"IQL uses expectile regression (ER) for policy evaluation\" Duplicate citations removed.", "Paraphrased Summary: This paper deeply examines current offline reinforcement learning algorithms concentrating on assessing their critics and policies using various metrics. These metrics encompass representation probing (where learned representations predict various quantities) and effective rank action ranking. The researchers detected a discrepancy between the quality of critic predictions and policy performance in certain algorithms. Based on this observation they suggest an IQL modification that outperforms the original. Strengths and Weaknesses: Strengths: Wellwritten and straightforward format Valuable analysis of offline reinforcement learning techniques Diverse metrics for separate evaluation of critic and policy Metrics aid in finetuning offline reinforcement learning components individually Practical applications for IQL improvement Weaknesses: Examines only a limited number of tasks from CQL and IQL Enhances only the IQL algorithm Questions: Can the analysis enhance other methods as well Can the analysis be applied to additional D4RL datasets (e.g. antmaze tasks)", "Paraphrase: Summary: This paper examines recent offline reinforcement learning (RL) techniques exploring the impact of various factors on their performance. It introduces innovative evaluation protocols to assess algorithms representation and behavior. Based on these analyses the authors propose a focused enhancement to the IQL algorithm which demonstrates impressive results on multiple datasets. Strengths: Thorough analysis of the representation learned by offline RL algorithms. Proposed algorithms (RIQL USS variants) exhibit strong performance supported by the papers earlier findings. Novel examination of integrating modelfree methods into a modelbased framework highlighting the limitations of a direct approach. Weaknesses: Line 208 and 269 employ the maximum mean discrepancy as an uncertainty estimate which deviates from the typical ensemble variance used in supervised learning. Evaluation focuses solely on medexp datasets an understanding of the algorithms performance on mixedrandom data (with a potentially higher extrapolation gap) would be valuable. Minor inaccuracies in the text: \"TD3BC\" should be \"TD3BC\" \"dose\" should be \"does\" The probabilistic model and modelbased training are attributed to MOPO (Minimal Offline Policy Optimization) not COMBO (Combinatorial Optimistic ModelBased Offline Reinforcement Learning). Total random seeds should be included in all tables and results. Questions: Could the analysis of modelbased algorithms be strengthened by using MOPO a more simplified algorithm to isolate the contributions of different techniques The assumption that an onlinelearned \"optimal\" Q would assign higher values to betterperforming actions may be flawed due to approximation errors. Could this present additional challenges when using a TD3learned policy which differs from the SAC policy used to generate the data"], "prQT0gN81oG": ["Paraphrased Summary: This research demonstrates how online knowledge distillation (DML) outperforms offline methods due to the optimization of teacher models for student models. To reduce teacher model finetuning costs in DML the proposed SHAKE approach transfers knowledge from the teacher to a proxy teacher offline and then performs DML between the proxy teacher and the student. Experiments in object detection and image classification demonstrate the methods effectiveness. Strengths: Innovative explanation for the superiority of online knowledge distillation (line 3749). Extensive evaluations across classification and detection tasks combining various KD approaches and data augmentation settings. Clear and wellwritten paper. Weaknesses: 1. Motivation: The claim that offline teacher models are ineffective because they are not optimized for student models (line 4449) is questionable given that both offline and DML approaches train teacher models to minimize crossentropy and KL losses. 2. Comparison: A comparison with the teacherassistant knowledge distillation (TAKD) method (AAAI2020) would highlight similarities and differences between SHAKE and TAKD. 3. Alternatives: Discuss the use of adapterbased methods for efficient teacher model finetuning and compare their performance to SHAKE. 4. Object Detection Results: The 0.31 AP improvement achieved by SHAKEReview over Review is not significant. Include results using detectionoriented knowledge distillation methods for a more comprehensive evaluation. The reviewer suggests the potential for a higher rating if these concerns are addressed in a rebuttal.", "Summary: Offline distillation despite using existing models falls short in performance compared to online approaches. This study investigates the root cause of this disparity and finds it to be the reversed distillation process where the student teaches the teacher. By modifying the process offline distillation can achieve comparable performance gains. However this still requires significant training resources. To address this a novel framework called SHAKE is proposed to bridge offline and online distillation while prioritizing efficiency over accuracy. Strengths and Weaknesses: SHAKE enables the proxy teacher to utilize the students backbone while employing an individual \"shadow head\" to maintain diversity in output representations. This approach offers several advantages: Training acceleration: Over 3\u00d7 faster than DML and finetuning the entire teacher model. No architecture selection costs: No need to carefully choose a proxy teacher architecture. Accuracy gains: Inheriting knowledge from proxy teachers also enhances the students backbone representation during weight sharing. Questions: Can the findings regarding performance margin be generalized to other datasets What is the specific definition and rationale behind the term \"shadow head\" Statistical tests are necessary to substantiate the performance differences between the proposed and previous approaches.", "Summary This paper introduces SHAKE (SHadow KnowlEdge) a new knowledge distillation approach that combines offline and online transfer techniques. It enhances performance in image classification tasks (CIFAR100 ImageNet). Strengths Clear and easy to follow Improved results on image classification benchmarks Weaknesses Lack of analysis: No visualization quantitative results or analysis to support the \"studentaware logits\" claim Limited ablation experiments on CIFAR100 that do not specifically address studentaware logits Object detection details: Insufficient technical details on object detection implementation Clarification needed on multilabel distillation and combination of ground truth labels Minor concerns: Figure 6 is not an attention map Label top5 accuracies not provided in Tables 2 and 3 Questions 1. Can the authors include top5 accuracies for Tables 2 and 3 2. Why is the paper considered theoretical in the checklist (Question 2)"], "pNEisJqGuei": ["Paraphrase: Summary: The paper suggests using value decomposition in the criticism of general actorcritic (AC) methods and demonstrates this technique using the soft actorcritic (SAC) approach. Value decomposition is not a new concept having been explored in previous works. The papers primary contribution lies in applying it to AC methods. Strengths and Weaknesses: Strengths: None specified. Weaknesses: The paper lacks a rigorous mathematical analysis of the impact of value decomposition on AC methods including its effects on biasvariance and convergence. Reweighting reward components can potentially destabilize the algorithm and a clear analysis beyond the presented experimental results is necessary. The presentation could be improved to provide a more comprehensive report. Key references on value decomposition are missing. Questions: The paper lacks thorough analysis and solutions to potential issues. Formal justification is crucial. Changing reward component weights can alter the prediction target and policy potentially impacting onpolicy learning and convergence. This requires further examination. The paper mentions using a norm but does not specify its type or the multidimensionality of the action space. This information should be provided for clarity. References: [1] van Seijen et al. (2016) [2] Fatemi et al. (2022) [3] Laroche et al. (2017)", "Paraphrased Statement: This research presents a tool to aid in diagnosing and designing reward functions for reinforcement learning (RL) agents. In RL reward functions often comprise various components (e.g. penalty for collisions bonus for velocity). The proposed tool assumes a linear combination of these components and learns a separate Qfunction for each. This breakdown permits the designer to analyze the effect of each reward term on the agents decisions minimizing the effort required to tune the reward function without guidance. The paper includes case studies that demonstrate how RL practitioners utilize this tool to design reward functions. Strengths and Weaknesses: Originality: Value decomposition is not novel but its application to guide reward function design and the proposed influence metric are original contributions. Quality and Clarity: The paper is easy to understand and the experiments are welldesigned. Significance: The proposed reward design method is essential in RL. Questions: The current method assumes a linear combination of reward terms. Can it be extended to nonlinear rewards In Section 6 some diagnosis results are applicable to SAC without value decomposition (e.g. weight scheduling Markov features). Did the authors test these techniques on SAC without value decomposition This could clarify whether diagnosis results can be applied to nondecomposed versions as well.", "Paraphrase: This paper introduces a new concept called \"value decomposition\" for understanding and enhancing reinforcement learning (RL) algorithms particularly actorcritic methods. Value decomposition suggests that the reward signal can be broken down into separate components each of which contributes to the overall value. Instead of combining these components value decomposition proposes training a separate \"Qfunction\" for each reward component. This results in multiple predictions (one for each component) instead of a single prediction. Value decomposition allows for more detailed diagnostics of RL algorithms highlighting flaws and suggesting improvements. It was previously applied to Qlearning but is now expanded to actorcritic algorithms resulting in a new method called \"SACD\" (Soft ActorCritic with Value Decomposition). Experiments show how SACD performs in various environments revealing the potential of value decomposition. The insights gained from diagnostics help identify specific reward components that influence the RL agents behavior leading to improved designs. This approach provides a practical tool for incrementally improving RL agents by analyzing the predictions of the Qfunctions and adjusting the agents parameters accordingly. Strengths: Novel perspective for diagnosing and improving RL agents. Simple and broadly applicable method. Detailed experiments demonstrate the methods benefits. Weaknesses: Ambitious scope with experiments not strictly necessary to prove the core idea. Minor writing and typographical errors. Questions: Why is SAC the focus rather than Qlearning in this paper Are there limitations to the linearity of the reward composition How does this approach differ from linear RL or assuming a linear reward decomposition", "Paraphrase: Summary: This study proposes a method to incorporate value decomposition into various actorcritic algorithms demonstrating its application to SAC (Soft ActorCritic). The resulting SACD algorithm is evaluated through extensive experiments showing its effectiveness compared to SAC. The concept of \"influence metric\" is introduced and used to assess the impact of each reward component on the agents decisionmaking. The paper also provides examples of using value decomposition to identify and address learning issues in agents. Strengths and Weaknesses: Originality: While the idea of integrating value decomposition into RL algorithms is not novel this study demonstrates its efficacy in a specific context. It lacks significant originality in this regard. Quality: The paper is wellwritten and accessible. However it has some limitations: 1. The incremental refinement of SAC to create SACD may limit the performance gain to a narrow range of experiments. 2. The iterative design process for RL agents proposed in Section 6 relies heavily on expert knowledge which may hinder its broader application. Clarity: The paper is clear and wellpresented. Significance: The work provides some value but does not contribute to new insights into value decomposition. Questions: 1. Is the performance improvement of SACD limited to the specific experiments presented in the paper 2. How does dividing value decomposition differently affect the effectiveness of the algorithm"], "krV1UM7Uw1": ["Paraphrase: Main Results: The authors investigate the robust leastsquares regression (RLSR) problem. Their work primarily focuses on developing an algorithm that provides strong resistance to adversarial attacks. The paper includes a theoretical analysis of the algorithms convergence and presents extensive experimental results demonstrating its superior performance compared to existing methods in terms of both robustness and efficiency. Assessment: Strengths: Thorough citation of related work High originality Theoretical and practical contributions to understanding adversarial attacks and defenses in RLSR Weaknesses: Limited experiments in complex settings Additional Comments: The authors could enhance the paper by conducting additional experiments in more challenging scenarios.", "Paraphrase: The paper focuses on robust linear regression models where an adversary can intentionally modify the model parameters. It proposes an enhanced method that withstands more sophisticated attacks by granting the adversary knowledge of model inputs. Specifically the paper introduces two methods TRIP and BRHT which incorporate probabilistic assumptions about model parameters and error terms. These methods improve robustness against adaptive adversarial attacks which are more aggressive than previous attacks considered. Strengths: Clear presentation of problem and approach Simple and logical methods Empirical evidence supporting improved robustness Weaknesses: Limited attack methods considered which may bias results Bayesian approaches in robustness are not entirely novel Assumptions about adversarial capabilities may not generalize to all possible attacks Questions: How closely are the proposed methods linked to the Gaussian weight prior assumption Why is the Gaussian prior effective against adaptive adversarial attacks but not against other types How mild are the assumed conditions for the robustness guarantees both theoretically and empirically", "Paraphrase: This study examines the complex problem of estimating linear regression models while handling adversarial attacks. It introduces two innovative algorithms that significantly enhance the resistance of a critical parameter known as the breakdown point under such attacks in comparison to existing methods. A remarkable discovery of this work is that incorporating even an inaccurate prior assumption can dramatically improve the breakdown point and lead to reliable estimates. This is contrary to previous research which restricted the development of reliable estimators to situations where the attackers influence on the observations was unknown. The algorithms focus on the Adaptive Adversarial Attacks (AAA) scenario where observations consist of a data matrix X and corresponding labels y. The key task is to estimate both an approximation of the true regression weights (w) and an indicator of uncorrupted data samples (S). One algorithm termed TRIP leverages a Gaussian prior on the weights effectively transforming the problem into a regularized least squares estimation. It employs a hardthresholding approach analogous to existing methods. The second algorithm introduces a more complex prior on both the weights and sample weights. It utilizes variational Bayesian expectation maximization to optimize a probabilistic model tailored to linear regression in the AAA setting. Strengths: Demonstrates the effectiveness of incorporating priors in improving the robustness of linear regression algorithms under adversarial attacks. Provides a novel approach to tackling the AAA problem where the attacker can manipulate both labels and data. Weaknesses: Needs clearer comparisons with related literature particularly regarding the corruption model and breakdown point achieved. The theoretical breakdown point for the Gaussian prior should be explicitly stated. The algorithms performance in scenarios with nonGaussian noise and adversarial attacks that specifically target data points with large values deserves further exploration."], "xdZs1kf-va": ["Summary: This paper introduces I2Q an algorithm for decentralized MultiAgent Reinforcement Learning (MARL). It addresses the nonstationarity issue in decentralized MARL and proposes using \"ideal transition probabilities\" to overcome it. These probabilities ensure that all agents converge to the optimal solution even when trained independently. The authors demonstrate how the next state in deterministic environments represents an action and induces ideal transition probabilities leading to convergence. Strengths: Provides an elegant solution for the nonstationarity problem in decentralized MARL. Clearly written with excellent presentation. Experiments on various domains showcase the benefits of the approach. Weaknesses: Limited theoretical depth with open questions remaining. Strong assumption of determinism not fully addressed. Lack of stronger results for stochastic environments. Dependence on a forward model which may present challenges in highdimensional tasks. Experiments lack comparisons to sufficient stateoftheart methods. Questions: Can \"ideal transition probabilities\" be applied to stochastic environments to address nonstationarity How does approximating the forward model affect convergence Are there alternative approaches that address stochasticity explicitly and provide tighter bounds for approximation errors", "Paraphrase: This study introduces a novel multiagent reinforcement learning (MARL) algorithm called I2Q within the decentralized trajectory decision epochs (DTDE) framework. I2Q is based on ideal transition probabilities (where agents predict optimal actions for each decision) and the Qvalue State Space (QSS) learning concept. Convergence guarantees for I2Q are established under specific conditions. Empirical evaluations demonstrate I2Qs superiority in matrix games model predictive control MuJoCo environments and StarCraft multiagent challenges (SMAC). Strengths and Weaknesses: Pros: Clear writing Diverse and comprehensive experimental evaluations Cons: Limited algorithm novelty Minor Issues and Typos: \"SAMC\" should be corrected to \"SMAC\" in line 299. Questions: Would adding a fixed randomly initialized reward function with performance tolerance improve the performance of other algorithms Given the QSS origin what are the key differences between I2Q and QSS Suggestions: Explore combining I2Q with other base RL algorithms (e.g. PPO SAC TD3) to assess its generalizability. Elaborate on the relationship between QSS and I2Q in the background or related work section.", "Paraphrased Summary: This paper introduces a significant and intriguing fully decentralized approach to MultiAgent Reinforcement Learning (MARL). The method is highly applicable in practical realworld scenarios and has been tested thoroughly demonstrating immense potential. Strengths: Wellgrounded theoretical analysis Extensive evaluations Novel perspective on MARL with potential for realworld use Weaknesses: Omission of certain relevant studies on fully decentralized MARL Absence of stateoftheart baselines Questions: While the concept is compelling and the work holds promise for realworld applications the paper could benefit from: Addressing related research in fully decentralized MARL Incorporating comparisons with leading methods to assess its potential and feasibility"], "rcrY85WLAKU": ["Paraphrased Summary: This paper introduces an efficient method for compressing tensors with tensor network (TN) structures into lowerdimensional spaces. The method leverages the inherent structure of TNs to improve compression performance. The main novelties include: A systematic framework for representing TNs in lower dimensions. Detailed analysis of the computational cost of the proposed method demonstrating its efficiency. Applications in two specific tensor decomposition algorithms: ALSCP and tensor train rounding. Strengths: Originality in leveraging TN structure for improved sketching. Comprehensive discussion of efficiency and computational cost. Strong theoretical analysis of complexity. Weaknesses: Complex presentation of algorithm details making it challenging to follow. Limited practical relevance due to the assumption of TN ranks being smaller than mode dimensions. Questions: 1. How is the structure of small TNs in the embedding network determined 2. Can you provide a simplified example of a constrained contraction tree 3. Explain the definitions of D(e1) and \\mathcalSI in more detail. 4. Is the sketching process itself contractible 5. Clarify the difference between Pcomplete and Phard. 6. Why are tensor entries drawn from a uniform distribution instead of a Gaussian distribution 7. Can you numerically evaluate the compression ratio as a function of the parameter \\epsilon", "Paraphrased Summary: This paper explores techniques for embedding tensor networks focusing on obtaining a sequence of sketching matrices. The authors establish a bound on embedding accuracy based on the sketch matrix size (Theorem 3.1). They develop an efficient sketching algorithm with a lower periteration cost than existing methods. Strengths: 1. Strong theoretical analysis particularly Theorem 3.1 on embedding accuracy. 2. Proposed sketching algorithm has reduced periteration computational cost. 3. Framework has broad applications in tensor decompositions such as CP decomposition and tensor train embedding. Weaknesses: 1. Disconnect between the theoretical part discussing general tensor network embedding and the practical part focusing on specific tensor decompositions. 2. Assumes the availability of a graph structure for the tensor network embedding but in practice this structure is often unknown. The paper does not address this issue. 3. While the periteration computational cost is efficient the overall computational cost is not addressed due to the reliance on exhaustive search. 4. Lack of experimental results comparing the proposed method to baseline methods. Question: Is there a reliable way to infer the hypergraph structure of a tensor network embedding", "Paraphrased Summary This paper presents a sketching method for tensor network (TN) data that is more general than previous approaches. It imposes TN structure on coefficients to reduce sketching size. Theoretical guarantees are established and the proposed method is used to enhance two tensor decomposition algorithms: CPALS and TTrounding. Strengths 1. Extends sketching algorithms to general TN structures broadening its potential applications. 2. Provides theoretical support for the proposed method. 3. Demonstrates the effectiveness of the method by improving CPALS and TTrounding algorithms. Weaknesses 1. Limited experimental evaluation focusing mainly on synthetic data. 2. Lack of experimental results for the applications of the proposed algorithm. 3. Experiments only used simple TN structures (TT and Kronecker) leaving the effectiveness on more complex TNs unaddressed. Questions Are there experimental results for the proposed methods application to real data Can the proposed method be evaluated on more complex TN structures"], "uOdTKkg2FtP": ["Paraphrase: This study explores the challenge of learning in cooperative games where player strategies may differ from those used to generate the training data. The proposed approach disentangles belief about previous actions from action evaluation enabling the algorithm to adapt to new teammates while maintaining its beliefs. The method has shown improvements in various Hanabi settings. Strengths: Accessible presentation using text graphics and pseudocode. Inclusion of ablation studies to highlight the algorithms merits. Weaknesses: Dense language and complex notation making comprehension challenging. Belief correction for strategic adjustments is a wellestablished approach limiting the originality of the proposed algorithm. Questions: 1. How crucial is the accuracy of the belief model for the algorithms theoretical and empirical performance given that it is learned concurrently with the policy 2. How does the algorithm handle target teammates with equilibrium strategies significantly different from those exhibited by teammates during training 3. Clarification on the true goal of ZSC which is to coordinate with unseen opponents rather than evaluate consistency across seeds. 4. Explanation of the focus on teammate differences alone rather than covariate shifts in environment dynamics or the combination of both. 5. Clarification on the specific number and measurement of trajectories considered \"a small number (3200).\"", "Paraphrased Summary: This paper presents a new approach called \"offteam learning\" to tackle challenges in multiagent reinforcement learning. It proposes two methods: Offteam Belief Learning (OTBL): Iteratively updates a belief model based on fictitious transitions to alleviate covariate shift. Offteam Reinforcement Learning (OTRL): Allows Qvalues to be learned with belief models of any distribution addressing covariate shift during testing against unseen agents. Both methods are evaluated in the Hanabi cooperative card game. OTBLs belief model generates more realistic fictitious samples than previous methods and OTRL outperforms these methods in various settings. Strengths and Weaknesses: Originality: Innovative ideas and concepts that address critical issues in multiagent systems. Quality: Technically sound with clear explanations and supportive experimental results. Clarity: Wellwritten and easy to understand. Significance: Demonstrates clear improvements over existing approaches. Minor Suggestions: Highlight best results in tables for improved readability. Increase text and label sizes in figures for better visibility.", "Summary: This paper identifies limitations in \"Offbelief Learning\" that cause significant changes in performance when evaluating zeroshot coordination during gameplay. The paper introduces two improvements: \"offteam belief learning\" and \"offteam reinforcement learning\" which enhance coordination with other agents and human players. Strengths and Weaknesses: The methods advance a specific approach to zeroshot coordination in \"Hanabi.\" Improvements are limited and the evaluation scope is narrow reducing the impact of the findings. The improvement in team play against a ranked bot is negligible. The effect on humanAI coordination is small. The paper heavily relies on external references making it difficult to reproduce the work. Critical details of the policy and belief model training are not provided. The differences between the proposed extensions and the original work are incomplete and potentially inaccurate. Questions for Authors: 1. How can you demonstrate that the \"clone bot\" is an effective proxy for human play 2. Assuming it is why is the small improvement in humanAI coordination considered significant 3. Please clarify the specific details mentioned in the \"Strengths and Weaknesses\" section (issues (a)(e)). 4. Explain the basis for the claim that zeroshot coordination is a requirement for humanAI coordination."], "tlUnxtAmcJq": ["Paraphrase: This research suggests using representations of spherical harmonic coordinates along with atomic features in building models of molecular force fields. This approach enables message exchange within nearby areas in spherical harmonic coordinates which translates to distant neighborhoods in Euclidean space. Therefore the model can capture longrange interactions that are impractical for existing messagepassing graph neural networks with hop limits. It also aids in scenarios with limited data because the reduced number of model parameters requires less data for training. Strengths and Weaknesses: This method is wellfounded and aligns with established signal processing techniques. The experiments and comparisons to previous approaches are persuasive. Questions: The use of harmonic functions in deep learning similar to wavelets has been explored previously (e.g. \"Wavelet Neural Networks With Applications in Financial Engineering Chaos and Classification\"). It would be helpful to understand how this proposal differs from and compares to those earlier applications.", "Paraphrased Statement: The study introduces a novel machine learning (ML) force field that leverages higherorder interactions and equivariant selfattention. Key features of the model include: Spherical harmonic coordinates to represent atomic features which are updated during network training Spherical neighborhoods constructed based on these representations enabling the model to learn longrange atomic relationships Computational efficiency requiring minimal parameters While the method demonstrates performance comparable to advanced techniques like Nequip it exhibits both strengths and limitations: Strengths: Efficient capture of global dependencies: Spherical harmonic representation enables the model to naturally capture interactions between distant atoms. Equivariance: The models equivariant nature enhances its data efficiency. Low parameter count: The method achieves good performance with a reduced number of parameters. Fast inference: The model is highly efficient for inference making it suitable for applications like molecular dynamics simulations. Weaknesses: Limited dataset coverage: The study only evaluates the model on one dataset leaving the generalizability of results uncertain. Lower accuracy than Nequip: Nequip achieves higher accuracy but at a higher computational cost. Hence the proposed method is only suitable when efficiency is prioritized over precision. Questions: 1. Performance with increasing nodes: How does the models performance change as the number of nodes increases This is crucial for largescale biomolecular and materials systems. 2. Diverse dataset generalization: Given that the model has been tested on a small homogeneous dataset it remains unclear how it would perform on a larger and more diverse dataset such as OC20.", "Paraphrased Statement: Summary: The authors propose a technique to incorporate nonlocal effects into molecular modeling using Spherical Harmonic Coordinates (SPHC). They transform molecular geometry into SPHC space using a wavelet transform with varying orders of spherical harmonics. This transformation establishes an equivariance between rotations in Euclidean space and WignerD transformations in SPHC preserving the models symmetries. The intent is that nonlocal interactions in Euclidean space become local in SPHC. The authors implement an adapted message passing scheme that maintains equivariance while passing messages in local neighborhoods in both Euclidean and SPHC spaces. They demonstrate their method on the example of cumulene twisting and benchmark it on the QM7X250 and MD17 datasets. Strengths and Weaknesses: The authors offer a principled approach to incorporate longrange geometric features into a message passing framework. The method is innovative and worthy of publication but the results could be more comprehensive. Originality: The use of spherical harmonic wavelets for convolution to provide geometric input is not novel. However the interpretation of these features as defining a coordinate space where message passing can capture distant interactions is a unique insight. Quality: The proposed model outperforms the NequIP baseline for the specific case of cumulene and achieves comparable performance to other methods on MD17 with a lower parameter count and higher training speed. Clarity: The paper is wellwritten and easy to follow. Significance: The significance of the paper is somewhat diminished by the absence of certain baselines. The comparison to NequIP may not be the most appropriate and the performance on MD17 could benefit from a more detailed analysis. Additionally while the method addresses the cumulene problem effectively its broader applicability to nonlocal interactions is not thoroughly explored. Questions: Are there other nonlocal interaction scenarios where SPHC representation is particularly suitable How does the method perform in capturing interactions such as dipoledipole or chargeddipole"], "rAVqc7KSGDa": ["Paraphrased Statement: Researchers introduce a new stochastic gradient ascent method to enhance the training of Gaussian Process Latent Variable Models (GPLVMs). The method involves selecting a small subset (\"minibatch\") of data to construct a GP and evaluating the posterior likelihood on the remaining data. This approach is supported by recent mathematical findings indicating that the full log marginal likelihood (computationally expensive at O(n3)) can be approximated by computing the marginal likelihood over subsets (O(b3)) and summing over terms excluded from the subset (O(nb)). The proposed alternative likelihood evaluation consists of a sum and an expectation making it suitable for unbiased stochastic gradient ascent. The researchers demonstrate the effectiveness of their method by applying it to standard and Bayesian GPLVMs with and without amortization across various datasets. Strengths: Clear exposition of GPLVMs and their evolution Significant impact by reducing the computational cost of GP training from cubic to smaller cubes and linear costs Comprehensive evaluation using GPLVMs and Bayesian prior GPLVMs with and without amortization Weaknesses: Stochastic subsampling of data for marginal likelihood evaluation is not a novel concept although the authors approach is theoretically sound. The only practical novelty over na\u00efve subsampling may lie in the additional terms in Equation (6) and (8) and the random batch size. Suggestions: Include a baseline experiment using na\u00efve random subsampling data without the additional terms to demonstrate the impact of the theoretical result. Clarify Equation 4 to avoid confusion regarding the constant term R. Provide a more detailed derivation of Equation 4 in the Appendix as it is not straightforwardly derived from Fong and Holmes (2020). Questions: Can the researchers confirm that the random batch size and additional terms in Equation (6) and (8) are the only practical novelties over na\u00efve subsampling in the standard GPLVM case Can they include a na\u00efve subsampling data baseline in their study as an ablation experiment", "Summary: This paper introduces an alternative method for training Gaussian process latent variable models (GPLVMs) utilizing the connection between crossvalidation (CV) and marginal likelihood (ML). The approach frames the leaveoneout CV objective as an optimization problem enabling efficient and scalable optimization of the ML objective for both deterministic and Bayesian GPLVMs. Experimental results reveal that the proposed framework generates more compact latent representations compared to existing techniques. Strengths: Solid theoretical basis connecting the ML optimization with CV. Impressive results indicating improvements over previous GPLVM formulations. Evidence suggests that the learned latent space may be more structured than that obtained with variational autoencoders. Weaknesses: Presentation clarity and notation issues: Equation (5) derivation from (4) is unclear. The positioning of xA in the conditioning of SPCV in (5) versus (3) is inconsistent. Insufficient explanation of the claimed drawbacks of training with inducing points. Experimental concerns: Loss curves exhibit excessive oscillations. Bayesian SAS results are missing for MNIST. Generalizability of findings is limited by consideration of only 2D latent spaces. NLPD tends to increase with active set size which seems counterintuitive. Questions: Clarification on the definition of KNN in (1). Confirmation of whether SCV(xR) in (3) should also be conditioned on z. Suggestion to define SPCV and SCCV formally in the main paper. Explanation of the discrepancy between r in (4) and R in the equation on the right. Inquiry into the interchangeable use of \\mathcalA and \\mathcalAp. Misc: While the framework may be applicable to GP models in general the focus is primarily on GPLVMs. An exploration of its performance on standard GP regression tasks would be beneficial.", "Paraphrased Summary: The paper proposes an efficient training method for Gaussian Process Latent Variable Models (GPLVM) based on a connection between marginal likelihood and crossvalidation. To make the estimation practical stochastic sampling is used to determine the size of the heldout set and the active set. Additionally amortization is employed to approximate a probability distribution. The methods performance is compared to GPLVM and Variational Autoencoders (VAEs). Strengths: Application of the idea of estimating marginal log likelihood using crossvalidation scores to GPLVM is innovative. Multiple ablation studies on different active set sizes. Weaknesses: The impact of holdout set size (R) on the estimation is not clear. The meaning of \"by first uniformly sampling R\" is ambiguous. The presentation of experimental results could be improved: Important details (e.g. kernel NN architecture) are relegated to the appendix. The metric (RMSEMAE) is not suitable for classification tasks. The choice of 2 dimensions for evaluating the representation quality is arbitrary. It would be beneficial to conduct ablation studies on both stochastic active sampling (SAS) approximation and amortization. The evaluation is limited to representation quality without comparison in terms of negative log predictive density or RMSEMAE. Including some regression tasks in the evaluation would enhance the study. Questions: 1. How is R selected and does uniform sampling of r lead to high variance 2. Why does Bayesian GPLVM underperform compared to the proposed method or VAE Additional explanation and ablation studies would be helpful."], "wS23xAeKwSN": ["Paraphrase: Summary: This study proposes a new data augmentation method PolarMix for enhancing LiDAR point cloud perception. It consists of two distinct operations: Scenelevel swapping: Divides point cloud sectors based on azimuth values and exchanges them to generate new training samples. Instancelevel copypaste: Selects instance points from one scan rotates them along the LiDAR scanning direction and integrates them into another scan. PolarMix has proven effective in boosting performance for both LiDAR semantic segmentation and 3D object detection. Strengths: Introduces a novel data augmentation method specifically for LiDAR point clouds. Demonstrates improvements in both LiDAR semantic segmentation and 3D object detection tasks. Weaknesses: Design insights and analysis: The paper lacks detailed explanations of why specific augmentation operations were chosen. Insufficient experimental analysis: The experiments do not fully support all the conclusions drawn. Elaboration: Some assumptions are made without clear logical connections. Questions Regarding Weaknesses: Weakness 1: Fidelity concerns: Augmented samples (e.g. cars scattered around the central vehicle) may not have realistic orientations. Sector selection: Why is only azimuth used for sector division rather than depth or inclination Range view networks: How is proper point projection ensured after pasting points in different LiDAR point cloud representations (e.g. range view networks) 45degree sector swapping: Why does this strategy potentially degrade segmentation performance Weakness 2: Benchmark methods: The semantic segmentation results are from older methods that are not as competitive as current stateoftheart approaches. Object detection comparison: Other augmentation techniques (e.g. copypaste) should be included for a more comprehensive comparison. Ablation studies: The effectiveness of each component in PolarMix needs to be explored more thoroughly. Weakness 3: Undefined symbols: Omega (C) delta1 delta2 sigma1 sigma2 are not defined. Adaptation effect: How does PolarMix mitigate domain discrepancy without explicit domain alignment components", "Paraphrased Summary: This paper presents an innovative approach called PolarMix that enhances cylindrical LiDAR point clouds for improved performance in 3D semantic segmentation and detection. PolarMix allows for cutting editing and mixing point clouds along the scanning direction at both the scene and instance levels resulting in a wide range of augmented data combinations. The proposed approach surpasses conventional augmentation techniques like global rotation scale changes CutMix CopyPaste and Mix3D. Strengths: Comprehensive review of existing literature on point cloud augmentation. Clear and accessible explanation of the proposed concept. Mixing point clouds on polar coordinates effectively improves cylindrical LiDAR performance. Impressive results on SemanticKITTI nuSceneslidarseg SemanticPOSS and SynLidar datasets for both 3D semantic segmentation and detection tasks. Enhanced data efficiency demonstrating similar performance with a smaller number of scans. Detailed explanation and supportive visuals to aid understanding. Weaknesses: Extension of the concept of mixing 3D scenes and rotating bounding boxes potentially limiting its novelty. Target domain is confined to cylindrical LiDAR datasets. Unclear selection criteria for baseline approaches used for comparison. Lack of information on computational overhead of PolarMix in relation to baseline methods. Questions: Would mixing more than two scenes enhance the effectiveness of the approach How does PolarMix handle instances in cluttered or distant scenes that may not be fully observed Can the authors elaborate on the selection process and limitations of the baseline approaches used for performance comparison", "Paraphrased Statement: Summary: This paper introduces a new technique for augmenting point clouds particularly those captured in road environments. The method involves cropping and combining two 3D scans at both the scene and instance levels. This concept can also be used for unsupervised domain adaptation (UDA) by merging knownlabel point clouds from a source domain with unknownlabel point clouds from a target domain. Experiments show its effectiveness compared to other point cloud augmentation methods. Strengths and Weaknesses: Strengths: Simple and easytoimplement point cloud augmentation. Potential applications in UDA tasks. Consistent performance improvements in various tasks. Weaknesses: Lack of analysis and reasoning behind the design. Questions: 1. Intuition: Why does the proposed data augmentation improve performance Does PolarMix enhance network robustness to point cloud density or noise 2. Experiments: What would happen if more than two scans were used Is misalignment a factor after data augmentation Why are point clouds cropped along the azimuth axis How does PolarMix compare to recent UDA methods for 3D semantic segmentation 3. Writing: The sentence on MinkUNet is misleading as it processes dense voxelbased architectures unlike SPVCNN. Qualitative results should be included in the main manuscript for a clearer understanding.", "Paraphrase: This paper introduces a new data augmentation technique specifically for LiDAR point clouds. It consists of two layers of augmentation: one that modifies the entire scene and another that adjusts individual instances. The authors have shown that this data augmentation method improves performance in various applications and it outperforms existing LiDAR data augmentation methods. Strengths: The concept is straightforward and effective. The evaluation is comprehensive covering three applications: semantic segmentation object detection and domain gap reduction. The proposed method surpasses current LiDAR data augmentation techniques. Questions: Will the authors make the code available after the papers publication", "Paraphrase: Summary: This study introduces PolarMix a data augmentation technique based on a polar coordinate system for enhancing the understanding of scanning LiDAR point clouds. PolarMix creates augmented data by combining two scans through scene swapping and instance rotation. The method has been extensively evaluated and shown to improve performance on three tasks (semantic segmentation object detection and domain adaptation) across three datasets. Strengths: Clear and wellorganized presentation Augmentation strategy benefits multiple tasks including object detection and domain adaptation Innovative approach of mixing data from different scans with scenewise swaps Modelagnostic meaning it can be applied to any model Weaknesses: Baselines for comparison are not uptodate Potential for limited contribution due to similarities with other methods Missing comparison to relevant work for object detection Questions: Would further subdivision along the inclination direction improve results Is it possible to mix more than two scans What do the values \u03c31 and \u03c32 in line 185 represent Why were specific rotation angles chosen for rotatepasting (line 183185)"], "o-mxIWAY1T8": ["Summary This study introduces a new layer called the Semantic Probabilistic Layer (SPL) that can be integrated with existing neural networks to model conditional distributions where additional constraints are placed on the output. The layer uses probabilistic circuits for efficient computation and enables maximumlikelihood learning of model parameters. The SPL has been shown to outperform existing methods on structure prediction tasks. Strengths Provides a bridge between neural networks and structure prediction problems. Computationally efficient due to smooth and decomposable probabilistic circuits. Backed by theoretical guarantees and empirical performance evaluations. Weaknesses Lack of selfcontainment and clarity especially regarding probabilistic circuits. Potential scalability issues as the evaluation experiments are relatively small. Incomplete baseline comparisons in experiments. Suggestions for Improvement Improve the clarity of the paper by providing definitions and explanations for technical terms. Expand the evaluation to include larger scale tasks with hard constraints. Compare the proposed method to a wider range of baselines to provide stronger evidence of its effectiveness.", "Paraphrase: This paper offers a method for predicting structured outputs that considers both enforced constraints and learned preferences. Specifically it models the distribution p(yf(x)) as a product where one component is derived using a neural network and the other is set to zero if the constraints are violated. Circuits are used to ensure practical predictions. The technique is tested in path prediction and hierarchical multilabel classification scenarios. Assessment: This approach is reportedly novel and addresses a key issue in the NeurIPS community: structured output prediction. It appears technically sound and wellfounded though expert verification is recommended. The paper is generally clear but there are some minor concerns: \"Expressive\" and \"General\" are not clearly defined in Section 2. The distinction between \"General\" and \"Consistent\" is unclear. Figure 1 and Table 3 do not display well in grayscale. Questions: Computational considerations: What are the computational implications of using this method Comparative analyses on learning and inference efficiency would be beneficial.", "Paraphrase: This study addresses the challenge of neural models trained with logical constraints performing well in downstream tasks while lacking a proper interface to enforce strict consistency in outputs. The paper proposes a set of six ideal requirements for constrained neural models and introduces the Semantic Probabilistic Layer (SPL) which enables efficient training and inference. Experimental results across synthetic and realistic tasks demonstrate that the proposed approach achieves strong performance while ensuring structural consistency in the output space. Strengths and Weaknesses: Strengths: Provides a theoretical framework for ideal characteristics of constrained neural models. SPL is a marginal inference layer capable of differentiable learning and generating fully consistent structures. Converting the model to circuits can have broad applications in various tasks. Guaranteeing output consistency in a general manner is a significant advantage. Weaknesses: Questions: The paper does not explicitly address the metric of Hamming score. However it is possible to infer from the context that the authors consider it a coarse metric that may not accurately reflect the structural recall of the ground truth and recommend its avoidance in future research.", "Paraphrased Summary: This paper introduces the Semantic Probabilistic Layer (SPL) designed to replace the predictive layer in neural networks. SPL utilizes probabilistic circuits and guarantees that outputs meet specified logical constraints. The authors demonstrate that SPL outperforms competing methods in terms of various criteria. They evaluate SPL in tasks such as path prediction preference learning shortest path finding and multilabel classification. Strengths: Clear writing and presentation of results Accessibility to readers without a background in probabilistic circuits Wellsupported claims with discussions and experiments Comprehensive comparison with stateoftheart methods Questions: None to note"], "xONqm0NUJc": ["Paraphrase: Summary: This study introduces a method for precise object categorization based on the relationships between different views of an object. The authors emphasize the importance of not only representing individual parts but also establishing connections between them. They evaluate their approach on FGVC datasets demonstrating improved performance compared to existing techniques. Strengths: The idea of using relationships between object parts for categorization is intuitively sound and experimentally supported. The motivation and methodology are clearly presented. Weaknesses: The mathematical explanation for the importance of part relationships is overly complex and could be streamlined for clarity. The authors proposed AST approach seems similar to attention layers in other architectures like TransFG making the distinction unclear. The experimental results show limited improvement from the global local and r information used in the distance metric. The authors do not provide standard deviation to assess the significance of these results. It is unclear whether the improvements are solely due to the proposed method or other factors such as data augmentation or architectural changes. Questions: How is the AST approach fundamentally different from attention layers in other architectures Can the authors provide standard deviation to validate the significance of their results Were the experimental setups carefully controlled to ensure that the improvements are specifically attributable to the proposed method", "Paraphrased Statement: Summary: This research introduces a new approach to finegrained image classification emphasizing the use of relational information between global and local image features for classes with visually similar local characteristics. They propose relational proxies that leverage this information to improve classification accuracy. Strengths: The paper proposes a novel hypothesis that relational information is crucial for finegrained classification. The hypothesis is supported by theoretical analysis and experimental validation. Weaknesses: The paper lacks visual demonstrations of the proposed methods effectiveness or limitations. Questions: How does the proposed method perform under varying conditions Are there specific scenarios where the method fails", "Summary In visual recognition tasks its crucial to understand how local features combine to form objects. This study introduces a framework and method that separate finegrained recognition tasks into two parts: extracting general features and learning relationships between parts across different perspectives. The superiority of this method is demonstrated through experiments. Strengths Thorough experimental testing shows consistent performance on various datasets. The theoretical framework is clear and wellsupported. Weaknesses Permutation Invariance Property: The necessity of the permutation invariance property for finegrained recognition is not fully justified. Undefined Term: The concept of \"relation\" is not explicitly defined making it unclear what is meant. Decomposition Justification: The reason for decomposing the problem into a relationagnostic encoder and a crossview relational function is not provided in the main text. Questions How does kdistinguishability for two classes affect the entire dataset How would the approach be modified for datasets with multiple coarselevel class groups How is kdistinguishability defined and how is the number of relational proxies (c) selected", "Paraphrased Statement: This research paper focuses on creating algorithms for recognizing specific categories of images known as finegrained image recognition. The authors suggest that relying solely on partial information is insufficient for accurately distinguishing between these finegrained categories. To address this they introduce relational proxies that utilize the connections between the overall and localized perspectives of an object. The paper provides theoretical support for the efficacy of their methods and demonstrates positive results through experiments on six benchmark datasets. Pros: 1. The methods proposed in the paper are wellreasoned and supported by both theoretical and practical analyses. 2. The paper is clearly written and straightforward to follow. Figure 1 provides valuable insights and illustrations. Cons: 1. Table 1 contains missing data which limits the ability to make comparisons. Completing the table is necessary for a complete evaluation. 2. The performance improvements are modest compared to bird image datasets such as CUB (0.3) and NA Birds (0.2). It is unclear why the proposed methods perform less effectively on bird images. Additionally uncertainty metrics (e.g. error bars) should be included in Table 1 as the accuracy margins are relatively small. 3. Limitations: To demonstrate the generalizability of the methods across different architectures testing with a wider range of network backbones is recommended. 4. Analysis in Figure 2: A comprehensive analysis should examine the correlation between the number of localized views (L) and the size of the localized patch. Questions: 1. Explain the modest performance improvements on bird image datasets (e.g. CUB and NA Birds). 2. Include uncertainty metrics in Table 1 to indicate the accuracy range. 3. Expand the testing to additional network backbones to assess the generalizability of the proposed methods. 4. Conduct a detailed analysis to explore the correlation between the number of localized views (L) and the size of the localized patch.", "Summary: The authors introduce Relational Proxies a new technique that uses relationships between local and global views of objects to assign semantic labels. Strengths: The Relational Proxies approach is novel. The authors provide a comprehensive analysis of the technique both theoretically and experimentally. Weaknesses: The paper could be clearer in explaining the proposed architecture. The authors need to conduct experiments on larger datasets to determine the impact of the Relational Proxies approach as data size increases. The performance gains from the Relational Proxies approach are limited. Questions: Are there sufficient experiments to demonstrate the effectiveness of the Relational Proxies approach Why are the performance gains from the Relational Proxies approach not more significant"], "rQ1cNbi07Vq": ["Paraphrased Statement: This research offers a novel understanding of Convolutional Neural Network (CNN) robustness in the frequency domain by quantifying the contribution of frequency components using the Shapley value. It proposes a hypothesis connecting accuracy and robustness in adversarial training. Additionally it investigates the bias in attack success and the fairness issue in adversarial training. The authors propose an input augmentation strategy by adding negative frequency components of the target class which is shown to enhance robustness in experiments. Strengths: Provides finegrained analysis of frequency component contribution in individual instances using Shapley value unlike previous studies that analyzed frequency spectrum at dataset level. Offers insights into the existence of adversarial examples and the effectiveness of adversarial training based on Shapley value quantification. Explores classspecific robustness and proposes explanations that could aid in resolving fairness issues in adversarial training. Demonstrates the effectiveness of the proposed data augmentation defense against strong attacks. Weaknesses: Lacks comparison with other methods that improve robustness by filtering out highfrequency components (HFC). Does not provide detailed analysis of the time complexity of the proposed strategy. Questions: 1. (Major concern) The paper mentions that several research works have improved robustness by filtering out HFC but these baselines are not included in the experimental evaluation. 2. (Minor concern) The authors mention that their strategy depends on a small data ratio despite the extensive sampling steps needed to calculate Shapley value. It would be beneficial to provide a more thorough analysis of the time complexity involved.", "Paraphrased Statement: Summary The paper focuses on explaining the relationship between the robustness of Convolutional Neural Networks (CNNs) and their frequency bias by proposing a method that uses Shapley values. The authors analyze the relationship between Shapley values assigned to frequency domain components and the CNNs highfrequency bias (HFC) and lowfrequency bias (LFC). Key Contributions 1. A novel method to explain the robustnessfrequency bias relationship 2. Tests conducted at instance class and dataset levels reveal connections between HFC LFC and PFC (positive frequency components) and NFC (negative frequency components) 3. A data augmentation approach inspired by the explanation to enhance model robustness Strengths 1. Relevant problem with a fresh perspective as limited studies have explored instancelevel explanations for CNN frequency bias 2. The methods overall structure and the use of Shapley values for frequency domain components are innovative 3. Reported results demonstrate the augmentation methods success in improving model robustness Weaknesses 1. Unclear rationale for using Shapley values as previous research has analyzed the significance of individual frequency components using simpler methods with similar results 2. Questionable explanations: For instance labeling \"transportation\" images as robust but HFCnegative weakens the connection between robustness and HFC 3. Limited experimental data: The observation that HFCLFC dependence varies across instances within a dataset is supported by a single instance more results would strengthen the argument 4. Minor errors: Typographical errors inconsistent axis labeling and potential confusion in Figure 4 Questions Refer to the previous section for questions regarding the paper.", "Summary Paraphrase: This study introduces a method for using Shapley values to assess the significance of various frequency elements in input images for CNN predictions. It also suggests a novel approach for training more resilient classifiers by reducing the impact of frequency components that typically hinder target class classification. Strengths: Novel integration of feature importance tools (Shapley values) and frequencydomain adversarial example analysis. Insightful analyses such as frequencyspecific Shapley value averages and classspecific accuracy under attacks. Clear explanations of related research and background. Weaknesses: Insufficient empirical support for claims. Findings based primarily on a single experiment with a single network with nonsignificant margins. Doubts about methodology raised by inconsistent experimental results (e.g. small difference between positivenegative frequency components). Ambiguous figure (Fig 4) cited to support claims regarding attacked frequency components. Inconclusive analysis of Figure 7 regarding class reliance on frequency components and adversarial robustness. Underwhelming experimental results in Table 1 with only marginal improvement in robustness at the expense of accuracy. Clarity Concerns: Frequent typos and unclear language especially in sections outlining main findings. Ambiguous terms (\"merely exploits\") used repeatedly. Confusing sentences that require clarification. Some figures (e.g. Figure 1) are challenging to interpret. Contributions: The studys contributions are limited. While it claims to extend previous research on featurebased adversarial robustness only the datasetlevel analysis seems valuable. The key finding that adversarial attacks target highfrequency features is not novel. Questions: Clarification on ambiguous sentences. Arguments for the validity of claims despite apparent inconsistencies in data.", "Questions and Responses: Weakness 1: Question: Experiments in Table 1 only use a small ResNet18 model. Could larger models such as WideResNet32 provide more convincing results Response: Yes conducting experiments on larger models like WideResNet32 would strengthen the results. Using a broader range of model sizes would offer a more comprehensive assessment of the proposed methods effectiveness. Weakness 2: Question: In Figure 7 class 3 exhibits significant negative values for highfrequency components while others show smaller negative values or positive values. Is there an intuitive or theoretical explanation for this Response: The authors do not provide an explicit explanation for this observation. Further analysis or theoretical exploration would be valuable in understanding the factors influencing the variations in Shapley values across different classes and frequency components."], "wKd2XtSRsjl": ["Summary: This research introduces a new performance measure called mutual information divergence for assessing models that convert text to images and images to text. The measure leverages features extracted by the CLIP imagetext encoder as a reference. The study demonstrates through theoretical analysis and empirical results that this metric performs better than existing measures in discriminating between real and generated images and captions. Strengths and Weaknesses: Strengths: Novel approach Superior performance compared to other metrics Strong theoretical justification Clear visualizations Weaknesses: Modest improvement over previous CLIPbased methods Limited discussion of CLIPs potential limitations Complex and occasionally confusing presentation Significance: Potential applications in various domains involving textimage relationships Despite the dominance of CLIPembedded generators the metric can still be valuable for evaluating systems that do not employ CLIP internally. Questions: The role of CLIPs training data in the metrics performance Potential performance improvements with larger training datasets", "Paraphrased Statement: Summary: This paper proposes the Mutual Information Divergence (MID) as a metric for multimodal models estimating the mutual information between conditions and outputs assuming a Gaussian distribution. Experiments on texttoimage generation and image captioning demonstrate the metrics effectiveness and validity. Notably human judgments are used to support the superiority of MID. Strengths: Introduction of the MID metric which provides reasonable results. Extensive experiments evaluating MIDs effectiveness on texttoimage and image captioning tasks. Weaknesses: Lack of analysis comparing MID to existing metrics mentioned in the literature. Questions: Are there any analyses comparing MID to metrics like FID or imagetext matching score Does MIDs focus on imagetext alignment compromise diversity in generated outputs", "Summary Paraphrase: This proposal introduces a novel metric for evaluating visuallanguage generation tasks. It utilizes negative crossmutual information with multivariate Gaussian distributions to compute mutual information. Experiments on texttoimage generation and image captioning datasets demonstrate the effectiveness of the proposed metric. Strengths and Weaknesses Paraphrase: Strengths: Clear motivation and easytounderstand framework. Openly shared source code for metric calculation. Thorough technical explanation with supportive experimental results. Ablation study showcasing metric utility on diverse datasets. Weaknesses: While the title suggests \"Multimodal Generative Models\" the metric currently applies only to visual and textual modalities. The method relies on CLIP embeddings which may limit its generalization. Experiments focus solely on texttoimage and image captioning tasks extending to a broader range of multimodal generation tasks would strengthen the metrics scope.", "Paraphrase: Summary This study presents Mutual Information Divergence (MID) a novel metric for assessing the quality of imagetotext and texttoimage generation. Like modelbased metrics such as FID MID leverages a pretrained backbone (CLIP) to extract features for score computation. The score is calculated by measuring pointwise mutual information between two feature spaces (assuming Gaussian distribution). Evaluations demonstrate that MID effectively measures the quality of imagetext pairs outperforming existing metrics on augmented datasets. Strengths and Weaknesses Pros: Simple and straightforward metric that leverages a robust multimodal backbone for crossmodal relations and easy computation of mutual information. Applicable regardless of inputoutput modality with potential flexibility in backbone selection. Experimental results indicate MIDs superiority over existing metrics and alignment with human evaluations. The authors consider various aspects of metric design and conduct experiments to demonstrate MIDs robustness across consistency overfitting and hallucination sensitivity. Cons: Evaluation is limited to a small number of models. More extensive testing across a wider range of models would be beneficial. While MIDs advantages over simplistic metrics like BLEU are evident its limitations are not explicitly discussed. Question: The assumption of Gaussian distribution for the features extracted from CLIPs transformerbased model (with layer normalization) appears contradictory. Is this assumption valid or does MID function regardless of feature normalization", "Paraphrased Statement This paper introduces an automated measure for evaluating multimodal generative models based on the divergence of mutual information. This method leverages the Gaussian mutual information framework and crossmutual information employing image and text encoders from the CLIP framework to calculate the mutual information divergence. Strengths: 1. The metric is theoretically sound and yields consistent results across different datasets and domains. 2. It aligns well with human evaluations on multiple datasets. 3. Extensive comparisons to existing metrics are provided. Potential Concerns: The results lack evidence to suggest whether the metric is absolute. It is unclear to what extent the empirical findings rely on the CLIP framework. Missing reference to VIFIDEL for evaluating the visual fidelity of image descriptions. Questions: In Section 4.1 it is assumed that fake images generated from \"foiled\" captions are of inferior quality compared to those created using regular captions. Is this assumption always true Cant foiled captions be plausible and result in higherquality generated images Some validation for this assumption would be beneficial."], "pp7onaiM4VB": ["Summary: The researchers propose a new method for creating models that map data from different domains to a common space. This mapping is suitable for data that comes from different domains and may have different distributions. These domain shifts can occur in EEG data when it is recorded from different individuals (due to structural and functional differences in brain networks) or in different sessions (due to differences in electrode placement). The proposed method consists of three steps: 1. Covariance Matrix Transformation: Data matrices from different domains are transformed into covariance matrices. These matrices form a set of symmetric positive definite (SPD) matrices which is a Riemannian manifold with an affineinvariant Riemannian metric. 2. Normalization: The first and secondorder statistics of these covariance matrices are normalized using a new algorithm called \"SPD domainspecific momentum batch normalization\" (SPDDSMBN). The normalized covariance matrices are then transformed into vectors by \"lifting\" them to the tangent space at the identity matrix using the logarithmic mapping. 3. Classification: The vectors in the tangent space are classified using a linear classifier. The main contribution of the paper is the SPDDSMBN algorithm which estimates the mean and variance of the covariance matrices on the SPD manifold using iterative updates. The researchers prove that under certain conditions the estimated mean converges to the true mean as the number of iterations increases. The proposed method is fully differentiable allowing it to be trained in an endtoend manner. This improves performance compared to classical methods that perform sequential operations associated with different optimization problems. Strengths and Weaknesses: Strengths: Originality: The proposed method is novel and leverages recent advances on the SPD manifold. Quality: The numerical experiments show high performance on EEG datasets. Clarity: The experimental part is clear and wellpresented. Weaknesses: Quality and clarity: The theoretical part about the convergence of the running mean to the Fr\u00e9chet mean is poorly written and unclear. Too many different notations are introduced which makes the paper difficult to read. Questions: 1. Clarify the confusion around Var\\thetak(\\mathcalT\\thetak) and Var\\theta(\\mathcalT\\thetak). 2. Prove that if f\\theta is Lsmooth then (14) is respected. 3. Explain how (19) gives (29). 4. Explain why the Proposition 1 is interesting and useful. 5. State the assumptions of Proposition 2 explicitly and explain why they are necessary. 6. Simplify the notation used in the paper to improve readability.", "Paraphrased Statement: Summary Update The authors revised submission has significantly improved and their score has been increased from 5 (borderline accept) to 7 (accept). The paper proposes a pipeline for EEG classification using SPD matrices addressing the challenge of domain mismatch between training and testing sets. Unique features include: Learning to choose SPD features rather than using predefined ones. Interpreting linear weights in relation to EEG channel distribution. Strengths and Weaknesses Innovative ideas tackle practical BCI challenges. The presentation is somewhat confusing and the logic is difficult to follow. The purpose of batch normalization and the relevance of the theoretical results (Proposition 1 and 2) are unclear. The phrasing of the main contribution (\"extending MBN to the SPD manifold\") lacks precision. The term \"theorybased\" machine learning framework is unclear. Excessive use of acronyms can be distracting. Questions When pooling data from multiple source subjects how is the importance of each subject considered Does the proposed method have theoretical limitations in correcting domain mismatches What is the benchmark method used for comparisons in Section 5.1 How does Equation 17 prevent the variance of the data points from being minimized Is the term \"latent observations\" appropriate Why is the term \"domainspecific\" used instead of \"intradomain\" Is Equation 7 crucial for the understanding of the paper", "Paraphrase: The authors propose a new batch normalization method called SPDDSMBN for enhancing the performance of neural networks in situations where adapting to different domains is crucial such as classifying EEG data across different sessions and individuals. This method integrates ideas from momentum BN and domainspecific BN while preserving the structure of SPD (symmetric positive definite) matrices. The authors provide theoretical justification for their approach and present compelling empirical evidence that shows how SPDDSMBN when used with an SPDaware deep neural network architecture improves crossdomain performance compared to existing techniques. This has been demonstrated on six EEG datasets involving braincomputer interface (BCI) and mental workload estimation tasks. An ablation study further highlights the significance of the proposed mechanisms in achieving these results. Strengths: SPDDSMBN is a novel method that combines and extends existing batch normalization concepts. The combination of ideas in SPDDSMBN leads to significant performance improvements over current stateoftheart approaches. The methods interpretability makes it appealing to researchers in neuroscience who may wish to investigate classrelated patterns in EEG data. The paper is wellwritten clearly organized and includes detailed code. Weakness: The manuscript could benefit from results on a broader range of classification tasks such as sleep staging seizure detection emotion classification or pathology detection to demonstrate the methods versatility.", "Paraphrased Statement: The study introduces a batch normalization framework tailored for unsupervised domain adaptation tasks involving symmetric positive definite (SPD) matrices. This framework SPD DomainSpecific Momentum Batch Normalization (SPDDSMBN) transforms SPD input matrices specific to a particular domain into domainneutral SPD outputs. It accomplishes this by gathering domainspecific batch statistics automatically. The SPDDSMBN framework can be integrated as a parameterized layer within geometric deep learning models in multidomain scenarios. The authors provide theoretical support for the accuracy of the retrieved batch statistics. Experiments using six EEG braincomputer interface (BCI) datasets demonstrate that the proposed framework improves performance in two transfer learning setups (intersession and intersubject) compared to baseline methods and ablation studies. Strengths: 1. Clear and concise presentation covering relevant technical details and background information. Wellarticulated contributions with theoretical and experimental support. 2. Innovative extension of momentumbased batch normalization to SPD manifolds for domain adaptation applicable to various fields beyond EEG BCI. Discussion of interpretability in the healthcare context is a notable benefit. Weaknesses: 1. Insufficient details on generating the interpretability maps in Figure 3 making it difficult to understand their content. 2. Numerous acronyms are used without a clear list for reference which can impede comprehension. Suggestions: 1. Provide more details on generating the interpretability maps in the supplementary material for clarity. Add explanatory sentences to Figure 3 to enhance understanding. 2. Include a list of acronyms for easy reference in the supplementary document to reduce the need for text navigation."], "xxgp42Qz6dL": ["Paraphrased Statement Summary This paper examines Stochastic Differential Generative Models (SDGMs) for imagetoimage translation. The authors propose using pretrained energy functions derived from both the source and target datasets. These energy functions aid the SDE inference process resulting in more realistic and accurate generated images. The proposed method surpasses existing SDGMbased approaches based on quantitative assessments. Strengths Incorporating knowledge from the source domain enhances SDGMbased methods effectively. Extensive quantitative experiments with relevant baselines and evaluation metrics. Clear and wellwritten presentation. Weaknesses The explanation of SGDMs requires simplification for better comprehension. The identity of f(yt) in its practical implementation is unclear. The absence of a qualitative analysis of component ablation. Questions 1. What is the practical interpretation of the drift coefficient f(yt) 2. With the inclusion of a faithful expert is sampling a starting point from x0 still crucial 3. How does the computational cost of the proposed method compare with SDGMbased baselines and GANbased methods Are the provided qualitative results representative or handpicked 4. It is recommended that the authors incorporate qualitative results demonstrating the effects of expert ablation and starting point variations.", "Paraphrase: The paper presents a type of diffusion model called Scorebased diffusion generative models (SDGMs) that incorporates information from both the source and target domains during training. This results in enhanced unpaired imagetoimage (I2I) translation. The model referred to as energyguided stochastic differential equations (EGSDE) employs an energy function pretrained on both domains to guide the inference process of a previously trained diffusion model enabling realistic and accurate unpaired I2I translation. Strengths: The concept is straightforward. The paper is wellwritten. Weaknesses: The model has not been evaluated on a wider range of commonly used datasets.", "Paraphrase: The proposed method introduces a novel approach for imagetoimage translation using scorebased diffusion generative models. Unlike previous approaches this method incorporates both domainindependent and domainspecific features by leveraging a diffusion model with an energy function pretrained on two image domains. Experimental results demonstrate the effectiveness of the proposed method. Strengths: Explores the usage of domainindependent and domaindependent features in diffusion models. Effectively measures content similarity using the energy function in the diffusion model providing theoretical insights into EGSDE. Provides comprehensive comparisons with stateoftheart methods. Weaknesses: Omits a comparison with DiffusionCLIP as a stateoftheart image translation model. Does not adequately differentiate the method from alternative image translation and content preservation approaches. Lacks a thorough ablation study to assess the effectiveness of specific components (e.g. Es and Ei) and explore alternative guidance methods. Could benefit from a human evaluation to compare the methods performance with other image editing tools. Questions: 1. Provide details on the implementation of Ei. 2. Explore the possibility of denoising from a perturbed image using DDIM reverse sampling rather than from random noise.", "Paraphrase: This study presents an EnergyGuided Stochastic Differential Equation (EGSDE)based method for imagetoimage (I2I) translation. Unlike prior methods it leverages source training data and designs an energy function that retains domainindependent features while discarding domainspecific ones. Results for face and animal face datasets demonstrate the superiority of the proposed method over existing I2I approaches using GANs and diffusion models. Strengths: Innovative interpretation of EGSDE discretization within a product of experts framework Weaknesses: 1. GANbased approach comparison: The claim that the proposed method outperforms GANbased approaches needs further support. The authors cite StarGAN a prominent GANbased approach but do not provide a comparison in the experimental section. Additionally StarGANs performance metrics as presented in the paper are superior to those reported in this study. 2. Computational efficiency: It is unclear whether the proposed energy functionbased method is significantly slower than GANbased approaches which execute in a feedforward step. If it is the authors could leverage this difference to achieve even higher quality performance. A user study could also provide valuable insights into perceptual preferences. 3. Applicability: The authors do not explore the applicability of EGSDE beyond I2I tasks such as colorization or inpainting. It would be valuable to investigate its potential in other domains. Despite these concerns the detailed comments from other reviewers have prompted the author to increase their score to 5. However it is imperative for the authors to address the concerns regarding GANbased approach comparison and provide additional experimental evidence or justification. Future research should maintain a high level of rigor and avoid overstating claims without substantial experimental support."], "l1WlfNaRkKw": ["Paraphrase: Modern computer vision models are designed to tolerate variations in input images like rotations tilts and reflections. This is typically achieved by data augmentation adding transformed images to the training dataset. To formalize this the authors propose extending the PAC (Probably Approximately Correct) learning framework by introducing transformationinvariant hypotheses and distributions. They analyze both realizable and agnostic learning scenarios calculating the modified VC (VapnikChervonenkis) dimension that describes learnability in each mode. Optimal learning algorithms are discussed. Strengths and Weaknesses: 1. Data augmentation techniques may be easier to apply to image data than text data as external resources (such as constituent parsers) may be needed for text. This could limit the practical applications of the proposed theory. 2. Unlike the VC dimension in traditional PAC learning the proposed theory does not provide a simple characterization of realizable and agnostic learnability. Different realizability modes require separate characterizations. 3. The proposed theory lacks a simple principle like the Empirical Risk Minimization (ERM) principle in traditional PAC learning. The optimal learning algorithm depends on the realizability mode. In some cases the optimal algorithm is transductive meaning it requires the entire training set to predict labels for new data points. This can impact computational efficiency. Question: If data augmentation is performed under standard PAC learning with the sample size defined as the number of examples in the original dataset how does it compare to the sample complexity estimate \\tildeO\\left(\\frac\\textVC\\textdim(\\mathcalH\\mathcalG)\\epsilon\\epsilon2\\right) for the invariantly realizable case", "Paraphrased Statement: Summary: This study examines the data requirements (sample complexity) for effective learning in scenarios where input data exhibits symmetry or invariance under a specific set of transformations. It compares the sample complexity of data augmentation (DA) with that of standard learning techniques providing insights into the usefulness of data augmentation. Contributions: The paper introduces a formal framework for learning with data subject to group transformations where the transformations are assumed to preserve the datas labels. It defines invariant hypotheses that produce identical predictions for transformed and original data. The study then provides sample complexity bounds for DA under three settings: 1. Invariantly Realizable PAC Learning: There exists an invariant hypothesis that perfectly classifies the augmented data. 2. Relaxed Realizable PAC Learning: There exists a hypothesis (invariant or not) that perfectly classifies the augmented data. 3. Agnostic Setting: No perfect predictor exists for the augmented data. The study derives upper and lower bounds on the sample complexity of DA in each setting: In the Invariantly Realizable setting DA helps improve sample complexity compared to standard learning but it may not be optimal. In the Relaxed Realizable setting DA can potentially worsen sample complexity. In the Agnostic setting the optimal sample complexity depends on the characteristics of the data. Strengths: Provides sample complexity bounds for DA based on the datas symmetry properties. Proposes an algorithm that achieves optimal sample complexity in the Invariantly Realizable setting. Clearly defines the problem setup and contributes a new theoretical perspective on data augmentation. Weaknesses: Assumes prior knowledge of the transformations which may not be realistic in practice. Some concepts such as the inclusion graph and edge orientation could be further clarified. Some proofs in the appendix could be more detailed.", "Paraphrased Statement: Summary This study explores the learnability of hypothesis classes with different levels of invariance by assuming data distributions that are invariant to group transformations. For each invariance case the authors determine sample complexity bounds for dataaugmented Empirical Risk Minimization (ERM) and a 1inclusiongraphbased learning algorithm. They introduce two \"invariant versions\" of VCdimension to restrict sample and hypothesis choices. The ERM algorithm shows suboptimal or worse performance than ERM while the 1inclusiongraphbased algorithm achieves optimal sample complexity. Strengths and Weaknesses Strengths: 1. Tackles important problems related to invariance and data augmentation. 2. Novel \"invariant versions\" of VCdimension and comprehensive sample complexity bounds. 3. Suggests potential benefits of the 1inclusiongraphbased algorithm. Weaknesses: 1. Practical relevance of theoretical results is unclear. 2. Proposed combinatorial complexity measures may be difficult to calculate in practice. 3. Lower sample complexity bounds rely on specific data distribution constructions raising questions about applicability to realworld data. 4. Lack of clear explanations for certain concepts such as \"distinguish between the original and transformed data\" and the 1inclusiongraphbased learning algorithm. Questions: 1. Provide an intuitive explanation for \"distinguish between the original and transformed data.\" 2. Provide an intuitive description of the 1inclusiongraphbased learning algorithm."], "px87A_nzK-T": ["Paraphrased Statement: Summary: This research addressed the classic problem of designing a neural network that can store patterns with high resistance to noise. It investigated various kernel attractor networks and provided new perspectives on this problem. Key Contributions: 1. Developed two mathematical models for memory networks both feedforward and recurrent for storing binary patterns. 2. Demonstrated that these models encompass common attractor networks like Hopfield networks andSDM. 3. Introduced a novel attractor network that can store a vast number of continuousvalued patterns with a finite region of attraction. Strengths: 1. Clear organization and informative illustrations for understanding the network structures. 2. Straightforward derivation of feedforward and recurrent memory networks. 3. Impressive result of storing an exponential number of continuousvalued patterns with a bounded attraction range. Weaknesses: 1. Excessive complexity making the paper difficult to follow and evaluate. 2. Limited comparison and analysis of the proposed networks with existing ones highlighting their advantages. 3. Lack of a clear definition for \"maximal noise robustness.\" The provided properties do not explicitly address noise or robustness. Questions: 1. What is the precise definition or formula for \"maximal noise robustness\" 2. Are there any comparative results showing the superiority of the proposed networks over previous approaches", "Paraphrased Statement: Summary: This research presents a framework for characterizing both heteroassociative and autoassociative memory models. It applies to existing models like Hopfield networks and sparsely distributed memories (SDMs) capturing their dynamics and capacity. Background: The paper begins by explaining kernel methods necessary for understanding the studys contributions: It demonstrates the equivalence of a kernel trick for \"onehiddenlayer nets\" to finding optimal weights with minimal norm and a general property of multilevel feedforward kernels. Contributions: Section 3 defines autoassociative memory models with binary states as recurrent support vector machines (SVMs). It establishes a relationship between attractor basin size and model robustness. This concept is extended to recent models involving continuous patterns. Strengths and Weaknesses: Pros: Wellwritten and concise paper that delivers on its objectives. Cons: Limited space (9 pages) restricts detailed discussions and future implications. Introduction and conclusion lack future work and application considerations. Questions: How will the proposed formulation contribute to the development of new memory models What are the potential applications of this work for autoassociative memory models Minor Revision: Remove one remark from lines 82 and 102 for clarity.", "Paraphrase: This theoretical paper creates mathematical models for memory networks that can recall input patterns without error. These models are based on binary classifiers and are categorized as either heteroassociate (feedforward) or autoassociate (recurrent) memory networks. The study also calculates the storage capacity of these networks using the VapnikChervonenkis dimension. The models in this paper generalize existing memory models like Kanervas sparse distributed memory and Hopfield networks. Additionally the autoassociate memory model is expanded to store continuousvalued patterns. The paper concludes by exploring how these models relate to the biological structure of neurons. Strengths and Weaknesses: This paper is a theoretical study that derives mathematical expressions for optimal memory network weights. It highlights the potential of the theory to improve memory models and enhance artificial intelligence. The mapping of the models to biological neurons is a promising avenue for further research. The presentation is clear and accessible making it easy to understand the main concepts. Questions: How might the theory lead to improved memory models"], "yCJVkELVT9d": ["Paraphrase: Summary: This study exposes a significant vulnerability in existing graph neural network (GNN) defense models demonstrating that they can be easily compromised through customized adaptive attacks. The researchers categorized 45 defense methods and developed methods for attacking seven representative models. Experiments showed that these adaptive attacks significantly reduce the robustness of previous defense models with a 40 average decrease. The researchers also identified key lessons and provided guidelines for evaluating new defense methods including a robustness unit test. Strengths: Clear and easytofollow explanations Addresses a crucial problem Proposes a new metric for assessing attackdefense effectiveness Extensive experiments demonstrating the effectiveness of adaptive attacks on different GNN defense models Provides valuable lessons and guidelines for adaptive attacking including a robustness unit test Weaknesses: Experiments conducted on only two small datasets Does not consider attacks and defenses on largescale graphs (millions of nodes) Spectral perspective on adaptive attacks not explored leaving open questions about their impact on the graph spectrum (e.g. whether adaptive attacks become lowrank attacks that reduce the effectiveness of defenses like SVDGCN)", "Paraphrased Statement: This study comprehensively examines the robustness of graph neural networks against adversarial attacks. It introduces tailored adaptive attacks to gauge robustness and offers guidelines for designing such attacks effectively. Experiments reveal that current defense methods overestimate robustness as they fail under adaptive attacks. Additionally perturbed graphs generated across different GNNs show transferability to other GNNs providing a tool to assess robustness in a blackbox setting. Strengths and Weaknesses: Strengths: Addresses a crucial issue by evaluating robustness against adaptive attacks. Reveals valuable insights such as the overestimation of robustness in existing defenses. Introduces a transferability experiment that offers a blackbox evaluation method. Clear and wellorganized presentation of ideas. Weaknesses: Tuning of hyperparameters in the proposed attack may be computationally demanding compared to visionbased adaptive attacks. The authors should clarify the reason behind the poor transferability of attacks generated from other GNNs to SVDGCN. Exploring the ensemble of attacks as in prior work (e.g. combining attacks from GCN and SVDGCN) could enhance attack strength and transferability. Questions: 1. Why do attacks generated from other GNNs not transfer well to SVDGCN 2. Have you investigated the ensemble of attacks as suggested in [38] If not consider exploring this approach.", "Paraphrased Statement: Summary: The study proposes that nonadaptive attacks overestimate adversarial robustness. Therefore the researchers advocate for using adaptive attacks as the benchmark. For GNN classifiers adaptive adversarial attacks involve modifying the adjacency matrix either before or after training. Adaptive attacks are found to be stronger than transfer attacks. Experiments demonstrate that adaptive attacks reduce robustness by an average of 40 across 7 defenses regardless of budget. The authors provide guidance for developing robust adaptive attacks. Strengths: Comprehensive review and analysis of GNN defense methods. Guidelines for designing effective adaptive attacks. Clear comparisons between nonadaptive and adaptive attacks as well as poisoning and evasion attacks highlighting the superiority of adaptive attacks. Rigorous and extensive experimental design. Weaknesses: The focus on structural perturbations (edge flipping) limits the scope of the study. Attacks on features (X) need to be investigated as they could affect the conclusions. The high attack budget of 15 can be challenging for existing defense methods. The adaptive attack approach draws heavily from previous works like PGD and Metattack. Questions: The potential impact of feature perturbations on the observed tradeoff between accuracy and robustness."], "sjaQ2bHpELV": ["Paraphrase: Summary: This study examines online policy learning in causal inference considering various policy scopes. It proposes online learning algorithms with limited regret and provides simulation results to support the theoretical algorithms. Strengths and Weaknesses: The paper is wellwritten and contains rigorous mathematical formulations and derivations. The motivation is clear and the proofs and algorithms appear to be correct. The simulation results align with expectations for a theoretical paper. While the novelty of the work is unclear it deserves positive recognition. Questions: The paper claims that most existing work focuses on offline learning. However the author notes that online causal learning algorithms exist particularly in practical reinforcement learning (RL). The primary distinction may lie in the different policy scopes considered in this study.", "Paraphrased Summary: This paper addresses the challenge of online learning for selecting optimal policies in systems with different types of treatments. The proposed CAUSALUCB algorithm tackles this problem and guarantees low regret. To enhance efficiency the study introduces a simplified structural causal model representation resulting in the CAUSALTS algorithm. While simple examples showcase the advantages of CAUSALTS certain theoretical claims need further support. Strengths: Novelty: The study proposes a novel problem and an algorithm to solve it. Clarity: The papers organization and use of examples enhance understanding. Weaknesses: Quality: Some claims lack sufficient evidence. Oversimplification: The experimental conditions are somewhat limited. Notation: The paper heavily relies on notation making it challenging to follow. Practicality: The algorithms reliance on finite supports for endogenous variables raises concerns about its applicability in practical settings with continuous variables. Questions: Theorem 3: Specify conditions for converting UCB regret to TS regret. Extensibility: Explore extending the algorithm to endogenous variables with infinite support. Notations: Provide a summary and intuitive explanations for the notations used.", "Paraphrase: Summary: The paper proposes a technique for online learning in causal systems with different policy scopes. It achieves sublinear regret as shown by simulations on small domains. Strengths and Weaknesses: Strengths: Inspired by practical problems in treatment optimization. Strong theoretical framework with sublinear learning rate. Simulated results support theoretical claims. Weaknesses: Proposed setting may be less useful compared to standard MDP settings. Difficult to understand the setting algorithm and analysis. Limited to small settings. Empirical results on simple domains significance of improvements unclear. Questions: Details on the spacetime analysis for CAUSALUCB and MINCOLLECT. Limitations due to learning efficiency or computational complexity. Simulation results for varying factors (stateaction pairs). Reason for considering only blood pressure in Experiment 2 (domain limitations or assumptions).", "Paraphrase: The paper explores algorithms that minimize regret when selecting the best treatment regime from a range of options that include both patientspecific characteristics and actions (e.g. medication). Specifically the authors introduce two algorithms inspired by a medical context. The first algorithm (UCBbased) is more accurate but requires complex planning. The second algorithm (Thompson sampling relaxation) is less computationally intensive but still requires finding an almostoptimal policy within a fixed decisionmaking framework (MDP). The study shows that these algorithms perform better than a basic approach and theoretically outperform naive UCB. However the authors acknowledge that computational limitations particularly in finding nearoptimal policies need to be addressed in more detail."], "kUOm0Fdtvh": ["Summary This paper introduces AdaFocal a method that dynamically adjusts the weight (\\gamma) of the focal loss during training. By minimizing the expected calibration error on a validation set AdaFocal calibrates the classifiers confidence to match the mean accuracy. Experiments show that AdaFocal outperforms existing calibration methods including the focal lossbased FLSD53. Strengths and Weaknesses Strengths: AdaFocal is novel and has impressive performance. It is relatively easy to implement and has applicable hyperparameters across datasets. It excels in outofdistribution (OOD) detection tasks. Weaknesses: The theoretical aspects of the loss function are not discussed. It introduces additional hyperparameters and increases training time. The accuracy using AdaFocal may not be guaranteed compared to standard training with crossentropy (CE). Reliability diagrams are not provided. Questions 1. AdaFocal may require longer training time compared to CE and other methods. 2. The performance of AdaFocal in simple settings where focal loss is reportedly worse than CE is unknown. 3. It is unclear if AdaFocal can mitigate the calibration difficulties of focal loss on the SVHN dataset. 4. The theoretical perspective of AdaFocal has not been explored by the authors. 5. It is unknown if early stopping was applied to the learner to ensure fairness in performance comparison. Minor Table 8 in the Appendix is empty.", "Paraphrased Statement: Summary: The researchers have modified Focal Loss by adjusting the focal hyperparameter dynamically during training based on the data. This approach generally results in bettercalibrated models compared to traditional crossentropy or Focal Loss especially when followed by posttraining recalibration. Strengths: Eliminates the need to manually set the focal hyperparameter. Clearly explains the problem and relevant concepts. Provides thorough and concise results within the chosen scope. Offers a new and valuable approach to model calibration which is crucial for reliability. Weaknesses: Focuses primarily on image classification so some observations may not apply to other scenarios. Sections 3 onward could use some editing for clarity. Inconsistent terminology (calibration errorgap) could be improved. The explanation of applying validation binning to training samples could be more detailed. Contains a few typos and uses the term \"innate\" when \"inherent\" may be more appropriate. Questions: Line 76: Clarification needed on the definition of \\mathbfH. Reasoning for Post Temperature Scaling CE yielding the bestcalibrated model on ImageNet.", "Paraphrase: Summary: This study presents AdaFocal a focal lossbased technique for improving model uncertainty calibration. During training AdaFocal dynamically adjusts the parameter \u03b3 in the focal loss function considering both the epoch and the specific samples being used. Experiments demonstrate AdaFocals effectiveness in calibration both with and without postcalibration techniques. Additionally it effectively aligns the average confidence with the accuracy within each category in the validation set. Strengths: 1. AdaFocal adaptively optimizes the important hyperparameter \u03b3 in focal loss making it a generalized version of Focal loss and Focal53. 2. The initial experiments provide clear motivation for the proposed method. 3. The experiments utilize datasets from various domains with differing difficulty levels showcasing the methods robustness. Weaknesses: 1. While AdaFocal adaptively determines \u03b3 it introduces additional hyperparameters such as \u03bb and Sth. It is unclear if these have a significant impact on performance especially considering the methods complexity. One would expect the optimization of these new hyperparameters to be easier than that of \u03b3. 2. Figure 2 demonstrates good correspondence between the average confidence of training and validation sets when using the same bin boundaries. However a more equitable approach would be to group the training and validation samples independently into their respective bins. Figure 9 suggests that the correspondence under this condition is weaker. Questions: The questions relate to the weaknesses mentioned above: How significant is the impact of the new hyperparameters (\u03bb and Sth) on the methods performance Can the correspondence between average confidence and accuracy be improved by grouping training and validation samples independently into bins (as shown in Figure 9)", "Summary Paraphrase: This paper introduces AdaFocal an adaptive focal loss function that dynamically adjusts the \\gamma hyperparameter for different sample groups based on their historical \\gamma values and calibration properties. The authors demonstrate the effectiveness of AdaFocal on various image classification datasets (CIFAR10 CIFAR100 TinyImageNet ImageNet) and a text classification dataset (20 Newsgroup). Implementation Details Paraphrase: The \\gamma update rule modifies \\gammat1 based on \\gammat and the discrepancy between the models accuracy and confidence within a specific validation bin. Training samples are assigned to validation bins with boundaries recalculated each epoch. The \\gammat1 adjustment is determined by the miscalibration of the validation bin the sample is assigned to. Strengths and Weaknesses Paraphrase: Strengths: Simple and straightforward update rule and algorithm. Empirical evidence supports the assumption that the confidence of the neural network on training data aligns with validation data justifying the AdaFocal method. Strong empirical results across various pre and posttemperature scaling setups. Weaknesses: Heuristics used in the method may not generalize to all datasets and tasks. Limited empirical evaluation on a narrow range of datasets and tasks. Only one model architecture tested for TinyImageNet ImageNet and 20 Newsgroup. Posttemperature scaling results show AdaFocal underperforms other methods on ImageNet and 20 Newsgroup tasks. Questions: Question: What impact would updating validation bin boundaries more frequently (e.g. every minibatch) have on the methods performance"], "mMuVRbsvPyw": ["Summary A Gaussian Mixture Model (GMM) is incorporated into semantic segmentation models providing probabilistic representation and wellcalibrated uncertainty estimates. The GMM maps extracted features to class labels and probabilities improving the interpretability and reliability of the model. Strengths: Probabilistic mapping from CNN representations to class labels. Joint training with a CNN using Sinkhorn EM for efficient parameter optimization. Good performance in anomaly detection. Weaknesses: Unclear advantages over CRFbased models in the same context. Not a fully combined optimization of EM and feature extraction. Limited to segmentation tasks. Ambiguous presentation of feature extraction. Questions: Are the EM parameters used in inference the same as those from the last training iteration How close are these EM parameters to \"ideal\" due to the limited memory bank used during training Minor Comments: Typos and grammar suggestions.", "Summary (Paraphrase): This paper introduces a generative image segmentation model called GMMSeg which utilizes generative mixed models (GMMs) to represent the probability distribution of pixel features and classes. By employing an endtoend deep learning approach GMMSeg extracts feature representations for GMMs in a discriminative fashion. This approach combines the advantages of both generative and discriminative models. GMMSeg demonstrates superior performance compared to purely discriminative models on various benchmark datasets including ADE20K Cityscapes and COCO. Strengths and Weaknesses (Paraphrase): Strengths: 1. Combining generative classification models with discriminative feature learning capitalizes on the strengths of both approaches. 2. GMMs facilitate the effective modeling of multimodal data densities allowing GMMSeg to identify and reject abnormal inputs. 3. GMMSeg is adaptable to different network architectures providing versatility. 4. Comprehensive experiments demonstrate the efficacy of GMMSeg on multiple benchmark datasets. Weaknesses: 1. The proposed generative framework is intricate and the underlying rationale for using GMMs in conjunction with deep networks is unclear. 2. The interpretation of GMM components in GMMSeg is ambiguous as they appear to be uniformly distributed across classes with an equal number of pixels. However class distributions and component pixel counts can vary. 3. A more thorough qualitative analysis of the learned components with visualizations would enhance the understanding of their significance. 4. GMMSeg incurs additional computational expenses compared to baseline models warranting a comparison of the speed of training and inference. 5. The pixelwise classification approach of GMMSeg simplifies segmentation and overlooks interpixel correlations within images. More sophisticated approaches like Markov random fields (MRFs) might be more appropriate.", "Paraphrase: Summary: This research proposes a new approach to semantic segmentation by constructing a generative model that estimates the probability of each pixel belonging to a specific class. This model differs from conventional discriminative approaches that employ a Softmax function. Strengths and Weaknesses: The proposed approach is innovative compared to existing semantic segmentation techniques. However the assumption that all classes have an equal prior probability may not always hold true in practice as the actual distribution of class occurrences is often biased. Questions: Exploring the use of alternative prior probability distributions for p(c) is suggested to improve the models performance.", "Paraphrase: Summary: This research investigates semantic segmentation proposing a Generative Semantic Segmentation Model based on Gaussian Mixtures. Experiments with various datasets demonstrate its efficacy. Strengths: Consistent performance improvements over existing techniques Extensive experiments validate the effectiveness of the method Weaknesses: Omission of relevant references: Topdown Learning for Structured Labeling with Convolutional Pseudoprior (ECCV 2016) Exploring CrossImage Pixel Contrast for Semantic Segmentation (ICCV 2021) Neglect in discussing the data distribution p(pixel featureclass) and classspecific feature learning as addressed in [b]. Questions: The impact of the proposed method on model inference speed is not discussed."], "nosngu5XwY9": ["Paraphrased Statement: Summary This study uses inverse reinforcement learning (IRL) to analyze maze exploration by mice. A unique IRL technique is presented that models shifting reward functions over time by dividing the reward into numerous goals with evolving relevance. The model simultaneously determines potential goals and a changing reward function from mouse trajectories. It can be employed in free exploration where mice lack explicit goal information. In such cases it still infers meaningful goal maps such as a \"home reward\" and an exploration reward map that resembles randomness. Despite assuming that mice have multiple goals and select actions based on these goals using a Boltzmann policy the model makes no further assumptions. Strengths and Weaknesses Strengths: Innovative and engaging research. IRL application to a practical issue. Theoretically significant \"exploration map\" that contributes to understanding exploration. Conciseness of the analysis due to the papers exceptional quality. Weaknesses: \"Exploration map\" raises questions about its origin and the mices behavior. Minor weakness due to the papers page limit. Questions: 1. Clarification on the logarithm in equation (4) and its purpose in the Qfunction formulation. 2. Correction of the terms \"coordinate ascent\" and \"gradient ascent\" to \"coordinate descent\" in lines 141 and 144 respectively.", "Paraphrased Statement: The study proposed the Dynamic Inverse Reinforcement Learning (IRL) approach and applied it to decipher the objectives of mice in a spatial navigation task. Unlike traditional IRL methods Dynamic IRL allows for gradual changes in the reward function over time. To facilitate inference the authors designed the reward function as a linear combination of weighted \"goal maps.\" The models performance was evaluated on simulated and realworld mouse data successfully recovering the animals targets in various conditions. Strengths: Clear and accessible presentation Highquality figures Introduction of temporal reward variability into the IRL model Validation on synthetic and animal datasets Comparison against an existing IRL model Weaknesses: Ambiguous number of goal maps with similar models fitting different data sets Requirement for data preprocessing before application to real data Results primarily confirming behavioral observations without providing novel insights Questions: While computational modeling is a critical aspect of neuroscience and psychology it should not be presented as the sole objective as understanding biological and psychological systems remains paramount. How did the authors select the hyperparameters on a validation set separate from both the training (80) and testing (20) data The indistinguishability of 2map and 3map models in simulation may indicate a lack of statistical power. Could longer or more training sequences address this issue How arbitrary is the data selection process Could the results change significantly without removing the first 25 trajectories Can the model account for noise in the reward function such as fatigue through a noise map or change point detection Beyond confirming behavioral observations how can the recovered weights contribute to further scientific findings for example by correlating with neural recordings", "Paraphrased Statement This paper presents a reinforcement learning algorithm specifically designed for tasks involving navigating nonstationary reward functions. This algorithm is intended for applications in neuroscience such as understanding the timevarying behaviors of animals in goalseeking or exploratory tasks. The algorithm is based on maximum entropy inverse reinforcement learning and assumes knowledge of the environments transition model. It is designed for discrete state spaces and defines rewards as a linear combination of learned goalmap features and weights. Nonstationarity in reward functions is handled by learning feature weights at each time step. While the algorithm is clear and wellwritten it may be possible to address the problem of nonstationarity by treating the environment as partially observable. Additionally the algorithm has limitations in scalability applicability to stochastic and unknown transitions and its specific feature structure may restrict it to certain task domains. Despite these limitations the algorithm has potential applications in understanding animal behavior in specific navigation tasks. The paper would benefit from further clarification on its relationship with existing work and a stronger explanation of the significance of its particular setting.", "Paraphrased Summary: This study presents a novel inverse reinforcement learning framework to uncover the timevarying reward function that guides an animals behavior. The reward function is represented as a combination of spatial reward maps each weighted differently. This method is evaluated using simulated data and realworld mouse tracking data. Paraphrased Strengths and Weaknesses: Strengths: Intriguing and wellmotivated approach. Innovative application of inverse RL to timevarying rewards. Wellwritten and clear paper. Comprehensive and organized figures and supplementary materials. Weaknesses: Limited impact on the machine learning community in terms of new algorithms. Missing labels in some figures hindering interpretation. Paraphrased Questions: How is the simulated mouse trained in Section 4.1 Is a specific RL algorithm used Can the framework be extended to handle scenarios with more than two reward maps (e.g. 5 or more) Consider discussing recent NeurIPS studies on RLIRL in decisionmaking problems in neuroscience (e.g. \"A neurally plausible model learns successor representations in partially observable environments.\" and \"Inverse rational control with partially observable continuous nonlinear dynamics.\")."], "xNeAhc2CNAl": ["Summary: This paper thoroughly explores the quantization of pretrained BERT models using low bit rates (1 bit and 2 bits). The authors identify four key observations: 1. Extended training and optimized hyperparameters minimize performance degradation. 2. Singlestage knowledge distillation is sufficient. 3. Data augmentation is crucial. 4. Pretraining distillation may not always enhance model compression. Based on these findings the paper introduces XtrmC a model compression technique that includes: Lightweight layer reduction using simple rules Extended quantizationaware training (QAT) Onestep knowledge distillation Data augmentation Experiments demonstrate that XtrmC achieves significantly higher compression ratios compared to previous methods like TernaryBERT and BinaryBERT. Strengths: Clear and organized presentation Adequate comparison with previous works Extensive experimental support The potential of avoiding high training costs for pretraining distillation SKIPBERTs promising results Weaknesses: Empirical nature of findings Lack of intuitive explanations for the observed phenomena Longer training schedules might not be practical Sensitivity of BERT finetuning to random seed which may affect XtrmCs robustness Questions: Advantages of previous approaches (e.g. ternarytobinary gradual quantization or multistep knowledge distillation) Potential for reducing QAT time Computational feasibility of extended training schedules Sensitivity of XtrmC to random seed given the known sensitivity of BERT finetuning", "Paraphrased Statement: Summary: This paper presents an empirical study on compressing BERT using distillation and quantization. The authors finetuned compressed BERT models and found that longer training iterations with low learning rates enhance quantization without requiring techniques like multistate quantization or weight splitting. Based on empirical observations they introduce XtrmC a method that jointly quantizes and distills an uncompressed BERT achieving acceptable performance with a 50fold model size reduction. Strengths: Comprehensive experimental results that report findings under various settings. Efficient compressed BERT strategy without complex or specific training methods. Weaknesses: Incremental novelty as it primarily discusses the effectiveness of strategies from TinyBERT TenaryBERT and BinaryBERT. Lack of originality compared to prior works in the field. Limited discussion on BERT variants (e.g. RoBERTa) and generative models (e.g. GPT). Questions: 1. How does the performance of a finetuned tinyBERT with extended training iterations compare to SkipBERT 2. Could the benefits of onestage distillation with greater training budgets and optimized learning rates outweigh the disadvantages of multistage distillation 3. Why was STE used for training during backpropagation given the availability of more effective alternatives in the literature", "Paraphrased Statement: The study investigates how different training stages influence the extreme compression of Transformer models for natural language processing (NLP). It proposes a simplified architecture and training process that drastically reduces model size. Strengths: Indepth analysis of how training length impacts heavily quantized network compression. Demonstrated that longer training with data augmentation enhances the accuracy of binarized networks. Simplified model with fewer layers (structured pruning) and a single distillation stage maintains accuracy. Weaknesses: Lack of a detailed analysis of computational complexity. Contribution is empirical without theoretical explanation for improved performance with longer training and reduced distillation stages. Important study details are in supplementary material not the main text. Questions: Why does extended training improve quantization of BERT models How does additional training aid in this process Would increased training time for competing methods achieve similar results without architectural changes Could a finer learning rate grid search further improve performance What is the actual computational tradeoff between the proposed method and competing methods considering longer training times", "Paraphrased Statement: Researchers conducted a comprehensive analysis of key hyperparameters and training tactics utilized in previous ultralow bit precision quantization studies. This led to the discovery that current baselines are likely undertrained. Based on their analysis they have devised a novel compression pipeline called XtrmC. This paper primarily investigates BERT. Strengths: Thorough and insightful examination of existing literature Comprehensive evaluation of key factors providing a foundation for future research Wellpresented experimental findings Acknowledgment of research limitations and future directions Weaknesses: Primary focus on the analysis rather than the new methodology More elaboration on the compression technique would enhance the overall presentation given the title emphasizes extreme compression rather than simply a literature review."], "lSqaDG4dvdt": ["Paraphrased Statement: Summary: This research introduces a framework called EZNAS which utilizes an evolutionary algorithm to locate zerocost proxies. The framework leverages an expression tree to model the calculation of a zerocost proxy and employs the DEAP evolutionary algorithm platform. Comprehensive experiments on NAS benchmarks demonstrate the methods efficacy. Strengths: Compelling and sound motivation: EZNAS focuses on discovering effective zerocost proxies instead of manually designing them. Impressive experimental outcomes: The method exhibits strong performance. Weaknesses: Limited novelty: The expression tree representation and evolutionary algorithm are not novel concepts. Informal presentation: Figure 1 should follow Figure 2. Figure 2 requires a detailed legend explaining the significance of T1(G) T2(G) T3(G) and T4(G). Figure 5 should be labeled as Table 5. Question: 1. How is the accuracy of architectures sampled from NDS obtained (Line 253256) Are these networks trained from scratch or is the accuracy directly obtained from NASBench301", "Summary This research aims to develop a trainingfree method to evaluate neural network architectures at initialization. Instead of relying on human experts the authors propose a genetic programming framework to automatically discover zerocost proxies. These proxies are represented as computational graphs of operation trees. The initial population is generated randomly and at each iteration a subset of the dataset is used to evaluate the proxies. Candidates with higher correlation to final performance are preserved. Experiments show that the discovered zerocost proxies outperform manually designed scoring functions. Strengths Offers a valuable insight for the AutoML community on using evolutionary search on computational graphs for specific operations with vast search spaces. Demonstrates the ability of searched proxies to generalize to various datasets with minimal search cost. Weaknesses Fails to acknowledge or discuss prior work on predictorbased NAS despite the paper presenting a new predictorbased approach. Misses key baselines particularly zerocost NAS on neural tangent kernels. The restricted search space limits the applicability to more advanced architectures. Questions What are the structures of the discovered EZNASA and EZNASB trees Visualizing or comparing them with typical zeroshot proxies could be beneficial.", "Paraphrased Statement: Summary: This study automates the process of finding free proxy models in neural architecture search (NAS). Lowlevel mathematical operations are combined and chosen via genetic programming to create effective proxy models that are indistinguishable from the original models. The proposed approach has been evaluated and proven effective on NASBench201 and NDS benchmarks. Strengths: 1. The paper presents a clear motivation and is wellwritten. It acknowledges its limitations transparently. 2. The method offers significant value for NAS research as discovering free proxy models is crucial for efficient architecture search. Using automl algorithms to evolve proxies from simple mathematical operations aligns with the industrys automl trend. 3. The discovered proxy models exhibit better accuracy in ranking candidate model performance compared to handcrafted proxies. They also generalize well to different search spaces. Weaknesses: 1. The proposed method includes complex elements like program representation NN statistics generation and an evolutionary scheme. Providing code would enhance the utility of this work. 2. The approach builds on concepts presented in previous research ([1]) which may reduce its perceived novelty. Recommendation: Weak accept.", "Paraphrased Statement: Summary: Despite automating network design Network Architecture Search (NAS) can be computationally expensive due to the need to evaluate several models. Proxy methods have been proposed to reduce costs but they suffer from low productivity and poor generalization. This paper proposes EZNAS a methodology that leverages genetic programming to automatically generate interpretable and generalizable ZeroCost Neural Architecture Scoring Metrics (ZCNASMs) that exhibit excellent scoreaccuracy correlation. Strengths and Weaknesses: Clarity: The paper is wellstructured and easy to understand with clear illustrations of concepts. Quality: The experiments appear welldesigned and the methodology is sound. Originality: EZNAS is innovative in that it automates the discovery of ZCNASMs unlike existing methods that rely on human intuition or theory. This approach can reduce bias and improve ZCNASM effectiveness. Significance: EZNAS outperforms previous metrics and methods in terms of scoreaccuracy correlation. This has the potential to reduce human bias in NAS and improve the efficiency of network design. Questions: The provided information only includes scoreaccuracy correlation. Please provide actual test accuracy and statistics as well as detailed information on sampled networks (e.g. training time search time FLOPs model size) for Table 15."], "rhdfTOiXBng": ["Paraphrase: Summary The researchers used the NaturalProofsGen dataset to finetune the GPT3 language model with two new loss functions: proof generation using nearby references and reference reconstruction. To promote generations that include input references they applied lexical constraints during constrained decoding. The models performance was assessed using various metrics including the correctness and usefulness of proof steps. Finally human evaluators compared the models errors on specific subsets of the evaluation dataset. Strengths The approach is clearly explained and accessible. The experiments and analyses are thorough and wellpresented. The error analysis is detailed and provides valuable insights for future research. Weaknesses Additional information about the specific GPT3 model (curie) should be provided in the appendix. The appendix should include proper citations for baseline models mentioned in lines 190199. Examples in the main text would clarify the distinction between stepwise and stepwise decoding. Questions What order was the training data presented to GPT3 during finetuning Was it randomly selected or were reference reconstruction examples presented first followed by incontext examples using those references", "Paraphrased Statement: Summary This paper introduces NaturalProver a proof solver that combines machine learning (ML) and symbolic information to extend the capabilities of large language models (LLMs) beyond natural language processing. NaturalProver excels in two aspects: 1. Generating complete mathematical proofs (though limited to a small number of steps) 2. Assisting with nextstep proof creation potentially applicable to any proof regardless of length The paper demonstrates NaturalProvers accuracy in nextstep assistance (claimed to be correct 40 of the time). It compares NaturalProver to a GPT3 baseline model. Strengths and Weaknesses Strengths: Clear and wellmotivated research Timely topic of leveraging LLMs for diverse tasks Dual approach of full and nextstep proof generation Novel use of LLMs for mathematical proofs Inclusion of LaTeX support for proofs Weaknesses: Limited comparison to other LLMs besides GPT3 Potential for human bias in proof evaluation due to awareness of the system generating proofs Grading accuracy concerns due to human graders of varying expertise Possible incorrect flagging of correct proofs due to deviation from the \"golden proof\" Questions: The authors should address the potential for bias and incorrect grading in their human evaluation methods.", "Paraphrased Statement: This study investigates the possibility of using large language models (LLMs) to prove theorems in natural mathematical language. Two main tasks are examined: creating the next step in an incomplete proof and constructing complete proofs from only a theorem statement. The researchers provide a dataset for these tasks (based on the NaturalProofs benchmark). Initial attempts to finetune GPT3 on these tasks fail because the generated proofs often include \"hallucinated\" references or logical errors. To solve this the study introduces \"NaturalProver\" which enforces the inclusion of specific background references in the output. When compared to GPT3 NaturalProver performs better on metrics such as usefulness and correctness as assessed by human evaluators. Strengths and Weaknesses: Originality: This research is the first to tackle the challenging task of generating proofs and proof steps from natural math language to natural math language endtoend. Quality (Method): The proposed method effectively decomposes the problem into reference retrieval and proof generation. However it assumes that all necessary references will always appear in the final proof which may not hold in practical situations. The paper does not assess the approach using references retrieved by a neural network. Experiments: Human annotators evaluate the different approaches on criteria that are challenging to quantify objectively. Automated metrics correlate well with these criteria but some concerns remain. Notably GPT3 with beam search decoding is not included as a baseline for comparison and error bars are absent from the main results. Clarity: The paper is written clearly and is easy to understand. However the meaning of the \"Incontext\" column in Table 4 is unclear. Significance: Endtoend theorem proving in natural mathematical language is a crucial issue. This study provides valuable insights into the problem including the reasons for the failure of naive LLM applications and potential future challenges. The lack of objective success measures may hinder future progress but the proposed automated metrics aim to address this. Questions: 1. Would a version of constrained decoding with soft rather than hard constraints be reasonable 2. Can NaturalProver be modified to provide an estimate of the proofs correctness"], "kgT6D7Z4Xv9": ["Summary Deep Equilibrium Models (DEQs) are constructed from layers that solve fixed point equations. Key features include: Layers may have varying numbers of solutions (unique multiple countably infinite uncountably infinite). Testtime computational budget can be adjusted to balance accuracy with budget in the fixed point solver. Strengths Potential for improved generalization with increased testtime budget. Innovative interventions in Section 6. Weaknesses Lack of understanding on how the number of solutions affects performance. Limited guidance on utilizing the testtime computational budget effectively. Unclear discussion of fixed point iteration method omitting convergence conditions. Weight tying interpretation is valid only under certain conditions. Lack of motivation and clarity regarding Algorithm 1 and Path Independence Hypothesis. Limited theory and discussion on the validity of the Path Independence Hypothesis. Questions Clarification on the understanding and application of Algorithm 1 (FPA). Motivation for seeking adversarial examples in the zspace. Explanation for the claim that learned networks are independent of random initialization. Definition of \"fixed point solver\" and \"residuals\" in Figure 7.", "Paraphrased Statement: Summary: This paper examines the \"path independence\" of equilibrium models where a system consistently reaches the same steady state regardless of its starting point. The study proposes that path independence is highly correlated with performance on challenging tasks. It introduces the FPA score a metric to measure path independence and demonstrates that it is strongly associated with model accuracy on unseen samples. Strengths: Investigating adaptive computational budgeting is a valuable topic. Weaknesses: Algorithm 1 does not correspond to the textual description and lacks a fixed point equation or rootsolving process. Figure 5(a) does not clearly indicate comparisons between different solvers. The reference to \"limit cycles\" in Section 7 is unclear. The term \"global stability\" is used ambiguously referring to both overall convergence and convergence to a specific point. The caption for Figure 2 does not explain why the blue and gray lines refer to different data sets. Questions: 1. The requirement for both weight tying and input injection to achieve path independence seems inconsistent with the definition of path independence which does not necessitate meaningful limiting behavior. 2. The citation of Geng et al. 2021ab for the use of an identity matrix in Eq. (1) is incorrect this was proposed by Fung et al. 2021. It is unclear why Geng et al. 2021a is cited in this context.", "Paraphrased Summary This study suggests a strong connection between generalization and \"path independence\" (PI) where the models state remains stable given inputs and weights. The authors present three key findings: 1. PI networks (like deep equilibrium models) outperform nonPI models in terms of upwards generalization. 2. They introduce a simple metric called Fixed Point Alignment (FPA) to quantify path independence demonstrating a strong correlation between FPA and upwards generalization. 3. An experiment shows that FPA is a robust metric for path independence even when training PI models is intentionally disrupted. Strengths and Weaknesses The authors write clearly present their thesis effectively and provide empirical evidence. The proposed FPA is intuitive and easy to use. However as an empirical study more extensive experiments are needed to validate the hypothesis. The benchmark dataset used is relatively easy raising questions about whether PI models would generalize as well on harder tasks. Questions 1. How well do PI models generalize on tasks with lower baseline accuracy (e.g. 90) 2. While the authors claim convergence is not necessary the proposed FPA metric measures convergence. How does FPA detect limit cycle behavior in models", "Summary Paraphrase: This paper introduces the idea of path independence and proposes using the FPA score to evaluate it in networks. The study demonstrates that the FPA score links with the upward generalization capability of equilibrium networks backed by experimental findings. Strengths and Weaknesses Paraphrase: Strengths: Clarity: The paper is wellwritten and straightforward. Novelty: The phenomenon is intriguing and the problem has potential for practical applications. Weaknesses: Lack of Theoretical Basis: The claims rely solely on experiments without theoretical justification. Insufficient Experimental Rigor: The experiments could be strengthened. Questions Paraphrase: 1. Meaning of x2z2z2 in Algorithm 1: The function of x2z2z2 in Algorithm 1 is unclear. It appears that only z1 is needed and its computation is independent of x2z2. 2. Validity of FPA Score: The comparison of FPA and attacked FPA in Table 1 does not adequately support the claim that FPA score reliably measures path independence. The table only considers two cases (PI and nonPI) while the FPA score is continuous. Additionally the significant gap between FPA and attacked FPA scores for nonPI networks raises questions about its effectiveness as a measure. 3. NonPI Networks and FPA Score: Figure 7(b) suggests that the correctness of nonPI networks is not always positively correlated with FPA scores when FPA is below 1. This requires further explanation particularly for nonPI networks where the significance of FPA score differences should be clarified. 4. Solver Convergence: The discrepancy in residuals between the two solvers presented in Figure 7(a) is insufficient to prove that they converge to different fixed points. A more rigorous approach would be to calculate the relative difference between their solutions. Additionally demonstrating that the attacked FPA score is close to 1 for pathindependent networks would strengthen the argument."], "zKBbP3R86oc": ["Summary Paraphrase: The authors present a novel technique for determining the optimal quantization precision for neural network parameters. They generate a random noise tensor and scale it dynamically for each parameter. The amount of tolerable perturbation determines the appropriate precision. Experiments on standard models in CIFAR and ImageNet demonstrate the effectiveness of the approach. Strengths and Weaknesses Paraphrase: Strengths: Intuitive and theoretically compelling approach. Unifies quantization with pruning. Promising potential for inspiring new ideas. Good empirical results (though more could be included in the main paper). Simple and implementable method. Wellwritten paper and clear visualization. Weaknesses: Perparameter precisions may not be practical in realworld applications. Limited model evaluation (only ResNets considered). Activation quantization is not sufficiently discussed in the main text. Limited exploration of quantization bits beyond 4 bits. Questions: Why are layerwise precisions not discussed in the main paper Can the approach be extended to other models such as MobileNetEfficientNet or Transformers How can the proposed method be adapted for activation quantization", "Paraphrased Statement: Summary: This study proposes assigning different precision levels to individual parameters during quantization. The precision allocation is calculated based on the magnitude of disturbances that the parameters can tolerate. Smaller disturbances warrant higher precision. Training includes a penalty term to ensure that precision allocation stays within a predefined budget. Strengths: Clear interpretation of precision allocation as the maximum permissible disturbance in parameter estimation. Weaknesses: Confusing writing (refer to Question section for details). Questions: 1. Why allocate precision to each parameter instead of a layer The main goal of quantization is to optimize performance and resource utilization. Different layers may require varying levels of precision to maintain performance. Assigning precision to each parameter allows for finegrained control and more efficient utilization of the available budget. 2. How is the learnt precision applied to each parameter The learnt precision determines the number of bits used to quantize each parameter. Higher precision (more bits) is assigned to parameters that can tolerate larger disturbances while lower precision (fewer bits) is assigned to parameters that can withstand smaller disturbances. 3. How is the quantization advantage realized with different precisions for each parameter Quantization benefits occur during inference when parameters and activations are represented with uniform precision. However assigning different precisions to parameters does not negate the advantages of quantization. By optimizing the precision allocation the model can achieve similar or better performance while minimizing computational and storage requirements.", "Paraphrased Statement This manuscript proposes a novel framework (SMOL) that: Goals: Reduces energy consumption during training and inference time. Mechanisms: Considers task objectives. Quantizes parameters into lowprecision numeric representations. Initializes network weights with fullprecision values and learnable noise magnitudes. Uses noise magnitudes as proxies for discrete bit precisions. Replaces noisy weights with quantized weights (after training). Results: Experiments on ResNet20 DCGAN (CIFAR10) ResNet18 and ResNet50 (Imagenet) compared to weightquantized CNNs for image classification. Demonstrates incremental improvements in performance. Strengths: Addresses an unexplored problem in machine learning: lowlevel mixedprecision numeric representations. Provides clear explanations of precisions and noise perturbations. Incorporates pruning of zeroprecision weights. Weaknesses: Claims of energy efficiency inference time and memory savings are not quantified or mathematically demonstrated. Results are incremental. No comparisons to standard metrics or networks. Minor spelling and grammar errors. Questions: Computational efficiency: How computationally efficient is the proposed method DCGAN generation quality: Why does lower precision representation yield higher generation quality for DCGANs without significant changes in network loss Potential generalization: Could the method potentially generalize to large language models"], "qcRgqCXv1o2": ["Paraphrase: The study proposes a robustness certificate for graph classifiers under adversaries that alter the graph structure specifically targeting graph isomorphism. The certificate is derived from a convex lower bound of orthogonal GromovWasserstein (OGW) distance which serves as the underlying metric. Using resistance distance as a proxy for shortest path distance the certificate is sound but potentially incomplete. Additionally an attack strategy is developed to demonstrate cases of nonrobustness. Strengths: Clear and accessible writing Addresses an important problem in the field Employs robust GromovWasserstein distance and feasible relaxation techniques Offers a complementary attack strategy for experimental evaluation Weaknesses: Experimental evaluation is limited to singlelayer linear GCN models Experiments focus on a fixed local budget which may constrain solutions and overstate certificate tightness Threat model is less intuitive than simpler edge perturbation models Questions: 1. Can the certificate be applied to multilayer GCNs and if so how do the results differ 2. How do the certificate results in Figure 3 change with larger local budgets 3. What graphs can be reached from an original graph under very large (practically infinite) local and global budgets with and without enforcing these budgets 4. What is the accuracy of the certified model and how does it compare to stateoftheart models 5. How do vanillatrained models perform in certification scenarios and does robust training reduce clean accuracy", "Summary: The paper introduces a new type of certifiable discrepancy called Orthogonal Gromov Wasserstein (OGW) for graph classifiers. Unlike previous graph distances (e.g. L1 distance between adjacency matrices) OGW accounts for graph symmetries and is invariant to node ordering. To compute the certificate the authors use a convex relaxation that leverages resistance distance as the metric and designs a convex relaxation of this computation. Additionally they present a convex relaxation of the OGW discrepancy measure. Strengths: Addresses an important problem of certifying graph classifiers under a new budget using OGW. Extends certification beyond global and local budgets. Convincingly motivates the use of effective distance. Weaknesses: Only experimented with a simple GCN graph classifier. Certificate is computationally expensive (O(n3)). Scalability concerns for larger graphs. Questions: What does \"bona fide distance\" mean Does the Kantorovich dual exist for Gromov Wasserstein distance How is setting Z equivalent to equation (20) Why does Z1 1 What is the spectral constraint of \\mathcalE and why should it be used", "Paraphrased Statement: Authors explore the certification and adversarial attack of graph convolution network (GCN) classifiers. They introduce an orthogonal GromovWasserstein (OGW) discrepancy to measure the effectiveness of attacks on graph topology. OGW offers a convex approximation of the conventional GW distance enabling efficient computation. Additionally OGW provides a tight outer convex approximation for resistance distance on graph nodes. Weaknesses: 1. Questionable organization: Introducing the OGW technique in Section 2 without first providing context for the problem may hinder readers understanding. 2. Undefined notations: Several terms are used without definitions making the paper challenging to follow. Questions: 1. Difference between E and \u02c6E (Line 109 Eq. (7)) 2. Purpose of the \"f\" notation (Line 134 Eq. (14)) 3. Difference between D and mathcalD (Line 142) 4. Definition of Omega 5. Difference between M and M"], "yZcPRIZEwOG": ["Summary The authors propose a formal method for incorporating a probabilistic understanding of LTL satisfaction into RL environments. They combine the state spaces of the LTL B\u00fcchi automata and the RL environment to create a comprehensive searchable space. Based on the ability to sample any (state action) pair as needed they present algorithms that separately handle the strongly connected components of reachable states that endlessly achieve the goal condition a necessity for infinitetrace LTL. Strengths and Weaknesses Originality: The unique approach to treating SCCs of the merged state space and the strategy for movement between them is original. However comparisons with recent works on reward machines for LTL control in RL are lacking. Quality: The quality appears high but evaluation is challenging due to the papers density. Clarity: The paper is dense and reads more like an extended abstract for a JMLR paper. Key sections with clarity issues include Definitions 3.2 and 3.3 Definition 4.2 and lines 217 to 234. Significance: The strong results are limited by the small size of empirical domains and sampling requirements. The applicability to RL environments with larger state spaces remains uncertain. Questions: 1. How can the method be modified to handle sample traces instead of arbitrary sampling of (state action) pairs 2. How can the techniques be used in environments where the reachable state space is too large to enumerate (e.g. with 100 atomic propositions)", "Paraphrased Statement: Summary: This research paper presents a new framework for reinforcement learning under strict logical constraints. The framework called LCP prioritizes satisfying the constraints over an infinite time horizon while also considering a secondary cost function representing preferences. It relaxes assumptions from previous approaches and provides theoretical justification for its claims. Experiments in two RL domains demonstrate the effectiveness of LCP compared to an existing method. Strengths and Weaknesses: Originality: The work combines existing concepts into a novel approach providing strong theoretical guarantees and differences from prior methods. Quality: The proposed modelbased approach LCP is supported by theoretical analysis although some proofs may require verification. However the framework is conceptually clear and the claims are plausible. Limitations: The performance of LCP may vary depending on specific constraints and should be tested on a wider range. The cost function seems to have minimal impact in the tested environments reducing the problem to satisfying constraints. Clarity: The paper is wellwritten but the experimental section could provide more details. Reporting the LBDAs and MECs for the domains would enhance understanding. Significance: The paper addresses the critical issue of learning policies that adhere to constraints. The novel framework and thorough theoretical analysis make significant contributions to the field. The experimental results provide some support for the claims but their significance could be clarified with additional details. Questions: Why are transient costs included in the optimization objective when only the longterm cost rate is typically considered Is the optimization problem (OPT) welldefined with a solution that outperforms all others This is relevant to the discussion of \u03b5greedy policies and Corollary 5.2. How would incorporating the food reward into the LTL constraint in the PacMan domain affect LCPs performance compared to having it in the cost function", "Paraphrase: In reinforcement learning (RL) optimizing policies often requires complex reward tuning which can introduce unintended consequences. Linear temporal logic (LTL) offers a formal language for representing costs especially when scalar cost functions are inadequate. This paper presents a novel method LTL Constrained Planning (LCP) for RL with LTL constraints. LCP converts the problem to a reachability task and establishes strong guarantees for constraint satisfaction along with bounds on the number of samples required for learning. The algorithm has three steps: (1) identifying recurrent states in the product Markov decision process (MDP) (2) finding a costefficient recurrent policy within those states and (3) finding a transient policy to reach the recurrent states. Experimental evaluations demonstrate that LCP achieves higher performance with fewer samples than baseline methods particularly in infinitehorizon tasks. Originality and Quality: LCP is a new approach that extends existing LTLconstrained RL methods with fewer assumptions and stronger guarantees. The paper provides a comprehensive framework including a method and empirical evaluation against baselines. Clarity: The paper is wellorganized and clearly written. The highlevel intuition section helps readers understand the complex terminology and concepts. A conclusion summarizing key findings and outlining future directions would enhance clarity. Significance: LCP introduces a unique approach to policy optimization by framing LTLconstrained optimization as a reachability problem to recurrent states. Questions: Practical Use: When is LCP suitable and when might it not offer significant benefits Performance Variation: Are there any scenarios where LCP performs worse than baseline methods Enhancements: Under which conditions does LCP provide a greater advantage over baselines"], "tbdk6XLYmZj": ["Paraphrase: This paper addresses the issue of neural network sparsity (NM) and presents a novel technique that leverages a structure parameter to prioritize the significance of various combinations. The proposed method yields impressive results. Strengths: Effective pruning outcome Weaknesses: The author highlights a potential limitation stemming from the analysis of [1] which suggests that gradientbased sparse training approaches may not surpass an acceleration factor of 1.5x due to dense gradient computation for structure parameters. This issue is considered fundamental and should be addressed before publication to avoid misleading the research community. Questions: The paper requires clarification regarding the cost of training particularly in relation to the concern raised in the referenced work.", "Paraphrased Statement: Authors address the issue of creating a trainable structural sparsity parameter using an NM pattern. They use a divideandconquer approach to identify the optimal sparsity patterns for each layer. The algorithm initially proposes all possible candidate combinations and then ranks them based on an importance variable during training. In each step the combination with the lowest score is eliminated. The combination that remains at the end is chosen. Experiments are conducted using Resnet50 and DIET on ImageNet. Strengths and Weaknesses: Strengths: Provides a comprehensive review of pruning paradigms. Clearly introduces Ampere sparsity NM. Formulates structural pruning as a combinatorial problem strengthening the approach. Adequate formulation in section 3.2. Weaknesses: Implementation of Learning Score Matrix (S): Unclear implementation. Needs clarification on whether all combinations contribute to the layers output during training which would increase memory and computational costs. If S is implemented through straightthrough estimators clarification is needed on sampling during training and gradient propagation. Score Matrix: Needs explanation on whether the score is a real value learned by the model. Empirical evidence should be provided to support the claim that the score reflects importance. Hyperparameter Setting: Guidance is needed on setting new hyperparameters and the rationale behind their choices. Training Recipe: Confirmation is needed on whether the training recipe is consistent with SRSTE.", "Summary Paraphrase: This paper transforms the nonzero (NM) element sparsity into an optimization problem. This approach is logical and has potential. The authors utilize a trainable score matrix to assess the significance of potential nonzero elements. They modify the score matrix by calculating the gradients of group elements. Strengths and Weaknesses Paraphrase: Strengths: The paper is clear and straightforward. The approach improves performance across different sparsity levels and models. Reducing sparse model training costs could drive research in faster sparse training. The code is available. Weaknesses: The authors should conduct experiments on other tasks. The authors only report floatingpoint operations (FLOPs) during training and should also report training time. Questions: See Strengths and Weaknesses. PostRebuttal Paraphrase: I have reviewed the rebuttal and my concerns have been addressed. Thank you.", "Paraphrase: Summary: This research focuses on efficiently training networks with neuron masking (NM) sparsity from scratch. Determining the ideal NM sparsity pattern involves solving a series of interconnected combinatorial problems with finite candidate sets. The researchers assign each potential combination a trainable score parameter and train it using data with the aid of a straightthrough estimator (STE). Experiments demonstrate that the proposed training approach delivers modestly better results with significantly fewer floatingpoint operations (FLOPs) compared to stronger baselines and significantly outperforms others. Strengths: Successfully demonstrates the effectiveness of NM sparsity in largescale neural networks under a gradual pruning approach. The gradual pruning methodology allows for an efficient approach to training sparse models from the beginning eliminating the need for a costly dense training process. Experiments show promising results in performance and FLOPs. Weaknesses: Overclaiming: Characterizing the problem as a series of combinatorial problems oversimplifies and does not reduce its complexity. The claim of using a \"divideandconquer\" strategy is inaccurate as the problem remains complex and is not broken down into simpler forms. Simplistic Approach: The method relies on a straightforward scoring function for each combination a technique similar to network slimming. The learnable scores are only used in one equation (eq. 8) raising questions about their actual significance. Lack of Efficiency Comparison: Missing energy or walltime comparisons between the proposed method and baselines. This is crucial considering the emphasis on training efficiency. Questions: The superscript of \\mathcalLl in eq. (9) appears to be misplaced. Please address the weaknesses and answer the questions raised in the \"Weaknesses\" section."], "tjFaqsSK2I3": ["Summary This study introduces a unified approach using a transformer framework and sequence modeling for four vision and visionlanguage tasks: object detection instance segmentation human pose estimation and image captioning. Results demonstrate that the approach achieves comparable performance to established taskspecific models. Strengths The approach aligns with the trend of task unification using transformer frameworks for sequence modeling. It simplifies training objectives and model architectures. Transformer architecture and sequence modeling enable text prompting to activate desired functionality. Competitive performance across the four tasks indicating potential for a unified design. Weaknesses The claim that it is a \"Unified Sequence Interface for Vision Tasks\" is overreaching as it only addresses a subset of vision tasks. The tasks selected for unification may not fully represent the spectrum of vision problems. A comparison with visionlanguage foundation models is missing. The approach may face challenges in learning finegrained spatial relationships which are critical for the selected vision tasks. Inference speed may be limited due to the autoregressive nature of the sequence modeling. The approach does not demonstrate a clear advantage in training efficiency compared to multiobjective training. Questions Why were other popular vision and visionlanguage tasks (e.g. imagetext retrieval visual question answering) not included in the study Can the approach be extended to a wider range of vision tasks effectively What is the potential advantage of the unified design over existing taskspecific models How does the approach address the challenge of learning finegrained spatial relationships for vision tasks What are the potential limitations of the approach in terms of efficiency and inference time", "Paraphrase: Summary: This article introduces a unified platform for generating sequences for four main vision tasks: object detection instance segmentation keypoint detection and image captioning. The model employs a single encoderdecoder architecture without taskspecific heads. To distinguish between tasks descriptive prompts are added to the beginning of the output sequence. Experiments show that this unified interface can achieve comparable performance to specialized taskspecific models. Strengths and Weaknesses: Strengths: The paper is wellwritten and easy to understand. The experiments demonstrate the feasibility of using a unified interface for vision tasks. Weaknesses: The papers only technical contribution may be extending the pixel2seq model to the other three vision tasks. Minor Comments: Important training and inference details should be included in the main paper. Questions: Are the results based on training from ImageNetpretrained weights This would provide context for the advantages over baselines. Provide more information about the segmentation inference strategy including how samples are generated the sampling strategy and the threshold determination. Discuss the choice of loss weights considering that greedy search may not be practical for multiple tasks.", "Paraphrased statement: Summary: The proposed framework provides a unified interface for various vision and visionlanguage tasks delivering performance comparable to existing baselines. Multitasking improves outcomes on certain tasks. However concerns arise regarding novelty incremental nature (compared to pix2seq) unclear pretraining details and subpar performance on some tasks. Strengths and Weaknesses: Strengths: Simple and useful unified interface for multitask learning. Consistent inputoutput for all tasks enhances scalability. Multitasking benefits certain tasks. Weaknesses: Uncertain whether baselines (Table 1) received pretraining on Objects365 object detection potentially biasing comparisons. Work appears incremental adding tasks to pix2seq framework. Benefits of multitask learning and unified interface need clearer explanation. Unified interfaces superiority over taskspecific architectures is not demonstrated. Multitasking does not improve Instance Segmentation and Keypoint detection. Lack of analysis and explanations for potential causes. Shared loss functions benefits are mentioned but not supported by ablation studies. Questions: Batch ordering for tasks and its impact on performance. Impact of curriculum learning. Influence of pretraining on different tasks on multitask performance. Relationship between obj detection performance and model initialization with pretrained weights. Comparison with other frameworks in terms of efficiency. Benefits of multitask training as a pretraining step for individual tasks.", "Paraphrase: This study introduces Pix2seq a unified interface for computer vision tasks. Pix2seq expands its capabilities beyond object detection to include segmentation keypoints and image captioning enabling the training of a single model for multiple tasks. By translating these tasks into sequence prediction problems Pix2seq unifies the decoder outputs. Using batch mixing rather than data mixing it effectively augments data for various tasks. Pix2seq outperforms existing methods on four benchmarks. Strengths: Unifies core computer vision approaches into a sequence demonstrating stateoftheart performance on multiple tasks. Simplifies the design of vision decoders fostering advancement in computer vision and unified model communities. Weaknesses: Keypoint tokenization requires clarification: its unclear if keypoints are represented as image coordinates or labels. Polygon representation for segmentation is not thoroughly explained (e.g. starting point selection long sequence constraints). Specific image augmentation methods for different tasks are not detailed in Section 2.3. Transformerbased caption method lacks citation and metrics like CIDEr should be included. Discussion on contrasting methods for instance segmentation (e.g. VQVAE vs. Pix2seq) is absent. Questions: How are keypoints represented in Pix2seq How are polygons converted into sequences for segmentation Are there any constraints on polygon starting points or sequence length What are the key differences in image augmentation for various tasks Why did the authors choose to use points rather than VQVAE for instance segmentation"], "pgBpQYss2ba": ["Paraphrased Statement Summary: The study examines a comprehensive adversarial decisionmaking framework and develops a novel complexity measure called the decision estimation coefficient. It introduces a generic algorithm with specific upper bounds determined by the decision estimation coefficient and establishes an algorithmindependent lower bound. Strengths: It presents one of the initial characterizations of complexity in adversarial decisionmaking potentially opening doors for further research in areas like bandits and reinforcement learning. The upper and lower bounds are innovative and significant. The paper is wellwritten and elucidates the connections between complexities in adversarial and stochastic settings as well as the relationships between different complexity concepts. Weaknesses: Some sections require clarification. Questions: 1. Can the first step of Algorithm 1 be extended to other update forms besides exponential weights 2. Is the minimax objective (Eq. 5) convexconcave Can it be solved efficiently 3. The reward estimator (Eq. 6) may require transition information which is often assumed unknown in online reinforcement learning.", "Paraphrase: This paper introduces a framework that covers both bandit problems (where rewards may be manipulated by an adversary) and reinforcement learning challenges (where the environments dynamics may be adversarial). Key Result: The \"DecisionEstimation Coefficient\" (DEC) is crucial for minimizing regret in this adversarial environment. It is both necessary and sufficient for achieving low regret. Other Findings: The paper explores the connections between the DEC and other complexity measures. Strengths: Extends previous research on stochastic decisionmaking to the adversarial setting. Provides sound and original theoretical results. Weaknesses: The proposed algorithm (Algorithm 1) may not be practical for large decision spaces limiting its use to theoretical analysis. Questions: What practical implications arise from the theoretical findings Explain what V(M) represents and the conditions for its finiteness. Why is convexity important in the lower bound and what are the tradeoffs involved in using convex hulls for the upper bound", "Summary Paraphrase: This paper introduces a new measure called the Decision Estimation Coefficient (DEC) to quantify the complexity of sequential decisionmaking in adversarial settings. The DEC provides a way to establish matching upper and lower bounds on the regret incurred by algorithms. The DEC was originally developed for stochastic environments where it measures the tradeoff between exploiting highreward actions and exploring to gather information. This paper extends the DEC to adversarial environments by using the convex hull of the model class which captures the potential distributions of models that an adversary may choose from. The paper presents both an upper bound on the regret of an abstract algorithm inspired by the ExplorationbyOptimization framework and a lower bound for a specific class of algorithms. These bounds demonstrate the effectiveness of the DEC in characterizing the complexity of adversarial sequential decisionmaking. Strengths and Weaknesses: The paper provides several original contributions including a highprobability variant of ExplorationbyOptimization and refined changeofmeasure arguments. It builds upon the strengths of its stochastic predecessor offering generality and unification. However the abstract nature and technical complexity of the main results may limit their applicability in practice. The DEC is described as an intermediate proof step rather than a clear measure of complexity. Questions: 1. The practicality of the ExO algorithm depends on the specific instantiation for bandits and MDPs. The optimization problem may be computationally challenging and the linear time complexity of the exponential weights update may not generalize to all cases. 2. The sufficiency of convexification to capture adversarial hardness remains somewhat unclear. The paper could provide further intuition on why small convexified DEC implies small regret. 3. The authors may know of concrete examples where the convex hull of the model class is strictly larger than the original model class but the adversarial problem is still tractable."], "pfEIGgDstz0": ["Summary (Paraphrased): This study introduces a novel nonrigid registration algorithm that can handle point clouds with partial overlaps. It utilizes three regularization mechanisms: 1. A hierarchical approach to break the problem into smaller subproblems 2. Scaling from lowfrequency deformations to higher frequencies 3. Assigning a rigid deformation (rotation translation and optional scaling) to each point The algorithm has been evaluated on 4DMatch and 4DLoMatch datasets demonstrating impressive results and surpassing various stateoftheart methods. Strengths: Innovative combination of different regularization techniques Relatively straightforward approach Convincing results on diverse shapes Insight into lowtohigh frequency deformations Weaknesses: Quantitative results are limited to synthetic data with nearisometric deformations Lack of experiments on significantly different shapes and preservation of structural features No analysis of noise clutter or scale factor inclusion No discussion of scalability with varying point cloud densities or partiality Minor Fixes: Line 125: Fig. 3 Fig. 1 Line 137: registration of of multiple scans registration of multiple scans Line 168: this regularization help to preserve this regularization helps to preserve [14] is not introduced in the previous works [a] would be a better alternative than [6] since it overcomes some LBO limitations in the point clouds setting [a] Correspondence learning via linearlyinvariant embedding Marin et al. 2020 Questions: Sensitivity to noise clutter and point cloud density Computational scalability with varying numbers of points", "Paraphrased Statement: Summary: This study introduces Neural Deformation Pyramid (NDP) a novel technique for nonrigid point cloud registration. Nonrigid registration is challenging due to noise outliers and occlusions in 3D data. NDP addresses these challenges by breaking down motion hierarchically. It employs coordinateMLP as its foundation like Nerfies and SAPE. NDP surpasses existing methods on the 4DMatch4DLoMatch dataset and is more efficient than Nerfies. Strengths: NDP is wellconceived and connects clearly to existing approaches. It achieves superior performance in both unsupervised and supervised settings compared to previous methods both quantitatively and qualitatively. NDP is faster than Nerfies. The authors thoroughly compare NDP with various existing methods. The paper is wellwritten and easy to understand. Weaknesses: Despite claiming to introduce a pyramid architecture similar structures have been discussed in related work limiting technical novelty. NDP is based on coordinateMLP which is also used in existing methods. Experiments were conducted only on the 4DMatch4DLoMatch benchmark other benchmarks should be considered. The paper lacks quantitative results on NDPs ability to handle data noise outliers and occlusions. Questions: The paper claims NDP is faster than existing methods but details on runtime speed are missing. The meaning of \"via skinning\" in line 53 is unclear. The implementation details such as MLP parameters are insufficient for reproducing the method. The registration algorithm in 3.3 lacks justification: Why are different levels optimized sequentially Why are maxiter set to 500 and theta to 15 iterations", "Paraphrased Statement: The paper presents a neural network approach for aligning flexible (nonrigid) point clouds. It breaks down the necessary deformations into a series of rigid and similar (similarity) transformations. A neural network is then trained to predict these transformations at different levels. The proposed method performs effectively in the evaluations. Strengths: The hierarchical deformation concept is innovative. The method demonstrates good performance in the evaluation. Weaknesses: The suitability of the loss function for partialtopartial registration is unclear. The loss function involves a squared distance metric which could perform poorly when parts of the point clouds do not overlap. This issue is typically addressed in nonrigid registration approaches by using more robust error norms. Questions remain about the loss functions effectiveness in partialtopartial registration especially in the unsupervised setting.", "Paraphrased Statement: This study introduces Neural Deformation Pyramid (NDP) a method for aligning point clouds without rigidity assumptions using machine learning. The NDP algorithm generates a dense warp field for each point in a source cloud using SE(3) (or Sim(3) for scaling). This warp field is then used to align the source cloud with a destination cloud. The NDP approach utilizes a hierarchical structure decomposing the warp field into layers of MultiLayer Perceptrons (MLPs). Each layer applies sinusoidal encoding with varying frequencies to the input coordinates. This design essentially decomposes the nonrigid transformation into a series of rigid transformations operating at different scales. The authors conducted experiments comparing NDP with numerous other methods and found that NDP outperforms in terms of accuracy and efficiency. Strengths: Intuitive and wellmotivated hierarchical motion decomposition. Clear and concise network design and description. Comprehensive experiments demonstrating the algorithms benefits. Acknowledged limitations. Questions: Does NDP effectively solve an unsupervised optimization problem Do the network weights initialization affect the optimizations quality"], "vGQiU5sqUe3": ["Paraphrased Statement: The paper introduces a new approach that utilizes contrastive learning to train goaloriented policies from existing data without additional reinforcement learning (RL). During contrastive learning the system learns similarities between data points. This knowledge is then used as a Qfunction in policy gradient optimization which improves policy performance. The paper provides mathematical proof that the contrastive objective approximates the Qfunction ensuring convergence. The approach is wellmotivated with extensive comparisons against both representation learning methods and RL algorithms that explicitly learn goalconditioned policies. Strengths: Clarity and presentation: The paper is wellwritten and easy to understand with clear explanations and results. Extensive evaluation: The method is compared against a wide range of baselines demonstrating competitive and sometimes superior performance. Theoretical foundation: The approach is grounded in theory providing a solid understanding of its operation and limitations. Simplicity: Despite its effectiveness the method is relatively straightforward making it accessible for implementation. Novel perspective: The paper offers a unique viewpoint on the use of contrastive learning for goalconditioned policy training. Weaknesses: None noted in the original text. Questions: Performance in partially observable large environments with firstperson views is unknown. Generalization capabilities to changes in background color object colors or object types are unclear. The plot in Figure 11 appears to have inverted color assignments with the center positions shown in the highest \"steps to center\" color.", "Summary Paraphrase: Researchers propose using contrastive learning to enhance value functions in goalconditioned reinforcement learning. By leveraging the learned representation the approach aims to improve performance on goaldriven tasks. Theoretical analysis and empirical results demonstrate the effectiveness of this framework. Strengths and Weaknesses Paraphrase: Strengths: Originality: The approach uniquely combines goalconditioned RL and contrastive learning to improve representation learning. Relevance: The authors thoroughly compare the proposed method to related work and discuss its significance for representation learning in RL. Algorithm and Evaluation: The algorithm is welldesigned and theoretically sound. The experimental evaluation includes comprehensive benchmarks and baselines showcasing strong results. Weaknesses (Questions and Discussion Points): Experiment Interpretation: In some cases the improvements compared to a specific baseline (Clearning) are marginal. The authors are asked to provide an explanation for this. Other Challenges: The researchers are questioned about the approachs ability to address other common challenges in goalconditioned RL particularly with offline datasets. Generalization: The inquiry is raised whether the learned representation can generalize to a broader set of tasks with limited interaction data. Questions: Goal Requirement: Can the goal requirement be relaxed to include more than just state observations Generalization Abilities: Can the approach be generalized to diverse tasks with a small amount of interaction data", "Paraphrase: Summary The authors present a contrastive learning method that simplifies representation learning in goaloriented RL tasks. This method uses the inner product of learned representations to approximate the Qvalue function eliminating the need for auxiliary losses or data augmentation. The authors demonstrate the effectiveness of their approach on various tasks. Strengths Simplifies representation learning by using inner product of representations as a Qvalue approximation. Particularly effective for pixelbased environments leading to performance gains in continuous control tasks. Provides theoretical motivation for the method and its relationship to deep RL critics. Thoroughly compares with related works. Weaknesses Unclear why random goals are used for actor loss training. Minimum goal dimension for effective contrastive RL performance is unknown. Contrastive RL variants show mixed performance in imagebased tasks with Clearning pipeline contributing significantly to performance improvements. Questions Typos in the paper: Line 141: \"The objective expected reward objective\" Line 188: \"critic is parametrized as an inner product\" (Algorithm 1 says it uses an outer product) Line 268: \"One\" \"On\" Line 879: \"has uses\" Dimension of goals in fetch and sawyer tasks (statebased). Request for different color palette in Figure 5 to improve line differentiation."], "kxXvopt9pWK": ["Paraphrased Statement: This study presents a novel image restoration model named Denoising Diffusion Restoration Model (DDRM). DDRM utilizes principles from variational inference and leverages a pretrained denoising diffusion generative model. Similar to highlevel vision applications DDRM employs a pretrained model to handle multiple downstream tasks. It has been evaluated in various restoration tasks such as superresolution deblurring inpainting and colorization. One notable advantage of DDRM is its computational efficiency making it practical for realworld applications. Strengths: Wellwritten and comprehensible Unsupervised approach with minimal assumptions about degradation models High computational efficiency Weaknesses: Convergence issues of DDRM are not addressed Questions: 1. If DDRM generates a sequence of restored images like SNIPS can MSE estimates be obtained and used for weighted averaging in Figure 4 of the Supplementary Material 2. What assumptions are made about the degradation model H in Equation 3 Can DDRM be applied to motion blur while still satisfying the separable condition in Equation 3 3. Clarification is needed on the difference between \"DDRM (20)\" and \"Denoised\" in Figure 1 of the Supplementary Material. Does DDRM reach a fixedpoint iteration where the denoised image satisfies a certain condition", "Paraphrased Statement: Summary: The paper presents a technique for solving inverse problems using a diffusion model. It can generalize well across different tasks due to its unsupervised nature. Strengths: The ELBO objective of the proposed method is analogous to that of DDPMDDIM enabling the use of pretrained DDPMDDIM models eliminating the need for lengthy training. The method exhibits strong generalization capabilities because it makes no specific assumptions about degradation. Weaknesses: The experimental evaluation is limited. In superresolution the study only utilizes block averaging as degradation which is uncommon in the field. Including bicubic downsampling would enhance the assessment. For deblurring the method should also address the more challenging case of motion blur in addition to anisotropic Gaussian deblurring. The proposed method requires prior knowledge of the degradation operator (H) which is often unavailable in realworld scenarios. This limits its practical applicability. Questions: How can the experimental evaluation be expanded to include more realistic scenarios and more recent methods How can the proposed method be extended to handle motion blur deblurring What alternative approaches can be considered to overcome the limitations of requiring prior knowledge of the degradation operator", "Paraphrase: Summary: This study presents Denoising Diffusion Restoration Models (DDRM) an innovative samplingbased method for solving inverse problems in various image restoration tasks. DDRM exhibits superior performance compared to existing approaches requiring fewer function evaluations (NFEs). Notably its efficacy increases in the presence of input image noise. Strengths: DDRM necessitates significantly fewer iterations than existing diffusionbased methods. Its versatility has been demonstrated in multiple restoration tasks including superresolution inpainting and colorization. Weaknesses: DDRMs focus on linear inverse problems suggests a potential limitation in handling nonlinear inverse problems common in realworld data. Analysis of limitations and implications in nonlinear inverse problem scenarios is recommended. Practical applications often encounter unknown degradation operators during inference. Guidance on applying DDRM in such situations would enhance its usability. Questions: It would be valuable to provide quantitative results based on the number of iterations for DDRM. How does DDRM perform with more than 20 iterations as shown in Table 12 While experiments primarily utilized ImageNet it would be informative to test DDRM on frequently used datasets specific to each task (e.g. Set5 Set14 B100 Urban100 Manga109 for superresolution). Assessing DDRM on realworld data such as medical imaging as mentioned in the introduction would provide insights into its practical applicability.", "Paraphrase: Summary: This paper tackles the efficiency limitations of unsupervised image restoration approaches. The authors propose DDRM a denoising diffusion generative model that leverages variational inference to capture the posterior distribution of the underlying inverse problem. Specifically they define a diffusion process for image restoration using variational distributions with trainable parameters. These parameters are optimized by maximizing an ELBO objective. Experiments on the ImageNet dataset demonstrate DDRMs superior performance compared to existing methods. Strengths and Weaknesses: Strengths: Robust technical approach with theoretical underpinnings. Efficient inference by defining the variational objective and distributions in the diffusion process. Novel and significant contribution. Convincing quantitative and qualitative results showing strong performance against competitors. Weaknesses: Limited experimental scope focusing solely on synthetic degradation models. Absence of analysis on the degradation model. Oversmoothing artifacts in certain visual results. Questions: 1. Can the authors incorporate a metric like SSIM into the evaluation to assess structural similarity 2. Can the authors provide details on the number of NFEs required for DDRM to outperform previous methods along with runtime statistics (memory time) per NFE 3. What are the implications of training separate models for different inverse problems from a practical perspective Could this enhance performance"], "n7Rk_RDh90": ["Paraphrased Summary: The paper presents a method that uses RGBD inputs to learn 3D concepts without explicit labels. It introduces \"Neural Descriptor Field\" (NDF) into neural operators (filter query and counting) to enable training on questionanswer pairs from a VQA dataset. The method is extended to part semantic segmentation and instance segmentation tasks demonstrating generalization abilities to unseen classes and real scans. Strengths: Weaklysupervised approach for grounding 3D concepts. Uses differentiable neural operators allowing for interpretability and easy training with questionanswer pairs. Wellwritten paper with clear explanations and illustrative examples. Demonstrates promising results on part semantic and instance segmentation tasks. Weaknesses: The method focuses on objectcentric reasoning and may struggle with complex scenes due to its reliance on NDF which treats the entire 3D space as a query space. Generalization claims are not fully supported as only qualitative results on unseen categories are shown.", "Paraphrased Summary: This research explores how 3D concept grounding (relating words to 3D shapes) can be improved using neural implicit field representations. The authors address specific questions and have developed a multistage pipeline to train models. They have also created a dataset containing questions specific to their task. Results indicate that models trained with 3D concept grounding perform better than previous approaches and adapt well to new shapes. Strengths and Weaknesses: Strengths: Clear and wellwritten Novel and significant research problem Use of a dedicated dataset tailored to the task Proposed framework incorporates advanced techniques like attention blocks and neural descriptor fields Promising results demonstrating performance and generalization improvements Weaknesses: Limited data size in the experimental dataset Framework may be purposespecific rather than generalizable Potential difficulty in obtaining 3D labeling and questionanswering datasets", "Paraphrased Statement: This paper proposes a novel approach that links natural language concepts with 3D representations of shape and appearance. Instead of storing concepts separately the method employs neural descriptor fields which provide continuous representations that can be differentiated. It utilizes a variety of question types (binary counting segmentation) and does not rely on additional supervision beyond shape and appearance data. Contributions include a novel slot attention module and neural operators specifically designed for counting and segmentation tasks. While the method shows promising results compared to existing techniques it lacks clarity in several areas: It should explicitly specify the types of data it handles (scenes or objects). It introduces a depthmapconditioned neural field but omits details about how it differs from standard neural shape fields. It provides insufficient information on the parsing process operator selection loss computation and the potential absence of a reconstruction module in the architecture. The related work section could be more comprehensive to cover recent advances in neural field representations and concept grounding. The dataset used includes both questions and answers not just questions and the selection process for the 8 questions and answers per shape is not specified. Overall the method provides a valuable contribution to concept grounding and 3D representation but improvements in clarity and a more thorough related work section would enhance its strength.", "Paraphrase: Summary: This research introduces a technique for linking concepts to 3D constructions inferred from pictures. The method extends implicit 2D to 3D methods like occupancy networks and neural descriptor fields which map an image to an embedding space. The model uses paired 3D query vectors to return vectors indicating relevant quantities. This setup integrates operators similar to \"neurosymbolic concept learners\" enabling the model to respond to inquiries about the input image. The model undergoes supervised training for occupancy and color prediction and then receives supervision on synthetic questions. Evaluations are conducted on simplified data featuring individual objects against white backgrounds with the model outperforming certain baseline approaches. Strengths: The use of an intermediary 3D representation for grounding is a promising approach. Weaknesses: Inferring the 3D representation for grounding is challenging and the paper focuses on simplified scenes where 2D to 3D mapping is easier. The true challenge is understated as the paper instead focuses on querying and combining operators for grounding tasks. The baselines used for comparison are unfamiliar making it difficult to assess improvements. Overstatements: The paper claims the emergence of 3D segmentations without explaining why this is relevant or how it is achieved. The paper asserts selfsupervised learning but this requires prior knowledge of 3D models or detailed object reconstructions. The paper claims generalization to unseen categories and real scenes but the experiments do not test the models ability to infer 3D representations from images. Questions: How does the model infer the occluded side of the car and correctly estimate the number of wheels in Figure 4b Was 3D ground truth provided for the generalization experiments in Figure 4b"], "zrAUoI2JA2": ["Paraphrase: Summary: This paper introduces the unified hidden unit BERT (uHuBERT) model which utilizes vast amounts of unlabeled speech data from various sources (unimodal and multimodal) for pretraining. The uHuBERT model consists of modalityspecific feature extractors and a universal transformer backend. This architecture allows uHuBERT to learn modalityindependent framelevel representations and enables finetuning on both multimodal and unimodal datasets. Experiments demonstrate the effectiveness of uHuBERT on speech recognition and speaker verification tasks. Strengths and Weaknesses: Strengths: Builds upon AVHuBERTs architecture extending it to handle both multimodal and unimodal data. Proposes a modality dropout method to map different inputs into a shared feature space enabling zeroshot modality generalization. Weaknesses: Incomplete experimental analysis lacking details on modality dropout rates during pretraining and finetuning. Limited comparison with baseline methods in the zeroshot modality transfer experiment. Insufficient descriptions of baseline models pretrainingtrainingfinetuning approaches and datasets used for comparison with stateoftheart models. Questions: What is the performance of baseline methods in Table 3 What are the settings approaches and datasets used for baseline models in Table 4", "Paraphrase: The paper presents a selfsupervised technique within the HuBERT framework for audiovisual speech recognition. This method can be trained on different modalities including singlemodal audio or video as well as multimodal (audiovisual) data. Strengths: The proposed model can be finetuned on any modality and perform well on all modalities achieving comparable results to modalityspecific models. The evaluation is comprehensive. Small absolute improvements were made in LRS3 but the baseline error rates are very low (e.g. 1.4 WER for AV ASR) so a reduction to 1.2 represents a significant 14 relative WER reduction. Such improvements are typically challenging to achieve. The results are valuable. The paper is wellwritten and clear. Weaknesses: The paper is not selfcontained as architectural details are primarily found in other references. The novelty introduced over AVHuBERT is relatively minor but still important. The proposed methods are practical but lack a strong theoretical foundation. Additional Questions: It has been suggested that w2vBERT generates better representations than HuBERT. Can you provide insights on the selection of HuBERT over w2vBERT Are the proposed additions to AVHuBERT applicable to w2vBERT", "Paraphrase of Summary: This study introduces a multimodal speech processing model combinable with various speech modalities (audio visual). The model consists of modalityspecific feature extractors followed by a shared transformer module that derives modalityneutral representations. Using audio and visual data the model exhibits proficiency in both speech recognition and speaker verification. Notably it performs well in generalizing to unseen modalities without additional finetuning. Paraphrase of Strengths and Weakness: This model closely resembles the recently published AVHuBERT architecture raising questions about its distinctiveness. Key differences are claimed such as incorporating both multimodal and unimodal data during pretraining. However AVHuBERT also utilizes modality dropout to handle unimodal inputs. Additionally the modality dropout method employed here is unclear. The models performance metrics (e.g. AVV WER of 1.227.2) are similar to those reported by AVHuBERT (AVV WER of 1.326.9). Unique contributions include zeroshot generalization and application to speaker verification. Minor Typos: \"it essential\" should be \"it is essential\" \"provide reference to AVHuBERT\" could be more specific", "Paraphrase: Summary: This study introduces uHuBERT a selfsupervised learning system that can leverage both multisensory and singlesensory unlabeled speech data such as audio visual and audiovisual to pretrain a unified model using modality dropout. After finetuning this pretrained model on audiovisual data using modality dropout it erzielt comparable or better outcomes than the most effective current modalityspecific models. Additionally the researchers demonstrate that modality dropout is crucial for acquiring modalityindependent features enabling models finetuned only on audio to function well with audiovisual or visual speech data and thus achieving zeroshot modality transfer for speech recognition and speaker verification. Strengths and Weaknesses: Originality: This work distinguishes itself by utilizing a common target token space for all singlesensory and multisensory data in a single model and finetuning it with labeled data exclusively from a subset of modalities encountered during pretraining. It can achieve good performance on other modalities in testing demonstrating zeroshot modality generalization. While modality dropout has been employed before the study demonstrates its significance in acquiring modalityindependent features and achieving substantial gains in zeroshot modality transfer situations. Quality: The proposed model surpasses or matches the performance of current modalityspecific models for speech recognition. The findings are encouraging as the model finetuned solely on audio data produces comparable outcomes to models finetuned on modalityspecific data when used with audiovisual and visual speech data. The experiment using labeled audio data from LibriSpeech that is outside of the domain for finetuning provides convincing evidence to support the claim that the models zeroshot modality transfer is not accomplished through memorization. The proposed model exhibits zeroshot modality generalization but only to a certain extent. The authors postulate that this is attributable to the speaker verification task where each modality can only deduce a subset of the speakers characteristics. This illustrates a restriction on zeroshot transfer. Clarity: The paper is presented clearly and logically. Significance: This research makes a major contribution by presenting a technique for pretraining on multisensory and singlesensory speech data to enhance audiovisual ASR results beyond what is currently possible and to achieve highquality zeroshot modality generalization. It is particularly valuable for tasks involving modalities with limited labeled data. Questions and Suggestions: The proposed framework assumes that multimodal features are aligned framebyframe. How can it be applied when this assumption does not apply such as between audio and text modalities Is the pretraining modality dropout probability of 0.250.25 optimized How sensitive are the findings to its value Table 2 shows that modality dropout during finetuning also decreases AVWER for models finetuned on AV data. Why is that the case Regarding the claim that uHuBERT \"performs on par or better than the best modalityspecific models\" it would be beneficial to also include the caveat that its visual speech WER (27.2) is slightly higher than the best modalityspecific model (25.9) despite using less labeled visual speech training data. Typos: Line 37: \"it essential\" should be \"it is essential.\" Line 327: \"supported uHuBERT\" should be \"supported by uHuBERT.\""], "nDemfqKHTpK": ["Summary Data orderings play a crucial role in accelerating training by eliminating random shuffling. This paper proposes a method that identifies similarities between \"herding\" and order discovery problems leading to a provably faster convergence algorithm. Further optimization reduces memory consumption resulting in a practical solution. Experiments show improvements in training speed and accuracy for a variety of tasks and models. Strengths and Weaknesses Strengths: Provides a practical method for selecting orderings with faster convergence than random reshuffling. Theoretical proof under certain assumptions. Low algorithm overhead. Weaknesses: Experiments use subpar models. Requires additional smoothness assumptions beyond L2. Questions How is the term \"norminf(grad)\" canceled in line 491 Why was LeNet used for experiments on CIFAR10 instead of a more optimal model like ResNet How do orderings evolve throughout training Can the ordering be determined once at the beginning of training How does solving Herding offline compare to using GraB", "Paraphrased Statement This paper examines a technique called \"herding\" for optimizing the permutation of data used in the Random Reshuffling algorithm. Original Algorithm: The authors first present a more intuitive but impractical algorithm. Gradient Balancing: They then introduce the Gradient Balancing technique which retains the benefits of herding but is more efficient. Gradient Balancing uses less memory (O(d) instead of O(nd)) and computations (O(n) instead of O(n2)). Convergence Analysis: The paper analyzes the convergence guarantees of Random Reshuffling with herding under various assumptions. The technique improves convergence from O(n13T 23) to O(T23) making it significantly faster (O(n13) times). Experimental Results: Experiments show that the new method outperforms other permutationbased algorithms for logistic regression and deep learning architectures. Strengths: Wellstructured and written Clear explanations of herding and gradient balancing Six informative algorithms Wellstated assumptions Sound theoretical analysis Extensive experiments with practical demonstrations Weaknesses: Considers only Random Reshuffling not Shuffle Once No discussion of choosing a better permutation for Shuffle Once Does not consider distributed or variancereduced Random Reshuffling Only analyzes nonconvex objectives Nonstandard smoothness assumptions Assumption of Bounded Gradient Error is stricter than standard bounded variance Lack of source code for experiments and parameter tuning Questions: 1. Can the results be obtained under bounded variance assumptions 2. Can you provide a comparison table of results with previous methods under the same assumptions 3. Can the technique be applied to find a better permutation for Shuffle Once 4. Can the technique be used for convex problems 5. How is the parameter alpha estimated", "Paraphrase: This paper aims to minimize a differentiable loss function for a set of examples. It builds upon Lu et al.s finding that using a permutation that minimizes the average gradient error (as defined in Equation 2) can accelerate convergence compared to random reshuffling. The paper makes the following contributions: Section 3: Minimizing the average gradient error is equivalent to the herding problem which seeks to order a sequence of vectors to minimize the maximum deviation of the partial sum from the average. The authors show that using a greedy ordering based on the previous epochs stale gradients (as suggested by Lu et al.) may perform worse than random reshuffling. However proper herding (achieving a constant bound for the herding objective) outperforms random reshuffling. Section 4: To address the memory overhead of storing gradients for each example the paper introduces vector balancing an online method. Each vector is assigned a sign as it is processed and a reordering is performed based on these signs. Multiple passes through the gradients can achieve the desired herding bound. Section 5: The paper presents solutions to two challenges: centering vectors during streaming and performing balancing and reordering only once per epoch without storing gradients. The resulting algorithm SGD with Online Gradient Balancing (GraB) is shown to achieve convergence rates similar to proper herding. Experiments: GraBs data order advantage is demonstrated in training various machine learning applications. Strengths: Contributes to a significant problem in machine learning. Enables the scaling of data permutation techniques to large problems without storing gradients from previous epochs. Presents a clear and wellstructured exposition. Weaknesses: Theory and experiments use a constant learning rate instead of the more common learning rate schedules used in modern neural network training. Experiments on CIFAR10 could be strengthened by using a more recent architecture than LeNet such as ResNet20. Additional Commentary: Equation 3 (herding objective) and Equation 2 (average gradient error) are connected using the relationship \\mathbfx\\infty \\leq \\mathbfx2. Extending results to optimization with learning rate schedules may be easier in the case of stepwise changes rather than continuous decay."], "pnSyqRXx73": ["Paraphrased Statement Summary Stochastic Gradient Descent (SGD) utilizes random data samples from the training set with each iteration. This sequence of data denoted as (Xt)t can include minibatches. Contrary to the prevailing belief that smaller Second Largest Eigenvalue Modulus (SLEM) promotes faster SGD convergence empirical evidence suggests that smaller Asymptotic Variance (AV) of the sequence (Xt)t enhances convergence speed. Consequently the researchers focus on analyzing the AV of commonly used data sampling approaches in SGD such as minibatch shuffling. They introduce an \"efficiency ordering\" for sequences based on their AV where lower AV indicates higher efficiency. They demonstrate that nonMarkovian sequences like shuffling exhibit better efficiency ordering than Markovian i.i.d. (independent and identically distributed) input sequences. Experimental results on the CIFAR10 dataset indicate that nonMarkovian sequences indeed accelerate SGD convergence. Strengths Provides novel insights into SGD opening avenues for developing more efficient algorithms. Utilizes innovative techniques in the proof particularly the lifting of nonMarkovian sequences to make them Markovian. Rigorous and wellpresented main body of the paper. Weaknesses The appendix could be improved for clarity. Plots could be enhanced with larger labels and better color schemes. Minor misprints in the main body of the paper. Questions What does the statement \"We can check the solution \\tildeF(\\theta z)\" in Line 69 refer to How does the proof of Proposition 4.1 (Appendix F) establish that NonMarkovian Brownian Random Walk (NBRW) is more efficient than Standard Random Walk (SRW) What is the purpose of the embedded plots in Figures 5a 5b 6a and 6b Are there any insights on the computational complexity of calculating the Asymptotic Variance for a given chain Does the efficiency ordering concept apply to other stochastic optimization methods like Adam", "Paraphrased Summary: This study investigates the stochastic gradient descent (SGD) algorithm as a random walk on a graph. It examines different Markov Chain Monte Carlo (MCMC) sampling methods and introduces a concept called \"efficiency ordering\" for Markov chains. The study demonstrates that efficient MCMC sampling can reduce covariances in SGD algorithms in the long run. Strengths: Clear organization with both theoretical and empirical analysis. Novel concept of efficiency ordering of Markov chains supported by theoretical insights. Comparison of nonMarkovian random walks with their Markovian counterparts. Weaknesses: Lack of simulation details. Small hardtoread figures. Needs improved readability. Questions: Consider moving the discussion of minibatch gradient descent to the appendix to make room for a more thorough examination of efficiency ordering and other SGD variants. Investigate the impact of graph structure on efficiency ordering. Add a conclusion and future work section to address potential limitations and areas for further research. Minor Comment: Correct the definition of \\hatmu(g) in the asymptotic covariance matrix to reflect the summation over g(Xk) not g(Xt).", "Paraphrased Statement: Researchers propose the idea of \"efficiency ordering\" to compare the effectiveness of stochastic algorithms. They argue that certain nonMarkovian dynamics can outperform Markovian ones in SGD (Stochastic Gradient Descent). Strengths and Weaknesses: Weaknesses: Lack of clarity Exaggerated claims unsupported by data Unpersuasive numerical results especially on a small graph with 62 nodes Strengths: Not provided in the original statement Questions: The authors are urged to demonstrate the superiority of their method on large graphs with nonconvex functions and to provide more convincing numerical evidence.", "Paraphrased Summary This paper provides an asymptotic analysis of SGD (Stochastic Gradient Descent) when the input data follows a noisy pattern that is not necessarily independent and identically distributed (iid) but instead may be modeled as a Markov chain such as a random walk over a graph. The key contribution is linking the concept of \"efficiency ordering\" (previously used to compare Markov chains with the same stationary distribution and thus compare MCMC algorithms) to an ordering of covariance matrices derived from Central Limit Theorems (CLTs) that describe the asymptotic behavior of SGD iterates driven by Markov chain noise sequences. This connection suggests that SGD algorithms can reduce covariance errors by employing more efficient noise sequences for MCMC sampling. As practical applications the paper explores: Decentralized optimization on graphs using highorder random walks for SGD which are more efficient than traditional simple random walks. Gradient descent with shuffling in minibatch settings which outperforms using an iid input sequence. The extension of efficiency ordering to nonMarkovian processes demonstrating that they can outperform Markovian input sequences for SGD. Numerical experiments support the theoretical findings. Strengths Establishes a novel connection between efficiency ordering in MCMC and asymptotic covariance for SGD. Provides theoretical support for the benefits of shuffling and highorder random walks in SGD. Extends previous results to nonMarkovian processes. Weaknesses Does not capture dependencies on problemspecific constants. The practical usability of asymptotic variance for performance improvement is unclear. Determining efficiency ordering between sequences may be challenging. Questions Is it easier to show low asymptotic variance than to control asymptotic covariance from CLTs Can examples be found where nonasymptotic measures like SLEM (SubEpoch to Local Minimum) are more relevant Can nonMarkovian processes be analyzed using nonasymptotic methods based on mixing times"], "lgNGDjWRTo-": ["Summary The paper proposes a hierarchical Variational Autoencoder (VAE) approach for generating periodic graphs. It aims to learn the generation of basic structural units and compose them into larger graphs. The VAE includes a disentanglement loss term to encourage the separation of global and local graph features. Strengths and Weaknesses Strength: Focus on introducing global structure generation and decoupling it from local structure generation which is novel in periodic graph generation. Weakness: Assumption of Known Node Ordering: The approach assumes a known node ordering limiting its applicability to specific problems. This assumption also raises concerns about the use of a permutationdependent loss function. Matrix Processing Alternative: Under the assumption of known node ordering the model essentially deals with matrices potentially making traditional matrix processing models like CNNs more suitable. Limited Graph Similarity Measures: The evaluation only uses density and clustering coefficient which focus on local features and do not assess global structure reproduction. Absence of GraphVAE Baseline: The authors argue that GraphVAE is too expensive but its nodeorderingbased matching loss could be replaced with the true order assumed for the proposed method. Questions Selection of Substructure Sizes: How are the optimal substructure sizes (m and n) selected for each dataset Graph Orderings for Comparison: Do GraphRNN and GRAN use default BFSDFS orderings or the true graph ordering used for the proposed model Dataset Clarification: In Figure 3 are the trees in the lower left corner part of the synthetic dataset This seems inconsistent with the dataset description.", "Paraphrase: Summary: This paper introduces a new approach to generate periodic graphs. It uses a Variational AutoEncoder called PGDVAE which efficiently learns and generates both local and global patterns in a periodic graph. The authors present the graph generation process as a sequence of fundamental units rather than generating the units and patterns separately. Strengths: PGDVAE is the first generative model specifically designed for periodic graphs. It effectively disentangles and generates both local and global graph patterns. Weaknesses: Despite the authors claim of independent generation of local and global patterns the contrastive loss introduced in the model actually conditions the local generation on the global pattern. The model doesnt consider the generation of node types limiting its applicability to scenarios where only structure matters. The method doesnt ensure permutation invariance which restricts its use when node types are considered. These shortcomings limit the proposed methods quality and significance. Clarity: The papers text can be challenging to comprehend with undefined variables appearing in equations and several unclear sentences. Question: If the goal is separate generation of local and global patterns why is a contrastive loss used to make local generation dependent on global patterns", "Paraphrase: Generating new periodic graphs is valuable and challenging as shown by the papers examples. The authors present three reasonable challenges for this task. They address these challenges with a novel model that automates the disentanglement of local and global patterns in periodic graphs during generation and assembly. This \"hierarchical\" approach is innovative and efficient for handling such graphs as shown in the complexity analysis. Experimental results demonstrate the models effectiveness compared to others. Strengths: 1. The paper proposes a novel framework for solving the important task of generating periodic graphs. The hierarchical generation strategy is natural and efficient as it only learns and generates local and global patterns. The node embedding clustering module in the local pattern encoder improves performance over simple sum pooling. 2. The model analysis is comprehensive including both theoretical and experimental aspects. The learning objective is correctly derived and the complexity is favorable compared to peer models. Performance results also show superiority. 3. The paper is wellpresented with a clear diagram of the frameworks operation (Fig. 2) and a demonstration of the disentangling of local and global patterns (Fig. 3). The authors have also released the code. Weaknesses: 1. There are some typos such as \"Al Ag An\" instead of \"A(l) A(g) A(n)\" in Fig. 2. 2. Some references are missing such as [1] which deals with periodic structure generation using different strategies and input data. Questions: 1. Can the authors include and discuss the missing references such as [1] in the Related Works section 2. Please review and correct the typos such as the one in Fig. 2.", "Paraphrased Statement: Summary: This research introduces a new method based on variational autoencoders (VAE) for creating periodic graphs which can be applied to generating MOF meshes and synthetic graphs. The proposed method (PGDVAE) separates the graph representation into local and global features (A(l) A(g) and A(n)) before reassembling them into a complete periodic graph. The paper is wellwritten and the proposed PGDVAE is innovative in the field of periodic graph generation. The disentangled representation of periodic graphs is a notable contribution and the evaluation results are promising. Strengths: Clear and wellwritten Novel PGDVAE method for periodic graph generation Interesting disentangled graph representation Impressive performance on evaluation metrics Weaknesses: Potential to use the disentangled representation as input to the PGDVAE Lack of ablation studies to demonstrate the importance of disentanglement Uncertain practical applications of generated graphs (e.g. stability and synthesizability of MOF structures) Questions: Why not use the disentangled representation of periodic graphs as input to the PGDVAE as well Can you provide ablation studies to support the claim of improved performance due to disentanglement Can you provide examples of how the generated graphs can be used in realworld applications specifically with regard to the stability and synthesizability of MOF structures"], "zbt3VmTsRIj": ["Paraphrased Summary: This study introduces a novel formulation of Inverse Reinforcement Learning (IRL) based on maximum likelihood which coincides with MaxEnt for linear reward functions. Leveraging this formulation the authors propose an efficient iterative algorithm that avoids solving the Markov Decision Process (MDP) repeatedly. Notably they establish a nonasymptotic convergence rate for the algorithm to a stationary point marking the first such result for IRL with nonlinear reward parameterization. Comprehensive experiments showcase the superiority of the proposed algorithm over existing IRL methods. Strengths and Weaknesses: Strengths: The nonasymptotic convergence result for IRL with nonlinear reward parameterization. Weaknesses: The significance of other contributions in the paper may be limited: The maximum likelihood formulation is known and the proof resembles an existing theorem. The algorithms computational efficiency and reward transfer capabilities are not unique to the proposed method. The experiments focus on policy quality rather than computational efficiency. The clarity of the paper could be improved particularly in the mathematical sections. Questions: Can the approximation in the soft Qfunction be incorporated into the theorems analysis Why is formulating IRL in the maximum likelihood framework crucial for the convergence proof given the similarity between the proposed gradient and the MaxEnt gradient Why do the authors differentiate between MaxEnt IRL and adversarial IRL despite their established connection", "Paraphrase: This research introduces a unique singleloop IRL algorithm that minimizes computational demands in existing IRL frameworks. The algorithms solution aligns with the maximum entropy IRL framework assuming a linear parameterized reward function. Additionally the researchers provide proof of convergence for the new algorithm. Preliminary experimental results are encouraging. Strengths: Clear and concise writing Addresses a significant research problem with a novel approach Provides theoretical support for the proposed framework Weaknesses: May require a more comprehensive review of MaxEntIRL literature While the proposed MLIRL framework reduces computational complexity its applicability to transfer learning tasks remains unclear Questions: Could the researchers elaborate on the MaxEntIRL literature Can they explain why the MLIRL framework is suitable for transfer learning tasks", "Paraphrased Statement Summary The paper presents a novel approach to Inverse Reinforcement Learning (IRL) by formulating it as an entropyregularized maximum likelihood problem. This approach is shown to be equivalent to the maximum entropy IRL formulation. The problem is then cast as a bilevel optimization. The authors propose an algorithm that alternates between evaluating the current policy and updating it unlike most maximum entropy IRL methods that iteratively solve RL problems. The algorithm offers a convergence guarantee within a finite time frame under specific conditions. Empirical results on MuJoCo locomotion tasks demonstrate the algorithms superior performance compared to other IRL methods. Strengths Provides the first convergence analysis for IRL without assuming linear rewards. Establishes a duality between maximum entropy IRL and the maximum likelihood formulation. Offers a comprehensive experimental evaluation showing the algorithms effectiveness. Accurately compares the proposed work to prior research. Weaknesses The duality relationship with maximum entropy IRL is not entirely novel. The experimental results lack a clear demonstration of significant differences. The evaluation is limited to simple MuJoCo locomotion tasks. The writing and structure of the paper could be improved. Questions Can standard errors be provided for the results in Table 1 Why does the stateaction algorithm perform poorly in the halfcheetah environment compared to the stateonly algorithm Typos \"inherent nested\" should be \"inherently nested\" (line 4) \"Mujoco\" should be \"MuJoCo\" (line 19) \"computational efficient\" should be \"computationally efficient\" (line 53)", "Paraphrase: The authors introduce a maximum likelihood approach for inverse reinforcement learning (IRL). Theoretical Findings: The algorithm converges within a finite period. For linear rewards the MLIRL problem aligns with MaxEntIRL exhibiting strong duality. Empirical Results: MLIRL surpasses existing IRL methods on standard benchmarks. Strengths: Novel and insightful connection between MLIRL and MaxEntIRL through the strong duality theorem. Proof of finite convergence guarantees for IRL with nonlinear rewards. Promising empirical results showing improved performance. Weaknesses: Novelty claims may be overstated as similar practices exist in previous studies. Lack of a clear explanation for why an ML approach should outperform others. Implemented algorithm appears comparable to prior work despite claims of novelty. Empirical results lack statistical significance due to limited seed runs and absence of confidence intervals. Related Work: Wang et al. (2019): Proposed approaches to enhance IRL efficiency without interleaving reward and policy learning. Brown et al. (2020): Developed an adversarial IRL method without the need for RL optimization. Criticisms: Claim of a singleloop approach is not novel as adversarial IRL methods commonly employ it. The twotimescale approach is wellestablished in GANs and GAILlike algorithms. Equation 3 introduces confusion by suggesting that reward parametrizes policy while line 119 correctly states the converse. Line 155 refers to IQLearn as an example of solving a computationally intensive problem yet IQLearn avoids such an inner loop solver. Lemma 1 resembles the guided cost learning update. Allowing stateonly demonstrations is not a significant contribution as other algorithms possess this capability."], "vKBdabh_WV": ["Paraphrased Statement: Summary: This paper utilizes the concept of amortized optimization to address the issue of optimal transport (OT). A novel approach is presented to accurately predict the OT maps from the input measures. These predictions can serve as an initial solution for conventional OT solvers. Empirically this method substantially reduces the computational time of these solvers in both discrete and continuous scenarios. Strengths and Weaknesses: This work introduces a unique approach for finding effective initializations to solve multiple OT problems efficiently even though amortized optimization in OT is not entirely novel. Experimental results demonstrate the computational advantages of this method. By incorporating the initial predictions from the proposed approach conventional OT solvers converge significantly faster. However the paper lacks demonstrations of standard machine learning tasks that commonly benefit from OT such as generative modeling and domain adaptation. This makes it challenging to evaluate the quality of the transport maps in practical applications. Overall the paper is wellorganized and easy to understand. Minor improvements could enhance its readability. Questions: 1. Clarification of the statement in Line 158 is requested. 2. The formula for the marginal error in Table 1 and Figure 4 needs to be specified. If a stricter threshold (e.g. 10e4 or 10e5) is employed does MetaOT Sinkhorn still exhibit faster convergence 3. The formula for the normalized dual objective value in Figure 7 requires explanation. Is 1.0 the optimal value 4. A recently published paper titled \"Rethinking Initialization of the Sinkhorn Algorithm\" shares similar research goals and should be cited and discussed. Minor Corrections: In Algorithm 1 the update for gi should use fi instead of fi1.", "Summary: Meta Optimal Transport (Meta OT) improves the initialization of OT solvers by utilizing knowledge from previously learned tasks. This speeds up training as demonstrated by experiments. However Meta OT requires that future tasks share similar distributions to previous ones which the authors acknowledge as a limitation. Overall the paper is wellwritten and clear. Strengths: 1. Predicting the parameters of convex Brenier potentials using neural networks addresses challenges in directly learning these parameters and ensuring convexity. 2. Meta OT ensures the optimality of the learned transport map by directly learning the Brenier potential parameters. Weaknesses: 1. Experiments are conducted on limited datasets with no quantitative performance evaluation. 2. An ablation study is needed to compare the initialization quality of Meta OT to random initialization and improved initialization. Questions (from Main Review and Weaknesses): How does Meta OT perform on more complex datasets Can Meta OT be applied to scenarios where the distribution of future tasks differs significantly from previous ones What is the impact of the ICNN complexity on the accuracy of the gradient approximation", "Paraphrase: In this study a new and highly efficient method is introduced to solve optimal transport (OT) problems involving both discrete and continuous distributions using a technique called amortized optimization. Specifically for scenarios where multiple OT problems share similar structures the authors propose an unsupervised learning approach to predict the optimal coupling instead of solving each OT problem independently. The learned predictor function parameterized by neural networks maps input distributions and their associated cost function to the optimal potential. Training involves minimizing the expected value of the dual OT objective over a distribution of problems (described by input measures and cost functions). If the network can generate sufficiently expressive functions this approach effectively solves each individual OT problem providing the true optimal potential. In the discrete setting the authors consider the dual formulation of entropic OT which is unconstrained and allows for a simple gradient descent optimization. For the continuous setting the authors focus on the Wasserstein2 distance and exploit its dual reformulation to obtain an unconstrained optimization problem. Here they aim to predict the inverse costconnection network (ICNN) by learning a function that maps input measures to the ICNN parameters. To refine predictions on test problems the authors employ a few iterations of either the Sinkhorn algorithm (discrete) or the W2GN algorithm (continuous). Experiments on realworld data demonstrate that the proposed approach approximates the true potentials well while being significantly faster. Strengths and Weaknesses: Strengths: The paper is wellwritten and concise. The proposed learning approach has potential for practical applications such as speeding up OT computations in various domains. Weaknesses: The authors could provide more context about the usefulness of their approach. It is unclear how the learned potential accounts for the underlying cost function in the discrete setting. Comparative analysis with a supervised approach would be valuable. The duration of the training stage is not specified. The versatility of the model needs to be explored considering different problem settings and network sizes.", "Summary This paper proposes a metamodel for predicting solutions to the Optimal Transport (OT) problem which has strengths in avoiding repeated learning and potential time savings. Strengths and Weaknesses Strengths: Novel idea Wellwritten paper Weaknesses: Experiments are limited to low dimensions (23) Limited scalability to high dimensions: The metamodel uses a Multilayer Perceptron (MLP) which may not handle highdimensional data effectively. Practical application concerns: The paper focuses on lowdimensional data for which computational efficiency may not be a significant advantage. Problemspecific pipeline: The proposed pipeline assumes a specific type of data (images) and may not generalize well to other data types. Choice of W2 solver: The paper uses W2GN an outdated W2 solver while better solvers are available. Lack of comparison with other computational methods: The paper does not compare its approach to existing efficient OT solvers such as GeomLoss and \"Fast geometric learning with symbolic matrices.\" Literature inaccuracies: Incorrect references to Li et al. 2020 Question The outofsample setting in line 4950 is questionable. If the metamodel accurately approximates W2GN it should generalize to samples beyond the training set."], "wjClgX-muzB": ["Paraphrase: The authors present a new approach to designing proposal distributions for handling stochastic programs in universal probabilistic programming languages (PPLs). Their method divides the stochastic parts of the program into a mixture model of sequences of operations (control flow paths) within a space of simpler programs (straightline programs). This approach differs from the common \"Divide Conquer Combine\" (DCC) method used in Markov chain Monte Carlo (MCMC) sampling for PPLs. The authors evaluate their method on a toy program an infinite mixture model and a task involving learning Gaussian process kernels in a probabilistic contextfree grammar (PCFG). Their results show significant improvements in both logpredictive density and evidence lower bound (ELBO) compared to existing methods showcasing the effectiveness of their approach. Strengths: Wide applicability of variational inference in deep PPLs to a range of problems Superior experimental results particularly for highdimensional tasks Nontrivial test cases that challenge variational inference methods Clear presentation with helpful colorcoding Mathematical explanations accessible to PPL researchers Weaknesses: Lack of comparison to importance samplingbased inference methods such as SMC within DCC Unclear definition of \"marginal likelihood\" for straightline subprograms within a larger program which may not always include observations before reaching stochastic control flow points Questions: Why not include importance samplingbased methods in the comparison against samplingbased inference methods How is the expectation of the importance weight ratio used to determine the allocation of computational resources", "Paraphrased Summary The paper introduces a variational inference approach called Support Decomposition Variational Inference (SDVI) designed for probabilistic programs with randomly varying parts (stochastic support). It aims to optimize variational inferences separately for subprograms with fixed ranges of values. The variational approximation has a mixture structure where each component is optimized independently for each subprogram. The method is evaluated against baseline algorithms on models with stochastic control flow an infinite mixture model with varying cluster counts and inference of kernel structure for a Gaussian process. Strengths and Weaknesses The paper is wellorganized and straightforward. It addresses the challenge of handling universality in probabilistic programming. The approach uses techniques to avoid wasting resources on less important subprograms and ensure finite computation results. The authors provide a Pyro implementation but implementing the method may be challenging due to the complexities mentioned. The paper lacks a discussion of computational costs and examples of scenarios where stochastic support is necessary. Questions The term \"rethinking\" in the title suggests comparison to complex methods but the algorithm is only evaluated against a simple approximation. The reference to \"Pyros AutoGuide\" in Figure 1 may be confusing an explicit algorithm name would be better. Its unclear how to differentiate between different paths of a program. Do users have to use different variable names for different paths In Table 1 the description \"MAP estimate of K6 the local guide qk corresponding to the SLP with 5 components\" should be corrected to \"SLP with 6 components.\"", "Paraphrased Statement: Summary: This paper introduces a new version of variational inference designed specifically for probabilistic programs incorporating stochastic support. To overcome the difficulties posed by stochastic support the paper presents an innovative approach to creating a variational guide by dividing the issue into smaller more manageable programs. The results of testing show that this strategy improves the effectiveness of inference. Strengths: 1. The paper tackles a significant and useful issue and the suggested method complements probabilistic programming languages effectively. Access to the code is particularly beneficial. 2. The paper is wellwritten and easy to comprehend. 3. Thorough experiments back up the efficacy of the method. Weaknesses: 1. The work appears to extend [3335] particularly [35] gradually. The authors must demonstrate how their work varies from the studies already mentioned. 2. The paper needs to expand its related work section and provide a more indepth explanation of how the proposed method compares to similar approaches. This subject has experienced considerable advancement necessitating a more thorough examination of the literature. Questions: 1. Can the authors outline the primary distinctions between their work and [35] for instance 2. How does it stack up against MCMC VI", "Summary: This paper proposes a new method for building and training variational families for probabilistic models. It divides the program into control flow paths (SLPs) and creates a separate variational family for each path. The variational families are combined into a mixture model with weights based on their estimated posterior probabilities. Experiments show competitive results against other automated inference methods. Strengths: Strong empirical results on several inference problems. Supports variational inference for models with complex control flow. Uses a technique to encourage exploration of different control flow branches. Provides an implementation in the Pyro probabilistic programming library. Weaknesses: Lack of explanation for how the \"Z\" term in rejection sampling is computed. Insufficient discussion of limitations and potential issues. Misleading claims about the expressiveness of default variational families and the state of the art in probabilistic programming. Questions: How is the rejection sampling method in Section 4.5 implemented How is the \"Z\" term computed Are both methods described in Section 4.5 for resolving the local guide issue applied simultaneously Revised Score: After the author response the reviewer has increased their score to a 7."], "uLhKRH-ovde": ["Summary The paper explores training neural networks in settings with limited communication (e.g. LocalSGD) using gradient clipping. Despite gradient clippings effectiveness in stabilizing neural network training its direct application in LocalSGD is challenging. To address this the paper proposes CELGC an algorithm inspired by prior work [1]. CELGC aims to make gradient clipping compatible with LocalSGD scenarios. The paper presents theoretical and empirical analyses of CELGC. Strengths and Weaknesses Strengths: The paper addresses a significant problem enabling gradient clipping in LocalSGD. It provides both theoretical and empirical foundations for CELGC. The paper includes a comprehensive experimental evaluation involving different neural architectures. Weaknesses: Lack of novelty due to similarity to [1]. Absence of comparisons to relevant works such as [2]. Difficulty in interpreting and understanding empirical results. Update Based on the authors responses and improvements the reviewer believes the paper should be accepted. Pros: Wellwritten and focused on an important problem. Correct theoretical analysis. Extensive experiments including hyperparameter tuning. Cons: Limited robustness evaluation and lack of confidence intervals in figures. Absence of comparison to more advanced optimizers and scalable configurations (e.g. ResNet50 on ImageNet). Questions: 1. Compatibility of CELGC with advanced optimizers (e.g. Momentum Adam). 2. Comparison of CELGC to local gradient clipping. 3. Applicability of [3] in the LocalSGD setting. 4. Explanation for volatility differences between Naive Parallel SGDClip and CELGC in Figure 1.", "Summary Paraphrase: This paper investigates a local stochastic gradient clipping method for nonconvex distributed optimization. It presents a convergence analysis for the proposed CELGC algorithm in an independent and identically distributed (i.i.d.) setting and conducts experiments that demonstrate its effectiveness. Strengths and Weaknesses: Strengths: Investigates a local stochastic gradient clipping method for nonconvex distributed optimization. Provides convergence analysis for the proposed CELGC algorithm in an i.i.d. setting. Conducts experiments to validate the algorithms efficiency. Weaknesses: The proposed CELGC algorithm only extends the classic FedAvg algorithm to a gradient clipping setting. The convergence analysis while relaxing the smoothness condition still relies on the i.i.d. assumption. The experiments lack a comprehensive set of comparative methods focusing solely on gradient clipping methods.", "Paraphrased Statement: Summary: The study introduces a distributed gradient clipping technique. Under specific conditions the algorithm will achieve an approximate optimum (\u03f5stationary point) with sample complexity of O(1(N\u03f54)) and communication complexity of O(1\u03f53). The algorithm is more efficient than the traditional naive parallel SGD clipping. Strengths: Operates under the weaker condition of (L0 L1)smoothness instead of Lsmoothness. Empirical evaluations demonstrate improved training speed based on wall clock time. Weaknesses: The number of communication iterations is still O(1\u03f53) even with L1 being zero while the lower bound for Lsmoothness is 1\u03f52. Assumption 1(iv) is relatively stringent although it holds in certain layers for specific experiments. Demonstrating the algorithms effectiveness in heterogeneous environments is challenging as most of the analysis pertains to homogeneous settings. Additional comparisons with other algorithms such as the clipping version of FedAVG would strengthen the empirical evaluation.", "Paraphrased Statement: This paper combines local stochastic gradient descent (SGD) with gradient clipping to address communication efficiency and gradient explosion issues in neural networks. Local SGD optimizes multiple models locally before aggregation reducing communication costs. Gradient clipping prevents excessive gradient values addressing the problem of exploding gradients in certain network architectures like RNNs and LSTMs. The combination of these techniques enhances efficiency and stability during training. Strengths: Clear rationale for combining local SGD and gradient clipping. Theoretical analysis demonstrating improved communication complexity compared to standard parallel gradient clipping. Experimental results showing faster training times in some cases. Weaknesses: Limited application due to the dominance of transformerbased models which offer easier parallelization and reduced gradient explosion issues. Lack of experiments in federated learning a primary use case for local SGD. Questions: Does the communication frequency (I) influence the communication complexity of the proposed method in Table 1 In the convergence plots (Figures 1a 2a 3a) why do all methods achieve similar performance even when communication frequency is significantly reduced Is there a theoretical explanation for this"], "rwyISFoSmXd": ["Paraphrase: Summary This research explores the concept of demographic parity in quantile regression. The authors build upon conformalized quantile regression by employing optimal transport and functional synchronization techniques to integrate demographic parity. Experimental findings indicate that this \"fair\" version of quantile regression effectively narrows the gap in demographic parity without compromising the coverage rate or interval length. Strengths and Weaknesses Pros: Novel approach to addressing fairness in quantile regression. Sound and logical techniques used in the proposal. Promising experimental results with a significant reduction in demographic parity gap at a minimal cost to coverage rate and interval length. Cons: Limited context provided on the specific issue of fairness in quantile regression. Omission of crucial information regarding comparative fairness algorithms mentioned in Figure 2. Questions: Clarification on whether the template is intended for use in submission mode.", "Paraphrased Statement: Summary: This paper introduces the study of fairness in quantile regression. The authors propose the CFQP algorithm by combining concepts from Wasserstein barycenter and smoothing techniques. They demonstrate the algorithms effectiveness through theoretical analysis and realworld experiments. Strengths and Weaknesses: Originality: The proposal to address fairness in quantile regression is novel. The combination of existing techniques while not remarkably innovative is still a unique approach. Quality: The benefits of the two smoothing strategies are insufficiently explained. An ablation study testing the effects of removing or modifying the smoothing strategies would enhance the papers quality. Clarity: The three conditions in Section 5 are ambiguous and require further clarification on their practical implications. The subscript \"n1\" needs clarification in Section 2. Typos and errors in Figure 1 (e.g. \"affacted\" and \"quartile\") need correction. Significance: The methodology is valid supported by both theoretical analysis and experimental results. Questions: Refer to the details in the \"Originality\" \"Quality\" and \"Clarity\" sections for specific questions and areas of improvement.", "Paraphrased Summary: This study examines how to incorporate group fairness into conformalized quantile regression. The authors adapt existing quantile regression methods to ensure demographic parity among groups. They develop a postprocessing technique to modify a quantile regressor to make it fair. The authors provide theoretical guarantees for the effectiveness of their approach showing that it produces models with both good prediction power and fairness. They compare their method to other fair regression methods and find it often outperforms in terms of fairness and error. Strengths and Weaknesses: Strengths: Clear writing Novel contributions to addressing fairness in quantile regression Provides both theoretical and experimental results Demonstrates superiority over stateoftheart methods Weaknesses: Limited experiments Lack of information on computational feasibility (running time equation 8 complexity) Narrow focus on a single fairness metric (demographic parity) No ability to tune allowed unfairness Questions and Comments: How can we theoretically justify or control the allowed amount of unfairness Can the model be adapted to different fairness metrics that depend on the dependent variable Can we provide more precise details on how other fair models are adapted to quantile regression Why is the \"fourfifths rule\" used to explain demographic parity in regression given that it originates from classification contexts", "Paraphrase: Method: The method aims to provide reliable prediction intervals in regression tasks while ensuring fairness. It leverages quantile regression to obtain prediction intervals and utilizes conformal inference to ensure valid coverage. Strengths: Novel approach to addressing fairness in prediction intervals. Combines advancements in quantile regression and conformal inference. Maintains competitive performance in terms of coverage length and prediction error. Weaknesses: Clarity in problem definition and motivation could be improved. Discrepancy between the fairness goal and the method should be addressed. Presentation may not be accessible to readers unfamiliar with the underlying techniques. Questions: How is fairness defined for different quantile functions related to overall fairness How does the proposed method compare to existing approaches for fair quantile regression Why is conformal inference necessary for fair prediction intervals How do the experiments demonstrate the tradeoff between fairness and coverage Suggestions: Explain the importance of fairness in quantile regression and uncertainty estimates. Describe the challenges in combining fair regression functions with conformal inference. Consider alternative fairness metrics for realvalued predictions such as equalizing calibration across groups."], "x8DNliTBSYY": ["Paraphrase: This research demonstrates that NTK (Neural Tangent Kernel) can be wellbehaved even with a relatively small number of parameters. Specifically they show that approximately \u03a9(N) parameters are required for a reliable NTK matrix and O(N) parameters suffice for memorizing training data. Using these NTK bounds they explore the memorization capacity and optimization through gradient descent with square loss. Strengths and Weaknesses: The investigation of neural networks in a sublinear setting is valuable. The authors provide insights into data memorization and optimization for square loss. The technical aspects are wellpresented and comprehensible. However questions arise regarding the significance and limitations of the results: 1. Assumption 2.5 implies a requirement for a large number of neurons contradicting the statement following Theorem 3.1. 2. When adjusting the network widths uniformly the upper and lower NTK bounds align but this requires a small training data size relative to the input dimension which could limit the impact of the findings. 3. Previous research has demonstrated that polylogarithmic widths suffice for ReLU networks to learn and generalize under certain assumptions. The authors could consider comparing their results to these with more lenient overparameterization conditions. Questions: 1. How do the results compare to those obtained in the classification setting 2. Can the large neuron count requirement be relaxed or accommodated 3. What are the implications of the small training data size condition when adjusting network widths uniformly", "Paraphrased Statement: Summary: This paper provides a lower bound on the smallest eigenvalue of the empirical neural tangent kernel (NTK) Gram matrix for loosely pyramidal neural networks. This bound is proportional to the product of the widths of the last two hidden layers. It suggests that \u221aN neurons are sufficient for memorizing N data points using gradient descent (GD). Strengths: The paper is wellwritten and the theoretical contributions are clear. The threestage centering argument is a novel proof technique. Weaknesses: Assumption 2.4 restricts the setup to loosely pyramidal networks hiding some constants in the bound of Theorem 3.1. The probabilistic bound in Theorem 3.1 includes a term CN\u22c5exp(c\u221aN) which raises concerns about its significance for large N. Figure 1 lacks context. Questions: Describe the hidden terms in Theorem 3.1. Explain under what conditions CN\u22c5exp(c\u221aN) is small enough to make the event in Theorem 3.1 highly probable.", "Paraphrase: Summary: This paper explores the issue of minimal overparameterization in neural networks. It specifically examines the scenario with layers of sublinear width and establishes a lower bound based on the smallest eigenvalue of the Neural Tangent Kernel (NTK). Strengths: The paper claims and demonstrates through Theorem 3.1 that the NTK is wellconditioned in the sublinear setting. It provides detailed proofs for the main steps. Weaknesses: The paper could benefit from clarification on its assumptions. For instance the condition \\alpha 1 in Eq. (4) might imply that the number of neurons exceeds the sublinear case. Additionally the proof for the smallest NTK eigenvalues dependence on the optimizer is unclear. Since NTK evolves with parameter updates its worth considering if the proof relies on any implicit assumptions about the optimizers behavior. Questions: The reviewer raises the aforementioned questions regarding assumptions and optimizer dependence.", "Paraphrased Statement: This paper examines the neural tangent kernel (NTK) in situations with minimal overparameterization. Specifically it establishes a lower bound for the NTK eigenvalue in highly underparameterized neural networks. These findings are then applied to analyze network memorization and training dynamics. Strengths: Thorough investigation of minimal overparameterization Rigorous proofs Two practical applications based on theoretical results Weaknesses: Highly theoretical and focused on minimal overparameterization Requires expertise in NTK research Questions: 1. Why investigate minimal overparameterization when modern deep learning typically employs significant overparameterization How does this research contribute to understanding NTK theory 2. Clarify the meaning of \"to be conditioned.\" 3. Regarding the experiments in Figure 2 were the labels randomly generated If so could the results be replicated using regular labels or datasets with lower complexity"], "rG0jm74xtx": ["Paraphrase: Summary: The researchers have created a model for predicting time series that works well even with limited data. They used a generative model based on a Bayesian variational autoencoder (BVAE) to achieve this. The model outperformed other similar methods in experiments. Strengths: The model is based on sound theoretical principles making it easy to understand and justify. The model performed significantly better than the chosen benchmark in experiments. Weaknesses: The estimated noise in Figure 2 exhibits notable discontinuities that are not reflected in the ground truth. This suggests potential numerical instabilities in the algorithms. The authors did not perform hyperparameter tuning which might limit the models potential performance. Questions: What impact would shortening the time series have on the models performance How can we explain the discontinuities in the estimated noise when the ground truth is continuous", "Summary Paraphrase: The proposed model D3VAE combines a diffusion process with a Bidirectional Variational AutoEncoder (BVAE) for enhanced time series forecasting. By introducing diffusion random noise is minimized. Total correlation is used to separate latent variables improving dependency extraction and overall performance. A denoising score matching module enhances accuracy. Experiments demonstrate D3VAEs superior performance over baseline models on synthetic and realworld datasets. Strengths and Weaknesses Paraphrase: Strengths: Ingenious model design with a disentangled representation learning module and a denoising module for improved prediction accuracy. Extensive experimental results showcasing the effectiveness of each module on various datasets. Improved performance compared to baseline models. Weaknesses: Insufficient evaluation of disentanglement: The claim that total correlation forces latent variable disentanglement lacks concrete evidence. Vague model structure: A clearer illustration of the D3VAE architecture is needed to understand module placement. Limited evidence for interpretability: The connection between disentangled representation learning and improved interpretability is not thoroughly explored. Questions Paraphrase: 1. What evidence can demonstrate the actual disentanglement of learned latent variables Z 2. Where in the results is it shown that interpretability is improved compared to baseline methods 3. Can you provide a detailed explanation of the D3VAE architecture including the placement of each module", "Paraphrased Summary: Authors Proposal: Use a diffusion model coupled with a denoising mechanism to improve time series data. Perform forecasting directly using the generative model. Employ diffusion models to handle random uncertainty. Utilize latent variable disentanglement for better interpretability. Strengths and Weaknesses: Originality: Presents novel insights on the connection between noise and diffusion models. Extends existing models (BVAE denoising disentanglement) to create a unique method. Quality: Generally highquality study. Thorough experiments with ablation analyses. Discusses methodological advances but lacks comparison to other generativediffusion models. Clear presentation but could benefit from a summary infographic and moving examples from the appendix to the main text. Clarity: Motivation for the method is clear (addressing noise in time series forecasting). Authors claim to reduce aleatoric uncertainty but the mathematical derivation in Lemma 12 is unclear. Connection between diffusion model and disentanglement is not well explained. Significance: Addresses a relevant issue and proposes a promising augmentation strategy for noisy time series data. Questions: Elaborate on the relationship between aleatoric uncertainty and the diffusion model. Specify the advantages of the diffusion model for time series applications. Explain the connection between diffusion models and disentanglement."], "qbSB_cnFSYn": ["Summary This paper presents a novel approach for solving differential equations using Generative Adversarial Networks (GANs). It extends the concept of physicsinformed neural networks (PINNs) by using a neural network to represent the flow equation. To optimize the neural network parameters it employs a discriminator and an adversarial loss function instead of a traditional loss function. This method reportedly outperforms PINNs trained with manual loss functions and numerical solvers on various benchmark differential equations. Strengths Wellwritten and structured Clear presentation of the method and results Extensive ablation study and discussion of training stability Weaknesses Inclusion of training hyperparameters in the main text distracts from the main message Insufficient explanation of some results including the ablation study and Figure 3 Limited discussion of the methods limitations While residual monitoring and instance noise improve training stability their contribution is not sufficiently novel Questions Justification for using MSE as the performance metric Clarification of the logic behind using Lg Ld to indicate generator performance Impact of sampling grid coarseness on GAN performance relative to other methods Minor Comments Confusing notation in section 4 Redundant legends and missing labels in Figure 2 and 3", "Paraphrased Summary: This paper introduces an innovative approach to solving differential equations using adversarial training with Generative Adversarial Networks (GANs). By \"learning\" the loss function for PhysicsInformed Neural Networks (PINNs) this method enables unsupervised training. Strengths: Provides a unique technique for solving differential equations with GANs. Demonstrates significant performance improvements on differential equation tasks. Integrates effective training techniques for GAN stability. Wellpresented paper with detailed experimental information in the appendix. Weaknesses: Lacks discussion on mode collapsing a potential issue with GANs. No theoretical analysis or guarantees provided. Questions for Further Exploration: Conduct an ablation study to evaluate the effectiveness of the training techniques used to stabilize the GAN. Investigate the potential for mode collapsing in the proposed GAN and its impact on finding solutions to differential equations.", "Paraphrase: Summary This research introduces Differential Equation Generative Adversarial Network (DEQGAN) a method for solving differential equations using GANs. The generator in DEQGAN learns to produce the solution while the discriminator learns an effective loss function. DEQGAN is compared with traditional PhysicsInformed Neural Networks (PINNs) on various ODEs and PDEs demonstrating significantly lower L2 error. Strengths Clear and concise writing Superior performance to PINNs in experiments Independence from predefined distances like L1L2 which may be advantageous in certain situations Weaknesses 1. Missing Discussion in Related Work: The paper overlooks important related works and lacks context for DEQGANs contribution. 2. Lack of Explanation on L2 Loss Issues: The authors criticize L2 loss without providing a clear explanation of its limitations or when it may be inadequate. DEQGAN does not provide theoretical justification for its learned loss function either. 3. Questionable Use of GAN Loss Function: GANs are designed to minimize the JensenShannon divergence which is not applicable to DEQGANs goal of solving differential equations with unique solutions. Minimizing divergence between two Gaussians simplifies to L2 loss. 4. Artificial Equations and Limited Initial Conditions: The experimental equations are artificial and only a single initial condition is used for each equation. This raises concerns that the results may be biased in favor of DEQGAN. 5. Training Instability: DEQGAN requires multiple runs and filtering out poor performances indicating training instability. Questions 1. Justification for Learned Loss Function: Why is the learned loss function better than L2 loss Can the authors provide theoretical justification 2. Comparison of Speed: How does DEQGANs speed compare to PINNs and other improved variants like SAPINNs 3. Robustness with Random Initial Conditions: How would DEQGAN perform if the initial condition were a random function from a random field"], "xp5VOBxTxZ": ["Paraphrased Statement: This paper investigates the behavior of networks trained using gradient descent with normalization layers and weight decay. A new metric called spherical sharpness is introduced to address the scale invariance of the loss function. The study demonstrates that theoretically networks approach a state of \"edge of stability\" where parameters and learning rate fluctuate near a local minimum. It is also shown that the parameters converge towards a period2 oscillatory sharpnessreduction flow. The authors provide experimental results to support their theoretical findings. Strengths: Important topic in the machine learning community. Comprehensive theoretical analysis provides potential explanations for the benefits of normalization in generalization. Solid technical contributions including a demonstration of sharpness reduction in gradient descent. Considerations given to the practicality of assumptions. Additional results included in the appendix. Weaknesses: Lacks motivation for the specific choice of setting (BNWD). Proofreading is needed to correct typos and improve clarity. Questions: Why is the combination of BN and WD particularly noteworthy What causes the jumps and drops in accuracy in Figure 2 How do Figures 1 and 2 demonstrate the \"sharpnessreduction bias\" induced by normalization", "Paraphrased Statement: Summary The authors have examined how Gradient Descent (GD) affects the spherical sharpness of neural networks using normalization layers. They track the trajectory of GD from stable conditions towards the Edge of Stability (EoS). Within the EoS the trajectory oscillates around a stable point and shifts in a direction that diminishes spherical sharpness. The authors demonstrate through inductive reasoning based on two EoS conditions that this cyclical behavior persists until the error is minimized. They have empirically validated their theory on CIFAR10 using batchnormalized VGG11 and ReNet20 with Swish activation. Strengths and Weaknesses As a regular user of deep neural networks the papers content may be challenging for nonspecialists. A significant portion of the mathematical proof is in the appendix which could lead readers to accept its validity without fully understanding it potentially compromising their grasp of the concepts. To enhance accessibility the authors could incorporate more visual examples or demonstrations from the neural network results at different optimization stages. This would illustrate what aspects are being learned (e.g. correctly classifying previously misclassified images) and highlight the role of batch normalization in this process. Questions Typographical Clarifications and Grammatical Corrections Page 3 Line 91: \"generalizaiton\" should be \"generalization\" Page 3 Line 95: \"with different with different\" should be \"with different\" Page 3 Line 201: \"moves towards\" should be \"moves toward\"", "Paraphrase: This paper focuses on the optimization of deep neural networks (DNNs) using normalization techniques. While normalized DNNs are commonly used and often perform well their theoretical underpinnings are not fully understood. The paper explores the effects of normalization and weight decay during gradient descent (GD) training. It demonstrates that normalization and weight decay facilitate GDs movement to flat regions and impose a bias that reduces the sharpness of the loss function. Specifically the authors theorize that after reaching the manifold of minimizers GD can enter an \"Edge of Stability\" regime. In this regime the use of additional GD steps leads to movement towards a less sharp region on the manifold. Experimental results on VGG11 ResNet20 matrix completion and linear regression using batch normalization provide support for this theory. Strengths: Provides insights into the training of normalized DNNs using GD expanding our understanding of practical DNN behavior. Demonstrates the role of normalization and weight decay in navigating flat minima and reducing loss sharpness. Links flat minima to generalization capabilities. Experimental validation on various DNN architectures and tasks. Weaknesses: The paper focuses on the \"scaleinvariant\" property of normalization which is not applicable to all normalization methods (e.g. batch normalization). This may limit the scope and accuracy of the theory. The experiments primarily use batch normalization which violates the scaleinvariance assumption. The authors should consider modifying their experiments or narrowing the scope of their theory. The use of the term \"reduce the sharpness of the loss\" may not accurately reflect the process as the loss function itself is often fixed. GD may instead move from sharp minima to less sharp minima. Questions: Why does the paper not consider the trainable parameters associated with normalization methods which are known to impact DNN performance The high fluctuation in training loss after 47k steps in Figure 2 suggests potential disconnections or \"holes\" in the manifold of minimizers. Does this phenomenon warrant further investigation", "Paraphrased Summary: This study demonstrates that normalization combined with weight decay influences fullbatch gradient descent with finite steps toward finding solutions with minimal curvature. Strengths and Contributions: The paper is wellorganized and comprehensible. The study provides theoretical explanations for the previously puzzling \"EoS\" phenomenon. The intuition behind the theory is presented effectively. Weaknesses: Grammatical errors throughout the text make the paper somewhat difficult to read. The empirical benefits of weight decay are not entirely clear. The study does not explore the implications of stochasticity in training which is more common in practice. The empirical evaluation is limited. The flatness measurements used in the study are scaleinvariant which may not be fully representative of all reparameterization problems. Questions: How do the findings relate to the effects of stochasticity on training Can the theorys predictions be validated under weaker assumptions Are there stronger invariant metrics or functionspace properties that could provide more comprehensive insights", "Paraphrased Statement: Summary: This paper explores the underappreciated generalization benefits of BatchNorm and proposes a novel analysis to support this claim. The analysis establishes that BatchNorm under weight decay and scaleinvariant loss leads to stable training in two regimes: stable and edgeofstable (EoS). In the EoS regime the model undergoes a \"sharpness reduction\" flow minimizing the \"sharpness\" of the sought solution by utilizing scale invariance. Strengths: The paper presents a strong analysis and introduces valuable tools that extend existing work. The overall narrative is wellorganized and engaging. Weaknesses: The paper lacks explicit ablative experiments to demonstrate the impact of BatchNorm on generalization and sharpness reduction. Relevant prior work such as Luo et al. and Tanaka and Kunin is not adequately discussed or compared to the proposed findings. The paper does not explore the potential absence of sharpness reduction in other normalization layers. Related Work: Luo et al. proposed an objective approximating BatchNorms regularization effects. Tanaka and Kunin established a connection between SGD with normalization layers and adaptive gradient methods similar to the analysis presented in this paper. Experiments: Suggested experiment: Train a model without BatchNorm using Daneshmand et al.s initialization strategy to verify if similar test accuracy and sharpness reduction can be achieved. Further discussion on why similar results may not be observed with other normalization layers is recommended. Extension to Other Layers: The paper should discuss why other normalization layers despite exhibiting scale invariance may not exhibit similar sharpness reduction effects."], "ywxtmG1nU_6": ["Paraphrased Statement: Summary The authors present an enhanced architecture for graph neural networks that respect E(n) symmetries. They make two improvements: 1. Expanding node features to include a matrix of E(n)invariant vectors instead of a single vector and using an MLP for message passing based on the pairwise norms of these vectors. 2. Incorporating attention for pooling between different hierarchical levels with careful normalization to maintain equivariance. The method outperforms previous models on datasets involving motion capture and molecular data. Strengths and Weaknesses Originality: The method builds on existing work with limited originality. Quality: The paper is of high quality with a sound method thoroughly evaluated on diverse tasks. Potential Improvements: Compare the method to a similar approach in reference [1] which also generalizes the EGNN. Clarify the hierarchical architectures relationship to a UNet with skip connections. Clarity: Explain the \"J\" experimental parameter more clearly. Provide a more detailed explanation of the second loss term in Equation (16). Significance: The method is easy to implement and improves performance in a widely applicable domain potentially having a significant impact in the community. Conclusion: Overall the paper introduces a wellmotivated and effective method that advances the state of the art. Acceptance is recommended.", "Paraphrased Statement: Summary: This paper proposes a hierarchical message passing neural network called EGHN which is invariant to rotations and translations. It consists of: An equivariant message passing layer that enhances the EGNN model. An equivariant pooling layer based on the Diffpool architecture. An equivariant unpooling layer that leverages clustering coefficients from the pooling layers. EGHNs performance is evaluated in prediction tasks for various settings showing promising results. Strengths: Clear and accessible writing style. Introduction of E(3) equivariant hierarchical message passing networks. Relevance to learning on protein structures. Positive experimental findings that surpass previous methods. Weaknesses: Insufficient ablation study. Lack of sufficient detail when discussing related works particularly concerning similarities with Diffpool. Unclear benefits of the hierarchical structure requiring further demonstration. Questions: Originality of the pooling and unpooling layers and reconstruction loss. Novelty of equations 3 and 4 which generalize EGNNs. Intuition and compatibility of equations 3 and 4 with equivariance. Quantitative assessment of how equations 3 and 4 improve upon EGNNs. Comparison of training times between EGHN and flat MPNNs.", "Paraphrase: Summary: The paper introduces a new architectural approach for modeling the behavior of systems with multiple interacting components. Key features of this approach include: Its responsive to spatial transformations of inputs. It can capture the hierarchical structure often found in such systems. These capabilities are achieved through specifically designed layers (EMMP EPoolEUnPool). Its claimed to be the first architecture that combines both equivariance and hierarchical modeling. Strengths: Clear and detailed explanation of the architecture. Empirical evidence shows its ability to model complex hierarchical systems effectively. Potential for further insight into model behavior through node clustering. Weaknesses: The EMMP layer offers only a minor variation from existing EGNN layers to handle directional matrices. EGNNs are still used as external layers within the architecture. The benefits of using the new EMMP layer remain unclear. The architecture requires manual specification of several parameters including cluster count and adjacency matrix construction. The authors provide guidance on the impact of cluster count but theres a lack of guidance on the other parameters. Questions: How does the architecture perform when the external EMMPs are not relaxed to EGNNs Can the authors provide a more comprehensive evaluation and discussion of the influence of the cluster count parameter"], "yZgxl3bgumu": ["Paraphrased Statement: Summary: This research introduces a substitute computational brain model (FEM) that mimics the biomechanics of the brain. The model utilizes a Graph Neural Network (GNN) to represent both node positions and interconnections (through message passing). Compared to existing methods this model offers comparable accuracy and speed. Additionally it supports the generation of surrogate models for various finite element approximations. Strengths and Weaknesses Strengths: Fast and accurate FEMbased surrogate models are enabled by these approximations using GNN. It incorporates interconnections between nodes thanks to GNN. Applicable to various finite element approximations. Weaknesses: Like other approximations it only considers forces whereas in scenarios like brainshift often only node displacement data is available. Only validated against synthetic models. Questions: How can this approximation be used in practical situations Can it simulate anisotropic tissue behavior The provided images could be improved for clarity.", "Paraphrased Statement: Summary This study introduces a technique for brain tissue deformation using Graph Neural Networks (GNNs). The GNN is trained on data from a Finite Element Model (FEM) simulation. Unlike other methods trained on FEM results the authors leverage a GNN to capture the graph structure of the FEM. The proposed method achieves comparable performance to existing approaches for brain tissue deformation. Strengths and Weaknesses The paper excels in its clarity and organization particularly in the introduction and prior work sections. However the method section lacks detail and is difficult to follow especially in Eq. 1 and Eq. 2. Providing a comprehensive description of each element in these equations would enhance understanding. Additionally the evaluation could be expanded beyond mean and standard error to include specific analysis of brain regions. Questions 1. Why were two different aggregation functions (mean and max) employed 2. Are the input node features the same for all nodes 3. What is the rationale behind providing input in both polar and Cartesian coordinates 4. Is the output dimension of the final linear layer 63x3", "Paraphrased Summary: Researchers developed a machine learning framework using graph neural networks (GNNs) to predict deformations of brain tissue during surgery based on finite element analysis (FEA) data. The framework was tested on two datasets and compared to baseline FEA results. Findings showed that GNNs can make accurate predictions rapidly on GPU hardware potentially enabling realtime guidance during surgical procedures. Strengths: Clear and concise presentation Introduction of a novel GNN approach for FEA data modeling with justifications for its suitability Comprehensive review of relevant prior work and motivations Weaknesses: Limited validation due to lack of comparison with other methods data Training data generated from FEA simulations which may have limitations compared to experimental measurements Lack of computational resources to directly compare against certain datasets or implementations Questions: 1. Please provide the computation time for both the GNN model and the baseline FEA method as well as the speedup factor gained by using GNNs. 2. Did you attempt to reach out to the authors of other studies to request access to their data or implementations for validation purposes 3. Can you explain the specific computational challenges that prevented you from using certain datasets or models due to insufficient computational power"], "xI5660uFUr": ["Paraphrase: Summary: This paper presents a variablerate image compression technique that combines selective compression with adaptive quantization. Experimental results suggest that its performance matches reference models in terms of ratedistortion (RD) while reducing decoding time. Strengths: Addresses the significant issue of variablerate image compression. Wellwritten with clear explanations. Effective in experimental testing. Weaknesses: The method lacks sufficient differentiation from existing adaptive quantization techniques ([1923]) and its originality is unclear. The concept of a 3D importance map has been used previously ([Li18]) albeit not for variablerate compression. The added benefits of selective compression appear limited especially for advanced base models like [7]. Evaluations against more recent compression models ([11 13 He21 Gao21 Qian21]) are missing. Comparative performance analysis with other variablerate compression approaches is absent. The BDrate results in Section 4.1 are difficult to comprehend. A tabular summary would be helpful. Questions: Are QVq and IQVq trained simultaneously with the compression model Would using a more complex network for generating the 3D importance map improve performance beyond a single 1x1 convolution layer (as used in [Li18]) What occurs if q1 or q8 is reduced Its remarkable that only 55.81 of features are used with q8 indicating that half of the extracted features are redundant. Discussing this observation could provide insights into pruning image compression models. In Figure 6 the difference between SCR(Full) and SCR(wo selective compression) increases at lower bpp values. Explaining this trend would be valuable. In Figure 5 not all features for a given q are used at higher q values. This is confusing since the adjustment curves are monotonic indicating that increasing q should only add additional features for encoding resulting in a 100 reuse rate. (Minor) Clarify whether \"BDrate gain of 3.21\" refers to an increase or decrease by 3.21.", "Paraphrase: Summary: Most existing variablerate image compression models either add complex components to the base model or rely on adaptive quantization which compromises coding efficiency. This paper proposes Selective Compression of Representations (SCR) which incorporates selective compression into variablerate models based on adaptive compression. SCR creates a 3D importance map from the hyper decoder output adjusts it using a channelwise parameter and binarizes it to indicate elements selected for compression. Additionally an interpolationbased method supports continuous variablerate compression. Strengths: SCR reportedly improves compression efficiency when used with \"Hyperprior\" and \"Meanscale\" base models. Selective entropy coding reduces decoding time particularly at low bitrates. SCR introduces minimal additional complexity with only a lightweight 11layer convolutional network. Weaknesses: The paper claims that adding modules to the base model for variablerate compression increases complexity but does not compare SCR to such models in terms of efficiency and complexity. SCR has limited impact on \"Context\" models which are commonly used in stateoftheart codecs. This raises concerns about its effectiveness with advanced codecs. Suggestions: Expand experiments to include models with diverse structures to assess SCRs versatility.", "Paraphrased Summary: The researchers introduce a deep learningbased approach for variablerate image compression. This method selectively compresses image representations using a generated 3D binary mask. Strengths: Improved RD performance at low rates compared to nonselective compression. Faster decoding times due to reduced entropy decoding. A visual representation that highlights the importance of features. Weaknesses: The method may require additional processing and network modules potentially increasing complexity. It does not reference or compare to previous studies on generating importance maps for deep image compression. Encoding time and computing overhead are not reported for the proposed method. A direct experimental comparison of RD curves with previous variablerate compression methods is missing. Questions: Can the authors provide information on encoding time and computing overhead for their proposed method Why do the proposed methods perform slightly worse than baselines in certain rate ranges Is it possible to experimentally compare the RD curve of the proposed approach with previous work on variablerate compression", "Summary This research introduces a method to boost the efficiency of an existing variable bitrate model using adaptive quantization and entropy coding time reduction. It generates a 3D importance map to selectively encode latent variables based on target bitrate and decoded side information. This map bridges the performance gap with fixed bitrate models. Strengths Significantly improves performance over baseline models especially at lower bitrates. Low overhead with a simple convolutional layer and importance adjustment curves. Applicable to various existing models as demonstrated with hyperprior meanscale and contextbased models. Clear and comprehensive paper organization. Weaknesses No ablation study on the importance adjustment a key contribution. Misleading presentation of decoding time reduction (Fig 9). Unfair comparison of BD rate by not comparing all variable bitrate models to the reference model. Third contribution (continuous bitrate through interpolation) already exists in previous work. Increased training cost and complexity. Questions 1. Can you confirm that even though results in Cui 2020 (e.g. Fig 2) seem to close the gap with fixed bitrate models it is still worse at lower bitrates 2. In your ablation study what exactly is \"the SCR variant without the selective compression\" 3. Do you have ablation results on importance adjustment such as using only adaptive quantization from Cui 2020 or using 3D importance maps with different inputs 4. For the meanscale hyperprior do you set maskedout latents to the mean value predicted by the hyper decoder 5. Are there decoding time profiling results for contextbased models 6. Explain why reshape time increases with rate and why the hyper decoder takes so long compared to the decoder network. 7. Provide details on the GPU used for profiling and any parallel entropy coding implementation."], "ypXcTtbBsnZ": ["Paraphrased Summary: The researchers introduce three techniques for crossmodal contrastive learning: 1. AugmentationAware Feature Embedding: An augmentation feature vector is added to image encodings before projection forcing the encoder to generate representations invariant to data augmentations. 2. Modified Noise Contrastive Estimation (MPNCE) Loss: Multiple positives and a \"strong positive\" pair are incorporated into the NCE loss along with a rebalancing parameter to equalize losses across domain pairs. 3. DomainDependent Similarity Measure: Separate similarity thresholds and temperature parameters are used for each domain pair to account for domain differences in similarity scoring. Strengths: Clear and detailed explanations of the contributions Benchmark results on wellestablished datasets and models Contributions effectively address limitations in existing methods Weaknesses: Limited demonstration of how each contribution solves its targeted problem Lack of analysis on how MPNCE and domaindependent similarity measures affect learned representations and similarities No discussion of ethical considerations or limitations Questions: How do MPNCE and domaindependent similarity measures alter learned representations and their similarities Are there any ethical implications or limitations associated with the proposed techniques", "Paraphrased Statement: This study presents a method for training a CLIP model within a single representational space addressing the limitations of previous approaches that trained each selfsupervised objective independently. The proposed model UniCLIP combines the contrastive losses from both inter and intradomain pairs to create a universal space. The authors employ three techniques to resolve discrepancies between domains: augmentationaware feature embedding MPNCE loss and a domaindependent similarity measure. Strengths: Clear and wellorganized writing Detailed experimental setup for enhanced reproducibility Substantial performance improvements on zeroshot linear probing and retrieval tests Comprehensive ablation studies Weaknesses: Incremental nature of the work building upon prior research Lack of experiments on a broader range of tasks (e.g. image captioning visual question answering) despite the claim of unified representation learning Inadequate discussion of potential limitations Unverified claims such as UniCLIPs applicability to various multimodal datasets Typos throughout the paper Questions: 1. A thorough analysis of the models limitations is missing. 2. Experimental evidence is needed to support the claim that UniCLIP can be utilized across diverse multimodal datasets. How can such experiments strengthen this claim", "Paraphrased Statement: This study presents UniCLIP a novel framework that unifies the contrasting loss functions of interdomain and intradomain pairs within a consistent space for VisionLanguage Contrastive Pretraining. It combines three key optimizations: Data augmentation MPNCE (MultiPerspective Negative Contrastive Estimation) loss Domaindependent similarity measure UniCLIP outperforms existing methods and demonstrates strong performance with a straightforward design. Strengths and Weaknesses: Strengths: Superior performance Clear and concise presentation Weaknesses: Lack of originality as it primarily combines existing techniques MPNCE loss and domaindependent similarity measures are not novel Data augmentation is a widely used method Questions: None identified in the provided statement.", "Paraphrase: Summary: UniCLIP combines contrasting learning methods for text and image data into a single training objective. Unlike previous approaches UniCLIP incorporates both positive and negative samples from both text and images into its loss function. It utilizes a unique loss called NCE to emphasize hard positives which is theoretically and practically superior to other options. UniCLIP also employs an augmentation encoder that separates the image encoder from the image projection to handle data augmentation while maintaining semantic representation. Strengths and Weaknesses: Strengths: Thorough analysis of existing contrastive loss functions. Strong zeroshot classification performance on various tasks. Weaknesses: Lack of comparison to finetuning results for imagetext retrieval. Zeroshot performance may not accurately reflect crossmodal retrieval capabilities. The use of positive and negative samples across modalities is not significantly novel compared to previous work in metric learning. Questions: Did the authors create and train their own versions of baseline models like SLIP and DeCLIP The different pretraining dataset used in this paper makes it difficult to compare the results to the original DeCLIP paper. Can the authors comment on why DeCLIP performs differently in this study compared to recent reports"], "lQ--doSB2o": ["Paraphrased Statement: The authors introduce a new technique for generating distortions in images that deceive artificial intelligence (AI) models but are easily recognizable by humans. This approach manipulates highlevel features of the image rather than individual pixels making the distortions more comprehensible to humans. Strengths: 1. Extensive experiments demonstrate the effectiveness of the technique in revealing insights about the behavior of AI image classifiers. The distortions created are interpretable allowing for a better understanding of how models make decisions. 2. The technique appears to be novel and supported by a comprehensive literature review. 3. The paper is generally wellwritten and accessible. Weaknesses: 1. The authors refer to the technique as an \"adversarial attack\" which is typically used to describe imperceptible distortions. Since the distortions in this technique are clearly visible alternative terminology is recommended. 2. The authors should consider discussing related methods such as cycleconsistencybased techniques that also aim to generate humaninterpretable distortions in image classification models.", "Paraphrase: Summary: Using a pretrained generator model researchers created adversarial examples that are interpretable to humans and robust (effective as attacks even in different contexts). They focused on image classification networks and utilized multiple loss functions: Penalizing predicted class similarity to the original image Reducing highfrequency patterns Enhancing image realism Encouraging a specific class while discouraging the true class They validated the utility of these loss functions through an ablation study. They tested three types of adversarial attacks: patch square patch and generalized patch. Remarkably their method generates attacks that can be physically printed and remain effective when applied to a different classifier. Additionally they can exploit the generated attacks to create targeted copypaste attacks providing insights into the target networks decisionmaking process. Strengths and Weaknesses: The paper is of high quality wellpresented and potentially impactful. While the method itself only slightly modifies existing work it makes significant contributions through: Extensive analysis Consideration of societal impact Provision of a valuable tool for neural network interpretation Criticisms: The perturbations in adversarial examples (e.g. Fig. 3) are sometimes difficult to identify hindering understanding. The labels and descriptions in Fig. 3s caption require clarification. The distinction between this work and Brown et al. (2017) is not fully explicated. Fig. 6 could be improved for clarity by grouping related rows together. The \"bad example\" in Fig. 11 may not be convincing and could benefit from a better one. Questions: How feasible is it to create copypaste attacks from adversarial examples generated by the method Were the exhibited copypaste attacks cherrypicked or representative Suggestion for Future Research: It could be beneficial to regularize the features such that adversarial features target specific layers of the target network enhancing interpretability."], "m2JJO3iEe_5": ["Paraphrase: Summary: This research adapts the randomized smoothing technique to a scenario where a deep neural network (DNN) generates embeddings classified using the nearest neighbor method to a class prototype. The study establishes a robustness guarantee bound and a method for estimating it with high probability. Strengths: Addresses the critical issue of robust learning in lowdata settings. Utilizes specific properties of this case to derive a simplified bound building on the work of Cohen et al. Presents a novel contribution. Clear and concise writing. Weaknesses: Lacks comparison to other approaches (e.g. Kumar Goldstein) making it difficult to assess empirical significance. Theoretical contribution is not substantial enough to stand alone. Uses Hoeffding inequality for confidence bounds but applies it to multiple estimates requiring union bound to ensure high probability bounds on all distances simultaneously. Runtime can be significant (up to 5 seconds per prediction) and it is unclear how this varies across datasets. Other Observations: Confusing use of \"n\" for both dimension and number of Monte Carlo samples. Algorithm descriptions contain inaccuracies in set notation. Introduction requires improvement. Line 19 requires rephrasing. Line 166 references undefined terms (g1 g2). Questions: Lack of comparative analysis is a major concern. Confidence intervals need adjustments to account for multiple estimates. Empirical robustness estimation and comparison against bounds could enhance the paper."], "oMhmv3hLOF2": ["Neural radiance fields (NeRFs) struggle with dynamic scenes where the relationship between 3D points and their projected images changes over time. To address this the paper presents a method for capturing the radiance fields of video sequences. The method involves constructing an explicit voxel grid representation of the radiance field for the initial frame (base model) and focusing on learning the differences (frame grids) between subsequent frames. The differences are identified using a narrow band around the contours of dynamic scene elements. To manage the large size of explicit voxel grids a compression strategy is introduced. The method also utilizes a multiscale learning paradigm to improve optimization efficiency. Strengths: Clear and wellwritten presentation Intuitive and motivated narrowband tuning strategy Innovative compression strategy for dynamic video representation Weaknesses: Need for clearer explanation and motivation for the pilot model guidance framework Lack of discussion on the qualitative impact of multiscale parameterization with narrowband tuning Ambiguity in the theoretical justification for comparing pilot model base training with fullscale training Questions: What is the theoretical basis for comparing pilot model base training with fullscale training beyond the computational benefit", "Paraphrase: This work extends the Plenoxel representation to dynamic scenes by estimating frametoframe differences instead of individual Plenoxel grids for each frame. By training only in areas where motion is expected (narrow band) and employing a pilot model for multiscale training the method reduces memory and training time significantly. Despite these optimizations the reconstruction quality remains reasonable. Strengths and Weaknesses: The method addresses long training times prevalent in neural 3D video reconstruction. It demonstrates reduced training and inference times compared to other methods while achieving acceptable reconstruction quality. Quantitative results are favorable. Visual results show good performance on existing datasets but reveal some artifacts on the proposed dataset possibly due to wider camera baseline less background texture or optimization tradeoffs. Technical Contributions: Time and memory savings by fitting only frametoframe differences Narrow band finetuning for training in motionprone areas Multiscale training guided by a pilot model Compression of frametoframe differences Questions: Does Eq. 2 use the correct notation Y(d) Can the authors clarify the process of selecting voxels for narrow band finetuning and differencebased compression and provide details on their contributions to compression"], "rDT-n9xysO": [], "nxl-IjnDCRo": ["Paraphrase: Summary: This study examines the behavior of diffusionbased deep generative models (DDGM) and proposes that they can be divided into generative and denoising components. The researchers combine DDGMs with denoising autoencoders to create the DAED model which enhances sample and reconstruction quality. Strengths: Clear and concise writing Novel interpretation of DDGMs as separate generator and denoiser providing new insights into diffusion models Demonstrated improvements in generative performance Weaknesses: Arbitrary Definition of Denoising and Generation Stages: The distinction between denoising and generation phases is based on tT vs. reconstruction which may be arbitrary. Support for this distinction could be strengthened by presenting standard deviation values. Potential Impact of Model Capacity: The improved performance could be attributable to increased model capacity rather than an inherently superior model design. Questions: The precision and recall metrics used to evaluate images require clarification."], "szt95rn-ql": [], "zvNMzjOizmn": [], "weoLjoYFvXY": [], "wfel7CjOYk": [], "lHuPdoHBxbg": [], "tglniD_fn9": [], "zAuiZpZ478l": [], "s71h4wo9bFI": [], "vsNQkquutZk": [], "qOgSCLE5E8": ["Paraphrase: Summary This paper explores distribution calibration methods aimed at aligning the feature distribution of new classes to match that of known classes for fewshot learning. The method is compared experimentally to an existing distribution calibration method (Free Lunch) that utilizes the average features of the two nearest known classes. The authors propose an alternative approach using optimal transport to compare new classes to all known classes. Additionally they learn the cost function for comparing a known class to a new sample as an optimal transport problem. Experiments are conducted on various fewshot benchmarks (miniImageNet TieredImageNet CUB and CIFARFS) with 5way 1shot and 5way 5shot settings demonstrating stateoftheart results. Strengths 1. The application of optimal transport for aligning distributions is logical and improves upon the previous distribution alignment method Free Lunch. 2. Background concepts are wellexplained although some improvement in writing clarity could enhance understanding. 3. The experimental outcomes are impressive establishing new benchmarks on standard fewshot benchmarks. Weaknesses 1. The computational cost of the method is not discussed especially considering the twolevel OT used. 2. The computational complexity scales with the number of classes which could become prohibitive for datasets with larger classes. 3. The work relies heavily on the Free Lunch paper in terms of methodology and writing. Some sections appear to be paraphrased and variable substitutions seem unnecessary. 4. The writing could benefit from additional rounds of editing for both grammatical and idiomatic correctness. Miscellaneous Line 21: The reference for image classification is unusual. Line 30: \"introduce to calibrate\" should be \"introduce for calibration.\" Line 38: \"Although with reasonably good performance\" is awkward. Line 51: \"labelled\" and \"labeled\" should be consistent. Line 75: \"Owing a rich theory\" could be rephrased as \"Due to its rich theory.\" Line 98: \"to generate from\" should be \"to be generated from.\" Line 167: \"severely biased\" may be an exaggeration for using only the mean. Line 214: \"Besides\" is used incorrectly. Table 1: The best results on CUB are not emphasized. Line 341: The sentence beginning with \"And\" could be rephrased to avoid the conjunction. Questions 1. Why must the features be nonnegative 2. How does the number of known classes affect the method particularly in terms of computational cost 3. Why does the proposed method benefit from increasing the number of known classes when Free Lunch found it to decrease performance", "Paraphrased Statement: Summary This study investigates fewshot learning and introduces a weighting scheme based on optimal transport to utilize examples from base classes when updating classifiers for novel classes. Expanding on the framework of a previous study (referred to as [14]) this work employs an optimal transportbased weighting scheme rather than Euclidean distance. The resulting weights for each base class implicitly account for the similarity of individual examples within that class to examples in the novel classes. Evaluation The proposed weighting scheme outperforms [14] and achieves competitive performance on established fewshot image classification benchmarks across various backbones. Comprehensive ablation studies validate its superiority over [14]. Strengths and Weaknesses Strengths Thorough experiments across multiple backbones crossdomain results and ablation studies Qualitative results on optimal transport weights aid in understanding the approach Code provided for reproducing experimental outcomes Weaknesses The improvement in performance over [14] is modest given the \"principled\" nature of the approach. It would be beneficial to assess the potential for improvement from distribution calibration to provide context for the findings. Demonstration of how the weights distinguish between examples within the same class would be helpful. Insights into the generalization of transport probabilities in crossdomain settings would be valuable. Questions Refer to the \"Weaknesses\" section.", "Paraphrased Statement This research introduces a calibrationbased technique for fewshot learning that resolves the issue of bias toward new samples. Optimal transport (OT) is used as a crucial tool to overcome the limitations of a previous study by substituting Euclidean distance with an OTbased distance that gauges the distance between distributions by determining the matching cost from one distribution to another. A lowlevel OT optimization problem is proposed to compute the matching cost matrix between base class and novel samples prioritizing the importance of samples within their classes. Experimental results indicate the superiority of the proposed method known as HOT over existing fewshot learning methods including FreeLunch. Strengths HOT advances upon FreeLunch by constructing a more statistically robust distance measure enhancing its performance. Experimentation is thoroughly designed and conducted demonstrating HOTs effectiveness in most settings especially crossdomain scenarios. Weaknesses The paper lacks experiments specifically addressing the claimed limitations of FreeLunch leaving the source of performance improvement unclear. The presentation requires improvement featuring complex notation and disjointed sentences. While HOT outperforms in overall performance its gain is relatively minor in 1shot settings. Questions Provide specific examples that illustrate how HOT distance effectively addresses the limitations of Euclidean distance. Highlight the strengths of HOT in crossdomain settings throughout the paper. Simplify the notations and clarify descriptions in Sections 3.2 3.3 for improved comprehension.", "Paraphrased Statement Summary This study presents a new algorithm for categorizing items with limited examples known as \"fewshot classification.\" The method uses distribution calibration. The algorithm is similar to \"FreeLunch\" (Yang et al. 2021) but with an improved approach for creating new features for the fewshot classes. FreeLunch used a simple method for generating new features by randomly sampling from Gaussian distributions whose parameters were determined by the statistics of the base classes. This study proposes a more precise approach based on hierarchical optimal transport (HOT). HOT models the relevance between base and fewshot classes and uses this information to calculate the parameters of the Gaussian distributions for generating new features. Strengths The algorithm is straightforward to understand and apply. The HOT approach appears reasonable and wellfounded. The algorithms effectiveness is demonstrated through extensive testing. The study includes an ablation study and helpful visualizations. Weaknesses The algorithm is incremental relying heavily on FreeLunchs ideas. Questions How does the algorithms running time (particularly the HOT step) scale with the number of data points Does HOT cause significant computational bottlenecks In situations where there is a noticeable divergence between base and fewshot classes can you detect a significant difference in the learned transport plan Is it relevant to cite the Yurochkin et al. 2019 study on hierarchical optimal transport for document analysis and how does it relate to this work"], "ymAsTHhrnGm": ["Summary: This paper examines the learnability of approximate Stackelberg games where followers respond rationally or using quantal response. The findings are generally positive and rely on specific utility properties. The paper explores sample complexities and conducts simulation experiments. Strengths and Weaknesses: The content is somewhat incremental and has been partially addressed in previous works excluding one important reference ([A]). The writing could provide more explanation and comparisons to earlier results. Proposition 2 is a small extension of work in [20]. Proposition 1 is a known fact for rational and quantal response games. The paper references the sample complexity for security games in [9 37] but its unclear how this relates to Theorem 1. Theorem 1 assumes a learned utility function with a specific property. Its not clear how this function can be inferred from response samples. Theorem 1 refers to the leader optimal strategy but it should be explicitly called Security Set Equilibrium (SSE). The paper also cites [A] which provides a sample complexity result for quantal response that achieves a similar result to Theorem 2. The paper lacks sufficient novelty and depth in comparison to prior works. Some citations are missing information such as venue for [20] and some seem irrelevant to Stackelberg games ([24 25]).", "Paraphrase: LeaderFollower Games in Algorithmic Game Theory Leaderfollower games are widely studied because their solution concept the strong Stackelberg equilibrium has realworld applications. However this equilibrium is sensitive to player preferences (utilities). If the followers utilities are not accurately estimated the leader may be forced into a harmful strategy. Proposed Approach This paper introduces a method to uncover the followers true utilities through repeated interactions. It relies on the assumption that the follower behaves according to the logit quantal response model meaning they play each action with nonzero probability. This allows the authors to estimate the number of interactions needed to determine the followers utilities with sufficient precision. Extensions and Experimental Results The method can be modified for specific utility structures and when the followers responses approach best responses. Empirical analysis shows that the method can accurately estimate utilities in games of varying sizes rationality levels and randomness. Strengths and Weaknesses The paper presents clear motivation novel results and understandable arguments. The empirical results support the theoretical claims although in some cases the error margins could be improved. The notation is concise and the definitions are wellexplained. However the term \"sequential game\" is used to refer to repeated leaderfollower games which may be confusing. Questions Can the approach be generalized to other quantal response models Would the result hold for a broader definition of defender strategies in sequential security games When does it become beneficial to use the PUREEXP algorithm over the PURE algorithm How do computational times scale with game size What are potential applications of the learning method Can the approach be generalized to sequential games considering issues like noncredible threats Is the mathematical program in Theorem 2 solvable or approximable in polynomial time", "Paraphrase: This study investigates the challenge of determining the followers utility in Stackelberg games assuming that the follower makes decisions probabilistically using a softmax function rather than a strict best response. Surprisingly this simplifies the problem. The paper examines the complexity of identifying follower utility first assuming that the followers full mixed strategy is observable then relaxing it to observing only realized actions. The study presents an algorithm and validates its theoretical findings through experiments. Strengths and Weaknesses Stackelberg games are often used in security work but they typically assume known follower utilities. This paper extends this setup by considering the more realistic scenario where follower utilities need to be inferred from their behavior. These novel results are likely to impact the field of security games. The paper is wellwritten and clear explaining its motivation results and proofs effectively. Minor Issues The paper cites incorrect references for the equivalence between mixed and quantal response strategies. It is unclear why the optimization in equation (4.1) uses \\haty instead of y. The assumption of observing the full mixed strategy should be explicitly stated. The use of \\tildey(t) and \\haty(t) in different sections should be clarified. It is not specified whether the empirical distributions \\tildey(t) and \\haty(t) are deterministic onehot distributions. The sampling distribution used to generate \\tildex in Algorithm 1 is not specified. The description of how the \\mathcalX and \\mathcalY lists are generated in Algorithm 1 is confusing. Questions The generation of the followers utility matrix V with a specific structure raises concerns about its impact on the empirical results."], "u8FDFtoMKp2": ["Paraphrased Summary: This research aims to transfer knowledge between node types on a heterogeneous graph where some nodes have many labels (source nodes) and others have none (target nodes). They introduce a technique called Knowledge Transfer Network (KTN) that enables knowledge transfer by learning a transformation that aligns the representation of target nodes to the representation of source nodes. Strengths: Addresses the practical problem of knowledge transfer on heterogeneous graphs. Proposes a novel method of mapping target nodes to the source node representation space using transformation matrices. The KTN architecture is grounded in theoretical observations and empirical evidence. Demonstrates significant performance improvements over baseline methods. The paper is wellwritten and easy to follow. Weaknesses: Does not cite previous work on transfer learning on graphs. The explanations of synthetic graph experiments need clarification. Questions: Discuss relevant transfer learning work on graphs. Explain the task used in the experiments (Table 1 and 2). Consider evaluating on fewshot tasks with labeled target nodes. Minor Suggestions: Clarify the description of the first contribution. Specify whether \"number of batches\" in Figure 2 refers to iterations or batch size. Consider minimizing KTN losses for all layers not just the last layer. Add a more informative caption to Table 2. Fix the typo in Section 3 Line 88.", "Paraphrased Statement: Summary: This study highlights the common issue of label imbalance in heterogeneous graph (HG) datasets. It proposes a zeroshot transfer learning module (KTN) to bridge the knowledge gap between abundant and zerolabeled node types. The paper is praised for its originality. Strengths: Timely research topic: Label imbalance is prevalent in HG datasets. Solid theoretical foundation and straightforward implementation for KTN. Effective performance improvement over existing HGNN models. Excellent reproducibility with provided source code and datasets. Weaknesses: Omission of a recent HGNN method ([TKDE 2021] Interpretable and Efficient Heterogeneous Graph Convolutional Network) in the literature review. Inconsistent use of the adjacency matrix (Ats) in the training and test algorithms. Lack of citation or explanation for HMPNN which appears to differ from standard MPNN despite being described as an extension. Questions: Please address the weaknesses outlined above including the missing literature citation adjacency matrix inconsistency and HMPNN explanation.", "Paraphrased Statement This paper explores zeroshot node types in knowledge transfer proposing a method to transfer knowledge from node types with ample labels to those without labels. The contributions include the problem definition and a model for knowledge transfer. The paper provides theoretical analysis and a graph neural network model. Strengths Novel and practical problem investigated with clear motivation. Proposed model supported by theoretical analysis despite potential limitations. Method outperforms baseline approaches. Weaknesses Misleading title as the study focuses on a specific aspect of transferable knowledge in graphs. Baseline methods are outdated more recent approaches should be considered. Lack of detail in the experimental setup for the proposed method. Questions Despite the stated availability of only source type labels the training data appears to contain target type labels. Feasibility of the proposed method for other graph tasks and complex graphs remains unclear. Potential for addressing the identified weaknesses to enhance the overall assessment.", "Paraphrased Summary: This work introduces a new approach to domain adaptation on graphs with different structures (heterogeneous graphs). It rebuilds the embeddings of nodes in the source graph using the embeddings of nodes in the target graph. Mathematical proofs support the accuracy of the proposed method. Tests demonstrate that it significantly outperforms existing approaches using heterogeneous graph neural networks (HGNN). Strengths: Wellorganized and informative paper Clear example in Figure 1 demonstrates HGNN training for different node types Figure 1 and Figure 2 effectively illustrate the papers motivation Easytounderstand theoretical results in Section 4.3 which lead to an efficient algorithm in Section 5 Comprehensive experiments validate the methods effectiveness and adaptability Weaknesses: Comparison methods in Table 1 may be outdated more recent works should be included. The authors should clarify the calculation of gradients in Figure 2 (b) and (c) specifically that they are gradients with respect to the loss function \\mathcalLs."], "pbILUUf_hBN": ["Paraphrased Statement: Summary: This study investigates online learning using feedback graphs and introduces an algorithm that achieves remarkable regret bounds in both stochastic and adversarial settings. The algorithms design incorporates elements from the EXP3 and EXP3.G algorithms combined with a distinctive exploration strategy that leverages graph structure. The analysis is extended to timevarying feedback graphs. Strengths: Wellwritten and accessible Significant contributions to the field Novel exploration scheme provides unique insights Weaknesses: Complexity of Algorithm 2 could potentially be improved Numerical experiments comparing the algorithm to nonadaptive benchmarks for a performanceadaptability tradeoff analysis would be valuable Questions: Algorithms 1 and 2 could be presented more clearly highlighting the specific location where Algorithm 2 and Equations (2)(3) are integrated into Algorithm 1. The statement in Line 216 regarding the observability probability should specify that it holds in the stochastic case potentially clarifying that Equation (4) represents a more general scenario. Theorem 2 could be expanded to cover broader parameter ranges that ensure the desired bounds. A unified theorem encompassing multiple cases (e.g. Theorem 4) would enhance readability. The complexity of Algorithm 2 is estimated as O(K3) which may be an overestimation further exploration of efficient algorithms (e.g. with O(K2) complexity) is recommended. The timevarying feedback graph analysis in Section 6 could potentially be extended to uninformed settings exploring concepts such as MAS numbers or exploiting knowledge of specific graph classes.", "Paraphrased Statement: This study focuses on achieving optimal performance in a setting where decisions are made based on feedback from a graphstructured system. The authors propose a novel algorithm that combines elements from two existing algorithms: EXP3.G designed for graph bandits and EXP3 designed for nongraph bandits. In a competitive setting their algorithm achieves nearoptimal regret bounds comparable to the best known results. When the feedback graph is timevarying the algorithm can be adapted to maintain similar performance guarantees. Strengths: Clear and concise writing with an intuitive explanation of the algorithm. Improvement upon previous work in both adversarial and stochastic environments with theoretical regret bounds close to optimality. The algorithm builds upon the established EXP3 framework providing a familiar foundation. Formal proofs for all theorems are provided. Weaknesses: The optimality of the regret bound in the stochastic environment is unclear as it may not match the instanceoptimal bound. The novelty of the algorithm may be limited as it generalizes EXP3 with a new exploration strategy. The adversarial regret bound presented in the main text includes an unnecessary logarithmic factor which can be removed. Typos: Line 483: \"alphas\" should be \"alpha.\" Lines 575576: The first inequality should be \"\u226412n\u03bb\" instead of \"\u2265n\u03bb.\" Line 632: \"1\\ellsi\" should be \"1\\tilde\\ellsi.\" Question: Can the authors clarify how the obtained regret bound in the stochastic environment compares to the instanceoptimal bound", "Paraphrased Statement: Summary: This research explores online learning in a novel context where feedback is provided as a directed graph. By merging principles from two algorithms EXP3 and EXP3.G the paper introduces an algorithm called EXP3.G. This algorithm achieves an almost optimal regret bound in both random and intentionally adversarial situations. Strengths and Weaknesses: Engaging and clear presentation. The regret bound surpasses prior results by improving logarithmic factors and approaching a nearly optimal threshold. Includes an extension for evolving feedback graphs and a regret bound for EXP3.G in this scenario. Questions: Can the authors comment on EXP3.Gs performance in extreme cases such as a singlearmed bandit or full information Can the authors discuss potential applications of graph feedback particularly directed graph feedback in realworld settings", "Paraphrased Statement: This research explores a form of multiarmed bandit problem where the learner receives loss information about all arms connected to the selected arm within a defined feedback graph. The proposed algorithm aims to strike a balance between worstcase robustness and stochastic loss minimization. Inspired by the EXP3 algorithm for multiarmed bandits the algorithm leverages the EXP3.G concept designed for adversarial feedback. It employs a novel exploration scheme to estimate gaps and control the interplay between exploitation and exploration. Strengths and Weaknesses: Demonstrates that the EXP3 approach can achieve balanced guarantees for weakly observable feedback graphs. Extends the algorithm to handle adversarial losses with logarithmic regret terms. The novel exploration scheme has potential applications beyond this work. Questions: 1. Can the \\frac1\\Delta2 term in the regret bound be removed as seen in other balanced regret algorithms for multiarmed bandits 2. Could the SAPO approach yield improved regret bounds in this setting"], "ucNDIDRNjjv": ["Paraphrased Statement This paper focuses on improving forecasting using a novel attention scaling technique. The input time series is normalized in chunks and these scaling parameters are used to scale the attention matrix through a series of neural networks. This process significantly enhances the performance of transformerbased forecasting models. Strengths Simple implementation Effective in improving performance Weaknesses Limited theoretical rigor Misuse of the term \"stationarity\" Questions and Suggestions Avoid using the terms \"stationary\" and \"stationarity\" unless they are used correctly. Reframe the method as a general chunkwise transformation that modifies the distribution of data highlighting the similarity between linear and nonlinear cases. Simplify the analysis by directly using the mean and standard deviation as scalars rather than approximating them. Clearly define \"stationarity\" and \"relative stationarity\" in the context of the overstationarization problem. Modify the conclusion to remove excessive claims about the frameworks generality and performance.", "Paraphrase: This research introduces a solution to the problem of nonstationarity in time series data. The proposed framework called Nonstationary Transformers consists of two main components: Series Stationarization: Converts raw time series data into a more predictable form by removing nonstationarity. Destationary Attention: Captures the unique nonstationary features of the original data during the selfattention stage. The results demonstrate that Nonstationary Transformers are effective when combined with various Transformer models consistently outperforming existing stateoftheart methods. Strengths: Clear motivation and accessible writing style. Destationary Attention module is simple making it easy to implement and understand. Impressive results with simple techniques significantly improving upon stateoftheart baselines. Weaknesses: The proposed Series Stationarization technique is similar to the \"Scale handling\" technique presented in [1]. The authors should clarify the differences between these methods. The study does not provide any information about the standard deviation of the results which would indicate the statistical significance of the proposed method.", "Paraphrase: Summary This study examines a nonstationary issue with transformerbased forecasting models. It reveals that normalization in data preprocessing can hinder transformers ability to generate distinctive temporal attention impairing prediction accuracy. To resolve this the paper introduces a \"destationary attention\" mechanism that incorporates mean and variance statistics into attention score calculations. Empirical tests demonstrate consistent improvements across various transformer types. Strengths and Weaknesses Pros: 1. Userfriendly writing and wellstructured organization 2. Effortless integration with diverse transformer backbones 3. Consistent performance enhancements Cons: 1. Insufficient motivation for using normalization: The authors present an analysis of vanilla selfattention but fail to adequately explain why normalization causes the disappearance of nonstationary information. They should consider omitting normalization before embedding to preserve this information. 2. Unfair experimental setup: The study uses normalized data for preprocessing a common practice in long sequence forecasting. However the reported RMSE and MAE are not inverted to reflect the actual forecasts. This conflates the effects of the proposed normalization strategy with the preprocessing normalization. Including scalebased metrics like sMAPE and MASE would enhance the evaluation. 3. Limited comparison: The study lacks comparisons with more advanced transformerbased forecasting models like ETSformer and FEDformer which have outperformed Autoformer significantly. Demonstrating that the proposed normalization also benefits these models would strengthen the argument. 4. Hyperparameter optimization queries: For long sequence forecasting the selection of hyperparameters for baseline models is crucial for fair comparisons. The paper should provide details on this process such as the hyperparameters used for NBeats and LSTNet.", "Summary Paraphrase: The authors recognize that making time series stationary by transforming it removes valuable information for prediction. This issue is termed \"overstationarization.\" While stationarity enhances model accuracy preserving the removed information is crucial. The paper introduces a \"Nonstationary Transformer\" model that addresses this concern. It comprises a stationarization component and a component that reintroduces the stationarized information into the time series. The key innovation is the Destationary Attention Module which transforms data after stationarization. Strengths and Weaknesses Paraphrase: Strengths: Recognizes the significance of recovering information removed by stationarization and introducing a method to do so. Demonstrates improved performance across benchmarks by reincorporating information. Weaknesses: The motivation behind the specific mechanism is unclear. The experiments do not compare other methods of incorporating nonstationary information. The writing is complex and could be simplified. The claim that the predictive capability of nonstationary series is essential is not adequately supported by the analysis presented. The stacked structure of Transformers is not a unique advantage as most time series deep learning models are stacked. Questions: Why does the Destationary Attention Module not directly operate on the nonstationarized features before stationarization"], "zGPeowwxWb": ["Paraphrase: Summary This research presents a new perspective on denoising diffusion implicit models (DDIM) based on equilibrium theory. By interpreting DDIM as an iterative system this approach enables parallelization leading to faster image generation and model inversion with reduced memory demands. Strengths The mathematical foundation of diffusion models is explored in depth. The novel view of diffusion models as iterative systems allows for efficient differentiation. Demonstrated inversion capabilities of the model are impressive as showcased in Figure 4. Weaknesses The claim that modern autograd packages require storing the entire computational graph for all diffusion steps is not entirely accurate. Alternative methods exist that avoid this issue. The approach is less effective for batched image generation. The reported DDIM results in Table 1 are incomplete as they do not include the models best performance. The reported results show only marginal improvements over existing methods. The suggested use of the new acceleration method in conjunction with other speedup techniques is not supported by experimental results. Minor Comments Typo on line 41: \"join\" should be \"joint.\" Typo on line 209: \"effect\" should be \"affect.\" Citations on lines 266268 misrepresent previous work. The cited papers do not manipulate latent codes for highlevel image editing but utilize the inherent properties of diffusion models for specific tasks. Questions Is the inversion method described in Algorithm 1 the same as the one proposed in DDIM What is the rationale behind the choice of T values for different datasets PostRebuttal The authors have addressed the concerns raised and expressed willingness to provide additional experiments in the final version of their paper. As a result the reviewers score has been increased to 7.", "Paraphrased Statement: This research introduces a variation called Deep Equilibrium Model (DEQ) based on the existing Diffusion Denoising Implicit Model (DDIM). DEQ aims to enhance sampling efficiency and model inversion while preserving performance. Specifically it approaches the DDIM sampling sequence as a fixedpoint set and utilizes a fixedpoint solver to minimize the entire sampling sequence simultaneously. The study demonstrates the practical usefulness of DEQ in singleshot image sampling and model inversion across multiple datasets. Strengths and Weaknesses: Strengths: Builds upon existing research and offers a novel and practical approach. Wellwritten and presents competitive results in both quantitative and qualitative evaluations for image sampling and model inversion. Weaknesses: The rationale for singleimage sampling is unclear and requires further justification. Singleimage sampling may be limited to shorter sampling lengths and smaller image resolutions. The claimed simultaneous solution for all equilibria needs further justification through ablation studies. Parallel computing capabilities are not fully explained. The applicability to more general diffusion models with stochastic sampling needs clarification. Questions: 1. Explain the practical significance of singleshot image sampling and explore potential applications such as image restoration and reconstruction. 2. Include comparisons with the original DDPM model to demonstrate the performance advantage of DEQ in different sampling length regimes. 3. Provide evidence to support the claim that jointly sampling leads to better estimation of intermediate states in fewer steps. 4. Expand on the implementation details of parallel computing and its applicability in multibatch cases. 5. Clarify the initialization strategy and explore the impact of different initializations on convergence and sampling performance. 6. Discuss the applicability of DEQ to diffusion models with nondeterministic sampling steps. 7. Evaluate the proposed approach against DDIM on largerscale datasets such as ImageNet with higher resolutions. 8. Examine if the architectural improvements of DEQ translate to scorebased diffusion models and conditional SDEs. 9. Consider leveraging the proposed method for exact gradient calculation in model inversion.", "Paraphrase: Summary: This paper introduces an improvement to the \"denoising diffusion implicit model\" (DDIM) a variation of \"denoising diffusion probabilistic model\" (DDPM) known as DEQDDIM. DEQDDIM treats denoising steps as fixedpoint iterations and efficiently reconstructs cleaned images using Anderson acceleration. Strengths: Innovative perspective of viewing denoising iterations as fixedpoint iterations. Weaknesses: Inherits DDIMs limitation of deterministic results due to noise removal reducing probabilistic model capabilities and image diversity compared to DDPM. Potential flaw due to nontimeinvariance in denoising as the denoising function depends on time which is crucial for variance estimation. Questions: 1. Clarify the use of \\alpha instead of \\bar\\alpha to avoid confusion. 2. Correct typos and grammatical errors: line 41: \"join equilibria\" should be \"joint equilibria\" line 137: \"compared to the traditional\" should be \"in contrast to the traditional\""], "rWgfLdqVVl_": ["Paraphrased Statement: Summary: This research introduces a novel transformerbased approach for creating prototypes representing visual concepts. It utilizes crossattention and selfattention mechanisms to establish relationships between visual input and trainable prototypes and proposes a concept disentanglement loss to promote correlation learning. Evaluation on 3D datasets demonstrates the methods utility. Strengths and Weaknesses: Strengths: Employing attention mechanisms for visual decomposition is innovative. Unsupervised visual decomposition capability. Extensive experimentation and analysis under different conditions including concept disentanglement based on language. Weaknesses: Limited experimentation to synthetic datasets and small image sizes. Questionable generalization to broader scenarios with complex visual scenes. Difficulty in validating the correspondence between disentangled representations and independent visual concepts. Inclusion of datasetspecific prototypes which may contradict the goal of capturing diverse visual scenes. Questions: (1) Request for experiments with broader data domains and exploration of applications like image editing. (2) Justification of unique concepts learned and attribution techniques for linking prototypes to concepts. (3) Detailed explanation of the mechanism and impact of datasetspecific prototypes.", "Paraphrase: This paper introduces VCT an unsupervised system for extracting visual concepts from images. VCT uses a tokenizer to turn visual information into \"concept tokens\" which represent distinct concepts within the image. By manipulating these tokens VCT can learn to distinguish between different concepts. VCT outperforms existing methods on multiple benchmark datasets demonstrating its effectiveness in learning disentangled representations and decomposing scenes into separate objects. Extensive testing validates the specific design choices used in VCT. Strengths: Clear and accessible writing VCTs strong performance on disentangled representation learning and scene decomposition VCTs ability to manipulate and visualize learned concepts VCTs decomposition capabilities even without explicit object annotations Robustness to hyperparameter variations Weaknesses: Limited testing on realworld images beyond CeleBA Unclear correspondence between learned concept tokens and specific factors of variation Potential influence of pretrained VQVAE in VCTs concept learning Need for clearer presentation of scene decomposition experiments Questions: Can VCT effectively handle more complex realworld image datasets How can we determine the specific semantic meaning of each learned concept token", "Paraphrased Statement: This study introduces a novel unsupervised transformerbased model for identifying distinct visual concepts in images. Key contributions include: A new transformer architecture that represents images as a series of tokens each indicating a distinct visual concept. A novel concept disentangling loss function that guides the model to anticipate modified concept tokens. Evaluations on simulated datasets (Shapes3D MPI3D Cars3D) reveal that the proposed model outperforms existing methods in isolating concepts and decomposing scenes. The framework supports languagealigned disentanglement when using the CLIP encoder as the image tokenizer. Strengths and Weaknesses: Strengths: The proposed architecture is flexible and can be integrated with various model designs. The concept disentangling loss enhances the discovery of disentangled visual features without manual labeling. Experimental results demonstrate the models ability to extract disentangled representations from widely used datasets. The study is wellstructured and accessible. Weaknesses: Some text lacks sufficient supporting evidence. Evaluation results are primarily focused on simulated datasets leaving questions about the models performance on realworld data. The designs motivation lacks clarity with the study mainly focusing on architectural innovations without exploring the underlying reasons for superior performance. Questions: How does the disentangling loss (Ldis) impact results on pretrained AE and VQVAE models Are the batch size and token number sensitivity results datasetdependent Have transformerbased architectures been previously used for disentangled visual representation learning If so how does the proposed model differ"], "kUnHCGiILeU": ["Paraphrase: CageNeRF enables the deformation of neural radiance fields for any surface or object unlike previous methods that were limited to specific categories. This deformation is achieved using an invertible deformation parametrization based on deformation cages. CageNeRF extends the work of Neural Cages to neural radiance fields which is challenging due to their volumetric and nondiscretized nature. The method supports deformation transfer reenactment and animation where a deformation cage is applied to a neural radiance field with deformations derived from the same sequence (in the case of reenactment) or a different sequence (for other applications). Strengths: Allows for editing of neural radiance fields on any surface or object. CageNeRFs parametrization effectively handles aliasing issues. Compares favorably to AniNeRF for humanspecific reenactment despite being a general method. Wellwritten in clear English. Weaknesses: Method section lacks details in certain areas. Deformation transfer demonstrations are qualitative and limited to a few ShapeNet chair instances raising concerns about its effectiveness on other categories. Quantitative results are based on a small sample of frame indices leaving room for cherrypicking. The deformed cage is determined by a neural network potentially limiting generalization to unseen deformed poses. Questions: Why is a comparison to facespecific NeRFs not necessary What is the speed of CageNeRF and how does it compare to using NeRF directly How does grid sampling interact with volume rendering How is the loss function defined for neural network optimization of the cage deformer Which parts of Neural Cage are used in CageNeRF and how are they applied What is the loss function for pretraining and during deformation transfer Typographical and minor issues should be addressed for revision.", "Paraphrase: Summary: The paper introduces a technique for deforming Neural Radiance Fields (NeRFs) using a \"cage\" deformation system. This involves training a poseoptimized \"cage\" deformer and then using a new pose to project the cage onto a solid geometry. A dense meanvalue coordinate field is calculated to generate a new visualization. The method is tested on both articulated (e.g. humanoids) and nonarticulated (e.g. chairs) objects. Strengths: The approach is innovative addressing the challenge of editing NeRFs. It can be used in various scenarios including both rigid and nonrigid objects and produces promising results. The method is thoroughly explained and justified with ablation studies to validate its design. Weaknesses: Some visual artifacts (e.g. holes under raised objects) are observed which may be due to the deformation paradigm or limitations of the NeRF representation. The method relies on an explicit geometry which limits the resolution power that neural representations typically offer. The animations presented are relatively simple and its unclear whether the method can handle more complex movements with numerous intersections. Minor Issues: Line 174: Comma missing after \"o is the camera origin.\" Line 291: Typo (\"Impact\" should be \"Impact on\") After Rebuttal: Based on the authors responses and the ensuing discussion the initial rating is maintained recommending acceptance. In the final version it is suggested that the methods limitations and tradeoffs as discussed in the authorreviewer exchange be incorporated into the main manuscript. Questions: What causes the hole artifacts in the results Why is the method not tested on more complex movements Are there any limitations in handling such movements", "Paraphrased Statement: Summary: This paper combines NeuralCages with NeRF to enable the deformation of implicit fields. It uses NeRF to obtain surface points creates a mesh cage around them and then deforms the cage using NeuralCages. All points enclosed by the deformed cage can be deformed accordingly. However the papers presentation is complex and the experimental results are underwhelming. Strengths: The idea of combining NeRF and NeuralCages is intriguing. The technical concept is sound. Weaknesses: 1. Presentation Issues: Terms and symbols are confusing. Notation is inconsistent and misleading (e.g. geometry can refer to points or meshes). Key concepts are not explained clearly. Mathematical equations are presented poorly. 2. Missing Information: Details of network architecture loss functions and training are absent. 3. Aliasing Issue: The paper briefly mentions the aliasing issue but lacks explanation and motivation for the proposed solution. 4. Synthesized Object Animation: Its unclear why CageNeRF is needed if animated meshes are already available. The methods operation remains ambiguous. 5. Pose Reenactment: The need for CageNeRF is unclear if meshes of humanoid objects are available. NeuralCages could be directly applied instead. 6. Experimental Results: Results are presented for only one shape which limits their significance. The improvements achieved are not substantial."], "sMezXGG5So": ["Paraphrased Statement: Summary: This research enables efficient message passing on graphs bypassing the N2 time complexity issue. It leverages \"kernelized\" operators and employs random features to approximate the kernel function. The effectiveness of this method is demonstrated through experimentation on nodelevel tasks. Strengths and Weaknesses: Strengths: Using random features to estimate the kernel function is a notable innovation that lowers complexity. The papers structure and presentation are commendable for clarity. Weaknesses: More extensive benchmark comparisons are needed to validate the accuracy of the random feature approximation. While the experiments provide some initial evidence they rely heavily on settings defined by the paper itself which may limit the generalization of the findings. In particular the evaluation on citation networks exhibits inconsistencies with established benchmarks possibly suggesting issues with baseline results. PostRebuttal: After the authors response the original concerns have been addressed. Question: Can we consider both observed edges and nonobserved relationships in the edge loss calculation (Eq. 10) If only observed edges are considered the loss may result in high values for all \u03c0uv.", "Paraphrased Statement: This study introduces a novel method for learning the topology of input graphs to address issues with graph neural networks (GNNs) such as oversquashing heterophily and longrange dependency edge incompleteness. The proposed structure incorporates learned topology into the attention matrix via the Softmax Gumbel operator. To improve efficiency a kernelized version of attention computation using random feature decomposition is developed along with its convergence properties. Extensive experiments demonstrate the methods performance on various node classification tasks. Strengths: 1. Comprehensive theoretical analysis including convergence results for the Softmax Gumbel approximation. 2. Motivating variational perspective. 3. Extensive numerical experiments encompassing small and large graphs transductive and inductive tasks and graphenhanced applications. Weaknesses: 1. The paper claims to address issues like oversquashing and heterophily but does not explicitly explain how the proposed method achieves this. 2. Despite being proposed for scalable GNNs the paper lacks a detailed comparison with SOTA scalable GNN methods in the related works section. Questions: 1. In Equation (12) the summation terms appear to be more complex than \\sumw 1N \\phi(\\mathbfkw). Please clarify this. 2. The error bound for Softmax Gumbel RF decomposition suggests exponential growth with the length of embedded vectors. Have the authors evaluated the approximation gap in experiments and how many random features are typically required", "Summary Researchers developed an innovative Graph Neural Network (GNN) called ALIGNS capable of inferring hidden connections and missing labels for nodes within graphs. They employ an approximation method to enhance the training process by converting the softmax function used in latent node structures into kernel form followed by relaxing the categorical distribution over edge pairs to permit efficient backpropagation. Evaluations across several datasets with up to 2 million nodes demonstrated that ALIGNS outperforms current benchmarks both in transductive and inductive settings while also handling noisy or absent graph data. Strengths Clear and readerfriendly presentation Method is applicable to a diverse range of graphstructured input Impressive scalability with increasing node count Kernelization and relaxation techniques perform effectively in realworld applications Competitive results with stateoftheart models in both homophily and heterophily graph transductive settings Stable performance even with edge noise in inductive settings Weaknesses Typo in line 331 (ELBO instead of EBLO) Inconsistent equation notation (Eq. 8 in the text vs. Algorithm 1) Fixed temperature parameter (tau) for all datasets despite varying feature size and graph structures Questions Whether the approximation method could be extended to GAT and other attentionbased methods and the advantages of using all pairwise approximations over existing structures The potential impact of using kNN in graphenhanced applications The need for observed adjacency when the current layers latent structure could potentially serve as the adjacency matrix The lack of dimension changes during feature transformation as denoted in Eq. 8", "Paraphrased Statement: This paper presents a new messagepassing approach that uses a kernelized GumbelSoftmax operator. This operator reduces the complexity of message passing from quadratic to linear time. The authors prove that the approach introduces some approximation error and present empirical comparisons that show it performs better than existing methods. Strengths: The paper is wellwritten and organized. Efficient graph learning is a significant issue in machine learning. Theoretical analysis proves the approximation error. The approach combines existing techniques in a unique way. Empirical results demonstrate the improved effectiveness of the approach. Visualizations and sensitivity analysis of key variables are provided. Weaknesses: The approach requires a parameter that needs to be set in advance. The authors suggest exploring ways to learn or automatically set this parameter based on the theoretical analysis."], "v1bxRZJ9c8V": ["Summary This paper uses Bayesian methods to model dynamic systems using Gaussian processes incorporating message passing to simulate the dynamics. Strengths Clear presentation Logical combination of established approaches Weaknesses Limited innovation (minor modification of GPODE) Minimal significance (INODE outperforms calibration is a minor improvement) Inefficient data presentation (Tables 2 and 3 should be merged) Exaggeration in abstract (\"first time uncertaintyaware modeling of interacting object dynamics\" see Xu et al. for a counterexample) Typo in Equation 6: p(Y1N\\cdot) Questions Can the paper demonstrate that the learned functions (fsfb) accurately represent the dynamics (If interactions are disabled does fb alone reproduce the correct dynamics)", "Paraphrased Statement: Summary The researchers have developed a unique approach to model the complex dynamics of interacting objects that accounts for uncertainty. They employ a latent Gaussian Process Ordinary Differential Equations (GPODEs) model that explicitly incorporates interaction terms into the differential equations and leverages inducing points for efficient variational inference. Their method demonstrates superior performance against existing techniques in capturing interactions and quantifying uncertainty. Strengths and Weaknesses This paper is highly significant as it introduces the first uncertaintyaware modeling framework for continuoustime dynamics of interacting objects. It makes a valuable contribution to latent variable modeling. The paper is wellwritten and the results effectively support the authors claims. Questions Table 4 and 5: Why does IGPODEL underperform (IGPODELS IGPODE IGPODEL) Figure 2 and Appendix Figure 5: Why do the shaded areas deviate from the inferred trajectory Citation and Figure Links: It seems that the links provided for citations and figures are not clickable. Is this intentional Line 123: The section number is missing in \"Finally please see Section for an investigation\". Equation 6 and Appendix: Consider citing [Learning interpretable continuoustime models of latent stochastic dynamical systems Duncker et al. ICML 2019] as it is the seminal work introducing GPSDEs and predates [Scalable Inference in SDEs by Direct Matching of the Fokker\u2013Planck\u2013Kolmogorov Equation Solin et al. NeurIPS 2021] in terms of inference algorithm. Appendix Tables: It would be helpful to highlight the best results in bold as in the main paper and improve their readability.", "Paraphrased Summary: The authors present IGPODE a model that simulates interacting objects over time while capturing uncertainty. IGPODE breaks down the movements and interactions of individual objects making it more efficient to handle complex dynamics. The model combines advanced techniques for solving differential equations and sampling uncertainties. Strengths: IGPODE offers a unique approach to modeling uncertainty in interacting objects. IGPODE outperforms rival models in uncertainty quantification as evidenced by lower error measures. The paper provides a comprehensive overview of the research context and clearly explains the model and its implementation. Weaknesses: The paper does not fully explore how the number of objects affects performance and computational feasibility. Questions: How do performance and computation change as the number of objects increases What is the limit of objects that can be modeled effectively Results with a large number of objects would enhance the papers findings.", "Paraphrased Statement: This study presents a technique for inferring the behavior of interacting objects while accounting for uncertainties. It employs a Gaussian processbased ordinary differential equation method and models the dynamics of interacting variables as a combination of isolated and interacting components. The method can handle cases with partial system observations. The authors demonstrate its usefulness with datasets on bouncing balls and electric charges. Strengths: The paper is wellwritten and explains the method and its relationship to existing techniques clearly. The proposed method extends the GPODE method to incorporate constraints for interactions and independent variables. Weaknesses: The method lacks significant originality as it heavily relies on the GPODE method. Despite showing improved mean squared error (MSE) and evidence lower bound (ELL) compared to GPODE the methods accuracy falls short of the GDE (INODE) method in MSE. The ELL improvement with IGPODE over GPODE is unclear given the methods primarily structural changes. Questions: Include figures comparing GPODE and IGPODE performance as in Figure 2. Specify the performance of IGPODE on both three interacting objects and individual objects in Section 5.2.3. Demonstrate the methods application in a realworld setting for further evaluation."], "ripJhpwlA2v": ["Paraphrased Statement: Summary This study suggests and evaluates conditions for group invariant learning methods aiming to prevent classifiers from relying on irrelevant features. Based on these conditions the authors present a novel group learning approach involving two steps: constructing group structures to meet these conditions and reweighting training samples based on group information. Strengths and Weaknesses This paper examines a crucial issue related to the efficacy of group invariant learning when irrelevant features are present. The proposed method is tested against a range of methods and datasets showing potential for improvement. However technical issues and presentation drawbacks hinder readers from fully grasping the results. Specifically Proposition 4.5 lacks clarity regarding the equation used and the definition of \"r.\" Similarly line 260 omits the definition of \"\\mathbbG.\" The use of conditional probability distributions requires clarification as it appears to refer to a specific event rather than a group label. Furthermore the paper does not fully establish the connection between the proposed criteria and the final conclusion leaving readers uncertain about the specific requirements for the random mapping \"G\" in terms of irrelevant features. Due to the central role of the probability model framework rigorous and seamless arguments are necessary. An ablation study would clarify the significance of the proposed criteria and the impact of varying inputs on the results including group assignments and objective weights.", "Paraphrased Statement: Summary: The authors assert that current group invariant learning techniques are inadequate in mitigating the reliance of classifiers on irrelevant links in training data. They introduce two criteria for evaluating the adequacy of such features. They then demonstrate the shortcomings of existing approaches based on these criteria. To address these limitations they propose a novel group invariant learning method that involves group construction and reweighting to meet the specified criteria. Empirical findings show the proposed methods superiority. Strengths: The two criteria developed by the authors effectively assess the adequacy of group invariant learning methods. The manuscript is wellwritten and coherent. The analysis highlighting the limitations of existing approaches is sound and the proposed model is effectively constructed based on this understanding. Weaknesses: Compared to the indepth analysis in earlier sections the proposed model in Section 5 lacks significant technical novelty. The experimental validation is somewhat limited in terms of dataset diversity. It would be beneficial to compare the proposed method to Group DRO or its variants. The experimental analysis is lacking in detail as it primarily presents quantitative results. Questions: 1. Why did the research refrain from using datasets like CelebA or CivilComments which are relevant to realworld scenarios 2. Explain why Group DRO or its variants were not included in the comparisons. 3. Is the proposed model susceptible to the value of the hyperparameter \u03bb in Equation (3) Would it be valuable to investigate the interplay between the \u03bb value and the authors two criteria", "Paraphrase The study introduced a learning technique called group invariant learning (groupIL) which divides the training data into groups based on factors unrelated to the target task. The authors investigated and clarified the conditions under which groupIL can fail to detect spurious correlations. They also found that many common datasets do not meet these conditions. Based on these findings they developed new methods to overcome these limitations. Experiments with image and natural language datasets demonstrated the effectiveness of the proposed model. Strengths The motivation and problem definition are wellreasoned and supported by literature. The analysis of the failure conditions is thorough and theoretically sound. The proposed model is applicable to multiple invariant learning methods. The experiments demonstrate the models effectiveness. The writing is clear and concise. Weaknesses The model relies on reference models trained using empirical risk minimization (ERM). ERM may not fully capture spurious correlations. Questions It would be helpful to include descriptive captions on the figures instead of referencing them by number. It is unclear if the h used in Criterion 4.1 is the same as the h representing the classifier in Line 131. How does the inference time of the model compare to baseline methods How does the potential ambiguity in spurious correlations obtained from ERM affect the models effectiveness Typos Line 72: \"Classification\" should be \"classification\" Line 207: \"))\" should be \")\""], "q2nJyb3cvR9": ["Summary Researchers have developed a new algorithm for reinforcement learning that uses randomized exploration. This algorithm guarantees a nearoptimal level of regret which is a measure of how much performance is lost compared to an ideal strategy. It is the first algorithm of its kind to achieve this result without using the principle of \"Optimism in the Face of Uncertainty.\" Key features of the algorithm include: Using a single scaled Gaussian noise for all transitions A specific shape of noise that ensures optimality A novel clipping procedure that clips noise values Strengths The algorithm is the first based on randomized exploration to achieve minimax optimal regret. Its novel proof technique allows for precise control of errors caused by pessimism. Weaknesses The algorithm lacks empirical validation. Questions An empirical comparison with other baseline algorithms would be valuable to demonstrate the effectiveness of the algorithm. Some minor textual issues in the appendix could be addressed.", "Paraphrased Summary: This paper explores techniques for minimizing regret in reinforcement learning (RL) with randomized algorithms. The authors introduce a novel singleseed algorithm that achieves a regret bound of approximately O(H\u221aST) where H is the horizon S is the state space A is the action space and T is the time step. The algorithm leverages Bernstein concentrationbased variance scaling. Strengths: The algorithms simplicity and singleseed approach is a significant strength. The analysis based on absolute difference between true and sampled value function policies provides insights that can benefit future research on sampled Markov Decision Processes (MDPs). Weaknesses: The additive term O(H4S2A) in the regret bound is substantial. The paper does not reference related work on Thompson Sampling and worstcase regret bounds for posterior sampling algorithms. Question: How does the performance of the proposed SingleSeed Regret (SSR) algorithm compare to the Posterior Sampling Regret Lower Bound (PSRL) algorithm presented in [56]", "Paraphrased Statement: Summary: The study focuses on a finitestate Markov decision process (MDP) with timevarying transitions and costs. A novel singleseed randomization algorithm is proposed that aims to achieve a regret bound of O(H\u221aSAT) within this MDP setting. Strengths: Addresses a gap in finite MDP reinforcement learning by proposing an alternative to UCB algorithms for randomized exploration. Weaknesses: The problem setting may be artificial with a repetitive sequence of transitions and rewards over multiple trajectories. The algorithm and analysis lack substantial novelty as similar concepts have been employed in previous studies (e.g. clipping Bernstein noise). Questions: Steps are missing in Algorithm 1 regarding trajectory generation. The analysis suggests that Bernstein noise rather than randomized exploration contributes significantly to the regret reduction. How is the claim in line 225 justified Information on expected regret and numerical results is absent. Simulations could provide insights into empirical performance and expected regret. The analysis does not address the behavior on sample paths with low probability (i.e. the complement of the set with high probability).", "Paraphrase: This study explores reinforcement learning in tabular Markov decision processes with the objective of minimizing regret. The proposed algorithm SSR employs randomized value functions for exploration and incorporates two noise variants. The Bernsteinbased variant achieves a worstcase regret bound (with high probability) that approaches the theoretical lower bound within logarithmic factors surpassing previous regret bounds for randomized exploration by a significant margin. Strengths: Addresses a classic problem in reinforcement learning. Explores the practical relevance of exploring via randomized value functions given its empirical success. Introduces novel techniques such as using a single randomization seed and a new clipping strategy. WeaknessesQuestions: The regret bound for the Bernsteinbased variant requires a large time threshold (T) while the other variant does not. The critical time value where the desired regret regime takes hold is significant for both variants. The statement that UCBtype algorithms are challenging to implement is not entirely accurate for modelfree variants. An empirical comparison of SSR against stateoftheart methods would provide insights into its practical performance. Minor Comments: Specify the constant probability for optimismpessimism in technical sections. Correct typos and grammatical errors."], "w5DacXWzQ-Q": ["Paraphrased Statement: Summary: This proposed method SAViT is designed specifically for transformer architectures. It simultaneously reduces parameters in multiple components by considering the relationships between them. This approach improves the efficiency and maintains the performance of the model. Experiments demonstrate the effectiveness of SAViT across different ViT architectures and computer vision tasks. Strengths: Novel concept of jointly pruning various model components. Significant performance improvements compared to existing pruning methods. Wellwritten and accessible paper. Weaknesses: Limited information on computational requirements and data consumption during the pruning process. Ablation study on the interactions between components (prominent characteristic of SAViT) needs clarification. Specifically it is unclear if the \"without secondorder interactions\" setting in the ablation study involves removing all Hessianbased terms or only the crosscomponent terms.", "Paraphrased Statement: Summary: This paper proposes a novel pruning technique for visual transformer networks. The technique involves pruning multiple components within the network including selfattention neurons and embedding neurons and is applicable to various vision transformer models like ViT and Swin Transformer. Experimental results demonstrate the efficiency and accuracy of the technique compared to existing pruning methods. Strengths and Weaknesses: Strengths: Welljustified motivation and theoretical underpinnings for the pruning technique. Clear and accessible writing style. Extensive empirical evaluation demonstrating the techniques effectiveness in multiple scenarios. Weaknesses: Concerns regarding the interpretation of accuracy gains observed when pruning DeiTBase. Inclusion of the core pruning algorithm pseudocode in the appendix rather than the main paper. Questions: No questions were identified.", "Paraphrased Statement: Summary: This paper introduces a method to simplify vision transformer models by simultaneously considering multiple dimensions. A new \"collaborative pruning\" technique is developed to address the challenge of optimizing these dimensions together. The results show that the method is effective across various network structures (DeiT and Swin) and tasks (ImageNet classification and COCO detection). Strengths: The approach of jointly optimizing multiple dimensions is logical and wellsupported. The method is shown to perform well on various models and tasks. Weaknesses: Joint optimization of vision transformers is not entirely novel many existing studies have explored this for network acceleration. For a more comprehensive comparison Table 3 should include results for various model sizes (e.g. ViTBST) as ViTDeiTBS is a commonly used benchmark. While Table 6 suggests that pruning multiple dimensions may not always lead to optimal speedup on GPUs the method introduces a new pruning algorithm that might be difficult to implement. The lack of publicly available code raises concerns about the methods reproducibility. Additional Comments: The paper provides a comprehensive analysis of efficient vision transformers. The method is evaluated on multiple backbones and datasets. However the joint optimization framework resembles that of previous methods for CNNs limiting its novelty. Questions remain about the adequacy of experiment results GPU speedup and reproducibility. Addressing these weaknesses would significantly enhance the papers value.", "Paraphrased Statement: This study presents a comprehensive pruning approach for Vision Transformers (ViTs) that considers the interdependency of their components. Unlike Convolutional Neural Networks (CNNs) with homogeneous components ViTs have heterogeneous components. Therefore the study develops a Taylorbased optimization function to leverage these interactions. To avoid the computational burden of the Hessian matrix an approximation is derived to convert it into pruning ratios. The optimization aims to optimize the tradeoff between accuracy and computational cost. The approach is evaluated on ImageNet using DeiT and Swin models and also in detection experiments. Strengths: 1. Clarity and readability 2. Clear motivation Weaknesses: 1. Discrepancy in the definition of \u0394w and its impact on the optimization target (eq. 1). \u0394w should be defined as \u0394w b \u2299 w w leading to a correction in the target function. 2. Questioning the derivation of eq. 2 from eq. 1 based on the definition of \u0394w. 3. Lack of experimental results on Swin models pretrained on ImageNet22k despite their superior performance compared to training directly on ImageNet1k. The authors should provide results using pretrained weights on ImageNet22k followed by pruning and finetuning on ImageNet1k. Questions: The studys motivation is sound and the approach has potential. However the definition of \u0394w and its implications for the optimization target raise concerns. The derivation of eq. 2 from eq. 1 also requires further clarification. Additionally the authors should consider providing results on Swin models pretrained on ImageNet22k."], "sYDX_OxNNjh": ["Summary This paper introduces the ReST algorithm for unsupervised discovery of skills in a sequential manner. This approach aims to address the exploration degradation problem where laterlearned skills fail to explore new areas due to competition with previously discovered skills. ReST demonstrates its effectiveness in various tasks. Strengths Proposes a novel problem and exploration degradation issue in unsupervised skill discovery which is a significant research area. Provides clear motivations and convincing toy examples to illustrate the effectiveness of sequential skill learning. The videos of discovered skills demonstrate ReSTs ability to learn various skills efficiently. Weaknesses The provided videos are not compared to other baselines making it difficult to gauge the advantages of ReST. The recurrent algorithm used in ReST may result in longer training times potentially disadvantaging comparisons with baselines. Quantitative results are limited especially for larger and more realistic scenarios. The writing quality could be improved and certain sections are difficult to follow. The code is not provided which may affect the reproducibility of the research. A broader impact section is missing as is standard for Neurips submissions. Suggestions for Improvement Increase the font size and improve the clarity of figures for publication quality. Provide a simpler diagram to illustrate the ReST algorithm. Eliminate redundant equations and focus on the essential information bound. Clearly define N and M in Algorithm 1. Organize and simplify the descriptions of network functions and why two networks are used. Provide ablation studies to support Equation 10. Questions for the Researchers Can exploration degradation be avoided without sacrificing sample efficiency How does the forced exploration of novel states impact the discovery of existing good states Can learned skills enhance hierarchical reinforcement learning Will the code be made available for the research community", "Paraphrase Ideal Unseen Ability Detection Scenario: Acquire a diverse set of abilities that explore a wide range of environmental states. This paper proposes optimizing two factors: Reward exploration based on the frequency of state visits. Train abilities iteratively ensuring each is distinct from others. Problem with Previous Unsupervised Skill Discovery: Abilities tend to avoid traversing \"bottleneck\" states because of low intrinsic rewards. This problem known as \"exploration degradation\" stems from simultaneous training of multiple abilities. Solution Proposed in this Paper: Train abilities one by one instead of simultaneously. This iterative training approach helps prevent the \"exploration degradation\" problem. Empirical Findings: The proposed method generates diverse and identifiable abilities that span a wider range of states. In contrast prior methods show limited state coverage. Strengths: Novel iterative skill learning approach in unsupervised skill discovery. Clear motivation approach and experiments. Welldesigned and explained qualitative and quantitative results. Weaknesses: Lack of comparison to other methods that aim for wide state coverage using continuous latent skills. Need for more insight into the impact of each proposed component. Scalability to continuous skills may not be straightforward. Questions Will the proposed reward signal become less informative as RNDs become more accurate Is there any randomness in the environment or skills Minor Suggestions Replace \"recurrently\" with \"iteratively.\" Add an intuition for the statement \"parallel training procedure inherently discourages exploration.\" Explain Figure 1 more clearly to enhance understanding of the motivation.", "Paraphrase: Joint training of skilllearning algorithms can hinder acquisition of specific skills due to unexplored states. To address this researchers propose alternating skill training where skills are trained sequentially for fixed intervals. Additionally adjusting the reward based on state coverage promotes diversification among skills. Empirical evaluations across 2D navigation and robot locomotion tasks demonstrate enhanced state coverage and divergence compared to existing methods. Strengths: Identifies a significant limitation in existing skill discovery methods. Offers a solution that addresses the issue. Clear and wellarticulated paper with thorough experiments. Weaknesses: Proposed method has reduced sample efficiency compared to baselines. Does not support continuous latent representations. Does not mitigate catastrophic forgetting of skills as the number of skills increases. Requires multiple neural networks for each skill. PostRebuttal: Despite its advantages the proposed method faces several drawbacks: Higher sample and computational complexity than baselines. Inability to handle continuous latents. Increased complexity with multiple networks for skills and novelty detection. Questions: How to determine the appropriate training duration for each skill Given the remaining weaknesses is the work still considered significant"], "xILbvAsHEV": ["Paraphrase: This study presents novel \"noregret learning\" strategies for extensiveform games with improved efficiency in both fully observable and adversarial feedback scenarios. The regret bound in the fully observable case is comparable to recent advancements by Song et al. In the adversarial feedback case the strategy achieves stateoftheart results even against adversarial opponents. The proposed algorithm converts an extensiveform game into a normalform game and utilizes the Phihedge algorithm for selecting trigger deviations. The key challenge is overcoming the exponential complexity caused by the normalform conversion. In this regard the study provides a positive result that complements the recent findings by Farina et al. on the kernelizability of OMWU (KOMWU) learning dynamics. Additionally the authors demonstrate that both KOMWU and their algorithm can be implemented in extensiveform games as specific instances of OMD with dilated entropy regularizers. The papers strengths include its originality technical rigor and the unexpected equivalence between KOMWU and OMDdilated regularizers. Its significance lies in its wellmotivated results and potential for impact within the learningingames community.", "Paraphrase: This paper focuses on finding an Equilibrium with Feasible Correlated Strategies (EFCE) in extensiveform games. It introduces an efficient implementation of \\PhiHedge that minimizes trigger regret. Furthermore the paper demonstrates that this algorithm is equivalent to a framework called EFCEOMD with dilated regularizers. Regret upper bounds are established for EFCEOMD in both full feedback and bandit feedback scenarios. These bounds can be refined using a modified Balanced EFCEOMD algorithm matching the lowest bound theoretically possible. Strengths and Weaknesses: The paper is clear and technically sound making significant contributions to understanding how to compute correlated equilibria in extensiveform games. Question: No questions were raised.", "Paraphrased Statement: Summary: This study focuses on finding an efficient algorithm for minimizing regret in extensiveform games (EFGs). Initially the authors examine minimizing regret in the normalform representation of EFGs which can grow exponentially in size with the games size. In this setting they present a computationally efficient version of the PhiHedge algorithm that can approximate equilibria such as the extensiveform correlated equilibrium (EFCE) under a specific definition of the Phi set. The authors then demonstrate the equivalence of their PhiHedge variant to followtheregularizedleader (FTRL) and online mirror descent (OMD) using appropriately defined dilated entropy and divergence measures. This equivalence allows them to obtain nearoptimal regret bounds for both full feedback and bandit feedback scenarios (with highprobability regret bounds in the latter case). Strengths: The paper significantly contributes to regret minimization in games. The paper is wellwritten and clear exhibiting exceptional presentation quality. The theoretical claims are wellfounded novel and nontrivial. The literature review is comprehensive and accurate. Weaknesses: As the paper focuses on an efficient algorithm experimental evaluation comparing their results to existing EFCE computation methods would have been beneficial. However this is a minor concern as the papers primary focus is theoretical."], "nX-gReQ0OT": ["Paraphrased Summary: This study introduces a new deep learning architecture for calculating the ground state energies of multielectron systems. The model combines aspects of PauliNet and FermiNet along with enhancements in input feature embedding and parameter initialization. Experiments demonstrate that the proposed method significantly reduces errors up to 4070 with a 48fold decrease in computational cost. Additionally a new benchmark is established for various deep learning and classical methods across different atoms and molecules. Analysis suggests that incorporating excessive physical knowledge into the model can hinder accuracy. Strengths: Original and novel method combining PauliNet and FermiNet with improvements. Comprehensive experimental validation proving accuracy and efficiency gains. Insightful analysis identifying that physical knowledge can limit optimization. Wellwritten and clear presentation. Achieves stateoftheart results for solving the electronic Schr\u00f6dinger equation. Weaknesses: Lacks theoretical explanation for the accuracy improvements under reduced computational cost. The discussion in Section 4 focuses primarily on experimental observations and theoretical justification for why the proposed method outperforms existing approaches would be valuable.", "Paraphrased Statement This paper aims to enhance the variational quantum Monte Carlo method for estimating molecular ground state energy. The authors explore various factors influencing model performance including: Tuning hyperparameters in the FermiNet and PauliNet models Selecting the envelope function and pretraining strategies Introducing a SchNetlike embedding and input feature transformation for improved expressiveness These design choices are empirically evaluated on molecules with up to 40 electrons resulting in a model that outperforms existing methods in both accuracy and speed. Ablation studies indicate that the most significant improvements come from hyperparameter tuning and the SchNetlike embedding while the feature transformation has a relatively minor impact. Strengths: Clear and concise writing Strong experimental results Extensive and insightful ablation studies Fusion of FermiNet and PauliNet approaches for potential further improvements Weaknesses: Limited novelty as the paper integrates existing methods Absence of direct comparisons and references to support claims of energy improvements and computational cost reductions Concerns about the accuracy of the QMC results especially for larger molecules Questions: How certain are we of the correctness of the QMC results given the possibility of sampling errors Under what conditions was the claimed \"4070 lower energy error at 8x lower computational cost\" achieved How is convergence time defined in the original FermiNet paper", "Paraphrase: Summary: The authors propose and evaluate enhancements to variational Monte Carlo methods using deep learning resulting in increased accuracy and reduced computational time. Strengths: Clear and concise writing Accessible introduction for nonexperts Explicit statement of key contributions Comparison to prior methods (including PauliNet) Welldesigned experiments Strong experimental outcomes Weaknesses: Limited immediate relevance to the broader ML community Questions: Clarify units in the equation before Eq. (1) Provide a citation for the RayleighRitz principle (line 57) Explain how H\u03c8(r) is calculated Distinguish between indices j and k in Eq. (2) Provide the exact form of Eq. (1) assuming \u03bb is a matrix of size nel x nel Clarify the k used below Eq. (3) in relation to the previous k Expand on the challenges faced in Section 2.2 compared to for example ML force fields Include information on the overall computational cost of each method", "Paraphrased Statement: A new method has been developed to simplify solving the Schrodinger equation by combining concepts from previous studies (PauliNet and FermiNet). Through various experiments with the removal of different components (ablation studies) the method has shown promising results. Strengths: Positive outcomes Comprehensive analysis and integration of previous approaches Ablation studies Realistic assessment of capabilities Thoughtful consideration of limitations Weaknesses: Unclear description of the method and ambiguous notation Potential argument that its mainly a combination of existing work Questions: Would studying transitional states in the butadiene system (as in the PauliNet paper) provide insights into this methods performance"], "y-E1htoQl-n": ["Paraphrased Statement: This paper introduces a new training algorithm for deep reinforcement learning agents that enhances their resistance to input disturbances. It utilizes guaranteed limits to offer allencompassing defense while incorporating a learned worstcase value function to encourage agents to adopt safe strategies. Evaluations on MuJoCo and Atari environments demonstrate that this algorithm matches or surpasses current robust training methods particularly against more potent attacks. Strengths: Innovative and promising ideas particularly the use of a learned worstcase critic and state importance weights. Efficient and effective solution for defending against adversaries with extended objectives through worstcase critic and policy optimization. Thorough empirical analysis showing strong results against powerful attacks. Clear presentation and comprehensive discussion of related work. Weaknesses: Some perplexing design decisions (see questions below). Missed connection to related work: Worst Attack Value is mathematically akin to the Greedy Worst Case reward evaluation metric. While the distinction is not critical it should be acknowledged. Uncertainties with mathematical notation: Using convex relaxation to solve for Aadv technically solves for a lower bound on the worstcase Bellman operator. This should be clarified in the main text. Evaluation could be strengthened by including more realistic scenarios such as realworld robotics. Questions: Is the attack in Figure 5 applied throughout training or only during \"test time\" checkpoint evaluation Why is a distinct clean policy necessary for WocaRDQN Appendix: Is it standard practice to select results from mediumperforming agents due to high variance Errors: Line 256: Lreg should be Lwst in Equation (4). Figure 5: Similar colors for SAPPO and RADIALPPO make them difficult to distinguish. Appendix Table 2: Bold RADIALPPO instead of SAPPO in the first two columns for Walker.", "Paraphrased Summary: This paper presents a robust reinforcement learning (RL) training framework designed to withstand adversarial attacks that alter the state. Unlike previous methods that involve training an additional RL adversary the proposed approach directly estimates the worstcase Q values without extra training. The framework introduces a new Bellman operator called the \"WorstAttack Bellman Operator\" which calculates the worstcase Q values by minimizing the Bellman error. Based on the worstcase Qfunction a policy is optimized to maximize the Qfunction using standard RL algorithms like PPO or DQN. Finally the framework includes a policy regularization technique that selectively minimizes action changes under adversarial perturbations based on the state importance weight. Experiments in MuJoCo and Atari environments demonstrate the effectiveness of the proposed method. Strengths: Achieves stateoftheart performance significantly. Introduces a novel Bellman operator with welldefined properties. Easytofollow and wellorganized presentation. Simple and effective framework compatible with existing RL algorithms. Weaknesses: Estimating WorstAttack Value: The minimization problem for the target of the Bellman error can be computationally expensive when the action space is continuous. Convex Relaxation Technique: The scalability of the convex relaxation technique to highdimensional input spaces (e.g. Atari) is unclear.", "Summary Paraphrase: The paper presents an improved training technique for reinforcement learning (RL) that handles uncertainty. It utilizes a \"worstcase\" Bellman operator to determine the worstpossible expected return under potential attacks. This value function can then be used to find optimal policies using either valuebased or policy gradient methods. The authors introduce a weighting scheme to prioritize the robustness of the neural network in critical states. Strengths and Weaknesses Paraphrase: The paper contributes a key innovation in RL robustness training. The novel approach to finding robust policies directly using a valuebased method without an explicit attacker module is a major improvement. The authors thoroughly evaluate their algorithm against previous work and showcase its adaptability to various existing RL algorithms. The paper is wellwritten and has a significant impact. However the paper lacks theoretical insights into the new operators dynamics particularly when the policy is being updated. The combination of robust values with natural values through the parameter \u03bawst raises questions about its necessity and impact on convergence. A broader discussion of design choices related to the scheduling of \u03bawst and its sensitivity would enhance the papers comprehensiveness. PostRebuttal Questions: Is \u03bawst essential for the convergence of algorithms like WorcADQN Can \u03bawst be set to 0 without impacting the algorithms performance How sensitive is the algorithm to the schedule of \u03bawst", "Paraphrased Statement: This study presents a robust training approach for reinforcement learning (RL) that directly aims to optimize the worstcase reward of a policy under potentially malicious attacks. The proposed framework achieves stateoftheart performance under severe attacks. Strengths: Addresses a crucial issue in robust RL. The method is clear and wellstructured. Some analytical insights such as the explanation for the methods robustness are intriguing. Weaknesses: Limited technical innovation. The worstcase Bellman operator resembles existing attack definitions. The worstcase value estimation relies on standard convex estimation which may be less valuable due to nonconvexity and assumptions in deep neural networkbased RL. Questions: Are there analytical proofs for the frameworks robustness such as lower bounds on robustness as a function of the convexity approximation While the frameworks explanation results were mentioned initially they were not evaluated in the experiments. Are there any noteworthy findings or observations on the explanation of this framework in various games"], "rnJzy8JnaX": ["Paraphrase: Summary: This research introduces ResKD a novel technique for maximizing video recognition efficiency. ResKD follows the knowledge distillation paradigm utilizing a highresolution teacher network and a lowresolution student network. To transfer knowledge ResKD calculates the Mean Squared Error (MSE) between the teacher networks feature and the upsampled student networks feature. Extensive experiments demonstrate the frameworks effectiveness. Strengths: ResKD is an innovative and straightforward concept for efficient video analysis. Thorough evaluations corroborate the frameworks efficacy. Weaknesses: ResKD employs a standard knowledge distillation framework lacking originality. Section 3 of the paper appears irrelevant to ResKD. Additional Notes: Reducing spatial resolution can compromise recognition performance due to excessive downsampling and decreased computation. Removing an early downsampling layer may not significantly reduce computation diminishing efficiency gains. Overall Assessment: The paper presents a sound empirical study but lacks significant novelty. Questions: Have alternative layers or multistage features been explored for ResKD What is the recognition performance for the specific scenario: Kinetics400 dataset SwinS teacher (224) and SwinS student (112)", "Paraphrased Statement: This study explored the reasons behind the performance drop when using lowresolution frames. It then introduced a technique called CrossResolution Knowledge Distillation (ResKD) to overcome this gap between the network and input size. The work is notable for its clear problem definition and wellwritten manuscript. However some experimental details lack specific descriptions. Additionally its unclear how the student network is trained for each different low resolution in Section 4.5.", "Paraphrase: Summary: This paper introduces crossresolution knowledge distillation (ResKD) for enhancing video recognition efficiency on lowresolution frames. The student network with a simpler architecture and lowerresolution inputs is guided by a teacher network with a more complex architecture and higherresolution inputs. ResKD transfers knowledge from the teacher to the student bridging the performance gap between efficient and large models. The study was conducted using various publicly available video recognition datasets and the findings show that ResKD achieves a desirable balance between performance and efficiency. Strengths: The paper is generally clear and understandable. ResKDs effectiveness is supported by the positive results obtained across multiple public datasets. Weaknesses: Novelty: The research follows the established practice of leveraging knowledge distillation for video recognition. The specific application of KD to distill knowledge between networks with varying input resolutions for video recognition has already been explored as in [34]. Thus the originality of this paper is limited. Video Data Insights: This paper focuses on video recognition but lacks specific insights into video data. ResKD approaches video recognition as a general lowresolution image classification problem neglecting the inherent temporal redundancy present in videos. Table 1: The table could be improved by including the depth information for ResNet in the \"Backbones\" column. Additionally ResKD is incorrectly marked as the best result in FCVID even though its performance is inferior to AdaFocus V2 which could be misleading. Throughput Comparison: The paper provides GFLOPs metrics in Table 4 but this is insufficient for assessing network efficiency. A direct comparison of throughput between ResKD and other methods would be valuable. Questions: The primary concerns raised here are about the papers novelty and its insights into video data."], "wu1Za9dY1GY": ["Paraphrase: Summary: This study establishes a link between Graph Neural Networks (GNNs) and dynamic programming (DP) through category theory particularly the integral transform. It demonstrates that both GNNs and DPs can be represented as polynomial span diagrams offering insights for enhancing GNN edge updates. Using this analysis the authors show that using thirdorder edge updates improves GNN performance on edgecentric problems in the CLRS Algorithmic Reasoning Benchmark. Strengths: Connects GNNs and DP deepening our understanding of GNNs. Uses polynomial span diagrams to bridge the connection between GNNs and DP. Demonstrates the practical application of this analysis through improved edge updates. Weaknesses: Requires a better explanation of the integral transform and polynomial span diagrams for noncategory theory experts. Experimental results are conducted on a smallscale GNN limiting the demonstration of the impact of thirdorder updates. Questions: Can the authors showcase the improvements of thirdorder updates on larger GNNs (e.g. 128dimensional embedding and 256dimensional hidden layer) How do the improved GNNs compare to stateoftheart models on the CLRS benchmark Can the thirdorder updates enhance the performance of existing stateoftheart models", "Paraphrased Statement: This paper presents a framework for understanding deep neural networks (DNNs) and graph neural networks (GNNs) through the lens of category theory and abstract algebra. By abstracting computations as diagrams the paper draws a connection between DNNs and GNNs. It then aligns GNNs with specific algorithm classes by aligning their diagrams. Experiments on six edgecentric tasks show the efficacy of this approach. Strengths: 1. The paper proposes a theoretical method to connect DNNs and GNNs using integral transforms. 2. It provides rigorous mathematical proofs to support its theoretical framework. 3. The paper clearly outlines the problem and presents a detailed discussion of the solution. Weaknesses: 1. The evaluation is limited to six algorithms with varying input dimensions from within and outside the defined domain. 2. The use of multilayer perceptrons (MLPs) for evaluation while GNNs are discussed in the paper lacks clarity. 3. The choice of indomain and outofdomain datasets for evaluation and the rationale behind it are not fully explained. Questions: 1. Why were twolayer MLPs with varying dimensions used in the evaluation despite the papers focus on GNNs 2. What is the reasoning behind choosing indomain and outofdomain datasets for evaluation", "Paraphrased Statement: The authors establish a formal connection between Graph Neural Networks (GNNs) and Dynamic Programming (DP) by representing their operations in a common form using integral transforms. This commonality enables the analysis of GNNs and DP within a unified framework. The authors exploit this connection to design GNN architectures that better align with the underlying DP algorithm resulting in improved empirical performance on benchmark datasets. Strengths and Weaknesses: Originality: The work connects GNNs and DP more formally than in previous research (e.g. Xu et al. 2019 ICLR). Quality: The introduction and background sections clearly articulate the motivation for the study. The representation of DP and GNN in a common framework is welldemonstrated. Weaknesses: The details of the polynomial span diagram for GNNs could be more explicit. The empirical results lack negative examples where the new GNN architecture does not outperform existing approaches. The authors should address the consistently low performance on some benchmarks and suggest strategies to improve it. The experiments setup information should be provided in the paper rather than confined to a checklist. Questions: 1. Provide more details on how the integral transform diagrams correspond to actual GNN operations. 2. Include negative examples or discuss scenarios where the new GNN architecture may not outperform existing approaches. 3. Explain the low performance on some benchmarks particularly for the Dijkstra problem and offer insights on potential improvements. 4. Consider revising the papers title to better reflect the experimental results which may not fully support the current claim that \"Graph Neural Networks are Dynamic Programmers.\" 5. Include experiments setup details in the paper to facilitate replication."], "pl279jU4GOu": ["Paraphrased Summary This study examines how gradient flow converges to zero loss during training and testing. It does this by establishing a Kurdyka\u0141ojasiewicz inequality using lower bounds on Rayleigh quotients. The authors demonstrate the efficacy of this technique for various machine learning problems. Strengths and Weaknesses The paper is wellwritten and easy to follow. However there are some weaknesses: Section 4.5 and its proofs as well as Proposition 4.2 are difficult to understand and verify. Proposition 4.6 may require slight modification and the conditions for its validity could be better specified. The discussion of the conditions for Propositions 4.5 and 4.6 is lacking. Contributions The paper highlights the significance of Proposition 3.1 for tracking the evolution of loss using the function \u03c6. It provides examples of \u03c6 when the PL condition is met and illustrates its use in multiclass logistic regression. The paper also employs techniques to decompose Rayleigh quotient bounds which are showcased in Proposition 4.1 for linear models with quadratic loss. Questions and Suggestions Proof of Proposition 4.6: The proof requires revision. \u03baisolated Condition: Clarify how and when this condition is satisfied. Seprating Ray Assumption: Cite existing work that assumes a \"nonzero\" separating ray. Convergence Result: Explain how the convergence of positive random variables in expectation implies a.e. convergence of a subsequence. Rate of Convergence: Discuss if Proposition 3.1 can be used to establish any convergence rate.", "Paraphrase: Summary: This research proposes a strategy to demonstrate the convergence of loss during training for losses of the form \\ell(F(\\theta)) where F represents neural network outputs for each data point and \\ell converts these outputs into a final loss value (e.g. MSE to target values). Previous analysis based on the Neural Tangent Kernel (NTK) assumed a lower bound on the smallest eigenvalue of the NTK matrix K DF \\cdot DF\\top and established the loss convergence by proving the PL condition under this assumption. This paper avoids this eigenvalue assumption and presents a proof strategy based on Rayleigh quotients of the NTK matrix and the KL condition. This strategy is applied to prove loss convergence bounds for linear regression and linear logistic regression with partial results for twolayer nets with squared loss. Strengths: The proposed proof strategy eliminates the need for a lower bound on the smallest NTK matrix eigenvalue enabling analysis when the number of data points exceeds the number of parameters. The strategy provides an alternative method for understanding loss convergence bounds for linear regression. Weaknesses: While extending loss convergence bounds beyond the overparameterized regime is promising this paper lacks concrete theorems for nonlinear neural nets with fewer parameters than data points. Some partial results in Sections 4.4 and 4.5 establish loss convergence under the assumption that the gradient flow remains close to the initial point which limits the impact of the results. The results on linear logistic regression are not adequately contextualized in relation to prior works. The paper claims a new loss convergence bound but it is unclear how it compares to existing bounds. The presentation is complex due to excessive notation that varies across settings. Questions: 1. Do the authors have examples demonstrating loss convergence bounds for nonlinear neural nets beyond the overparameterized regime without additional assumptions 2. How does the loss convergence bound for linear logistic regression compare to previous findings What is the known convergence rate for linear logistic regression 3. Under what conditions does the isolation assumption in Proposition 4.5 hold", "Paraphrased Statement: This paper introduces a novel perspective on demonstrating convergence in deep learning training. By employing Rayleigh quotients the authors establish that KL inequalities hold for a wider range of network architectures and loss functions even without the need for overparameterization. Illustrative examples are presented to support their approach. Strengths: The proposed technique offers a novel way to prove GD convergence in deep learning without relying on overparameterization. The Rayleigh quotient approach proves convergence by bounding the gradient rather than relying on eigenvalues of the neural tangent kernel (NTK). The paper is wellwritten and easy to follow with convincing case studies demonstrating the approachs effectiveness. Weaknesses: The paper primarily focuses on twolayer NNs in its case studies. Further examples could strengthen the argument for applicability to deep architectures. Minor Issues: A table of notations would enhance readability. The meaning of \"uniform conditioning\" in the paper could be clarified."], "wKhUPzqVap6": ["Paraphrase: Summary This paper investigates how models learn when presented with misleading relationships in data. The authors perform numerous tests that demonstrate that training a model using Empirical Risk Minimization (ERM) with subsequent Deep Feature Reweighting (DFR) (involving retraining the last layer on data without misleading relationships) produces results comparable to Group Robustness Training (Group DRO). Strengths and Weaknesses We acknowledge the authors efforts in this research. Overall it is a significant contribution with the following strengths: Comprehensive and informative experiments Clear and concise writing Wellreasoned comparisons and conclusions However there is a weakness in the paper: The conclusion relies heavily on the assumption that similar heldout performance implies similar learning patterns. This assumption is not necessarily valid and the experiments do not conclusively prove it. To strengthen the conclusion additional analyses could be performed. One way to verify that ERMDFR and GDRO are learning similar features would be to compare the representations they produce. This could involve extracting and comparing the learned representations under identical weight initialization and running the experiment multiple times. It would be helpful to clarify why early stopping is crucial for RWYRWG and GDRO. Additional simple experiments could shed light on this aspect. Questions: The questions raised in the paper are addressed within the \"Strengths and Weaknesses\" section.", "Paraphrase: This research examines deep learning when training data exhibits misleading correlations. It concludes that traditional optimization methods (empirical risk minimization) alone can achieve topnotch performance while specialized robustness techniques offer minimal benefits. This finding holds true across various datasets in image and text domains. Strengths: Addresses a significant concern (spurious correlations) in neural network training. Provides convincing experimental evidence. Weaknesses: The scope is narrower than the title suggests it only considers labeled not latent spurious features. Specialized robustness methods show no advantage over standard optimization. Relies heavily on a nonpeerreviewed technique (Deep Feature Reweighting DFR). Additional experiments with textbased models (e.g. BERT) would have been beneficial. The paper requires familiarity with a separate preprint (Kirichenko et al. [22]) to fully comprehend the DFR technique. Question: While the study examines labeled spurious features it remains unclear how latent spurious features influence the findings.", "Summary Paraphrase: This paper examines how deep learning models rely on coincidental correlations in data. The authors compare feature representations learned by models trained with traditional methods to those trained using methods that promote group robustness. Using a procedure called Deep Feature Retraining (DFR) they assess the quality of these representations which reveals the extent to which causal factors are captured in learned representations. The authors investigate how architecture pretraining and regularization affect these representations and find that choices in these aspects significantly influence the quality of representations. Their results highlight the importance of considering data augmentation architecture and pretraining when aiming for representations that prioritize causal information. Strengths and Weaknesses Paraphrase: Strengths: Investigates a compelling and practical problem. Welldesigned experimental setup with insightful results. Clear and organized presentation. Weaknesses: NLP experiments rely on a relatively outdated BERT model. Lacks proper positioning within existing NLP literature on spurious correlations. Additional Comments: The authors should clarify the role of spurious correlations in anticausal models in light of previous research. The authors should consider exploring why DFR may not improve representations in certain cases such as in the CXR dataset. The authors should discuss whether the observed trends in regularizations effect on FMOW may be attributed to excessive regularization in these models."], "kpSAfnHSgXR": ["Paraphrased Statement: Summary This research proposes an enhanced version of box embedding for handling directed graphs containing cycles. This concept is known as binary code box embedding which involves learning to project box embeddings into a smaller subspace within the box space. The study demonstrates the effectiveness of the proposed model theoretically and empirically. Strengths and Weaknesses Pros: Novel extension of box embeddings to represent cyclicity Impressive empirical performance Cons: Theoretical findings in Section 3.1 are taken from an external source [2] and should be properly attributed. Theoretical results in Section 3.2 are somewhat evident as it is clear that box embedding cannot capture cycles in any dimension. The motivation for using binary code embedding is not fully explained. Questions: The authors of [1] established that box embedding faces limitations in encoding certain directed graphs. It would be valuable to understand how the proposed binary code box embedding handles such limitations as illustrated in Section 7.3 of [1].", "Paraphrase: The researchers propose a novel model for simulating directed graphs using an updated version of box embeddings which can represent data more precisely especially when dealing with cycles. This can be seen as dividing a graph into a collection of smaller graphs each of which is modeled separately using (adjusted) box embeddings. With enough embedding dimensions the model can represent any directed graph without cycles. Experiments on synthetic graphs and realworld data show the effectiveness of the approach. Strengths and Weaknesses: Strengths: Provides an innovative solution to issues with geometric embedding models for graphs specifically the ability to handle directed cycles. Weaknesses: Limited theoretical and empirical advancements compared to previous work. Questions about the general motivation for the research. Concerns about the empirical evaluations which show minimal performance differences between the proposed model and existing methods. Specific Questions: How does the baseline vector model differ from existing node embedding techniques like node2vec Can the authors clarify the counting of parameters for the binary box models Current code suggests a discrepancy in the reported parameter count. Can the authors provide empirical evidence that the proposed models improved performance is due to its ability to represent cycles", "Paraphrase: Main Points: Binary code box embeddings allow for arbitrary directed cyclic graphs. Proposed models outperform vectorbased baselines in link prediction tasks. Strengths: Novel concept with theoretical foundation. Relieves box embedding constraints while balancing transitivity. Theoretically and empirically demonstrates the advantage of modeling cyclic graphs. Interval graph comparison provides clear intuition. Weaknesses: Questions: Gradient descent training capabilities unclear. Lack of information on optimization challenges. Impact of binary codes on learned geometry structures remains unclear. Visual comparison of learned boxes withwithout binary codes could provide insights."], "uPdS_7pdA9p": ["Paraphrased Statement: Summary: This research introduces a new method to enhance the robustness of specific subgroups within complex models known as transformers. This method addresses the challenge of group robustness when certain subgroups are underrepresented in the training data. Traditional approaches rely on balancing data by oversampling or reweighting or training models in multiple phases. However the proposed method utilizes \"contrastive adapters\" without requiring retraining from the beginning. Contrastive adapters aim to improve the structure of embeddings by contrasting the embeddings of samples belonging to the same class. Experimental results indicate that this approach effectively improves robustness against various data shifts. Strengths: Clear presentation and welldefined research goal High relevance and practical importance for realworld applications of foundational models Innovative concept of contrastive adapters Simple and practical implementation Weaknesses: Limited advancement in the idea of contrasting embeddings within the same class Inconclusive results regarding the superiority of contrastive adapters in all distribution shifts considered (standard adapters may perform better with complex datasets) Questions: Is the standard adapter also considered contrastive based on the arguments and equation provided Were standard adapters trained using ERM or the InfoNCE loss in the experiment results If ERM was used it would be valuable to compare the performance of adapters trained explicitly with InfoNCE loss. How large is the P value in Equation 6 and have different values been tested Can you clarify the method used to combine CIFAR10 and CIFAR02 datasets", "Paraphrased Statement: Summary: This study examines the robustness of foundation models against group shifts and reveals that even largescale models can struggle with certain group biases. The paper introduces a contrastive adaptation technique that leverages supervised contrastive learning to enhance adapter training and improve alignment and cosine similarity metrics. Experiments demonstrate the effectiveness of the proposed adapter in boosting the group robustness of foundation models. Strengths: Addresses a crucial issue as foundation models find wider application. The proposed method significantly improves robustness over baseline approaches. Weaknesses: Some experimental results require further clarification. The supplementary material accidentally reveals the authors identity. Questions: Why does the learned adapter perform worse on CelebA than on Waterbirds despite their shared group shift (Confounder) Could SupCons benefits in group robustness be attributed to improved overall model performance How does the proposed method compare to visuallanguage adapters from [R1] and [R2] Discuss the differences and similarities between this method and the one proposed in [57].", "Summary This study investigates the performance of largescale imagetext pretrained models in zeroshot classification under the grouprobustness protocol. The researchers find that the worstgroup accuracy is significantly lower than the average accuracy. They propose a method using a metric learning loss (contrastive loss) on a MLP adapter to improve accuracy on target datasets. Strengths Addresses important questions about model adaptation and transferability. Shows that neither zeroshot classification nor linear probing are optimal for adaptation. Demonstrates improvements in worstgroup accuracy on benchmarks. Weaknesses Missing essential baselines: Prompt tuning for zeroshot classification. Decoupling hardnegative sampling from contrastive loss. Twostage cascade classification with negative sampling. Concern about the use of the term \"foundation model\" without scientific basis. Questions for the Authors How do suggested baselines compare to the proposed method Why are the suggested baselines inferior or unnecessary"], "k_XHLBD4qPO": ["Paraphrased Statement: Summary: This study focuses on overcompression a challenge in Continual Learning and presents two enhancements for the ClassIncremental Semantic Segmentation (CISS) problem. The first enhancement expands network layers before the final classification layer to enhance feature quality. The second enhancement adds a dropout layer postencoder during the models offline training phase to develop more robust features for future CL steps. Strengths: Tackles the important issue of overcompression in CL model training. Demonstrates improved performance under specific settings using the proposed enhancements. Potentially novel use of a dropout layer exclusively during offline training to mitigate overcompression. Weaknesses: Originality: Lacks originality as it employs previously introduced enhancements in a specific problem context. Proposed evaluation framework is not original and resembles a typical online learning setup. Quality: Lacks direct evidence supporting the claim that increasing feature expressiveness improves CL performance. Ablation study is missing obscuring the impact of individual solution components. Minor enhancements and ideas are not wellintegrated into the overall solution e.g. the evaluation scheme proposal. Insufficient computing resources hinder the inclusion of error bars in reported results. Clarity: Incomplete explanations e.g. the authors refer to the background shift problem but do not adequately elucidate it. Unclear connections between related work and the proposed solution. Table 2 highlights results in bold that are not the best in their respective columns which can be misleading. Missing words in Table 1 caption. Questions: 1. Explain the rationale behind preventing interference in pretrained encoders while maximizing the effect on the decoder. 2. Why does DCSS perform worse in offline training despite having more parameters (Fig. 1) 3. Are standard deviations available for the results presented in Tables 1 and 2 4. How does the proposed evaluation scheme diverge from a standard online learning setting where each data sample is seen once 5. Discuss the limitations of the proposed enhancements.", "Paraphrase: This study addresses the overfitting problem in continual semantic segmentation models. To enhance the models adaptability to new tasks the authors employ wider convolutions for final feature extraction and apply dropout to the feature encoder output. These techniques previously successful in continual learning tasks are adopted for semantic segmentation. Despite achieving promising results on the Pascal VOC dataset the studys contributions are limited due to the established effectiveness of wide convolutions and dropout in continual learning as demonstrated in prior research. The authors attempt to ground their approach in the Information Bottleneck principle lacks supporting evidence. Furthermore the evaluation is restricted to the Pascal VOC dataset neglecting comparisons on ADE20K. The fixed feature extractor in the proposed method may limit its generalization to scenarios with a small base class set and a large new task set. In summary while the study offers a simple framework for continual semantic segmentation its contributions are modest and its evaluation is inadequate.", "Summary Paraphrase: The authors demonstrate that excessive feature compression during backbone training hinders model performance in continual segmentation tasks. They attribute this to the information bottleneck principle. To address this issue they modify the backbone training by incorporating an additional Dropout layer and increase the feature count in the classifier compared to the SSUL method. Experiments validate the efficacy of these modifications. Strengths and Weakness Paraphrase: While this approach extends the SSUL method the lack of adequate SSUL background may lead to confusion. Additionally although the authors emphasize the importance of a strong backbone for continual segmentation they suggest that the classifier merely classifies features. While their proposed methods aim to extract more robust or highquality features they do not fully resolve the overcompression issue. Experimental findings provide partial evidence of effectiveness but theoretical justification and ablation studies are lacking. Questions Paraphrase: 1. Highquality features are crucial for continual learning tasks. Explain the characteristics of highquality and lowquality features. Why can wider networks extract higherquality features 2. The proposed models output channels deviate from the original DeepLab V3. What datasets were used for model pretraining How was the backbone model initialized Did it utilize DeepLabV3 pretrained parameters 3. Increasing the network width typically enhances model parameters aiding in memory management and mitigating catastrophic forgetting. While more channels typically indicate greater model capacity they do not necessarily resolve overcompression. So how does increasing output channels address overcompression An ablation study comparing SSUL head modules with similar channel counts is needed. Would increasing SSULs channel count affect performance 4. Dropout layers are commonly used during model design to enhance feature extraction. Explain how the proposed Dropout layer differs from others and why it is inserted before the ASPP module. What would happen if Dropout layers were inserted in all convolutional layers", "Paraphrased Statement: The paper addresses the challenge of continuous learning specifically for semantic segmentation tasks. The authors suggest two straightforward enhancements to improve the models architecture: Wider shared convolution modules Dropout integration in the encoderdecoder structure The proposed methods have been evaluated on the PASCAL VOC dataset demonstrating superior performance compared to existing approaches. Strengths and Weaknesses: Strengths: Clear and concise writing Weaknesses: Extensive discussion on the information bottleneck theory whose relevance to the subsequent empirical tricks is unclear. The scenario considered may not accurately represent practical applications. Insufficient experimental evidence on more challenging datasets with a larger number of classes. The proposed solutions lack depth and come across as more like empirical tweaks rather than insightful advancements. Questions: If overcompression is a concern how would a more robust architecture fare as suggested by the authors in their future work plans"], "v9Wjc2OWjz": ["Paraphrased Statement: Summary: This research analyzes two algorithms (Bayes estimator and AMP) used to restore lowrank matrices from incomplete data in the presence of noise that does not match the assumed noise model. It derives a precise prediction of the errors for the theoretically optimal Bayes estimator and traces the behavior of the AMP algorithm over time. Additionally it presents observations from simulations on the performance of these algorithms under different metrics. Strengths: Provides mathematical guarantees for the accuracy of the Bayes estimator and AMP in recovering lowrank matrices under mismatched noise. Addresses a common realworld scenario where the noise distribution is unknown demonstrating the algorithms robustness in such situations. Compares theoretical predictions with empirical results verifying the analysis and highlighting areas for further discussion. Clearly written and accessible. Weaknesses: Questionable impact: The analysis extends existing work on mismatched Gaussian noise but may not be sufficiently novel for this venue. Some aspects are not fully explored: While it discusses the performance difference between Bayes and AMP it could delve deeper into the underlying reasons. Limited practical insights: It does not provide specific guidance on when to select one algorithm over the other in realworld applications. Questions: Can the authors elaborate on the significance of the performance gap between AMP and Bayes in this setting Are there any practical scenarios where one algorithm would be preferred over the other", "Paraphrased Statement: Summary: The authors investigated the issue of estimating rankone signals corrupted by noise that follows a structured rotationally invariant distribution. The study focused on scenarios where the assumed noise statistics (Gaussian) differ from the actual noise distribution. Three estimation methods were proposed and analyzed: the mismatched Bayes estimator approximate message passing (AMP) and spectral estimators. Theoretical analysis and simulations were used to compare the performance of these methods and provide insights. Strengths: Comprehensive analysis of the mismatched estimation problem for rankone matrices with theoretical and empirical support. Comparison of different estimation methods highlighting their advantages and drawbacks. Identification of a surprising observation that AMP based on Gaussian assumptions underperforms the mismatched Bayes estimator. The authors discuss potential reasons for this phenomenon. Weaknesses: Equation (8) presents the Gaussian AMP algorithm but its derivation and underlying objective function are not fully explained. More technical details would enhance understanding. Equation (9) defines mismatch performance metrics without sufficient context or explanation. Clarifying these metrics would provide clarity on the motivations behind the equations. Questions: The authors suggest that the performance gap between AMP and the Bayes estimator is caused by AMPs inaccurate estimation of the signal norm. A logical question arises: can AMP be modified to improve its signal norm estimation and potentially reduce the performance gap with the Bayes estimator", "Summary The paper investigates the penalties associated with reconstructing a rank1 matrix from noisy partial observations assuming Gaussian noise when it is not. Two algorithms are analyzed under different conditions: a Bayesian estimator in an asymptotic regime and an approximate message passing (AMP) algorithm in a state evolution regime. The analysis reveals a performance gap between the two estimators that varies with noise level due to AMPs imprecise estimation of the signal norm. Strengths and Weaknesses The experimental results align well with the analytical performance predictions. The dependence of the performance gap on noise level and model mismatch is shown. However some unnecessary simplifications in notation could compromise clarity. For example dropping the index N in line 171 and omitting a definition of terms used in Assumption 2. The discussion after Theorem 2 could be more explicit as a corollary and the authors could provide a statement more relevant to evaluating estimator performance using the metrics presented earlier. Questions Is the rescaled overlap measure (5) equivalent to the asymptotic cosine distance between X and X\u0302(Y) If so this could be described more clearly. The concern about the Bayesian estimator in line 179 seems unrelated to the overlap measure. Would it not also apply to the MSE Where is the function h(t) in (8) described The sketch of Theorem 2s proof mentions \"choosing denoisers carefully.\" In practice what does \"carefully\" choosing denoisers entail In line 254 should \"converges... to the random vector\" be \"converges... to that of the random vector\" Clarifying that the second random vector represents the state evolution in the AMP analysis would enhance clarity.", "Paraphrased Summary: This paper investigates the estimation of rank1 matrices and evaluates the performance of two inference methods: a Bayesian estimator and an AMP (Approximate Message Passing) algorithm. Paraphrased Strengths and Weaknesses: 1. The author did not address whether Assumption 3 holds for representative examples (e.g. soft thresholding). 2. The results and quantities discussed in Theorem 2 need to be formally presented as a theorem or corollary. 3. The dependency of Theorem 1 on the prior distribution on signal X (line 163) should be clarified as it contradicts the papers focus on robustness to noise distribution assumptions. 4. The AMP analysis requires an initialization that is independent of the noise and satisfies certain conditions which may not be practical. 5. The paper lacks detailed discussion relating the theoretical results to the central topic. 6. The main result for spectral estimators is missing despite its apparent inclusion in Section 2.3. Paraphrased Questions: See \"Strengths And Weaknesses\"."], "xTYL1J6Xt-z": ["Paraphrased Statement: Summary: The paper introduces a method for learning risk scores that are both simple and effective. Using a beam search algorithm it generates a set of sparse continuous solutions with varying support sets. These solutions are then converted into feasible integer solutions using multipliers to expand the potential solution space. The technique is computationally efficient. Strengths: The paper provides a clear framework for developing simple interpretable models. Numerical results demonstrate the effectiveness of the approach. Weaknesses: The method involves three distinct steps which could lead to potential issues such as error accumulation and computational cost for coordinate descent and line search. Real scores are not considered in this study. Questions: The definition of \"m\" is introduced late in the text and it is unclear from the beginning of Section 3 that it represents a multiplier. In Section 3.2 the term \"computationally efficient\" may not be appropriate when considering a large number of features. Section 4.3 seems redundant since the lack of real score results is already mentioned in the Introduction and Appendix. The paper does not provide information on: The range of \"m\" values used in experiments. The number of intermediate (pool) models generated. The percentage of reasonable final (integer) models obtained.", "Paraphrase: Summary: This research introduces a fast algorithm FasterRisk for calculating sparse risk scores that works well with large datasets. The authors have pinpointed significant drawbacks in existing methods and explained how their algorithm addresses these issues through three separate components. They conducted extensive testing on various data sizes and demonstrated that FasterRisk outperforms competing approaches. FasterRisk is implemented in standalone Python code unlike competitors that require mathematical programming solvers. Strengths and Weaknesses: Strengths: The authors provide an original solution to ongoing challenges in risk score development. The algorithm consists of welldefined components that tackle specific limitations. They provide detailed evaluations comparing FasterRisk to baselines and demonstrate the impact of each component. Weaknesses: The studys scope limits its impact and significance. The authors did not consider recent approaches that have partially addressed the limitations they claim to solve. FasterRisk only shows a slight advantage over a single competitor in performance evaluations. The mathematical notations could be more precise and welldefined. Questions: Question 1: The authors should expand their discussion on alternative approaches such as those that address sparsity and rounding issues through variable selection and scaling techniques. They should include more recent works in their evaluation for a more comprehensive comparison. Question 2: The authors should provide empirical evidence to support their claim that hyperparameter choices have minimal impact on performance. In particular they should justify the choice of tolerance gap level (\\epsilon0.3) and provide results on the impact of other hyperparameters. Question 3: The authors generated a pool of \"equally good\" scores but did not fully utilize them. They could explore how this pool can be used to select both highperforming and fair risk scores addressing a potential limitation of their approach.", "Paraphrase: Summary: This study developed a new approach to generate highquality risk scores. It combines a beam search algorithm for logistic regression (LR) with a star search for integer solutions. The method outperforms existing approaches in some datasets while being more efficient. Strengths: Provides a comprehensive framework with three steps: (1) collecting diverse solutions with varying support sets using a beam search algorithm (2) optimizing star search results using multipliers and a theorem (3) implementing sparse solutions with fewer nonzero coefficients. Clear and wellorganized introduction and literature review. Achieves industryleading performance with reduced computation time. Substantial theoretical foundation and supplementary materials. Weaknesses: Outdated examples of scoring systems in the introduction. Lacks a brief comparison of traditional scoring methods and the proposed approach. Performance improvements may not appear significant in some datasets. Consider using tables for more detailed comparisons. Limited discussions or experiments focusing on efficiency improvements despite the methods main advantage. Although model AUC assesses discriminatory ability it may not sufficiently indicate consistency between predicted scores and actual risk. Including calibration curves would enhance the practical significance of the generated scoring system.", "Summary The authors present a technique for generating risk scores (simple linear models with integer coefficients) efficiently through: A search algorithm to identify features. A method to find numerous sparse solutions with similar performance but different features. A \"star ray\" search to determine integer coefficients. They assess their methods speed and accuracy on various datasets. Strengths and Weaknesses Strengths: Innovative method for identifying risk scores quickly. Improved speed compared to previous approaches. Weaknesses: Speed improvement may not be significant in highstakes settings where models can be trained for longer. Lack of discussion on societal impact despite the intended use cases for the risk scores. Questions Importance of laptopspeed computation: Why is it essential for risk scores to be generated within a few minutes on a laptop in highimpact situations Figure 4: The timeout of 15 minutes for baselines is arbitrary. Including a log scale and data dimensions would provide a better understanding of baseline scaling. Example scoring systems: How were the examples in Section 4.3 chosen Are risk scores exclusively interpretable using the proposed approach Solution pool: The description mentions a \"pool\" of solutions without specifying its size diversity or intended purpose."], "uOii2cEN2w_": ["Paraphrased Statement: Summary: This paper introduces BORE an algorithm that improves uncertainty quantification over existing approaches. It provides guarantees for BOREs performance under specific conditions. Strengths and Weaknesses: The related work section is brief and should be expanded. Theorem 1 assumes a classifier that is not optimal and the optimality case is not addressed. The relationship between BORE bounds and GPUCB bounds with kernels is unclear. It is questionable whether the new bounds apply to BORE without a kernel or if they are merely GPUCB bounds. The distinction between GPUCB and BORE is not fully explained. BORE is claimed to be more flexible but theoretical guarantees require a kernel. Figure 1 requires more yaxis marks. The experimental section lacks depth and scope hindering any meaningful conclusions. The papers theoretical contribution needs to be clearly differentiated from existing approaches. Questions: Is BORE Bayesian considering its sequential updating mechanism but lack of a prior What is meant by \"valid\" approaches for uncertainty estimation as mentioned in lines 121123", "Paraphrased Summary: This paper analyzes the effectiveness of Bayesian optimization with expected improvement (BORE) by examining its regret. BORE assumes a discriminator function within a specific function space and uses kernel least squares to model it resulting in a bound on potential error. Based on this the paper derives regret bounds for both individual and cumulative iterations. The authors propose a UCBlike BORE to address potential nonvanishing cumulative regret. They also introduce a batch variant of BORE using Stein variational gradient descent and demonstrate its regret bounds through synthetic and realworld experiments. Strengths and Weaknesses: Strengths: The paper provides a clear and comprehensive analysis of BOREs regret. The authors introduce a batch variant of BORE with regret guarantees. Weaknesses: The experimental results do not include comparisons to nonbatch BORE. The batch BORE results are not particularly impressive. The paper could benefit from clearer organization by moving the batch description to the relevant section. Questions: Does Theorem 3s result depend on the true distribution lying within the leastsquares kernels function space How does BORE compare to other nonbatch Bayesian optimization methods How can the Lipschitz constant of the true distribution be bounded away from zero without compromising the task of finding the global minimum", "Paraphrased Statement: The paper proposes an improvement to the BORE algorithm called BORE by using a direct density ratio estimation method. The authors provide a regret analysis for BORE similar to the original GPUCB algorithm. Strengths and Weaknesses: The paper is wellstructured and straightforward to understand. The theoretical regret analysis is valuable to the community. The paper highlights a problem with the original BORE algorithm and demonstrates how BORE overcomes it. Weaknesses: The empirical evaluation is insufficient focusing only on the batch setting. A more comprehensive comparison with other acquisition functions in both batch and nonbatch settings would be beneficial. Questions: 1. Question 1: How is \u03c3t(x) in Equation (23) estimated in BORE Although empirical quantile estimation is mentioned in Section 5.1 the specific uncertainty estimation method used and its impact on the theoretical guarantee are not clear. 2. Question 2: It is claimed that batch BORE can achieve better performance than BORE while using the same number of function evaluations. This is counterintuitive since the nonbatch model typically has access to more observations for each decisionmaking round. Can you explain this rationale in more detail", "Paraphrase: Summary: The paper explores multiple issues: 1. It analyzes the theoretical foundations (especially the regret analysis) of the existing BORE architecture by Tiao et al. [9] providing insights into its convergence properties. 2. It tackles the nonconvergence issue of BORE by introducing the BORE approach which enhances uncertainty estimates and utilizes an upper confidence bound (UCB) on predicted class probabilities as the acquisition function to improve convergence. 3. The paper introduces the batch BORE method and its associated convergence analysis. Strengths: Clear and wellwritten paper with wellpresented methodologies theoretical analysis and discussion. Novel and compelling theoretical analysis for BORE BORE and batch BORE potentially inspiring future research. The authors acknowledge the impact of the theoretical analysis and discuss underlying assumptions. While limited the experimental assessment provides some evaluation of the theoretical analysis. Weaknesses: Insufficient experimental comparisons particularly between BORE and other baselines like BORE or GPUCB. The experiments solely used synthetic objective functions omitting realworld applications. Questions: In Section 7.1 the parameters for BORE BORE and GPUCB (e.g. betat Lepsilon) are not specified. Further insights into the comparative performance of BORE versus BORE for various objective functions are desired."], "tJBYkwVDv5": ["Paraphrase: Summary: This paper examines nonstationary linear regression with noise that follows a subGaussian distribution (variance \u03b72). The regression variable X experiences modifications from a zeromean process noise h with independent components (variance \u03c32). Estimation Method: The paper introduces an algorithm that estimates the noise parameters \u03c32 and \u03b72. This approach differs from the traditional maximum likelihood estimation (MLE). Algorithm Process: 1. Transform the process equations into a linear equation in \u03c32 and \u03b72 with a fluctuation term. 2. Apply the HansonWright inequality to bound the fluctuations. 3. Obtain a second equation from a truncated time series. 4. Solve the system of equations for \u03c32 and \u03b72. 5. Ensure the system is wellconditioned. Advantages: 1. Performance guarantees for the general subGaussian noise model (unlike MLE). 2. Novel mathematical techniques. Drawbacks: 1. Complex presentation especially in Section 4. 2. Unclear explanation of Theorem 3 and Section G. Issues: Typos in lines 146 185 and 186. Potential incorrect notation in equation (78).", "Paraphrase: This research introduces new finitesample guarantees for estimating parameters in dynamic linear regression models which involve timevarying coefficients. By reformulating the dynamic regression issue as a linear dynamical system the researchers highlight that existing techniques such as the Kalman Filter rely on accurate knowledge of certain variance parameters. While these parameters can be estimated using maximum likelihood estimation the underlying Gaussian noise assumption is often restrictive and asymptotic guarantees exist for the estimates accuracy. To address these limitations the authors propose a method with finitesample guarantees for estimating variance parameters using a less demanding subGaussian noise assumption for the dynamical system. Simulations demonstrate that the new estimators error diminishes at a rate consistent with theoretical bounds. In an empirical case study of electricity consumption prediction the researchers show that their estimator when combined with the Kalman Filter algorithm performs comparably to using maximum likelihood estimates. Strengths: Novel finitesample guarantees in a valuable generalization of linear regression Innovative and complex proofs that provide bounds previously considered unreachable Novel approach that leverages the Laplacian operator on a line Clear exposition of the problem setup and links to prior research Cogent overview of the literature Weaknesses: Need for enhanced experimental section to address the similarities between the novel approach and MLE Clarification of the significance of finitesample guarantees Lack of performance metrics in the realworld data example Computational or other constraints behind parameter choices in the simulation study need clarification Absence of discussion about the Kalman filter in the context of data assimilation methods", "Paraphrase: This research focuses on estimating noise variances in a specific type of linear dynamical system (LDS) where the states evolve based on the equation Xt1 Xt ht and observations are represented as Yt Xt ut zt where ut are known observation vectors. The study introduces an innovative algorithm for estimating variance within this LDS subclass providing theoretical guarantees even with limited data samples. The approach employs variance estimators that utilize datadependent operators. Strengths: Provides the first variance estimation for the LDS subclass with finitesample guarantees. Introduces a straightforward algorithm for variance estimation. Employs original techniques in the development of the approach. Offers clear explanations and compares the approach to alternative methods. Weaknesses: The considered system is a subset of LDS with scalar observations potentially limiting practical applications. The experimental evaluation is somewhat limited and STVE (the proposed method) did not outperform traditional MLEbased approaches in the presented experiment. Additional empirical evidence would be valuable to demonstrate STVEs advantages particularly in dealing with nonGaussian noise. Additional Related Work: Rashidiejad Paria Jiantao Jiao and Stuart Russell. \"SLIP Learning to Predict in Unknown Dynamical Systems with LongTerm Memory.\" NeurIPS 2020. Tsiamis Anastasios and George J. Pappas. \"Linear systems can be hard to learn.\" CDC 2021."], "msBC-W9Elaa": ["Paraphrased Statement: The researchers examine how parametric models trained with SGD (Stochastic Gradient Descent) generalize. While traditional methods for analyzing generalization error focus on bounding it uniformly over the entire parameter space ignoring the optimization process the researchers propose bounding the error over points reached by SGD updates. Under the assumption that updates contract points reached by SGD updates eventually become close. Therefore the researchers can reduce the analysis of SGDs action on the parameter space to studying SGD trajectories starting from a single point (0) over a fixed number of iterations (T). Since the number of trajectories is independent of the parameter space dimension the dependency on dimension in the generalization bound is eliminated. This approach is applicable to strongly convex and smooth functions as well as more general piecewise smooth convex functions. While the results do not improve known bounds in all cases they demonstrate improved generalization bounds for various learning tasks. Strengths: Novel and intuitive approach of considering only the parameter space under SGD action. Clear and accessible presentation simplifying arguments under the contraction property. General framework covering most common ML problems even if it doesnt enhance existing results. Convincing demonstration of improved generalization bounds in various ML settings. Accurate comparison to related research. Weaknesses: Limited novelty as some results overlap with those of [KSW22]. The contraction requirement is less strict than in [KSW22] but the results come with high probability guarantees rather than in expectation. While the Appendix provides a proof a more intuitive one using metric entropy reasoning could have been included.", "Paraphrased Summary: This paper explores the generalization gap for objectives optimized using stochastic gradient descent (SGD). By leveraging the contractive nature of SGD updates in the presence of strongly convex objectives the authors develop an epsiloncovering argument that eliminates the dependence on dimensionality for the generalization gap in certain cases. They also show that any smooth bounded nonconvex function can be approximated by piecewise strongly convex functions extending their results to a broader class of objectives. The paper demonstrates the effectiveness of this approach by improving the generalization gap for multiindex models and Kmeans with hard label assignment. Strengths and Weaknesses: Strengths: Originality: The paper introduces two novel insights: SGD iterates at step T cover future iterates with an appropriately chosen T. Nonconvex functions can be approximated by piecewise strongly convex functions. Application: These insights lead to an improved generalization gap bound that is independent of dimensionality. Impact: The paper demonstrates the potential of using algorithmicdependent stability for understanding SGD in deep learning. Weaknesses: Previous Work: The paper acknowledges some prior related ideas but these do not diminish the significance of the current work. Clarity: There are a few minor exceptions to the overall clarity of the paper. Questions: 1. Are the previous bounds mentioned for Kmeans and multiindex models specified for tail probabilities or average SGD iterates 2. Is there a general theory that describes when a smooth function can be represented as a piecewise strongly convex function with minimal components", "Paraphrased Summary This submission proposes a localized covering technique to derive generalization bounds for the Stochastic Gradient Descent (SGD) algorithm. By constructing an \\epsilonnet of points along the SGD trajectory the technique provides dimensionindependent bounds with a constant step size even without early stopping. Strengths Clear structure and logical flow. Weaknesses Strong assumptions including bounded deviation. Insufficient explanation of key lemma. Questions Assumption 2: Can the bounded deviation assumption be removed Equation (1.3): Why is the generalization bound independent of the iteration count Line 213: How are \"multiindex linear models\" inherently different from \"singleindex models\" Equation (1.4): Have the authors considered minibatch SGD Lemma 2.1: Is the implicit assumption of boundedness for the iteration sequence \\theta(t) reasonable Main Concerns The proof of Lemma 2.1 may be incorrect specifically the claim about the boundedness of \\\\theta(t) \\phi\\\\. The lemma lacks an expectation term. The application of differentiable function results to nondifferentiable objective functions (Line 272) may be questionable.", "Paraphrased Statement: The paper introduces a localized technique to enhance Stochastic Gradient Descent (SGD) trajectories. When dealing with objectives that involve perturbations of functions exhibiting piecewise strong convexity and smoothness the technique offers a generalization error bound that transcends dimensionality. Furthermore applications like multiclass Support Vector Machines and kmeans clustering show rate improvements with this approach. Strengths and Weaknesses: Strengths: Clear and wellstructured presentation of theoretical concepts. Insightful proof sketches that provide a glimpse of the full proofs. Accuracy of verified theoretical results. Weaknesses: Providing a table of known rates and assumptions from previous studies would enhance readability. The paper may not be easily accessible to nonspecialists due to its technical nature. Determining the novelty and significance of the approach may require expertise in the specific research area. Questions: Are there any additional insights or clarifications that could further elucidate the approach", "Paraphrased Statement: Summary: This paper examines the concept of generalization gap in stochastic gradient descent. By expressing the generalization gap as the number of specificsized balls needed to cover the functions domain the study focuses on limiting the number of balls required to cover the SGD trajectory. For strongly convex functions this is accomplished by utilizing the contracting nature of the gradient descent step which reduces distances between points consistently. In the case of nonconvex functions the paper leverages the approximation of its gradients using those of piecewise strongly convex functions. While the number of pieces may grow exponentially with dimensionality the paper demonstrates practical applications with a manageable number of pieces showcasing superior performance and improved dimensionality dependence compared to previous methods. Strengths and Weaknesses: The paper is accessible and succinctly conveys its ideas. While the significance of certain applications (e.g. singlelayer neural networks) may be debatable the presented examples illustrate the potential of the approach particularly with regularization as discussed in the \"Discussion\" section. Questions: Expanding on the derivation of the second term in the second line of Equation (B.3) would further enhance the papers clarity."], "r-CsquKaHvk": ["Paraphrased Summary: This paper presents a novel algorithm for analyzing survival data with features that change over time. It leverages the similarity between survival analysis and reinforcement learning. The algorithm is based on temporaldifference learning a technique borrowed from reinforcement learning and can be seen as its adaptation to survival analysis. Strengths: It establishes a connection between survival analysis with timevarying features and reinforcement learning. It introduces a novel algorithm called TCSR inspired by temporaldifference learning for survival analysis. It proposes a new approach to survival analysis by estimating the difference in outcomes between consecutive states rather than for a single state. Weaknesses: The proposed algorithms superiority over existing methods is supported by experiments in one section but these results are less applicable to another section due to differences in data structures. The paper implicitly assumes that similar states have similar survival distributions but this assumption is not explicitly stated. The experiments could be improved by directly comparing the algorithms outputs with the true probability distribution which is known for the synthetic data used. The presentation needs improvement including defining terms and correcting mathematical errors. Minor Issues: It should be clarified that the time in the algorithm is discrete. The use of the term \"converges\" in the propositions is questionable as they only describe calculations based on known transition probabilities. There are several grammatical errors. The phrase \"estimate survival\" should be replaced with \"estimate survival distribution.\"", "Paraphrased Summary The authors utilize concepts from temporal difference learning to create a model for sequential survival analysis that maintains temporal consistency. They discretize time and covariates to obtain exact solutions using dynamic programming and approximate solutions using parametric models. Their approach demonstrates sample efficiency and superior predictive performance in empirical evaluations. Paraphrased Strengths and Weakness Strengths 1. The authors propose a framework for modeling temporal consistency in sequential survival predictions. 2. They formulate the problem as a sequential decisionmaking process and provide elegant solutions using dynamic programming and parametric models. 3. They demonstrate the benefits of their method in terms of sample efficiency and predictive performance on three datasets. 4. The writing is clear and accessible. Weakness 1. The authors model assumes a finite number of states but realworld features are often continuous. It is unclear how they handle this discrepancy in practical applications. Paraphrased Questions 1. How do the authors handle continuous covariates in realworld studies 2. Can their model accommodate continuous survival times 3. How does their framework handle the history of covariates that accumulate over time steps", "Paraphrase: Summary: The authors introduce a novel algorithm for estimating survival models in a dynamic setting by utilizing a temporal consistency principle inspired by reinforcement learning. The algorithm fits parametric survival models using sample trajectories. They argue that temporal consistency can enhance statistical accuracy. Empirical evaluations on realworld and synthetic datasets are provided. Strengths and Weaknesses: Originality: The algorithm presents a significant methodological advancement in survival analysis. The authors acknowledge related work and highlight how their approach addresses limitations. Quality: The authors provide a detailed description of the methodology. However the empirical analysis could benefit from a more thorough discussion including explaining performance variations across datasets. Clarity: The paper is wellorganized and accessible. Visualizations are clear. Significance: The authors address an important problem and demonstrate the potential benefits of temporal consistency in survival analysis. However the paper could be strengthened by a more indepth analysis of the experimental results. Questions: Why does the algorithms performance degrade with a larger number of sequences Is there a specific reason for limiting the number of sequences to 200 Would it be possible to evaluate the methods on a dataset generated from known parameters to assess their ability to converge to true values", "Paraphrased Summary: The authors proposed a dynamic survival analysis approach that aims to model the distribution of the time when a sequence of states reaches a terminal state. Assuming that the state sequences follow a Markov chain the survival distribution is determined by the transition matrix. This matrix can be obtained using a recursive algorithm enabling efficient survival distribution computation. However in cases where the transition matrix is unknown the authors developed the Temporally Consistent Survival Regression (TCSR) algorithm to estimate parametric survival models using state sequence data. They compared TCSR with models based solely on initial state information and the landmark approach using synthetic and realworld data. Strengths: The proposed method leverages the connection between dynamic survival analysis and reinforcement learning potentially allowing the application of existing reinforcement learning algorithms to survival model estimation. The manuscript provides a comprehensive overview of the method explaining the process from using a known transition matrix with nonparametric models to estimating parametric models with an unknown transition matrix. The method demonstrates improved data efficiency by exploiting temporal consistency. Weaknesses: The method requires regular measurements of covariates at fixed time points and a finite state space which may not be feasible in all practical scenarios. The generalizability of the method to continuous state spaces and irregularly sampled continuoustime sequences is not discussed. The discussion and comparison with timedependent Cox regression another prominent dynamic survival analysis approach are missing. The accuracy of the fixed point \\barQ in estimating the true survival matrix is not evaluated. The Markov chain assumption impacts the characterization of the survival distribution but its implications are not explicitly explained. The definition of the event time in line 91 is unclear and may require clarification. Questions: How generalizable is the TCSR method to continuous state spaces and irregularly sampled continuoustime sequences How does the proposed method compare with timedependent Cox regression What are the potential implications of the Markov chain assumption on survival distribution estimation Can you provide a clearer definition of the event time in line 91"], "omI5hgwgrsa": ["Summary Paraphrase: This paper examines decentralized variational inequality problems specifically focusing on strongly monotone VI problems. It presents a stochastic algorithm for solving these problems in a finite sum setting deriving a lower bound on communication and computation complexity and proposing an algorithm that achieves this bound. The paper also includes numerical experiments to evaluate the proposed algorithm. Strengths Paraphrase: The paper introduces a novel setting for decentralized stochastic VI algorithms and demonstrates the strong performance of its proposed \"optimal\" algorithm through numerical experiments. Weaknesses Paraphrase: While the algorithm setting is new the technical contributions may be limited. There are also issues with the problem definition: The role of the convex function `g` in the lower bound analysis is unclear as it appears to be independent of the algorithm class under consideration. The proposed \"optimal\" algorithms use a proximal operator that depends on `g` making them inconsistent with the analyzed algorithm class. The analysis appears heavily dependent on a reference that is not discussed in the main paper which limits understanding of the novelty. Questions: Clarify the role of `g` in the analysis and motivate why it can be ignored. Discuss the main novelty in relation to [68] and the proof techniques employed. TyposIssues: Equation (7) may be incorrect potentially suggesting a fixed `z`. Equation (3) should use `\\maxy` instead of `\\miny`. Line 761 is missing the word \"graph\" after \"fixed.\"", "Paraphrase: Summary: Variational inequality methods are widely used in fields such as optimization machine learning image processing and game theory. This study establishes lower complexity bounds for communication and computation in decentralized stochastic variational inequalities under specific assumptions. It then proposes optimized algorithms that meet these bounds. The paper focuses on strongly monotone operators and demonstrates linear convergence. Strengths: Lower complexity bounds are provided. Optimization algorithms are designed to match the lower bounds. Weakness: Stringent assumptions may restrict the applicability of the algorithms. Strong monotonicity is required for convergence. Questions: Why are the terms n log(1epsilon) and sqrt(n)chilog(1epsilon) omitted from the complexity estimates in Theorem 42 Are there any differences between Algorithm 4 and the algorithms presented in [1] beyond the gradient estimator Could the supplementary material provide more analysis for the nondistributed case", "Paraphrase: This research focuses on decentralized stochastic variational inequalities (SVI) a type of optimization problem. The study: Establishes lower limits: Determines the minimum amount of communication and computational resources required to solve decentralized SVI. Proposes new algorithms: Designs algorithms that efficiently meet the identified lower limits in both fixed and changing networks. Strengths and Weaknesses: Merits: Explores an important topic and presents sound and thorough results. Area for improvement: The writing could provide more context and explanations to enhance reader comprehension."], "sPNtVVUq7wi": ["Summary This study introduces a method for creating a contextdependent global map using the Wasserstein2 distance between the pushforward and target distributions. The source and target distributions are assumed to be paired and the map is parameterized using a PICNN (Piecewise Isometric Continuous Neural Networks). The contexts can be varied including scalar covariates and discrete actions. An embedding module and a combinator module are designed to incorporate the contexts and help the map learn the conditional dependencies. The embedding module can use either onehot or multidimensional scaling embedding and the combinator module can use either multihot or deep set combinator. The method is primarily applied to biological data. Strengths Flexible parameterization of context dependence allowing for application to scalar multilabel and soft label data. Choice of combinator modules based on context complexity enabling generalization to unseen data. Thorough evaluation on singlecell data cancer data and other datasets. Weaknesses The core loss function and algorithm are not included in the main text but are deferred to supplementary material. Use of the Makkuvas dual formula instead of the primal formula is unexplained. Concerns about the use of the pushforward distribution by the ICNN OT map as a baseline when all source distributions are equal. Lack of bidirectional maps. Conflicting results in Figure 6 regarding the performance of onehot and MOA embedding. Questions Why was the Makkuvas dual formula chosen over the primal formula Is the goal to obtain bidirectional maps Can the combinator module only be used when there is a combination action Are the source distributions the same for all experiments except for the covariates condition Is the \"unknown combinations\" label referring to combinations of existing perturbations or unseen perturbations What is the reason for the conflicting result in Figure 6 between onehot and MOA embedding Has the authors considered the concurrent paper \"Neural Optimal Transport with General Cost Functionals\" by Asadulaev et al. (2022)", "Paraphrased Summary: This paper suggests learning a single transport map that can apply to multiple pairs of probability distributions. The transport depends on a context function for each pair with the Monge map modeled by a convex neural network. Strengths and Weaknesses: Strengths: Generalizing transport map learning to multiple measure pairs is welljustified for applications. Weaknesses: The Gaussian initialization may not be suitable for nonGaussian distributions suggesting an assumption about their geometry. For the framework to be feasible there must be a strong relationship between the pairs of measures. This is evident in the papers examples. The papers contribution is limited as using convex neural networks for transport maps is not novel. Minor Comments: Sentence structure issue at line 99. \"Figure 3\" should be \"Figure 1\" in lines 128 and 144. Figure 7 is not mentioned in the text. Additional Questions: Why are the proposed initializations (identity or Gaussian) effective Are there theoretical reasons The paper considers point clouds as the probability measures. Could this be explicitly mentioned", "Paraphrased Statement: This research examines the challenge of identifying the optimal transport map (known as the Monge map) that transforms between two given measures in a contextdependent manner. The authors present a framework that combines an input convex neural network and a context embedding. They also propose two effective methods for initializing the network. Through empirical studies they demonstrate that the proposed model can effectively predict and generalize in the context of predicting population dynamics across diverse contexts. Strengths: Wellwritten and presented paper Straightforward approach to incorporating additional contexts into traditional ICNNs for learning OT Monge maps Novel design of the PICCN network which is convex only in the main input (source measure support) Weaknesses: Lack of experimental comparison between the proposed initialization methods Generalization claim for the proposed method is not strongly supported in all experiments In the experiments the comparison with ICNN OT is not fair as it discards all context information Unclear why the UMAP embeddings in Figure 7 are different across plots Questions: How would the model perform if the embedded context were fed directly to the ICNN (not PICCN) or other simpler methods were used to incorporate context Should the experiments include outsample results for covariate conditioning Why are the UMAP embeddings in Figure 7 different despite representing the same data", "Summary (Paraphrased) This research tackles the challenge of creating a mapping function T between two spaces: a context space (C) and a data space (Rd). The goal is to find a mapping that given observed biological data (c \u03bc) where c represents the context (e.g. treatment dosage) and \u03bc represents data before or after treatment can accurately predict the transformed data T(c)(\u03bc) to match the actual observed data after treatment \u03bd. The key elements of the proposed approach include: 1. Utilizing optimal transport as a method for selecting the mapping function. 2. Parameterizing T using a partially input convex neural network inspired by the fact that the optimal transport map is the gradient of a convex function. 3. Transforming context variables into Euclidean vectors using an embedding module and combining them using a combinator module to account for multiple treatments. 4. Training the entire model endtoend using a dual formulation of the optimal transport problem. Contributions (Paraphrased) 1. Establishing a stateoftheart framework for creating mappings that can handle unseen composite contexts and exploit the continuity of the context space. 2. Developing an initialization technique for the partially input convex neural network to facilitate training. Strengths (Paraphrased) 1. Addressing an important and practical problem. 2. Providing a wellwritten and accessible presentation. 3. Demonstrating superior performance compared to existing methods on relevant tasks. 4. Enabling generalization to unseen contexts and combination of multiple contexts which was previously impossible. Weaknesses (Paraphrased) 1. Many components of the framework have been previously introduced in related literature. 2. The impact of the proposed initialization technique on test performance and training speed is unclear. CommentsQuestions 1. Consider incorporating Algorithm 1 and the training objective into the main text for clarity. 2. Clarify the regularity requirements for the existence of the optimal transport map in Section 2. 3. Provide a definition for \u03bb in Equation 4 and at the bottom of page 6. 4. Elaborate on the message in line 129 regarding the McCann interpolation. 5. Explain the rationale behind training g more frequently than f. 6. Describe the motivation behind the perturbation signature metric and its significance in the literature. 7. Simplify the explanation of the perturbation signature metric to emphasize the L2 distance between means. TyposSuggestions Remove the repeated word \"optimization\" in line 3. Change \"from of\" to \"from\" in line 18. Use more consistent notation for f in Equation 2 and 5. Clarify the labels in Figure 6b (perturbation vs. combination). Use \"unique\" correctly in line 73. Replace \"condition\" with \"context\" in line 119. Change \"encourages\" to \"allows\" in line 155. Correct the definition of \u03a8 in line 552.", "Summary This paper presents a novel approach that extends Monge maps to handle cases with multiple pairs of labeled measures. The goal is to find a single global map that assigns each labeled measure to its corresponding counterpart. Strengths Clear and concise writing Detailed descriptions of concepts and methods Helpful diagrams Wellmotivated problem with potential applications Novel approach with detailed implementation Encouraging empirical results Weaknesses Limited theoretical context and justification for the proposed conditional Monge map Lack of theoretical results to support and strengthen the approach Limited originality as it combines existing ideas rather than introducing many novel concepts ConcernsQuestions In equation (2) should both f\u03bc\u03bd and \u03c8\u03bc\u03bd appear In Section 3.2 why is \u03c8\u03b8 referred to as an OT map when it should be an OT potential In Section 4 clarify the meaning of \"approximate mapping\" of a measure. In equation (5) the function argument should be added to the notation. Why was the ICNN OT method not included in the experiment comparison in Figure 6 Consider clustering figures closer to the text they relate to for improved readability."], "qx51yfvLnE": ["Paraphrased Statement: The paper explores how to create greedy online content resolution schemes (OCRSs) that can dynamically select content in realtime. For scenarios with a single item and partition matroids the authors present an OCRS that employs a greedy strategy to choose content with an accuracy of \\frac1e. They extend this approach to transversal matroids producing a stronger result. For certain settings they demonstrate an even more efficient solution. They prove that their results are optimal for greedy OCRSs. The paper concludes with an experimental evaluation and comparisons to existing schemes. Strengths: Enhanced performance compared to previous OCRSs. Demonstration of the upper bound on algorithm performance. Clear and detailed analysis of related work. Weakness: The technical derivation of the results appears relatively straightforward. Questions: Why is the greedy nature of OCRSs a key feature Are greedy OCRSs merely effective against a powerful adversary", "Paraphrase: Summary Online Contention Resolution Schemes (OCRSs) are rounding techniques for online optimization problems particularly Bayesian optimization (e.g. prophet inequalities). This paper focuses on greedy OCRSs and presents an optimal greedy OCRS for the singleitem setting and more general constraint sets like partition matroids and transversal matroids. It also establishes a matching lower bound. Strengths and Weaknesses The paper provides an elegant greedy OCRS for the singleitem case. The scheme is straightforward selecting each element i in a set F with probability qi 1 xi2. It then chooses the first item (in adversarial order) that is active and selected in F. Notably this greedy OCRS is proven to be optimal among all greedy schemes. However the paper may be of limited interest to the Neurips community as its core concepts align more closely with theoretical computer science venues. Questions 1. In Definition 2.1 property 2: R(x) should be replaced with A. 2. In Definition 2.1 the order of quantifiers in properties 1 and 2 is unclear. Are these properties simultaneously satisfied for all sets A and points x 3. In Definition 2.1 property 2: The righthand side (RHS) should be cxi not cxi2. 4. In Definition 2.2 clarify that the random subset R(x) is drawn such that the marginal probability of i being in R(x) is xi. 5. In line 246 there is a typo: \"1 xj(1xj 2)\" should be \"1 xj(1 xj)2.\" 6. In line 261 correct the phrase to \"no greedy OCRS can.\"", "Paraphrased Summary: The paper explores ways to fairly allocate a set of items among multiple users in an online setting. It presents a particularly striking result for choosing a single item. Imagine you have a random set of items with each item having a certain probability of being selected. You can sort the items by these probabilities and present them to users in any order. You want to ensure that each user has a fair chance of choosing any item. The paper shows that the best way to achieve this is to discard some items with a probability equal to the square of their probabilities. After this cleanup the first item that remains not discarded should be chosen. This approach outperforms previous methods by improving the probability of selecting items with lower probabilities. The paper also extends this idea to more complex mathematical structures called matroids demonstrating its applicability in other settings. Strengths and Weaknesses: The main result is straightforward and practical. The authors demonstrate its usefulness in related scenarios. Weaknesses: The paper is technical and requires specialized knowledge. Critical definitions and concepts are missing making it difficult for nonspecialists to follow. The authors could simplify the introduction by explaining the problem in simpler terms before introducing technical terminology.", "Summary This research examines online contention resolution schemes that approximate fractional solutions for packing problems. Greedy Online Contention Resolution Schemes (OCRS) Given a fractional solution x an OCRS randomly rounds x to an integral solution y by selecting each yi as 1 with probability xi. However y may be infeasible. The OCRS then switches some yis to 0 to obtain a feasible solution z (i.e. z \u2264 y). An OCRS is online if the yis are revealed one by one and the OCRS must decide zi immediately after yi is shown. An OCRS is cselectable if P(zi1) \u2265 c\u00b7xi for each i. Greedy OCRSs first select a family of feasible solutions and then greedily attempt to set each zi to 1 while maintaining a solution within this family. Results The paper presents a 1selectable greedy OCRS for singleitem problems (i.e. \u2211xi \u2264 1) an improvement over previous 14selectable greedy OCRSs but inferior to the known 12selectable nongreedy OCRS. 1e is the lower bound for selectable greedy OCRSs for singleitem problems. The results are extended to transversal matroids. Experiments show that the 1selectable greedy OCRS improves upon the previous 14selectable greedy OCRS. Strengths and Weaknesses The main result is significant improving the greedy OCRS performance in the singleitem setting. The writing is generally clear but some terms lack sufficient definition. Questions Why would one prefer a greedy OCRS over a nongreedy OCRS if the latter performs better Does the 12selectability of the known nongreedy OCRS also hold against powerful adversaries Specific Comments Line 172: Clarifies that mathcalPmathcalI is the same as mathcalP. Line 175: Defines R(x) as the set of solutions feasible after rounding x. Lines 211 237: Indicates that subscripts of \\mathcalF may be omitted. Line 234: Uses a different definition of R than before. Line 246: Corrects a typo (xj instead of xi). Line 274: Clarifies that Rv contains only neighbors of v. Figure 1: Corrects the number of iterations from 100000 to 200000. Figure 2: Labels the xaxis as the size of N(u)."], "xwBdjfKt7_W": ["Paraphrased Statement: Summary: This research explores the resilience of Spiking Neural Networks (SNNs) to adversarial perturbations. To enhance this robustness a novel regularized adversarial training technique for SNNs is proposed. The regularization utilizes the spiking Lipschitz constant. Empirical evaluations across multiple datasets indicate that this training approach outperforms standard adversarial training in terms of robustness. Strengths: Innovative and pertinent to the NeurIPS conference. Achieves improvement over existing methods. Weaknesses: Key concepts require further clarification. Experiments lack coverage of eventbased datasets. Questions: 1. In Section 1 \"improve the adversariality of SNNs\" should be clarified as \"improve the adversarial robustness of SNNs.\" 2. In Section 1 \"The update amount of weights is the key to constructing adversarial attacks\" should be revised to state that adversarial attacks modify input intensities not weights. 3. Noise budget utilized in attacks for Table 1 results is requested. 4. The regularization method details are lacking its novel aspects should be elaborated upon. 5. Section 5 comparisons should include other published approaches. 6. Experiments should incorporate eventbased datasets as SNNs are frequently used in this domain.", "Paraphrased Summary: This paper introduces Regularized Adversarial Training (RAT) a method to enhance the robustness of Spike Neural Networks (SNNs) through adversarial training. To address the nondifferentiable nature of SNNs it utilizes three gradient approximations (CBA BPTR BPTT). To generate stronger adversaries it modifies attacks like FGSM RFGSM and PGD using these approximations. The paper also proposes spiking Lipschitz constant a variation of Lipschitz constraint that enforces Lipschitz smoothness on the model. RAT is tested on various architectures. Strengths: Clear and concise writing style Effective against several powerful adversaries Weaknesses: Lack of summary of previous research on SNN robustness No comparisons with other stateoftheart defense algorithms Inadequate explanation of ablation study (missing definitions and lack of baseline) Questions: 1. Provide a comprehensive literature review on SNN robustness and Lipschitz constraints in ANN. 2. Conduct comparisons with other robust SNN defense mechanisms. 3. Clarify the ablation study setup and define the terms \"MIX\" and \"REG.\" Specify the baseline used for comparison.", "Paraphrased Statement: Summary: The study advocates for using robust regularization in training spiking neural networks (SNNs) to enhance the robustness of trained models. Weaknesses: 1. The paper overlooks previous research demonstrating robustness under rate coding but limited robustness under direct coding. 2. The claim made in Line 208 lacks empirical support especially postfinetuning in the SNN domain weakening the studys motivation. 3. The inspiration for using batch normalization (BN) in SNN models is not clearly stated and model definitions are absent. 4. The use of FGSM attack variants during training and PGD variants during testing raises concerns about the experimental setup. 5. The authors should provide evidence that the observed robustness is genuine as it could potentially be due to gradient obfuscation. 6. The claim of mixing and matching different FGSMattacked images during training for diversity lacks theoretical or empirical justification. 7. An analysis of the additional training costs is missing. Other studies have shown improved robustness without increasing training costs. Overall: The paper lacks sufficient motivation contributions and comparison with stateoftheart approaches. The authors are encouraged to address these issues to strengthen the paper."], "s7SukMH7ie9": ["Paraphrased Summary: This study introduces an effective adversarial training (AT) approach for situations with imperfect supervision specifically when complementary labels (CL) are available. The work demonstrates that directly combining AT and CL is ineffective. To overcome this two attack methods are proposed: Warmup Attack: Progressively increases the attack budget throughout the training process. Pseudolabel Attack: Utilizes pseudolabels from model predictions to create adversarial examples. Experiments validate the success of the proposed attack techniques in the imperfect supervision context. Strengths: Innovative combination of adversarial training and complementary labels. Relevant and practical motivation for studying AT in imperfect data scenarios. Clear and straightforward Algorithm 1. Weaknesses: Incomplete literature review: Overlooks existing studies on AT with imperfect supervision such as [a]. Comparative evaluations with these methods are lacking. Limited novelty in proposed methods: Warmup Attack has been widely employed in deep learning. Pseudolabel Attack resembles techniques used in adversarial training under semisupervised learning [d]. Incomplete analysis: Figure 3 only examines the LOG method. The effectiveness of the proposed attack methods should be demonstrated across various complementary learning baselines [a]. Questions for the Author: Address the concerns raised in the Weaknesses section particularly regarding the novelty of the proposed work. Provide a more comprehensive comparison with existing approaches.", "Paraphrase: This study proposes a solution to a novel challenge in adversarial training: incorporating Complementary Labels (CLs). Combining adversarial training with CLs in a straightforward manner has not led to improved results. The authors have pinpointed the issue behind this ineffective combination and introduce Warmup Attack and Pseudo Label Attack to resolve it. The proposed approach outperforms the naive combination and a simple twostep method. Strengths: Clear and concise writing Comprehensive theoretical analysis Exploration of an unexplored problem Identification and solution of a unique problem Weaknesses: Limited originality in the proposed method. Pseudo Label Attack is comparable to the simple twostep baseline and Warmup Attack has been previously explored. Modest performance improvement over the simple twostep baseline Questions: The primary concern is the novelty of the proposed method. Can the authors provide further insight into its originality", "Paraphrase: Summary: This study explores how to use adversarial training (AT) in a new setting where training data contains complementary labels (CL) instead of ground truth labels. The authors identify two main challenges in CLbased AT: complex adversarial optimizations and lowquality adversarial examples. They address these issues by proposing warmup and pseudolabelbased attacks. Experimental results indicate that the proposed method effectively generates robust models in the CL setting while baseline methods struggle. Strengths and Weaknesses: Strengths: The proposed approach is wellconceived and logical. The authors analyze the limitations of AT in CLbased settings and develop targeted solutions. Weaknesses: The studys CLbased AT setup is questionable. The authors assume there is no perfect supervision data for robust model training but they do not justify why AT should be considered in such a scenario. It would be more logical to evaluate ATs performance on noisy data rather than using CL to intentionally introduce imperfections in supervision. The robustness of models trained with CLbased AT is generally inferior to that of models trained with standard AT primarily due to imperfect supervision. This is evident in the experimental results where vanilla AT often outperforms the proposed methods in the CL setting. The setting proposed by the authors is arguably unreasonable. Exploring ATs performance on noisy datasets would be more sensible allowing a fair comparison between vanilla AT and the proposed CLbased approach. Questions: Why introduce CL into AT to increase the difficulty of AT Why is a noisy dataset for AT not considered What is the practical advantage of robust models trained with CLbased AT given their inferior performance compared to standard AT models", "Paraphrase Summary The study examines a novel and challenging setup by integrating adversarial training (AT) with complementary labels (CL). While incorporating CL into AT is theoretically beneficial due to common inaccuracies in realworld AT scenarios empirical observations consistently reveal that direct integration leads to failure. Theoretical analysis demonstrates an inconsistency between complementary risk and ordinary risk of adversarial optimization with limited CLs. In addition empirical studies of gradients expose two challenges: intractable adversarial optimization and lowquality adversarial examples. To address these challenges the study proposes a novel attack strategy incorporating a warmup phase to alleviate optimization difficulties. The models predictions gradually augment adversarial training with pseudo labels. Extensive experiments on various datasets and baselines demonstrate the effectiveness of the proposed algorithm in enhancing adversarial robustness and stability. Strengths and Weaknesses Pros Wellwritten and accessible Clear motivation and challenging setting Intriguing theoretical and empirical analyses Simple and intuitive proposed techniques (warmup and pseudolabels) Comprehensive evaluation with favorable results Cons Limited empirical evidence supporting the use of model predictions as strong supplementary information No ablation studies of the warmup attacks step count Questions 1. Provide empirical evaluation of the accuracy of pseudo labels. 2. Include more ablation studies to examine the impact of different warmup strategies."], "nyCr6-0hinG": ["Paraphrase: Summary: SpaceCraft enhances the efficiency of designing tensor program schedulers by defining a composable and modular search space. It leverages probabilistic programming to model the search space allowing designers to combine reusable program transformation modules with expert knowledge. Additionally SpaceCraft provides a learningdriven search framework that utilizes a learned cost model to guide efficient searching. Experimental results show SpaceCrafts optimization capabilities comparable to TVM. Strengths and Weaknesses: SpaceCraft introduces a novel programming model that simplifies search space representation and enables the integration of domain expertise. The method is theoretically sound and complements existing autotuning and autoscheduling approaches. It exhibits favorable performance on specialized accelerators like Tensor Core. However direct comparisons with stateoftheart techniques like FlexTensor and Ansor would enhance the evaluation. Additionally a quantitative assessment of the search space quality (e.g. the proportion of highperforming candidate programs) in comparison to previous work would be beneficial. The unique advantages of the learningdriven search framework over AutoTVM which also employs cost models remain unclear. Ablation studies would clarify the effectiveness of the proposed learning framework. Questions: 1. Clarify the potential advantages of using Tensor Core in TVM as illustrated in Figure 10(b). 2. Conduct further analyses to address the raised weaknesses such as direct comparisons with stateoftheart approaches and a quantitative evaluation of the search space quality. 3. Provide more details on the ablation studies conducted to demonstrate the effectiveness of the learningdriven search framework.", "Paraphrase: Summary Optimizing programs involves transforming them into equivalent but more efficient versions. These modified programs are found within defined optimization spaces which can be challenging to tailor. This paper introduces SpaceCraft a language abstraction that simplifies creating and modifying tensor program search spaces. SpaceCraft does this by providing modular components to build program transformations. It combines SpaceCraft with MAP estimation for automatic tensor program optimization. Strengths SpaceCraft tackles the critical problem of constructing program search spaces reducing trialanderror in the process. The papers figures clearly illustrate the concepts. Probabilistic programs are used innovatively for tensor program optimization. SpaceCraft unifies previous methods under a single framework. Weaknesses Presentation: The paper needs refinement in terms of grammar and clarity. It fails to specify the customization hurdles in prior work and does not adequately explain SpaceCrafts abstraction. The presentation in Section 3.3 is confusing. Experiments: The paper does not evaluate the effort required to modify search spaces with SpaceCraft compared to prior approaches. It also fails to compare optimization results with newer techniques like AutoTVM and CHAMELEON. The search space comparisons in Figure 10(b) are unfair since the search spaces are different. Performance measures in Figures 8 9 and 10 are not defined. The paper does not provide measurements of optimization time which is crucial for practical use. Minor Weaknesses: The paper claims to opensource the framework but marks the code and data as proprietary. There is no discussion of the works limitations. Questions for Authors: 1. Can the authors clarify the weaknesses mentioned in the previous section 2. Can they address the drawbacks of their experimental design 3. Can they explain why specific important experiments have not been performed", "Summary: Summary SpaceCraft is an enhancement over previous TVM IR providing: No need to define templates (like AutoTVM) Same programmability as Auto Scheduling (like Ansor) It introduces a probabilistic language abstraction and search algorithms that outperform existing blackbox optimizations. The paper presents a significant contribution to tensor program code generation by introducing a third approach probabilistic programming which builds on previous TVMbased techniques. Strengths and Weaknesses: Strengths: Timely contribution of probabilistic programming to tensor program compilation Proposed abstraction in Section 3 has potential to inspire research in optimization Weaknesses: Unclear details in Section 4 about learning Lack of clarity on how it relates to prior works Focus on CPUs and GPUs only not considering the wider range of accelerators Questions: Can Section 4 be clarified Would the new abstraction require changes to previous blackbox optimization works How would transfer learning in AutoTVM be implemented Could the paper include examples with fused operations like ConvPoolReLU Are there experimental results available for common DNN patterns", "Paraphrase: This paper introduces a framework (SpaceCraft) that simplifies the optimization of tensor programs for various hardware platforms by incorporating knowledge from previous optimization approaches. The framework leverages random walk MetropolisHastings sampling to explore alternative optimizations with the compiled programs latency (or an estimate) influencing the probability of selecting optimizations. Integrating hardwarespecific tweaks is possible leading to performance enhancements in modern Deep Learning (DL) models compared to TVM and PyTorch. Strengths and Weaknesses: Strengths: Novel approach addressing the optimization of ML models for different hardware substrates. Enhances model performance on deployment devices such as mobile CPUs and GPUs. Weaknesses: Specifying kernels in the probabilistic DSL may be cumbersome. Transformation modules are seemingly specific to tensor programs potentially limiting applicability. Using MCMC for sampling may be less effective due to the nonlinearity of optimization outcomes. The \"user experience\" of specifying kernels in the probabilistic DSL looks a little bit awkward. Clarity: The paper focuses on demonstrating the functionality of SpaceCraft but lacks detailed explanations of its implementation. The time spent exploring optimizations compared to deterministic systems like TVM is unclear. Significance: The reported 2x speedup over PyTorch for BERTLarge is significant. The claim of simplified integration of novel TCspecific modules needs further elaboration. Questions: Integration with existing ML frameworks: How easy is it to call SpaceCraft from frameworks like PyTorch or JAX Optimization caching: Does optimization need to be rerun each time or can previous results be cached Importance of priors: How significant is the prior probability and how is it specified Continuous random variables: Does SpaceCraft support optimization decisions involving continuous random variables Numerical issues: Can the optimized programs lead to numerical inaccuracies due to reordered arithmetic operations"], "tZUOiVGO6jN": ["Paraphrased Statement: Summary: This paper introduces Joader a data loading system designed for data sources and preprocessing that are shared across multiple training tasks. Joader addresses the issue of training jobs with varying speeds which can lead to cache thrashing. Strengths: The novel dependent sampling algorithm (DSA) is theoretically sound provides flexibility in varying training speeds and allows partial overlapping of jobs. The integration with PyTorch and the ability to support distributed training make Joader accessible and convenient. Preliminary experiments show the effectiveness of this approach. Weaknesses: The evaluation was limited to a single workload (ResNet). Distributed training was not included in the experimental setup despite its relevance to scenarios involving multiple overlapped training jobs. Questions: Is there a reason why only ResNet was used in the experiments Can Joader be applied to a wider range of models beyond image classification tasks", "Paraphrased Statement: Summary: Researchers have encountered an issue with training multiple deep neural networks (DNNs) in parallel on a single graphics processing unit (GPU). This problem stems from the excessive use of inputoutput (IO) operations and data preparation which can hinder training speed particularly when multiple DNNs are being trained concurrently. To address this issue the authors have devised an innovative data loading approach for efficient parallel DNN training. This approach incorporates a data sampling algorithm that enhances sampling locality while preserving randomness. For practical implementation the system employs a data structuredependent sampling tree and a dedicated cache. The authors have developed a prototype called \"Joader\" which integrates with PyTorch to demonstrate the effectiveness of their approach in accelerating parallel DNN training. Strengths and Weaknesses: Strengths: Practical problem with societal relevance Novel and clear approach Maintains randomness in sampling while reducing IO and data preparation overhead Theoretical proof of algorithm correctness and optimality in the twojob case Prototype implementation (Joader) integrated with PyTorch Evaluation results demonstrate significant improvements in training speed Weaknesses: Complexity in the description of the Njob case Lack of optimality proof for the Njob case Questions: 1. Can the method ensure that identical data is sampled when datasets are the same If so is there a compromise between strict correlation and randomness compared to standard random sampling 2. Is it feasible to provide a proof of optimality for the Njob case", "Paraphrased Statement: To make multitask training on overlapping datasets more efficient a new method focused on data preparation is introduced. It involves a dependent sampling algorithm (DSA) and a domainspecific cache policy to enhance data locality. A new tree data structure is developed to facilitate the implementation of DSA. Experiments have shown significant improvements in the speed and versatility of training without compromising accuracy. Strengths: Proposes a data loading method for training multiple concurrent jobs on overlapping datasets. Introduces DSA to optimize data access locality and preserve randomness. Designs a tree data structure for effective implementation of DSA. Backs up claims with convincing experimental results. Weaknesses: The process requires additional operations such as dataset partitioning and probability calculations which could introduce overhead with a high number of training jobs. This is not addressed in the text. The motivation is limited as it focuses on a specific scenario where multiple tasks are trained on overlapping datasets on the same machine which may not be common. Questions: The text does not provide information on the technologies used to make CoorDL superior to Joader in synchronous scenarios."], "z2cG3k8xa3C": ["Paraphrased Summary This work analyzes the Gaussiansmoothed Wasserstein distance (GOT) under conditions where the Gaussian kernel parameter \u03c3 is relatively small. It is previously known that GOT approximates the true Wasserstein distance with a difference of order \u03c3. This paper improves this estimate showing that under certain assumptions about the uniqueness of the transport plan GOT exhibits an exponential rate of approximation rather than a linear one in the small\u03c3 regime. Strengths Clear presentation of definitions and proofs. Novel and interesting theoretical results. Potential applications beyond the specific context of GOT. Useful concepts such as strong implementability and robustness of transport plans. Weaknesses Focus on a narrow and technical problem. Lack of discussion on potential applications or implications for wider fields. Insufficient detail in numerical results. Questions Clarification on the practical verification of condition (6) in Remark 2.14. Discussion of the connection between strong implementability and the results in [PdC20]. Elaboration on the proof of Theorem 2.7 and its relation to [THG17]. Conclusion The paper presents valuable theoretical insights into GOT particularly in the small\u03c3 regime. However it could benefit from providing more context and discussing its broader implications. With these improvements it would be a stronger contribution.", "Paraphrase: Summary This paper examines the approximation of the Wasserstein distance between two discrete probability distributions \\mu and \\nu using a technique called Gaussiansmoothed optimal transport (GSOT). Specifically it demonstrates that the existence of a perfect match between \\mu and \\nu affects the behavior of the GSOT distance under low noise conditions. Strengths and Weaknesses GSOT is valuable because it approximates the true optimal transport distance and overcomes sampling complexity issues. The papers key findings include the existence of a phase transition in the context of finitely supported measures. Additionally the study is wellorganized and effectively presents its methodology. Minor Comments The notations for the Gaussian kernel parameter \\sigma and permutations \\sigma could be clearer. The order of proofs in the appendix could be improved for better readability. Questions Can these results be extended to entropyregularized optimal transport where the uniqueness of the transport plan is guaranteed but perfect matching may not apply for large regularization parameters Why is the condition of compact support essential in Proposition 1.1", "Paraphrased Summary: This paper presents an approximation rate for the distance between two discrete probability distributions using a Gaussiansmoothed Wasserstein distance. The approximation rate is significantly faster in cases where the distributions have a \"perfect matching\" plan but it is linear in all other cases. This effect is subject to a phase transition where the rate changes from exponential to linear as the smoothing parameter increases. Strengths: Interesting results with technically sound theoretical foundations. Weaknesses: Restrictive setting: Both distributions are assumed to have a finite number of support points limiting the applicability of the theory to general Gaussiansmoothed Optimal Transport problems. Room for improvement in presentation: Clarify the dependency of the constant c in Proposition 1.1 on the dimension d. Provide a more coherent explanation for the choice of proofs presented in Sections 3 and 4. Organize the results in the appendix more logically. Questions: 1. Present sufficient conditions for \"perfect matching\" in Optimal Transport. 2. Explore the possibility of extending the results to arbitrary probability measures where \"perfect matching\" is more likely. 3. Clarify the meaning of \"asymptotically greater\" in the statement of Theorem 3.3 as the fixed smoothing parameter \u03c3 does not allow for an asymptotic relationship.", "Paraphrased Statement Summary: This work examines the asymptotic behavior of the 2Wasserstein distance between two discrete measures after they have been smoothed by Gaussian kernels. Specifically the authors provide bounds on the difference between the Wasserstein distances of the original discrete measures and their smoothed versions when the Gaussian kernel variances approach zero. Scenarios: The authors consider two scenarios: 1. Perfect matching exists between the discrete measures before smoothing. 2. No perfect matching exists between the discrete measures before smoothing. Results: Scenario 1: The asymptotic gap between the unsmoothed and smoothed Wasserstein distances decays exponentially in regions near zero. Otherwise the gap decays linearly. Scenario 2: The gap between the unsmoothed and smoothed Wasserstein distances decays linearly even near zero. Strengths: Wellwritten and presented paper. The questions addressed are theoretically important and the answers are novel and thorough. The authors consider all possible scenarios and use innovative tools in the proofs. Weaknesses: The toy example in the experiment is helpful but the authors should consider verifying their theory in largerscale settings. Questions: Would the asymptotic behavior be similar for both (\u03bc \u03bd) and (\u03bc \u03bd) if \u03bd is slightly modified to \u03bd using k \u2248 0 and there is a unique optimal transport plan between (\u03bc \u03bd) Does the separability of means in the Gaussian mixture approximation play a role in the asymptotic behavior Is the uniqueness of the optimal plan related to the use of the 2Wasserstein distance in studying asymptotic behavior Minor Error: In the figures \"p\" should be replaced with \"k.\""], "m7CmxlpHTiu": ["Summary Paraphrase: This paper investigates an intriguing issue in longtailed recognition: the training data distribution is skewed while the testing data distribution is diverse (rather than uniform as previously assumed). To address this challenge the paper introduces a novel method that outperforms current techniques in both standard and testagnostic longtailed recognition scenarios. Strengths and Weaknesses: Strengths: The problem addressed is novel and unexplored. Extensive experiments demonstrate the efficacy of the proposed approach. The writing is clear and accessible. Weaknesses: Technical significance is limited: Skilldiverse expert learning is not a groundbreaking concept as multiple experts have been employed in previous studies. The aggregation of diverse models has also been explored elsewhere albeit in a different context. Testtime selfsupervised aggregation merely adjusts three models weights. The method is restricted to transductive settings where all test data is available simultaneously. The approach is computationally more expensive than existing methods due to the need for reweighting at test time. Some bold results in Table 9 are not the best achievable. Questions: 1. Why were Balanced Softmax and a variant logit adjustment loss chosen 2. What criteria should be used to determine the number of experts 3. Is the classifier reweighting strategy optimal Can Testtime Selfsupervised Aggregation find the best weights 4. Why should this approach be chosen over existing longtailed methods like RIDE given the modest performance improvement and higher computational cost", "Paraphrased Summary: This study extends the idea of longtailed learning by allowing models trained on imbalanced class distributions to perform well on any testing distribution even those with unknown or nonuniform distributions. To address this challenge the paper introduces a method with two main components: 1. Diverse experts with specialized knowledge for different classes 2. A selfsupervised weighting strategy that dynamically combines these experts at test time to adjust to unknown testing distributions The proposed method has proven effective in handling various class distributions in testing settings. Strengths: The \"testagnostic\" setting is innovative and highly applicable in realworld scenarios. The paper is wellwritten and easy to understand. The aggregation strategy is intriguing and has demonstrated its usefulness. The comprehensive experiments and ablation studies provide convincing evidence of the methods efficacy. Weaknesses: The primary concern is computational complexity. While the independent experts in different ResNet blocks and fully connected layers appear manageable for such a challenging task the authors could explore the tradeoff between accuracy and complexity further. Specifically it would be beneficial to examine how accuracy varies at the two extreme points: When nothing is shared between experts When everything is shared except for the fully connected layers Questions: In line 762 how are the expertiseguided loss functions modified to accommodate different distribution types", "Paraphrased Statement: The goal of this study is to create a mixtureofexperts (MOE) model to address the issue of testagnostic longtailed recognition where the distribution of test classes can vary greatly from the training set. The model called SADE utilizes three experts and includes two key strategies. During training SADEs experts are assigned distinct class distributions to handle the challenge of different test distributions. At test time it employs a selfsupervised learning approach to determine how to combine expert predictions based on the unknown test class distribution. SADE demonstrates superior performance on various longtailed datasets including CIFAR100LT ImageNetLT PlacesLT and iNaturalist 2018. Strengths: Demonstrates the performance of longtailed algorithms across diverse test scenarios. SADE consistently achieves stateoftheart results. Clear and concise presentation. Weaknesses: SADEs largest improvements are observed with backward longtailed test distributions which may be less common in reallife applications. The testtime aggregation strategy requires access to all test data (unlabeled) before deployment which may not be feasible in practice. Questions: Is it fair to compare SADE with RIDE when RIDE was trained for 100 epochs while SADE was trained for 200 Can MCDropout be used as a metric for expert aggregation to eliminate the need for all test data"], "uP9RiC4uVcR": ["Summary: This work investigates the ability of Large Language Models (LLMs) to understand the flexibility of human moral judgments. While previous studies have examined LLMs performance on rulebased moral judgments this work focuses on flexible judgments where it may be permissible to break rules depending on the context. The authors propose a challenging dataset consisting of three different moral rules each with a range of scenarios where it is either acceptable or unacceptable to break the rule. This dataset is based on extensive human annotations with each scenario carefully crafted to avoid being predictable based on the models training data. By evaluating various language models including Delphi (specifically trained on moral judgments) and InstructGPT (a stateoftheart LLM) the authors demonstrate that most models struggle to perform well on this task. However their proposed method called \"Moral ChainofThought Prompting\" significantly improves performance by 11 F1. Strengths: Addresses a significant issue in understanding LLMs moral judgment abilities. Uses a dataset of human annotations that captures the flexibility of moral judgments. Demonstrates the effectiveness of their proposed prompting method. Provides detailed error analysis and includes examples of highstakes wrong predictions in the appendix. Weaknesses: Does not provide information about the human annotators involved (e.g. demographics). Requires access to the appendix for a complete understanding of the results. Does not include analysis of prompt sensitivity or human annotation agreement in the main text. Questions and Suggestions: Analyze the sensitivity of the SOTA model to prompt wording and ordering. Include conclusions from the appendix analyses in the main text. Provide data on the level of agreement among human annotators on moral judgments. Cite relevant work on ChainofThought prompting. Clarify the usage of the `concat()` operation in line 259. Correct the reference to Table 8 in line 312 to Table 4.", "Paraphrased Statement: Summary: This research investigates a method to enhance language models by incorporating human moral reasoning. The goal is to develop AI systems capable of determining whether its acceptable to violate moral rules in various scenarios. This task is essential for AI safety as it involves comprehending ethical considerations in diverse contexts. The proposed approach employs a strategy that generates a series of questions and answers to predict human moral judgments. Strengths: Emphasizes the critical need for AI systems to consider human moral judgment. Highlights the importance of AI safety as a motive. Introduces a novel RuleBreaking Question Answering (RBQA) task dataset for NLP systems seeking to incorporate human moral judgment. Weaknesses: The model and approach descriptions are overly brief lacking details and analysis. Questions: How is the sequence of questions (q1 ... qN) determined Is it predetermined or tailored to each case Were alternative aggregation functions explored instead of concatenation for generating the chained prompt (ci)", "Paraphrased Statement: To explore the adaptability of human moral reasoning researchers created a series of questions that require breaking ethical rules. Using the advanced language model InstructGPT they developed MoralCOT based on moral reasoning theories. Their model performed better than existing language models on these tests. Strengths: The authors methods such as breaking down moral decisions and estimating costs are innovative from a psychological perspective. They provide detailed examples to illustrate the input and output of their model which can benefit the research community. Weaknesses: The model only considers three types of ethical norms which is limited. It relies heavily on the open AI API reducing its reproducibility. The details of human responses are not presented making the model less transparent. Questions: The use of InstructGPT raises concerns about reproducibility. Could InstructBERT be a comparable model for testing Applying MoralCOTs mechanism to a pretrained model like BERT would be valuable. The interrater correlation of binary predictions for human moral judgment would be informative. Comparing the models performance to individual human assessments would provide insights."], "xUK4E1jpV7z": ["Paraphrased Statement: Summary This research examines whether concept classes with a specific property called VC dimension d can be compressed to a size of roughly d or poly(d). The study demonstrates that exceptional classes meeting this specific property can be compressed to a size of d without requiring labels. The paper presents three applications of this finding: 1. Exceptional classes with VC dimension d containing maximal classes with dimension d1 can be compressed to a size of d without labels. 2. If exceptional classes with VC dimension 2 have compression schemes without labels of size 2 then this is true. 3. Intersectionclosed classes with VC dimension d can be compressed to a size of O(d) without labels. Strengths Addresses a foundational issue in learning theory and contributes advancements. Overall the paper is wellwritten. The findings may be valuable to researchers exploring compression. Weaknesses The papers contribution may be perceived as somewhat evolutionary. Despite this the paper offers a step forward in this field and is worth considering for presentation to the ML theory community. Questions Line 31: The sentence describes \"VCd classes\" referring to concept classes with a specific property called VC dimension. Line 57: The paragraph states that intersectionclosed classes can be embedded into exceptional intersectionclosed classes with a linear increase in VC dimension. However it also claims that intersectionclosed classes can be compressed without labels linearly in their VC dimension. The latter implies no increase in VC dimension. Line 57: Theorem 4.4 should indicate that intersectionclosed classes can be embedded into exceptional intersectionclosed classes with only a constant increase in VC dimension.", "Summary Paraphrase: This research explores techniques (\"compression schemes\") used to organize concepts based on their ability to be compressed. Its believed that the VC dimension (d) of a concept class should indicate a compression scheme size of at most d. This paper presents a key finding: the size of compression schemes for concept classes with VC dimension d and closed under intersection is limited to 11d. Strengths and Weaknesses Paraphrase: This paper contributes to understanding the connection between compression schemes and VC dimension a significant topic in computational learning theory. However the findings are highly specialized likely appealing only to a small audience. The writing could be improved with several awkward and ambiguous phrases. Questions Paraphrase: How do the results relate to the interests of the NeurIPS audience Are there any applicable takeaways or insights for individuals unfamiliar with compression schemes", "Paraphrased Statement: Summary: This research investigates compression techniques for highly complex concept classes. The authors establish a condition (involving a smaller class with a reduced VC dimension by 1) for an extremely complex class denoted as C to tolerate a compression scheme of size VC(C). Additionally they demonstrate that concept classes that can be represented by intersection operations can be compressed with a size proportional to their VC dimension. Strengths and Weaknesses: Originality and Quality: The authors contribute to the understanding of conditions for concept classes to have compression schemes matching their VC dimension. Their findings and proofs appear valid and the techniques employed are intriguing. Significance: While the subject matter may not align with my expertise the papers relevance to the learning theory community is evident from the cited related work. Presentation and Clarity: Despite the intriguing topic the papers main sections (except for 5.1) lack sufficient context and highlevel explanations of the proof techniques. While brevity can be appropriate for theoryheavy conferences the presentation may not be accessible enough for NeurIPS. Questions and Suggestions for Improved Accessibility and Clarity: Include a definition of shattering in Definition 2.1 in relation to projection. Provide a clear definition of extremal classes including an equation to describe the mapping to the Icube. Formally define (unlabeled) sample compression schemes within the paper. Expand Section 4 as well as other sections (3 5 and 6 apart from 5.1) to include text introducing the results their significance and the main ideas behind the proofs. Consider moving full proofs to the appendix if page limits are a concern.", "Paraphrased Summary: The study investigates methods for compressing unlabeled samples in certain important classes of functions: external classes (where no function can be obtained from another by relabeling) and intersectionclosed classes (where the intersection of any two functions is also in the class). These classes are significant for understanding the sample compression conjecture which posits that every function class has a labeled sample compression of size proportional to its VapnikChervonenkis (VC) dimension. While external classes have labeled sample compressions linear in their VC dimension maximum classes and extreme intersectionclosed classes also have unlabeled compression schemes linear in their VC dimension. The main contributions of the paper are: Demonstrating that intersectionclosed classes can be embedded into extreme intersectionclosed classes with a modest increase in VC dimension. Simplifying the construction of unlabeled compression schemes for maximum classes. Showing that every external class that contains a maximum class has an unlabeled compression scheme of size proportional to its VC dimension. Assessment: The paper advances our understanding of unlabeled sample compression for external classes a key area in sample compression research. It introduces novel results and techniques that could aid future investigations in this field."], "r5rzV51GZx": ["Paraphrase: Summary This study focuses on the challenge of reliable collaborative reasoning in the presence of corrupted data. It introduces a cleansing technique for features specifically tailored for this problem. Using the block sparsity structure a nonlinear decomposition method is proposed to detect compromised agents. Experiments demonstrate the methods effectiveness in identifying corrupted data. Strengths The study addresses a significant practical issue: handling data from both reliable and malicious agents. The nonlinear decomposition method for identifying compromised agents is plausible and expected to be effective. Weaknesses The originality of the paper is unclear. It would be helpful to clarify whether the problem of robust collaborative reasoning in the presence of both reliable and adversarial agents has been studied before. From equation (5) it appears that the proposed framework attempts to find a combined feature that aligns with a certain manifold and is close to a reference feature. However the manifold projection may yield similar results for adversarial subfeatures. The authors should provide further discussion on this. Writing The structure of the paper (especially the introduction of the method in Section 2.3) can be confusing. The notations are not straightforward particularly with multiple symbols denoting features. The citation format may not be consistent with conference guidelines (e.g. ICLR vs. NeurIPS). Theoretical Analysis The paper provides extensive theoretical analysis but it would be beneficial to explain its implications more clearly. The authors should provide an intuitive understanding of how the sparsity parameter \u03b1 affects the performance of the proposed method. Experiments Ablation studies demonstrate the effectiveness of CoPur compared to manifold projection. It would be helpful to extend the analysis to scenarios with varying corruption levels for reliable and adversarial agents. Additional analysis such as comparing optimization efficiency would enhance the papers credibility.", "Paraphrase: Summary: This study presents CoPur a verifiable robust inference method for collaborative feature purification. CoPur exploits the blocksparse nature of adversarial perturbations and redundancy within feature vectors (assuming features lie on a lowerdimensional manifold). The paper provides theoretical evidence for CoPurs ability to accurately recover true feature vectors. Experimental results demonstrate its effectiveness. Additionally an untargeted distributed featureflipping attack is introduced. Strengths and Weaknesses: Pros: Theoretical analysis demonstrates that recovered benign features are close to true features with bounded error. Featureflipping attack highlights the vulnerability of manifold projection defenses. Cons: Theoretical results require more clarity and rigor. Experimental results are limited. Questions: 1. The main contribution (Eq.6) assumes blocksparse adversarial perturbations (e). Theoretical results could more explicitly define the sparsity of e. 2. The proposed featureflipping attack lacks sufficient evaluation. While experiments show CoPurs resilience to this attack there is no evidence that it is stronger than other attacks. 3. The impact of adversarial feature sparsity on robust accuracy is not explored. 4. Figure 2 suggests CoPur outperforms Oracle at low amplification. Can you explain this Would CoPur applied to unattacked features improve standard accuracy 5. The solution to Eq.6 is not provided in the main text.", "Paraphrase: Summary: This research addresses feature fusion in distributed machine learning environments. It introduces a feature reconstruction technique to restore noisy or incomplete features provided by various local clients. The approach entails decomposing observed features into underlying noisefree features and blockwise sparse noise components. Subsequently a signal recovery method (L21) is applied to estimate the noisefree features by imposing sparsity constraints on the estimated noise components. Additionally an AutoEncoder model is trained on the reconstructed features to generate a smoothed representation for enhanced classification accuracy. Theoretical analysis and experimental evaluations demonstrate the effectiveness of the proposed feature reconstruction method. Strengths: Recovered noisefree features provide more accurate information for classification than aligning noisy features with normal data. Utilizing the blockwise sparsity structure of noise components in the L21regularized regression process enhances feature recovery. Theoretical analysis of noisy conditions clarifies the practical implications of the proposed method. Weaknesses: The connection between the novel feature corruption attack and the feature recovery method (CoPur) is unclear. The effectiveness of CoPur against distributed feature flipping attacks (as shown in Table 2) is not addressed. The number of malicious agents that the CoPur defense method can tolerate or its breaking point is not specified. Questions: 1. How does the CoPur method perform against the distributed featureflipping attack 2. How many malicious agents can the proposed CoPur defense method tolerate", "Paraphrased Summary: This research aims to enhance the stability of collaborative inference deep learning models against attacks during the inference phase. The authors introduce CoPur a defense strategy based on preprocessing. CoPur learns purified embedded features from observed embedded features using a generative model. These purified features are then utilized by the Fusion Center for accurate predictions. Theoretical analysis demonstrates the methods ability to recover true features precisely. Empirical evaluations using targeted and nontargeted attacks showcase the effectiveness of CoPur. Strengths: 1. CoPur offers a reliable defense against inference attacks in collaborative inference tasks. It utilizes a preprocessing approach minimizing computational costs and preserving original information. 2. Comprehensive theoretical analysis proves the methods ability to recover true features accurately. 3. The evaluation employs a nontargeted distributed featureflipping attack allowing for comparison with baseline and proposed defenses. Weaknesses: 1. The method assumes minimal redundancy among features held by IoT devices. However it does not provide a mechanism to specifically address this issue or evaluate its impact on performance. 2. Communication requirements are not considered particularly in communicationlimited IoT applications. 3. Some related works and attacks are not discussed or compared. 4. Writing and presentation require improvement including unclear figure captions and inconsistencies in notation. Questions: 1. Why are wellknown robust aggregation methods not applicable to Horizontal Federated Learning (HFL) 2. How does the proposed method obtain and exploit redundant information 3. Is the defense effective against onmanifold attacks 4. Why were PGD and Autoattack not considered for nontargeted adversarial noise 5. How does CoPur compare to other preprocessingbased adversarial defense methods 6. Have adaptive attacks been considered for a more comprehensive evaluation of robustness", "Paraphrased Statement Summary: This study investigates robust inference in the context of vertically split features a scenario prevalent in applications like IoT. It examines three threat models: distributed adversarial attacks missing feature attacks and combined attacks. To address these threats the study introduces a robust inference algorithm rooted in sparse recovery and establishes its recovery capabilities. Empirical results demonstrate the proposed methods superiority. Strengths: Originality of the research problem: Robust inference in vertically split feature settings. Theoretical foundation and superior empirical performance of the proposed method. Weakness: Lack of theoretical guidance for hyperparameter selection in the recovery algorithm. Questions: Is it possible to provide theoretical insights to guide the selection of critical hyperparameters (e.g. delta) ensuring certified recovery under all conditions"], "peFP9Pl-6-_": ["Paraphrased Statement: Summary: This paper introduces the OGradient sampling method which enables defining equality constraints. It uses a twocomponent strategy to minimize distance to a manifold while decreasing KL divergence as rapidly as possible (using a velocity field). The paper presents formulations using Langevin dynamics (OLangevin) and SVGD (OSVGD). The paper provides theoretical analysis of OGradients convergence using Proposition 5.2 to define a limit that formulates an orthogonalspace Stein equation. This is then used to generalize the Fisher divergence. The experimental evaluation used synthetic and real scenarios. In synthetic cases OGradient was compared to constrained Langevin dynamics and constrained Hamiltonian Monte Carlo. OGradient showed better convergence when sampling from inside or outside the manifold. Baseline methods had nearzero Mean Absolute Error (MAE) when sampling from inside the manifold but were inapplicable when sampling from outside. Realworld experiments were conducted for income classification loan classification and image classification. OGradient methods generally outperformed alternatives. Strengths and Weaknesses: Strengths: Novel application of variational gradient descent to sampling with equality constraints. Interesting approach to define sampling in terms of closeness and orthogonality to a manifold. Weaknesses: Limited accessibility to readers outside the specialized subcommunity working on variational inference and sampling. Insufficient explanations in certain sections such as: Why the FokkerPlanck equation (FPE) of dxt does not follow Equation 10 (line 150). The functions used to compute the CV score in Income Classification with Fairness Constraint. The functions used to compute the errors in Loan Classification with Logic Rules and \"PriorAgnostic Bayesian Neural Networks.\"", "Paraphrase: Summary: This paper introduces algorithms for sampling from a target density while satisfying constraints defined by the equation g(x)0. It utilizes variational formulations of Langevin dynamics and Stochastic Variational Gradient Descent (SVGD) to create an orthogonal vector field with two components: an orthogonal component that guides samples toward the constraints and a tangential component that distributes samples according to the target. The algorithm is straightforward requiring only the evaluation of the targets score constraint g(x) and its gradient. The paper also establishes convergence rates based on a new characterization of conditional measures which seems to generalize the Stein characterization. Strengths and Weaknesses: The concept is novel and potentially impactful for conditional sampling under constraints. The algorithms are easy to use and effective in the provided examples. The presentation is clear and wellmotivated. Concerns: The soundness of the theory is questionable due to potential errors or unclear steps in the proofs. The main result relies on two assumptions: the Poincar\u00e9 inequality for conditional densities and an inequality in Equation 17 on the constraint g. Questions: Proof of Proposition 5.4: The authors appear to apply the Poincar\u00e9 inequality incorrectly. Statement of Theorem 5.5: The theorem should be modified to correct an incorrect claim. Inequality in Equation 17: A factor of 2 is missing rendering the proof in the appendix invalid. Upper bound in Theorem 5.6: The conclusion is incorrect the upper bound holds only for the minimum over [0T] not for any t in the interval. Inequality in Equation 17: The assumption is inappropriate it becomes invalid for smoother g. It can be fixed by scaling the lefthand side and modifying the last term.", "Paraphrased Statement: This research introduces a new variational approach for sampling from spaces with specified constraints. The proposed OGradient gradient flow allows sampling from manifolds defined by general equality constraints. This OGradient can be implemented using either the Langevin dynamics or the Stein variational gradient descent (OLangevin and OSVGD). Unlike existing methods OLangevin and OSVGD do not require initialization of samples on the constraintdefined manifolds. Additionally the paper establishes that the OGradient converges to target distributions at a rate of approximately O(1T) where T is the number of iterations. In practical applications OSVGD and OLangevin have demonstrated their ability to generate samples that closely approximate target distributions while adhering to specified constraints. Strengths and Weaknesses: Strengths: 1. Addresses the important topic of variational samplers. 2. Improves upon previous methods by eliminating the need to initialize samples on the target manifold. Weaknesses: 1. May negatively impact the original task (e.g. decreased accuracy in income classification). 2. Computational cost in comparison to baselines is unclear. Questions: 1. Definition of \"illposed\" in Section 4.1 regarding \\(\\fracdd\\piq\\pi \\) and \\(KL(q \\pi)\\). 2. Interpretation of Eq. (5) and the rationale behind choosing \\(\\psi(x) \\alpha sign(x)x1\\beta\\). 3. Explanation of \"projection intuition\" on line 149. 4. What is meant by \"energy distance\" in line 252", "Paraphrase: Summary: The paper proposes a method called \"orthogonal space gradient descent\" for optimizing machine learning models. This method involves creating a new direction perpendicular to the original gradient direction. Strengths: The paper is wellwritten and uses consistent language and notation. Weaknesses: The paper does not provide sufficient information about the complexity of the algorithm. The applications of the method may not be wellsuited for a conference such as NeurIPS. Questions: Due to limited expertise in the field the reviewers cannot provide detailed questions or comments on the technical aspects of the paper."], "zz0FC7qBpkh": ["Summary This paper introduces a new approach for Invariant Risk Minimization (IRM) called MirrorReflected IRM (MRI). MRI addresses inconsistencies in the original IRM formulation leading to improved performance. Strengths: The authors claims about IRMs overspecification and the benefits of reformulating it as a perturbation method are supported by the literature. The empirical results demonstrate MRIs effectiveness in enhancing accuracy compared to IRM. Weaknesses: The paper relies heavily on perturbation theory concepts making it challenging for readers who are not familiar with the field. The notion of \"mirrorreflected\" constraints is misleading MRI imposes a different condition compared to IRMs original formulation. The experiments only include a limited subset of available testbeds raising concerns about the generalization of the findings. Questions: 1. The authors claim that MRI is not a true mirror of IRM because it imposes constraints on label perturbations instead of prediction output perturbations. However this alternative condition may not be the intended \"mirror.\" 2. Definition 3.2s validity relies on assumptions about the loss function L which should be explicitly stated in the definition. 3. There is a discrepancy between Figure 3 and Table 1s results for IRMrelaxed in most scenarios. Suggestions: 1. Clarify the conservation variable for all \"conserved\" expressions. 2. Revise the notation to avoid confusion such as distinguishing between the test function and functional derivative uses of \\delta. 3. Provide a more intuitive explanation and illustration of Figure 2. 4. Include Table 3 in the main text and conduct further experiments on a wider range of testbeds to enhance the generalizability of the findings.", "Summary This research focuses on generating data from a different distribution (outofdistribution generation) based on an assumed datagenerating process where Y influences X. The authors propose an \"invariance regularization\" method to learn features (zi) that maintain a consistent relationship with Y across different environments under specific assumptions (linear relationship between latent variable and input large number of environments). They demonstrate the effectiveness of their approach through experiments on color MNIST and shapetexture linear regression datasets. Strengths The proposed generating process and invariance regularization complement the work on invariance regularization for the X Y generation process. The paper is wellstructured and clearly written. Weaknesses Misleading Result Analysis: The claim that the MRI method has lower loss than IRM which is consistent with its better accuracy is incorrect. The accuracy comparison on test domains is more relevant in this scenario and IRM actually performs better than MRI in this regard. Incomparability with IRM: The authors claim to correct a \"flaw\" in IRM but this is misleading because the two works consider different datagenerating processes. IRM assumes Y is generated after X while MRI assumes the opposite. These different processes require different invariance regularization methods. Incorrect Claim: The conservation of Ee[oy]EPe(o)[Ee[yo] \u00b7 o] is not a necessary condition for the conservation of Ee[yo]. Pe(o) can vary across environments even if Ee[yo] remains invariant. Questions The weaknesses listed above raise concerns about the validity of the proposed approach and the conclusions drawn from the experiments.", "Paraphrase: The authors expand on the findings of Kamath et al. (2021) on environmentindependent output invariance using Arjovskys Invariant Risk Minimization (IRM) principle. They introduce a new principle called Mirror Reflected IRM (MRI) based on label perturbation equivariance. They mathematically demonstrate that MRI can eliminate unimportant feature dimensions in linear structural equation models more effectively than Rosenfeld et al.s (2020) IRMv1 method. For nonlinear cases they propose optimization using the Augmented Lagrangian method with constraints for all environment pairs. Evaluations on a new shapetexture dataset and two versions of the colored MNIST dataset show that MRIv1 outperforms ERM and IRMv1 approaching the performance of an ideal model. Strengths: The approach is clearly explained and wellreasoned. The theoretical foundations appear sound. Empirical results support MRIv1s superiority over IRMv1. Weaknesses: The empirical advantage of MRIv1 over IRMv1 in linear scenarios (Table 2) seems limited. The applicability of MRI to loss functions other than binary crossentropy or squared loss is unclear. The authors refer to their approach as \"mirror reflection\" based on label perturbations which may not align with the standard definition of invariance. The tuning of penalty parameters (\u03bb) and Augmented Lagrangian parameters (\u03bc) in the experiments is not explained. Questions: Should \u03c3(\u03c8(o)) in equation (6) be \u03c3(o) Do the colors in Figure 2 represent different model parameters or initial conditions Why use E2 environments instead of E1 and what are the computational implications", "Paraphrase: The paper demonstrates that IRM imposes an unnecessary constraint leading to its limitations. To address this a modified approach is introduced that optimizes the appropriate constraints. The approach is theoretically justified in a linear setting and experiments on simplified tasks demonstrate its advantages and optimization capabilities. Strengths: Clear and wellmotivated analysis of IRMs shortcomings. Alternative formulation as a perturbationbased method aiding in IRMlike method analysis. Illustrative experiments on ShapeTexture data highlighting IRMs susceptibility to spurious features. Weaknesses: Limited realism of synthetic experiments more realistic benchmarks such as DomainBed should be included. Lack of comparison with other domain generalization methods (e.g. GroupDRO MMD IBIRM). Incomplete specification of the proposed methods practical implementation including constraint definitions and estimations. Potential unfair comparison of IRMv1 using the reformulated constraints instead of the original implementation. Questions: In Equation (9) the output perturbations are assumed to be linear but are they not constant for linear functions \\psi Equation (15) and Table 1 do not explicitly define \\vecc. Details on estimating constraints are also needed. Was IRMv1 implemented with constraints from Equation (15) or with the original approach Clarification on the assumed noise model (i.e. additive and nonGaussian) Undefined \\vecc(\\theta) in Equation (20) which should be \\vecc(f(\\theta)). The \"Convexity matters\" section makes general claims about convexity that the experiments do not fully support the claims should be refined accordingly."], "x7S1NsUdKZ": ["Paraphrased Summary: This study proposes a sampling strategy called IDS for labeling data in a limited sample scenario. The goal is to maximize the sum of label values by selecting the next point to label based on information gain. The algorithm shows improved performance for specific model structures such as lowrank matrices and feedback graphs. The authors also demonstrate the application of IDS in a realworld problem involving reaction discovery. Strengths: Thorough review of related work Demonstrates application in various model types Features a practical use case Weaknesses: Some sections could be clearer The realworld application could be explained more thoroughly Certain details (e.g. parameter values) are not fully explained Questions: Is the feedback graph model in this paper similar to the sleeping expert setting Could algorithms for sleeping experts be applied to this setup Does the realworld application differ from similar studies and if so how", "Paraphrased Statement: Summary: This paper presents a decisionmaking process where the objective is to select a limited number of samples to maximize the total reward (or minimize regret). The process involves selecting samples adaptively balancing exploration and exploitation using information gain to identify the next sample. Heuristics are proposed for cases where information gain calculation is difficult. The paper concludes with an experimental evaluation and a practical case study demonstrating the methods performance. Strengths and Weaknesses: Strengths: Clear presentation Thorough discussion of the relationship with MultiArmed Bandits (MAB) and related problems Interesting problem relevant to nontraditional machine learning areas Meaningful algorithm using information gain to minimize Bayesian regret Practical instantiations of the algorithm under various model assumptions Weaknesses: Insufficient detail on the computation of \\Deltat(x) and gt(x) in the algorithms pseudocode Incomplete explanation of whether expectations are taken with respect to the posterior in the definitions of \\Deltat(x) and gt(x) Limited discussion of the approachs limitations Question: The provided statement raises a question regarding the use of the posterior in Algorithm 1. Specifically it asks if the expectations in \\Deltat(x) and gt(x) are taken with respect to the posterior and if so why it is appropriate.", "Summary: The paper proposes a solution to an active learning problem where the goal is to maximize the total number of observed labels while using a limited labeling budget. The policy aims to select the most informative unlabeled data to label balancing the potential information gain with the expected immediate reward. Strengths: Wellwritten and clear explanation of the problem and proposed policy. Demonstration of the policys performance in various settings including linear models lowrank matrix problems and graphs. Experimental results show competitive performance against existing baselines. Weaknesses: Oversight of Relevant Literature: The paper fails to acknowledge relevant research on the \"active search\" problem which shares similarities with the problem addressed. Unclear Value of InformationTheoretic Approach: It is unclear if the proposed policy based on minimizing the ratio of expected regret to expected information gain offers any substantial advantage over a policy that minimizes total regret. Questions: How does the proposed policy compare to existing active search policies designed specifically for optimizing the sum of observed labels Would a variant of the policy that minimizes cumulative regret instead of instant regret perform similarly or better Miscellaneous: Improve plot aesthetics (larger labels larger legend). Correct grammatical errors."], "m8vzptcFKsT": ["Paraphrased Summary: This study examines the average robustness of neural networks to the number of layers (depth) the number of neurons per layer (width) and the initialization method. It provides theoretical bounds on robustness (measured by perturbation stability) for both \"lazy\" (Theorem 1) and \"nonlazy\" (Theorem 3) training methods. By interpreting these findings the study sheds light on conflicting results from previous literature. Experiments on a simple dataset support the theoretical results. The studys strengths include its novel findings and extension of existing knowledge on neural network robustness. It clarifies the role of width in robustness revealing a \"phase transition\"like phenomenon under certain training conditions. Weaknesses include a lack of analysis of training loss. To improve clarity the study could report on training validation losses after early stopping is applied. Overall this is a wellwritten and comprehensive study with a broad scope including various initialization schemes and training regimes. Limitations are welladdressed in an appendix.", "Paraphrase: The study analyzes the stability of overparameterized feedforward neural networks with ReLU activation functions under perturbations in network width depth and initialization. The results show that: NTK Regime (Narrow Networks): Wider networks are more stable. Depth can improve or diminish stability depending on initialization. Nonlazy Regime (Wide Networks): A criterion is provided to determine when a network will train in the nonlazy regime (with significant weight changes). The theoretical findings are supported by empirical experiments. The paper is wellwritten providing significant and insightful results that are properly interpreted and tested empirically. Strengths: Indepth theoretical analysis Empirical validation Clear interpretation of results Reproducibility (code provided) Weaknesses: Requires some clarification in terminology (average robustness vs. \u03b5stability) Unnecessary repetition of Definition 2 Ambiguous use of \"exacerbate\" Question: How do the results in [2] relate to this study particularly regarding the connection between loss surface flatness and average robustness", "Paraphrased Statement This paper examines how network width depth and initialization impact average robustness supported by theoretical bounds. The authors demonstrate that in underparameterized networks width reduces robustness while in overparameterized networks it enhances robustness. The effect of depth depends on network initialization and training method. In lazy training depth improves robustness with LeCun initialization but impairs it with NTK and He initialization. In nonlazy training the authors illustrate that width promotes robustness in a twolayer ReLU network. Strengths Originality: The authors provide novel theoretical bounds on network robustness with respect to width depth and initialization. Quality: The theoretical derivations are rigorous and supported by numerical evidence. Weaknesses Clarity: Certain statements in the paper are unclear. For example Definition 2 refers to measurement perturbation stability but indicates that low stability equates to high robustness. Significance: While the authors explore the relationship between width depth and initialization on robustness their numerical experiments only consider two choices of depth limiting the evidence. Additionally NTK initialization is not included in the validation. Questions 1. Can you clarify the apparent contradiction between Definition 2 and the statement in 4.1 regarding measurement perturbation stability and robustness 2. To strengthen the numerical evidence could you extend the analysis to include more depth choices and incorporate NTK initialization"], "uOJZ_zU9qZm": ["Original Statement: Existing automatic differentiation frameworks lack the versatility necessary for designing complex learning algorithms. This paper introduces Automatic Propagation (AP) software a generalization of AD that facilitates the construction of these algorithms. Paraphrased Statement: Current automatic differentiation frameworks limit the flexibility of creating learning algorithms. This paper presents AP software which extends AD capabilities and enables the development of intricate learning algorithms. Strengths and Weaknesses: In the reviewers opinion this paper primarily serves as a software demonstration for incorporating advanced learning algorithms. Therefore it lacks significant academic contributions. Question: Refer to the \"Strengths and Weaknesses\" section.", "Paraphrase: Summary: The authors propose a software framework (Proppo) that allows for the creation of propagation algorithms on any type of computational graph. This framework can be used to implement a wide range of learning methods. The authors argue that the limitations of current ML frameworks in handling generic computation graphs hinder research efforts. Proppo is showcased through examples of Monte Carlo gradient estimators and the Total Propagation algorithm. An experiment demonstrates how Total Propagation enhances the stability of an RNNbased system with nearchaotic dynamics. Strengths: The concept of a generic framework for propagation on computational graphs is innovative and provides the foundation for developing new learning algorithms beyond gradientbased approaches. Weaknesses: The paper lacks detailed information about the framework itself. The documentation including API semantics for implementing other learning algorithms and implementation details is missing. This makes it challenging to assess the quality and accuracy of the authors implementations. Performance benchmarks particularly for Monte Carlobased estimation methods are absent. Total Propagation and similar methods are not novel and while a generic framework implementation is useful its computational efficiency compared to other implementations is unclear. The authors do not present any novel learning algorithms created with Proppo instead they evaluate existing ones. Case studies demonstrating the frameworks utility in more detail would strengthen the paper. The Monte Carlo gradient estimation example while interesting for studying stability does not clearly demonstrate broader applicability. The authors should justify its relevance and explore these algorithms in a wider range of learning settings using the frameworks interoperability with PyTorch. Questions: Python Implementation: Since Proppo is fully implemented in Python how does its performance scale for large graphs Performance benchmarks should be included to address this. Message Implementation: The paper states that messages are implemented via parent node pointers. More detailed implementation information would be beneficial. Parallelism: The authors mention propagation managers working in parallel. How is this achieved Is threading used Is graph traversal optimized for efficiency", "Paraphrased Summary: This paper introduces Automatic Propagation (AP) an improved software tool for machine learning (ML) development. AP allows users to create custom learning algorithms or \"propagators\" which can be reused for various tasks. Traditional ML software heavily relies on surrogate loss and backpropagation for calculation neglecting other potential optimizations. AP addresses this limitation by providing a general messagepassing framework that supports a wide range of computations in ML. The core component of AP is the propagator which handles both forward computation (performing operations and storing data for backward computation) and backward computation (sending and receiving messages between nodes). The propagation manager orchestrates the entire process. The benefits of AP and its implementation Proppo are demonstrated through the creation of gradient algorithms for Monte Carlo gradient estimation. These algorithms show significantly improved gradient accuracy compared to the standard autodiff method with surrogate loss particularly for specific types of problems. Strengths: Wellwritten and wellmotivated Original and important contribution Validated experimental results Weaknesses: Lack of opensource code for further exploration Potential confusion in the code example due to grammatical errors Suggestions: Consider providing a link to opensourced Proppo code for easy implementation examples. Clarify the code example by indicating that Proppo can also perform standard backpropagation. Revise the sentence on lines 167169 for clarity."], "swIARHfCaUB": ["Summary This study presents a modified version of the Online FrankWolfe (OFW) algorithm called Online FrankWolfe with Arbitrary Delays (OFWAD). OFWAD enables delayed gradients by allowing them to arrive after several rounds. This algorithm overcomes limitations of the original OFW and addresses difficulties in Online Convex Optimization (OCO) related to projecting onto the decision set for feasibility. By updating similarly to OFW upon receiving delayed gradients and playing the latest decision OFWAD achieves the same regret bound as OFW. Strengths and Weaknesses Strengths: OFWAD extends OFW to the delayed setting reducing storage and computational requirements compared to nondelayed scenarios. Unlike previous techniques that convert nondelayed OCO algorithms to delayed settings OFWAD offers improved regret bounds for convex and strongly convex losses. OFWAD possesses the same regret bound as OFW contributing to the field of convex optimization and enhancing its practicality. Weaknesses: The study lacks empirical experiments to validate the proposed algorithms performance in practice. OFWAD focuses solely on extending OFW to the delayed setting neglecting potential delayed variants for other projectionfree algorithms. Questions The study does not provide qualitative or quantitative experimental results to assess the algorithms effectiveness. Elad Hazan et al.s \"Faster ProjectionFree Online Learning\" offers an efficient algorithm with a regret bound of O(T23) for general OCO with smooth cost functions. It would be beneficial to compare OFWAD to this algorithm to determine any improvements it offers.", "Paraphrased Summary: This research focuses on online convex optimization situations where the gradient of the objective function at the current point is only revealed after a variable delay. The authors introduce delayed versions of the online FrankWolfe algorithm for both convex (Algorithm 1) and strongly convex (Algorithm 2) optimization. If the maximum delay is sufficiently small the proposed methods can attain similar regret bounds as those for convex and strongly convex loss functions. Strengths: The issue of delayed feedback is wellmotivated. The paper is clear and easy to follow. The main contribution is the ability to recover regret bounds for small maximum delays in convex and strongly convex scenarios. The authors provide a comprehensive literature review and discuss their contributions. Weaknesses: A numerical comparison with the work of Joulani et al. [2013] could enhance the paper. Question: It is possible to apply similar ideas to develop a delayed version of the online mirror descent algorithm.", "Paraphrased Statement: This research paper examines online convex optimization focusing on situations where there are delays in receiving gradients. The authors investigate the Online FrankWolfe (OFW) algorithm which promptly executes the FrankWolfe update for all available gradients as they become available. They establish regret bounds for both convex and strongly convex scenarios demonstrating that these bounds remain comparable to the initial OFW regret bound when the delay falls below a specific threshold. Strengths and Weaknesses: The paper addresses an intriguing issue with a straightforward algorithm and impressive results. The writing is clear and the analysis appears sound. The resilience of the regret bounds to delays up to a certain threshold is both unexpected and advantageous. Questions: However the paper does not explicitly address whether OFW is the most effective algorithm for handling delayed gradient feedback.", "Paraphrased Statement: In this study researchers address the challenge of online convex optimization where an individual selects an action at each time interval and receives a reward. However the gradient information necessary for decisionmaking is delayed by a variable amount (under a fixed maximum). The objective is to minimize regret the difference between the rewards obtained under optimal choices and those obtained under the players selections. The researchers present a modified version of the FrankWolfe algorithm designed to handle these delays. Under common assumptions about gradient boundedness and set diameter they prove that their algorithm achieves regret bounds of O(T(34) dT(14)) in the convex case and O(T(23) d log(T)) in the strongly convex case. These bounds are improvements over existing results for online convex optimization with delays. When delays are small these rates approach those of the classic FrankWolfe algorithm. Strengths and Weaknesses: Strengths: Originality: Modification of the FrankWolfe algorithm to account for gradient delays. Quality: Technically sound and wellanalyzed. Clarity: Wellwritten and clear explanations. Weaknesses: Lack of empirical evaluations: The paper does not include simulations comparing the proposed algorithm to others. Questions: Is the FrankWolfe algorithm particularly suitable for delayed feedback due to the form of its update rule Could similar regret bounds be achieved with other online convex optimization algorithms Intuitively why do delays not lead to a significant increase in regret"], "p9lC_i9WeFE": ["Paraphrased Summary: This work presents a new generalization bound for Message Passing Neural Networks (MPNNs) on both continuous graphs and discrete graphs. The bound shows that the generalization error can be limited by a formula that improves with both the number of training graphs and the size of the graphs themselves. This suggests that MPNNbased graph classifiers can generalize well even with limited training data provided the graphs are sufficiently large. The bound is supported by empirical evidence from training MPNNs on simulated graphs. Strengths and Weaknesses: Originality: The derived bound is novel and incorporates a decay term that improves with increasing graph size unlike previous work. This is achieved through a new Monte Carlo analysis that treats nodes as samples. Quality: The mathematical results are solid but the experimental evaluation could benefit from realworld analysis. Clarity: The paper is wellwritten but could improve by presenting the contributions and implications earlier. Significance: The bounds are significant compared to existing work but connecting the results to realworld graph distributions would strengthen the impact. Questions: 1. Can you clarify the practical implications of your work and incorporate this discussion earlier in the paper 2. Can you provide further experimental evidence for the bounds either on a wider range of generative distributions or on realworld graph datasets", "Paraphrase: This paper studies the generalization performance of messagepassing neural networks (MPNNs) on graphs drawn from random graph models (RGMs). The authors introduce continuous MPNNs (cMPNNs) and demonstrate that the error between cMPNNs and their discrete counterparts decreases as the graph size grows (Theorem 3.1). This result is the primary contribution of the paper. Building on this they establish a generalization bound for cMPNNs (Theorem 3.3). Numerical experiments support the theoretical findings. Strengths: Rigorous Theoretical Analysis: The paper provides a solid theoretical foundation establishing the relationship between MPNNs and cMPNNs and deriving convergence and generalization results. Clear Definitions and Background: Despite the technical nature of the work the paper provides clear definitions and background information making it accessible to readers with varying levels of expertise. Novel Convergence Bound: The papers convergence bound improves upon previous results demonstrating a decrease in error with increasing graph size. Weaknesses: Practical Implications: While the generalization bound is an important result it is unclear how it directly applies to the design of MPNNs in practical settings. Assumptions: The theoretical results rely on several assumptions. The paper should discuss the validity and limitations of these assumptions. Questions: The papers citation format should be updated to conform to the NeurIPS 2022 guidelines. The authors should provide additional insights into the practical implications of their work. Which assumptions made in the paper are considered reasonable and which might be too restrictive in practice", "Paraphrase: Summary: A novel mathematical formula for estimating the accuracy of a specific type of neural network (MPNN) on graphs is presented. This formula shows that the gap between the networks performance on a small sample of graphs and its performance on all possible graphs decreases as the average number of nodes in the sampled graphs increases. This is different from previous findings which typically show that the performance gap increases with the number of nodes. This result relies on a mathematical proof that shows how close the output of an MPNN on sampled graphs is to the output of a continuous version of the network on the underlying graph model. Strengths: The formula for accuracy includes a term that decreases with the size of the graph unlike previous results. This is a new finding and may have significant implications. The paper introduces a continuous version of MPNNs with mean aggregation which is a commonly used type of MPNN. Previous work has only done this for a different type of MPNN (spectral GNNs). Unlike previous work this paper proves that the continuous and finite versions of the MPNN are close to each other regardless of the specific model parameters. This is important for obtaining useful accuracy estimates. The assumptions and data distribution used in the analysis appear reasonable. Weaknesses: The paper includes only a few small numerical experiments and it would be interesting to conduct more experiments with different types of graphs and MPNNs. The proofs in the paper are lengthy and involve complex calculations. Questions: In the papers appendix it is mentioned that the continuous MPNN is approximated with a finite MPNN on a large graph. This may make it easier for the finite MPNNs to closely match the continuous MPNN. It would be better to sample several large graphs and compare the finite MPNNs to an average of the continuous MPNN outputs on independently sampled small graphs. It would be helpful to explicitly state in the introduction that the analysis holds for mean aggregation."], "rYkGxHPnCIf": ["Summary The paper presents two main contributions: 1. Theoretical Analysis of NonEquilibrium Importance Sampling (NEIS): Provides theoretical conditions for the existence of an optimal velocity field that yields an estimator with zero variance. Establishes a connection between NEIS and optimal transport maps. 2. Practical Implementation of NEIS with Learnable Velocity Field: Explains how NEIS can be implemented in practice using a neural network to parameterize the velocity field. Demonstrates how to train the parameters to minimize the estimators variance. Strengths Clear and wellwritten presentation of original theoretical findings. Theoretical analysis addresses important aspects of NEIS. Weaknesses Empirical Evaluation: Claims of effectiveness in high dimensions are not supported by empirical evaluation conducted on lowdimensional models. Slow training times are not mentioned in the main text even for simple models. Vanilla estimator (importance sampling with Gaussian proposal) is not a strong baseline. Practical Considerations: Training uses large minibatches which may raise concerns about variance and computational cost. More training details should be provided in the main text. The cost of training should be explicitly discussed. Questions Have you explored using smaller batch sizes Do theoretical results hold for finite integration limits used in practice", "Summary This paper introduces nonequilibrium importance sampling (NEIS) a method that leverages flow lines to improve the estimation of normalizing constants. By evolving samples along these flow lines NEIS avoids the limitations of standard importance sampling and offers the potential for lower variance estimators. Strengths Clear presentation of technical results Promising approach with demonstrated efficacy in reducing variance Thorough theoretical analysis in the appendix Concrete examples and explicitly solvable dynamics Weaknesses Lack of discussion on the impact of discretizing flow along the vector field Difficulty in training the neural ODE with convergence challenges and long runtimes Neglect of the importance of the proposal distribution Observed bias in NEISbased estimators despite theoretical predictions of unbiasedness Questions Methodology for selecting flow fields in direct gradient descent (GD) methods for toy models Rationale for not using neural ODE in the funnel toy model Explanation for observed bias in NEISbased estimators given theoretical unbiasedness claims", "Paraphrase: The research aims to calculate the normalization constant of a distribution given only its unnormalized density. It builds on the NonEquilibrium Importance Sampling (NEIS) method which generates samples from a known normalized distribution and propagates them along chosen vector fields. The target density is estimated as the weighted average of the propagated samples. This papers key contribution is its use of the fact that NEIS provides unbiased estimates regardless of the vector field selected. By leveraging this freedom the authors develop a parametric model of the vector field that minimizes the variance of the resulting estimator. Theoretically the paper demonstrates that an infinitetime NEIS estimator with zero variance is achievable under specific conditions. Furthermore it proves that every local minimum of the variance minimization problem is also a global minimum. Practically though the paper focuses on finitetime estimators. Empirical evaluations in 2D and 10D Gaussian mixtures and a 10D Funnel distribution show that the proposed method outperforms other techniques. The authors also reduce computational costs by parameterizing the vector field as a linear model for the Funnel distribution. Strengths: Clear presentation Emphasis on intuition in the main body Detailed appendix Weaknesses: Limited empirical evaluation Computational expense due to backpropagation through ODE trajectories and continuous integral discretization Question: Does it make sense to vary the time interval [t t] given the networks ability to generate arbitrary vector fields", "Paraphrase: The paper introduces a recently developed method Nonequilibrium Importance Sampling (NEIS) for estimating intractable partition functions. NEIS offers an impartial estimator with limited variance often outperforming other estimators (which may have infinite variance in some cases). Strengths: NEIS is original and connects with emerging concepts like Neural ODEs and neutral transport maps. The mathematical development is sound and connects to related work. The experiments demonstrate NEISs unbiased nature and low variance. Weaknesses: The papers mathematical explanations may be too condensed with many details relegated to supplementary materials. The experimental densities are limited to basic mixtures of Gaussians and Neals funnel and do not represent a wider range of complex distributions. The experiments compare NEIS to AIS with only 100 transitions which raises questions about the fairness of the comparison. The paper highlights NEISs computational cost limiting its practical use. It would be beneficial to compare NEIS with a trainable AIS approach to assess efficiency. Questions: Why were specific densities selected for the numerical examples Could AIS produce better results with extended runtimes How does NEISs training approach differ from diffusion models Is the bias in Neals funnel due to the variational approximation or other factors"], "s_mEE4xOU-m": ["Paraphrase: This paper investigates how to balance load across multiple agents in a network. It does this by using a Markov potential game framework and a fair workload distribution as the potential function. The paper also proposes a multiagent reinforcement learning (MARL) algorithm based on this and allows for decentralized learning. Additionally the paper creates an eventbased simulator and realworld network setup to test the proposed algorithm against several MARL baselines. Strengths: The network load balancing problem is rigorously formulated as a Markov potential game with sound proofs. Its an interesting and wellmotivated application for MARL with potential benefits. Weaknesses: Concerns about the representativeness of the baselines used for evaluation. The practical benefits in terms of communication overhead and training time could be better highlighted. Detailed Comments: Overall: The paper is engaging to read and the problem is welldefined. The problem formulation as a Markov potential game seems promising. Evaluation fairness: The baselines used for evaluation may not represent the most advanced techniques. The proposed algorithm should be compared to more recent competitive baselines. Practical benefits: The reduced communication overhead and training time benefits should be emphasized more. Robustness: The paper discusses the robustness against dynamic changes in the network setup but does not fully address its significance in realworld environments. Notation confusion: A few notation issues need clarification. Minor notes: Some abbreviations are not explained such as \"NE.\" The superscript notation in Equation 6 should be defined earlier. Questions: Are there reasons why the paper cannot compare against more recent CTDE schemes Can the authors provide further comments on other concerns raised", "Paraphrased Statement: Summary This paper introduces a decentralized Multiagent Reinforcement Learning (RL) approach for networklayer load balancing modeled as a Markov Potential game. Traditional load balancers lack visibility into workload and server performance making them susceptible to misconfiguration due to network heterogeneity and flexibility. Centralized Traffic Distribution Engineers (CTDEs) incur additional overhead from centralized communication. This approach tackles these issues by utilizing a localized variancebased fairness function within each load balancer. Maximizing this function approximates the Nash equilibrium of the Markov potential game minimizing the potential function. Strengths and Weaknesses Strengths: Substantial improvements over existing load balancing algorithms Robust theoretical foundation based on Markov potential game modeling Clear and logical presentation Weaknesses: While the paper mentions the potential communication overhead of CTDEs it does not provide convincing evidence in the main text. The evaluation section refers to an evaluation in the appendix which would strengthen the argument. The \"largescale\" DC network described in the paper only comprises 20 servers which may not accurately represent realworld data centers that often have thousands of servers and load balancers. Fault tolerance considerations are not addressed. Questions: Can this approach be extended to multidata center setups leveraging the lowobservability approximation Have offline learning techniques been considered to initialize the system Why were 23 agents used in Figure 2b Were different traffic types (e.g. databaseintensive or video streaming) considered Are the numerical values in Tables 1 and 2 mean or median Nitpicks: Page 3 line 100: \"set of servers\" should be \"set of LB agents 1...M.\" Realworld setup specifications would be helpful for comparison to actual data centers.", "Paraphrase: Summary: This study addresses the challenge of balancing load across multiple diverse servers and load balancers. It employs a multiagent reinforcement learning approach particularly focusing on a Markov potential game. In this scenario multiple load balancers distribute tasks to a group of servers. There may be overlap in the servers assigned to different load balancers leading to partial observability of the system status. Using cumulative total fairness as a potential function where fairness is measured by either variance or product fairness the article demonstrates that the task allocation game aiming to minimize makespan while maximizing variance or product fairness is a Markov potential game. Strengths and Weakness: Managing load in a network with multiple load balancers and overlapping servers is a complex task. The interactions between load balancers make finding a closedform solution for load balancing challenging. This approach of creating a potential game within a reinforcement learning framework is innovative and appears to be new. The authors put forward a dispersed load balancing strategy where each agent independently develops a policy using policy gradient techniques. The reward function is determined by variance or product fairness per load balancer. They show that optimizing these local fairness metrics is sufficient for minimizing makespan a global metric. Questions: 1. Are all servers assigned tasks by all load balancers 2. How do the results change when the servers overlap is partial This problem may be more difficult to solve.", "Clarifications on Weaknesses: 1. Poor Evaluation: Limited scaling experiments: The evaluation only considered 2 load balancers which may not reflect practical data center scenarios. No real traffic workload: The evaluation used a mockedup small testbed that does not simulate actual traffic distribution or scale. Limited traffic and IOCPU variation: Experiments did not explore a wide range of traffic and server utilization conditions. Lack of QoS evaluation: The paper did not evaluate 99th percentile behavior an important metric for load balancers. 2. Invalid and Inconsistent Assumptions: Fixed server workload: The paper assumes that each server can process a fixed amount of workload which ignores the variability of request characteristics and the impact of different traffic types. Impractical active probing: The paper assumes that active probing for server state is impractical but there is no justification for this assumption. 3. Limited Insights: Lack of comparison to alternative approaches: The paper does not compare MPG to other methods like meanfield theorem or RLbased solutions. No evaluation of RL vs MARL overhead: The paper does not assess the performance and overhead implications of using MARL over RL. Safety concerns: The paper does not address potential risks or consequences of RL making bad decisions. 4. Writing Issues: Unclear introduction and related work section: The paper needs to provide a clearer introduction and describe the relevant related work. Undefined abbreviations: The paper uses abbreviations (e.g. NE) without defining them initially.", "Paraphrase: This study explores load balancing in data centers using multiagent reinforcement learning (RL). The goal is to minimize the time it takes to complete tasks (makespan). The authors demonstrate that this problem can be formulated as a Markov Potential Game by defining a fairness potential function. They then leverage this result to develop a distributed algorithm for approximating Nash equilibrium policies. The study includes extensive experiments that show the effectiveness of the algorithm. Strengths: Combines theoretical and practical aspects. Novelly models load balancing as a multiagent RL system. Weakness: The result that the proposed framework is a Markov potential game is somewhat expected since load balancing games are typically potential games. Question: The authors were unable to provide theoretical guarantees for their algorithm due to unspecified technical difficulties."], "vdxOesWgbyN": ["Summary Paraphrase: This research paper examines five model extraction attacks on split federated learning where the attacker cant access model predictions from the central server. Instead the attacks use gradients to reconstruct the serverside model. Each attack targets different data assumptions. Strengths: Investigates five model extraction attacks specifically designed for split federated learning. Evaluates attacks in various scenarios including nonuniform data distributions adversarial attacks and situations where the attacker does not know the model architecture. Weaknesses: Prediction Assumption: The paper assumes the clientside attacker cannot access model predictions which is not entirely accurate during the inference phase where clients can obtain predictions. A comparison with existing model extraction attacks using predictions would strengthen the analysis. Novelty: The proposed attacks heavily leverage existing model extraction techniques limiting their originality. TrainME the most effective attack is essentially a basic approach that does not require gradient queries. This may suggest model extraction in split federated learning is a less complex issue. Data Assumption: The \"no data\" assumption is unclear. Clientside attackers should have access to at least the clients local data. Baseline Comparisons: Comparisons with baseline approaches would enhance the analysis. For instance evaluating the attack performance when the attacker uses the clients data to train a model. Scalability: Experiments only consider a limited number (10) of clients. It would be valuable to assess attack performance with a larger number of clients as is typical in realworld settings. Performance on ComplexDatasets: The attacks perform poorly on complex datasets (e.g. ImageNet CIFAR100) and models (e.g. ResNet). Narrow Scope: The attacks are only applicable to split federated learning limiting the papers scope. Questions: Please explain in more detail the \"no prediction\" assumption (weakness 1) and the \"no data\" assumption (weakness 3).", "Paraphrased Statement: Federated learning involves multiple clients and a server collaborating to train a model without sharing data. However it has privacy concerns as the server and clients have access to the entire model. Split federated learning aims to address these concerns by dividing the model into parts held by the server and clients. Despite the belief that split federated learning protects against model extraction attacks research has shown that malicious clients can extract surrogate models with similar accuracy to the server model. Strengths: Introduces five model extraction attacks in various scenarios including whether the attacker has local training data. Demonstrates the effectiveness of the attacks through extensive experiments considering factors like the number of queries and model complexity. Provides a clear and accessible paper structure. Weaknesses: Lacks comparison with a baseline attack that trains a model locally using available training data which is essential for assessing the attacks effectiveness. Does not analyze the complexity of the proposed attacks which would elucidate their attack cost. Overlooks the potential defense against the attacks offered by Byzantinerobust federated learning methods. Insufficient explanation of why ResNet architecture exhibits greater resistance to model extraction attacks. Requires a more explicit discussion of the societal implications. Additional Questions: Accuracy of the locally trained baseline model versus the proposed attacks Complexity analysis of the proposed attacks Effectiveness of Byzantinerobust federated learning methods against the attacks Reasons for ResNets better robustness", "Paraphrase: Summary: The researchers conducted five model extraction attacks on split federated learning (SFL) systems. These attacks utilize gradient information from the server to steal models. The paper also suggests a method for attackers to obtain training data for their attacks. Strengths: Thorough analysis of model extraction attacks in SFL with experimental results. Weaknesses: Attack methods are presented conceptually making it challenging to understand how attackers generate data and conduct attacks. Questions: 1. CraftME: Explain how randomly initialized inputs and a random surrogate model can generate plausible labels. How are these labels used to train the surrogate model 2. Gradient Matching Model Extraction (GMME): How is the gradient term \\nablaxi L(V(C(xi)) yi) computed without access to the prediction query V 3. Trainingbased Model Extraction (TrainME): What is the amount of training data required for the surrogate model compared to the SFL systems data size", "Paraphrase: Summary: The researchers highlight the vulnerability of Split Federated Learning (SFL) to model extraction attacks where malicious clients can reconstruct the server model by querying gradient information. They present five different types of model extraction (ME) attacks for SFL leveraging various gradient schemes and data assumptions. They show that ME attacks can achieve high accuracy and low degradation even in scenarios with no or limited data. Strengths and Weaknesses: Strengths: Original work investigating attacks on SFL. Clear and wellwritten. Weaknesses: Missing related work on SFL. Questions: The researchers thoughts on using regularization to mitigate ME attacks. Whether LEAF datasets were used for experiments as they are commonly employed in FL research."], "xqyDqMojMfC": ["Paraphrased Statement: In this study the authors investigate a category of constrained stochastic optimization issues with data drawn from a Markov chain. To solve this problem they present a conditional gradienttype method with proven bounds on its convergence rate. Numerical experiments are performed to demonstrate the effectiveness of the algorithm. Strengths: Significance of constrained stochastic optimization problems with Markovian data in machine learning. Comprehensive theoretical analysis. Clear and wellwritten paper. Weaknesses: Despite being labeled as \"projectionfree\" the proposed algorithm still employs a projection step solved by conditional gradient methods which is a known technique. The experimental results are inconclusive. Although the proposed method has a time complexity of O(n) on the unit ball its effectiveness is not evident compared to the projection method. A more meaningful comparison would involve considering a scenario where the computational complexities of the two methods differ significantly such as nuclear norm balls. Questions: 1. Clarification on the misleading \"projectionfree\" label. 2. If Assumption 2.4 and Proposition 2.1 reduce the problem to classical constrained smooth stochastic optimization why is the Markovian nature of the data significant Is Assumption 2.4 overly restrictive or are there additional technical complications", "Paraphrased Statement: Summary The paper investigates optimization problems where the objective function is nonconvex and subject to constraints. Data is generated iteratively by a Markov chain that depends on the current parameter. The paper presents two algorithms and analyzes their convergence properties. Strengths and Weaknesses Originality The paper leverages a technique to decompose stochastic errors using a Poisson equation associated with the underlying Markov chain. This technique is not novel but its application in this optimization setting is innovative and addresses a theoretical gap. Clarity The paper is generally wellwritten but some sections could benefit from improved clarity. Quality The paper appears theoretically sound with all claims supported. The experimental setup is thoroughly described. Significance The paper contributes to the optimization theory for problems with Markovian data. It may inspire future research in the machine learning community. Questions 1. The presentation could be improved by providing more explanations of certain terms and concepts (e.g. L in (7) Assumption 2.1 and 2.2 before Proposition 1.1 Algorithm 1 definition of z in Definition 1). 2. The uniformly bounded gradient assumption in Assumption 2.3 could potentially be relaxed to a bounded gradient assumption at the optimal point. 3. The choice of step sizes in (18) is unconventional and requires justification. 4. The paper mentions that the proposed setting includes reinforcement learning (RL). However it is unclear how the proposed methods could be applied to optimize policies in RL as the value function is often a nonconvex function of the policy. 5. A lower bound analysis on the number of oracles required under statedependent Markov data would enhance the understanding of the optimality of the proposed methods.", "Paraphrased Statement: Summary: This research investigates conditional gradient techniques for solving stochastic optimization problems involving data that follows a Markov chain with statedependent dynamics. The study utilizes a moving average gradient estimator to account for the expected value of the stationary distribution of the random vector. Significant contributions include nonasymptotic bounds on gradient complexity and linear minimization oracle complexity. Strengths: Pioneering nonasymptotic analysis of conditional gradient methods for stochastic optimization with statedependent Markov data. The moving average estimator is wellsuited for Markov data and provides insights for extending gradientbased approaches to this setting. Weaknesses: Assumption 2.4 is restrictive requiring the existence of a stationary distribution for any parameter value. The motivation behind Algorithm 1 is not sufficiently explained hindering its application to different contexts. Questions: Is the available data a predefined sequence of statedependent Markov data or is each data point provided online How does the gradient mapping convergence criterion in the FrankWolfe gap compare to the performance prediction criterion used in previous research (DX20) Minor Comments: Fix the spacing issue in equation (11).", "Paraphrased Statement: Summary: The study employed a stochastic conditional gradient method for solving nonconvex optimization problems with constraints within a Markovian setting where data originates from a Markov chain. The authors establish the complexity bounds for stochastic firstorder oracle (SFO) and linear minimization oracle (LMO) for this algorithm in attaining epsilonstationarity. Strengths: The study tackles nonconvex constrained optimization under Markovian sampling providing complexity analysis for the corresponding algorithms. Weaknesses: Question 1: The construction of an auxiliary Markov chain for bounding Markovian sampling error is not novel having been utilized in unconstrained problems. Question 2: The motivation for introducing a constant constraint is unclear. Question 3: The experimental evaluation is limited in scale. The claimed computational advantage of the projectionfree algorithm is not demonstrated empirically. Additional References: [1] Doan et al. (2020) [2] Xu et al. (2019) [3] Wu et al. (2020)"], "yfNSUQ3yRo": ["Paraphrased Statement: The researchers devised an efficient method that utilizes an attention layer to assign weights to individual data samples during training. The method leverages the memory capabilities of neural networks to learn these weights effectively. It simplifies the training process by eliminating the need for preselecting reliable examples. The researchers provide theoretical support for their method and demonstrate its superiority to existing approaches through experiments. Strengths and Weaknesses: 1. Innovation in training an attention layer to output weight scores. 2. Simplicity and empirical effectiveness of the proposed method. 3. Clear and wellorganized presentation. Questions: 1. How does the attention branch specifically differentiate mislabeled samples and challenging examples that the model failed to learn during initial training 2. Can you elaborate on why some noise samples received high weights while clean samples received low weights in the CIFAR10 experiment with 60 noise 3. Does the proposed method have the potential to be combined with commonly used label noise learning techniques such as MixUp or ensemble methods to further improve performance", "Paraphrased Statement: This paper introduces a technique to train models using training data that contains incorrect labels (i.e. noisy labels). The method assigns weights to each sample based on its \"noise level\" (measured by the discrepancy between model predictions and the assigned labels) using an additional attention mechanism. When tested on commonly used datasets the proposed method outperformed other approaches designed for training with noisy labels. Strengths: The concept is straightforward and the experimental results are encouraging. Weaknesses: The theoretical foundation is limited. Lemma 4.1 merely restates that noisy labels can hinder models trained with crossentropy loss which is a wellestablished concept. It is unclear how presenting this as a lemma contributes to the papers discussion. Questions: 1. The method relies on the assumption that noisy samples have higher loss than clean samples. How does it handle samples with varying degrees of difficulty 2. It is unclear if the proposed noise attention loss is solely used for model training or also combined with crossentropy loss. 3. The mathematical notation needs improvement. For instance \"j\" is undefined in Section 3.3 but is used differently in Lemma 4.1 and Theorem 4.1. Additionally \"t\" and \"\\hatt\" in Algorithm 1 are not clearly differentiated.", "Paraphrase: Summary This research presents an attentionbased technique to identify and mitigate the influence of mislabeled data during training. The authors introduce an attention branch following the penultimate layer which generates a confidence score for each sample. This score is integrated into the loss function allowing the model to differentiate between clean and mislabeled samples. The gradients of clean samples contribute to training while those of mislabeled samples are suppressed as the model learns to reduce their impact using the attention score. The approach is supported by both theoretical and empirical evidence. Strengths and Weaknesses Strengths: Clear and understandable writing Logical integration of an attention layer and temporal ensembling for target estimation Robust theoretical foundation and empirical validation Weaknesses: Reliance on two hyperparameters especially `\\lambda` which requires manual adjustment for each dataset making the method cumbersome Inconsistent baseline results potentially due to data copying from original papers Lack of variance reporting for accuracy results on specific datasets (Clothing 1M and Webvision) Questions How to address the hyperparameter tuning challenge particularly for `\\lambda` How to ensure consistent baseline results and variance reporting What implications does this method have for dealing with mislabeled data in realworld applications"], "oR5WIUtsXmx": ["Paraphrase: Summary: Authors analyze the spectral properties of implicit neural networks (INRs) and propose a regularizer to address a bias towards suppressed lowfrequency components in these architectures. Fourier transforms and empirical results suggest that INRs tend to learn functions with diminished lowfrequency content. To correct this the authors introduce a regularizer designed to enhance the lowfrequency component of the learned signal. Strengths: Original and innovative regularizer based on spectral insights. Clear distinction between theoretical and empirical results. Thoughtprovoking message about the potential tradeoff between expressivity and generalization in INRs. Weaknesses: Incomplete formalization of some main claims. Lack of a clear definition of the desired spectral properties for INRs. Unconvincing theoretical arguments especially regarding the impact of depth. Insufficient empirical evaluation particularly the absence of comparisons against existing regularizers. Difficulty reading text on some figures in print. Questions: Can the authors analytical tools be applied to investigate the impact of network width on spectral properties", "Summary Paraphrase: This paper examines how implicit neural representation methods such as coordinatebased MLPs naturally introduce regularization due to their structure. The authors analyze the weaknesses of coordinatebased MLPs by studying shallow MLPs and examining how depth and other parameters affect their spectral bias. They propose a smoothness regularization technique and test it on image encoding models. Strengths: Provides a thorough analysis of widely used implicit neural architectures. Includes specific examples to support the theoretical findings. The proposed regularizer is straightforward and improves image encoding quality. Weaknesses: Focuses solely on images testing on other modalities would strengthen the findings. The limited experimental evaluation includes only a small subset of ImageNet STL and synthetic data. The study lacks code for reproducibility and provides insufficient experimental details. Some minor typos and formatting issues need correction. Questions: Can the proposed regularizer be beneficial in general deep learning tasks beyond implicit data representation Could it enhance tasks that incorporate \"coordinates\" as inputs or employ periodic activation functions such as diffusion models and neural operators", "Paraphrase: Summary: This study reveals that coordinate MLPs sacrifice some of their underlying regularization abilities when modified to enhance their depiction of highfrequency signals. This implies that implicit regularization in neural networks is not solely attributable to stochastic gradient descent (SGD) but rather to the networks architecture. The authors propose a novel explicit regularization technique that enables coordinate MLPs to retain their accuracy in representing lowfrequency signals while improving their handling of highfrequency signals. The efficacy of the suggested explicit regularization is validated through various experiments. Strengths and Weaknesses: Strengths: Identifies and explains the observed loss of implicit generalization in coordinate MLPs. Introduces a novel regularization method to address this issue. Provides extensive references and a cohesive argument throughout the paper. Presents theoretical results supported by derivations and offers intuitive reasoning backed by mathematical evidence for unproven findings. Weaknesses: Clarity: The explanations are clear and accessible but could benefit from explicit definitions of LPSNR RPSNR and TPSNR in Table 1. Typos and Minor Issues: Line 147: Typo \"positonal\" should be \"positional\". Line 162: Typo \"transform of the activation\" should be \"transform of the input activation\". Figure 2 caption overlaps with xaxis titles. Line 210: Typo \"follwing\" should be \"following\". Figure 4 caption: Typo \"densly\" should be \"densely\". Figure 7 caption: \"is able to achieve\" should be \"can achieve\".", "Paraphrase: This paper introduces coordinateMLPs a type of neural network designed to model highfrequency signals within a lowdimensional space. The authors examine three main categories of coordinateMLPs based on Fourier features sinusoidal activations and Gaussian activations. They propose a method for calculating the Fourier transform of these MLPs and analyze the impact of their hyperparameters on the frequency spectrum. Furthermore they study the effect of increasing the network depth and find that in all three cases adjusting hyperparameters to enhance highfrequency representation reduces spectral energy in lower frequencies. Based on this observation the authors propose a regularization technique that minimizes the trace of the Jacobian matrix associated with the penultimate layer. This approach suppresses highfrequency components of the MLP that correspond to lowenergy regions in the target signal. Additionally they introduce a simpler regularization method that involves minimizing the output smoothness of the penultimate layer along randomly selected directions eliminating the need for Jacobian calculations. The authors conclude the paper with experiments demonstrating that regularized MLPs learn better on signals with varying sampling intervals and specific spectral characteristics. Overall the paper presents novel contributions to the field of implicit neural representations and provides valuable insights into the behavior of these networks. However minor improvements could be made such as clarifying results in the text expanding on certain reasoning enhancing the visibility of figures and providing a comparison of the two regularization techniques in practice."], "s776AhRFm67": ["Summary This study demonstrates that strongly robust learnability with respect to perturbation set \\mathcalU is equivalent to barely robust learnability with respect to \\mathcalU1(\\mathcalU). It provides algorithms for boosting the robustness of barely robust learners and obtaining barely robust classifiers. While barely robust learnability is not sufficient for strongly robust learnability the study provides a counterexample. The paper also discusses algorithms for boosting robustness with unlabeled data. Strengths Establishes necessary and sufficient conditions for strongly robust learning Presents a novel method for enhancing the robustness of barely robust learners Provides rigorous theoretical results Contributes to the study of robustness in machine learning Offers highlevel strategies for understanding the algorithms and proofs Weaknesses Verifying classifier robustness for a given perturbation set remains computationally challenging Random smoothing techniques may not effectively verify robustness in highdimensional scenarios The proposed algorithms practicality depends heavily on the development of reliable verification methods Questions The proof of Theorem 6 uses a randomized learner without training data but it proves barely robust learnability with a different definition than that used in the theorem statement. Corollary 2 is claimed to offer a stronger boosting guarantee but it is unclear in what specific way it improves upon Theorem 1 (e.g. sample complexity reduction relaxed assumptions on the barely robust learner etc.).", "Paraphrased Statement: Summary: This paper explores how to train models with weak adversarial robustness that can still resist strong adversarial attacks. The authors follow a logical approach and provide comprehensive solutions. Contributions: 1. The paper defines \"barely robust learning\" which is crucial for adversarial robustness and demonstrates its importance. 2. An efficient oraclebased method is presented to convert adversarial robust learning into barely robust learning. This shows that barely robust learning with a wider perturbation range is sufficient for adversarial robustness (Theorem 1 Algorithm 1). 3. The paper proves that adversarial robust learning implies barely robust learning (Theorem 4) but not vice versa (Theorem 6). This highlights the necessity of barely robust learning for adversarial robustness. Strengths and Weaknesses: The papers clear writing style particularly in the introduction allows for easy comprehension of the questions posed and main contributions. The authors anticipate and address potential concerns in the \"Landscape of boosting robustness\" and \"Related work\" sections. The discussion section effectively handles questions regarding the agnostic setting and computational efficiency while also introducing future research directions. However the novelty of the algorithm and the depth of the theoretical analysis are difficult to assess without examining the details thoroughly. The concept of barely robustness is significant but it is unclear if the analysis follows standard procedures after its introduction.", "Paraphrased Summary: This research explores a novel approach to adversarial training that focuses on differentiating between \"barely robust\" and \"strongly robust\" learners. The paper presents theoretical analysis on the possibility of transforming barely robust classifiers into strongly robust ones and introduces two algorithms to ensure the creation of strongly robust learners. Additionally the authors demonstrate that every distribution contains a barely robust classifier that is not strongly robust. Strengths: Clear and original definition of barely robust learners Theorems elucidating the connection between barely robust and strongly robust classifiers including their equivalence under certain conditions Introduction of algorithms to guarantee strongly robust learners Weaknesses: Ambiguity regarding the significance of training strongly robust classifiers Lack of empirical experiments to demonstrate the practical benefits of strongly robust classifiers Absence of comparisons with existing literature limiting understanding of the relevance of strongly robust learning to previous research Questions: 1. Is the claim that \"weaker relaxations of barely robust learning do not imply strongly robust learning\" compatible with the statement that U1(U) and U are balls with different radii 2. Are blackbox oracle calls to the algorithm A feasible in realworld applications", "Paraphrase: Summary: This research examines different types of machine learning learners: \"barely robust learners\" and \"strong robust learners.\" Strengths: Addresses the significant issue of adversarial robustness in machine learning. Weaknesses: The analysis is challenging to comprehend especially in Section 1.1 where the term \"U1\" is frequently used without being adequately explained. The correctness of the theorems and proofs has not been verified. Including an empirical analysis to demonstrate the effectiveness of the proposed methods would be beneficial. The termination rounds for realworld applications may be excessively long. Questions: Is it feasible to incorporate an empirical analysis to demonstrate the effectiveness of the proposed methods", "Paraphrased Statement: Summary The authors present the concept of \"barely robust learning.\" They establish its equivalence to \"strongly robust learning\" by showing: If a concept is strongly robustly learnable it is also barely robustly learnable (Theorem 4). An oracleefficient algorithm can transform a barely robust learner into a strongly robust learner (Theorem 1). They prove a formal equivalence statement (Corollary I): strong robust learnability with respect to set \\mathcalU implies barely robust learnability with respect to a larger set \\mathcalU1(\\mathcalU). They also demonstrate (Theorem 6) that barely robust learnability does not always imply strong robust learnability. Strengths and Weaknesses Originality: The authors introduce a novel algorithm that enhances the robustness of \"barely robust learners\" and offer a comprehensive characterization of this new concept in relation to existing ones. Quality: The theory is wellstructured and addresses key issues. The discussion provides context for the findings. Clarity: Good overall but placing relevant definitions before theorem statements would improve readability. Significance: While the authors leave open the question of how to enhance barely robust learners error without sacrificing robustness it limits the significance. Further investigation into whether existing adversarial learning techniques are barely robust would also be valuable. Questions: How can the error of barely robust learners be improved without sacrificing robustness Do any currently known adversarial learning methods exhibit barely robust behavior"], "wjqr6aqkLUV": ["Paraphrase: Summary This study examines the concept of condensation in neural networks using different activation functions. The maximum number of condensed directions (neural connections that become nearsingular during training) is shown to be twice the \"multiplicity\" of the activation function. This finding is supported by empirical evidence and theoretical calculations. Strengths and Weaknesses Originality: Condensation has been analyzed previously (e.g. by Maennel et al.) but this paper extends the analysis to multiple activation functions. Quality: Numerous experiments are conducted using models ranging from ResNets to twolayer models and a sixlayer network with skip connections. Weakness: Theorem 5.1 presents informative calculations but should not be labeled as theorems due to their lack of rigor. Clarity: The writing is mostly coherent although some sentences have minor grammatical issues. The figures are clear and illustrative. Significance Understanding the initial training dynamics of neural networks can lead to new inductive biases and training algorithms including those relevant for pruning and compressing networks. Questions Can the authors explain the difference in the number of condensed directions between sigmoidal and softmax activation functions in Figures 2 and 4 Could the authors provide citations for the mentioned \"embedding principle\" (line 64) Additional Notes Minor typos and grammatical errors have been corrected: line 101: \"dose\" \"does\" line 103: \"can across\" unclear meaning line 184: \"It is reasonable\" \"it is reasonable\" line 220: \"suppose we only consider the leading term\" Remove theorem format from calculations.", "Paraphrased Statement: Summary: Understanding the weight convergence process in neural networks is crucial in deep learning. This research explores the convergence behavior of weights in the initial training phase of deep networks. Experiments show that with minimal initialization hidden neuron weights tend to converge in a maximum number of directions determined by the multiplicity of the activation function. Multiplicity is defined as the order of the activation functions derivative at a point where it first becomes nonzero. Theoretical Support: A theoretical analysis based on a Taylor approximation of the activation function suggests that under certain conditions weight orientation convergence early in training translates to alignment along specific directions specified by the multiplicity. This analysis holds for activation functions with multiplicity 1 in highdimensional data and general multiplicity in onedimensional data. Strengths and Weaknesses: Originality: Previous studies observed weight condensation in shallow networks with ReLU activation but this paper extends this observation to deep networks with smooth activation functions and explains the underlying mechanisms. Clarity: The paper is generally clear and easy to comprehend. Significance: The connection between multiplicity and condensed orientations is interesting and could inform future research on gradient descent bias in early training. Limitations: Motivation and Significance: The paper lacks discussion on the motivation and implications of studying vanishing initialization. TheoreticalEmpirical Gap: The theoretical analysis uses gradient flow while experiments employ Adam. It is unclear how the results would differ with gradient descent. Network Complexity: The implications of weight condensation for network complexity are discussed but the specific generalization bounds are not explored. Convergence Assumption: The theoretical proof assumes weight orientation convergence and convergence near initialization. It is unclear why these assumptions are valid for this setting.", "Paraphrased Summary: The study investigated the phenomenon where neuron weights in hidden layers of twohiddenlayer neural networks concentrate in a limited number of directions during training. It was found that this number is connected to the nonlinearity used for neuron activation particularly its multiplicity. The theoretical analysis based on certain assumptions explained how this relationship emerges. Paraphrased Strengths: Clarity in writing welldefined concepts and clear presentation of claims. Convincing empirical evidence showing the connection between condensation and nonlinearity multiplicity. Paraphrased Weakness: Questioning the relevance of the studied regime to realworld applications. The findings suggest that tanh nonlinearity may lead to condensation into a network with only two effective neurons which is considered too small for practical applications. The authors propose condensation to reduce overfitting due to fewer parameters. However it is argued that effective network size in fully trained networks is likely larger than the condensed size observed in the study. Further explanation is requested from the authors on how the condensed stage relates to the expressive capabilities of fully trained neural networks."], "rjDziEPQLQs": ["Paraphrase: The research paper presents a Newton Method with a straightforward step size and conducts an affine invariance analysis determining convergence rates at both local and global levels. Strengths: The paper is exceptionally wellwritten and easily understandable. The contributions are both sophisticated and impactful. Convincing numerical experiments are provided. The paper is considered outstanding among the reviewers current workload. Questions: None.", "Paraphrase: Summary: Newtonsofts method can struggle with convergence even for smooth convex functions. This paper presents a damped Newton method with a step size schedule designed to achieve a global convergence rate of O(1k2) matching Nesterovs cubic regularization method. It involves minimizing a cubic upper bound based on a local Hessian norm. The proposed algorithm features an explicit step size formula and promises fast local and global convergence. Strengths and Weaknesses: Strengths: Wellwritten and accessible providing an overview of issues with Newton methods and their previous solutions. Motivates the proposed \"AIC Newton\" algorithm clearly and presents its proofs effectively. Under certain assumptions the algorithm is easy to implement and offers favorable local and global convergence guarantees. Weaknesses: Convergence guarantees require strong assumptions including weakstrongly selfconcordance and convexity. Other methods such as Nesterovs cubic Newton offer similar convergence rates with less restrictive assumptions. The claim that AIC Newton has an explicit step size and avoids line searches is not entirely accurate as it still requires knowledge of a selfconcordance parameter (Lf). Numerical experiments suggest that cubic Newton often outperforms AIC Newton contrary to the authors claims. Questions: Explain the practical advantages of AIC Newtons affine invariance. Describe the actual benefits of selfconcordance for this method. How are the parameters Lf L2 and \u03b1 finetuned in the numerical experiments How to choose Lf effectively for AIC Newton Does AIC Newton offer any periteration cost advantage over Nesterovs cubic Newton Consider optimizations to the cubic subproblem.", "Summary: This paper introduces a new Newtons method called the AffineInvariant Cubic Newton (AIC) algorithm. The AIC method improves upon existing algorithms by achieving stateoftheart global and local convergence rates for objective functions exhibiting affineinvariance. Its step size computation is also simplified without requiring subproblem solving. Numerical experiments demonstrate its competitiveness with other advanced algorithms. Strengths and Weaknesses: Strengths: Achieves stateoftheart convergence rates for both global and local optimization Simple step size calculation Applicable to objective functions with affineinvariance property Weaknesses: Parameters Lf in definitions need clarification Assumptions (weaklystronglysemistrongly selfconcordance) require further explanation Computational complexity per iteration is high due to Hessian matrix inversion like other Newtons method variants Question: The authors did not provide a summary of the algorithms limitations in the conclusion section despite stating that they would in the checklist."], "qk1qpCN-k6": ["Paraphrase: Summary The paper explores overfitting in linear regression using information bottleneck and Gibbs posterior least squares regression. The authors define information efficiency (Equation 11) as the ratio of conditional mutual informations between the optimal Gaussian solution and an algorithms solution. Since ridge regularization produces deterministic solutions the authors employ Gibbs posterior (Equation 14) for analysis. Information efficiency is derived in Equation 28 and specialized for the thermodynamic limit. After considering the authors rebuttal providing clarifications on assumptions and conceptual framework the reviewer acknowledges the papers potential as a first step towards understanding overfitting using information bottleneck. The reviewer raises concerns about the utility of the information efficiency definition (Equation 11) for overfitting assessment. Strengths Information bottleneck has been widely explored in neural networks and linear regression based on Gaussian channels has also received attention. This paper proposes using Gibbs posterior to provide an informationtheoretic analysis of overfitting in linear regression. Weaknesses The information efficiency definition (Equation 11) lacks clear justification for its usefulness in overfitting studies. Unlike neural networks where the information bottleneck objective originates from a ratedistortion problem the objective in linear regression should stem from minimizing generalization errors. The assumptions of a fixed design matrix and Gaussian error distribution may impact overfitting analysis. Difficulty in obtaining informationtheoretic results for linear regression and ridge regularization leads to the use of Gibbs posterior. The reviewer questions the extent to which the results deviate from the original linear regression model.", "Paraphrased Summary: This study investigates linear regression models using the concept of information bottlenecks. It imagines a process where a regression parameter vector (W) is generated randomly and a set of target values (Y) is generated based on W with added noise. A learning vector (T) is then generated to reconstruct W from both input data (X) and target values (Y). The amount of information encoded in T about the data is divided into three terms: information about W (I(T W)) information about Y given W (I(T Y W)) and information about residual errors (I(T Y W)). The study analyzes the information learned by an ideal algorithm (minimizing residual information) and a Gibbs regression algorithm. It shows that Gibbs regression is optimal when the dataset size (NP) is small or large and that it extracts more residual information than the ideal algorithm with the same amount of relevant information. The study also explores the relationship between the amount of residual information and the double descent phenomenon where models first improve and then worsen as the dataset size increases. Strengths: Novel application of information bottlenecks to linear regressions Potential impact in understanding double descent Weaknesses: Unintuitive results regarding residual information reduction and double descent Lack of clear explanations in Section 3.1 Use of S instead of Y for target values Ambiguous notation in equation 232528 Questions: What does the arrow notation in equation 232528 represent Could the residual information reduction be due to plotting curves for fixed values of \u03bc Why is NP used as the parameter on the xaxis rather than N What does it mean to let PN \u2192 \u221e while only NP changes on the xaxis", "Summary Paraphrase: This research investigates the issue of overfitting in linear models using information theory concepts. The authors introduce information efficiency which measures the ratio of mutual information between optimal and learned representations and the response variable. They analyze the information efficiency of ridge regression with the Gibbs posterior deriving its limit when relevance and temperature approach certain values. Additionally they examine the behaviour of information in highdimensional scenarios with isotropic and anisotropic covariates. Strengths Paraphrase: 1. The authors depart from the typical use of the information bottleneck as a regularizer and quantitatively assess information efficiency with asymptotic limits. 2. They explore mutual information behaviour from various angles including efficiency extensivity and redundancy. 3. The authors cleverly employ the Gibbs posterior with zerotemperature limit to overcome the issue of deterministic ridge regression. Weaknesses Paraphrase: 1. The analysis is based on a simple linear model limiting the applicability of the information bottleneck theory. It is unclear how the theorems can be extended to more complex scenarios such as nonlinear neural networks. 2. The highdimensional analysis with anisotropic covariates only considers a specific spectral distribution. Generalizing to more diverse scenarios would be beneficial. Questions Paraphrase: 1. Are there any restrictions on the optimal representation in equations (6) and (7) 2. The authors define the information bottleneck as minimizing the mutual information between mapping W and representation T rather than between input X and representation T as is commonly done. Can they provide further clarification"], "t0VbBTw-o8": ["Paraphrased Summary: This research suggests a way to ensure the robustness of graph data neural networks (GNNs) against malicious attacks. The proposed technique randomly removes nodes and edges from the graph effectively blocking messages from malicious nodes preventing their negative effects from reaching target nodes. The authors demonstrate that this approach provides strong robustness particularly over longer distances in the graph. Strengths: The robustness certification applies to arbitrary modifications of malicious nodes. It does not limit the scale of modifications on individual nodes recognizing that the number of malicious nodes is more critical in realworld graph data scenarios. The experimental evaluations are thorough examining various settings and configurations showcasing the adaptability of the technique. Weaknesses: The robustness certification is specific to only featurebased attacks ignoring edgemodification attacks commonly used in GNN attacks. The certification is nodespecific limited to verifying the robustness of specific node sets rather than allowing arbitrary numbers of nodes to be attacked. Questions: Subgraph Generation: How are the subgraphs generated Do edges exist only if both connecting nodes are included in the subgraph Graph Connectivity: How is subgraph connectivity and sparsity ensured Certification Scope: Why do the authors emphasize \"graybox\" certification instead of \"blackbox\" Given that the model itself performs the certification it would seem to fall under the \"whitebox\" category.", "Paraphrased Statement This study introduces a new technique for verifying the robustness of Graph Neural Networks (GNNs). Based on randomized smoothing the proposed approach leverages the network structure to generate various certificates (e.g. for accuracy). For the specified threat model these certificates surpass existing methodologies. Strengths The authors innovatively utilize network structure to provide robust GNN certification particularly in the context of randomized smoothing. The findings indicate that the proposed method enhances the certifiable proportion of cases compared to previous approaches particularly as the number of features increases. The threat model is relatively wideranging with potential realworld applications. Weaknesses Clarification of the threat models relevance and realworld occurrences would enhance readability. The certification techniques reliance on smoothing limits its applicability to offline analysis or costprohibitive online scenarios (e.g. sampling network configurations on a machine). Expanding the literature review and experiments to include conventional robustness approaches (e.g. rrobustness) and demonstrating their full certifiable nature could strengthen the works broader implications. Questions The authors are encouraged to address the following weaknesses: Explain the significance and practical relevance of the threat model under consideration. Discuss potential limitations of the smoothingbased certification technique for online applications. Explore comparisons with other robustness approaches to provide a more comprehensive evaluation.", "Paraphrased Statement: Summary This article introduces a novel approach to reliably verify (certify) graph neural networks (GNNs) using a smoothing technique. Unlike traditional methods this approach leverages the message passing principle of GNNs (such as edge removal or node ablation) to provide stronger guarantees. To achieve certification the authors develop a method for estimating the worstcase label probability change and propose a practical approach to establish a reliable lower bound. Strengths Offers stronger certification compared to previous methods Presents novel theoretical findings with practical implications Demonstrates improved speed and certification accuracy over blackbox techniques in experimental evaluations Weaknesses The certification confidence level of 95 (alpha 0.05) may be a concern as it implies that robustness is only guaranteed with a 95 probability. Some existing deterministic certification methods avoid this limitation. Questions Can the authors provide results for different confidence levels to assess the methods performance with lower alpha values What is the number of samples required for certification and how does the certification accuracy vary with different alpha levels", "Summary Paraphrase: This paper introduces a new approach using randomized smoothing to make graph neural networks (GNNs) more robust. The goal is to guarantee the accuracy of GCN classifications even if an attacker alters node features within specific limits. Unlike previous methods this approach considers both random edge removal and nodelevel removal. As a result it achieves significantly better certified accuracy compared to existing techniques. Strengths: Technical Contributions: It introduces randomized edge ablation which is not found in prior GNN certification methods. This considers graph topology in the certification process leading to improved robustness. Writing Quality: The paper is clear and concise with few errors or ambiguities. It thoroughly examines the proposed smoothing scheme including theoretical analysis of its limitations and advantages. Weaknesses: Model Specificity: The approach is targeted at GNNs with node perturbations potentially limiting its applicability in broader threat models. Training Enhancements: Additional training techniques such as SmoothMix SmoothAdv or DRT could further strengthen the certified robustness. Suggestions for Authors: Expand the method to include other threat models such as node additionremoval or edge feature perturbations. Explore additional training techniques to improve the certified robustness guarantee. Questions: How is the node mask t optimized during training What does \\epsilon0.022 represent Writing Suggestions: Remove Figure 4(a) and move the corresponding text to the appendix as the certifications independence from perturbed feature dimensions makes it redundant."], "pV7f1Rq71I5": ["Paraphrased Statement: This study presents a method for estimating entropy while using only O(1) words of memory. Compared to the current best approach it improves sampling complexity by 1\u03b5 factor where k represents the alphabet size. The technique follows earlier research by randomly choosing a sample size id and estimating probabilities pi using that many samples. The innovation lies in sampling from negative binomial distributions allowing bias estimation and correction via Taylor expansion. Additionally bucketing reduces the reliance on k in sampling complexity resulting in a significant improvement. Strengths and Weaknesses: The use of negative binomial distributions to enhance sampling complexity is novel and commendable. The bias estimation technique using Taylor expansion is innovative and significantly improves sampling complexity. Theoretical findings support the papers claims providing thorough and compelling evidence. Questions: The sampling complexity is only expected to be bounded due to negative binomial properties raising the possibility that practical complexity could significantly exceed expectations with a low probability.", "Paraphrased Statement: Summary This research examines how to calculate the entropy of a distribution over a limited set given a continuous stream of independent samples from that distribution. The study demonstrates that a constantmemory algorithm can estimate entropy within an additive error of epsilon (eps) using keps2polylog(1eps) samples. This significantly improves upon previous methods reducing sample complexity by a factor of 1eps. The paper hypothesizes that the bound is optimal for constantspace algorithms barring polylog(1eps) factors. The papers central idea is to estimate the entropy as the expected value of log(1pi) where pi represents the probability of observing element i from the distribution. A novel estimator estimates log(1pi) by approximating a negative binomial random variable X that gives the number of samples required to see t exact copies of i. The estimate is then log(Xt). Strengths The innovative and nontrivial approach improves significantly on previous techniques. The bias of the new estimator is a polynomial in pi allowing for precise estimation. The clear and logical structure of the paper makes it easy to follow. Weaknesses The paper lacks specific applications of entropy estimation. Some explanations could be clearer especially at certain points. Specific Questions: 1. Provide more detail on Expression (1) in the proof clarifying the inequalities and their implications. 2. Define the parameters in the discussion about space complexity (e.g. b l r) before referencing them. 3. Explain in simple terms how the bucketing technique eliminates polylog(k) factors.", "Paraphrased Statement: This paper focuses on estimating the entropy of an unknown distribution over an alphabet of size k using independent streaming samples. The streaming model simulates a realworld scenario where data flows in continuously and estimations must be made with limited memory. Previous research (ABIS19) developed an algorithm for this problem with O(1) word memory (O(log k log (1\u03b5)) bits) and O(k\u03b5\u00b3 polylog(1\u03b5)) sample complexity where \u03b5 is the additive error. This paper presents a new algorithm that improves the sample complexity to O(k\u03b5\u00b2 polylog(1\u03b5)) while maintaining O(1) word memory. The proof structure follows ABIS19 but with key differences in the basic estimator. The conjecture is that this bound is optimal up to polylog(1\u03b5) factors for constant memory. Strengths: Relevance of the model and problem Concrete improvement over ABIS19 Deep and innovative techniques Weaknesses: Incremental nature (only one result) Limited conceptual contribution Lack of a formal lower bound proof to support the conjecture Questions: Can a lower bound better than the one for the unlimited memory model be established Any steps towards a better lower bound on sample complexity with O(1) memory would significantly strengthen the paper.", "Paraphrased Statement: This paper examines how to estimate the entropy of an unknown discrete distribution with a specific alphabet size (k) while limiting memory usage to a fixed number of \"words.\" A previous method proposed an algorithm with a sample complexity of O(k\u03b5\u00b3). This paper enhances that algorithm reducing the sample complexity to O(k\u03b5\u00b2). Estimating entropy involves computing the expectation of log(1p(X)). The algorithm accomplishes this by generating multiple estimates of log(1p(X)) and averaging them. A key innovation in this paper is a lowbias lowmemory algorithm for estimating log(1p(X)). Rather than using a plugin estimate this method samples a negative binomial random variable to determine how many times an element (i) appears (t). While the random variable is unbounded it is shown that the expected number of samples for this step is approximately k. Additional samples are utilized to approximate log(1p(X)) using a Taylor series expansion with the error of this approximation serving as the bias. Instead of averaging the estimates directly the paper employs a bucketing method similar to previous approaches. Strengths: The paper offers an algorithm that improves entropy estimation using a limited number of words. The core concept behind the improvement is evident. The paper is wellwritten and easy to understand. Weaknesses: The absence of a matching lower bound is a limitation but the authors provide a thorough explanation in the appendix. Questions: Included in the previous section. Additional Note: The typo in line 246 has been corrected to \"pi\" instead of \"qi.\""], "zkk_7sV6gm8": ["Paraphrased Statement: This study focuses on decisionmaking scenarios where executing actions entails substantial costs. The authors introduce LICRA (Learnable Impulse Control Reinforcement Algorithm) a method that simultaneously determines the timing and selection of actions. LICRA incorporates a novel Bellman operator and convergence guarantees. Additionally LICRA with state augmentation can address reinforcement learning (RL) problems with budgetary constraints. Strengths: LICRA effectively employs separate policies for determining action timing and selection. LICRA extends to RL problems with budgets. Weaknesses: The motivation for using LICRA and learning two separate policies is not sufficiently explained. Alternative approaches exist such as augmenting the action space with a null action which may be simpler and more computationally efficient. Constrained RL techniques could also be applicable. The authors fail to demonstrate LICRAs advantages over these methods. The empirical evaluations are limited and lack comparisons with relevant constrained RL algorithms. LICRAs benefits are unclear in certain tasks. Baseline algorithms are not adequately described in the figures. The theoretical results and notations in Section 5 are confusing. The \\mathcalM\\pig operator should be applicable to functions in S x A R yet its definition implies independence from Q\\pig. Additionally the definition of T in line 195 appears to contradict its dependence on \\mathcalM\\pig Q\\pig and v\\pig. Further Considerations: The paper overlooks related work on RL problems with limited action usage [1]. [1] JiCheng Pang et al. \"Sparsity Prior Regularized Qlearning for Sparse Action Tasks.\" arXiv preprint arXiv 2105.08666 (2021).", "Paraphrase: Summary: This paper presents a method called LICRA for making decisions in sequential settings where actions have a cost. The method involves two reinforcement learning (RL) policies: one for determining when to take an action and the other for selecting which action to take. By separating these decisions LICRA reduces the complexity of the problem compared to traditional RL approaches. Strengths and Weaknesses: Originality: LICRA is a unique approach to impulse control that uses two RL policies. Quality: The paper demonstrates the effectiveness of LICRA in applications such as finance and selfdriving cars. The theoretical proofs appear sound but a more detailed review of the appendix is needed. The related work section lacks specific examples of similar methods. Clarity: The paper is wellstructured and easy to follow. Algorithm 1 provides a clear outline of the LICRA procedure. Minor Issues: The introduction needs grammatical revisions. Minor errors in wording throughout the paper (e.g. \"in\" should be \"of\" on line 28). Missing details on the motivational example for impulse control. Significance: LICRA has practical applications in domains involving impulse control. Researchers and practitioners can benefit from its effectiveness and ease of implementation. Questions: What are the performance metrics used in Figures 1 2 and 3 How many random seeds and hyperparameter tuning iterations were used to obtain the reported results Provide brief summaries of the baseline algorithms used and why they were chosen. Describe the meaning of \"minibatches\" and \"optimization epochs\" in Appendix 12. Explain whether hyperparameters are consistent across all algorithms and policies used. Do the reported Average Episodic Return values include the costs associated with actions It would be valuable to compare the baselines using the original reward function.", "Paraphrased Statement: Summary This paper addresses a problem in reinforcement learning known as the \"impulse control problem\" which concerns deciding when to act or abstain. In realworld scenarios choosing inaction can be the optimal choice due to associated action costs. The authors present a framework that integrates the impulse control problem into reinforcement learning by incorporating action costs into the reward function. They decompose the agents policy into two parts: an action space policy and a binary policy that decides whether to act. This approach avoids inefficiencies in learning due to action cardinality. Theoretical results demonstrate that the proposed approach converges to the optimal Q function. The framework has been experimentally validated and shown to handle challenges effectively. Strengths and Weaknesses The paper addresses a significant and understudied problem in reinforcement learning. The theoretical results support the validity of the proposed algorithms. The frameworks compatibility with existing reinforcement learning algorithms makes it practical for realworld applications. The authors problem formulation is adequate. However the experimental evaluations need improvement. Further validation is required for the theoretical results. The authors fail to evaluate Qlearninglike algorithms as standalone options. The experiment design lacks consistency in the reported LICRA algorithm versions. Questions Are features shared between the two policies or are they separate Can the action decision policy be a separate linear head with a different reward signal Were experiments conducted to validate the theoretical results Why was the scenario of \"if the ratio of s to si is large learning g before pi is essential\" not tested In Theorem 2 why is the action choice maxat instead of maxat Define \"delta\" in Equation 4. Describe the state variable of the Merton problem and its sequential relationships. Are LICRAs results sensitive to the environment parameters of the SDE Provide details of the hyperparameter tuning process and clarify the use of square brackets. Explain the inconsistent usage of \"lambda\" in Equation 14. Tighten the notation in the proof of Theorem 1. Consider including a discussion on Impulse Control Problem. Restructure the experiment section to highlight key findings and ablation results. Move unnecessary details of the Merton problem to the appendix. Correct the reference to Figure 5. Remove nonvalueadded examples from Section 3. Use a more meaningful performance metric for learning curves. Provide information on computational resources used. Consider using a radar plot to visualize performance across cost functions. Clarify the interpretation of Figure 5."], "opw858PBJl6": ["Paraphrase: Summary: The study introduces and defines new evaluation measures for saliency methods based on completeness and soundness. Completeness ensures that the networks output remains unchanged when using the masked (with the saliency map) input instead of the full image as demanded by most current saliency method evaluation methods. Soundness ensures that the same saliency method cannot create masked input that causes the network to output a different label ensuring that saliency map evaluation accurately reflects the models likelihood of assigning labels. Strengths and Weaknesses: The papers contributions are substantial and clearly stated. The examples are relevant and helpful. The papers originality lies in linking saliency methods to logical proof systems and formalizing an evaluation approach that surpasses current methods limitations making them more precise and theoretically grounded. The paper proposes a straightforward saliency method based on optimization that satisfies soundness at a slight completeness expense due to a change in pixel replacement strategy. The methods effectiveness is confirmed by validation on various datasets and comparison to other saliency methods. The authors formal framework of definitions provides theoretical justification for why heuristic approaches used to enhance the appearance of masks (TV regularization and upsampling) are effective in improving soundness. Questions: Are there any previously proposed intrinsic evaluation methods mentioned in the \"Prior approaches\" section The M2 AUC score in Fig. 2 should be 0.29 according to Fig. 2 not 0.37. In the caption for Fig. 4 the TV penalty factor LambdaTV in line 5 should be 0.001 not 0.01.", "Summary This paper proposes a new method for identifying the pixels in an image that influence a classification outcome (saliency). It considers the balance between completeness (identifying all relevant pixels) and soundness (excluding irrelevant pixels) unlike previous methods that overlook soundness. Strengths and Weaknesses Despite a few typos the paper is wellwritten and presents a novel approach to saliency attribution. It outperforms previous methods by addressing soundness in a principled manner. The paper supports its claims with quantitative results while qualitative results may be limited by space constraints. Questions 1. Why is intrinsic saliency (not guided by an external network) valuable 2. How can one determine the optimal balance between completeness and soundness in saliency attribution for effective human interpretation", "Paraphrased Statement: Summary: This study introduces a new concept called \"additional dimension soundness\" to assess saliency methods for explainable AI. Using this concept the authors explain the effectiveness of current heuristic methods and propose innovative saliency methods. Strengths: The authors provide a wellwritten paper with convincing arguments. The concept of additional dimension soundness is useful and insightful. Weaknesses: The presentation of the concepts could be improved for clarity in certain sections. Questions: Does the method in Section 4 address the \"mask shape\" problem by using random pixels or simply make it more challenging Should other label values besides \"a\" be considered in Line 166168 Is it appropriate to use \\alpha in Line 184 or should it be \\beta Are the terms \"completeness\" and \"soundness\" similar to \"sensitivity\" and \"specificity\" Clarifying this would enhance understanding. Figure 4 contains a typo: \"completenese\" should be spelled correctly as \"completeness.\"", "Paraphrase: Original Statement: Summary New Definitions and Evaluations for Saliency Methods introduces intrinsic evaluation metrics for saliency methods (completeness and soundness) that do not require additional models or human evaluation. These metrics are grounded in logical proof concepts and force the method to output a saliency map that only explains the class of interest. The paper proposes a maskbased saliency method that optimizes for soundness as well as completeness. Evaluations compare the proposed saliency method to other maskbased saliency methods on soundness and completeness deletion and insertion game metrics and saliency metric. Paraphrase: This paper introduces new intrinsic metrics for assessing saliency methods (completeness and soundness). These metrics are based on logical principles and require no external models or human evaluations. By evaluating saliency methods on both completeness and soundness the proposed metrics ensure that saliency maps explain only the relevant class improving specificity and interpretability. The paper also presents a novel saliency method that maximizes both soundnesss and completeness. Evaluations show the superiority of the proposed method over existing maskbased saliency methods. Strengths and Weaknesses: Strengths: Intrinsic evaluation methods for saliency methods Reframes saliency by introducing completeness and soundness as constraints Clear and wellwritten paper Weaknesses: Novelty of pixel replacement strategy may need further discussion Missing related work on saliency evaluation methods Lack of reproducibility information Limited discussion of limitations and ethical considerations"], "wmdbwZz65FM": ["Paraphrased Statement: Summary: Researchers have addressed the \"posterior collapse\" issue in autoregressive variational autoencoders (VAEs). They modified the \"word dropout\" method which uniformly omits words during decoding and cast it as a minimax optimization problem. They introduce an adversary that identifies and masks the most informative words for the model. To prevent overfitting they only mask the top K words and add noise to the adversarys prediction by sampling from a Gaussian distribution. Additionally they regularize the adversarys scores using KullbackLeibler divergence to a standard Gaussian distribution. Strengths and Weaknesses: Strengths: Clear and concise writing Novel and original approach Theoretical insights into \"posterior collapse\" Experimental results supporting the method Weaknesses: Unclear significance of the improvements Limited ablation study specifically regarding the parameter \u03bb Lack of discussion on the absence of KL of MI improvement over uniform sampling on the Yahoo dataset The paper mentions alternative approaches to generating dropout probabilities in Section 4.1 but does not specify what approaches were unsuccessful. Questions: Could the authors elaborate on other approaches they attempted for generating perword dropout probabilities", "Paraphrased Statement: Summary: This paper investigates the posterior collapse issue in variational autoencoders (VAEs) proposing an adversarial training strategy to address it. The strategy involves training an adversary to predict the probabilities of dropout in the decoders input theoretically removing pointwise mutual information and experimentally demonstrating improvements on various benchmarks. A quantitative analysis further elucidates the method and its tradeoffs. Strengths: 1. The method targets a common issue in autoregressive decoders benefiting the wider community that uses them. 2. The method is theoretically sound with a clear motivation and a straightforward implementation. 3. Empirical results show that the method outperforms baselines on several tasks. Weaknesses: 1. The proposed probabilistic word dropout technique lacks novelty and may resemble probabilistic teacher forcing. The extensive notations and formulas add unnecessary complexity. 2. The improvements over previous work are modest and further analysis beyond the primary experiments is lacking. 3. The method is limited to double LSTM decoders excluding popular transformer architectures and pretrained language models. Its applicability to wider model structures remains unclear. Questions: 1. Is double LSTM an inherent part of the method or can other decoder architectures be used 2. Are the notations and formulas essential for understanding the concept or can the method be presented more concisely 3. An ablation study to compare the method with different decoder types is necessary. 4. The claim of posterior collapse issues should be supported by previous literature. 5. A referenced figure in the appendix appears to be incorrect. 6. The yaxis label for Figure 2 is missing. 7. Tables 2 and 3 could be moved to the appendix to create space for additional experiments or analysis.", "Summary Paraphrase: This study focuses on addressing the posterior collapse issue in training sequence variational autoencoders (VAEs). The authors first establish the theoretical efficacy of wordlevel Dropout regularization in mitigating this issue. They then propose a novel nonuniform datadependent Dropout scheme incorporating an adversarial training strategy with appropriate regularization. The effectiveness of their method is empirically validated using three datasets and compared to other sequence VAEs. Strengths and Weaknesses (Paraphrase): Strengths: Clarity and Organization: The writing is clear and wellstructured. WellMotivated Problem and Solution: Solving posterior collapse is an important problem and the proposed solution is intuitive. Weaknesses: Insufficient Empirical Demonstration: Hyperparameter Tuning Bias: Hyperparameter tuning was extensive for the proposed method but not for baselines potentially biasing comparisons. Limited Improvement: The improvement over vanilla Word Dropout is marginal despite increased computational cost. Missing SNLI Results: Quantitative results for the SNLI dataset are not provided. Lack of Quantitative Support for Qualitative Results: Qualitative results on the effectiveness of addressing posterior collapse lack quantitative reinforcement. Other Comments: Minor Errors: Misspellings in lines 185 and 190. Questions: Address the concerns regarding hyperparameter tuning bias and provide a fair comparison with baselines. Explain the marginal improvement compared to vanilla Word Dropout. Provide quantitative results for the SNLI dataset. Consider providing additional quantitative support for the qualitative results to enhance the credibility of the proposed method."], "pZsAwqUgnAs": ["Paraphrased Statement: Summary: This study examines the optimization characteristics of SGD (Stochastic Gradient Descent) and multipass SGD for quadratic functions using SDE (Stochastic Differential Equations also known as HSGD). Firstly it demonstrates that SGDs risk is comparable to GDs indicating the absence of inherent regularization in this context. Secondly it establishes a link between SGD and HSGD allowing the researchers to investigate SGDs properties using HSGD. The primary contribution is in highlighting that SGD has a unique condition number which under certain situations facilitates a faster convergence rate than GD. Additionally it examines streaming SGD within the same framework and compares it to regular SGD to assess generalization. Strengths: The novel finding that SGD possesses a distinct condition number for convex quadratic functions. SGDs advantage over GD when the Hessian spectrum exhibits outliers a prevalent feature in realworld applications (as shown in Figure 2). The analysis of the Volterra Dynamic though not novel to this paper provides valuable insights into SGDs behavior. Weaknesses: Figure 2 displays random feature models for practical scenarios (e.g. CIFAR10 CIFAR5m) suggesting that the Hessian in these cases is illconditioned supporting their claim that SGD outperforms GD in these scenarios. However it must be noted that the random feature model is a convex optimization problem unlike real neural networks. Hence a low ICR (Implicit Condition Number Ratio) in the random feature model does not directly imply SGDs superiority over GD in real optimization scenarios. The authors should provide an actual convergence comparison between SGD and GD even for random feature models to support their main observation. ICR is merely an indicator for comparing SGD and GD and inaccuracies might arise from using HSGD and finite dimension iteration times. Without such simulations the claim that SGD outperforms GD in terms of convergence speed is unconvincing. The argument that no implicit regularization exists might be overly simplistic. In convex optimization all converging algorithms should converge to the same function value (or point in strongly convex cases). Previous studies on implicit regularization have primarily focused on nonconvex optimization. The introduction of Volterra Dynamics is insufficient for readers unfamiliar with the concept. Providing additional background information would enhance comprehension. A missing title is present in reference [36]. Questions: Refer to the \"Weaknesses\" section for specific questions raised regarding the study.", "Summary The authors demonstrate: In high dimensions multipass stochastic gradient descent (SGD) behaves like a stochastic differential equation (SDE) with a specific noise covariance. In this limit the SDE converges to a deterministic Volterra dynamic. Noise impairs population loss but may hasten convergence. Strengths Clear and concise writing. Innovative comparison of loss dynamics to an SDE. Insightful analysis of LotkaVolterra dynamics. Weaknesses Limited to linear regression. Overstated claims regarding SGDs superiority. Incomplete literature review and confusing introduction. Inaccurate statement regarding the connection between SDE and nonconvex learning. Minor Flaws Empty reference [36]. Missing dt in Equation (5). Unexplained derivation of tn in Equation (166). Parenthesis issues in Equation (250). Questions Why is the quadratic case crucial for deriving the SDE limit How can the SDE limit be generalized to: General convex losses Nonconvex settings", "Paraphrased Statement: Summary: This paper investigates the generalization ability of multipass Stochastic Gradient Descent (SGD) for highdimensional convex quadratic problems. It establishes a connection between SGD and a stochastic differential equation called Homogenized Stochastic Gradient Descent (HSGD). Using HSGD it derives an accurate risk trajectory for SGD revealing conditions where SGD excels over Gradient Descent (GD). The analysis is extended to streaming SGD demonstrating its limitations compared to multipass SGD. Strengths: Approximation result between SGD and HSGD Demonstration of SGDs negative impact on generalization Clarification of SGDs convergence acceleration Identification of streaming SGDs deficiencies Weaknesses: Complex formulas for Psit and \\Omegat hindering interpretation Lack of nonasymptotic insights (t not approaching infinity) Missing explanation for SGDs increased efficiency over GD Lemma 1s lack of novelty (previously shown in another work) Potential irrelevance of convergence rates when the condition number is high Need for a rigorous comparison of SGD and GD efficiency Questions: Refer to the \"Weaknesses\" section for questions raised.", "Summary This work presents a continuous function called HSGD as an approximation to stochastic gradient descent (SGD). It proves that HSGD approximates SGD closely and studies the dynamics of HSGD to understand the behavior of SGD. The paper concludes: SGD with multiple passes does not introduce bias in certain settings. SGD implicitly conditions on the data which speeds up convergence. Strengths Implicit conditioning provides a new perspective on SGDs effectiveness. Weaknesses Related work is not adequately cited. Theorem 1 and Theorem 2 are similar to results in another paper and should be attributed. Theorem 2 is unclear and differs from similar results. Questions Theorem 1: How does its approximation error compare to other continuous approximations Theorem 2: How can the approximation error be minimized Lemma 1: What is the value of \u03b3 Line 119124: How does this work differ from previous continuous approximations of SGD Line 148: Why is [21] cited for the i.i.d. subGaussian assumption Line 158: Is the step size still \u03b3"], "w-Aq4vmnTOP": ["Paraphrase: This paper investigates the number of samples needed for learning and disproving in the noninteractive local differential privacy (LDP) model. In noninteractive LDP all users send a single message to a server simultaneously and independently. For agnostic learning and disproving the paper demonstrates that the sample complexity is essentially the same (up to a slight approximation). The main finding is a lower bound on sample complexity based on the \"approximate \\gamma2 norm\" of a matrix related to the concept class. This combined with previous work shows the equivalence of learning and disproving in the agnostic setting. For realizable learning and disproving the paper establishes that disproving implies learning. Strengths: Resolves an open question regarding lower bounds for agnostic learning and disproving in noninteractive LDP. Weaknesses: The presentation could be improved by moving some proofs to an appendix and providing more intuition for key quantities (\\gamma2 and \\eta). A separate \"Contributions\" section would aid in identifying the main results. Questions: Why is the approximate \\gamma2 norm not defined in the main text given its prominent role What is the meaning of the operator in Lemma 9 Why is the realizable disproving setting more challenging than the agnostic case What is a clear explanation of the quantity \\eta(\\mathcalC\\alpha) in Definition 13 which is used in the realizable case bounds in Theorem 2", "Paraphrased Statement: Summary Setting and Contributions: This paper investigates learning and disproving concepts under the local differential privacy (LDP) model which ensures user privacy even from central authorities. Main Results: Agnostic Model: Establishes a lower bound on the sample size required for agnostic learning completing the understanding of these models. This result complements upper bounds for learning and refutation and a lower bound for refutation previously established by Edmonds Nikolov and Ullman. Realizable Model: Presents an LDP protocol with upper and lower bounds on the sample complexity for learning and refutation. Strengths and Weaknesses: Strengths: Addresses fundamental questions about learning in the privacycentric LDP model. Closes the gap in literature on learning and refutation in the agnostic model. Introduces an elegant technique for proving the lower bound based on the difference matrix. Defines a quantity (eta(Calpha)) that measures sample complexity in the realizable case. Weaknesses: None explicitly stated.", "Paraphrase: Summary This paper explores two related tasks in noninteractive local differential privacy: agnostic PAC learning and refutation. It establishes that both tasks share the same lower bound on sample complexity which aligns with the upper bound determined in prior research. This finding demonstrates the equivalence between learning and refutation in the agnostic context. Strengths and Weaknesses Strengths: The results are robust and comprehensive. It bridges the gap between the upper and lower bounds for learning and refutation in agnostic PAC learning with noninteractive local differential privacy. Weaknesses: The proof could benefit from a more hierarchical structure. There is a minor typo in Lemma 8: \"mv x ma x\". Questions Since the paper focuses on finite concept classes it would be interesting to explore whether the results can be extended to settings with infinite concept classes.", "Paraphrase: Summary This paper explores the concept of learning and refutation in the context of noninteractive local differential privacy (LDP). It determines the required sample size for agnostic PAC learning in noninteractive LDP protocols. Additionally it demonstrates the equivalence of learning and refutation in agnostic learning environments. Strengths and Weaknesses The papers mathematical foundations appear solid. The presentation is clear and organized. The topic of learning and refutation in LDP is intriguing. However a minor error may be present in the definition of the learning problem. Questions Major Concern: What is the relationship between the refutation in this paper and the refutation concept introduced in Vadhans [2017] work While it is acknowledged that the two concepts are distinct their exact equivalence remains unclear."], "ptUZl8xDMMN": ["Paraphrase: This paper generalizes the scattering transform to graphs defining its components (filters nonlinearities projections) and proving its stability and energy preservation. The approach is versatile but also abstract leaving some questions about its practical implementation. Strengths: Encompasses many variations and preserves classical scattering properties. Extensive supplemental materials and convincing experiments particularly in graph regression. Weaknesses: The abstract formulation and lack of concrete graph examples make it difficult to grasp the underlying architecture. The choice of filters is not fully explained and seems arbitrary. Many variants are described but not tested. The theorems rely on assumptions that may not be easy to meet in practice. Questions: Should the focus be broadened beyond graphs or narrowed to provide more concrete examples How is the higherorder architecture used in the regression experiment Is the constant \"2\" in Definition 4.3 arbitrary Could proximity in Theorem 4.2 be defined in terms of spectral norm instead of Frobenius norm", "Summary The paper proposes a generalization of graph scattering networks that extends beyond graph wavelets. The authors provide: Stability guarantees for their generalized scattering transform Layerwise energy decay bounds A feature aggregation method for transforming graphs into Euclidean space Strengths and Weaknesses Originality: The methods and ideas presented are original and meet the novelty criteria for publication at NeurIPS. Quality: The mathematical derivations appear sound but the use of technical background and abstract motivations may limit accessibility. Clarity: The organization and exposition of the paper need improvement leading to difficulties in comprehension. Significance: The topic of graph scattering networks is significant for NeurIPS but the clarity issues may hinder its impact. Questions Quality The Lipschitztype bounds in Theorems 4.1 and 4.2 may be impractical due to exponential terms in the bounds. The analysis of expressivity in the limit of infinite network depth raises questions about the validity of the assumption of strictly positive eigenvector entries. The connection between energy bounds and expressivity is also unclear. Clarity The paper contains excessive material for a conference paper with important aspects relegated to the appendix. The authors define concepts in abstract terms and then make specific choices for experimentation without sufficient justification. Suggestions for Improvement Provide more context and definitions for a wider audience. Include tables or pseudocode to illustrate specific instantiations of the proposed architecture. Discuss alternative parameterization or modeling choices and justify their use in the experiments.", "Summary Paraphrase: This research paper investigates the development and analysis of graph scattering networks exploring variable branching ratios and customizable filters. The authors establish stability guarantees for perturbations at both node and graph levels regardless of spectral characteristics. Strengths Paraphrase: The paper is wellstructured presenting insightful theoretical findings on stability guarantees and practical results on predicting quantum chemical energies. Weaknesses Paraphrase: 1. Optimality of Bounds: The authors provide numerous upper bounds for stability in Section 4 but the optimality of these bounds remains unclear. 2. Higher Order Scattering Background: The section on higher order scattering lacks sufficient background information making it less accessible to readers. Questions Paraphrase: 1. Optimality of Bounds: The authors are encouraged to comment on the optimality of the upper bounds established in Section 4. 2. Background for Higher Order Scattering: Additional background and details would enhance the understanding of the section on higher order scattering."], "wlqb_RfSrKh": ["Paraphrased Summary: The paper introduces SaVos a selfsupervised method for amodal segmentation. SaVos tracks shapes of partially obscured objects in video frames to reconstruct their full (nonoccluded) shape. Using visible segmentation masks SaVos employs both spatialtemporal and categoryspecific information to estimate amodal masks and object motion. The method demonstrates generalization capabilities via oneshot adaptation prioritizing spatialtemporal priors. However categoryspecific priors aid in situations where object parts remain unseen. Paraphrased Strengths and Weaknesses: Strengths: Clear and comprehensive writing Intuitive and relevant key concepts Improvements over stateoftheart methods despite distribution shifts Informative introduction connecting to human vision Weaknesses: Inadequate comparison with related work (e.g. Ling et al. Sun et al.) Lack of baseline comparison with Ling et al. a newer and betterperforming model Absence of results as a function of occlusion levels hindering understanding of partial occlusion effects Lack of comparison with methods using temporal information despite the importance of this factor for the proposed model Misleading use of \"selfsupervised\" due to strong assumptions about perfect supervision at training time (bounding box instance label visible segmentation) Unrealistic assumptions about supervision in realworld scenarios Need for more informative results considering noise sensitivity Insufficient generalization testing across different categories Lack of experiments supporting the claim of squeezing out type information Subjective basis of statements regarding \"reasonable\" learning", "Paraphrase: The papers method for amodal video object segmentation does not require training on amodal segmentation annotations. Using image optical flow and conventional segmentation as input it calculates an amodal mask and motion for each instance which it then propagates through time using the estimated flow. The method learns from consistency checks with the input mask and flow over time. Strengths: Clear motivation for amodal segmentation without amodal supervision Comprehensive explanations Intuitive approach based on the assumption that most of an object will become visible over time Good performance highlighting the benefits of testtime finetuning and generalization to unseen objects and sequences Weaknesses: Lack of positioning and comparisons with related work: Amodal segmentation methods include: Imagebased (Nguyen Khoi and Sinisa Todorovic 2021) Singleobject instance completion (Ling Huan et al. 2020) Occlusion learning for people (Zhou Qiang et al. 2021) Weakly supervised instance segmentation (Arun Aditya C. V. Jawahar and M. Pawan Kumar 2020) Potential classification as weakly supervised rather than unsupervised: The method uses segmentation and flow input which aligns with weakly supervised methods in amodal segmentation. Importance of input accuracy: The methods reliance on optical flow and instance segmentation input raises questions about its sensitivity to the accuracy of these inputs. Questions: 1. How does the paper compare to the related work mentioned above particularly regarding unsupervised vs. weakly supervised approaches 2. What is the impact of the accuracy of the input optical flow and segmentation on the methods performance 3. To what extent do the proposed architecture changes contribute to the performance improvements over [42] which used a different architecture and formulation 4. Why is the supervised baseline referred to as a \"supervised oracle\" in Table 1", "Paraphrased Summary: In this research the authors focus on segmenting moving objects in videos even when they are partially hidden (amodal segmentation). They introduce a technique that uses information from other frames to predict occluded regions. Their method outperforms existing approaches on benchmark datasets. Paraphrased Strengths and Weaknesses: Strengths: The paper is clear and wellorganized. The method relies on the logical assumption that occluded object parts may be visible in other frames. Weaknesses: The assumption that every object pixel is visible in at least one frame may be too strict. The authors acknowledge the limitations of this assumption and provide examples of exceptions. Questions: Could the authors relax the strong assumption while still pursuing this approach Can the authors provide statistics on the number of cases that meet the strong assumption Is there an alternative approach for cases that do not meet the strong assumption", "Summary Problem: Predicting masks for obscured objects in videos. Solution: A selfsupervised model that predicts masks and object motion. Strengths Organization: Clear and wellwritten. Related Work: Thorough and relevant. Model: Novel architecture using a CNN LSTM and autoencoder. Objective: Unique selfsupervised objective using modal masks. Results: Outperforms existing methods and demonstrates generalization capabilities. Weaknesses Some design choices are not fully explained: LSTM Cold Start Problem: A potential solution is suggested but a more intuitive approach (e.g. transformers) could be considered. Necessity of Velocity Term: It is unclear why this term is needed for mask estimation. Image Patch Necessity: It is questionable if the image patch is essential for mask prediction."], "syU-XvinTI1": ["Summary This paper examines the use of deep learning models in computational neuroscience and argues that their effectiveness depends on explicit inductive biases that may not accurately reflect biological phenomena. The authors demonstrate that deep learning simulations of grid cell emergence in the hippocampus may be driven by specific and potentially biased implementation choices. They caution against relying on deep learning as a panacea for creating predictive models in neuroscience. Strengths and Weaknesses Originality: The paper presents a valuable critique of deep learning in neuroscience. It provides a detailed examination of existing models and highlights the importance of bias. Quality: Extensive experimentation and opensource code replication are commendable. However hyperparameter sweeps may not have been exhaustive and reproducibility may be limited by original publications lack of information. Threshold choices are crucial and may lack consensus. Clarity: The paper is wellwritten and structured. Figures could be improved for readability. Significance: The paper raises important questions about the limitations of deep learning in neuroscience. The findings could stimulate further research on model assumptions and implications. However they do not necessarily negate the relevance of deep learning models. Questions: The paper focuses primarily on entorhinalhippocampal circuits. It would benefit from considering a broader range of computational cognitive neuroscience applications of deep learning. The critique may overlook successful deep learning applications in neuroscience. The authors could provide further justification for their claim that deep learning models should be robust to variations in hyperparameters such as activation functions.", "Paraphrased Statement: Summary: Deep learning models may not be reliable substitutes for brain models because their behavior can be strongly influenced by specific design choices rather than underlying cognitive processes. For example the formation of grid cell patterns in a pathintegrating neural network (RNN) is primarily determined by design features (e.g. DoG firing fields) rather than the path integration function. Strengths: Explores the limitations of current approaches to using deep learning models in neuroscience. Provides a different perspective on interpreting the emergence of cognitive patterns in deep learning. Weaknesses: Does not offer practical solutions for improving the reliability of deep learning models for brain modeling. Focuses only on a single example (grid cells) to support the \"no free lunch\" argument. Questions: Are there other instances in neuroscience where deep learning has led to erroneous conclusions How can we enhance the use of deep learning models for interpreting brain functions while mitigating potential biases How do expertdesigned computational models compare to deep learning models in terms of reliability in modeling brain functions", "Paraphrased Statement: This study questions the validity of using Deep Neural Networks (DNNs) as accurate models for understanding brain function. Specifically the study challenges the widely held belief that taskrelated loss functions (like path integration in RNNs) can produce gridlike neural tuning which has been used as evidence supporting the use of DNN models in neuroscience. Through comprehensive hyperparameter sweeps the study found that while almost all trained networks could perform path integration tasks effectively only a few exhibited gridlike codes. They identified that networks with neurons resembling Difference of Gaussians (DoG) receptive fields were more likely to display gridlike codes but even then these cells only emerged within a narrow range of receptive field parameter values. The study provides theoretical insights based on Fourier analysis to explain why only specific ranges of DoG tuning curve values can lead to lattice cells. Additionally inconsistencies between method descriptions and code implementations in previous studies are highlighted. The study emphasizes the need for transparency and critical assessment of existing claims in neuroscience regardless of the prestige of journals in which they are published. While it warns against exaggerating the claims of DNN models it also cautions against going to the opposite extreme of pessimism. The study acknowledges that hyperparameter selection is crucial but argues that it should not be used as an excuse to dismiss findings. Instead studies should clearly disclose the chosen hyperparameters for reproducibility. The study also explores the relationship between model dimensionality and neural prediction scores suggesting that DNNs may excel due to their highdimensional bases. However it emphasizes that this generalization cannot be extended to vision or language DNN models. The study concludes with a reminder that all computational models exhibit the potential for multiple solutions and losses due to inherent nonlinearities. It stresses that the focus should be on assessing the extent of this ambiguity rather than dismissing models based solely on this possibility.", "Paraphrased Statement: This research reexamines published grid cell models utilizing deep neural networks. It demonstrates that gridlike patterns only emerge under very specific path integration task implementations and may even rely on unstated choices in implementation. Therefore the task alone does not guarantee the development of grid cells. The authors propose that the emergence of gridlike representations in networks trained on path integration reflects the retrospective nature of these studies as models may have been adjusted until they matched existing biological observations. They analyze these representations and provide explanations for their occurrence. They argue that neural network modeling requires stronger integration of biological and theoretical constraints. Different training objectives can result in similar neural activity patterns while the \"correct\" objective may not lead to observed neural patterns if biological constraints are absent. Strengths and Weaknesses: This is a thorough and wellpresented critique of using connectionist approaches to explain grid cells. However the claim that multiple training tasks map to the same activity patterns is not strongly supported by the empirical results which appear to show the opposite. Questions: Why should the specificity of gridlike representation emergence not be viewed as a feature of deep neural network modeling How is the grid score defined Is the code for reproducing the studys findings accessible Open research practices would enhance the impact of this work and align with the practices of the studies under critique. Comments: Pie charts (Figure 1) are outdated and potentially biased histograms or scatterplots would be more informative."], "sc7bBHAmcN": ["Paraphrase: This paper presents a unified analysis of subgraph Graph Neural Networks (GNNs) that are based on node selection (such as egonetworks and node deletion). The analysis shows that these subgraph GNNs correspond directly to 3IGNs (invariant graph networks) with different components (subgraph selection layers pooling and MLP) mapped to 3IGN layers. Based on this correspondence the paper demonstrates that any subgraph GNN based on node selection (where every subgraph is calculated through a bijection between the selected node and the original graph) can be implemented through a 3IGN. This is proven by showing that all known graph selection policies can be emulated by a 3IGN (Lemma 4) and by demonstrating how subgraph GNN components can also be captured by the same model (Lemma 5). Using this result the paper establishes that subgraph GNNs based on node selection have expressive power that is limited by 3WL the upper bound for 3IGNs. The paper proposes novel designs for equivariant operations over nodes and subgraphs drawing inspiration from 3IGNs and 2IGNs. These designs focus on equivariant functions with limited memory footprint and the incorporation of local node neighborhood aggregation resulting in a model called ReIGN that captures all known subgraph GNNs. Finally the paper introduces subgraph union networks (SUNs) which are built on the ReIGN framework. The paper evaluates SUNs on subgraph counting benchmarks demonstrating their strong performance. Strengths and Weaknesses Strengths: Unified analysis of subgraph GNNs Establishment of an upper bound for expressive power ReIGN framework provides possibilities for extending subgraph GNNs Good coverage of existing models Weaknesses: Limited experimental section lacking ablation analyses and case studies Explanation relies on appendix for understanding orbits and 3IGN construction Nonstandard font may limit space for detailed explanations", "Paraphrase: This paper presents a theoretical foundation for subgraph GNNs. It simplifies the analysis of subgraph GNNs with nodebased subgraph selection policies. By embedding nodebased subgraph GNNs into the Invariant Graph Networks (IGN) model the paper demonstrates that the expressive power of these GNNs is bounded by 3WL. Additionally the paper explores the design space of subgraph GNNs and proposes a new model called SUN which exhibits strong generalization capabilities. Strengths: Clear and wellwritten paper. Novel connection between subgraph GNNs and 3WL. Importance of subgraph usage in enhancing expressivity. Weaknesses: Reliance on IGN without providing its introduction. Disparity between the design space based on IGN(2) and the theoretical analysis based on IGN(3). Lack of time complexity discussion. Absence of exploration into how subgraph selection rules influence expressivity. Questions: 1. The extent of the expressivity gap between IGN(3) and ReIGN(2). 2. The time complexity of IGN(3).", "Paraphrased Statement: Summary: This paper expands and examines the category of Subgraph Graph Neural Networks (SGNNs). The study demonstrates that the expressive capacity of SGNNs is limited to 3WeisfeilerLehman (3WL) coloring. The authors propose a novel class of layers for SGNNs that enhance generalization capabilities. The research presents a foundational understanding of SGNNs. Strengths and Weaknesses: Strengths: Provides a deeper theoretical comprehension of SGNNs. Introduces a new SGNN layer family that outperforms previous architectures empirically. Weaknesses: Does not address scalability or computationalstorage costs. Empirical improvements are not significant. Questions: What is the computational complexity of subgraph GNNs", "Paraphrased Statement: Summary This paper explores the expressive capabilities of subgraph graph neural networks (GNNs). The authors introduce a nodebased subgraph selection strategy and demonstrate its representational power equivalent to the 3WL test linking it to 3IGNs. The proposed algorithm ReIGN outperforms other existing subgraph GNNs. Strengths Clear and comprehensible presentation Subgraph GNNs are an emerging area in GNNs and their connection to invariant and equivariant GNNs provides insights into design choices and applications. Extensive experiments show superior performance compared to other subgraph GNNs and comparable performance to established traditional GNNs. Weaknesses None identified. Questions 1. Why does ReIGN not outperform traditional GNNs on benchmark datasets Insights into why a subgraph GNN with only 1 MLP operator may not surpass methods with geometric group symmetries would be valuable. 2. Are predefined motifs required in nodebased selection policies"], "ob8tk9Q_2tN": ["Summary and Strengths: The MinAM method is an Anderson acceleration technique with a memory size of one reducing the storage requirements for acceleration. The method utilizes additional projection steps to prevent information loss. Its effectiveness is demonstrated by its relationship to conjugate gradient for quadratic functions and BFGS for updates. Extensions for nonlinear or stochastic functions are proposed including restarts regularization and adaptive step sizes. Weaknesses: Clarity issues in section 3.1 could be improved with better explanations and notations. Proofs in the appendix are dense and could benefit from smaller chunks to enhance comprehension. Questions: Is there a connection between MinAM and the Conjugate Residual method While Theorem 2 claims an improved bound on gradient descent it should be noted that: The extent of improvement with min(mk) is unknown. The constant sqrt(Lmu) can exceed 1. There is a quadratic factor.", "Paraphrased Statement: Summary The authors propose a new Anderson mixing method MinAM (Minimal Anderson Mixing) to address the memory requirements of other Anderson mixing algorithms. MinAM iteratively approximates the Hessians inverse by storing only a vector pair (pk qk) representing changes in iterates and residuals. This allows the algorithm to achieve minimal memory consumption (m1). Theoretical guarantees are provided: linearquadratic convergence for the Restarted MinAM version and convergence guarantees for the stochastic version resulting in O(1\u03b52) iteration complexity. Experiments demonstrate MinAMs performance in three scenarios: 1. Quadratic strongly convex synthetic problem comparing accuracy with firstorder and quasiNewton methods and evaluating online eigenvalue estimation. 2. Regularized logistic regression on real datasets again comparing to other methods and monitoring spectrum evaluation. 3. Largescale image datasets (CIFAR10100 and Imagenet) comparing efficiency to SGD with momentum when training deep neural networks. Strengths: Provides a solution to the memory issue of Anderson mixing methods. Introduces a method to choose mixing parameters based on Hessian spectral knowledge. Offers strong convergence proofs for Restarted and Stochastic MinAM variants. Extends MinAM to the stochastic setting making it applicable to deep learning training. Experiments show competitive performance against SGD with momentum on CIFAR10100 and Imagenet. Weaknesses: The stochastic version relies on four hyperparameters which is not discussed in detail. Experiments focus on iteration count omitting computations for Hessian spectrum evaluation. No discussion of limitations or potential drawbacks of the method.", "Paraphrased Statement: Summary: This article introduces a modified version of the Anderson mixing algorithm that requires minimal memory. Strengths and Weaknesses: The underlying theory of the proposed algorithm appears sound. While the numerical results show some improvement it is relatively minor. Questions: 1. Table 1 Page 9 Upper Panel: Compared to STAM the algorithms improvement in test accuracy is not significant except for VGG16. It may not be feasible to conclude that the algorithm significantly enhances accuracy. 2. Table 1 Page 9 Lower Panel: The algorithms improvement appears negligible for largescale datasets like CIFAR100. Please elaborate on how the algorithms performance varies with the dataset size.", "Paraphrased Statement: Summary: The paper introduces MinAM a memoryefficient variant of the Anderson mixing acceleration technique commonly used in fixedpoint recursions. Strengths and Weaknesses: The main ideas and findings are wellorganized and clearly presented. Sections 3.2 and 3.3 contain the primary novel contributions of the work. The appendix provides indepth technical support for the claims made in the main text. Questions: Clarity Issue: Figures 1(d)(e)(f) lack clear correspondence between pluses and circles making them difficult to interpret. Request for Enhancement: Consider plotting the error between corresponding pairs of eigenvalues to provide a more visually clear representation of the results. Limitations: Kindly highlight any limitations of the proposed MinAM method."], "pkfpkWU536D": ["Paraphrased Summary: This paper introduces a method for learning mesh deformations of dynamic objects using userprovided handles. The deformations are represented in a canonical space where the source mesh is transformed into a reference pose and then into the target pose. The method uses transformer deformation networks to learn backward and forward deformation fields for this transformation. Results show promising accuracy compared to existing techniques with both quantitative and qualitative evaluations. The paper also includes ablation studies to assess the impact of various components. Strengths and Weaknesses: Strengths: Clear and wellwritten presentation of the method and technical details. Strong motivation and clear definition of the contribution. Effective use of a canonical space to simplify the deformation learning process. Consistent outperformance of competing methods in both quantitative and qualitative evaluations. Weaknesses: Potential complexity of learning two deformation models for backward and forward transformations. Requirement for dense correspondences for training which can be challenging in realworld scenarios with noisy data. Concerns about the physical realism and interpretability of the learned deformation priors. Lack of exploration of more extreme poses and realistic animal meshes. Questions and Suggestions: Can the method handle nonrealistic or physically unrealistic userspecified handles Can the authors provide a clearer explanation of how the test set was divided into seen and unseen sequences Could the authors consider incorporating ground truth meshes or colorbased error visualizations to better assess the accuracy of the results Would the method be robust to capturing deformations of more complex or partially observed meshes such as those acquired through real vision sensors", "Paraphrased Statement: Summary This paper introduces a new AIbased technique for changing the shape of 3D models. This technique uses a large database of previous examples of shape changes to predict smooth continuous changes in an objects shape throughout its volume. The input model can be in any pose. The proposed method first returns the input model to its default shape and then modifies it to the desired shape while considering userspecified constraints. The researchers propose Transformerbased Deformation Networks (TDNets) which use an encoder to learn local deformation patterns from simplified versions of the input mesh and output the desired shape change. Strengths and Weaknesses Strengths: The method is technically wellfounded. It has been thoroughly tested and compared to current deformation methods like ARAP and NFGP. Weaknesses: The methods explanation could be improved. Questions: What distinguishes the randomly sampled surface point cloud PS from the query spatial points QS More information about the query point set should be provided. Additionally in Line 195 \"querying spatial points (PS PC PT)\" should be amended to \"querying spatial points (QS QC QT).\" Are equations (6) (7) and (8) all employed during training or are equations (6) and (7) just used to create equation (8) There are several modules in the suggested network: PAB PTB Point transformer encoder and Attentive deformation decoder. Im curious about how data travels through these modules. A network diagram depicting the module connections would be useful.", "Paraphrased Summary: This study addresses surface deformation by training a Transformer network on point clouds. The approach centers around a canonical model like those used in human modeling with deformation networks that transform source models into target models. Strengths: Effectively handles mesh deformation based on userdefined handles. Clear and easy to understand presentation. Demonstrates strong performance on the DeformingThing4DAnimals dataset. Weaknesses: Limited evaluation to the DeformingThing4DAnimals dataset raising concerns about generalization. Comparison methods are more general and applicable to diverse models potentially favoring the proposed method. Overfitting to the specific dataset is a possibility. Rotation is not considered potentially limiting the deformation range. Questions: 1. Tab. 1 shows an increase in L2 error for ARAP on unseen identities which seems unexpected since ARAP is not a learning method. 2. Was the learningbased baseline method NFGP retrained on the DeformingThing4DAnimals dataset 3. What is the core technical contribution of the proposed point transformer approach", "Paraphrased Statement: Summary: This paper introduces a deep learningbased framework for shape deformation. It allows users to modify surfaces by defining desired locations for specific points addressing the limitations of geometric methods such as ARAP. The authors employ a transformerbased architecture with a twostage learning paradigm. The backward deformation network initializes the shape to a canonical position while the forward deformation network uses this canonical shape and user input to produce the deformed mesh. The training process involves two stages with three loss functions. The method demonstrates promising results on the DeformingThing4DAnimals dataset outperforming ARAP and NFGP. Strengths and Weaknesses: Strengths: Innovative approach using transformers for shape deformation. Clear motivation and wellstructured experiments. Honest and insightful discussion of limitations. Weaknesses: Complex notation and unclear descriptions in Section 3. Heavy reliance on supervised learning limiting its applicability in datascarce scenarios. Demonstrations limited to a single dataset raising questions about transferability. Lack of robustness analysis and comparison with similar studies such as \"ShapeFlow.\" Additional Questions and Suggestions: Explain the significance of model rotation equivariance. Investigate the reasons behind smooth deformation results despite not imposing explicit regularity. Clarify how ground truth matching between input and target points is established."], "prKLyXwzIW": ["Paraphrase: In 2019 the SEVER algorithm was introduced as a robust method for approximating critical points of a collection of functions f1 ... fn despite the presence of corrupted samples (up to a fraction of \u03f5n). This paper presents a modified version of SEVER that specifically estimates generalized method of moments (GMM) estimators. These estimators can be expressed as argmin \u03b8\u2208\u0398 1n \u2211i1n g(Xi\u03b8)\u00b2 where g is a vectorvalued function. The proposed algorithm efficiently computes GMM estimators even in high dimensions and exhibits robustness against a fixed proportion of outliers. Its performance is evaluated on synthetic and nonsynthetic datasets. Strengths and Weaknesses: The paper is wellwritten and provides a solid rationale for the modified SEVER algorithm. GMM methods have broad applicability so a robust and computationally feasible method for estimating them is valuable. However most technical results and proofs are adaptations from the original SEVER paper with no novel techniques introduced. While generally clear several sections (4 and 5) include sections that are difficult to follow due to potential typos or unexplained concepts. Additionally the proofs in Section 6 require further verification. Questions and Typos: The filtering algorithm is described as standard yet Lemmas 4.1 and 4.2 appear to be new. Lemma 5.1 likely requires assumptions on the norm of w0 and R. Theorem 5.4 lacks a definition for p. Lemma A.1s proof assumes \u03bb \u2264 L without justification. Lemma 4.1s proof contains potential numerical inconsistencies. Lemma 5.2s proof may implicitly use assumptions on w0 and R. The proof of Theorem 5.4 lacks details on why St \u2265 2n3 for all t and the initialization of X1. Typos in Original Text: Line 182: \"had\" should be \"held\" Line 478: \"\u221aL\u03b5\" should be \"\u221aL\u03b5\" Line 511: Step 3 is not defined Line 518: \"u\" is not defined Line 528: Third line should be \"E[Igood]\" instead of \"E[I1good]\"", "Paraphrase: Summary: This study presents an algorithm and its theoretical protections for the Generalized Method of Moments (GMM) algorithm where a fraction of samples could be intentionally corrupted. These protections hold under specific assumptions about the uncorrupted data and the moment function. The samples from instrumental variables linear and logistic regression models often meet these assumptions. The approach builds on previous work but offers significant advancements. The methods are claimed to be computationally manageable and efficient compared to current Robust GMM approaches. Strengths and Weaknesses: The paper is wellwritten and makes a valuable contribution. However the sample complexity of the instrumental variables algorithms presented in the paper is theoretically high potentially limiting their practical feasibility. The synthetic data experiments were conducted with parameters that violate these theoretical bounds suggesting that the bounds may be overly conservative. The NLSYM data experiment used a reduced dataset which may impact the results. Questions: The authors are requested to clarify how their approach compares to existing methods [20] and [11] which were described as computationally inefficient in the paper. Additionally they are asked to comment on the potential consequences if the NLSYM experiment were conducted with the full dataset.", "Paraphrased Statement: Summary: This research examines robust inference using the norm of generalized moments as the loss function. The authors introduce a robust GMM algorithm based on the SEVER algorithm and iterative filtering. The algorithm simultaneously filters moments and their directional derivatives. Strengths and Weaknesses: The authors develop a novel robust GMM algorithm based on SEVER and iterative filtering. Theoretical guarantees are provided for the proposed algorithm. Case studies are presented for robust IV linear regression and logistic regression. However concerns arise regarding the optimality of the convergence rates particularly in Theorems 5.5 6.2 and 6.3. Questions: 1. The authors are asked to comment on the optimality of the results particularly in Theorems 5.5 6.2 and 6.3. In the case of robust mean estimation it is pointed out that Theorem 5.5 provides a tight bound with the exception of a lower breakdown point compared to previous work. The authors are encouraged to mention this after Theorem 5.5. 2. The optimality of Theorems 6.2 and 6.3 is questioned. It is noted that for a strongly convex function with bounded variance the parameter estimation error is typically of order O(\\epsilon). However the rate under bounded 8th moments is still O(\\sqrt\\epsilon). The authors are asked to comment on the tightness of this rate and whether other properties (e.g. strong convexity) can improve the rate in Theorem 5.5.", "Paraphrased Statement: Summary: This study introduces a computationally efficient Generalized Method of Moments (GMM) estimator resistant to a proportion of outliers. The authors apply this estimator to instrumental variable (IV) linear regression and IV logistic regression under specific assumptions about variables. Numerical results support the theory. Strengths: Efficient and robust GMM estimator. Wellwritten paper with sound theoretical findings. Weaknesses: Limited Novelty: The algorithmic and theoretical contributions appear incremental compared to previous work particularly the SEVER algorithm for robust optimization. The authors need to highlight unique contributions. Complex Assumptions and Parameters: Assumption 3.1 and the algorithms require numerous parameters making it difficult to understand and use for practitioners. Unconvincing Experimental Results: Synthetic data results are inconclusive while real data results are messy and show mixed performance against alternative methods. Missing References: The authors should provide references for claims about the robustness of classical estimators and the importance of IV regression. Suggestions: Specify the full name of the SEVER algorithm. Present experimental results as separate figures for clarity."], "rOimdw0-sx9": ["Paraphrase: In metareinforcement learning its often assumed that the distribution of tasks during training matches the distribution of tasks encountered during testing which isnt always true. This paper tackles this problem by introducing an adaptive method that learns multiple policies with varying degrees of distribution shift. Strengths: Addresses a common issue in practical metareinforcement learning. Proposes a reasonable approach for learning policies that can handle distribution shifts when the test distribution is unknown. The paper is wellstructured and easy to follow. Weaknesses: Theoretical analysis is only applicable to a simplified environment. The method only captures distribution shifts in the reward function while in practice tasks often vary in both reward and transition functions. Policy selection during testing is based on a multiarmed bandit problem which could potentially be improved. Questions: 1. Why didnt the authors consider capturing distribution shifts in both the reward and transition functions 2. The assumption of a normal prior with zero mean and identity covariance for the training task distribution seems unrealistic. 3. Can the multiarmed bandit problem faced during policy selection be shown to have unimodal properties If so could this be used to improve the selection method", "Paraphrased Statement: Metareinforcement learning (metaRL) agents often struggle when faced with test tasks that exhibit distributional shifts from the training distribution. To address this challenge this paper proposes a method called DiAMetR that leverages distributional robustness to train a population of metapolicies for varying levels of distribution shift. DiAMetR begins by training a single metapolicy in the original training distribution. Using the collected trajectories it learns a probabilistic latent space that represents task distributions. Subsequently each additional metapolicy is trained in an adversarial game against a task proposal distribution that introduces a controlled degree of distribution shift. During test time a bandit algorithm based on Thompson sampling selects the appropriate metapolicy (corresponding to a specific distribution shift level) to interact with the test task. DiAMetRs performance is evaluated in various continuous control environments demonstrating its improved adaptation to test distribution shifts compared to existing baselines. The paper also includes theoretical analysis and ablations to validate the methods components. However concerns remain regarding its computational cost during test time and its ability to handle more complex distribution shifts in the dynamics of metaRL environments.", "Paraphrased Statement: This research presents a novel metareinforcement learning (metaRL) algorithm designed to address distribution shifts in testing environments. The approach involves training a diverse population of policies with varying levels of distributional robustness through an adversarial algorithm. During testing an efficient bandit algorithm selects an optimal policy from this population to handle distribution shifts. Empirical results show that this method outperforms existing approaches in adapting to such shifts. A theoretical analysis bounds the expected regret difference induced by misspecification of the robustness factor epsilon. Strengths: The proposed metaRL algorithm is innovative and demonstrates improved performance compared to other methods. The presentation is clear and easy to follow. Weaknesses: Lack of detail in the experimental section. Focus on simplified task spaces with uniform goal distributions and limited shifts in reward functions excluding transition dynamics. The extent of policy conservatism chosen by the bandit algorithm is unclear. Absence of gametheoretic treatment despite initial mention. Questions and Concerns: Major Concerns: Equation 1 should express expected regret as the expected return of the metapolicy not an MC estimate. Figure 5 has illegible fonts and incorrect legend. \"Diameter oracle\" is unexplained. Its performance should be below that of the \"diameter adapt\" policy. A less conservative approach may yield better results by optimizing policies for specific task distributions. Figure 7 does not show the growth of conservatism with epsilon. The distribution of selected policies from the metapolicy population should be analyzed. Equation 3 does not incentivize exploration as claimed. The number of adaptation policies in the experiments is unclear. Minor Concerns: Clarify \"test reward distributions\" in Section 6.2. Explain the projection method used in Section 6.2. Justify the use of both conservative and nonconservative policies. Address all listed points for a higher score.", "Paraphrased Statement: DiAMetR a novel framework for meta reinforcement learning (meta RL) aims to enhance meta policies adaptability to varying degrees of task distribution shifts. It reformulates the meta RL objective within a distributionally robust framework optimizing for the worstcase empirical risk under an uncertain set of testtime task distributions. DiAMetR models the latent representation of reduced reward functions parameterizing task distributions. The training process alternates between: 1. Training the meta policy against multiple imagined task distributions each with varying distribution deviation constraints and 2. Adversarially training the imagined task distributions. DiAMetRs effectiveness is demonstrated in goalreaching environments exhibiting superior adaptability to various levels of task distribution shifts compared to existing methods like RL2 VariBAD and HyperX. Strengths: Clear presentation of the algorithm Novel and general idea in meta RL Formal analysis and evaluation from multiple perspectives Weaknesses: Limited scope as DiAMetR assumes unchanged transition dynamics and sparsely rewarded environments Unclear performance in densereward or nongoalreaching environments Reliance on a specialized model structure (structured VAE) for sparsereward environments Uncertainty in the generalization ability of reward function models due to reliance on samples collected under the original task distribution Missing experimental details on task distribution deviation selection stateaction trajectory collection and baseline method finetuning Questions: 1. How do the authors address the generalization ability of reward function models trained with samples from the original task distribution 2. How are stateaction trajectories collected during meta policy training with imagined task distributions 3. How are the task distribution deviation constraints chosen 4. Do the baselines use finetuning during testtime adaptation 5. Can the authors provide an analysis of the relationship between the adaptively chosen task distribution arm and the oracle test distribution"], "sQ2LdeHNMej": ["Summary This paper presents a method for concurrently optimizing the step size number of epochs and batch size in the FedAvg federated learning algorithm. It uses hypergradientbased updates for these parameters. The algorithm has been evaluated on the FEMNIST and StackOverflow datasets. Strengths and Weaknesses Strengths: Addresses a critical problem in federated learning. Considers communication and local computation complexity. Evaluates the method on relevant benchmarks. Provides code for reproducibility. Weaknesses: Lack of detailed explanations in the derivation of hypergradients. Assumption 2 used in all theorems appears overly strong. Limited experimental comparison to FedAvg only excluding other methods. Some incorrect references to previous federated hyperparameter optimization work. Optimization is restricted to three specific hyperparameters. Questions 1. Clarification on the term \"openboxed\" (line 12). 2. Request for improved clarity on a sentence (line 56). 3. Definition of \"relaxed condition\" (line 68). 4. Explanation of the incomparability of two previous studies (line 98). 5. Suggestion to use a different notation for the set of real numbers (\\mathbbR instead of \\mathcalR) (line 129). 6. Query about the representation of A B and I in Equation (150). 7. Confirmation of the objective function for two previous papers (line 169). 811. Questions regarding assumptions bias as a hyperparameter and the nature of the function f (convex or nonconvex).", "Paraphrase: Despite not being an expert in federated learning I lack experience with hyperparameter optimization in this context. The paper introduces an efficient approach for online hyperparameter optimization in federated learning (FATHOM). FATHOM outperforms existing methods with manually tuned hyperparameters. However its focus on optimizing certain hyperparameters limits its applicability. The evaluation against a baseline (FedAvg) demonstrates FATHOMs efficiency and improved accuracy. Strengths: Clear motivation and limitations. Improved results compared to FedAvg with various hyperparameter initializations. Transparent evaluation methodology. Questions: The reviewer cannot assess the evaluation protocol due to a lack of knowledge of related work. The target accuracy values appear arbitrary but may be standard for this type of evaluation.", "Paraphrase: Summary: This paper proposes a method to optimize both hyperparameters and model parameters of a federated learning model simultaneously using gradient descent. It extends existing work on \"hypergradients\" to calculate gradients with respect to local learning rates and the number of gradient steps. The methods convergence is analyzed and empirical improvements are demonstrated over the \"FedAvg\" algorithm through realworld federated learning experiments. Strengths: The method can jointly optimize hyperparameters and model parameters in a single online pass which is promising and natural. The approach is reasonable involving gradient calculations for the target hyperparameters and subsequent optimization with model parameters. Weaknesses: Related Work: Relevant works are omitted: Reference [a] focuses on singleshot hyperparameter optimization for federated learning. Reference [b] employs Bayesian optimization for hyperparameter optimization in federated learning. Reference [c] provides a benchmark for federated hyperparameter optimization. Experiments: The proposed method is only compared to vanilla FedAvg which may not provide a comprehensive evaluation. Theoretical Analysis: Theorem 3: Instead of presenting detailed expressions the paper could offer interpretations or insights on the theoretical results. It could explore how the proposed algorithms additional components impact convergence compared to FedAvg. Technical Details: Assumption 2 requires clarification and justification. The derivation of Equation (5) should be explained. The design of Equations (14) and (15) should be discussed. Typos: line 34: \"a\" \"an\" line 56: \"justifying a\" \"an\" line 104: \"directing\" \"directly\" line 145: \"K\" \"Ki\" Questions: See \"Weaknesses\" section above."], "qTCiw1frE_l": ["Summary: This paper presents a phenomenon termed \"policy churn.\" It explores the rapid changes in greedy policy (10 of states) during valuebased RL algorithm learning processes. The authors empirically show that this churn drives exploration in Atari games. They observe it in DoubleDQN and R2D2 suggesting its a common phenomenon. Potential causes are investigated including the hypothesis of nonlinear global function approximation and noisy learning. The paper discusses implications and future research directions. Strengths: Exploratory and explanatory work is undervalued but crucial for understanding RL algorithms. The paper is wellwritten and clear demonstrating the phenomenon with empirical evidence. The research claims are sound and cautious. Weaknesses: The phenomenon is not fully explained although it is a high bar for acceptance. The generality of the phenomenon is limited to Atari environments. Recommendations: Accept the paper due to its strengths. For a stronger acceptance rating the following improvements could be considered: Extend the investigation to include other environments and RL methods. Operationalize and investigate the mechanistic hypothesis in more detail.", "Paraphrase: Summary: This paper introduces the concept of \"policy churn\" a phenomenon in reinforcement learning (RL) that has previously gone unnoticed. The paper demonstrates that policy churn is linked to deep learning and suggests that it contributes to effective exploration. Strengths and Weaknesses: This papers exploration of policy churn is valuable as it sheds light on a previously overlooked phenomenon. However there are some areas where improvement could be made. Questions: 1. In Figure 2 how is a policy change caused solely by churn differentiated from those caused by a combination of epsilon and churn or only epsilon 2. Policy churn may not always be beneficial particularly when the policy is converging. Could the authors discuss techniques for managing policy churn during convergence and how this might enhance RL performance 3. The claim that policy churn complements basic noisebased exploration (line 323) seems questionable as Figure 2 shows that combining the two approaches does not consistently yield the best results. 4. Could the authors clarify the distinction between policy churn and noisebased exploration Did they conduct experiments to determine whether policy churn offers similar exploration benefits such as controllable exploration Minor Issue: It may be clearer to use a subscript for P instead of the variable \\barW.", "Paraphrased Statement: This study investigates policy churn a rapid change in the chosen action for each state in valuebased reinforcement learning. Experiments demonstrate that this churn occurs frequently during training and continues even after performance plateaus. The churn is shown to have an exploratory effect reducing the need for additional exploration strategies. Removing it leads to a decline in performance. Ablation studies eliminate potential causes of churn suggesting that it arises from the nonlinearity of the function approximation and stochastic updates. Strengths: Policy churn is a novel and significant finding. Comprehensive experiments and ablations provide strong evidence. Clear and accessible presentation. Weaknesses: Practical applications of policy churn are not yet clear. The exact cause of policy churn remains unresolved. The evidence for the churns exploratory effect is unconvincing. Questions: 1. How is it concluded that removing churn reduces performance given the comparable performance of blue curves to green and gold curves in Figure 2 2. Removing churn by \"acting with the target network\" introduces other changes that may confound the results. Is there a more isolated way to remove just the churn"], "mXP-qQcYCBN": ["Paraphrased Summary: This study introduces a method for learning keypoints from images. The keypoints are connected by lines to form a shape representing an object class. The method reconstructs the original image using masked versions and the generated edge map. The edge weights are learned and shared across all objects in the dataset. The method is evaluated on human pose datasets and shows the importance of edge modeling. Strengths: Novel selfsupervised keypoint learning approach with learned edge weights. Extensive evaluations and ablation studies. Clear presentation with visualizations. Weaknesses: Requires images of the same object class for training. Keypoints may cover image edges instead of meaningful object locations. Relationship to unsupervised foreground segmentation methods should be clarified. There are simpler and comparable methods for keypoint discovery. Table 2 could be updated with a relevant reference. Questions: Further clarification on \"structured background.\" Impact of using masked images vs. extracted appearance features.", "Paraphrase: This research presents a new method for identifying landmarks in image collections without manual annotations. Unlike previous methods that find individual landmarks this approach focuses on establishing connections (skeletons) between them. The technique leverages an autoencoder that uses edge maps and masked images as intermediate representations forcing the network to parse structural information into salient keypoints and edges. Experiments on human and face datasets show improved performance compared to existing landmark detection and part discovery techniques. Visual results suggest that the method also applies to other objects though the quality of those results is less clear. Strengths: Simple and effective landmark discovery method Masking strategy focuses on image structure rather than appearance Differentiable edge estimation models skeletal connections High accuracy on facial and human datasets Weaknesses: Limited results on 2Dlike images with minimal viewpoint variations No quantitative evaluations on more 3Dlike datasets (e.g. PascalVOC) Unknown performance on objects with complex skeletons (e.g. zebras hands) Limited visual examples for objects other than horses and zebras Questions for Further Strengthening: Can unsupervised saliency improve results by directing the network to foreground regions Can the method adapt to given object skeletons attaching them to correct image locations Suggestions: Provide a simpler explanation of edge computation (Eq. 3) Increase font size in Figures 4 and 5 for clarity Minor Corrections: Eq. 1: Missing opening bracket in the denominator Eq. 3: Missing equal sign Line 264: \"a masked images\" should be \"masked images\"", "Paraphrased Summary: This research presents a straightforward but effective selfsupervised learning framework for predicting human skeletons and object outlines. It employs an encoder to map key points onto a heatmap. Using these predictions it generates an edge graph with adjustable edge weights. An edge map is then derived based on keypoint locations and edge weights. To eliminate image structure the original image is randomly masked providing appearance information for edge map reconstruction. The model is trained using a perceptual reconstruction loss. Experiments on human skeleton and object outline datasets demonstrate its strong performance (accurate key point and edge predictions). Strengths: Clear and novel approach. Effective framework that leverages edge graphs and weights for image reconstruction. Introduces image masking to augment appearance information. Impressive experimental results including low key point prediction errors and stable predictions across continuous frames. Insightful ablation study on key point count and edge thickness. Weaknesses: Lack of training process details in the main paper. Blurriness in some figures when zoomed in. Argument about texture irrelevance may be influenced by joint training with textured objects. Questions: Are concentration constraints used to facilitate heatmap learning How can figure quality be improved for easier visualization What impact would training on textured objects only have on the models performance"], "xl39QEYiB-j": ["Paraphrased Summary: This paper proposes a technique for singlecamera human pose estimation using 2D key points and a 3D simulated scene. The simulated scene enhances the pose estimation accuracy. The method has been tested on the H36M and PROX datasets. Strengths: Simplified approach with a single inference step eliminating complex contact modeling and multistage optimization. Inclusion of a temporal gradient projection for smoothing pose estimates over time. Demonstrates effectiveness on the PROX dataset. Weaknesses: H36M dataset presents challenges due to its sparse foreground objects affecting scene modeling and potentially distracting from the main focus. The Motion Prediction Gradients (MPG) algorithm heavily relies on a previous work [3] without substantial experimental analysis. Limited discussion of performance on the PROX dataset making it challenging to fully evaluate the approach. Disparity between 2D keypoint detection and 3D depth generation in the simulated scene. Questions: Could alternative temporal prediction models like LSTMs or transformers improve the MPG performance Are there more suitable datasets that could better demonstrate the methods effectiveness than H36M", "Paraphrase: Summary: This paper introduces a framework for estimating 3D human poses using both simulated data and 2D3D human estimation techniques. The framework can estimate both overall 3D poses and finegrained local poses. It can be applied to realworld activities like those captured in the PROX dataset. Strengths: Combining simulations and learning is a novel approach. Simulations provide valuable physicsbased knowledge that can enhance the learning process. However more evidence may be needed to demonstrate the novelty of the simulation aspect. Weaknesses: The paper does not compare the method to \"Neural MoCon Neural Motion Control for Physically Plausible Human Motion Capture.\" A comparison would help highlight the differences and strengths of both methods. The evaluation results (Table 2) indicate that the performance may not be stateoftheart. There are several typographical errors including \"scene penetration frequency Freq.\" and \"distance Pen.\" \"keytpoints\" and \"rotationrR.\" Questions: Given that PROX contains soft geometry (e.g. sofas) is it appropriate to treat them as rigid and flat Should the framework consider the deformation of geometries during interactions", "Paraphrase: This paper introduces a technique for estimating the 3D pose of a human subject based on a singleview RGB video. The scene in which the person moves is assumed to have been reconstructed in advance as a 3D mesh. The technique involves simulating the humans movement through a simplified version of the scene. This simulation is guided by a gradientbased method that connects 2D pose keypoints to the controls driving the simulated humanoid pose. The method has been tested against various existing approaches using two datasets: PROX and H36M. It performs well in terms of pose accuracy and plausibility showing comparable results to imagebased methods on H36M and generally surpassing previous work on PROX. Strengths: The novel use of physicsbased formulation offers a unique approach to 3D human pose estimation. The method achieves strong performance despite limited input information. The paper is wellwritten and includes a comprehensive analysis of the methods components. Weaknesses: The method relies on \"semiautomatic\" scene geometry simplification to handle noise in the reconstruction. The process for creating these simplified scenes and its impact on performance need to be elaborated on further. Question for Authors: Can the authors please provide more details on the process of simplifying the 3D scene geometry and its impact on the methods performance", "Paraphrase: In this study: A novel method for estimating sceneaware pose relies on the concept of embodiment. Unlike previous approaches that use multistage optimization and complex modeling this method employs a simple singlestage process to recover sceneaware pose in simulated environments. To simulate realistic poses the researchers separate camera pose from body pose and utilize a multistep projection gradient as a movement cue. This method requires only 2D observations and can be trained on synthetic data for realworld pose estimation. Remarkably it achieves stateoftheart results on the PROX dataset without using any training data from that dataset. Strengths: The method presents a significant advancement in simulating sceneaware humanoid motion to mimic visual observations and recover 3D poses in realworld settings. It serves as a solid baseline for sceneaware pose estimation. The method delivers promising results on challenging poses in the PROX dataset. The multistep projection gradient contributes significantly to 3D pose recovery. The paper is wellwritten and comprehensible. Weaknesses: The simplified scene representation may lead to errors in poses involving sitting or lying. The contact modeling between motion and the scene needs further clarification. Questions: 1. How does the proposed method handle contact information modeling a key distinction from other simulationbased mocap methods 2. Is it necessary to train new agents for different motion sequences using this method"], "p9_Z4m2Vyvr": ["Paraphrase: Summary: This study introduces a new amortized clustering technique that combines two key strategies: IntraCluster Mixing and InterCluster Coupling. IntraCluster Mixing uses optimal transport to allocate data points to reference vectors. InterCluster Coupling then uses this allocation to generate new clusters and update existing ones. The results on both synthetic and realworld datasets show that the method is both accurate and efficient. Strengths: The method is innovative and practical. Unlike previous methods it directly models the cluster generation process. It leverages optimal transport for optimization in the E step. The paper is wellorganized and clear. Weaknesses: The explanation of Figure 2 is brief. Adding more details would enhance understanding of the learning procedure. The differences between the proposed method and previous work (reference [23]) are not fully explained. More information on these differences would be beneficial. Questions: How sensitive is the method to the choice of prior distribution settings (e.g. l1KU(09) in line 302)", "Paraphrased Summary The research presents a model for clustering data incrementally using a metalearning approach to determine the underlying cluster distribution. Inference is performed sequentially for each cluster considering both intercluster and intracluster similarities. To optimize the process the authors employ a linear optimal transport strategy that avoids computing distances between all cluster pairs. The models effectiveness is demonstrated on simulated data and realworld datasets (MNIST TinyImageNet CIFAR10). Strengths and Weaknesses Strengths: Paper is wellwritten and easy to understand. Novel use of optimal transport in cluster inference. Demonstrated benefits over traditional EM algorithm. Weaknesses: Some sections contain complex mathematical equations. Limited potential impact due to lack of practical applications. Questions Section 4.1: Clarification on what the \"training set\" refers to in dataset with 200 samples and 4 clusters. Tables 1 and 2: Confirmation if results are based on a single test set or averaged over multiple sets. Additional Experiments: Comparison of model performance during training. Qualitative toy example demonstrating the difference between the optimal transportbased approach and existing models.", "Paraphrase: Summary In this work a novel clustering technique known as Amoritized Mixing Coupling Processes (AMCP) is presented. AMCP incorporates both intracluster mixing (IntraCM) and intercluster coupling (InterCC) to capture samplereference distribution connections and assign samples to clusters or generate new ones. Experimental evaluations using synthetic and realworld data demonstrate the effectiveness of ACMP. Strengths and Weaknesses Strengths Introduction of the AMCP clustering algorithm Superior performance in clustering accuracy and computational efficiency compared to existing methods Weaknesses Lack of clarity regarding the papers main contributions Insufficient detail on related work and how AMCP differs from existing methods Ambiguity about the relationship between InterCC and other amortized clustering approaches Lack of information on whether the paper is the first to apply optimal transport to clustering Absence of a brief introduction to optimal transport plan entropic regularized Kantorovich relaxation and its relevance to clustering Unclear assumption behind the clustering method and justification for using samplereference distribution relationships to define clusters Potential convergence issues due to the dependency between bij(k) and Ck in Equation (11) raising concerns about the validity of separating the optimization process into IntraCM and InterCC Questions: 1. What are the specific contributions of this paper 2. What assumptions are made in the proposed clustering method and how does it compare to existing amortized methods 3. Is the objective function in Equation (11) guaranteed to converge and if so how 4. In Table 1 does \"time\" refer to the training time for each method"], "m97Cdr9IOZJ": ["Summary This paper demonstrates the expressive capabilities of invertible neural networks (CFlows) in approximating diffeomorphisms. It establishes that if CFlows can approximate affine transformations in a certain function space they can approximate a broader class of diffeomorphisms. The paper also shows that a specific type of CFlow SACF can approximate any diffeomorphism with infinite smoothness. Based on these findings a new CFlow architecture called ParaCFlow is proposed which guarantees universality under certain assumptions. The practical utility of ParaCFlow is tested in a contextual Bayesian optimization task. Strengths Extends existing theories with simplified proofs. Experiments involve tasks requiring function derivatives aligning with the theoretical analysis. Clear and wellorganized presentation. Weaknesses Similar research has explored the universality of invertible models. ParaCFlow may not be suitable for common applications of invertible models such as image generation. Novelty Builds upon prior work on the universality of invertible models with respect to function derivatives. Does not significantly diminish the novelty of the cited study due to recent publication. Quality Mathematical proofs are sound. Application to contextual Bayesian optimization is appropriate. Clarity Paper is wellstructured and easy to follow. Mathematical descriptions are clear. Significance Proves the universal approximation capabilities of SACFs extending the results of previous work. Simplifies the proof of universality making the concept more accessible. Deepens the understanding of the expressive power of invertible models. Questions Correct a typo in line 221: Replace \\mathcalA\\in \\mathbbRda with \\mathcalA\\subset \\mathbbRda. Define \"Te\" in line 353.", "Paraphrased Summary: This paper explores the ability of coupling flows to approximate arbitrary functions beyond simply matching probability distributions. The authors prove that with added padding dimensions invertible neural networks generated by coupling flows possess Ckuniversality. They extend these results to parametric problems. These approximation theorems support the development of ParaCFlows a new model that leverages parametric problems and dimension augmentation. Empirical results demonstrate ParaCFlowss effectiveness in learning complex transformations. Strengths and Weaknesses: Strengths: Clear and wellwritten presentation. Detailed roadmap of theorem development. Convincing experimental results. Weaknesses: May be overly technical for readers focusing on normalizing flows for distribution learning. Reviewer cannot fully assess the significance due to their expertise limitations. Suggestions for Improvement: Organize Section 3 by stating the main theorem first followed by supporting techniques and partial theorems. Highlight the model construction steps in Section 4 using indentation or a text box. Consider moving Figure 6 to the main text if possible.", "Paraphrased Statement: Summary This paper demonstrates that specific types of coupling flows called flows with single coordinate affine coupling layers can approximate any diffeomorphism (smooth transformation) with a nonzero derivative in Ck norms (measures of smoothness). These flows can be extended to include context (additional parameters shared across all layers). Experiments comparing them to other models on synthetic data show promising performance in diffeomorphism approximation and contextaware optimization tasks. Strengths Theoretical Soundness: The proofs are clear and accessible. Novel Proof Technique: The approach potentially applies to other theoretical results in deep learning. Theoretical Support: The paper supports the recent finding that padding improves coupling flow training. Weaknesses Limited Bounds: The bounds provided do not quantify the number of layers or parameters required based on function properties. Novel Distance Function: Its unclear how the distance function used relates to common diffeomorphism norms. LowDimensional Experiments: The experiments on diffeomorphism approximation are conducted in low dimensions and the impact of higher dimensions is unknown. Questions Proof Clarification: Theorem 3.3 requires additional explanation especially in relation to Lemmas B.4 and Corollary B.6. Norm Equivalence: Its not evident that the C1 norm is sufficient to ensure approximations of the identity in all Ck norms. Convergence Bound: The derivation of the bound on s1 in Theorem 3.3 requires further clarification. Additional Comments Typographical errors have been identified in the text. Inconsistent notation for the function b(ac) can be confusing."], "o8vYKDWMnq1": ["Summary (Paraphrased): This research examines the theoretical foundations of the deep Qnetworks (DQN) algorithm in an online episodic Markov decision process (MDP) setting with T episodes. The study utilizes Besov and Barron function spaces to represent a \\alphasmooth Q function in a ddimensional feature space. Under certain assumptions the paper demonstrates that DQN algorithms with limited layers and reasonable widths can effectively learn the MDP resulting in sublinear regret. Strengths: Provides theoretical insights into the widely used DQN algorithm used in deep reinforcement learning (RL). Presents novel theoretical results indicating that DQNlike algorithms can achieve sublinear regret using modestsized neural networks under specific assumptions. Aligns with practical observations. Weaknesses: The algorithm relies on minimizing a neural network function which can be challenging to achieve in practice. The regret bound in Theorem 1 does not depend on the dimension d while in Theorem 2 it does. The reason for this difference is unclear. The role of the sparsity parameter S defined in Eq. (5) is unclear. Questions: 1. What is the typical magnitude of the norm parameter B in practical DQN applications 2. Why is the regret bound in Theorem 1 independent of the dimension d while in Theorem 2 it depends on d 3. What is the significance of the sparsity parameter S in Eq. (5)"], "q5h7Ywx-sS": ["Summary In constructing a similarity graph from a set of points the goal is to connect similar points while avoiding edges between dissimilar points. Traditional methods like distancebased thresholds or selecting knearest neighbors can be computationally expensive and lead to overly dense graphs. This paper proposes an alternative approach that connects similar points through paths of length 2 instead of direct edges. This reduces the graphs sparsity and computational cost. Bucketing points based on similarity and selecting a representative from each bucket allows for a straightforward graph construction. To generate a more robust graph the algorithm repeats the process with several random partitions. Previous research on this approach has been theoretical but this paper applies it to nearneighbor preserving graphs on a large scale. Strengths and Weaknesses This paper presents a valuable contribution to the field of largescale graph construction. It is wellwritten and demonstrates the practical applicability of the proposed approach. However it lacks some theoretical novelty as it leverages existing LSH techniques. The technical writing could be improved by clarifying undefined notation and addressing lapses in the proofs. These issues may hinder the papers accessibility for some readers. Questions and Limitations The paper raises questions about the potential drawbacks of using 2hop paths instead of direct edges. It does not explicitly discuss which downstream tasks this approach is suitable for or the implications for tasks where the greedy traversal method needs to be modified. Additionally it is unclear whether there are guarantees that dissimilar points are excluded from the 2hop neighborhood. This could lead to latency issues in downstream tasks where irrelevant nodes are included in the query."], "q-LMlivZrV": ["Paraphrase: This study introduces CLOOB a technique that uses modern Hopfield networks to enhance the understanding of covariance patterns in original multimodal data. To reduce the impact of Hopfield networks on InfoNCE the authors suggest using the InfoLOOB objective. Experiments on various classification datasets have shown the effectiveness of CLOOB. Strengths: Clear motivation: improving cohesion in multimodal data. Demonstrated effectiveness in downstream classification tasks. Questions: Additional computational cost introduced by the Hopfield Network. Applicability to other pretraining methods (e.g. DECLIP FILIP). Evaluation on imagetext retrieval tasks. Concerns about the performance of CLOOB on RN50 pretrained on CC3M compared to DECLIP.", "Paraphrase: Summary: This research introduces the CLOOB objective for imagetext training. However it lacks originality as similar concepts (modern Hopfield networks and InfoLOOB) have been explored before. While CLOOB shows improved zeroshot performance its linear probe results are inconsistent raising concerns. Strengths: CLOOB outperforms reimplemented CLIP baselines on ResNet architectures. The authors provide insights into the advantages of Hopfield networks and InfoLOOB. Weaknesses: CLOOB lacks novelty due to the existence of previous research on InfoLOOB and modern Hopfield networks. Improvements from InfoLOOB have already been reported by previous studies. The evaluation only includes 8 tasks less than the 30 tasks assessed by CLIP. The work compares CLOOB to a small batch size (5121024) compared to CLIPs large batch sizes (16K32K) limiting the comparison. Linear probing results are mixed contradicting the claimed improvements in zeroshot performance. Some content in the paper is repetitive. Questions: How does CLOOBs training time compare to CLIP Have the authors tested CLOOB with larger backbones like Vision Transformers (ViTs) Could larger datasets be used to train larger models"], "p_g2nHlMus": ["Paraphrased Statement: Summary: This research employs a pretrained Vision Transformer (ViT) model for feature extraction in fewshot classification problems. To leverage connections between patches the authors introduce a token reweighting mechanism during both training and testing. This approach demonstrates effectiveness in various FSL benchmark datasets. Strengths: The paper is wellstructured and easy to comprehend. Utilizing selfsupervised ViT for patchlevel representation is innovative and beneficial for fewshot learning. Selfsupervised pretraining improves model generalizability enabling it to perform well on diverse downstream tasks. Patchlevel features from ViT provide finergrained information. Weaknesses: [1] The authors claim that selfsupervised pretraining significantly outperforms supervised pretraining in Figure 4. However previous SSL literature suggests that SSL pretraining often only slightly surpasses supervised pretraining. The authors should provide a more thorough explanation for the large margin observed in their results. [2] The tSNE visualization in Figure 5 merely demonstrates that embeddings from the same instance are clustered together. It does not illustrate class discrimination which is crucial for FSL. The authors should also show if embeddings from the same class are grouped and separated from other classes. [3] The paper emphasizes \"generalization\" in fewshot learning but it lacks comparisons to recent crossdomain fewshot learning approaches. [4] The authors chose Masked Image Modeling (MIM) as the selfsupervised pretraining task but it would be beneficial to explore other options such as contrastive learning. They should justify their choice of MIM or provide insights on its suitability. [5] Unlike some SOTAs the proposed method requires additional learning for test data. The authors should address this aspect and explain its implications.", "Paraphrase: The paper focuses on the challenge of classifying images with limited training data (fewshot classification). Its unique approach establishes connections between patches in support and query images to determine the class of the query image. To minimize the effects of irrelevant background patches the authors use an online optimization strategy to identify the most relevant patches in support images for classification. The Vision Transformer model is applied to encode patches from both support and query images. The Vision Transformer is initially trained unsupervised using a masked image modeling task to develop robust general features. This selfsupervised pretraining strategy outperforms supervised training methods. The proposed approach achieves stateoftheart results on four benchmark datasets for fewshot classification. Strengths: Clear and accessible written content Innovative method for fewshot classification using patch correspondences Online optimization for identifying critical regions for classification Joint reasoning over support and query images Effective selfsupervised pretraining for general feature learning Stateoftheart results on multiple benchmarks Thorough analysis and ablation studies Weaknesses: Lack of comparison with existing methods using the same backbone network (Vision Transformer) after selfsupervised pretraining Absence of model size comparisons with other backbones used in prior work Potential computational concerns when dealing with large datasets or numerous classes and samples Questions: Benefits of the proposed classifier compared to simple linear prototype classifiers with the same backbone Model size comparisons for different backbone networks used in the study Strategies for addressing computational complexity when dealing with large datasets or numerous classes and samples", "Paraphrased Statement: Summary To address issues with standard fewshot training on weak imagelevel labels the paper introduces a tokenbased approach using unsupervised vision transformers. This approach reweights tokens based on their discrimination during training. The model outperforms previous methods demonstrating the effectiveness of vision transformer models in fewshot tasks without additional pretraining. Strengths The use of vision transformers for fewshot learning is a significant contribution. The model demonstrates the feasibility of fewshot learning using unsupervised pretraining. The results show notable improvements across various benchmarks and architectures. The token aggregation and reweighting approach is clear and practical. Weaknesses The strong results come with a caveat: the ViTsmall and Swintiny architectures have significantly more parameters (22M and 29M respectively) than the baseline ResNet12 architecture (12M). This raises concerns about comparing models of different sizes. The study lacks an empirical evaluation of vision transformers specifically for fewshot classification. This makes it difficult to determine the improvement attributed to token reweighting versus the vision transformer backbone. There is no ablation study to assess the impact of token reweighting and the logsumexp aggregation scheme. It is unclear how these components contribute to the models performance. The models design choices such as the choice of token similarity metric are not fully explained or supported by empirical evidence. The paper omits a relevant citation regarding a similar token reweighting scheme in \"Batch Folding from Few Shot Learning with Localization in Realistic Settings CVPR2019.\" Questions How should readers interpret the differences in model sizes and their impact on the results Can the authors provide ablations to clarify the contributions of the vision transformer backbone token reweighting and logsumexp aggregation Is the innerloop token reweighting scheme compatible with more advanced patchtopatch techniques How does the model perform under a standard metafinetuning setup (e.g. Prototypical) Additional Comments There are a few minor typos throughout the paper.", "Paraphrased Statement: Summary: This research addresses the task of imagelevel annotation in fewshot classification. The authors employ a transformer architecture to generate feature tokens from image patches prioritizing tokens relevant to the given class label. During testing the model selects tokens from within the support images that are most discriminant for the task. Experimental results demonstrate consistent improvements in performance. Strengths: 1. The transformeronly architecture simplifies the model and sets a potential baseline for future research. 2. The feature reweighting strategy enhances taskspecific learning by focusing on relevant information. 3. The approach consistently improves performance across various datasets. Weaknesses: 1. A comparison with other feature reweighting techniques in fewshot classification would strengthen the discussion. For example Lee et al. [A] have reported significant improvements by incorporating an attentionbased reweighting module for finegrained fewshot classification. 2. The concept of \"meta finetuning\" should be clarified to explain its role during inference time. Question: The definition of \"meta finetuning\" is unclear. Does it refer to the process of adjusting the models parameters based on the support data at test time [A] Cited Reference: Lee B. G. et al. \"Task Discrepancy Maximization for FineGrained FewShot Classification.\" CVPR 2022."], "s_PJMEGIUfa": ["Summary: This study investigates the feasibility of using pretrained language models (PLMs) without altering their input and output formats for finetuning on nonlanguage tasks. Method: The authors propose a framework called LanguageInterfaced FineTuning (LIFT) that transforms nonlanguage tasks into natural languagelike sentences using predefined templates. They finetune autoregressive PLMs on these transformed datasets and demonstrate their success on various nonlanguage tasks. Contributions: Demonstrates the effectiveness of finetuning PLMs for nonlanguage tasks without format changes. Provides insights into the properties of LIFT and its potential benefits. Strengths: Novel approach to nonlanguage task finetuning using PLMs. Extensive evaluation on diverse datasets. Weaknesses: Unclear whether the performance is due to the model size or human language pretraining. Questions about the practicality and usability of LIFT in realworld scenarios. Input length limitation and potential memory inefficiency. Fragmented and incomplete presentation of the content. Poor formatting and small figures in certain sections. Missing reference to previous work on finetuning BERT for nonlanguage tasks.", "Paraphrased Statement: Summary This study explores the potential of leveraging large pretrained language models (LLMs) for machine learning tasks involving unstructured realvalued inputs. Findings from experiments on synthetic classification and regression tasks indicate that finetuning LLMs yields comparable performance to traditional algorithms. Additionally LLMs exhibit Bayesian prediction capabilities and staged finetuning and feature name incorporation enhance performance. Strengths Clear writing and ample experimental details Timely and potentially impactful topic Abundant empirical results with practical insights Weaknesses Underwhelming performance of the proposed LIFT algorithm Lack of empirical support for key claims LIFTs performance is not consistently superior to shallower models indicating limited use of LLM representation. The performance advantage of larger models is unclear with GPT3 Davinci underperforming GPT3 Curie. Evidence for Bayesian inference is weak based solely on variancenoise correlation rather than true posterior inference. LIFTs use of feature names is limited and uncertain with inconsistent results across datasets. Questions How can the authors justify LIFTs performance claims given its frequent underperformance compared to simple MLPs Why do the authors conclude that larger models perform better when GPT3 Davincis results are inferior Can the authors clarify their definition of Bayesian inference and provide stronger evidence for it beyond calibrated predictions What specific datasets and features support LIFTs ability to leverage feature names and what is the intuition behind this behavior", "Paraphrased Statement: This study modifies pretrained language models (LLMs) to handle nonlinguistic tasks like function regression and classification. They assess the models performance on various tasks compared to traditional algorithms (KNN SVM RF). Additionally they examine their sample efficiency extrapolation capability and robustness. Strengths and Weaknesses: Strengths: Section 5.2 presents a novel application of LLMs for nonlinguistic tasks using pretrained knowledge to understand data and leverage context. The summary table in Table 1 provides a helpful overview. Figure 4 offers valuable insights into the relationship between training samples and accuracy for different task categories. Weaknesses: The paper lacks a clear justification for choosing LLMs over traditional methods for nonlinguistic tasks considering their increased computational cost. Nonlinguistic tasks are not defined upfront and its unclear if both classification and regression tasks will be covered. The rationale for the study as stated in the sentence \"We note that though achieving tremendous success with natural data deep learning still faces difficulties with standard machine learning tasks especially on tabular data...\" should be introduced earlier. The paper could be more concise by moving some experiments to the appendix. Additional Feedback: Consider using \"what should the y be\" instead of \"what should be the y\" in prompts. Provide the sizes of GPTJ (6B) and GPT3 (175B) in the section on pretrained language models. Increase the font size for Figure 4 text. Highlight notable results in Table 3 as the text is hard to read. Include task names in the text for context. Questions: Were alternative prompts explored How sensitive are model performances to different prompts Were fewshot prompting experiments conducted Provide details on how adversarial examples were created.", "Summary: The paper examines using existing language models for classifying nontextual data without altering the models structure. Textlike sequences are generated from the input data and fed into the model as context. The models predictions are used for classification. While its not the best performer overall it achieves unexpectedly good results. Strengths and Weaknesses: Strengths: Wellwritten and clear Intriguing research direction Extensive experimentation Surprising and counterintuitive findings (e.g. describing each MNIST pixel in text) Weaknesses: Optimal model setup is not explored. The performance of a previous study that modified input and output layers is not reported. Its unclear if using textasinput and language model predictionsasoutput helps or hinders performance. The influence of the models architecture and extensive textdata training is not discussed. The disparity in model parameters between GPT3 and baselines is not addressed."], "wwWCZ7sER_C": ["Paraphrase: Summary: In recent years researchers have explored enhancing optimization algorithms with predictions to improve performance when predictions are reliable and preserve worstcase performance when predictions are inaccurate. This study extends this concept by incorporating multiple predictions into optimization algorithms. The authors demonstrate the computational cost (which is generally minimal) of obtaining equivalent performance by using multiple predictions compared to the single best predictor. Strengths and Weaknesses: Strengths: The paper is wellwritten and addresses a timely problem. The results are innovative and the proofs appear accurate although the technical contributions to learning theory are limited. Weaknesses: The contribution is incremental and lacks significant practical utility. The study addresses the impact of using a single predictor as previously explored but focuses on improving the performance metric for multiple predictors. However the extent of this improvement relative to the added computational cost is not quantified. Questions: How much improvement in performance can be achieved by using multiple predictors considering the problem parameters and data properties Is the computational cost of using multiple predictors justified by the improvement in performance", "Summary: This paper explores using multiple machine learning predictions to enhance traditional algorithm design focusing on three specific problems: bipartite matching online makespan minimization and nonclairvoyant scheduling. For matching the goal is to improve runtime while for the other two its to enhance competitive ratio. The paper presents efficient algorithms for each setting that utilize multiple predictions. When some predictions are accurate these \"learningaugmented\" algorithms may outperform traditional worstcase counterparts. Additionally the paper establishes learnability conditions demonstrating that accurate predictions can be efficiently learned from data using PAC learning. Strengths: Wellwritten with minor typos. Novel results compared to recent literature on learningbased algorithms. Builds on previous work and provides a valuable addition to the field. Theoretically sound. Weaknesses: Some results are considered weak such as the matching extension which is seen as a simple observation. The work does not provide a general framework like previous research. Experiments are not included. Fractional solution rounding is not addressed for online makespan minimization. Minor Errors and Missing References: Several typos and missing references are highlighted in the review. Questions: Matching: Can multiple predictions enhance realworld or synthetic matching performance Graph Problems: Can multiple predictions improve the runtime of various graph algorithms Online Makespan Minimization: Is it possible to round fractional solutions online", "Paraphrased Statement: Summary: This paper investigates using a limited set of predictions (k predictions) to solve specific online problems such as matching load balancing and scheduling. The goal is to achieve performance comparable to the best possible prediction. Strengths and Weaknesses: Originality: The papers approach is original in applying k predictions to three additional problems. Techniques used in sections 3 and 4 are standard but exploring the \"learnability\" of k predictions is new. Clarity: The nontechnical aspects are wellwritten and understandable. The correctness of the technical aspects is assumed as there was not enough time for verification. Quality: Results and writing are of good quality. The algorithm in section 5 is intuitive but the mathematical proofs were not checked. Significance: The paper adds to the growing field of algorithms that incorporate predictions. The work is particularly relevant given the popularity of predictionaugmented algorithms in the research community. Questions: Related Work: The authors could compare their approach to a related work by Anand Keerti et al. which does not consider the computational complexity of obtaining optimal predictions. Line 290 (Theorem 3.4): The proof claims to find an approximate solution in polynomial time that is within O(1) of the best possible solution. However it is unclear how this is achieved given the potential exponential number of candidate solutions that need to be considered for each choice of k.", "Summary Machine learning predictions can enhance the performance of algorithms. This paper explores adapting algorithms to situations where multiple machinelearning models are employed. By using multiple specialized models the aim is to improve the worstcase performance of algorithms. The authors investigate this issue in the context of bipartite matching load balancing and clairvoyant scheduling. They demonstrate that searching for multiple predictions increases complexity by a factor of k compared to single predictions. For bipartite matching the existing algorithm for single predictions performs similarly to the algorithm for multiple predictions when k is less than or equal to O(\u221an). For load balancing there exists a polynomialtime algorithm that minimizes the expected maximum error with a logarithmic cost with respect to k. For clairvoyant scheduling it is feasible to determine the optimal k permutations in polynomial time with a bounded error compared to the best predictor. Strengths: Wellmotivated problem Careful writing Ample opportunities for future research Weaknesses: Mismatches between the main paper and supplementary material Lack of selfcontainedness in Section 2 Ambiguous communication Absence of experimental validation Questions: How does predictor quality impact algorithm performance How do single highly accurate predictors compare to uniformly good predictors Why were these three specific problems chosen Can the results be generalized to other problems"], "zfQrX05HzBO": ["Paraphrased Statement: Summary: This paper proposes a method for \"continuous category discovery\" (CCD) which expands on the concept of \"novel category discovery\" by gradually incorporating portions of target datasets into a model. The method leverages existing approaches for novel category discovery and continual learning (AutoNovel and iCaRL) within a framework featuring \"grow\" and \"merge\" phases with dual branches. Experiments on three datasets demonstrate superior performance to competing methods. Strengths: Addresses the problem of CCD combining novel category discovery with continual learning. Achieves strong results on three public datasets outperforming baselines. Weaknesses: Relies heavily on AutoNovel and iCaRL components. Additional branches and \"growmerge\" phases increase training complexity and resource demands. Experiments on relatively small datasets with a limited number of new classes at each increment. Data splits between time steps show significant variation. Assumes knowledge of new category count in each incremental step. Questions: Clarify Figure 4 and its accompanying illustration including the significance of selected clusters gray dots and time steps. Repeat experiments on multiple random data splits for enhanced credibility. Explain why DRNCD with LwF results are not reported in Tables 2 and 4 despite its reported superiority over AutoNovel in NCD. Provide details on how the novelty detection threshold of 0.6 was determined.", "Paraphrased Statement: This paper presents a novel task called continuous category discovery where a model continuously processes unlabeled data to identify new categories. The paper introduces a \"grow and merge\" framework to address this problem by allowing the model to discover new categories while integrating them with existing knowledge. Strengths: Welldefined experimental setup for novel category discovery. Effective performance of the proposed \"grow and merge\" framework on the experimental setup. Weaknesses: Complex overall pipeline and inadequate evaluation and discussion of results. Unrealistic assumption that the number of categories is known. Questions: Q1: Suggestion to use quantitative measures for visualization of feature differences. Q2: Clarification on the reasoning behind using maximum distance for sample sifting. Q3: Proposal to incorporate methods for estimating the number of categories in each time step. Q4: Request for clear definitions of concepts and a method diagram. Q5: Inquiry about the balancing of multiple losses in the framework. Q6: Concern about inconsistencies in experimental results and a request for error bars. Minor Issues: Lack of selfcontainment in defining terms like \"exemplars.\" Typographical error on line 266.", "Paraphrase: Authors introduce a continuous adaptation of the Generalized Category Discovery (GCD) algorithm called Continuous Category Discovery (CCD). CCD allows for the inclusion of both labeled data from known categories and unlabeled data from novel categories arriving in batches. To handle this setting the authors propose a framework that iteratively updates the representation of the data and discovers novel categories in CCD. Framework: The framework begins by detecting novel categories and updating the representation using pairwise similarity. Following this outliers are removed from clusters based on estimated cluster assignments. Exemplars are then updated and the representation is retrained using pseudo labels merging categories. Finally the static and dynamic representations are merged using a moving average strategy. Strengths: Proposes CCD as a continuous adaptation of GCD. Presents a framework to address the challenges of catastrophic forgetting and noisy gradients in CCD. Addresses a significant practical problem. Demonstrates good performance in experiments with various settings. Weaknesses: Lacks evaluation with different techniques especially with other continuous versions of GCD or selfsupervised representation updates. Some details of the setting and experiments are unclear including the use of additional data beyond exemplars. It is assumed that the number of novel classes is known a priori which should be stated explicitly. Suggestions: The proposed framework could be applied to other techniques to decouple its effectiveness from specific selfsupervised learning methods. An ablation study could analyze the sensitivity of the framework to the estimation of the number of novel classes. Questions: Can you clarify the size of the small dataset of training examples used for known and discovered categories and specify where this dataset is used (including whether it is used for merging)"], "xbgtFOO9J5D": ["Paraphrase: Summary This study examines the rank aggregation issue with a wide definition of proportional fairness. The authors develop a lineartime algorithm to find the closest fair ranking (CFR) using a greedy approach for Kendall tau and Ulam metrics. They also provide a novel toolkit for addressing various rank aggregation objectives meeting these fairness constraints. Mathematical proofs demonstrate that the ranking algorithm is linear in time. Strengths Presents a novel metaalgorithm applicable to any generalized mean objective and fairness criterion. Mathematically proves the existence of a lineartime ranking algorithm. Weaknesses Incomplete structure hinders readability. Lacks sections for related works and a conclusion. Limited comparison with only one existing work making it difficult to establish its superiority. Incorrect formulations such as in the generalized mean objective definition. Contribution The study introduces a new greedystrategy objective function for fair ranking and verifies that the algorithm is linear in time. However the greedy approach for ranking is not particularly innovative. Questions Which distance function Kendall tau or Ulam is more appropriate for fair rank aggregation What is the performance of the ranking algorithms in terms of precision", "Paraphrased Summary: Researchers investigate fair ranking systems where candidates from different groups are represented proportionally in the final ranking as defined by the system designer. They develop: A fast algorithm for finding the closest fair ranking under specific fairness and distance criteria. A dynamic programming algorithm for handling a fixed number of groups. They also show that combining biased rankings can result in fair rankings with minimal quality loss. Strengths: Flexible and general definition of proportional fairness. Novel fair ranking problem seeking a fair ranking that minimizes distance to input rankings. Simple metaalgorithms for fair ranking aggregation. Weaknesses: Currently only relatively simple algorithms are available. Potential for more sophisticated algorithms that avoid the closest fair ranking subroutine. Unanswered Question: Whether the researchers have considered fair ranking aggregation algorithms that do not rely on closest fair ranking as an intermediate step.", "Paraphrased Statement: Summary: This research investigates ranking algorithms that ensure fair representation of diverse groups in the final rankings. The authors develop an exact algorithm for finding the closest fair ranking under specific fairness criteria. They also provide an algorithm for weak fairness with multiple groups and propose a metaalgorithm for general rank aggregation under fairness constraints. The authors demonstrate that biased rankings can be aggregated into a fair ranking with minimal reduction in quality. Strengths and Weaknesses: The paper offers a comprehensive analysis of fair rank aggregation and includes detailed theoretical underpinnings. However the abundance of technical details may make it challenging to follow. The paper lacks experimental validation on realworld data which would enhance its credibility. Missing Related Work: In addition to the cited papers the authors should acknowledge the related work of Kuhlman and Rundensteiner (2020) which presents a method for fair ranking that is robust to label noise and maximizes ranking utility subject to demographic parity constraints. Questions: Have the authors implemented their algorithm for realworld data evaluation Can the paper be improved by moving some technical details to an appendix to enhance readability", "Paraphrased Summary: Researchers have developed methods for finding fair rankings and aggregating fair rankings. They define two types of fairness: weak fairness where fairness is only required for a specific prefix of the ranking and strong fairness where fairness is required for all prefixes. They provide an algorithm for finding the closest fair ranking for two metrics Kendall Tau and Ulam under certain conditions. Strengths: Fair ranking is a significant problem. The methods are wellexplained. The algorithms are relatively easy to understand. Weaknesses: The algorithms are simple to analyze. Fair rank aggregation may not be as important as fair ranking. The work lacks comparison to similar approaches in fair group formation and partitioning. Questions: Can the algorithms be extended to multiple dimensions where each group has a fairness criterion represented by a vector Do the algorithms work well when there is overlap between groups"], "mfxq7BrMfga": ["Paraphrased Statement: Summary: This research proposes a technique for transferring style from a target image to a source image using a singleshot domain adaptation approach. A pretrained Generative Adversarial Network (GAN) processes the source image to obtain a latent vector. The generator is then finetuned with reconstruction style and entity loss functions along with a Laplacian regularizer. The method allows selective blending of objects from the target image into the source image using a binary mask. Quantitative and qualitative results demonstrate successful style transfer and optional object transfer from target images to source images. Strengths: Motivating problem and innovative use of a binary mask for object transfer. Applicable to practical scenarios and beneficial for content creation. Quantitative metrics provide an objective evaluation of the results. Weaknesses: Lack of clarity and missing information in the paper. Insufficiently detailed related work section. Quantitative metrics not averaged across multiple samples potentially skewing results. Incomplete experimental setup for a comprehensive quantitative evaluation in comparison to baseline methods. Questions: Please provide averaged quantitative metrics across multiple sourcetarget samples. Explain the claim that slice Wasserstein distance is more efficient than GAN loss with supporting evidence. Indicate where in the figures the artifacts observed in methods with large entities can be seen. Describe the hyperparameter selection process. Clarify the task definition to emphasize that the generator is finetuned on a single target image. Move methodology details to the experimental section and present a generalized approach in the main section. Add notation and improve clarity in Figure 2. Explain the obtaining of fent and m for the auxiliary network including whether a pretrained UNet is used. Move experimental details (e.g. upsampling use of pretrained LPIPS layers) to the experimental section. Provide references for the \"Sketch Disney and Arcane\" images.", "Paraphrased Statement: This study introduces a flexible framework for adapting GANs (Generative Adversarial Networks) by utilizing manifold regularization. This framework addresses the challenge of generalizing oneshot GAN adaptation where the target domain includes both styles and entities. To achieve this the researchers modify the generator architecture by incorporating an auxiliary network for generating entities. They employ the sliced Wasserstein distance to minimize the distribution mismatch between the source and target domains. Additionally they propose a regularization technique called Variational Laplacian Regularization (LVlapR) to preserve the structural integrity of the source data reducing content distortions. While the paper is wellwritten and the proposed method demonstrates strong visual results it is noted that the definition of \"style\" may be limited. Geometric features are also considered part of style and it is observed that the adaptation results for certain domains (Disney and Zelda) could be improved. Domain adaptation should account for geometric changes as well but the Zelda results lack the distinctive CG appearance and the Disney results deviate from the exaggerated features.", "Paraphrased Statement: Summary: This study presents a technique for adapting the StyleGAN model to specific domains in a single step allowing for the addition of new entities. This is achieved through several strategies including: Using \"sliced Wasserstein distance\" to measure the internal distribution difference between images. Employing \"variant Laplacian regularization\" to minimize distortions in crossdomain correspondences. Strengths: The method demonstrates promising results in experiments. The use of sliced Wasserstein distance and variant Laplacian regularization is theoretically sound. The code and supplemental materials are available indicating potential reproducibility. The learning time is faster than previous methods. Weaknesses: The papers writing could be improved particularly the clarity of the abstract and introduction regarding the contributions of the proposed approach. Figure 2 is challenging to interpret. The placement of Figure 1 at the bottom of the title is unconventional. Questions: Why is a mask generated using UNet The mask provided by users could potentially be used directly. The underlying rationale for this step should be clarified and its benefits should be supported by experiments. The paper lacks comparisons to other internal distribution distance metrics such as the mean square error between Gram matrices. The entity loss and style loss could also be implemented using MSE between Gram matrices. The authors should consider these alternative loss functions to demonstrate the advantages of sliced Wasserstein distance. The paper refers to the task as \"generalized oneshot GAN adaptation\" but this terminology seems imprecise. While the method allows for the addition of entities it is unclear why this qualifies it as \"generalized.\"", "Paraphrase: This study addresses the challenge of singleshot domain adaptation by separating it into style and entity transfer. Unlike other approaches that primarily focus on style transfer this method incorporates a binary entity mask and a special GAN design with regularized manifold constraints. The generator architecture is modified to isolate style and entity adaptation. The introduced method also includes a modified Laplacian regularization to enhance network smoothness. The methods performance is tested in depth using numerous datasets with and without entities. Strengths and Weaknesses: Strengths: The study investigates a fascinating problem of singleshot domain adaptation. The idea of separating style and entity transfer is wellfounded. Weaknesses: The study only examines domain adaptation with a specific binary entity mask on the target image which may not be applicable in all singleshot domain adaptation scenarios. The disentangled GAN structure is not a novel concept and the papers unique contributions are unclear. The experimental evaluation is inadequate with a restricted sample size and quantitative analysis based solely on images from Figure 3. Relevant Manifold GAN techniques are absent from the comparison since the proposed method emphasizes manifold regularization. Questions: Binary Entity Masks: How are binary entity masks obtained Sliced Wasserstein Distance: Why is sliced Wasserstein distance chosen Can you provide more theoretical background StyleFixed Code Transformation: The document mentions transforming w into a stylefixed code in Line 139. More details are requested on how this transformation is carried out."], "pHdiaqgh_nf": ["Paraphrased Statement: Summary: The authors have developed a method that uses fMRI data and CLIP embeddings to generate realistic images. The technique involves aligning neural representations with the CLIP embedding space and then using a generative model based on CLIP to create photorealistic images. Strengths: Innovative concept and promising approach for generating realistic images from limited fMRI data. Welldesigned experiments that provide valuable insights into the strengths and weaknesses of the method. Weaknesses: Concerns about Image Constraints: The method relies on pretrained CLIP and GAN models which constrain the generated images to the natural statistics learned by these models. This raises concerns about the accuracy of the reconstructed images especially for images that deviate from the pretrained models training sets. Specific Questions: How would the pipeline perform if presented with images different from the pretrained models training sets Would the pipeline still generate highly realistic images even if these images are misaligned with what the participants actually saw Are the authors aware of these potential limitations Concerns about Image Veracity: The reconstructed images often contain spurious details or missing information yet the method assigns high confidence to them. This raises concerns about the validity of the images and their proximity to the ground truth. Additional Considerations: Using human raters to evaluate image accuracy could provide valuable insights but this was outside the scope of the review.", "Paraphrased Statement: Summary: The proposed method employs the CLIP model to reconstruct visual stimuli from brain scans (fMRI). By mapping fMRI signals to the CLIP latent space a generative model is finetuned using these embeddings to create an image that resembles the viewed object. Experiments indicate successful reconstruction of highquality images with strong semantic similarity to the original. However the paper has some limitations and presentation issues but its unique approach may appeal to the scientific community. Strengths: Leveraging powerful pretrained models like CLIP for fMRI data is a valuable concept. The CLIP models high representational capabilities allow for efficient fMRI analysis. The generated images demonstrate excellent quality and semantic similarity to the original. The authors provide a thorough discussion of limitations including previous setbacks. Weaknesses: The papers presentation could be improved by avoiding assumptions about the readers knowledge of various models and techniques. The reliance on supplementary materials for quantitative results and direct references in the main manuscript makes it incomplete. A related work section is missing which hinders contextualization of the research. A significant limitation is the methods focus on semantic similarity rather than precise image reconstruction which may be problematic for specific image details. The approach is heavily engineered with numerous tunable parameters and losses and more evidence of their impact would be beneficial. The evaluation is primarily qualitative with limited examples and comparisons. The proposed evaluation metric has limitations due to the approachs focus on semantics and the conclusions made based on minimal examples may require further evidence. Questions: Was the DALLE 2 model considered which combines CLIP with diffusion models Was the incorporation of a pixellevel loss function explored to enhance image similarity Why were maxpool values utilized instead of concatenation for embedding representation", "Paraphrase: Summary: This study introduces a method to recreate realistic and accurate images from both fMRI scans and text. The researchers convert fMRI signals into a visual language latent space and employ a generative model to reconstruct images based on these embeddings. They also conduct a microstimulation study to understand how different brain regions contribute to this process. Strengths: The study thoroughly evaluates the proposed method and optimizes its components and parameters. It effectively reconstructs naturallooking and faithful images from brain activity captured by fMRI. Weaknesses: While the models and methods used are sound they are not novel in the field. Questions: For any further inquiries please refer to the \"Limitations\" section of the paper.", "Paraphrased Summary: Researchers present a novel approach to reconstruct images from fMRI signals. They utilize text descriptions of images as training data for their model overcoming the limitation of scarce fMRI data. The model leverages the pretrained latent space of a visionlanguage model to align fMRI signals resulting in photorealistic reconstructions that capture the semantic content of images. Paraphrased Strengths and Weaknesses: Strengths: Innovative use of a multimodal models latent space for fMRI reconstruction. Robust experimental evaluation with clear ablations and relevant metrics. Wellwritten and clearly explained methodology. Weaknesses: Unfair comparison to previous methods due to the use of different datasets and reconstruction quality. Misleading claim of photorealistic image generation as it relies on a pretrained generator. Lack of quantitative comparison with baselines to assess competitiveness. Inconclusive microstimulation results with no quantifiable interpretation. Paraphrased Questions: Main Concern: Insufficient comparison with previous work. Specific Concerns: Missing quantitative comparisons. Unfair qualitative comparisons due to different datasets and reconstruction quality."], "vkhYWVtfcSQ": ["Paraphrased Summary: This study suggests using surprise minimization in reinforcement learning among multiple agents (MARL). It introduces temporal energy models to capture uncertainty across agents. Specifically the authors propose an energy function that extends the energybased model theory and an operator to update this function over time. They also establish a connection to soft Qlearning and provide convergence analysis. The study demonstrates the superiority of the proposed EMIX algorithm with practical function approximation exhibiting a slight performance advantage over baseline methods. Strengths: Clear problem statement and comprehensive methodology Valid theoretical proofs Adequate discussion of related work Comparison to the maximum entropy framework Weaknesses: Assumption 2 is inconsistent with the discounted problem setting and requires more clarification Baseline results may underestimate performance compared to previous studies Questions: Provide further explanation for the inclusion of a termination state in Assumption 2 in the context of discounted problems. Explain the derivation of Equation (17) in the study.", "Paraphrased Statement: This Summary Paper presents a novel algorithm named EMIX which operates within the Centralised Training and Decentralised Execution model for multiplayer games. EMIX enhances QMIX by incorporating a global surprise minimization objective into the centralized value function training. Theoretical analyses establish that surprise converges to a stable point and is minimized as the policy approaches optimality. EMIX demonstrates superior performance compared to existing baselines in various Starcraft II games. The paper highlights the importance of both energybased model (EBM) approximations and surprise reward shaping. Strengths and Weaknesses: Strengths: Strong theoretical foundation Clear presentation Original contribution to the Centralised Training and Decentralised Deployment algorithm family Weaknesses: Limited description of the global surprise encoder Performance advantage over baselines is modest Questions: 1. Can the authors provide more details on how EMIX outperforms COMA and QMIX similar to the analysis in E1 2. Could the beta constant be tuned during training 3. Clarification on the setup and training of the global state encoder and standard deviation encoders 4. Explanation of how Vsurp is learned or obtained and how surprise is estimated during training", "Paraphrased Statement While surprise quantification (estimating environmental changes) has gained attention in singleagent reinforcement learning it remains unexplored in multiagent settings. This work introduces a surprise minimization approach in multiagent reinforcement learning based on free energy across agents. The energybased model formulated resembles the minimum conjugate entropy objective. Empirical evaluations on benchmark environments support the theoretical claims. Strengths Problem Significance: The issue addressed is crucial for both single and multiagent reinforcement learning. Novelty: The proposed methods theoretical perspective is innovative. Weaknesses Empirical Evaluation: The comparison algorithms (QMIX VDN COMA IQL) are outdated. Results are not presented for more challenging StarCraft II scenarios. Significance: EMIX achieves only marginal improvements over outdated baselines. Clarity: The writing requires improvement (e.g. Figure 1s purpose). Questions Equation (3): Why is agent \"a\" still present as a superscript after summing over \"a\" Vsurpa(su\\sigma): What is its definition and implementation What is the state deviation \u03c3 Figure 2: What do the black and white grids represent Surprise Minimization: Why is it necessary in StarCraft II and PredatorPrey environments Minimum Conjugate Entropy Objective: Can the authors provide further details"], "w0O3F4cTNfG": ["Paraphrased Statement: This article focuses on identifying causal relationships in linear systems with measurement errors. It introduces two new theorems: Theorem 1: A system with measurement errors can be represented as a latent variable model without parents. This allows identifiability results based on the mixing matrix to be applied to both types of models. Theorem 2: Under specific assumptions the system can be identified up to a group structure known as Ancestral Ordered Grouping (AOG). However within these groups the direction of edges cannot be determined. Additional Condition: A further condition ensures that the direction of edges between groups can also be identified leading to a graph with the Directed Ordered Grouping (DOG). Structure Recovery Algorithm: The article proposes an algorithm for structure recovery: 1. Determine the mixing matrix. 2. Identify the AOG of the true system. 3. Select the center for each group that minimizes the number of edges. Simulation results show that this algorithm outperforms existing methods in recovering the correct model structure. Strengths: Clear presentation of technical concepts. Accurate mathematical analysis with illustrative examples. Novel theoretical results that enhance identifiability. Weaknesses: The assumption of nonGaussian noise terms may limit the applicability in some realworld scenarios. More detailed annotation in Figure 4 would enhance clarity.", "Paraphrased Summary: The paper establishes a relationship between linear structural equation models (SEMs) with measurement error and linear SEMs with latent variables. It identifies the conditions under which these models are equivalent and provides an algorithm for recovering the model parameters. Simulations show that the algorithm outperforms existing methods. Strengths: Provides theoretical insights into the identifiability of linear nonGaussian models. Offers a detailed and clear exposition of the model and algorithm. Weaknesses: Lacks a thorough discussion of the motivation and implications of the work. Presents limited empirical results with minimal interpretation. Questions: How do these findings advance the field of causal discovery What potential applications could benefit from these results", "Paraphrase: Summary: The authors investigate the discovery of causal relationships in linear models when there are measurement errors and unobserved variables that do not cause any other variables (called \"parentless causes\"). They show that these two types of models are related such that every model with measurement errors corresponds to multiple models with parentless causes. They analyze the ability to identify both types of models and propose additional assumptions to make their structures and in some cases their parameters identifiable. They also define \"graphical partitions\" that remain the same for all models that can be identified. Strengths and Weaknesses: The comparison between linear causal models with measurement error and those with latent variables is novel and insightful. The paper is wellwritten and technically sound. The simulation results are comprehensive. Weaknesses: Confidence intervals are missing from the simulation results. Some minor writing errors make the text difficult to read. Minor Comments: \"application\" \u2192 \"applications\" \"casual\" \u2192 \"causal\" \"latter\" \u2192 \"later\" \"passes\" \u2192 \"pass\" \"exist\" \u2192 \"exists\" \"parent\" \u2192 \"parents\" \"child\" \u2192 \"children\" \"the edge in\" \u2192 \"edges in the\" \"are\" \u2192 \"is\" \"group\" \u2192 \"groups\" Add \"the\" before \"Reconstruction\" Questions: Why does the structure defined in Definition 4 lead to a star graph Please explain with a brief description of a star graph. How many simulations were performed in Figure 4 Why are there no confidence intervals reported given that the experiments were likely replicated multiple times"], "qSs7C7c4G8D": ["Summary Paraphrase: This study provides a comprehensive analysis of federated learning under arbitrary client participation presenting an amplification method to enhance gradient updates in such scenarios. Four primary client participation patterns are examined: regularized ergodic participation mixing and independent. The proposed amplification method is demonstrated to converge to zero under these patterns and under certain conditions it matches the linear speedup in convergence rate observed in previous works with an increasing number of participating clients. Empirical evaluations validate the proposed methods effectiveness in accelerating convergence on FMNIST and CIFAR10 datasets. Strengths Paraphrase: Provides a comprehensive analysis for federated learning with arbitrary client participation. Addresses a significant practical challenge in federated learning where clients may join or leave at will. Compares the proposed approach to existing work highlighting scenarios where linear speedup in convergence is achievable. Proposes an amplification method that assists in achieving zero convergence error under certain client participation patterns. Weaknesses Paraphrase: Limited intuitive explanation of the amplification method in Algorithm 1. Lack of detailed analysis on the impact of the hyperparameter \u03b7. The four client participation patterns considered are general but may not fully capture the complexity of realworld client participation behaviors in federated learning.", "Paraphrase: Summary: This paper explores federated learning where clients may participate irregularly and introduces a new analytical framework. The study examines various participation patterns and their effects. Strengths: 1. Clear and intuitive writing with adequate mathematical explanations. 2. Significant contribution to the issue of inconsistent client participation. Weaknesses: 1. Readers may need to refer back to previous sections to recall terminology particularly in Sections 4.2 and 5. Questions: 1. Whether the results can be applied to scenarios where clients participate only once due to a large client pool. 2. Why the bound in Corollary 4.7 does not reduce to O(1T) in the case where \u03c3 0 and if there is a solution to this apparent suboptimality. 3. Clarification on the reasoning behind the statement that P need not be small as mentioned in the remark following Corollary 4.8.", "Paraphrase: This research focuses on the partial participation feature in local stochastic gradient descent (SGD) algorithms. Previous studies often assumed uniform sampling of participants which differs from practical scenarios. The authors introduce a framework for examining various partial participation strategies and isolating their individual effects. They establish general principles and analyze specific cases. The paper concludes with extensive experimental findings. Strengths: The framework is versatile and applicable to many realworld situations. Clear explanations of assumptions and definitions. Precise theorem formulations. Seemingly accurate theoretical analysis. Interesting results that open new research avenues. Meaningful and insightful experimental comparisons. Weaknesses: The analysis assumes bounded gradient divergence which may not apply to experimental designs where data is partitioned by majority class label. Only nonconvex objective functions are considered. Does not include analysis for PL condition. Questions: Can this analysis be applied to local methods that address heterogeneity Can the analysis be extended to scenarios without bounded heterogeneity"], "p0LJa6_XHM_": ["Summary This study introduces a bilevel optimization technique for hidden trigger backdoor attacks (HTBAs) where the poisoned samples have a different trigger pattern than the actual trigger. To achieve effective HTBAs three key techniques are employed: 1. Bilevel optimization with gradient matching 2. Poison selection 3. Model retraining The proposed attack significantly improves performance over existing HTBAs based on finetuning. Ablation studies demonstrate the crucial role of each technique. Strengths Novel and effective HTBA technique in a realistic threat model Empirically validated effectiveness under varying poisoning rates and in different settings Identification of practical strategies for enhancing HTBA performance Weaknesses Insufficient explanation of HTBA Inconsistent notations and unclear equations Inadequate experiments against recent defense methods Limited attack success under true blackbox conditions Questions 1. Clarify the definition of HTBA and its distinction from \"invisible backdoor attacks.\" 2. Explain the concepts of p \u03b4 \\mathcalD and \\mathcalT in the equations. 3. Describe the differences between Equations (1) and (3) and their integration into the overall training loss. 4. Elaborate on the Algorithm 1 steps including data sources parameter updates and retraining details. 5. Explain the meaning of \"Poisoned model patched source val\" rows in Tables 1 and 2. 6. Address the low attack success rate under blackbox conditions. 7. Evaluate the attack against uptodate defense methods like ABL and ANP. 8. Correct the typo in line 127.", "Paraphrased Summary: This report introduces a new attack method called invisible poisoningbased cleanlabel backdoor attack which targets image classifiers that are trained from scratch. This attack leverages the Hidden Trigger Backdoor Attack which combines the advantages of poisonlabel and cleanlabel attacks but is limited to transfer learning scenarios. The authors formulate the attack as a bilevel optimization problem and employ gradient alignment techniques to solve it. To enhance the attacks effectiveness the authors introduce four techniques: ensemble training retraining random patch application and poison selection. They also propose a poison selection module to improve the attack success rate. The method is evaluated on the CIFAR10 and ImageNet datasets using various deep neural networks. Strengths and Weaknesses: Strengths: Relevant topic for NeurIPS audience Wellwritten and easy to follow Novel and effective attack method for backdoor learning Resistance to existing backdoor defenses Weaknesses: Limited details on results in Table 7 regarding detectionbased defenses Lack of reference to advanced backdoor attacks in Related Works Insufficient font size in Figure 1(a) Potential citation issues Questions: 1. Provide more information on the results in Table 7 specifically how ASRs and BAs are calculated under detectionbased defenses. 2. Review more recent backdoor attacks such as [13] in Related Works. 3. Cite relevant work on ensemble techniques in Line 210214. 4. Adjust the font size in Figure 1(a) to improve readability. 5. Verify and cite the official versions of all references including [48].", "Summary Paraphrase: The authors innovative backdoor attack Sleeper targets blackbox settings with unlabeled data. They create poisoned samples by matching gradients between the training loss function and modified samples. Sleeper was tested on CIFAR10 and ImageNet using various architectures and achieved high attack success rates (80) with poison rates greater than 1. It also showed robustness against standard defense mechanisms. Strengths and Weaknesses Strengths: Practical and relevant problem Comprehensive evaluation of efficacy robustness and practicality Clear writing Weaknesses: Threat model: The assumption of using a pretrained model for poisoning should be explicitly stated. Evaluation: Transferability among different surrogate models with varying accuracies should be examined. Robustness evaluation could be enhanced by considering more uptodate defense methods. Questions: How does Sleeper perform when surrogate models are less accurate than target models Why are STRIP and Neural Cleanse ineffective against Sleeper given their reported effectiveness against static backdoor triggers", "Summary Paraphrase: This research presents a novel poisoning attack on image classifiers that employs clean label data. The attack involves training substitute models to generate corrupted examples and using gradient matching to identify malicious alterations. Notably the \"Sleeper Agent\" attack assumes full access to the victims training data allowing the attacker to select which samples to contaminate. The method does not require knowledge of the target models architecture. The study demonstrates that this attack significantly outperforms previous clean label poisoning techniques in terms of success rate and its backdoor is resistant to existing mitigation methods. Strengths and Weaknesses Paraphrase: Strengths: Effectiveness: The attack is highly effective requiring minimal assumptions about the victim model. Transferability: The attack is transferable across various model architectures and datasets including ImageNet. Comprehensive Experiments: The results are supported by thorough experiments that show the positive impact of larger perturbations on attack success. Clear Organization: The paper is wellorganized and provides comprehensive insights into the attacks operation. Weaknesses: Limited Defense Evaluation: The study does not extensively evaluate the effectiveness of defenses against adversarial examples such as adversarial training. Missing Runtime on ImageNet: The runtime for the attack on the ImageNet dataset is not presented in the paper. Questions: Data Overlap Dependency: It is unclear how the attacks success depends on the attacker having access to the same training data as the victim. Finetuning Effectiveness: The effectiveness of the \"Sleeper Agent\" attack against models finetuned using pretrained models is not explored."], "xZmjH3Pm2BK": ["Summary Researchers aimed to reduce computation costs in Scorebased generative models (SGMs) by minimizing time steps. They discovered that time steps needed to achieve the desired error depend on the condition number of the Gaussian covariance. Their solution involves a novel SGM with wavelet whitening presenting significant theoretical and technical advancements. Strengths Theoretical Analysis: Determined how the regularity of the Stein score influences discretization during image generation highlighting the relationship between time steps and covariance condition number. Wavelet Decomposition: Proposed an innovative approach to decompose and subsample input images using wavelet orthogonal transforms reducing image size. Improved Efficiency: Demonstrated that the modified model allows for a fixed discretization schedule across image scales leading to faster generation in SGMs where time complexity is proportional to image size. Weaknesses Lack of Wavelet Explanations: Authors did not provide sufficient explanations of wavelet transforms making it challenging for readers without prior knowledge to grasp the main concepts. Comparative Performance: The proposed models performance in accelerating SGMs was not compared to other baselines mentioned in references \"[16 15 23 46 36 31 37 20 10 25 35 45].\" Question The competitiveness of the proposed model against other baselines (e.g. models mentioned in the references) in accelerating SGMs remains unclear.", "Paraphrased Summary: This study introduces the Wavelet scorebased generative model (WSGM) a multiscale model that utilizes wavelet coefficients to speed up sampling by reducing the required iterations for estimating the score. Notably WSGM provides a theoretical analysis that the number of sampling iterations is independent of the signals dimensions at each scale. Strengths: The paper offers a rigorous theoretical basis and mathematical analysis linking normalized wavelet coefficients and SGM sampling iterations. The renormalized wavelet decomposition method effectively reduces the Lipschitz irregularity of the score estimator. Experimental results on Gaussian processes physical processes and natural images validate the theoretical framework of WSGM demonstrating superior performance over baseline SGM methods with the same number of diffusion steps. Weaknesses: While the wavelet decomposition approach is theoretically sound it has been previously explored in multiscale generative models like Wavelet Flow and SWAGAN. A more comprehensive overview of related work using wavelets is recommended. The experiments are limited to lowresolution images. Extending WSGM to highresolution images could showcase its potential for realistic image generation. The authors acknowledge that highresolution image experiments may be challenging but they speculate on the expected positive results based on the theoretical framework. Questions: Can the authors elaborate on any practical barriers to evaluating WSGM on highresolution images How would WSGM perform in more realistic experimental setups that include popular acceleration methods for SGM inference", "Summary: The paper introduces a wavelet diffusion model that guarantees an error of \\epsilon regardless of the step size. Strengths: The model can synthesize images with a fixed number of evaluations regardless of resolution. The theoretical underpinnings are sound. Weaknesses: The experimental results are limited to the CelebAHQ dataset which may not be representative of other datasets. The proof of convergence is only provided for Gaussian processes not wavelet diffusion models. The practical implications of the models theoretical properties are not fully explored. Questions: Will the vanilla SGM outperform the WSGM with a larger discretization size (O(1000)) Does the models limited applicability to specific datasets pose a limitation"], "rBCvMG-JsPd": ["Paraphrased Statement: This paper presents a method for parameterefficient fewshot learning called TFew. TFew achieves stateoftheart performance on various tasks and outperforms full model finetuning and incontext learning with large language models. The method involves rescaling the inner activations of LMs using learned vectors allowing for efficient parameter updates. However there are some concerns with the evaluation. Strengths: Demonstrates strong fewshot performance on benchmarks and the challenging RAFT benchmark Introduces useful engineering practices for parameterefficient learning Weaknesses: Evaluation may not be fair as baselines do not use the same training techniques as TFew (e.g. new losses) Potential errors in the methodology description (e.g. maximizing instead of minimizing LLN)", "Paraphrase: This research introduces a new parameterefficient finetuning technique (PEFT) for NLP transformer models. By carefully selecting which parameters to finetune this method is significantly more computationally efficient than incontext learning (ICL) and outperforms ICL approaches. The paper proposes a novel method called IA3 which involves using vectors to scale layer activations. Experiments with the T0 model demonstrate that the proposed method outperforms both previous ICL and PEFT approaches. The paper also includes a detailed analysis of the computational requirements of the proposed and other methods. Strengths: Clearly detailed experiments facilitate replication. Comprehensive experiments and impressive performance. Practical guidance for fewshot learning. Wellpresented motivations. Computationally advantageous and superior to ICL and other parameterefficient methods for fewshot learning. Weaknesses: The presentation could be restructured to emphasize major contributions such as introducing IA3 earlier. The dense text could benefit from improved organization. While Figure 2 provides detailed comparisons it is unclear if the authors considered variations in model architectures and pretraining data. Questions: How were hyperparameters and other design choices determined Was the \"true fewshot learning setting\" strictly adhered to or were any validation sets used Since IA3 is applied to T0 it is challenging to gauge the independent performance contributions. An analysis of IA3 applied to plain language models would be helpful. Were baseline methods (e.g. adapters prefix tuning) tested with and without unlikelihood training and length normalization Specific Question on Figure 2: How do the authors account for differences in architecture and pretraining data in Figure 2 Ideally IA3 should be compared to other methods using the same base model.", "Paraphrase: This paper focuses on fewshot learning in natural language processing. It highlights the limitations of fewshot incontext learning methods such as high computational costs. To address these limitations the paper proposes an adaptation strategy that involves adding small trainable parameters (rescaling vectors) to a frozen pretrained model. This strategy only requires finetuning the trainable parameters on a few labeled samples for fewshot learning. The experiments show that the proposed strategy outperforms existing fewshot incontext learning and parameterefficient finetuning methods while maintaining a lower computational cost for inference. However it requires higher computational costs disk space and memory during training. Strengths: Clear writing and a wellstructured paper Novel and promising idea of adapting pretrained models with lightweight parameters for fewshot learning Use of lightweight rescaling vectors Weaknesses: High computational cost of training the rescaling vectors especially for large models or numerous tasks Potential challenges in applying the method to larger language models Related Literature: The paper omits references to related literature including: [ref1] Fast and Flexible Multitask Classification Using Conditional Neural Adaptive Processes [ref2] Learning a Universal Template for FewShot Dataset Generalization [ref3] Universal Representation Learning from Multiple Domains for FewShot Classification", "Paraphrase: Summary: This paper presents an efficient method for finetuning language models with limited parameters making it suitable for fewshot learning. Key contributions include: Extending T0 to fewshot learning. A comprehensive recipe for finetuning fewshot models. A new parameterefficient approach that improves over existing methods like prompt tuning and adapters. Strengths and Weaknesses: Strengths: Clearly written and enjoyable to read. The parameterefficient approach (IA3) performs well compared to other methods. The proposed method achieves strong fewshot results with moderately sized language models outperforming larger models like GPT3. A single configuration for finetuning across all tasks. Weaknesses: Some ablations in the appendix show inconsistent improvements and high standard deviations. Figure 2 demonstrates the benefits of IA3 compared to prior work only for T03B. Its unclear if IA3 is crucial within the proposed framework or if alternative parameterefficient models could be used. Only GPT3 is used as an ICL baseline. More recent ICL advancements like Chinchilla could have been included. Questions: Is IA3 essential within the proposed framework or can alternative parameterefficient models be substituted Can the ablations with inconsistent results and high standard deviations be further analyzed Would the benefits of IA3 be retained if a different ICL baseline was used"], "kI_kL5vq6Oa": ["Paraphrased Statement: The authors propose a technique to assess the control risk caused by perception errors in systems with incomplete observability such as those where observations are images. They assume the existence of a dynamic model and controller and focus on enhancing risk awareness within the system. Using a prespecified probabilistic model of perception errors risk is defined as the Conditional Value at Risk (CVaR) of the cumulative cost distribution under the possible dynamics control policy and perception error model. The distribution is estimated using a discrete categorical distribution. The CVaR risk metric is then employed as an additional objective for refining the perception model and for guiding active data acquisition during training. The method is tested on a simulated image pendulum and an aircraft collision avoidance dataset demonstrating substantial enhancements in riskaverse performance. Strengths: Considers perception errors which are critical for system performance. Potentially applicable to various settings. Provides separate evaluations of active data acquisition and risksensitive loss. Uses an intuitive running pendulum example. Quantifies relative perception risk of states by comparing it to errorfree estimates. Weaknesses: Assumes partially observable systems with observation spaces that differ from state spaces potentially deviating from the Partially Observable Markov Decision Process (POMDP) paradigm. State estimators may need to consider observation history but the proposed method focuses on current observations only. The objective for refined state estimators mixes state estimation and control leading to potential conceptual issues. Data availability and query freedom may not be realistic in online reinforcement learning scenarios. Evaluation could benefit from testing on additional systems.", "Paraphrase: Summary This paper presents a technique called \"riskdriven loss and augmentation.\" It aims to enhance the safety of imagebased control systems. The technique involves creating a Markov Decision Process (MDP) that connects the perception model to the controller. This MDP measures the risks linked to actions taken at specific states as well as risky states. The MDP quantifies risk and is trained using distributional reinforcement learning. Experiments with a pendulum and a basic aircraft avoidance system demonstrate better outcomes than traditional perception models. Strengths Innovative use of a Qfunction to assess the risks of a perception module. Clear problem statement sufficient background information and sensible implementation details. Use of an inverted pendulum example provides clarity. Evaluation using an airplane avoidance system adequately tests the papers hypotheses. Weaknesses Lack of information on Qlearning implementation details such as whether a neural network is used and the hyperparameters involved. Reliance on discrete systems throughout the paper which may be a limitation due to the use of distributional Qlearning. Simplistic evaluation examples. Evaluation in renowned benchmarks or simulators would enhance credibility. Figure 2s line plots are challenging to interpret. Questions None.", "Paraphrased Statement: Summary: The paper presents a method for designing perception systems that accounts for the impact of perceptual errors on the systems overall performance. It uses conditional value at risk (CVaR) and a Markov decision process (MDP) to estimate the potential safety risks associated with perception errors. The method then uses this risk function to identify highrisk errors and collect additional training data to improve the models performance in these areas. Strengths and Weaknesses: Originality and Novelty: The concept of linking perception errors to control for specific performance goals has been explored before. However the papers contributions include a novel risksensitive loss function data generation and application illustrations. Reference to Past Work: The paper does not cite relevant past literature on uncertainty and distribution propagation in computer vision for system identification. Significance of Work: The paper integrates risksensitive loss functions into perception module design demonstrating the importance of linking perception error models to overall risk in control scenarios. The case studies provide a practical demonstration of the concept although they are limited in scope. Questions: The paper does not clarify whether the ACAS example datasets used are simulated or realworld. It also remains unclear how the model assumptions can be validated considering the complexity of probability of misdetection and probability of false alarm estimates in realworld sensing conditions."], "vRwCvlvd8eA": ["Paraphrased Statement: Summary This paper introduces two approaches for generating positive random features for approximating Gaussian (Softmax) kernels: Generalized Exponential Random Features (GERF) and DiscretelyInduced Random Features (DIRF). The authors provide formulas for the variances of both approximations. They also demonstrate that the variance reduction in OPRF (a type of GERF) with orthogonal samples applies for any d0 not just asymptotically for large d as previously believed. The paper proves uniform convergence for approximating transformer attention with lowrank transformations. Strengths and Weaknesses Pros OPRF shows potential for creating positive and bounded random features that could improve Transformer performance while maintaining stability. Experimental results show significant variance reduction (up to e60 times). The paper is wellwritten and structured. Cons DIRF may be sensitive to outliers due to the condition (Line 181). It is also not rotation invariant unlike Gaussian and Softmax kernels. Theorem 4.1 introduces undefined symbols (e.g. C(x y)). Questions 1. Do OPRF and DIRF still generate positive random features for new data despite being computed from a given dataset X 2. The variance reduction performance of GERF in Figure 2 is remarkable. The authors should provide more analysis and explain why it outperforms the baselines. They should also plot the baselines in the top and bottom rows together for better comparison. It would be helpful to see how the variance changes with the number of features (M).", "Summary To overcome the high computational cost of fullrank attention matrices in Transformer networks this paper introduces \"Chefs Random Tables\" (CRTs). These are novel random features that aim to provide unbiased lowrank approximations while addressing limitations of previous methods including lack of positivity boundedness and limited variance reduction. The CRT family includes two classes of random features: Generalized Exponential Random Features (GERFs) and Discretely Induced Random Features (DIRFs). GERFs generalize existing random features based on exponentials and Gaussian projections also encompassing positive random features for attention matrices. DIRFs lift a discrete probability distribution to a random feature map using the Taylor expansion of the exponential function. Specific instances of GERFs and DIRFs such as Optimal Positive Random Features (OPRFs) Poisson Random Features (PoisRFs) and Geometric Random Features (GeomRFs) are proposed with explicit formulas for their variances. Additionally the use of blockorthogonal ensembles of random projections as employed in previous works is incorporated for further variance reduction. The paper establishes theoretical guarantees including variance bounds and exponential tail bounds for OPRFs as well as a uniform convergence result for approximating attention matrices using positive random features. Experiments Experiments on synthetic data UCI classification tasks NLP and speech modeling benchmarks and Vision Transformers demonstrate the reduced variance and compatibility with Transformer finetuning using the proposed random features. Strengths and Weaknesses Strengths: Clear and accessible writing Original and interesting ideas Strong theoretical analysis with novel guarantees for attention matrix approximation Experimental support for the proposed methods Weaknesses: Limited comparisons to alternative methods in the experiments Lack of experiments on longrange datasets The domain constraint on A in Theorem 3.3 may limit the potential for lower variance estimators Questions Is it possible to obtain positive RFs outside the specified domain and would this yield improved variance estimators Do theoretical results for OPRFs hold when the optimal \\rho computed in eq.(7) is not used for matrix computations How were the degrees of freedom A and s chosen for GERFs in the experiments Could the model be adapted for kernelbased nonparametric learning tasks to demonstrate its usefulness in approximating the Gaussian kernel Are there any practical drawbacks for the proposed methods that could be identified through experiments on longrange datasets", "Paraphrased Statement: Summary: This paper proposes an efficient and unbiased estimator for the softmaxGaussian kernel using positive random features. Key contributions include: Introduction of Optimal Positive Random Features (OPRFs) Development of new random feature mechanisms called Chefs Random Tables that avoid trigonometric functions Theoretical results demonstrating significant variance reduction and exponentially small tail bounds for kernel estimation First uniform convergence results for softmax attention approximation using OPRFs Comparison to Random Kitchen Sink (RKS) techniques showing that orthogonal random projections and OPRFs reduce variance for any dimensionality Strengths: Novel and theoretically sound approach Adequate numerical evaluation Weaknesses: Writing is geared towards experts Lack of detailed explanations in proofs making comprehension challenging for nonexperts Writing assumes familiarity with the field which may hinder accessibility for less knowledgeable readers", "Paraphrased Summary This paper introduces a new type of random features that address the limitations of negative Fourier features in approximating Transformers selfattention mechanisms. By ensuring positivity in these new features the authors aim to improve the stability and approximation quality of the resulting kernel matrices. Strengths and Weaknesses Originality: The theoretical results presented for the novel random features appear to be original and innovative. Quality: The paper provides sound theoretical foundations with concentration bounds on approximation properties. Empirical results also demonstrate significant improvements over existing methods. However the sharpness of these bounds is unclear and a detailed ablation study comparing proposed and classical random features would be valuable. Clarity: While the paper is generally clear it relies heavily on prior work especially the FAVOR algorithm without providing sufficient context. A more thorough discussion would enhance comprehension. Significance: The proposed approach offers potential speed enhancements and improved theoretical properties for selfattention computations. However the impact on practical model performance may be limited. Questions and Comments: The relationship between positive random features and the Gaussian kernel initially used in the motivation should be clarified. The significance of Theorem 3.1 in terms of approximation properties and numerical stability should be discussed. Details on variance computation in Figure 2 (Gaussian kernel vs. softmax kernel in Transformers) are needed. The numerical stability of features in Section 3.2.2 (geometric) with respect to the factorial denominator should be addressed. Arguments from prior work motivating the use of positive random features would be insightful. The reasons for FAVORs failure in \"Uptraining\" (Figure 4) warrant further explanation."], "xL8sFkkAkw": ["Paraphrased Statement: Summary: This paper introduces a new method for initializing neural network parameters using a theoretically inspired optimization algorithm. It aims to replace existing initialization methods like Kaimings. The proposed approach is related to Kaimings method but uses the cosine similarity of samplewise gradients to optimize parameters. It has been tested on various network architectures (ResNet DenseNet WideResNet Transformer) and has shown improved training results compared to Kaimings and other learningbased methods. Strengths: 1. The approach is based on a theoretical analysis of network parameter initialization and samplewise local minima. It derives a quantity that approximates the upper bound of training and test error. By minimizing this bound with constraints on gradient magnitude the method achieves initialization parameters that aid in better training outcomes. 2. Experimental results demonstrate that the proposed initialization method enhances network training performance. Weaknesses: 1. The theoretical analysis is similar to that of Kaimings method although the proposed approach measures the difference using cosine similarity of samplewise gradients. 2. The approach involves an optimization problem that requires finetuning of hyperparameters (e.g. \u03bb). The impact of these hyperparameters on performance including the number of optimization iterations needs further exploration. 3. Since the optimization process introduces randomness (e.g. minibatch sampling) the performance results should ideally be reported over multiple runs of the same task to provide a more accurate comparison between methods. 4. The paper mentions that the proposed approach introduces additional computational cost during optimization. To make a fair comparison it would be beneficial to align the number of initialization iterations for all methods and evaluate the speed and performance tradeoffs.", "Paraphrased Statement: This research proposes a new initialization technique for neural networks. The researchers demonstrate the limitations of using the Manhattan distance to measure local optima proximity which results in a cosine similarity metric. They propose an approximation method that employs onestep gradient descent and introduce the GradCosine metric (Equation 4) to approximate the cosine similarity of local optima. Weights are initialized by maximizing GradCosine and its norm. Experiments validate the effectiveness of the approach showing superior accuracy and speed compared to existing methods. Strengths: 1. Clear and coherent writing 2. Strong experimental results 3. Intriguing motivation Weakness (Questions): 1. The authors state that existing methods use firstorder training dynamics as their optimization objective but MetaInit and GradInit also utilize gradient descent. 2. The theoretical analysis in Section 3.2 does not demonstrate a tighter bound for \\ThetaSl than \\PhiSl suggesting that it could be moved to a supplementary section. 3. A typo exists in Line 6: GradCoisne should be GradCosine. Questions for the Authors: 1. The claim that the optimization path from initialization to local optima is more consistent in Figure 1(d) than 1(c) is questionable as \\theta0 in Figure 1(c) is closer to \\theta1 and \\theta2 resulting in a larger angle \\angle(\\theta1\\theta0 \\theta2\\theta0). The authors should clarify that the angle may not accurately reflect the optimization path quality. 2. The approximation in Equation 3 may not be reasonable for initialized weights \\theta0. The authors should provide an explanation for its rationale.", "Paraphrase Summary The article presents a new metric called cosine similarity of samplewise local optima to assess the performance of initialization models. They prove that this metric serves as an upper bound for both training and generalization errors under certain conditions. They approximate the samplewise optimum using a firstorder approach to make the metric differentiable and manageable. This simplification results in an upper bound metric that can be maximized through a gradientbased method for initialization. Empirical results indicate improved performance on various datasets and network structures compared to existing initialization methods. Strengths and Weaknesses Strengths Wellorganized and accessible content Sound theoretical analysis with clear motivation Strong empirical results particularly on CIFAR datasets Weaknesses Lack of experiments demonstrating the utility or superiority of the proposed cosine similarity metric over previous methods given its role in motivating the initialization method Incorrect citation in Appendix A for Lemma 1s proof Questions 1. Experiments to demonstrate the suitability of the cosine similarity metric for evaluating initialization quality 2. Applicability of the initialization framework to other optimization methods such as sharpnessaware minimization 3. Exploration of the dataset dependence of the method and the benefits of initializing network parameters using a different dataset 4. Potential for repeated use of the method during training and its effectiveness", "Paraphrased Statement: Researchers have developed GradCosine a metric that gauges the effectiveness of neural network initialization. This metric measures the similarity between the gradients of individual samples and can be optimized using an iterative process called NIO (neural initialization optimization). The paper explores how GradCosine relates to model training and demonstrates its correlation with the density of local optima per sample. Theoretical analysis suggests that minimizing GradCosine leads to desirable bounds on the training loss. For practical applications a subbatch version of GradCosine is presented which can be computed more efficiently. Strengths and Weaknesses: The theoretical analysis is wellsupported by reasonable assumptions. The experiments provide a convincing demonstration of NIOs effectiveness. The inclusion of a Dockerfile in the code repository facilitates reproducibility. Suggestions: Comparing NIO with other initialization methods that claim to work without warmup such as GradInit. Reporting standard deviations in Table 4 to provide a more comprehensive view of the results. Exploring algorithmic methods to reduce the memory usage of batched GradCosine especially for large models like transformer language models."], "p-56bnzZhQ7": ["Paraphrase: Main Idea: The paper proves that it is computationally hard to learn halfspaces (linear classifiers) that are corrupted by specific noise known as Massart noise. Background and Significance: Previously only weaker \"statistical query\" lower bounds for learning Massart halfspaces existed. Cryptographic hardness results like this have recently been established for other learning tasks. Main Contribution: The authors show that if the LWE problem is hard to solve then no efficient algorithm can learn Massart halfspaces with error better than a certain threshold even if the optimal classifier has a lower error. Strengths: The result is clear and the paper is wellwritten. The technical contribution is a reduction from continuous LWE to Massart halfspaces learning. Questions: Is this cryptographic hardness result important given the existing statistical query lower bounds What algorithmic approaches could potentially bypass these lower bounds and efficiently learn Massart halfspaces (Gaussian elimination and LLLbased algorithms were mentioned in the original statement.)", "Paraphrase: Learning to differentiate halfspaces (regions divided by a straight line) from noisy data is a classic problem in machine learning. In the Massart noise model data points are drawn from the same underlying distribution but with a fraction eta of labels being flipped. Recent advances have shown that its possible to learn halfspaces with low error rate (eta eps) using polynomial time and samples. However its unknown whether its possible to learn halfspaces with even lower error rates (OPT eps) where OPT is the error rate of the optimal halfspace. Statistical evidence suggests that this might not be feasible. This paper provides further evidence by constructing a conditional lower bound under the Learning With Errors assumption. It states that if solving the LWE problem requires strongly subexponential time then no polynomialtime algorithm can learn halfspaces with Massart noise with error rate better than Omega(eta) when OPT is less than eta. Strengths: Original and important contribution to learning theory. Clear writing despite the complexity of the subject matter. Weaknesses: A large number of parameters in the construction could use more explanation. Some parts of the proofs were difficult to follow particularly the statistical query lower bound construction. Lemma 3.3 could benefit from more detail. The results are of interest primarily to the learning theory community.", "Paraphrase: This study provides a novel cryptographic hardness result for the fundamental problem of identifying halfspaces with Massart noise in a distributionfree setting. Specifically they assume subexponential hardness of the Learning with Errors (LWE) problem and demonstrate that for a distribution consistent with a halfspace except for a bounded Massart noise (e.g. \\eta \u2264 13) and where the true OPT is almost polynomially small no polynomialtime algorithm can achieve a 01 loss better than \\Omega(\\eta). They also show the hardness of a related decision version of this learning problem. The approach involves a carefully constructed reduction from continuous LWE (as hard as LWE) to the problem of learning Polynomial Threshold Functions (PTFs) with Massart noise using a natural monomial feature map. PTFs are essentially LTFs over a higherdimensional space. The reduction relies heavily on previous work showing a superpolynomial SQ lower bound for the same problem. At a high level the final hard Massart distribution is designed to be consistent with a Massart PTF where the conditional distributions x y 1 and x y 1 resemble \"pancake distributions.\" These distributions are Gaussian in all but one dimension where they resemble a discrete Gaussian mixture that matches several moments with a standard Gaussian. The generation of this distribution requires intricate rejection sampling techniques. This approach aligns with a major strand of work on lower bounds for nonGaussian component analysis and related problems using reductions from continuous LWE. Strengths: Provides an important hardness result for a fundamental problem in learning theory. The SQ lower bound is compelling and the cryptographic lower bound has no model limitations. The approach builds on prior work but requires substantial additional technical effort. The paper is wellwritten and clear. Weaknesses: Assumes subexponential hardness of LWE. Lacks novelty compared to the SQ lower bound of DK21. Questions: Why is subexponential hardness of LWE necessary in the overall proof Would quasipolynomial hardness suffice Typo in the statement of Theorem 1.2 (should be OPT \u2264 2\\log\\zeta N)."], "n7XbkHOwKn6": ["Paraphrase: Summary: This research introduces a texttovideo model that has been pretrained on a large scale. It also presents a multiframe rate training approach that helps improve the alignment between text and video segments. Strengths: The paper is wellstructured and comprehensible. The proposed model is the largest pretrained texttovideo transformer model for generalpurpose use and it is opensource. Experiments demonstrate that the suggested techniques outperform existing benchmarks. Weaknesses: The model requires a large amount of GPU memory but the paper does not specify the specific GPU type or number used. This information should be included. The author lacks expertise in the fields of DALLE and CogView. The final evaluation score will depend on the feedback from other reviewers and the authors response. Questions: 1. Is there a discrepancy between the created movie and the actual video such that the actual video is not included in the human assessment for comparison 2. Im curious if the alpha blending factor will eventually converge to 0 or 1 making the dualchannel attention block redundant.", "Paraphrased Statement: Summary: This paper introduces CogVideo a novel opensource pretrained transformer framework for video generation from text. It employs a twostage approach: finetuning a texttoimage model and utilizing multiframerate interpolation to generate videos. Strengths: CogVideos unique twostage pipeline effectively captures temporal relationships in videos. The finetuning approach reduces training costs compared to training from scratch. Visual samples demonstrate the models ability to generate videos. Weaknesses: The paper is not the first to investigate hierarchical transformer structures for texttovideo generation and it does not provide a clear comparison with existing models. The experimental results are limited to human action video datasets raising questions about the models performance on more general domains. Questions: 1. Is the framerate (fps) a learnable parameter or a fixed hyperparameter in the transformer model and how is the fps token incorporated into the transformer architecture 2. How does CogVideo compare to other training methods in terms of training speed What hardware and training time were used for the model 3. How does the full CogVideo model address the issue of frame collapse in the sequential generation stage where a single corrupted frame can impact subsequent frames"], "u6GIDyHitzF": ["Paraphrase: This paper investigates a simplified version of the dueling bandit problem where its assumed that there exists a clear favorite among the available options. Unlike most existing work that focuses on making individual decisions for each round this work considers a scenario where decisions are made in batches. A new algorithm is proposed for this scenario which reduces regret compared to previous methods. The paper includes a comparison of the algorithm with existing approaches on realworld datasets. Strengths: The batched learning approach is relevant to practical applications. The paper is wellwritten and the algorithm is clearly explained. The algorithm provides stronger theoretical guarantees than existing methods. The proofs appear sound. Weaknesses: Some relevant literature is missing including work on roundbatch complexity for topk arm selection and finding the best arm in the multidueling bandit setting. The experimental study could be improved by using more uptodate competing algorithms and clarifying the elimination strategy used. It would be helpful to discuss whether the assumptions required by some methods are satisfied by the datasets used. Questions: Why are two different confidence intervals used in the algorithm Why isnt the theoretical analysis carried out for the KLdivergencebased confidence intervals if it leads to similar results Minor Points: There are a few typos in the text. The proof of Theorem 1.2 is inspired by the RUCB algorithms expected regret bound but the RUCB authors should be given appropriate credit.", "Paraphrased Summary: The researchers examine the batched dueling bandit problem where Condorcets condition applies. They develop the C2B algorithm with a limited number of adaptive rounds and determine its asymptotic regret upper bound. Tests with realworld data validate the algorithms efficacy. Strengths: 1. The algorithm can perform effectively with a small number of batches (O(\\log t)). 2. Its limited adaptive rounds suit contemporary largescale applications. 3. The paper is wellorganized and thorough including detailed background information. Weaknesses: 1. The regret bound derived (O(\\tildeO(\\fracK2\\Delta\\min2))) is not as optimal as the referenced lower bound (O(\\fracK\\Delta\\min\\cdot poly(\\log T))) especially when K is large or \u0394\\min is small. 2. The algorithm compares pairs a fixed number of times each round regardless of previous results. It may be beneficial to allocate comparison times dynamically based on past performance to enhance accuracy."], "wgRQ1IM4g_w": ["Summary Paraphrase: The study investigates black box optimization problems with soft constraints where the goal is to optimize a function f while keeping another function g below zero. Both f and g belong to potential infinitedimensional spaces. The aim is to minimize the regret (difference between the optimal solution and the obtained solution) and constraint violation in a sublinear manner. A general algorithm framework is proposed that works for a class of exploration strategies meeting specific criteria. Two algorithms GPUCB and GPTS are shown to satisfy these criteria and regret and violation bounds are derived for them. Numerical evidence supports the effectiveness of the algorithms. Strengths:  Interesting analysis techniques that extend the analysis of GPUCB to GPTS.  Insightful discussion on convexopt and Lyapunovopt techniques in the Appendix. Weaknesses and Questions: 1. Better Bounds on \u03b3t: Update citations to incorporate known improved bounds on \u03b3t. 2. Definition of mathcalV(T): Explain why the max operation is applied to the entire sum in the definition of mathcalV(T) rather than to each summand. It raises concerns whether the algorithm solves the \"soft\" constraint problem where violations are allowed occasionally but not excessively. 3. Sublinear Guarantees: The regret and constraint violation are only sublinear for certain values of \u03b3T. It would be beneficial to incorporate nonadaptive approaches that achieve optimal regret guarantees for all kernels. 4. Constraint Violation in Theorem 1: Explain why the constraint violation is always governed by the nonsmooth kernel even if the kernel corresponding to g is smoother."], "pn5trhFskOt": ["Paraphrased Summary: The proposed method Momentumbased Visual Sound Localization (MoSVL) addresses two challenges in visual sound localization: 1. Previous approaches rely on labeled data and suffer from overfitting in large datasets. 2. Existing methods neglect the influence of silent objects on sound localization. Strengths:  Focus on a relevant problem with potential applications.  Clear and straightforward writing. Weaknesses:  Major:  Omission of significant prior works such as DSOL and IEr which address the silent object problem and provide evaluation metrics.  Lack of novelty in addressing overfitting relying on basic techniques like dropout and exponential moving average.  Absence of a demo video to showcase performance in the presence of silent objects.  Minor:  Lack of code or data for reproducibility."], "pHd0v8W30O": ["Summary: In offline reinforcement learning heterogeneous datasets pose challenges for standard policies as they may assign high probabilities to outofdistribution actions leading to suboptimal results. To overcome this this paper proposes a method (LAPO) that learns a latent variable model to generate highadvantage actions. Additionally LAPO trains a latent policy to obtain statedependent latent actions resulting in improved performance. Strengths and Weaknesses: Strengths: 1. Innovation in using a more flexible policy than standard deterministicGaussian policies. 2. Promising empirical results on multiple datasets. Weaknesses: 1. Incremental novelty compared to prior work combining ideas from PLAS and AWAC. 2. Insufficient explanation of the proposed method particularly the motivation behind each components design. Questions: 1. Why does learning from heterogeneous settings lead to a lack of sufficient highreturn trajectories Why is the dataset constructed by relabeling stateaction pairs from different tasks 2. Why does AWACGMM perform competitively with LAPOVAE in Figure 2 3. How can a stateconditioned latent space represent highreturn actions if the dataset only contains lowreturn actions at a specific state 4. Why aim for both representing multimodal action distributions and selecting which actions to use Why not simply use a deterministic policy and advantageweighted learning 5. Why are the latent values decoded into indistribution actions Why not outofdistribution actions 6. How can the latent policy be trained if the decoder does not provide gradient information Can the Q function be defined as Q(sz) for direct gradient backpropagation to the latent policy 7. How does the latent policy avoid sampling OOD actions without the involvement of importance weights 8. Equation (4) appears to be incorrect. Could you verify it against the original PLAS paper 9. How is the estimation of the advantage function exempted from the influence of OOD actions"], "rcMG-hzYtR": ["Summary Paraphrase: This work focuses on addressing the challenges of transferring policies from simulation to realworld environments. The authors propose a robust policy improvement method M2TD3 which aims to minimize the worstcase performance over a set of uncertain parameters. The method solves a maxmin optimization problem leveraging the uncertainty parameter to enhance policy robustness. Critique Paraphrase: Strengths:  Clear problem formulation and comprehensive literature review  Extensive experiments demonstrating the methods effectiveness  Wellstructured organization and clear writing Weaknesses:  While the authors claim to develop a robust policy for sim2real transfer they assume the uncertainty parameter remains fixed in each episode. This setting may be closer to transfer learning rather than robust RL. The authors should consider continuously changing uncertainty within episodes and adversarial uncertainties introduced by competing policies.  The authors solution to the maxmin optimization problem involves observable uncertainty in the Q value function. This approach resembles domain randomization methods. The authors should provide a more detailed comparison with DR(TD3) which exhibits competitive performance.  The proposed refresh strategy and uncertainty parameter sampler are common techniques for policy stability and environmental exploration.  The authors could consider incorporating recent works in the field such as references [15] described in the critique."], "tzNWhvOomsK": ["Paraphrased Statement: Summary: The paper presents a new theoretical bound for zeroshot learning (ZSL) representing the worstcase performance that any classifier can achieve given a specific attributeclass matrix. The bound assumes perfect knowledge of itemtoattribute mapping. The authors demonstrate that calculating this bound can be formulated as a linear programming (LP) problem. They provide a closedform solution for the binary case and an approximation method for the multiclass case. The bound has been shown to be tight in general. Strengths:  Clear and accessible writing style.  Introduction of a novel problem of characterizing ZSL difficulty.  Substantial technical contributions including the bound formulation computation methods and tightness proofs.  Potential practical relevance of the binary case classifier which performs optimally under all distributions. Weaknesses: Interpreting Experimental Results:  The gap between the theoretical lower bound and practical methods in binary experiments seems to be largely attributed to generalization issues with itemtoattribute mapping.  Some methods surpass the lower bound in adversarial experiments suggesting that the bound may be overestimating the \"true\" optimal performance. Motivation for Using WorstCase Bounds:  The paper does not provide a clear explanation of why worstcase bounds are useful in ZSL.  The lower bound is often much higher than practical performance on real datasets making its relevance questionable.  Bestcase lower bounds could be more informative by indicating the best possible performance under favorable conditions. Sanity Checking:  Binary experiments would have been simpler to interpret and verify as they eliminate confounding factors such as distribution choice and bound approximation.  The paper does not experiment with binary situations nor does it provide reasons for this omission."], "y5ziOXtKybL": ["Summary: The paper proposes theoretical results for Bayesian neural networks in the Besov space. These results extend the prior work from Holder space to Besov space. The paper demonstrates that Bayesian neural networks with spikeandslab priors or even relaxed shrinkage priors converge to the true regression function in the Besov space. Strengths and Weaknesses: Originality:  Strengths: The main results appear to be novel and extend previous results to a broader setting. Quality:  Strengths: The theoretical contribution is substantial.  Weaknesses: The numerical example is limited and does not fully align with the theoretical results. Clarity:  Strengths: The text is generally wellwritten and easy to follow.  Weaknesses: The heavy use of mathematical notation may hinder readability for some readers. Significance:  Strengths: The paper contributes to the important area of theoretical guarantees for deep learning. It provides an important step in extending these guarantees to a larger space. Additional Comments:  The use of variational inference (Bayes by Backdrop) instead of MCMC methods raises questions about the accuracy of the numerical example.  The authors should expand on the claims about the practicality of the method and provide empirical evidence.  Clarification of certain terms and notations would improve readability."], "mE1QoOe5juz": ["Paraphrase: Researchers examined a twolayer structure in stochastic bandits and Markov decision processes (MDPs) where one layer consists of \"exploiters.\" Both layers played the same game simultaneously with the exploiter group using a conservative strategy based on upper confidence bounds (LCB) and exploiting information from the other group (explorers). The exploiter group achieved a constant regret that depended on the problem while the regret of the explorer group remained almost optimal. While research on explorationexploitation in bandits existed this work extended it to MDPs using UCBLCB algorithms because these algorithms do not require sample weights making them compatible with MDPs. Strengths and Weaknesses:  Originality: The decoupled explorationexploitation idea was borrowed from previous work which made it less original.  Quality: The analysis was convincing but the researchers proved that constant regret could only be achieved if it depended on the problem.  Clarity: The paper was wellorganized and easy to follow.  Significance: It was a significant contribution that constant regret could be achieved in reinforcement learning by separating explorers and exploiters. Questions:  Are there realworld examples of exploiter groups sharing a common model with explorers"], "mvbr8A_eY2n": ["Paraphrase: The authors propose a method for distributing items among individuals while ensuring fairness and maximizing overall benefit. They formulate the problem as an optimization task aiming to allocate items based on their value to different individuals. The authors utilize optimal transport theory to show a connection between allocation policies and optimal transport. They also develop a practical algorithm that leverages stochastic gradient descent with sample data to find solutions. Their experiments demonstrate the effectiveness of their approach showing the tradeoff between fairness and efficiency. Summary:  The authors address the problem of allocating items fairly among individuals considering both overall benefit and individual envy.  They establish a connection between optimal transport and allocation policies.  They provide an algorithm for practical optimization using stochastic gradient descent.  They conduct experiments to demonstrate the effectiveness of their approach. Strengths:  Novel theoretical results linking optimal transport and fair resource allocation.  Practical algorithm with convergence guarantees.  Relation to existing optimal transport literature. Weaknesses:  Lack of theoretical results on the efficiency loss due to fairness constraints.  Incomplete comparison of their model to existing fair resource allocation literature.  Unclear practical motivation and model description. Questions:  What is the difference between the convergence guarantees in sections 5 and 6  Are there theoretical results to support the claim of addressing the welfare cost of envyfree solutions"], "nLKkHwYP4Au": ["Paraphrased Statement Summary: This paper introduces a twostage approach for 3D object detection using voxels. It generates highquality box proposals by leveraging scenes spatial and semantic features and refines these proposals through an ROI pooling module employing a novel Sparse Abstraction operator. Strengths:  Empirically validates the results with multiple trials accounting for stochasticity in training and inference.  Demonstrates strong performance on indoor detection benchmarks significantly improving mAP0.5 compared to previous works.  Employs a classaware proposal prediction which provides additional supervision for the feature backbone.  Uses an efficient 3D backbone inspired by HRNet balancing resolution and computational cost. Weaknesses:  Despite the title suggesting direct point cloud processing the method converts point clouds to voxels potentially misrepresenting the approach.  The revoxelization step introduced in Equation (4) appears complex involving multiple parameters and a custom schedule requiring careful hyperparameter tuning. Questions:  How are the weights in Equation (8) optimized and how sensitive is the model to their values  Alternative approaches to the revoxelization step were explored such as incorporating semantic probabilities into the aggregation module  Is gIOU loss used in place of IOU loss as mentioned in Line 237 If so was the VoteNet baseline in Table 2 also trained with gIOU loss  Are there specific classes for which this model outperforms previous methods especially thinsmall objects given the effort invested in object size during feature aggregation"], "wcBXsXIf-n9": ["Paraphrased Statement: Summary: This study proposes a Deep Simplex Classifier (DSC) that maximizes the margins between classes in both Euclidean and angular spaces for open set recognition. It treats the vertices of a regular Csimplex in the (C1)dimensional feature space as class centers for the C known classes in the dataset. Class samples are then encouraged to be near their corresponding centers. If the feature dimension is less than C1 a Dimension Augmentation Module (DAM) is introduced to increase the dimension to C1 allowing for the construction of a Csimplex. The effectiveness of the proposed method is demonstrated through experiments on five datasets for open and closed set recognition and face recognition. Strengths:  The novel approach of using a regular Csimplexs vertices as class centers provides a clear geometric interpretation for open set recognition.  The method is straightforward to understand and implement.  Extensive experiments demonstrate the efficacy of the method for open set recognition closed set recognition and largescale face recognition. Weaknesses:  Novelty and Related Work: The idea of maximizing class margins by manipulating class centers is not entirely new. Existing methods with similar goals such as [1 2 3] should be discussed and empirically compared. The necessity of maintaining identical pairwise distances between class centers in DSC requires further justification.  Motivation and Introduction: The motivation for considering both Euclidean and angular spaces in open set recognition could be clarified. The presentation of the introduction could be improved to avoid redundancy and enhance logical coherence.  Feature Dimension Restriction: The restriction that the feature dimension must be greater than or equal to C1 limits the methods practical applicability. The proposed DAM module may introduce an excessive number of parameters particularly for largescale datasets. Alternative solutions to alleviate this restriction are crucial.  Hyperparameters: Despite claiming to have no hyperparameters DSC incorporates the radius of the hypersphere (u). The value of u and other hyperparameters in Equation 5 warrant an ablation study to determine their impact on performance.  Unfair Comparisons: In the open set recognition experiments the proposed method utilizes a vast amount of data from 80 Million Tiny Images dataset as the background class which may bias the comparison with existing methods that do not employ such additional data.  Closed Set Recognition: It is unclear whether the hyperparameters of SphereFace CosFace and ArcFace were optimized for the specific classification datasets or if the settings from the original papers were adopted."], "sOVNpUEgKMp": ["Paraphrased Summary This research proposes a new approach for improving the adaptability of deep learning models to solve the vehicle routing problem across various datasets. The method involves knowledge distillation where multiple experienced models (teacher networks) are used to guide the training of a smaller more generalizable model (student network). An adaptive strategy is employed to select the most informative teacher models. Experiments on two datasets demonstrate that the proposed method significantly outperforms existing approaches when combined with them. Strengths and Weaknesses Strengths:  The proposed method is theoretically sound.  Evaluation on multiple datasets confirms its effectiveness compared to existing methods.  The method is presented clearly. Weaknesses:  The method may rely on taskspecific parameters (e.g. training policies model architectures hyperparameters) which may limit its generalizability in practical applications.  Some results in Table 1 (particularly for LKH and Gurobi) appear to be consistently 0.00 which may require explanation or alternative presentation.  The performance variability of the proposed method with respect to hyperparameter settings is not discussed. Questions:  Concerns regarding the generalizability of the proposed method due to its reliance on taskspecific designs.  Explanation or improved presentation of 0.00 results in Table 1.  Assessment of the methods sensitivity to hyperparameter settings."], "uxWr9vEdsBh": ["Paraphrase: This research introduces a novel algorithm for active learning in batch mode where the goal is to select and label data points in a batch to optimize the performance of a subsequent model. The algorithm utilizes Lagrangian duality in convex optimization to quantify the informativeness of each labeled point and predicts the informativeness of unlabeled data using a trained model. It then selects the most informative points within different clusters in the feature space. Strengths:  The use of Lagrangian dual variables as an informativeness metric is innovative.  Estimating dual variables for unlabeled data provides a novel approach for capturing informativeness. Weaknesses:  The assumption that minimizing training set loss is equivalent to minimizing outofsample error is incorrect as it does not account for the impact of noniid data and distribution assumptions.  The methodology relies heavily on strong duality which may not hold for nonconvex loss functions and hypothesis sets.  The use of multiple models has been explored in previous works like VAAL and WAAL. Questions:  Do empirical results confirm strong duality If not what is the duality gap How would a lack of strong duality impact Theorem 3.2  How does the method scale and perform on larger datasets such as ImageNet  Comparing correlations between BADGE and ALLY embeddings would be valuable to assess the methods behavior with nonconvex losses and hypothesis classes."], "ldRyJb_cjXa": ["Paraphrased Summary: The proposed technique Star Temporal Classification (STC) addresses the challenge of missing labels in weakly supervised sequence labeling tasks (e.g. speech recognition handwriting recognition). STC introduces a unique star token that permits alignments that cover all potential tokens when tokens are potentially absent. The loss function is implemented using differentiable weighted finitestate transducers (WFSTs). Experiments on speech recognition (Librispeech) and handwriting recognition (IAM) support the effectiveness of STC. Strengths:  Clear problem definition and solution within the WFST framework  Novel use of a star token and token insertion penalty  Complex implementation integrating WFST and neural network gradients Weaknesses:  Lack of experiments in fullylabeled scenarios which is important for practical use  Underexploration of scenarios where labels are discarded due to filtering  Limited evaluation in a simplified setting where labels are truly missing  Suggestion to explore STCs performance in more realistic scenarios such as semisupervised or transfer learning"], "oDRQGo8I7P": ["Paraphrased Summary: The authors propose an extension of scorebased generative models to data residing on compact Riemannian manifolds. They adapt various concepts from the Euclidean setting including:  Reference distribution: Uniform distribution on the manifold which approximates the limit of Brownian motion on the manifold.  Reverse SDE: An SDE that reverses the Brownian motion allowing sample generation via geodesic random walks.  Score estimation: Using the heat kernel to estimate scores on the manifold.  Vector field parametrization: Neural networks for vector field parametrization.  Exact likelihood computation: Exact likelihood computations similar to the Euclidean case. They prove convergence showing that as the score estimation cost decreases the models density approaches the true density in Wasserstein1 distance. The model is computationally efficient outperforming Riemann continuous normalizing flows and Moser flows on highdimensional manifold data. Strengths:  Extends scorebased generative models to compact Riemannian manifolds.  Indepth theoretical foundation and convergence guarantees.  Better computational complexity than alternatives especially in high dimensions. Weaknesses:  Limited to compact manifolds excluding certain data types (e.g. SPD matrices hyperbolic spaces).  Empirical evaluation on geoscience data (sphere) shows no significant improvement over other methods likely due to low dimension and simplicity of the problem. Questions:  Derivation of the denoising score matching identity.  Potential for using nonuniform reference distributions and comparison to Gaussian base distribution in the Euclidean case."], "wo-a8Ji6s3A": ["Paraphrased Statement: This research explores the effectiveness of projected stochastic gradient descent (SGD) in solving optimization problems with constraints. It identifies a balance between bias and variance when selecting the projection frequency. The biasvariance tradeoff is demonstrated through:  A nonasymptotic study  An asymptotic approach using continuoustime stochastic processes Understanding this balance allows for optimizing the communication rate in federated learning. Strengths and Weaknesses: The continuoustime approach offers a clear understanding of the projection frequency choice. It predicts either convergence to a diffusion process (variancedominated) or a process with Poisson jumps (biasdominated). The papers writing style is clear concise and wellstructured. It introduces necessary concepts effectively providing references for further exploration. The papers techniques may be applicable to other stochastic approximation algorithms. Questions:  Line 782: Can you elaborate on \"entrywise linear interpolation\"  Line 780 4th equality: Why is there no projection in the last term  Equation 16: Should it be vn1 on the righthand side  Line 58: Is pn intended instead of \\betan  Line 185186: Can you provide an explanation or reference for this statement  Line 232: Is it Yt instead of Yt"], "u4KagP_FjB": ["Paraphrase: The research paper introduces Spartan a novel algorithm for training neural networks with sparse parameters. This technique enables direct control over the number of nonzero parameters through a predetermined budget. Contributions: 1. The algorithm combines the soft topk operator and a sharpness parameter allowing it to seamlessly transition between iterative pruning and TopK sparse training. 2. Spartan outperforms existing sparse training methods and is competitive with dense models. 3. The authors analyze the effect of the sharpness parameter on accuracy highlighting the tradeoff between exploration and exploitation. Strengths and Weaknesses:  The complex Algorithm 4 is unclear and may be identical to the Sinkhorn algorithm raising questions about the motivation behind the stopping criteria.  The description of the Sinkhorn iterations in Algorithm 4 does not appear to be correct.  The paper lacks details on several key elements:  Definition and calculation of the soft topk operator  Backward pass calculation  Forward pass calculation including the specific entropic regularization problem  Dimensionality of input vectors  Rationale for the initialization method  Implementation details of the projection \\Pik Questions: 1. Provide a clearer definition of the optimal transport problem in (2) including cost matrix admissible coupling and marginal constraints. 2. Specify the type of entropic regularization used either KL divergence or negative entropy. 3. Expand on the following in the Appendix:  Soft topk operator  Backward pass calculation  Forward pass calculation and the entropic regularization problem 4. Disclose the input vector dimensionality for the applications. 5. Explain the basis for the initialization method and how it was derived. 6. Describe the practical implementation of the \\Pik projection."], "rHnbVaqzXne": ["Paraphrase: The authors present a modification to the Hyperbolic Wrapped Normal (HWN) distribution which models distributions on hyperbolic space. The HWN distribution maps a distributed tangent vector from an origin to a mean point and takes the exponential map to obtain the distribution. The authors note that the standard basis vector in the tangent plane at the origin when transported to the mean point and projected to a Poincar\u00e9 disk is not aligned with a standard basis vector in the disk. They propose that this alignment is undesirable and posit a distribution where one of the tangent vectors points from the mean point to the origin. To address this they propose the Rotated HWN (RHWN) distribution constructed such that the first dimension of the tangent plane points from the mean point to the origin. Empirically the RHWN distribution outperforms the HWN distribution with diagonal and full covariance matrices on several tasks including synthetic data correlation word embedding correlation and Atari rollout embedding correlation. Strengths:  Clear and wellwritten paper with insightful illustrations  Effective diagnosis and simple solution to an identified issue  Simplicity and ease of implementation  Improved performance over prior work Weaknesses: Symmetry of the Geometry:  The papers analysis assumes a specific choice of coordinates and origin in hyperbolic space which is arbitrary.  The RHWN distributions added rotation may not be necessary due to the isometry symmetry of hyperbolic space.  The choice of origin in the prior distribution p(z) may influence the results which is not discussed. Other Points:  It is not clear why the x vector should be fixed and not learned by the encoder network.  The rotation method could be generalized to other manifolds with a prior and expmapped normal distributions.  There are multiple rotation matrices that satisfy the desired alignment and the paper does not discuss why the specific one chosen is desirable.  The term \"prior\" is used confusingly in the paper.  The method is an incremental change over prior work limiting its originality. Conclusion: The papers analysis should be revised to account for the isometry symmetries of hyperbolic space and the role of the prior distribution. If these issues are addressed satisfactorily the paper could be recommended for acceptance. Addressing the additional points raised could enhance the papers significance."], "yHFATHaIDN": ["Paper Summary: The MABSplit paper proposes using multiarmed bandits to estimate the performance of splits during decision tree induction rather than evaluating them exactly. This technique helps identify and eliminate unpromising features early on by only evaluating them on a subset of data points. Strengths and Weaknesses: Strengths:  Novel and practical extension to decision tree induction  Solid theoretical foundation  Comprehensive literature review Weaknesses:  Lack of thorough experimental evaluation  Unclear wording in some sections Detailed Review: The idea of combining bandit learning and decision tree induction is innovative. The related work section is thorough but it would be beneficial to mention XGBoost LightGBM and CatBoost explicitly. The theoretical sections are clear and wellpresented but the runtime analysis could be more detailed to address concerns about worstcase runtime scenarios. The experimental evaluation is limited to synthetic data and MNIST with no comparison to existing methods like Hoeffdings Trees or FForest. This evaluation could be enhanced by using a wider range of realworld datasets. Improvements for Rejection:  Clarify the concept of \"linear runtime\" and provide pseudocode of the split induction process for comparison.  Provide worstcase and expected runtimes for MABSplit and related methods.  Add comparisons to Hoeffdings Trees andor FForest.  Include more (realworld) datasets in the evaluation. Questions:  The definition of \"linear runtime\" in the paper seems to refer specifically to binning while a more general \"standard approach\" may have a runtime of O(N log N). Clarify this distinction.  While MABSplit may have a sublinear expected runtime it is still possible for the worstcase runtime to be O(N2). Confirm this understanding."], "tbId-oAOZo": ["Paraphrased Summary: The paper introduces a novel method for estimating poses of multiple individuals through a fully trainable system. It combines principles from MaskRCNNRoiAlign and TransformersAttention within a staged approach that gradually improves box and keypoint predictions. The paper includes comprehensive analyses to evaluate the effectiveness of each component and achieves impressive performance on standard pose estimation datasets like COCO and CrowdPose. Strengths:  Excellent experimental outcomes on widely recognized benchmarks.  Despite the complexity of the method the paper provides clear explanations of each modules function with Figure 2 being particularly illustrative.  The authors plan to release their code which is highly desirable due to the methods intricacy and the difficulty of replicating the reported results with the information provided in the paper. Weaknesses:  The method is indeed complex involving multiple components.  The paper lacks information on runtime performance making it difficult to gauge its speed and efficiency. Both training and inference times should be included in the revised manuscript. Minor Typos:  Typo in Figure 2(e): \"Normalizing\"  Line 166: \"Multilayer Perceptron\" Questions:  The most notable missing information is the methods runtime performance during training and inference stages.  While the systems complexity makes it somewhat difficult to reproduce based on the papers information the authors intention to release the code as stated in the abstract will sufficiently address this concern."], "yKYCwTvl8eU": ["Weaknesses 1. Model Interpretability Using transferred features for known concepts and residual features for unknown concepts may reduce interpretability compared to attributebased CBMs. The implicitly learned features may not be as semantically meaningful. Potential Ways to Close the Interpretability Gap:  Explore methods for extracting meaningful representations from transferred features.  Develop techniques to explain the decisionmaking process based on both known and unknown concepts.  Investigate how the model utilizes known concepts to mitigate reliance on shortcuts. 2. Lack of Qualitative Analysis The study primarily relies on accuracy comparisons to demonstrate the models reliance on concepts. Suggestions for Qualitative Analysis:  Perform visual grounding analysis on datasets where regions of interest can be identified.  Quantify the models attention to different image regions to determine if it is avoiding spurious regions. 3. Generalization to RealWorld Challenges Testing on artificial shortcuts in CUB may not fully capture realworld challenges. Suggestions for More Realistic Evaluation:  Utilize benchmarks like WILDS that introduce more realistic distribution shifts and shortcuts.  Explore the use of known concepts or domain expertise to mitigate shortcuts in such benchmarks."], "zuL5OYIBgcV": ["Paraphrased Statement: Summary:  This paper presents a shallow network designed for low processing time.  Key features include a multistream architecture RepVGGlike structural reparameterization and a SSE block.  Results on ImageNet CIFAR and COCO datasets demonstrate that smaller models can perform comparably to larger models. Strengths:  This study demonstrates that a modest 12layer model can achieve over 80 accuracy on ImageNet and an AP of 48 on MSCOCO highlighting the relevance of research on shallow models.  The proposed techniques including RepVGGstyle reparameterization multistream design and SSE block are wellexplained and their effectiveness is supported by experimental results.  The findings on performance with custom hardware settings are noteworthy showing potential communication overhead benefits. Weaknesses:  The presented results mainly involve small and mediumsized models and its unclear whether the depthperformance tradeoffs apply to large models.  The paper lacks specific proposals for implementing the discussed future hardware.  The writing could use some improvement. Suggestions:  Compare this work to RepVGG to emphasize this papers contributions and highlight differences.  Use a more precise term than \"VGGstyle block\" (e.g. \"VGGstyle architecture\" or \"RepVGGstyle design\").  Include missing references.  Use the correct mathematical notation (e.g. 3x3 instead of 3X3).  Correct the duplicate \"consists of\" phrase."], "ue4gP8ZKiWb": ["Paraphrase: The study introduces a new certified machine unlearning method (PCMU) that enhances the effectiveness of unlearning for complex models used with large datasets. The method establishes a theoretical framework linking certified robustness in classification to certified machine unlearning in gradient quantization. PCMU utilizes randomized data smoothing and gradient quantization to generate reliable certificates for data removal. A practical framework is proposed for efficient certificate generation. PCMU offers three key advantages: it allows for simultaneous training and unlearning facilitating unlearning efficiency it enables a single training session to address multiple unlearning requests and it eliminates the need for prior knowledge of forgotten data before unlearning. Strengths:  Focus on the important topic of prompt machine unlearning addressing the challenge of enhancing unlearning efficiency for complex models in largescale scenarios.  Clear motivation and rationale for using randomized gradient smoothing and quantization.  Thorough description of the method and theoretical foundation including analysis of certified radii data removal budgets and correlations between radii.  Integration of certification and training for machine unlearning into a unified framework.  Convergence analysis demonstrating the \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c effectiveness and efficiency of the PCMU method.  Comprehensive evaluation on real datasets showing superior performance compared to other methods. Weaknesses:  Typos and grammatical errors that should be corrected for clarity.  Inclusion of the related work section in the main body of the paper would improve readability and allow for better comparison with existing studies.  Layout adjustments to avoid splitting tables and relevant text across pages."], "zyrBT58h_J": ["Summary Paraphrase: This study aims to tackle the challenge in autobidding where training is typically conducted offline for safety concerns. However this practice can lead to a performance gap between the learned policy and the actual online environment. The authors propose a continuous online reinforcement learning approach that utilizes a safe policy to explore the real environment. Strengths and Weaknesses: Strengths:  Novel and significant problem addressed.  Intriguing and technically sound idea.  Realworld platform experimentation. Weaknesses:  Approximationsrelaxations with insufficient justification or theoretical support.  Potential flaws in proofs.  Absence of comparisons to stateoftheart autobidding methods. Detailed Comments:  The relaxations and approximations in Sections 3 and 4.3 require further justification and theoretical analysis.  The proof for Theorem 1 may be flawed due to the possibility of the upper bound being violated.  The authors do not compare their approach to existing multiagent RL methods for autobidding. Minor Issues:  The presentation and writing could be improved in clarity and consistency.  Specific questions:  What is the meaning of pjt and vjt in Proposition 2  What is the significance of \\mus in Figure 6  Does \\pie represent the SER policy after 7 iterations  Why are results only shown after three iterations in Table 3"], "sZAbXH4ezvg": ["Paraphrase: Summary: A novel graph neural network variant MGNNI is proposed to capture distant dependencies in graphs. The authors identify limitations in existing implicit GNNs such as limited range and inability to handle multiscale information. MGNNI addresses these issues with multiscale propagation to improve information capture and experimentation on various datasets demonstrates its effectiveness. Strengths:  Clear presentation and motivation.  Thorough experimental and theoretical analysis.  Important and sufficient theoretical comparison of effective range.  Potential for advancement in implicit GNN research. Weaknesses:  Theoretical analysis assumes contraction factor assumption not used by all implicit GNNs which should be acknowledged as a limitation.  Computational complexity comparison is lacking for IGNN and CGS.  Impact on computational analysis due to MGNNIs multiscale propagation and attention mechanism is not discussed.  Oversmoothing handling in MGNNI is not addressed.  Experimental benchmarks are limited to small graphs raising questions about performance on larger datasets.  Evaluation on a limited number of bioinformatics datasets may not be comprehensive. Questions:  Can IGNNEIGNNCGS effectively capture multiscale information to demonstrate MGNNIs superiority  How can implicit GNNs without contraction factors be included in the theoretical analysis framework  How does MGNNIs multiscale propagation and attention mechanism affect computational analysis  Does MGNNI experience oversmoothing issues  What is MGNNIs performance on massive graphs and diverse types like social networks"], "x0LCDsbJ5JF": ["Paraphrased Statement: Summary: This research introduces the SpatiallyAdaptive SqueezeExcitation (SASE) module which endeavors to understand a full attention mechanism as a threedimensional attention matrix. The module has two variations for image categorization and creation tasks. Trials on publicly available datasets validate its effectiveness. Strengths and Weaknesses: Strengths:  Understandable writing  Novel concept of a full attention matrix  Comprehensive experiments Weaknesses:  Lackluster experimental outcomes  Similarity to the dual attention mechanism Questions: 1. SASEs idea resembles that of the dual attention mechanism which concurrently considers spatial and channel attention through attention matrices. 2. The experimental results are not impressive in Table 8 improvements are minimal and in Table 6 the proposed method exhibits the poorest diversity performance. An explanation is requested."], "xnuN2vGmZA0": ["Paraphrase: Summary: This research presents a groundbreaking approach for offline video instance segmentation by explicitly using objectoriented queries. Specifically the method extracts objectspecific information using an image detector which is then converted into \"object tokens.\" These tokens are subsequently connected to an additional object encoder to achieve videolevel comprehension. The methods strength is demonstrated through experiments conducted on YouTuveVIS2019 YouTuveVIS2021 and OVIS. Strengths:  The use of object tokens for direct modeling of spatiotemporal information is novel and efficient.  The paper is clearly written and wellexplained. The code for reproducing the results is also provided.  Experiments on the VIS benchmark showcase the effectiveness of the proposed approach.  The research offers a fresh perspective for exploring longrange video understanding in areas such as video object detection multiobject tracking and video instance segmentation. Weaknesses:  The method relies on mask2former for image instance segmentation which presents an unfair comparison with the stateoftheart SeqFormer method. It would be beneficial to compare the results using a consistent image instance segmentation approach with SeqFormer.  Employing all image queries could lead to redundancy. To address this the paper suggests considering only foreground queries for video instance segmentation tasks.  The number of frames varies between videos. An alternative approach is to utilize a subset of object queries to ensure a uniform number of queries per frame.  The paper does not include experimental results on YouTubeVIS2022. Questions: Overall the paper is wellstructured with a clearly explained approach and promising experimental outcomes. The primary concern revolves around the unequal comparison with related work."], "mmzkqUKNVm": ["This research introduces a novel semantic segmentation approach that utilizes semantic difference convolution. By integrating highlevel semantic information into lowlevel convolution the method enhances feature extraction and improves semantic boundary sensitivity. Compatibility with existing segmentation baselines makes it easy to implement. Experiments with various models on multiple datasets demonstrate the methods effectiveness. The paper is wellorganized and provides detailed analysis including computational complexity and ablation studies showcasing the stateoftheart results achieved."], "pm8Y8unXkkJ": ["Paraphrased Statement: This paper explores the classification issue of unbalanced binary data. Contributions: 1. A revised characterization of classifiers optimizing performance metrics derived from the confusion matrix. 2. Existence of a Bayes classifier for stochastic classifiers but not deterministic ones. 3. Performance guarantees under general performance measures linked to the estimation error of the class probability function. 4. Analysis of knearest neighbor classification under uniform loss. Strengths:  Generalization of the Bayes optimal classifier.  Regret bounds.  Formulation of uniform class imbalance.  Analysis of knearest neighbor classification. Weaknesses:  Lack of realworld dataset experiments.  Unclear motivation for uniform class imbalance and knearest neighbor analysis.  Misleading abstract that implies broader study of class imbalance.  Unclear definition of uniform class imbalance. Questions: 1. Request for experiments on realworld unbalanced datasets to validate theoretical results. 2. Suggestion to clarify the abstract and highlight the generalization of the Bayes optimal classifier. 3. Inquiry into the rationale behind the definition and prevalence of uniform class imbalance. 4. Request for justification for focusing on knearest neighbor classification and the feasibility of analyzing other commonly used classifiers."], "tIZtD2kZ6zx": ["Paraphrased Statement: Summary: This research introduces a model for creating images (sketches or characters) using sequences of lines and a model for decoding line sequences from an image. These models are trained together by minimizing a lower bound. The imagegenerating model employs a Bezier spline curve system where control points are generated by a recurrent convolutional neural network. A recurrent spatial transformer network is used to define the location of the next line. The technique can reconstruct images from character and sketch databases perform crossdatabase generalization generate new characters and classify characters with only one example. Strengths and Weaknesses: Originality This approach shares similarities with other methods in the field such as using splines for drawing (Mihai and Hare 2013) employing variational objectives (Tomczak and Welling 2018) recursively generating images (Feinman and Lake 2021) and utilizing spatial transformer networks for character recognition (Hewitt et al. 2018). The combination of recursive image generation and variational objectives was previously explored by AIR (Eslami et al. 2016) while the line model resembles GNS (Feinman and Lake 2021). DooD combines aspects of AIR and GNS using a variational objective with a line model and being trained on images instead of line sequences. It appears to be the first recursive imagesketching model that incorporates spatial transformer networks into its composition process. Quality The papers strengths include evaluations on various datasets including crossdataset transfer and diverse evaluation tasks. Additionally the authors analyze the methods internal representation compared to related methods. The recurrent structure of the model is shown to be beneficial through an ablation study. The model qualitatively outperforms baselines in terms of image reconstruction but the difference is not quantified. Weaknesses include the lack of strong motivations for many modeling choices. Particularly the choice of Laplace noise which seems to accumulate over time appears unusual and possibly not suitable for the image distribution of human sketches. The exclusive use of only one real baseline (AIR) is considered insufficient especially since AIR was not primarily designed for line drawings. Clarity The paper is wellwritten and organized. However the methods details are not described sufficiently. The line and rendering components of the model are particularly difficult to understand because crucial information is missing from the main text and the appendix uses notations that are not defined in the main text. The authors recognize the challenge of explaining such a complex method within the given space limitations and express willingness to place more details in the appendix. If the methods clarity is improved the authors would reconsider their \"reject\" score due to the lack of reproducibility. They emphasize the need for a complete description of the methods details either in the main text or the supplement as opposed to solely relying on code submission. Welldocumented code would further enhance the papers reproducibility. Significance DooD presents a model for capturing human line sketches and handwritten characters from static images. It claims to outperform previous approaches in crossdomain generalization computational efficiency and interpretability. The methods interpretability appears to be superior to AIR and although methods that do not use recursive drawing are not evaluated in terms of interpretability they are likely to have even lower interpretability. DooD is evaluated against GNS BPL and VHE which require linelevel data. However more relevant comparisons would be with oneshot image classification models trained on static images. Questions:  Is there evidence to support the claim that DooD is computationally more efficient than related methods in practice  Does DooD surpass other character recognition models based on static images in oneshot classification  Given that GNS (Feinman and Lake 2021) can already \"draw out of distribution\" what is the motivation behind the name DooD  Why is the decision to stop made before composing the current line (rather than afterward) Doesnt this result in wasted effort if the stroke is ultimately not used  What is the justification for the Laplace noise model Why is its scale fixed What is the range of pixel intensities and is the Laplace distribution suitable given this range Why are pixel intensities assumed to be conditionally independent given the composited image Does the addition of Laplace noise accumulate over time If so doesnt this increase the variance of pixels from earlier lines  Equation 6: What is the range of the index t  Supplement B.1: The notation used here (equations 1014) differs from the notation in the paper. How are the terms in equations 16 defined in terms of these quantities  Line 119: How is beta tuned"], "nYrFghNHzz": ["Paraphrase: Summary: This study aims to select the best treatment from a pool of candidates for an individual or group of individuals with similar treatment responses. The authors propose a treatment clustering approach that groups treatments with comparable effects. The approach utilizes a statistical regression model that considers the reward as a function of common treatment effects treatmentspecific effects and noise. The common and treatmentspecific effects are determined by patient characteristics represented in a feature vector. Using data in the form of (patient treatment reward) triplets the goal is to estimate the regression coefficients. A grouppromoting penalty encourages clustering of treatments with similar effects. Technical Details: To eliminate irrelevant covariates in the patient feature vector the authors initially apply group lasso regression. The main regression problem can be solved using an accelerated proximal gradient algorithm. Evaluation: The method has theoretical groupconsistency properties and has been validated on synthetic and real data demonstrating its efficiency. Limitations:  The model is designed for continuous outputs while discrete outputs may be encountered in some medical scenarios.  The model uses linear regression which may not adequately capture complex data patterns. Questions:  Have the authors considered using more flexible models such as kernelized linear regression  The interpretation of random variable A in section 2 should be clarified.  How is M0(.) estimated in the real data experiment in section 5 Minor Suggestion: Avoid using bold letters for scalar variables to enhance readability."], "riIaC2ivcYA": ["Paraphrase: Summary: This research focuses on active learning a fundamental issue in the field. It expands upon EENet for active learning and presents an analysis of its regret. The authors demonstrate an improved regret bound compared to previous work. Experiments confirm that the proposed algorithm outperforms prior approaches. Strengths:  Enhanced regret analysis shows an O(log T) improvement in regret bound eliminating dimensionality and function complexity issues.  Theoretical justification for the proposed algorithm is provided.  Empirical findings support the effectiveness of the method. Weaknesses:  The proposed algorithm and regret analysis are largely extensions of EENet potentially limiting novelty. Questions:  Is DerivativeContext (DC) Embedding distinct from the input of f2 in EENet  While Algorithm 1 reduces \\betat by \\sqrt(t) how is \\betat controlled experimentally  Was hyperparameter grid search conducted on the same datasets used for regret evaluations  Do budget trends observed for 3 of instances hold for different budget levels Minor Comments:  Missing closing parenthesis in equation (7.28).  Typo: \"lesat\" should be \"least\" in equation (7.28).  Subscripts of expectations in equations (7.38) (7.39) (8.12) (8.13) (8.14) (8.15) should be x\\tau and y\\tau.  Equation (7.78) appears to use max instead of min.  \\thetat(k) in Lemma 7.13 is not used in the main text.  Missing equation number in Lemma 7.13.  Correction in equation (8.2): f(xti\\thetat1)f(xti\\thetat1) instead of f(xti\\thetat1)f(xti\\thetat1).  \\calHt1 in Lemma 8.2 is not used in the main text."], "rlN6fO3OrP": ["Paraphrased Statement: Summary: This study investigates a backdoor assault on continuous learning prompted by prompts. After demonstrating the limited impact of common backdoor attacks in lowshot learning environments in the introduction the study proposes BadPrompt to build effective triggers for promptbased learning. Experiments are performed on five datasets and two promptbased learning models. Strengths and Weaknesses: Strengths:  The paper is lucid and concise.  BadPrompt comprises two modules:  The first aims to mitigate the substantial drop in classification accuracy (CA) faced by existing backdoor baseline methods by selecting triggers with minimal similarity to samples from nontarget classes.  The second module optimizes trigger set combinations for higher attack success rate (ASR).  The motivations and solutions are logical and meaningful. Weakness:  The paper relies on Natural Language Processing (NLP) baselines that may not be familiar to all readers.  The experiments appear comprehensive and effective but the author suggests using CA  ASR in Figure 4 and CA  ASR  CA in Figure 3 to represent the accuracy of samples with correctly classified triggers. Question:  Please review the weakness section for further details."], "o4neHaKMlse": ["Summary: This paper introduces FIBER a transformer model for visionlanguage processing. FIBER incorporates imagetotext and texttoimage crossattentions. Pretraining occurs in two phases: one for general visionlanguage interaction and one for languagebased localization. FIBER achieves strong performance on various visionlanguage and object detection benchmarks. Strengths:  Clear and wellorganized paper  Improved architecture with additional texttoimage crossattention and refined pretraining  Comprehensive evaluation on multiple tasks Weaknesses:  Combining known techniques without novel insights  Insufficient ablation studies to determine the impact of individual components  Careless claims of performance superiority without considering pretraining image size and backbone strength Questions:  Does coarsegrained pretraining benefit other localization tasks besides grounding  How are object names converted into text for training with bounding box annotations  Provide evidence to support claims of superiority over GLIPs fusion mechanism  Compare the efficiency and performance of Swin Transformer and ViT in visionlanguage tasks  Quantify the benefits of using RoBERTa over BERT in FIBER"], "mhe2C2VWwCW": ["Paraphrased Summary: The paper explores \"predictive querying\" in language models where probabilities are estimated for future events based on current data. They develop an importance sampling framework to handle this task and improve the accuracy of beam search by combining it with importance sampling. The authors highlight the limitations of beam search for predictive querying linking its error to the models conditional entropy. Paraphrased Strengths and Weaknesses: Strengths:  Addresses a crucial issue in forecasting with neural models.  Introduces a novel framework and hybrid method for predictive querying.  Provides insights into the limitations of beam search. Weaknesses:  The motivation for the proposed distribution could be clearer.  The setup while new for neural models has been explored in other domains with simpler probabilistic models. Paraphrased Questions:  Why was coveragebased beam search omitted from the experimental results  Could the error compound when performing multiple related queries  Is there an alternative evaluation method for longer horizons without requiring the computation of true probabilities"], "zFW48MVzCKC": ["Summary: The paper introduces QSCAN a technique for breaking down a multiagent teams value function into smaller parts focusing on the teams smaller cooperation groups. QSCAN builds on QPLEX by assigning each agent weights that indicate their contribution to various potential subgroups. The paper evaluates two variants of QSCAN: QPAIR (which considers all twoagent subgroups) and QSCAN (which uses selfattention to determine agent weights). They compare QPAIR and QSCAN to QMIX and QPLEX in different settings. Strengths and Weaknesses:  Originality: A related concept of hierarchical value factorization for subgroups was proposed in 2021 but QSCANs specific architecture and its relationship to QMIX and QPLEX are novel.  Quality: The paper appears technically sound and experiments support QSCANs performance. However the evaluation could be expanded to include more agents and scenarios with more complex coordination requirements.  Clarity: The approach is welldescribed. However the attention heatmaps in Figure 6 are difficult to interpret due to their small size.  Significance: QPAIR and QSCAN generally outperform QMIX and QPLEX but the significance of these results needs to be further assessed by providing error bars or details about training runs. Minor Comments and Questions:  QPAIR is mentioned on line 72 before it is properly defined which can be confusing.  The global reward function is denoted with \"the\" on line 91.  The text and labels in the figures are difficult to read when printed.  Line 283 refers to \"several stateoftheart MARL approaches\" but only mentions QMIX and QPLEX.  Its unclear if the importance weight g\\textitSTi is nonnegative (line 225).  The number of training runs per task is not specified."], "xOK40an4ag1": ["Paraphrased Statement: Summary:  The researchers examine how training recurrent neural networks without restrictions results in organized synaptic connections.  They introduce a novel way to break down the synaptic matrix to reveal this structure which is more compact than a standard decomposition.  They propose defining \"operative dimensions\" as pathways in weight space that cause significant changes in neural behavior. These dimensions are determined locally at different points of interest and then globally measured. Strengths and Weaknesses:  Strengths:  The question is important and underresearched.  The proposed approach is unique.  Weaknesses:  The approach overlaps with the work by Schuessler et al. (2020) which the paper only briefly acknowledges.  Other relevant studies such as Smith et al. (2021) are not cited or addressed.  The paper lacks analytical explanations for the emergence of low dimensionality in the operative dimensions. Questions: 1. What are relevant studies in the feedforward domain such as Saxe et al. (2014) and Timor et al. (2021) 2. How can the findings be reconciled with previous research that highlights the importance of constraints in RNN training 3. Why is the random noise in Equation 1 not scaled with the time step size (sqrt(dt)) 4. How do operative dimensions relate to cases where small local changes result in large global changes in neural dynamics 5. What is the connection between the Smith et al. (2021) study and the operative dimensions concept 6. Is there any relationship between the operative dimensions and the eigenvalue spectrum of the synaptic matrix or its linearized versions"], "oPzICxVFqVM": ["Paraphrased Statement: Summary: This paper demonstrates that scorebased generative models aim to minimize the Wasserstein distance between themselves and a target distribution. The Wasserstein distance is shown to be bounded from above by a measure derived from the models objective function with constant multipliers and a fixed offset. Strengths:  The paper presents mathematical proofs establishing the connection between scorebased models and Wasserstein distance.  The derived bounds provide insights into the performance of scorebased models. Weaknesses:  Assumptions: The main theorems rely on assumptions that may be restrictive in practical applications.  Limited Theoretical Results: The abstract suggests that scorebased models minimize the Wasserstein distance but the main theorems do not directly support this claim. The theorems establish only upper bounds on the Wasserstein distance.  Unclear Contribution: The significance of the papers results for deep learning and related fields is not explicitly explained. The relevance of Wasserstein distance bounds in these applications is not made clear. Questions: 1. Do the theorems provide conclusive evidence that scorebased models minimize the Wasserstein distance 2. Can practical examples that satisfy the strong assumptions in the main theorems be provided 3. What are the specific contributions of this paper and how do the results advance the field of deep learning"], "ksVGCOlOEba": ["Paraphrased Statement: This study presents a framework that combines quantization (reducing bit precision) and pruning (removing unnecessary weights) for compressing models after they have been trained. By applying these techniques layer by layer it achieves compression without the need for complete retraining. Evaluations on ResNet and BERT models show a 2 drop in accuracy when quantizing and pruning models together while reducing model size by 12x. Strengths:  Explores a unique combination of sparsification and quantization based on the \"optimal brain surgeon\" framework.  Introduces techniques for inducing sparsity and quantization in a layerbylayer manner.  Demonstrates promising results in combining quantization and sparsification using layerwise compression without extensive retraining. Weaknesses:  The problem definition is confusing as it focuses on layerwise compression rather than the actual optimization objective of minimizing model size or latency with accuracy constraints.  The paper discusses challenges faced by an outdated framework (OBS) rather than addressing those faced by current stateoftheart compression methods.  It fails to provide a clear comparison with existing unified compression frameworks that also perform quantization and pruning.  The difference between the proposed posttraining compression and existing techniques is not welldefined raising questions about its fundamental advantages over them. Questions:  How does the proposed unified method compare directly with existing methods that combine sparsification and quantization without layerbylayer compression such as weight magnitudebased pruning and quantizeaware training"], "zfo2LqFEVY": ["Paraphrased Summary: This paper tackles the issue of predicting event categories over time in audiovisual data using limited labels. It proposes an approach that utilizes attention between audiovisual features and learned class representations to derive classspecific embeddings for event detection. The models performance is assessed on the Look Listen and Parse dataset consisting of 11K video clips with video labels for training and 2K video clips with both audio and video labels for testing. Strengths and Weaknesses:  Strengths:  The proposed method demonstrates potential in addressing the problem.  Modest improvements are attained on a challenging task evaluated on a substantial test set.  The ablations in Table 2 provide valuable insights.  Weaknesses:  Audio event performance remains unimproved.  False positive analysis in Figure 3 suggests bias towards different operating points more comprehensive metrics are recommended. Clarity Concerns:  The papers notational issues impair comprehension.  Some empirical claims and hypotheses lack clarification.  Equation (6): The crossentropy formula appears reversed and a clarification of its vectorized form is needed.  The crossmodal attention equations (1) and (2) are likely written incorrectly.  Selfattention notation abruptly changes leading to confusion.  Equation (10): The necessity of separate learned weights for class tokens and modalityspecific features requires explanation.  Equation (12): The notation is unclear especially regarding the term \"1.\" A more detailed explanation of the crossentropy formula is recommended. Errata:  \"grouping\" is misspelled as \"groupinging\" in the quoted text. Questions:  See the \"Weaknesses\" section for questions regarding the limitations of the model and the analysis in Figure 3."], "vK53GLZJes8": ["Paraphrased Statement Offpolicy reinforcement learning (RL) using function approximation and bootstrapping may lead to instability. While l2 regularization is often assumed to prevent this the authors demonstrate examples where l2 regularization can actually worsen the error in linear models. They provide both analytical and geometric arguments for this and experiments show that l2 regularization can indeed increase value estimation error or even cause divergence in both linear and nonlinear models. Strengths  Examines a fundamental issue in offpolicy RL with practical implications.  Wellwritten and logically organized.  Geometric interpretation helps build intuition. Weaknesses  Lacks specific guidance on the pitfalls of regularization as the authors mainly suggest that it should be avoided.  The neural network experiment in Example 4 is somewhat misleading due to the logarithmic xaxis and the yaxis not starting at zero.  The generalization of results from linear to nonlinear models is not fully convincing. Minor Edits  Correct grammar in line 48.  Fix typo in \"regularization\" in line 96.  Remove colon at the end of a section in line 129.  Change \"trajectory (1)\" notation to avoid confusion with equations in lines 145 and 149.  Use \"reweighting\" instead of \"reweighing\" in line 195. Questions 1. How can it be guaranteed that \\hatwTw(\\eta) \\leq 0 \\forall\\eta in Example 1 Are there theoretical results indicating when such a distribution \\mu exists or not 2. Why does the geometric interpretation of vacuous models (trajectories within the halfplane tangent to the l2 ball) extend to nonlinear (NN) models 3. What would happen if a NN with more than 3 neurons per layer was used in Figure 5b Would error still increase for some \\etavalues"], "z9poo2GhOh6": ["Paraphrased Statement: Summary: The paper analyzes two methods for optimizing quadratic problems using Stochastic Gradient Descent (SGD). It highlights two distinct convergence rate regimes: a large batch setting with accelerated rates and a small batch setting with nonaccelerated rates. Strengths and Weaknesses:  The papers approach is commendable and the theoretical findings align with practical observations.  The separation of convergence rates into two regimes is intuitive and consistent with practical behavior.  While momentum improves robustness and generalization in practice it does not appear to provide acceleration in small batch settings as predicted by the theory.  The authors inquire about previous research showing acceleration improvements with momentum in largebatch training. Additional Observations:  The papers use of discrete Volterra equations is unfamiliar to most readers but the arguments are generally understandable.  The precise results regarding the parameter \\zeta and its relationship with the spectrum are impressive.  Additional plots illustrating the behavior in the crossover region would be helpful for visualization.  The use of textwrapped figures is impractical. The authors are encouraged to stack the figures sidebyside for better readability. Questions:  To avoid confusion it is suggested to use a different notation for the kernel such as K instead of capital kappa.  The beginning of section two may require additional context from the appendix to be fully comprehensible.  The eigenvalues of H should be clarified as \\sigma or \\sigma2. Using \\sigma2 may be unusual."], "ySB7IbdseGC": ["Following consideration of the authors response I have increased my rating to a weak accept. The reviewer acknowledges the improved clarity of the paper after restructuring. However the reviewer maintains that the extension of SVAE to structured factor potentials is not particularly novel and that the primary contribution of the work is the AEAsvGPFA model. The reviewer also admits unfamiliarity with the applications and related literature of the AEAsvGPFA model which may affect its assessment within the Neurips community. Therefore the reviewer will consult with other reviewers and the area chair for further evaluation."], "nLGRGuzjtoR": ["Summary: The authors question the effectiveness of concept removal methods that rely on probing models. They believe that these methods are unreliable because the representation space can be corrupted or the model may still rely on the concepts that were supposedly removed. They support their hypothesis with toy examples and experiments on controlled datasets where the concept is highly correlated with the output label. Strengths:  Addresses a challenge in debiasing neural networks.  Provides compelling toy examples. Weaknesses:  Failure mode of INLP: The authors argue that INLP corrupts the representation space even when there is no conceptlabel correlation. However the experiments where INLP is shown to fail involve a dataset with all training examples having the same concept label which naturally leads to information loss from the representation. This setup does not demonstrate a valid failure mode of INLP.  Failure mode of adversarial removal methods: The experiments testing adversarial removal methods rely on a dataset where the concept label is perfectly correlated with the main task label. In such cases it is impossible to ensure that the removal process removes conceptrelated information making it difficult to claim a failure mode for these methods. Open Questions:  Is \"w.r.t. task label yp\" in Assumption 3.3 a typo and should be concept label instead  Figure 1(a) appears to contain errors in the annotations.  The Proof Sketch should be included in the main text to support the hypotheses.  Clarify what the \"property\" refers to in Section 4.2.  Define ERM (empirical risk minimization) before using it as a term."], "thgItcQrJ4y": ["Paraphrase: Summary: This study aims to elucidate the \"Edge of Stability\" (EoS) phenomenon posing two key questions: 1. Why does sharpness tend to increase during stable gradient descent 2. Why does sharpness decrease or halt its increase after gradient descent becomes unstable The paper comprises two components: 1. Rigorous Mathematical Analysis of TwoLayer Linear Networks (Section 4):  The NTK (Neural Tangent Kernel) is split into two terms: one from gradients concerning the first layer and the other from gradients concerning the second layer.  It is shown that the second NTK term remains relatively stable during the initial sharpening phase implying that the NTK remains diagonalizable by the input data Gram matrixs eigenvectors.  Progressive sharpening is proven by demonstrating that the secondlayer weight norm increases during gradient descent and that the inner product between predictions and residual is consistently negative.  After destabilization gradient descent transitions to a flatter region where sharpness is below 2  eta. 2. NonRigorous Mathematical Analysis of General Neural Networks (Section 3) Combined with Experiments:  Experimental evidence suggests that insights gained from the twolayer linear NN analysis carry over to general neural networks.  The sharpness dynamics often correlate with those of the lastlayer weight norm although this correlation weakens towards the end of training. Strengths:  First proven demonstration of progressive sharpening in a specific architecture (twolayer linear network).  First proven demonstration of sharpness reduction due to instability in a specific architecture (twolayer linear network). Weaknesses:  Claims regarding the extension of the EoS analysis from twolayer linear networks to general neural networks are less convincing due to reduced correlation between sharpness and lastlayer weight norm and the involvement of multiple layers in influencing sharpness. Questions: For the twolayer NN it remains unclear:  If the degree of sharpening depends on the input dataset (e.g. would sharpening occur with random Gaussian data).  If the degree of sharpening depends on the lastlayer initialization (which is larger than the standard initialization)."], "oUigTwc7Cw5": ["Summary: The researchers looked at how the retina organizes itself to process information efficiently. They updated their model to consider how neurons respond to movement and video inputs. This is important for understanding how the brain deals with the visual world around us. They also came up with a new way to create different optimized versions of the retina. Strengths:  Expanded existing models to work with moving images  Examined how patterns in natural videos affect how neurons work  Developed an approach that allows for multiple optimized versions of the retina Weaknesses:  Some parts of the writing are a bit technical and hard to understand.  The model is based on the idea of efficient coding but the researchers dont really discuss how accurate this assumption is. Questions:  Lines 124: Could you explain the Bravais lattice and principal and reciprocal vectors in more detail  Fig 2A: What do the colored points represent  Fig. 5: Can you add more clear labels or explain the figure more intuitively"], "q__FmUtPZd9": ["Paraphrased Statement: Summary: The research introduces a framework for reassigning tasks in the management of disease spread within networks. This framework allows for the resolution of a current task using data gathered from a previous task. A generalized problem formulation is established encompassing both enhancing and limiting disease spread. Additionally an optimized problem reformulation is derived which can be integrated with various algorithms for efficient approximate solutions. Strengths and Weaknesses: Strengths:  Novelty: The framework is an innovative approach to task migration between distinct social management tasks with a comprehensive problem definition and objective reformulation.  Quality: Theoretical analysis demonstrates the frameworks generalization capability with error bounds established. Design choices are optimized based on identified conditions affecting generalizability. Extensive empirical evaluations assess the frameworks performance in diverse scenarios and benchmarked against baselines. Weaknesses:  Significance: The frameworks relevance and context within existing literature are not clearly established. The discussion of related work is limited. The connection between the approach and structure prediction methods is not elaborated upon. The generalizability and applicability to other domains and realworld networks are uncertain limiting the impact of the research.  Clarity: While the writing is generally clear the papers structure and content organization could be improved. More background information and algorithmic details would enhance comprehension. Questions:  Task Migration and Transfer Learning: How does the concept of \"task migration\" relate to \"transfer learning\"  LearningBased Methods: The authors refer to \"learningbased methods\" without fully defining them. Is the proposed framework not a learningbased approach itself"], "sj9l1JCrAk6": ["Summary Paraphrase: Federated learning clients often use local data that covers only a small portion of the full feature space. In models with large sparse embeddings this means clients only update a small part of the global model (a submodel). However popular features (like words in a vocabulary) can lead to unevenly updated embeddings. This \"heat dispersion\" slows down algorithms like FedAvg. The authors propose a new method where weights have different learning rates based on the number of clients participating in their update. This improves convergence speed. Strengths and Weaknesses: Strengths:  Addresses the problem of sparse feature dispersion in FL.  Includes theoretical analysis.  Compared to 4 baselines including those designed for noniid data.  Promising results. Weaknesses:  Some assumptions in the evaluation.  Uncertain compatibility with privacypreserving mechanisms. Questions:  The fair comparison of the proposed method to baselines should consider hyperparameter tuning for all methods including different learning rates.  Results should be extended to larger K (number of clients) to reflect realworld FL scenarios.  The authors should discuss the compatibility of their method with privacypreserving approaches like secure embedding retrieval and local DP noise."], "tz1PRT6lfLe": ["Paraphrased Summary: This paper introduces methods for privacypreserving stochastic gradient descent (DPSGD) which improves upon prior work by reducing communication complexity for various gradient descent methods. Strengths and Weaknesses: Strengths:  Enhanced communication complexity bounds  Rigorous mathematical analysis  Aims for generality Weaknesses:  Primarily theoretical lacking practical implementation  Straightforward methods with unclear novelty  Difficulty in assessing significance without experiments  Undefined terms and concepts limit clarity Originality Quality Clarity Significance:  Original analysis  Verified proofs with some clarity issues  Significance remains uncertain due to lack of practical evaluation Issues:  Unclear definitions of \"databases\" and \"entries\"  Ambiguous nature of \"mechanism\" and its effects on privacy  Concerns about the applicability of theoretical results without empirical validation Questions:  Clarification on undefined terms  Verification of the importance of compression in the analysis  Justification for the significance of theoretical results in the absence of practical experiments"], "kvtVrzQPvgb": ["Summary: This paper proposes a method for ranking systems based on their performance on multiple tasks. The authors claim that traditional averaging of task scores can be biased due to varying score ranges. Instead they suggest a ranking method based on summed pertask ranks (Bordas count) which has theoretical support. A modified \"twolevel\" algorithm is introduced for cases where instancelevel scores are available. Comparisons with mean score ranking show:  On synthetic data the twolevel approach is more resistant to noise.  On real NLP tasks both proposed methods are more robust to adding or removing tasks.  They generally align with baseline rankings while making finegrained adjustments that may change overall winners. Strengths:  Addresses a significant problem and offers a wellmotivated solution.  Provides extensive theoretical support and motivation.  Wellorganized and presented.  Evaluates the methods on a wide range of tasks. Weaknesses:  Limited substance with a considerable portion dedicated to the methods formal presentation.  Does not consider the potential loss of information when using ranks instead of scores.  Lacks discussion on confidence intervals and statistical significance in the results. Questions:  The papers claim that the method captures task difficulty is questioned.  The complexity argument against pairwise comparisons is disputed.  The intuitive appeal of using mean ranks instead of Borda ranks is raised.  A typo in Figure 2 is noted.  The \"sigma\" column in Table 3 is queried as it contradicts the expected ordering of Borda rank sums.  A missing reference is mentioned."], "wTp4KgVIJ5": ["Paraphrased Statement: Summary: The paper introduces a novel algorithm named SAGDA for federated learning in minmax learning. SAGDA provides a theoretical guarantee on communication complexity surpassing previous methods without sacrificing sample complexity. It employs control variates to minimize model drift among clients eliminating the need for bounded gradient dissimilarity assumptions. Experiments demonstrate the effectiveness of SAGDA. Additionally the paper proves that the standard FSGDA algorithm is a special case of SAGDA. Strengths and Weaknesses: SAGDA significantly contributes to the design of efficient federated learning algorithms addressing the growing communication costs. However the paper could benefit from clearer presentation particularly by focusing on one SAGDA variant and moving the second to the appendix. Questions: It remains uncertain whether the proposed techniques can be generalized to minimization learning beyond minmax learning. If not the paper should explore the reasons for this limitation."], "wA7vZS-mSxv": ["Paraphrase: This research presents a novel method IDNF for estimating intrinsic dimensionality using normalizing flows. It can handle large datasets and highdimensional data (e.g. 64x64x3). IDNF involves:  Defining a dataset D sample points x and noise levels \u03c3\u00b2.  Injecting noise into the data and using normalizing flows to approximate the resulting distribution.  Calculating eigenvalues of the inverse Jacobian of the normalizing flows for each data point and averaging them.  Fitting a model to the eigenvalues to estimate intrinsic dimension. Strengths:  Introduces a new method for dimensionality estimation based on normalizing flows.  Presents experimental results on synthetic and realworld data showing the effectiveness of the method. Weaknesses:  Assumes the data distribution is approximately Gaussian which may not hold for all datasets.  Training normalizing flows and computing eigenvalues can be computationally expensive especially for highdimensional data.  The averaging operation performed in Algorithm 4 needs further clarification. Minor Errors:  Line 188: \"greater\" should be \"greater\"  Line 159: The ratio \u03b2i\u03b1i should be larger for directions with small variability. Questions:  Table 1 may not be a fully accurate comparison as the dimensionality estimated by IDNF tends to be integervalued while LIDLs estimates may not."], "xT5rDp5VqKO": ["Summary: The authors propose using neural circuits to address pattern recognition problems. They cite evidence from animal brains and experiments (reference [10]) involving deep learning for classifying bacteria. Their proposed multinomial logistic regression \u043c\u043e\u0434\u0435\u043b\u044c inspired by coincidence detection theory showed higher accuracy in classifying bacterial isolates than the deep learning model used in [10]. Strengths and Weaknesses:  Strengths: The authors demonstrate the potential of coincidence detection for classification problems by achieving higher accuracy than a deep learning algorithm.  Weaknesses:  The results are based on a single trainvalidation split while the reference [10] used a statistical mean over multiple runs. For a fair comparison the authors should provide statistical results over random trainvalidation splits.  Details about the baseline deep learning model (ResNet26) are missing such as network architecture training time and training accuracy for a comprehensive comparison.  There is no explanation for why their model outperforms the deep learning model for this specific problem.  The contributions of the standardization and smoothing preprocessing steps are not analyzed.  Valuable figures are included in the supplementary material but not discussed in the main paper such as the baseline models higher accuracy in antibiotic treatment classification. Questions for Authors:  Can you provide additional statistical results over multiple trainvalidation splits for a fairer comparison  Can you share details about the baseline model (ResNet26) to enable a comprehensive comparison  Can you analyze the contributions of the standardization and smoothing preprocessing steps  Can you explain why your model fares better than the deep learning model for this specific problem"], "pk1C2qQ3nEQ": ["Paraphrased Summary: This paper introduces BalEntAcq a new acquisition function for active learning with Bayesian neural networks (BNNs). BalEntAcq balances uncertainty (entropy) and diversity (decision boundary proximity) in its estimation. It enables parallel computations across classes and batch selection. Unlike BatchBALD CoreSet and BADGE BalEntAcq operates independently eliminating the need for relational computations. Experimental results show that BalEntAcq outperforms other Bayesian active learning algorithms. Strengths and Weaknesses: Strengths:  Novel acquisition function  Facilitates parallelization over classes and batch selection  Captures both informativeness and diversity Weaknesses:  Limited baselines  Only tested on MC dropout BNNs  Limited dataset coverage Additional Questions: 1. Computational Complexity:  Discuss the computational complexity of BalEntAcq compared to other AL algorithms. 2. NonBayesian AL Algorithms:  Explain why nonBayesian AL algorithms cannot be directly applied to BNNs."], "xbJAITw9Z6t": ["Paraphrased Statement: Summary: The authors present an architecture and learning algorithm for unsupervised clustering of MNIST digits without using backpropagation. The model consists of \"energy layers\" that calculate responses as vector lengths in subspaces combined with adaptive thresholding to introduce competition among output features. These layers are trained sequentially to optimize KSubspaces clustering minimizing the distance of samples from their closest subspace. The authors provide a novel minibatched algorithm for training. After performing an architecture search the authors identify an architecture that achieves a 2 error rate on MNIST. Strengths and Weaknesses: While the construction of this system is impressive it is unclear what new insights it provides. The authors suggest it demonstrates the potential of architectureonly optimization or the effectiveness of the specific architecture for unsupervised learning tasks. However they do not provide clear conclusions or highlight the implications for future unsupervised learning research. Specific Points: 1. The authors claim that their approach is more biologically plausible than backpropagation but they do not discuss the plausibility of subspace clustering in relation to biological learning. 2. The finding that 4 layers perform worse than 3 layers on MNIST is unexpected. The authors do not provide an explanation for this observation. 3. While the authors argue that supervised learning of architecture is not necessarily incompatible with biological plausibility they acknowledge that it requires supervised data for metalearning. They do not discuss the implications of this for the interpretation of the results as truly unsupervised learning. Questions: 1. What are the broader conclusions for unsupervised learning research drawn from this work 2. What is the authors position on the biological plausibility of their approach 3. How does the interpretation of supervised metalearning on top of unsupervised learning results differ from unsupervised learning only"], "u5oLvX8x4wH": ["Paraphrased Statement: This research extends the analysis of mSGD and SGD by relaxing the previously considered step size conditions. This is significant because using an initially large step size has been shown to enhance convergence in practice. Hence a theory for convergence with large step sizes is valuable. Traditionally step sizes have been assumed such that \\sumt \\epsilont  \\infty and \\sumt \\epsilont2  \\infty. This work extends this to \\sumt \\epsilont2\\eta0  \\infty. Under these conditions assuming smoothness asymptotic convergence to a stationary point is established (Theorem 3.1). With additional growth assumptions Theorem 3.2 provides convergence rates as a function of the step size sequence demonstrating the influence of the momentum parameter \\alpha. However it does not explain the benefits of using larger step sizes. Theorem 3.3 addresses this by showing that larger step sizes can help the algorithm reach a superior point than the initial one more consistently. The proof techniques and assumptions are largely based on [7] but with modifications. Strengths and Weaknesses: Strengths:  Establishes convergence for common stochastic gradient methods under conditions not previously considered. Weaknesses:  Does not justify why larger step sizes are advantageous.  Theorem 3.2 does not indicate that step size sequences with \\sumt \\epsilont2  \\infty improve convergence rates.  Theorem 3.3 uses a nonstandard stopping time \\tau(a) which limits its usefulness as a comparison metric.  Arguments about step size effectiveness based on upper bounds may be flawed due to uncertainty about their sharpness. Questions: 1. How can we justify using larger step sizes based on the provided theoretical results 2. The authors should more explicitly compare the techniques used in [7] to those in this work especially considering the similarities in main techniques and assumptions. 3. The authors should clarify the discrepancy between their assertion about the assumption \\mathbbE\\\\nabla g(\\theta\\zeta)\\2 \\leq M \\\\nabla g(\\theta)\\2 being used in [7] and its absence in the cited paper."], "xijYyYFlRIf": ["Paraphrase: This study expands on NeRF generative models which previously focused on individual objects to allow for modeling 3D scenes that lack a consistent coordinate system. The study proposes separating camera pose and scene representation into distinct latent variables and decoders. These variables are then learned without an encoder and trained with a diffusionbased prior to facilitate sampling. Experiments demonstrate the models effectiveness in reconstruction interpolation and generative modeling for both unconditional and conditional cases. Strengths and Weaknesses Strengths: The paper addresses a significant issue in 3D scene modeling with NeRFbased generators which have struggled to deal with realworld scenes where the camera is not stationary. The authors propose a method that effectively separates camera pose and scene representation and introduces a diffusionbased prior for sampling. The model shows promising results in both reconstruction and generation tasks. Weaknesses:  Novel View Synthesis Performance: It is unclear how well the model performs in novel view synthesis where novel camera viewpoints are used to reconstruct scenes.  Test on Novel Objects: The model has not been tested on novel scenes that were not present during training leaving questions about its generalization ability.  Unconditional Generation: The evaluation of unconditional generation using FID scores does not fully capture the models performance at the trajectory level as it only assesses the quality of individual images.  Conditional Generation: Baselines are not provided for conditional generation making it difficult to compare the models performance to existing approaches.  Denoising Reconstruction Objective: The meaning of the term \"denoising reconstruction objective\" in line 133 is unclear. The authors should clarify what they mean by \"denoising\" and how it relates to the L2 and L1 reconstruction losses used in the study."], "siG_S8mUWxf": ["Paraphrased Summary The paper develops a new model for predicting physical dynamics. It introduces \"subequivariance\" similar to restricted representations in group theory. Additionally the model employs hierarchical message passing for extended particle interactions and enriches input data by calculating geometric features for particles and their surroundings. The model is evaluated on Physion and RigidFall datasets. Strengths and Weaknesses Strengths:  Adds a \"binary operation\" (position difference between adjacent features) to create a translationinvariant input representation.  Thorough experiments with multiple baselines. Weaknesses: Major:  \"Subequivariance\" has been previously explored as restricted representations of the E(3) group.  The paper criticizes GNNs for not explicitly considering object information but this appears to be a design choice rather than an inherent limitation.  The concept of distinct edges for different information exchange resembles automorphism equivariant GNNs which are not mentioned in the paper.  Experiments only consider rotations around the gravity axis including all rotations would strengthen the models validity. Minor:  Typos and grammatical errors hinder readability.  Object features are particle feature averages effectively subtracting the mean from particle features. Questions:  How does subequivariance differ from restricted representations  How does object information differ from specific feature choices  How does edge diversity differ from automorphism equivariant networks  Why are only rotations around the gravity axis used in experiments"], "mWaYC6CZf5": ["Paraphrase: This paper examines a problem known as \"representational collapse\" in Transformer layers that employ Mixture of Experts (MoE) models. For softmaxbased gating functions the paper shows that the gradients updating token representations have a component that is a linear combination of the expert vectors. Since the number of experts (N) is significantly lower than the token vector dimensionality this gradient update is \"lowdimensional\" ( N). The authors argue that this can cause representational collapse where token vectors receive updates that dont fully utilize their dimensionality resulting in poorly learned representations. To address this the paper proposes a simple solution: projecting the representations into a lower dimensional space before entering the MoE layer. This ensures that token vectors interact with MoE layers and their corresponding gradients through the lowdimensional mapping. Experiments demonstrate that this solution reduces representational collapse (Figure 2c) and leads to modest improvements on downstream tasks (Table 1). Strengths:  Identifies and addresses an important issue in MoE models.  Focuses on conditional computation an emerging area critical for scaling models.  Demonstrates improvement in crosslingual NLP tasks. Weaknesses:  The problem of representational collapse is only shown for softmax gating but not for sigmoid gating.  The experiments evaluate a model with only one MoE layer which may not fully showcase the benefits of the proposed approach.  Its unclear if the performance improvements are due to MoE layers or increased model parameters.  The UMAP plots do not clearly show the differences in representational collapse. Questions:  How do other techniques such as cosine similarity and temperature in the softmax contribute to preventing representational collapse  Why were the MoE parameters frozen during finetuning and are there alternative approaches to prevent overfitting"], "p_BVHgrvHD4": ["Paraphrased Summary: The paper explores a crucial issue in Machine Learning: establishing an information theory foundation for Deep Learning (DL) networks. Despite being primarily heuristic in nature DL can benefit significantly from information theorys insights into understanding the complexity of network structures. Specifically the paper investigates the possibility of considering multiple layers (depth variation) without an exponential dependence on depth and potentially infinite breadth with limited weight parameter sums. The paper provides a clear introduction and overview of current approaches in the field. Strengths and Weaknesses: Strengths:  Legitimate and valuable goal of understanding network complexity.  Clear motivation and objectives. Weaknesses:  Occasional sloppy notation and mathematical inaccuracies.  Key assumptions are impractical such as assuming independent identically distributed (IID) data which is not realistic in realworld applications. Questions: 1. How does the independence assumption in the IID property impact the results 2. For a known distribution and an associated energy functional (as in Gibbs distribution) could it guide the analysis to estimate a bound on the depth complexity"], "xatjGRWLRO": ["Paraphrased Statement This study introduces an unsupervised technique for enhancing point cloud representation learning. The technique can be used with existing models for various downstream tasks. Strengths  The techniques core concept realized through two losses (PointInfoNCE and ClusterNCE) utilizes patchbased learning to harness repetitive structures in largescale point clouds.  Evaluations on diverse datasets including outdoor environments demonstrate the techniques effectiveness.  Pretrained models based on the technique may provide value to the research community. Weaknesses  Concerns exist about the approachs principles:  Sparse outdoor point clouds may yield local crops containing insufficient foreground information.  The techniques reliance on local crop sizes raises questions about handling scale variations during testing.  The paper may be difficult to comprehend for readers unfamiliar with unsupervised learning particularly Section 3.3.  The baseline methods computational cost is not explicitly described making it unclear how the technique improves efficiency.  The comparison with [67] is not fully transparent with discrepancies in reproduced results not adequately explained. Questions  How is perpoint feature extraction performed in PointNet given its downsampling nature  Should the reference in L151 be [3] instead"], "mjVmifxpKqS": ["Paraphrased Statement: This paper introduces a \"batched\" reinforcement learning (RL) framework where interactions are divided into multiple \"epochs\" (batches). The RL agent prespecifies the batch lengths before starting interactions and can only adjust its policy at the start of a new batch. This setting mimics realworld scenarios where frequent policy updates are impractical. Inspired by the multiarmed bandit (MAB) problem the paper demonstrates that any minimax algorithm requires \\Omega(\\log\\log K) policy batches (where K is the total number of episodes). To counter this the authors propose a policy elimination algorithm that achieves a tight upper bound on regret with M  H  \\log\\log K batches. Strengths and Weaknesses:  The framework and results are noteworthy.  The algorithmic design is novel and innovative.  The resulting algorithm is complex with various subroutines and computational complexities making it difficult to understand.  The paper lacks clarity and may be hard to follow.  The initial exploration stage requires prior knowledge of the horizon limiting its practicality as an anytime algorithm. Questions:  Why are there two exploration stages  Why are separate confidence bounds used for each next state rather than standard L1 bounds Is it because of the tightness requirement from the transition region Could this be modified to allow standard intervals"], "lme1MKnSMb": ["Summary Paraphrase: This research employs transformers to determine the relevance between latent representations of consecutive video frames. By leveraging latent representations from two preceding frames as inputs to an entropy model it enhances probability estimation for the current frame. Furthermore an optimized autoregressive model is introduced requiring 16 steps to decode a frame. Strengths and Weaknesses: Strengths:  Explores an entropy coding framework rather than relying on residual coding which typically requires motion estimation and compensation.  Analyzes various synthetic video types including shifting blurringsharpening and fading. Weaknesses:  Novelty is limited due to previous research in similar areas:  Entropy estimation using past frame representations has been explored.  Transformer utilization in neural image codecs is wellestablished.  Blockbased autoregressive models have been studied.  Despite claims of simplifications the proposed framework still involves complex and handcrafted designs.  Compression improvements over previous methods are minimal.  Comparisons against industryleading encoders (e.g. JM HM VTM) are recommended.  Training and testing time measurements should be provided for more accessible platforms.  An explanation is needed for the specific choice of two previous frames and 16 block partitions."], "rg_yN3HpCp": ["Paraphrase: Summary: This paper introduces a new augmentation technique for graphstructured data that maintains labels. Unlike traditional methods that modify nodesedges or extract subgraphs this technique operates in the representation space generating augmented samples in the direction most challenging to distinguish while preserving the original samples label. Experiments show the proposed techniques effectiveness. Strengths:  Novel and sensible concept.  Clear and concise writing.  Convincing empirical evidence. Weaknesses:  Lack of theoretical justification for the methods correctness.  Inadequate explanations for certain model details.  Potential technical issues. Questions: 1. Can the proposed technique extend to node classification on a single graph as existing graph contrastive learning methods do 2. Clarify the relationship between the proposed labelinvariant augmentation and supervised contrastive learning which also utilizes labels but not for downstream classification tasks. 3. Consider the label rate of 50 in Fig. 1 which may be too high for semisupervised learning. Lower label rates could demonstrate the advantages of certain augmentation strategies. 4. Explain how Eq. 4 represents the most challenging direction and how the label of augmented data remains unchanged. 5. Address concerns about using high label rates (30 50 70) in the experiments which may not align with the definition of semisupervised learning (limited labeled data)."], "mUeMOdJ2IJp": ["Paraphrased Statement: Consider a set of points obtained by drawing randomly from a mixture of subGaussian distributions with unknown means (\u03bc\u2081 ... \u03bcn). Assume that all the points share a common unknown mean (\u03bc) and a known subGaussian constant (\u03b7). The goal is to estimate the subspace containing the means of the points given their coordinates. The researchers propose a spectral estimator and establish conditions under which it can accurately recover the subspace as measured by the maximum angle between the estimated and true subspaces. They also prove a matching lower bound in some cases. In addition to the subspace estimation problem the authors also discuss estimating the subspace from noisy linear measurements. The paper uses standard techniques for analyzing spectral estimators and provides a brief overview of its application for the lower bound. Strengths and Weaknesses: The papers focus on subspace recovery under complex assumptions can be overwhelming and a clearer explanation of these assumptions would enhance comprehension. The arguments in the proofs could benefit from a more organized presentation with milestones highlighted throughout to guide the readers understanding. Questions:  Can you generalize the discussion about subspace recovery to a general subspace not just the special case of uuT  Provide a proof or reference for the statement that the \"optimal estimator\" minimizes the sum of squared errors (Line 75).  Explain how the assumption of known \u03b7 values is made in practice especially in a federated learning setting.  Provide a clearer understanding of the conditions under which the upper and lower bounds match.  Define \"the subspace is incoherent\" in the context of the paper.  Confirm that the \"equivalence\" claim on Line 238 is correct.  Provide references for the sections (Lines 256272 and Appendix A) that build upon existing literature."], "nC8VC8gVGPo": ["Paraphrased Statement: This research introduces Local Tandem Learning (LTL) an advanced learning technique that enables a Spiking Neural Network (SNN) to imitate the feature representations of a pretrained Artificial Neural Network (ANN). LTL accomplishes this by leveraging layerspecific local loss functions. The SNN learns the intermediate features of the ANN through these loss functions resulting not only in high accuracy but also rapid convergence. LTL operates in both offline and online modes delivering remarkable performance in both. Comprehensive experiments on CIFAR10 CIFAR100 and ImageNet datasets confirm the effectiveness of LTL. Additionally its compatibility with hardware platforms enables efficient onchip implementation and resilience to hardwarerelated noise. Strengths:  LTL integrates the benefits of ANNtoSNN conversion and gradientbased training guiding the SNN to learn the essential features of the pretrained ANN.  Extensive experiments demonstrate that LTL accelerates training convergence achieving rapid results within five epochs for CIFAR10.  SNNs trained with LTL attain accuracy levels comparable to their teacher ANNs and surpass existing stateoftheart learning algorithms. Weaknesses:  The paper lacks an introduction to the TeacherStudent (TS) learning concept which would provide context for LTL.  Additional experiments with architectures like ResNet could showcase the wider applicability of LTL and highlight its versatility. Questions:  To assess the methods effectiveness in capturing ANN knowledge a comparison of LTL with a setup where only surrogate gradients are used for SNN training could be included.  Extending the experiments to diverse network architectures such as ResNet would further validate the performance of LTL and demonstrate its adaptability to different network structures."], "x4JZ3xX5mtv": ["Paraphrase: Summary: This paper introduces a novel nonautoregressive approach to generating views from a single image. It combines explicit and implicit rendering methods to create explicit and implicit feature maps. A transformation similarity loss ensures the alignment of these feature maps within the visible and outofview regions. Strengths and Weaknesses: Originality: The combination of explicit and implicit methods is a unique and innovative approach. The nonautoregressive nature significantly reduces processing time. Quality: The method is welldescribed overall. However some concerns arise:  Reprojected pixel preservation is best evaluated for the reprojected regions only. SynSin shows competitive PSNR and better projected content preservation. The papers explanation for this requires further clarification.  The ablation study on set attention could be included in the main paper as it provides a key contribution. Clarity: The paper is wellwritten and the motivation is clear. However improvements can be made:  The definition of explicit methods needs clarification (e.g. Tatarchenkos method).  Qualitative results showcasing the range of viewpoint changes would enhance the understanding of the method. Significance: The idea of dual feature map rendering has potential for application in other frameworks. Questions:  Artifacts in the second row of Figure 4 may be related to upsampling in the decoder."], "pfI7u0eJAIr": ["Paraphrased Statement: Summary:  The paper focuses on embedding numerical features for deep learning in tabular data.  It proposes two basic approaches: Piecewise Linear Encoding (PLE) and periodic activation functions.  Experiments show that these numerical feature embeddings can improve model performance including MLPs and Transformers to match or surpass Gradient Boosted Decision Trees (GBDTs). Strengths and Weaknesses: Strengths:  Comprehensive experiments with various embedding methods and model combinations.  Addresses the lack of established baselines in tabular deep learning. Weaknesses:  Does not consider embedding methods used in other domains specifically natural language processing.  Lack of clear preferred method and analysis of results.  Insufficient explanation of dataset selection criteria for evaluation. Questions:  Which numerical feature embedding method is preferred and under what circumstances  Why do different methods perform differently on different datasets or is it random variation  How were the datasets selected for evaluation and what does \"GBDTfriendly dataset\" mean  Could the authors provide more information on related methods from the NLP literature  Please ensure consistency in providing full definitions of abbreviations (e.g. GBDT CTR DL).  Avoid subjective language in the paper."], "u6OfmaGIya1": ["Summary This study aims to control large language models (LMs) to generate ethically sound text free from harmful or offensive content. Unlike existing methods that restrict LM decoding algorithms or create prompts the authors propose an iterative text editing process. They insert delete or replace text segments to correct unethical statements. Multiple edited sequences are generated both positive and negative and the LM is finetuned using these examples. Reinforcement learning is employed to encourage contextually relevant and ethically aligned text generation. Human evaluations confirm the effectiveness of the method in generating higherquality text compared to previous models. Strengths 1. The proposed editing approach is novel and interpretable allowing for multiple iterations to achieve desired text quality. 2. Extensive ablation studies and analyses demonstrate the performance gains and the methods contributions relative to prior work. Weaknesses 1. The method requires multiple rounds of LM finetuning which may pose challenges for larger LMs or largescale applications. 2. Section 3.2 describing the finetuning process lacks detailed information making it difficult to fully understand the method. 3. The human evaluation questions used to assess text quality are too general and lack specific definitions of \"human values\" and \"coherence.\" Questions 1. Can you provide more details about the finetuning process in Section 3.2 including inputoutput format steps and objective 2. Do you believe the approach could be scaled to larger LMs Are there alternative methods to avoid full LM finetuning such as using adapter layers for parameter efficiency"], "zzDrPqn57DL": ["Paraphrased Statement: Summary: This paper proposes BEVFusion a straightforward and efficient framework for fusing LiDAR and camera data. BEVFusion separates the camera processing from the LiDAR network and employs a dynamic fusion module. This approach ensures strong performance and resilience to hardware failures involving either LiDAR or camera. The paper also suggests a practical enhancement to the camera pipeline further improving overall performance. Strengths:  Clear and accessible writing  Importance of addressing robustness in autonomous driving  Extensive testing and validation of claims including achieving stateoftheart results in both standard and robust settings  Flexible framework allows integration with various camera or LiDAR architectures Weaknesses:  While the dynamic fusion module is valuable further insights into its design and analysis would be beneficial exploring how it handles incomplete data  Lack of runtime analysis in the experiments comparing performance with other methods Questions: 1. What method is used by the baseline method in Table 7 to combine features 2. Curiosity question: Would BEVFusion remain effective in the event of simultaneous failure of both the camera and LiDAR sensors"], "wph_3smhuec": ["Summary Paraphrase: Bilevel optimization models become ambiguous when the subproblem has multiple solutions. Traditional approaches handle this by considering optimistic or pessimistic versions of the problem. This paper introduces a selection map approach to resolve this ambiguity. Under specific conditions the selection map is differentiable making the model computationally feasible. Strengths and Weaknesses Paraphrase: Strengths:  Strong technical rigor  Expected results under specified conditions Weaknesses:  Unclear notation especially for nonexperts  Questionable relevance of the selection map model due to:  Dependence on the specific selection map used  Potential equivalence to optimistic bilevel optimization with a different objective function Questions:  How should the BGS problem notation be interpreted  Does the selection maps y variable represent an initial condition or state variable"], "yts7fLpWY9G": ["Paraphrased Statement: Summary: This study investigates the potential benefits of using adaptive readouts in graph neural networks (GNNs). While GNNs typically use nonadaptive readouts to maintain permutation invariance this paper demonstrates that adaptive readout functions can enhance performance. Strengths:  Thorough empirical analysis using adaptive readouts across diverse datasets from various domains.  Significant performance improvements compared to standard readout functions.  Clear explanations and informative discussions from an applied machine learning perspective. Weaknesses:  Lack of discussion on the impact of adaptive readouts on GNN transferability.  Focus on performance improvement without considering interpretability which may be crucial in certain applications. Questions: 1. Explain the Matthew correlation coefficient to enhance readability. 2. Provide details on the bioaffinity prediction task including the meaning of SMILES representation. 3. Discuss the relevance of neighborhood aggregations in GNNs with adaptive DENSE readout layers considering the potential dominance of the DENSE layer. 4. Include discussions on transferability of GNNs with adaptive readouts."], "zSdz5scsnzU": ["Paraphrased Paper Summary This paper presents a new method for robotic planning in a latent space where visual observations are used to create the representation. The planning algorithm employs a samplingbased approach similar to that used in robotics motion planning involving the sampling of new configurations (in the latent space) assessment of feasibility and reevaluation of connectivity. The method is implemented in various simulation tasks including nonrigid object manipulation which is challenging for traditional samplingbased planners. Strengths and Weaknesses of the Method Strengths:  Novelty: The method extends samplingbased planning a wellestablished technique in robotics to domains where it was previously inapplicable.  Efficiency: The method incorporates learningbased techniques to reduce the need for explicit information.  Clarity: The paper is wellwritten and accessible. Weaknesses:  Limited References: The paper lacks a comprehensive review of prior work in the area.  Lack of Theoretical Guarantees: The method lacks theoretical guarantees on its performance such as completeness.  Lack of Comparison: The best solutions in Table 1 are not highlighted. Questions  Image Sequence: How are the input images sequenced and represented in the initial and final states  Function f: What is the function f used in line 197 and what does it represent  Neighbor Search Metric: What distance metric is used for neighbor search in the Rewiring phase  Latent to Euclidean Distance: Is there an assumption that the distance from a tree node in latent space to the goal can be computed in Euclidean space  MPC Limitation: Why is the method restricted to an MPC setup instead of evaluating the execution of a complete path to the goal"], "qZUHvvtbzy": ["Paraphrased Statement: The paper combines a shallow neural quantum network based on Restricted Boltzmann Machines (RBMs) with the Lanczos method to improve its accuracy. Additionally it leverages symmetries through projections to enhance performance. The results showcase highly competitive accuracy against current benchmarks (J1J2 model in 2D). Strengths:  Firsttime application of Lanczosstyle improvements to neural quantum states.  Significant improvements over \"pure\" neural quantum states. Weaknesses:  Lanczos iterations lack \"size extensivity\" scaling limiting effectiveness for large systems.  Lack of comparison with the current bestknown result on the J1J2 model by Nomura and Imada. Questions: 1. How does the Lanczosstep approach compare to the bestknown result on the 10x10 J1J2 model as reported by Nomura and Imada 2. What would be the impact of imposing translation symmetries in the network weights instead of using explicit group summations"], "qw3MZb1Juo": ["Paraphrase: This study introduces a technique to address catastrophic forgetting in federated learning. The technique aims to enhance the local models performance by incorporating knowledge from the global model using an enhanced knowledge distillation method. Experimental findings demonstrate the techniques effectiveness. Strengths:  Comprehensive experimental setup considering various federal settings.  Straightforward experimental approach with favorable outcomes.  Thorough method analysis. Weaknesses:  Limited comparisons. A related study by Mendieta et al. (2022) employs a similar knowledge distillation approach and achieves comparable results. Questions: 1. The MOON method exhibits inferior performance in the table compared to the original study. The method is sensitive to hyperparameters and it is recommended to include additional comparisons with various hyperparameter settings in the appendix. 2. The paper does not explicitly explain how discarding \"nottrue\" information positively affects the methods performance. Could you provide a simplified description"], "yhZLEvmyHYQ": ["Paraphrase: Summary: This work proposes using fully Bayesian Gaussian Processes (GPs) in Bayesian optimization and active learning. They create Bayesian versions of acquisition functions that incorporate information from the hyperparameter posterior and suggest using the variance of the posterior predictive distribution of the GP to derive an acquisition function. These Bayesian variants are claimed to be more effective because they better approximate the true predictive uncertainty. They evaluate the performance of their methods on six simulator tasks. Strengths:  Interprets the multimodal marginal likelihood surface in terms of the biasvariance tradeoff.  Presents the work clearly and succinctly.  Provides a decent experimental evaluation.  Focuses on approximating the hyperparameter posterior an understudied area. Weaknesses:  The computational cost of running fully Bayesian GPs at each iteration is a major concern. A naive approach has a scaling of O(MN3) where M is the number of samples.  The underperformance of BQBC on heteroscedastic data is not fully explained.  BQBC and QBMGP use hyperparameter samples from the posterior so their mean functions should be the same. However their performance is different. Questions: 1. Isnt the most likely mode the one with the highest marginal likelihood 2. Why does BQBC underperform on heteroscedastic data and why is QBMGP the best performer despite having similar criteria to BQBC 3. How can the difference in performance between BQBC and QBMGP be explained given that they both use hyperparameter samples from the posterior"], "wuunqp9KVw": ["Paraphrase: Summary: This paper proposes a theoretical framework for generating diverse plausible images from reconstruction processes in the presence of occlusions. The approach employs a mixture of Gaussian experts to capture diversity demonstrating its effectiveness in experimental evaluations. Strengths:  Develops a comprehensive theoretical framework for pluralistic image completion.  Utilizes a Gaussian mixture model (GMM) to address diversity.  Generalizes previous work (e.g. [52]) that considered only unimodal Gaussian distributions with predefined parameters. Weaknesses:  The paper lacks clarity in explaining the details of the 1step Monte Carlo (MC) sampling process.  It is unclear how the Expectation Maximization (EM) algorithm is used to determine the mixture components of the GMM.  The approach relies on predefined loss functions for each component of the optimization problem but the impact of the first term in the loss function for image alignment (LA) is not well explained.  The paragraph on lines 210215 contains conflicting information about the generation of images.  The number of mixture components (k) is not automatically determined within the framework and the issue of model selection is not addressed.  It would be beneficial to include a visual representation of the proposed architecture for improved understanding. Minor Comment:  The acronym \"PGM\" (line 69) should be introduced earlier in the paper."], "znbTxnBPlx": ["Summary: Stabilized supralinear networks (SSNs) model cortical neural circuits. Previous training methods for SSNs have faced stability issues. This work proposes a novel training method using gradientbased techniques with minimal constraints enabling SSNs to perform probabilistic inference under a Gaussian scale mixture (GSM) model. The method gradually builds the SSN during optimization starting with a small stable network and iteratively adding neurons. Results: The new method significantly reduces training cost compared to existing methods. It effectively infers from a GSM model optimized for CIFAR10 images. Strengths:  The gradual network building approach is unique for SSNs.  The use of gradient optimization is original.  The method outperforms previous approaches. Weaknesses:  The paper lacks a comprehensive discussion of related work and evolutionary approaches.  Clarity could be improved in the explanation of experiments and evaluation methods. Questions:  What details about the previous training method and experiment setup are missing  Why was the previous method not evaluated on CIFAR10  How can the method be generalized to different tasks Minor Points:  Correct typos in lines 9596 and 123.  Clarify the distinction between coding and noncoding neurons.  Use a different letter for the variable in equation 10 to avoid confusion.  Add parentheses in equation 16."], "wt7cd9m2cz2": ["Paraphrase: This research investigates how to learn a specific type of linear regression model using a unique twolayer neural network structure. This network differs from standard ones in that its bias terms are randomly initialized and fixed and the weights are shared across all neurons. Key Results: 1. Population Landscape Theorem: The first critical points occur either along the true linear relationship or perpendicular to it. 2. Empirical Loss Convergence: The networks gradient flow (with certain initial adjustments) leads to a solution with desired accuracy under a sufficient number of samples. Strengths:  The landscape theorem clearly identifies the critical points.  The gradient flow approach demonstrates optimal accuracy under manageable data requirements.  The analysis combines existing ideas in a novel way. Weaknesses:  The neural network structure is simplified and key details (e.g. tied weights) are not explicitly mentioned in the title and abstract.  The necessity of initial adjustments for practical training is not addressed.  Some questions need clarification:  Can Theorem 5.1s probability bound be improved  Are the initial adjustments (i.e. warmup and finetuning) in Theorem 5.2 necessary in practice  Why is finetuning required for optimal accuracy Typos:  Line 119: \"\\varphibi\" instead of \"\\varphibj\"  Line 133: \"first study\" instead of \"first the study\""], "tYAS1Rpys5": ["Paraphrased Summary: This paper proposes methods for solving complex optimization problems (NPhard) efficiently and accurately using machine learning particularly beam search. Recent approaches use neural networks to assess candidate solutions but they can suffer from inaccurate predictions. The authors introduce SimulationGuided Beam Search (SGBS) which combines neural beam search with simulated rollouts. SGBS generates candidate solutions using a neural network and simulates their outcomes to select the best candidates for further search. To enhance performance SGBS is combined with the Efficient Active Search (EAS) method (SGBSEAS). SGBSEAS leverages SGBS to prevent local optima while EAS improves the neural network model over time. Both SGBS and SGBSEAS outperform traditional techniques and stateoftheart ML solvers in both solution quality and efficiency. Strengths and Weaknesses: Strengths:  Clear and wellwritten explanations  Novel combination of neural beam search and simulated rollouts  Easy implementation  Strong performance on benchmark problems Weaknesses:  Lacks comparison to graph neural network methods that do not require rollouts  Some unclear aspects in the reward function formulation and action sampling methodology Questions:  How do SGBS and SGBSEAS compare to graph neural network techniques that do not use rollouts  Could the MDP formulation and reward function be further clarified  What is the action selection policy used in SGBS rollouts"], "tadPkBL2gHa": ["Summary Paraphrase: This study introduces methods for determining the optimal set of job offers to extend during recruitment. The goal is to recruit the best possible candidates while adhering to a soft limit on the number of hires (acknowledging uncertainty about candidate acceptance). When considering a specific type of soft constraint the authors propose an efficient strategy that can closely approximate the optimal solution. For another type of constraint they provide an efficient strategy with a consistent approximation factor. Strengths and Weaknesses Paraphrase: The authors present a wellmotivated problem. They highlight the practical advantages of their softconstrained multibatch approach compared to previous research that focused on singlebatch hardconstrained settings. However their justification relies heavily on anecdotal evidence which could be strengthened with additional references. The authors fully explore the problem proposing strategies for both twosided and onesided penalty functions. However they do not discuss the factors that would determine the preferable strategy. The experiments appear to be the least robust part of the paper. They show that a greedy strategy performs comparably to a practical implementation of the proposed constantfactor approximation algorithm. The authors raise questions about the sensitivity of the latter strategy to various parameters but these questions are not addressed empirically. They also suggest that a comparison of the proposed L2 strategy with the greedy strategies could be valuable."], "qSYVigfakqS": ["Summary: This study presents a method for weakshot semantic segmentation using MaskFormer that includes both proposaltopixel and pixeltopixel similarity transfer. The method was tested on COCOStuff and ADE20K datasets. Strengths:  Favorable results and thorough evaluations.  The concept of similarity transfer for segmentation appears promising. Weaknesses:  Presentation:  Unclear contribution statement (e.g. \"proposalpixel similarity transfer\" is part of the proposed framework).  Method section is difficult to comprehend due to mixing implementation details with concept explanations.  Relationship between the proposed framework and \"similarity transfer\" is unclear.  Figure 2 symbols are missing references in the text description.  Natural language descriptions dominate Section 3.4 a mathematical description would be beneficial.  Method:  Potential information leakage in novel class masks due to use of \"ignore labels\" in PixelPixel Similarity and Complementary Loss.  Concerns about Complementary Loss effectiveness when novel and base classes do not cooccur in images.  Presentation:  Poor figure quality (lightcolored text in Figure 1(b) blurring upon zooming). Questions:  See questions raised in the \"Weaknesses\" section.  How does the proposed methods performance compare to other weakshot semantic segmentation approaches  Are there any alternative designs or optimization strategies that could improve the methods effectiveness"], "uNYqDfPEDD8": ["Paraphrased Statement: Summary: This research presents a pipeline based on neural networks for chip design involving macro and standard cell placement and routing. For placement a Reinforcement Learning (RL) model is proposed for mixedsize macro placement to reduce collisions. For routing a Conditional Generative Routing Network with adaptive routing order is introduced to enhance previous fixed order methods. Experimental evaluations demonstrate reduced overlap area wire length and routing congestion compared to existing approaches. Strengths:  The chip design problem is significant and complex.  The proposed approach is technically sound and outperforms prior methods. Weaknesses:  The primary innovation lies in the routing module while the placement modules extension is less remarkable. It is unclear how this minor adjustment leads to improved performance.  Evaluations are mostly presented separately for the placement and routing modules obscuring the overall joint design performance.  Error bars are missing for tables 1 3 and 4. Questions: 1. How does incorporating macro size into the state significantly reduce overlapping areas 2. Does the detailed state information in the placer increase training costs 3. Given the combinatorial nature of the problem what is the proposed approachs scalability 4. What is the estimated maximum number of circuit components that can be handled within a reasonable timeframe"], "lxdWr1jN8-h": ["Paraphrase: Summary: The paper suggests using steerable convolutions in the value iteration networks (VINs) framework to make the networks invariant to rotations and reflections. This approach improves VINs but has some limitations. Strengths:  The proposed method significantly enhances the performance of VINs.  Invariance to rotations and reflections is a valuable property. Weaknesses:  The paper is difficult to understand for researchers who are not familiar with steerable CNNs.  The performance gains over ConvGPPN (a similar method) are modest. Questions:  Can the method be extended to rotations other than 90 degrees  Would the method improve performance over data augmentation techniques that generate rotated and reflected versions of the environments"], "mhQLcMjWw75": ["Paraphrased Statement: This research introduces Federated Prototypewise Contrastive Learning (FedPCL) a novel approach that leverages pretrained models in Federated Learning (FL) to minimize communication costs. Contributions:  Pioneer the integration of pretrained models into FL for reduced communication overheads.  Propose FedPCL a new algorithm for this purpose.  Demonstrate FedPCLs superiority to existing FL methods through experiments. Strengths:  Clear and concise writing style.  Wellargued motivation for reducing communication costs in FL.  Derivation of a generalization bound for FedPCL. Weaknesses:  Increased computational demands on clients which may be problematic for resourceconstrained devices.  Potential privacy concerns as the server may infer client data from transmitted information.  Omission of citations for significant PFL papers.  Contested experimental results where local training outperforms some PFL methods.  Lack of hyperparameter tuning information and experimental details such as data splits.  Limited number of clients in experiments which may not accurately reflect realworld FL scenarios."], "mamv07NQWk": ["Paraphrased Summary: This study investigates multilabel classification (MLC) problems particularly in sparse label settings where the number of active labels is limited. The paper establishes upper and lower regret bounds for two common MLC performance measures Hamming Loss and Precisionk in both nonparametric (KNN) and empirical risk minimization (ERM) settings. The analysis considers an MLC variant of Tsybakovs lownoise condition and demonstrates that both sparse label regimes and lownoise conditions enhance regret limits. These favorable conditions align with practical requirements and contribute to the significance of the findings. Strengths and Weaknesses: Strengths:  Theoretical rigor and originality  Comprehensive background and literature review  Wellpresented and accessible insights Weaknesses:  The potential for extending the analysis to other performance measures like rank loss and micromacro Fmeasures is unclear. Additional Details: The paper highlights the impact of sparse label regimes and lownoise conditions on regret bounds for MLC problems. The authors acknowledge the limitations of their analysis for more complex performance measures and welcome further research in this area."], "vsShetzoRG9": ["Paraphrased Statement: The authors present INSNET a novel insertionbased model that leverages sequential and parallel decoding strategies to address the suboptimal training efficiency of basic insertionbased models. Evaluations on two unsupervised lexically constrained text generation datasets and three machine translation datasets demonstrate INSNETs superior performance on all tasks. Moreover the proposed method significantly reduces training costs compared to previous approaches (as presented in Table 4)."], "nVV6S2sb_UL": ["Paraphrased Summary: The paper focuses on privacy concerns in federated learning that involves multiple training rounds. It observes that current secure aggregation protocols only provide guarantees for a single round while federated learning models require iterative training. To address this issue the paper proposes a method to measure and mitigate privacy leakage over multiple iterations. The mitigation approach involves forming fixed groups of participants for model updates. This makes it harder for an adversary to identify the contribution of any individual participant as their contribution is consistently mixed with that of the group. The method also ensures fairness by selecting each participant for updates an equal number of times. The paper analyzes the convergence properties of the proposed approach and tests it empirically using various network structures and datasets. Strengths and Weaknesses: Strengths:  Introduces the concept of secure aggregation guarantees over multiple iterations.  Provides theoretical results to support the proposed methods convergence. Weaknesses:  Omits discussion of differential privacy (DP) a widely used privacy definition in machine learning.  Does not define the consequences of providing privacy in the context of the papers approach. Additional Questions:  Why do the experiments stop training prematurely  The proposed method assumes honesty within fixed groups. What is the threat model  Remark 7 should be formally stated as a theorem."], "nRcyGtY2kBC": ["Paraphrased Statement: This paper introduces the Heterogeneous Transfer Causal Effect (HTCE) framework to enhance treatment effect estimation accuracy in heterogeneous transfer learning settings. Originality: While the problem addressed has been acknowledged the HTCE framework is a unique combination of layer sharing and FlexTENet. Quality: The framework effectively leverages metalearners and TARNet demonstrating the authors expertise. Clarity: The paper is wellwritten particularly Section 4. Significance: While the problem addressed is important the experiments do not fully demonstrate the significance of the proposed method in addressing it. Questions: 1. The definition of transfer learning typically involves label access unlike [32]. 2. Replace \"\" with \"\" in lines 230231. 3. Figures 3.a and 3.b contain minor inconsistencies with the text. 4. The experiments lack evidence of the methods effectiveness in addressing significant differences between source and target datasets especially with small target sample sizes. 5. The appendix contains typos including \"size size\" and uncertainty about the value of \"twins\" (11400 or 114000)."], "uCXNOeL0TG": ["Paraphrased Statement: This paper examines constrained resource allocation problems as restless bandit dilemmas. Unlike earlier studies which treated resources as interchangeable this work:  Recognizes the uniqueness of workers (in terms of costs budgets and effectiveness).  Aims to optimize resource allocation while limiting the difference in costs between any two workers to a threshold \\epsilon. To achieve this the authors:  Define an augmented action space with each worker representing a possible action.  Compute Whittle indices for each workerarm combination.  Adjust these indices to determine the minimum charge required for workers to consider alternative actions.  Use the adjusted indices for fair and budgetconstrained roundrobin allocation of resources. Strengths:  Wellwritten and addresses a practical need in resource allocation.  Extends the restless multiarmed bandit framework to accommodate heterogeneity in workers. Weaknesses:  Assumes observable arm states and workerspecific transition matrices. This assumption should be justified with regard to the availability and \"learnability\" of such information in relevant domains. Questions: 1. How is fair allocation measured and implemented in the empirical results 2. Can the knapsack portion of the Hawkins approach be adapted to enforce the type of costdisparity threshold fairness considered here"], "vdh62914QR": ["Paraphrase: This paper introduces generalization bounds for the ZoSS (zerothorder stochastic subgradient) algorithm based on the study of its algorithmic stability. These bounds align with prior work on SGD (stochastic gradient descent) by HRS16. The key technical element is a recursion lemma that monitors the growth rate of the expected distance between two ZoSS instances that differ only by one example. This result is then applied to various scenarios. Strengths and Weaknesses:  The approach resembles the HRS16 analysis of SGD resulting in unsurprising bounds. However it is notable that comparable generalization bounds are possible for zerothorder optimization (although it also inherits the same limitations as HRS16 such as linear dependence on T).  The recursion lemma is intriguing.  The writing is clear and accessible. Suggestions:  The title could be more specific to prevent confusion such as \"Generalization Bounds for ZerothOrder Optimization via Algorithmic Stability.\"  The analysis is limited to ZoSS algorithms and may not apply to all zerothorder algorithms. Conclusion: This paper provides valuable and meaningful results by presenting the first generalization analysis for zerothorder optimization. However it could benefit from deeper technical contributions and the papers importance may be overstated. Questions:  What is \"highprobability analysis\" (line 43) Is it a typo  Is it possible to obtain similar outcomes for onepoint zerothorder algorithms such as the \"gradient descent without a gradient\" method by FKM04"], "wJwHTgIoE0P": ["Paraphrased Statement: Summary: This research introduces a synthetic dataset for training machine learning models to identify patterns and relationships without using real images. The dataset called \"OpenGL 21K\" consists of 21000 computer programs that generate various images. The goal is to reduce concerns about using sensitive data such as human faces in training models. Experiments demonstrate that training on this dataset improves model performance compared to previous synthetic datasets and the study of what makes an effective generative image program is intriguing. Strengths and Weaknesses:  Strengths:  Important practical applications for learning representations without real images  Valuable insights for researchers developing image generation programs for representation learning  Weaknesses:  Limited evaluation of learned representations to linear readout which may not fully represent their capabilities in more realistic scenarios  Lack of comparison with alternative methods that also aim to learn representations without ethical concerns such as using a single image or filtered real images Questions:  How can the visualization of learned representations and perlayer linear readout performance provide further insights  Should the data augmentation methods used for synthetic data differ from those used for real data considering the different characteristics of the two data types  What are the connections between using synthetic images and alternative methods for reducing privacy concerns in real data such as PASS  How well do representations learned on the proposed dataset transfer to different datasets when the model is finetuned"], "lHj-q9BSRjF": ["Paraphrased Summary: In light of recent advancements in incontext learning this study explores the underlying reasons for this learning phenomenon in the context of imagebased fewshot learning. Our findings indicate that incontext learning arises when training data exhibits a bursty distribution. Furthermore when data follows a skewed Zipfian distribution both incontext learning and inweight learning can coexist within a single model. Strengths:  Novel investigation of the interplay between incontext learning and inweight learning.  Vital examination of the causes behind incontext learning.  Intriguing experimental observations. Weaknesses:  Formatting issues (page limit exceeded).  Lack of precise definitions for key concepts (burstiness inweight learning incontext learning). Questions:  The definition of burstiness should be more rigorous than simply a binary feature.  Quantitative definitions are needed for \"inweight learning\" and \"incontext learning.\"  It is unclear whether nonbursty sequences cannot be solved by either learning method.  How does incontext learning integrate with fewshot learning  Is burstiness partially analogous to a high number of sparsely occurring classes"], "lhLEGeBC-ru": ["Paraphrased Statement: Background and Problem: SDPs are common optimization problems with large input sizes. To address this Burer and Monteiro proposed the BM proxy problem which uses less memory than SDP. However BM is nonconvex making it difficult to obtain guaranteed global solutions. This Papers Contribution: This paper provides the first polynomialtime guarantee in smoothed analysis that solving BM also solves SDP. Specifically finding a secondorder critical point of BM corresponds to a solution of SDP. This overcomes the limitations of previous works that offered asymptotic guarantees or approximate criticality. Originality and Quality: The papers originality lies in two key subresults: a polynomialtime algorithm for computing 2AFAC points and a proof that a 2AFAC point of BM is approximately optimal for SDP. The paper provides a geometric characterization of spurious AFAC points to establish this connection. However the paper could improve its quality by providing a clearer comparison to related works particularly the study by Bhojanapalli Boumal Jain and Netrapalli (BBJN). While BBJN addressed a penalized version of SDP it is unclear how this differs from the papers approach. Clarity and Significance: The paper is wellwritten and motivated. Its significance lies in establishing a theoretical foundation for the practical success of the BM proxy problem in solving SDPs. This understanding can lead to improved algorithms for SDPs and broader insights into nonconvex optimization algorithms. Question to Authors: Could the authors please clarify how their paper compares to the work of Bhojanapalli Boumal Jain and Netrapalli"], "tvDRmAxGIjw": ["Paraphrased Statement: Summary:  The MREM method quantizes a large language model (PLM) without Quantization Aware Training (QAT) which is resourceintensive.  MREM minimizes reconstruction errors for individual modules of the PLM.  Each module is trained independently using either fullprecision outputs or actual outputs from the previous module.  To leverage modulewise behavior modules are divided onto separate devices.  Annealed teacher forcing simulates error propagation through layers.  Experiments demonstrate MREMs resource savings and performance benefits. Strengths:  Clear and coherent writing especially the comparison between QAT and PostTraining Quantization (PTQ).  Thorough experiments supporting the advantages of MREM.  Sufficient coverage of previous works.  Teacher forcing is a practical approach for module division and optimization.  The linear annealing schedule is welljustified. Weaknesses:  The key concept of module division and reconstruction error minimization is similar to the BRECQ method.  Incremental improvements over existing techniques but practical nonetheless.  Lack of comparison with BRECQ in the evaluation tables. Questions:  Is it appropriate to categorize MREM as a type of PTQ despite its inclusion of parameter updates  How would QAT compare to MREM if limited to the same number of training samples (4K)  Could QAT also benefit from using multiple GPUs to reduce memory usage creating a more balanced comparison"], "pqCT3L-BU9T": ["Paraphrased Statement: Summary: Periodic graphs such as those found in crystal materials are prevalent in various realworld applications. Learning representations for periodic graphs is crucial for tasks like property prediction. This study introduces a novel transformerbased framework for representation learning on periodic graphs. It employs a unique graph construction strategy and an effective method for encoding the lattice matrix. Experimental results demonstrate the methods efficacy. Strengths:  Clear formulation of periodic graphs using matrices (A P L)  Demonstrated effectiveness of the proposed method Weaknesses:  Motivation for using graphs: Explain the benefits of using graphs instead of directly using coordinates and provide examples of existing works on 3D position representation learning.  Motivation for selfconnecting edges: Explain why selfconnecting edges are necessary to capture lattice matrix information instead of concatenating it with singlecell representations.  Section combination: Merge sections 3.1 and 3.2 as they provide the theoretical foundation of the model.  Title clarification: Change \"Crystal property prediction\" to better reflect the content of the paragraph which focuses on the definition of periodic lattices.  E(3) invariance formality: Define E(3) invariance in section 3.1 instead of mentioning it in the abstract without formal definition.  Structure definition clarification: Specify that \"structure\" in Line 87 refers to the output of the \"f\" function defined earlier.  Necessity of periodic invariance: Elaborate on why periodic invariance is necessary for generating valid crystal representations.  Time complexity analysis: Provide a brief analysis of the time complexity of the proposed method and compare it to ALIGNN.  Include related works discussion: Discuss relevant literature on periodic graphs generation such as [4] and [5].  Code availability: Mention when the code will be made available for reproducing results."], "ohk8bILFDkk": ["Paraphrase: Summary:  The paper introduces a new framework called semiinfinitely constrained Markov decision processes (SICMDPs) which extends constrained MDPs to handle an infinite number of constraints.  The authors convert SICMDPs into semiinfinite linear programming problems and use techniques from semiinfinite programming to solve them.  They provide theoretical analysis and empirical evidence to support their claims. Strengths and Weaknesses:  The paper is wellwritten and proposes a significant advancement in incorporating a continuum of constraints into CMDPs.  The theories and algorithms are presented clearly and supported by convincing experimental results.  Examples in the paper illustrate the motivation behind the SICMDP framework.  The SICRL algorithm is straightforward and easy to understand.  The theoretical analysis is rigorous and provides complexity results for the algorithm.  Experimental analysis demonstrates the superiority of SICRL over a baseline method in reducing constraint violations and minimizing errors. Questions:  The term \"optimistic approach\" is used in line 128 without prior introduction. It would be helpful to define it earlier in the paper.  Algorithm 1 incrementally adds new constraints from a set of constraints.  If this set is discrete how does it align with the claim of incorporating a continuum of constraints  If the set is continuous how does the algorithm achieve incremental constraint incorporation"], "wtuYr8_KhyM": ["Paraphrased Statement: Summary: This study introduces an activation function that employs a rectifier with a threshold value set at a specific percentile (k) of the input tensors values. Strengths and Weaknesses:  Originality: The novel aspect of this approach lies in determining the threshold based on a percentile of the input data. This has not been previously explored.  Quality: The study is limited by the absence of statistical significance testing for the results. Without this it is not possible to draw meaningful conclusions reducing the papers quality.  Clarity: While the mathematical equations are sound the writing style can be difficult to follow and comprehend.  Significance: The study would hold greater significance if it included repeated experiments with statistical significance tests. Otherwise its impact remains limited. Questions:  k Determination: How is the percentile k determined Is it fixed adjustable or layerspecific Additionally which values of k were used in the experiments  Approximation of Heaviside Step Function: This activation function relies on a large alpha value. How does this affect the analysis presented in Equation 10"], "pz2UcXyX0Cj": ["Paraphrased Statement: Summary: The researchers present a novel framework for hierarchical reinforcement learning that leverages causal relationships between environmental variables to identify subgoals and develop policies to achieve them. The discovered subgoals and policies are then refined by a hierarchical agent that solves subsequent tasks. Testing in 2DMinecraft and Eden environments reveals promising results including faster convergence and interpretable causal graphs. Strengths: 1. High relevance of the problem addressed. 2. Welldesigned and principled approach with clear illustrations. 3. Consistent performance across multiple seeds in challenging environments. Weaknesses: 1. Directionality:  The methods ability to determine causality is unclear.  If environmental variables are selected for intervention in a specific order how does the causal graph adjust 2. Scalability with Many Environmental Variables:  Information is lacking on the approachs scalability when dealing with a large number of environmental variables.  The process for selecting and training lowerlevel policies needs clarification. 3. Missing Algorithm Details:  The algorithm should be included in the main draft to provide essential implementation information. 4. Assumption on Causal Graph Density:  The authors should clarify their assumption that the underlying causal graph is dense and that every environmental variable influences the others. 5. Subgoal Hierarchy Description:  A mathematical definition is necessary to describe the concept of a subgoal hierarchy and its role in the algorithm. 6. Utility of Causal Graphs for Subgoal Generation:  An assumption is made that causal graph nodes are all relevant for subgoal generation which is often not the case. This should be addressed. 7. Outperformance by Oracle:  The oracle using the groundtruth causal graph should theoretically outperform the proposed method. This discrepancy should be clarified. 8. Missing Citations:  Relevant publications should be cited such as \"Discovering Causal Signaling Networks for Control through Interventions\" and \"Learning Causal Signaling Networks for Inverse Reinforcement Learning.\""], "xqYGGRt7kM": ["Summary: This research investigates a multiarmed bandit problem with limited memory. The authors propose an instancesensitive algorithm when additional information is given. They also provide lower bounds demonstrating the necessity of additional information for effective performance with limited memory. Strengths:  The \"arm trapping lemma\" is a valuable theoretical result that formalizes the intuition about the difficulty of the streaming setting.  The concept of instancesensitive sample complexity for this problem is novel.  The work clarifies the difficulties of this problem without additional information.  The paper is written clearly. Weaknesses:  The dependency of the sample complexity on \\Delta27 (Theorem 5) may not be practical and introduces significant limitations.  The lengthy proof of Lemma 3.1 in the main text could be more concise.  The algorithms presented in the appendix require further explanation including their inputs and how they utilize the INSTcomplexity information.  The relevance of this work to NeurIPS may be questionable given its focus on theoretical computer science concepts rather than machine learning applications. Questions: 1. Can the classes of instances identified by the lower bounds be generalized or extended 2. Can the algorithm still function correctly if an approximate value of H2\\delta is provided and where is this parameter used in the main algorithm"], "l5UNyaHqFdO": ["Paraphrased Statement: This paper demonstrates that the Adam optimizer with hyperparameters \\beta1 \\leq \\sqrt\\beta21 and \\beta2 large converges to a vicinity of stationary points under certain conditions. If the Strong Growth Condition is satisfied Adam converges precisely to stationary points. The authors emphasize that analyzing Adam with high \\beta1 which induces strong momentum is complex and previous studies have struggled to handle nonzero \\beta1. Strengths and Weaknesses: The paper is wellwritten and effectively explains the main ideas particularly in the detailed proofs. The authors clearly outline the limitations of prior work in handling nonzero \\beta1 and make intriguing theoretical contributions. Limitations include a few typos and the authors setting \\epsilon0 in the proofs despite initially claiming that \\epsilon could be nonzero. Questions: 1. Intuition for SGC in overparameterized regime: SGC is reasonable because in overparameterized settings the optimization landscape is relatively flat and gradients provide less information. 2. Constant \\epsilon in proofs: The authors confirmed that they set \\epsilon0 during the proofs for simplicity. 3. Convergence with \\beta10 and large \\beta2: This is a distinct approach from Reddis proof which requires \\beta10. 4. Batch size and \\beta2: Smaller batch sizes require larger \\beta2 because they lead to higher variance in gradients which requires stronger damping. Typos: Grammatical:  Line 93: randomly  randomly.  Line 125: converge  converge to.  Line 161: require  requires.  Line 205: \"and\" deleted.  Line 262: \"red\" deleted.  Line 328: \"reduces\"  reduces.  Line 509: \"some more\" deleted.  Line 597 and 622: \"require\"  requires.  Line 658: \"as\"  because.  Line 677: \"an\"  a.  Line 690: converge\"s\"  converges.  Line 732: this  This.  Line 732: \"a\"  an. Math:  Line 225 and 699: fi(x)\\frac12(x2)232 for i0.  Line 291 eq. 3: Add vk0 to the righthand side.  Line 311: mlki should reference f\\tau instead of f.  Page 22 bottom: \\beta2 in nominators should be \\beta1. This also occurs in line 734.  Line 725 first line: j0.  Line 739: add 1\\sqrtk to the righthand side. Add 1n to the righthand side of line 740.  Line 789: v should reference \\beta2 instead of \\beta1.  Line 793 eq. 17: Change f\\tauki1(xki) to f\\tauki(xki1).  Line 815: xk  xk1 instead of xk1  xk.  Line 837 2nd line: Add \\frac1\\sqrt1\\beta2 to the righthand side.  Appendix G3: Add subscript \\ell.  Page 40 bottom: Add 1n to the lefthand side.  Page 46 bottom: \\frac1\\sqrtk instead of \\frac1\\sqrtk.  Equation 45: f0 cancels out.  Lines 1159 and 1166: \\beta2n in the denominator.  Line 1163 2nd equation: (Flk)k1 instead of (Flk)k1."], "tNXumks8yHv": ["Paraphrased Statement: Summary: The researchers introduce a probabilistic model that captures the concepts of dimensionality reduction methods like tSNE and UMAP. The goal of the model is to embed data points (X) into reduced dimensions (Z) by aligning the latent graph structures of the original data and the embeddings. Conditional distributions are defined for data points and embedding graphs and the crossentropy between their posteriors is minimized to obtain the embedding. Different graph priors lead to variations in the crossentropy objective with existing neighbor embedding methods corresponding to specific formulations. Since these methods use shiftinvariant kernel matrices the relative positions of data clusters are not always preserved in the embeddings. To address this issue the researchers combine the crossentropy objective with PCA (ccPCA) when using a Gaussian kernel matrix. This approach aims to preserve both the positioning of clusters and their local structures. Strengths and Weaknesses: Clarity: The paper may be challenging to understand for readers without a strong background in measure theory. An improved outline section could enhance clarity. Originality: The paper presents a novel statistical framework for dimensionality reduction providing insights into the limitations of existing methods and suggesting modifications for specific objectives. Quality: The theoretical foundation for neighbor embedding methods is robust. However the proofs were not independently verified. Significance: The proposed framework enables the design of dimensionality reduction methods that preserve largescale dependencies in data. These methods have wide applications making the approach significant. However its nonintuitive nature may limit its accessibility for further improvements. Notes: Outline: The outline could be improved to clarify the relationships between XZ and WXWZ and the nature of the posterior distribution. The similarity kernel concept should also be introduced. Technical Details: The authors use specific terms and notation that may require clarification for a broader audience. Questions:  Quantitative measures of cluster positioning similarity could enhance the evaluation of methods for preserving largescale dependencies.  More intuitive explanations of the connection between PCA and neighbor embedding methods based on the Wishart prior would improve comprehension."], "wzJcEb5Mm4": ["Paraphrased Summary: Traditional CNNs rely on multiple layers of convolutions and pooling to achieve large effective receptive fields (ERFs). However this strategy can hinder performance due to vanishing gradients and may not effectively increase ERF. Instead increasing kernel size is a more efficient way to enhance ERF. The authors propose a parameterefficient method for expanding ERFs by defining kernels in logpolar space. They demonstrate the benefits of their approach compared to conventional convolutions in various experiments. Additionally they provide a computationefficient implementation using standard convolution operations. Strengths and Weaknesses:  Clear motivation and overview of the method.  Original work as far as the reviewer knows.  Comprehensive overview of the method in Section 3.1 but an illustration could enhance understanding.  Difficulty in reading due to numerous inline equations and parameters.  Wellchosen experiments that demonstrate the superiority of logpolar space convolutions over conventional ones.  Exploration of an optimized implementation using conventional convolutions for direct comparison. Additional Questions and Responses: Question 1: When to use ellipsoid LPSCs  Answer: Not addressed in the original summary. Question 2: Applicability to irregularly sampled data  Answer: Not addressed in the original summary. Question 3: Weight sharing and resolution tradeoff  Answer: The authors provide a stronger motivation by including an analysis of pixel statistics acknowledging the tradeoff between increased ERF and lower resolution information at higher distances. Question 4: Factors contributing to performance improvement  Answer: The authors suggest that both the increased receptive field and the way they process local and nonlocal information at different resolutions contribute to the performance gains. Question 5: Use cases where locality assumption may not hold  Answer: The authors acknowledge that their assumption of locality may not apply to certain domains like speech signals but do not provide specific examples of how their method might perform in such scenarios."], "scfOjwTtZ8S": ["Paraphrase: Summary: This paper presents a reward shaping technique for languageconditioned reinforcement learning (LCRL). The authors suggest creating questions based on natural language instructions and evaluating the likelihood that the current trajectory satisfies these questions. The probabilities of answering the questions correctly serve as intrinsic rewards. The Baby AI benchmark demonstrates that the reward shaping method enhances sample efficiency and model performance. Strengths:  The concept of using question generation and answering for reward shaping in LCRL is novel and innovative akin to selfassessment in human task completion.  The paper demonstrates the effectiveness of this approach in the Baby AI benchmark. Weaknesses:  The simplicity of the method may limit its applicability in more intricate settings:  Question generation revolves around masking words in the instructions but this might not suffice for complex instructions where inference is possible from unmasked portions.  The QA model is trained on the complete trajectory but intrinsic rewards are computed using partial trajectories. The authors have not addressed the impact of short partial trajectories on QA model performance. Questions:  Section 4.1 mentions removing questions once answered. How is a question deemed answered Does the QA model need to predict the correct answer with high probability"], "nxw9_ny7_H": ["Summary: The paper addresses the challenges in current bilevel optimization methods for automatic data augmentation (ADA). The authors propose AugNet a novel approach that combines a base trunk model with learnable augmentation layers and an aggregation module. The network training and augmentation learning are optimized endtoend at a single level. Experiments demonstrate the versatility of AugNet across various tasks including image and physiological data classification. Strengths:  Welljustified motivation and review of related work  Mathematical intuitions and practical approximations provided  Detailed experimental setup documentation  Error estimates provided for key results  Testing on diverse synthetic and real tasks Weaknesses: Major:  Weak empirical evaluation:  Small or toy problem datasets  Limited testing with larger and more realistic models  Incomplete experiments to evaluate hyperparameter sensitivity and regularization effectiveness  Poor performance on Cifar10 dataset suggesting limited practical advantage  Lack of comparison with established augmentation methods like Adaptive Augment (AA) and Random Augment (RA) Minor:  Excessive mathematical details could be moved to the appendix  Discussion of limitations and potential improvements is lacking"], "y8FN4dHdxOE": ["Paraphrase: This paper introduces a novel formulation and algorithm (ALS) for clustering tensors. The algorithms theoretical convergence rate is bounded and empirical results demonstrate improved clustering accuracy compared to current methods. Strengths:  Importance of tensor clustering problem  Thorough theoretical analysis  Superior experimental performance Weaknesses:  Orthogonalization Step: While the orthogonalization step before each ALS iteration aims to prevent local minima and accelerate convergence the paper lacks specific analysis of its benefits. Experiments comparing algorithms with and without orthogonalization would be beneficial.  Hyperparameters: The formulation involves several hyperparameters whose impact is described in Section 3.2. However it remains unclear if the formulation is resilient to these parameters and whether alternative parameter selections exist. Discussions on this topic would be helpful. After Rebuttal: The authors have addressed my concerns. Questions:  Line 70: The spectrum norm formulation appears incorrect it should be \"max\" instead of \"min.\"  Line 79: Remove the redundant bracket at the end of the line."], "lHy09zPewmD": ["Paraphrased Summary: Researchers propose an expanded version of the Bandits with Knapsack problem where resources can be randomly replenished over time. They develop an algorithm that minimizes the difference between its performance and the maximum potential reward regardless of whether there is a single or multiple resources. This algorithm requires extensive analysis. They also create a learning method that demonstrably results in logarithmic regret over time. Additionally they demonstrate that Bandits with Knapsacks can be reduced to their extended version resulting in comparable bounds. Strengths:  Detailed theoretical analysis featuring counterintuitive findings like Lemma 3.5 (threshold selection eliminates horizon dependency in regret).  Main findings:  Constant regret in case of known reward and cost distributions achieved by tailoring the solution to specific scenarios where optimal choices are identifiable and tractable.  Log(T) regret in online learning leveraging a previous multistage approach to pinpoint and improve upon \"best choices\" in early stages enabling optimal decisions in the final stage.  Connection to Bandits with Knapsacks provides context for the results.  Clear writing despite the technical complexity. Intuitive explanations are provided in proof outlines (e.g. lines 183190).  Novelty of the problem. Weaknesses:  Limited motivation for the generalized problem. The example of dynamic pricing with random shipments is not convincing.  Lack of empirical findings which could showcase practical implications.  Insufficient discussion on the limitations imposed by assumptions in section 2.3. Questions:  Can you elaborate on the significance of the generalized BwK problem  Can you offer empirical evidence or practical insights related to the theoretical results particularly with consideration to the assumptions in section 2.3  Can your algorithm and findings apply when costs are deterministic as in Case 2 of Flajolet and Jaillets work"], "lhl_rYNdiH6": ["Paraphrased Summary: This research introduces CGI a new deep learning architecture for graphs. CGI creates a vector space for users and items for use in recommendation systems. It uses two views (nodes and edges) and applies data augmentation with perturbation and the Information Bottleneck framework to enhance robustness against noise. CGI outperforms existing models on various datasets and has undergone thorough testing. Strengths:  Innovative model with welldefined goals  Strong performance compared to recent advancements Weaknesses:  None significant Questions:  Can a more accurate baseline be developed by adjusting node centrality weighting before employing existing literature solutions  The significance of Proposition 1 as a novel contribution and its potential applications in other research areas."], "uyEYNg2HHFQ": ["Paraphrase: This paper presents a method for generating weights for neural networks by learning a generative model of the weight distribution. The generative model is trained on a collection of pretrained neural networks (a \"classifier zoo\") and can generate new weights that capture the structure of the networks \"weight manifold.\" These generated weights can be used to initialize new networks or create model ensembles. The paper discusses various techniques for capturing the weight distributions and compares their performance to natural baselines. Strengths:  The idea of learning the weight manifold and training a generative model is intriguing and conceptually sound.  The paper introduces an effective layerwise loss normalization technique and several practical approaches for learning generative models.  Empirical results demonstrate that the trained generative models can capture the weight manifold and generate accurate neural networks even competitive model ensembles. Weaknesses:  The proposed method requires significant computational resources for pretraining the classifier zoo potentially making it impractical in some applications.  The proposed invertible function from \\mathbbRD to \\mathbbRd may not exist for Dd making the explanation in Section 3.2.3 unclear. Questions:  Would an ensemble of pretrained models finetuned and averaged on a new task provide a more informative baseline for capturing the weight manifold  Could the smoothness of the loss function along linear segments in weight space indicate an approximate global linearity in the mapping from embedding to weight  Does the contrastive loss favor generating equivalent network weights (up to channel permutation) potentially limiting the generation of model ensembles"], "t6O08FxvtBY": ["Paraphrased Statement: The authors propose a method to identify optimal subsets of weights (called \"winning tickets\") in neural networks for pruning. They use a technique called bilevel optimization (BLO) to efficiently extract these winning tickets. The authors demonstrate that their method consistently outperforms other pruning approaches on various datasets including CIFAR10100 TinyImageNet and ImageNet. Response to Authors:  Definition of \"Winning Tickets\": The authors acknowledge the ambiguity in the definition of winning tickets and clarify that their definition (models smaller than the original network with improved generalization) is consistent with the abstract.  Pruning Timeline: Pruning is performed continuously throughout training.  Comparison with Prior Work: The authors acknowledge the existence of prior work on bilevel optimization for pruning and provide additional references. They also clarify that their method differs from previous approaches in its focus on structured pruning and its use of BLO for iterative optimization.  Benchmark: The authors justify their choice of reference 18 as the benchmark based on its wide adoption in the literature and the availability of results for different models and datasets.  Speed of Channel Pruned Networks: The authors acknowledge the importance of measuring speedups and provide additional results in the revised submission. Minor Thoughts:  The authors acknowledge that Grasp is not SOTA and provide a comparison to ProsPr in the revised submission.  The authors provide additional evidence to support the claim of robustness to rewinding."], "wlrYnGZ37Wv": ["Paraphrased Statement: Summary: This research proposes Sequencer which substitutes selfattention with deep Long ShortTerm Memory (LSTM) networks for image classification. The paper compares Sequencer to various existing methods to demonstrate its effectiveness. Strengths:  Introduces Sequencer utilizing LSTMs instead of selfattention for sequence modeling.  Presents a twodimensional version of Sequencer enhancing performance through the decomposition of LSTMs into vertical and horizontal components.  Demonstrates Sequencers advantages over selfattention in adaptability to different image resolutions and transferability between tasks.  The paper is wellwritten and clearly explains the research. Weaknesses:  Certain experimental results lack detailed explanations. Questions: 1. Table 1 suggests that Sequencer performs worse than ConvNetXt in terms of FLOPS and throughput while achieving a modest accuracy improvement of 0.20.3. Also finetuned Sequencer underperforms finetuned ConvNetXt. 2. The performance comparison between Sequencer and ConvNetXt in Figure 3 appears to contradict the results presented in Table 1."], "pBJe5yu41Pq": ["Paraphrase: The authors present a novel approach that links data augmentation and tempering. Their approach based on Dirichlet distributions helps understand this relationship and offers guidance for its effective use. Strengths:  The research is wellmotivated and clearly presented.  The proposed approach clarifies the connection between tempering and augmentation. Weaknesses:  The writing lacks clarity in certain areas making familiar concepts appear novel.  The authors fail to acknowledge significant prior work using Dirichlet distributions in Bayesian classification models. Additional Observations:  Figure 2 should include both unaugmented and augmented training data.  Figure 3 and Figure 5(a) lack error bars.  The number of experimental runs for error bars should be stated in the main text. Question: The authors suggest that softened likelihood is \"counterintuitive.\" The reviewer questions this arguing that data augmentation should reasonably result in a less confident model."], "oDWyVsHBzNT": ["Paraphrased Statement: Summary: Previous modelbased offline reinforcement learning (RL) studies frequently gauge the uncertainty of the acquired dynamics and leverage this information to regulate the reward. However this strategy may lead to inconsistent evaluations of the uncertaintys impact and cause an unforeseen tradeoff between model use and risk aversion. The authors propose a novel offline RL algorithm that uses a belief distribution over system dynamics for sampling instead of explicitly quantifying the uncertainty. This sampling procedure is modeled as an alternating Markov game (AMG) and the degree of pessimism can be adjusted via hyperparameters. The algorithm addresses the AMG through iterative policy optimization with guaranteed improvement. Experimental results demonstrate the proposed algorithms efficiency and improved performance. Strengths:  The methodology is wellexplained and the paper is accessible.  Empirical results and ablation studies demonstrate the algorithms effectiveness.  The AMG formulation leads to the development of pessimismmodulated dynamics belief derivation and theoretical results provide guidance for algorithm design. Weaknesses:  The experimental results could be more comprehensive.  The tunability of hyperparameters (k and N) and the determination of weights in the objective function (Eqn. 11) are not addressed.  A reference to Eqn. 18 in the text is undefined. Questions:  How can a sufficient belief be constructed for an arbitrary task without expert knowledge  How are hyperparameters (k and N) tuned for different tasks  How are the weights in Eqn. 11 determined for optimizing the Bellman residual of the AMG and empirical MDP"], "pGcTocvaZkJ": ["Paraphrased Statement: Summary:  Researchers have developed a novel method that replaces linear models in censored quantile regression with neural networks (CQRNN).  The researchers enhanced the sequential grid algorithm with bootstrap weights improving its effectiveness.  The researchers provide an explanation for the validity of their method by drawing parallels to the EM algorithm.  The CQRNN method was evaluated and compared to three standard approaches on synthetic and realworld datasets. Strengths: 1. Novelty: The method introduces neural networks into quantile regression and proposes a new bootstrap algorithm. 2. Explanation: The researchers provide an explanation for the workings of CQRNN using an EM analogy and its selfcorrecting nature. 3. Clarity: The paper is wellwritten and easy to understand. Weaknesses: 1. Abstract Explanations: While the EM analogy and selfcorrecting property concepts are explained they could benefit from a simpler example particularly for a linear model. 2. Thoroughness in Empirical Evaluation:  The synthetic datasets were only analyzed for three specific quantiles (0.1 0.5 0.9). However for survival modeling a broader range of quantiles may be relevant.  The evaluation solely focused on Brier score comparison with a lognormal model which is often assumed in survival analysis. However other models like DeepHit and SODEN may also be applicable and provide better results. Questions: 1. Convergence Guarantee: A more concrete assurance on the convergence of the algorithm would be beneficial including for linear models. 2. Obtaining the Survival Distribution: While quantile regression provides specific quantiles it is unclear how to obtain the entire survival distribution using this method. 3. Choice of Lognormal Model: The rationale behind using a lognormal model as the baseline for survival modeling should be further explained."], "q-snd9xOG3b": ["Summary: This study explores \"Canonical Noise Distributions\" (CNDs) for differential privacy with function sensitivity (\"fDP\"). CNDs are probability distributions that satisfy fDP without using up the privacy budget. The study establishes the existence and special properties of logconcave CNDs. It also generalizes the concept of CNDs to higher dimensions. The work relates CNDs to other privacy notions such as typical differential privacy and Gaussian differential privacy. Strengths:  Provides a theoretical foundation for understanding optimal noiseadding strategies in differential privacy.  Extends previous work by showing the existence of wellbehaved CNDs in one dimension.  Generalizes CNDs to higher dimensions and provides an easy construction method for certain scenarios.  Connects CNDs to existing differential privacy mechanisms clarifying relationships between different approaches.  Presents the ideas clearly and logically. Weaknesses:  Does not address CNDs for fDP with nonzero \u03b4 and l2 sensitivity which is a common setting. Minor Question:  The meaning of \"G1\" in line 227 is not specified."], "lJx2vng-KiC": ["Summary: This paper provides exact mathematical formulas that describe how deep linear neural networks learn. The authors use these formulas to gain insights into how different initialization strategies affect network learning behavior. Strengths:  Clear and wellorganized writing and explanations.  Valuable theoretical framework that deepens understanding of neural network learning dynamics.  Novel insights such as the effect of large weight initialization on learning speed and the relationship between inputoutput correlation and forgetting. Weaknesses:  Lack of clarity in the inputoutput correlation matrix shown in Fig. 4A.  Absence of explanation for transient increases in alignment dynamics (Fig. 5).  Ambiguity in the definition of the forgetting metric (Fig. 6).  Inconsistent referencing style throughout the paper.  Typos (Eq. 8 Fig. 3C).  Inconsistent labeling of RSA plots in Fig. 4.  Possible typo in Fig. 5C.  Offtopic example in the \"Revising structured knowledge\" subsection (Sec. 6). Questions: In addition to the weaknesses mentioned above the authors may consider addressing the following questions:  Why do some tasks exhibit transient increases in alignment part way through learning (Fig. 5)  What does negative forgetting represent in the metric used in Fig. 6  Why do the discrepancies between analytical and numerical forgetting appear different in Fig. 6B and Fig. 6C  Can a concise summary table or discussion be provided to outline the specific theoretical contributions made by the paper"], "pF8btdPVTL_": ["Paraphrased Statement: This research examines a phenomenon called \"benign overfitting\" in twolayer convolutional neural networks (CNNs). If the data has a high signaltonoise ratio the authors demonstrate that training a twolayer CNN using gradient descent can result in extremely low training and testing errors. However when the signaltonoise ratio is low overfitting becomes damaging and the CNNs testing error remains constant. The paper highlights a sharp transition between these two types of overfitting which is influenced by the signaltonoise ratio. Strengths:  Intriguing and novel observation  Convincing experimental evidence Weaknesses:  Technical and complex theoretical analysis  Lack of practical examples Additional Questions:  Is there a general property that characterizes benign overfitting  To what extent can these findings apply to a broader range of functions and neural network architectures"], "snUOkDdJypm": ["Summary: This paper establishes the first nonasymptotic convergence rates for the extragradient (EG) and optimistic gradient (OG) algorithms in solving constrained smooth monotone games. These rates are expressed in terms of the gap function Lipschitz constant and monotone variational inequality and they match the existing lower bounds of \\mathcalO(T12). The concept of tangent residual is introduced as a novel proximity measure to Nash equilibrium and is utilized for convergence analysis. Strengths and Weaknesses:  Originality: The algorithms and problem setting are known but the nonasymptotic convergence rate in constrained smooth monotone games is novel and optimal. The notion of tangent residual and the choice of potential functions for convergence analysis are also original.  Quality: The work is wellexecuted and supported by detailed theoretical analysis. However the experimental evidence could be strengthened by extending the iterations and presenting plots of the potential function value and performance measures.  Clarity: The paper is wellorganized and clear. Certain concepts could benefit from additional clarification.  Significance: The results provide a nonasymptotic convergence rate for constrained smooth monotone games which matches the lower bound and is of potential importance. Questions: 1. Why is the EG algorithm not included in contribution 1 on page 3 2. Table 1 has a typo: \"cocoercive\" should be corrected. 3. Line 95 contains a typo: \"requires the setting to be unconstrained.\" 4. In Definition 1 clarify that F\\mathcalG is monotone if and only if f(i) is convex for all i. 5. In Definition 2 explain that the gap functions are nonnegative and vanish only at Nash equilibrium. Consider framing them as distance measures rather than proximity measures. 6. Provide intuition for the concepts in Definitions 3. 7. Cite Definition 7 in Appendix E when introducing r\\mathcalG\\textnat in line 230. 8. Clarify whether the potential differences are polynomial and how SOS programming can be applied if they are not. 9. Extend the experiment iterations and plot the potential function value and performance measures in Appendix J to illustrate their convergence."], "tmUGnBjchSC": ["Paraphrase: Summary: This study introduces a new approach to Bayesian Optimization (BO) by utilizing a family of acquisition functions (AFs) based on decisiontheoretic entropies specifically the HlA\\textitentropy. AFs select queries that aim to minimize the uncertainty in HlA\\textitentropy quantified as \\textitexpected HlA\\textitinformation gain (EHIG). EHIG is a flexible framework that can be adapted to various problem categories by adjusting its parameters. The study also presents a gradientbased method for optimizing the acquisition process. Extensive experiments on example datasets demonstrate the effectiveness of the proposed approach. Strengths: 1. Clear and accessible organization. 2. Proposes a novel AF that combines informationbased and decisiontheoretic approaches providing a unified framework for BO. 3. Outperforms baseline methods when combining the tailored EHIG AF with the proposed gradientbased optimization procedure. Weaknesses: 1. Experiments focus on tasks lacking prior AF development limiting comparisons with other traditional BO methods on wellestablished problems. 2. Lack of explanation for the observed increase in negative loss for Random Search (RS) despite random sampling from the full domain. 3. Omission of discussion and visualization of other routes in the Sequence Search task potentially obscuring a thorough comparison with the proposed HES method. 4. Insufficient size of some plots for clear visualization of details and axes."], "muvlhVKvd4": ["Paraphrased Statement: Summary: The study examines the convergence of SGDlike algorithms demonstrating that the gradient norm at the final iteration converges to zero in both an expected and almost sure sense. They establish a unified framework to analyze convergence for SGDlike methods encompassing SGD Random Reshuffling ProxSGD and ModelBased methods and prove the convergence of these methods using this framework. Strengths and Weaknesses: Strengths:  The authors establish a general Theorem (Theorem 2.1) that extends existing convergence results.  The framework provides a useful tool for analyzing a wide range of SGDlike methods. Weaknesses:  Despite being a \"Unified Convergence Theorem\" the paper lacks sufficient examples to support this claim.  The framework relies on Assumption (C.3) which may limit its applicability for certain regularizers (e.g. the common quadratic regularizer). Questions and Suggestions:  The authors suggest that it may be possible to unify the proof of Theorem 2.1 by not distinguishing between the cases of q  1 and q  1. This simplification could potentially strengthen the proof."], "sNcn-E3uPHA": ["Summary Paraphrase: This paper applies quantum physicss Born Rule to text classification a fundamental task in machine learning. The method treats text as a superposition of words and uses Born Rule to predict the category of a given text. It works as a standalone classifier or integrated with a neural classifier. On three large datasets it surpasses traditional and neural network methods particularly with more training epochs. Notably it offers efficient runtime performance. Strengths and Weaknesses Paraphrase:  The papers readability and accessibility make it appealing to a wide range of readers. Its implementation on popular libraries like Scikit and Pytorch simplifies replication.  However the paper lacks a comprehensive discussion of the methods limitations in various applications and data types. It would benefit from addressing its potential performance on tasks like sentiment classification noisy social media texts and smaller text instances.  A comparative analysis with existing text classification methods including language models is missing. A tabular summary would provide valuable insights for researchers. Recommendation: Despite these weaknesses the papers novel approach and empirical results warrant its acceptance. However the authors should address the limitations more thoroughly in a revised version. Questions:  Why were traditional algorithms used instead of more advanced language models like BERT which are now considered standard in text classification  What criteria were used to select the three datasets given that popular choices like IMDB Amazon Yelp and AG are commonly used for evaluating new text classification methods  Does the method favor certain classes as indicated by its confusion matrices This could be a potential limitation that should be investigated."], "vWUmBjin_-o": ["Paraphrased Statement: This paper presents a technique that identifies geometric invariants in the embedding space rather than the input space. The authors argue that equivariant networks perform well because simple group actions are learned as injective functions are invariant to any group. By not constraining network structures the proposed method allows for nonlinear or unspecified group actions on the input space. Experiments show that adding the geometric invariant to the loss function improves the neural networks learning of meaningful embeddings and allows for symmetry group decomposition. Using the learned embeddings in downstream tasks achieves top performance. Strengths:  Novel approach of inducing equivariance in the embedding space.  Insightful observation that equivariance can be a weak inductive bias.  Clear distinction from previous work.  Informative visualizations supporting the learned embeddings meaningfulness.  Comprehensive experiments demonstrating the methods benefits over baselines. Weaknesses:  While the method addresses complex or unknown group actions in the input space it still requires knowledge of the group to apply to the embedding space.  All experiments use datasets with welldefined symmetries. It is unclear how to select a group for tasks without obvious symmetry. Minor Typos:  Line 136: \"X x G\" should be \"G x X\" for consistency.  Figure 3 caption: \"againts\" should be \"against\". Questions:  World Modeling Experiment: Why were the two specific games chosen Provide justification that strong performance on these games indicates SymRegs success in other environments (e.g. grid and 3body physics).  Equivariance for NonInjective Functions: Given the argument in section 4 it seems equivariance remains useful for noninjective functions like many neural networks. If the group action is known would SymReg perform better than traditional equivariant models"], "q_AeTuxv02D": ["Paraphrased Statement: This study investigates the ability of graph message passing neural networks (gMPNNs) to predict links outofdistribution (OOD) with varying graph sizes. The authors show that while gMPNN predictions can deteriorate to random guesses for larger graphs they propose pairwise embeddings as a solution to this issue. Theoretical results are validated through experiments on random graphs. Strengths and Weaknesses: Strengths:  Originality and Significance: Studying the OOD capabilities of gMPNNs is valuable to the community.  Quality: Extensive theoretical analysis is provided. Weaknesses:  Scope and Novelty:  The scope is limited to OOD inductive link prediction in terms of graph sizes. Other potential distribution shifts such as covariate shifts and graph model variations are not considered.  The theoretical tools and solution are not entirely novel as they draw from existing literature.  Empirical Results:  The convergence results rely heavily on a specific graph model (SBM) and it is unclear if they extend to other graph models or more realistic graphs.  Minor Points:  The causal graph lacks substantial implications.  A typo exists in line 190 (i \\in SA should be I \\in Sa). PostRebuttal The authors have adequately addressed concerns raised during the rebuttal process leading to an upgraded rating of 7."], "lUyAaz-iA4u": ["Paraphrased Summary: DecSPS a new algorithm enhances Stochastic Proximal Stochastic Gradient (SPS) convergence. It achieves this by: 1. Substituting the optimal minibatch loss in step sizes with a lower bound. 2. Demonstrating that DecSPS converges to the exact solution under strong convexity where iterates are bounded. Paraphrased Strengths:  Clear writing and organized proof.  Clearly stated contributions. Paraphrased Major Concerns:  Limited theoretical novelty with the step size adjustment being more practical than groundbreaking.  Expansion of results suggested including properties under strong convexity and nonconvexity and comparisons to Adam and RMSProp.  Additional numerical results on deep learning benchmarks could enhance comparisons. Paraphrased Minor Concerns:  Table 1 should be incorporated into the main text.  Explanation of the \\gammab parameters role in SPS and DecSPS is needed.  Font size improvement in figures for clarity.  Correction in line 227 (change \"k\" to \"0\").  \"And or\" correction to \"andor\" in line 282. Paraphrased Questions:  See concerns above."], "wiHzQWwg3l": ["Paraphrase: Summary: The research introduces a method for adapting nonstationary streaming data in real time without supervision. It incorporates a Bayesian neural network that modifies the feature extractors parameters over time acting as a \"meta network\" with dynamic output. A discriminator enables adversarial domain adaptation training. Additionally the method includes a predictive module that relies on the representation produced by the invariant feature extractor. Bayesian inference is performed through Variational Inference utilizing a continuous time particle filtering approach. Strengths and Weaknesses: Domain adaptation for nonstationary streams in real time is a significant theoretical and practical challenge. The proposed framework is innovative with a clear description of the method. However the papers complexity evident in the intricate loss function necessitates further justification. Ablation studies are absent raising questions about the necessity of this complexity. The reason for using a Bayesian approach is unclear. It is plausible that it enhances performance but it is not evident that it does. Exploring the drawbacks of a point estimate approach versus a Bayesian approach for encoder parameter estimation would provide valuable insights. The problem setup could be clarified further. It is unclear if the source and target domains differ in input space (as in the growing circles dataset) or temporally (as indicated in Figure 2B). The unusual use of the prior as a target in the variational lower bound (Line 210212) goes against the traditional Bayesian framework. Questions:  Clarify the training process. Figure 1 suggests that two embeddings are generated but it is unclear if they correspond to the source and target domains or both domains with additional embeddings.  Is lu(\\cdot  \\theta) itself an extreme value (Equation 4) or part of a saddle point optimization  Equation 5 and 6 have unclosed brackets.  Provide a stronger rationale for using the temporaldomaininvariance loss (li) in addition to the Vd term (lu). An ablation study would be beneficial."], "noyKGZYvHH": ["Paraphrased Statement: Researchers introduced a new technique called the CoVariance Neural Network (VNN) inspired by graph neural networks. It utilizes covariance matrices like those in principal component analysis (PCA) as a basis for analysis. They proposed a \"covariance transform\" to project new data points into PCA space and a \"covariance filter\" to learn coefficients of polynomials derived from the covariance matrix. Strengths:  Recognition of similarities between covariance matrices and graph adjacency matrices suggesting VNNs as a specific type of GNN.  Demonstration of stable PCA using polynomial filters. Weakness:  The proposed technique does not constitute a novel neural network but rather an application of GNNs to covariance matrices.  Robust PCA methods already exist for stable PCA.  Covariance matrices only capture linear relationships between data features limiting the applicability of VNNs compared to GNNs with more complex relational models.  Covariance filters resemble lowrank approximations done through SVD making their use in neural networks questionable. Question: What other applications of the covariance filter could potentially extend beyond the capabilities of SVDbased lowrank approximations"], "zBlj0Cs6dw1": ["Paraphrase: Summary: This study investigates using reinforcement learning to enhance the column generation algorithm. The authors create a state space based on a bipartite graph (nodes: columns and constraints) and an action space derived from the problems RMP and SP formulations. Two problems (cutting stock and vehicle routing) demonstrate the superiority of their technique over a baseline. Strengths:  RL in combinatorial optimization is a growing research area.  Their method excels in two significant problems (cutting stock and vehicle routing).  Figure 1 provides clear clarification.  Opensourced code is available. Weaknesses:  Lack of detail on the RL aspect (e.g. DQN equation).  Figure 6 mentioned in Appendix A should be included in the main text.  Minimal conceptual illustration despite extensive text. Questions:  Section 3: Clarification needed on the derivation of the SP formulation from the RMP formulation.  L165168: How problemindependent are the designed features  L172: Detailed explanation of the GNN structure is missing. Conclusion: A solid paper that explores the application of RL to the column generation algorithm. The unique design of the state and action spaces and the resulting performance improvement make it a valuable contribution. While it is not groundbreaking given recent RL applications in combinatorial optimization it does offer a valuable addition to the field."], "qtQ9thon9fV": ["Summary: This research introduces a novel technique for representing humans in 3D known as Field of Fourier (FOF). It uses Fourier series to estimate the threedimensional occupancy of a pixel in an image and trains this representation using data to predict the coefficients of the series. The technique is effective and versatile as demonstrated by its application to realworld data. Strengths:  Novelty: The FOF representation is innovative and uses the Fourier series to model occupancy.  High Quality: The reconstructions generated using this technique are of excellent quality.  Clarity: The research is presented in a clear and accessible manner.  Significance: It addresses the urgent need for efficient and effective 3D representations for realtime applications. The research showcases a realtime pipeline that incorporates the FOF representation. Weaknesses:  While weaknesses are not readily apparent the paper could improve by providing a more indepth analysis of the reconstruction quality.  Specifically the research could address why the FOF representations may not provide the same level of detail as other methods such as PIFuHD. Questions:  Regression Sensitivity: The training of FOF involves perpixel regression to estimate Fourier series coefficients. The research could explore the robustness of the FOF representation to errors in these coefficient estimates by:  Displaying the training loss in terms of relative error.  Introducing controlled noise into the coefficients and assessing the impact on reconstruction quality."], "yjWir-w3gki": ["Paraphrase: This work extends the HamiltonJacobiBellman (HJB) equation for continuous time reinforcement learning to arbitrary discounting schemes. A deep learning algorithm based on this extended HJB equation is proposed for reinforcement learning under hyperbolic discounting. Additionally the authors investigate the inference of the discount function from data and present a deep learning algorithm for this task. The effectiveness of both the RL and discount function inference algorithms is demonstrated in simple environments. Strengths:  Clear and wellwritten  Novel study of continuous time reinforcement learning  Collocation algorithm provides an elegant approach for handling timeinhomogeneous value functions  The discount function inference problem is of interest in various fields Weaknesses:  Some minor mathematical issues (e.g. incorrect dimensions)  Experimental results are qualitativespeculative and do not convincingly demonstrate the value of arbitrary discounting  Novelty of the contributions is limited Questions:  Line 104: Clarify the interpretation of the hazard rate for reward unavailability.  Equation after Line 127: Address the incorrect dimensions and add a trace operator if necessary provide details about the reward function (e.g. is it timehomogeneous).  Theorem 1: Explain the significance of \\alpha0 and clarify why R must be bounded below for the integral to diverge.  Line 275: Discuss the feasibility of supporting continuous action spaces using a technique from [2]. Minor Issues:  Line 59: Remove the unnecessary comma  Line 60: Consider mentioning \"[1] Theodorou et al. 2010\" when discussing path integral approaches  Line 63: The introductory paragraph may be considered redundant"], "kRgOlgFW9aP": ["Paraphrase: The article examines Thompson sampling for linearquadratic stochastic optimal control problems in continuous time. The analysis and algorithms use a Gaussian conjugate prior distribution. The paper provides theoretical insights: 1. A limit on the probability of system destabilization using a linear exploration strategy. 2. A bound on the squared estimation error. 3. A finitetime regret bound. Numerical simulations on synthetic examples demonstrate the developed algorithms performance. Strengths:  Provides theoretical guarantees for Thompson sampling.  Offers insights into the algorithms behavior.  Addresses a research gap in continuoustime modelbased reinforcement learning. Weaknesses:  Overstates the comparison with existing RL policies (based on limited comparisons).  Insufficiently acknowledges related work on adaptive and dual control which offers optimal solutions. Unanswered Questions:  The meaning of \"exists is unique and can be computed using continuoustime Riccati differential equations\" in line 131.  The absence of hyperparameters in the bounds after line 192.  The possibility of deriving similar bounds under different posterior distributions (such as sparsity priors)."], "ofwkaIWFqqv": ["Paraphrase: Summary: This research focuses on developing a method for automating chemical reaction planning using reinforcement learning (RL). The proposed approach GRASP is an actorcritic framework that allows for goaloriented planning. This addresses the challenge of evaluating the quality of reaction pathways by directly meeting the specific requirements set by chemists. To enhance learning speed GRASP employs hindsight goal relabeling. Their method shows superior performance compared to other algorithms on both academic and industrial benchmark datasets. Strengths and Weaknesses: The reviewer lacks expertise in retrosynthetic planning but the proposed method appears sound and outperforms existing approaches. The empirical evaluation supports the following claims:  Goaloriented planning improves pathway quality evaluation.  GRASP surpasses baselines in general retrosynthesis metrics. Questions:  A minor correction in line 189: \"Segler et al.\" should precede [24]."], "wXNPMS11aUb": ["Paraphrased Summary: This paper introduces a refined locallysensitive hashing technique building on the FlyLSH algorithm. Unlike FlyLSHs random network weights this approach strategically assigns stronger input dimensions to distinct output neurons based on specific stimuli. The upgraded model exhibits enhanced performance in various tasks including image and text data analysis. Paraphrased Strengths and Weaknesses: Strengths:  Computational efficiency particularly in weight learning and nearest neighbor search.  Rigorous testing on standard benchmarks with improved performance over FlyLSH.  Comprehensive introduction and related work analysis.  Potential implications for neuroscience in replicating olfactory system characteristics. Weakness:  No specific weaknesses identified."], "w4X7GLThiuJ": ["Paraphrase: Study: The authors investigate alternating mirror descent updates for games where players aim to minimize or maximize a bilinear function while adhering to constraints. By alternating updates (rather than updating simultaneously) the method achieves an average regret of O(K23) over K iterations improving upon the O(K12) regret of simultaneous updates. Unlike simultaneous updates these iterates converge to a constant energy level mirroring the dynamics of continuoustime models that inspire the approach. Strengths:  Bilinear zerosum games are crucial in game theory and machine learning making this research valuable.  The O(K23) average regret bound stands out as innovative and cuttingedge in this domain.  The connection to continuoustime models provides insights into alternating updates and the energy analysis is elegant.  The regret notion aligns with the duality gap in the context of this study. Weaknesses:  The thirdorder smoothness condition though seemingly reasonable warrants further explanation and examples.  The regularizers domain must coincide with the constraint set which is not standard in mirror descent and limits the applicable constraints.  The alternating update requires coordination between players raising questions about its practical applicability compared to other methods with better regret.  The study builds on [2] by introducing alternating updates and addressing nonzero Bregman communicators but the extension may be relatively minor with the additional smoothness assumption. Questions:  Does this work handle mirror descent in its general form or is it limited by the constraints on the regularizers domain Can it apply to constraints like polytopes Note: Mirror Descent and FTRL differ in online learning when constraints and projections are involved."], "zUbMHIxszNp": ["Summary Paraphrase: The authors have developed a novel approach for designing graph generative models that accurately capture graph statistics. They demonstrated the effectiveness of their approach through experiments and visualizations showcasing that the generated graphs closely resemble realworld graphs. Notably the proposed objective function allows for tailoring the model to reflect specific graph statistics easily. Strengths and Weaknesses Paraphrase: Originality: The proposed method is original and innovative as it introduces a new VAEbased objective function for capturing graph statistics. Quality: The study focuses on generating graphs that accurately reflect graph statistics. While theoretical support and experimental results are provided the performance on realworld datasets particularly when using a single graph statistic is limited. The authors could provide more clarification regarding the selection and synergy of the combined statistics that yield better performance. Clarity: The paper is wellwritten and provides sufficient mathematical details. Adding an architecture overview figure would enhance understanding. Significance: The model performs well on synthetic datasets. However it would be more impactful if the authors could demonstrate improved performance on realworld datasets such as molecules or medical data. Additional experiments are necessary to support the claim of generating accurate graphs using the QM9 dataset. Questions: 1. Can the model generate node and edge features for realworld datasets Exploring this aspect could enhance the models potential. 2. a) Are the graph statistics used for evaluation or reflected in the objective function assumed to follow Gaussian distribution b) Can alternative methods be used to reflect graph statistics without relying on the calibrated Gaussian framework 3. It would be beneficial to provide evidence showing that the proposed model improves upon the \"connected components\" aspect highlighted in Figure 1."], "pMumil2EJh": ["Summary: Key Points:  Utilizes dynamic correlation graphs to enhance time series forecasting.  Leverages graph neural networks (GNNs) to extract higherorder data representations.  Employs timevarying coefficients (later the author argues that this is not explicitly the case).  Provides theoretical comparisons focusing on the papers graphbased component.  Demonstrates strong numerical results across multiple datasets. Strengths:  Explores forecasting using dynamic graphs which is relatively underexplored.  Establishes connections between correlation graphs and GNNs.  Strong numerical results support the proposed method.  Offers some theoretical insights into the role of graph filters (though not the entire method). Weaknesses:  Overclaims Contribution: Asserts that it proposes solutions for learning on timevarying graphs but later acknowledges prior work in the area.  Limited Related Work: Fails to compare the papers contribution with recent research on polynomial filtering for multivariate time series (MTS).  Ignores Causation: Focuses primarily on correlation as an aid to forecasting overlooking the importance of causation in reallife MTS.  Incomplete Theoretical Analysis: Presents theoretical results that are not directly applicable to the overall method focusing mainly on the graph filter.  Unclear Benefits: It is uncertain whether the benefits arise from the graphbased encoder with its dynamics or the nongraphbased decoder. Questions: 1. Causation Limitation: The paper should acknowledge the limitations of using correlation graphs as causation is often more significant in reallife MTS. 2. Scientific Challenges: The paper should further explore the scientific challenges associated with using correlation graphs which may not always provide benefits. 3. Comparison with Alternative Approaches: The paper needs to contrast its contribution with nonGNN graphbased methods and GNNs on dynamic graphs. 4. Addressing Sensitivity: The authors should elaborate on why their proposed model does not suffer from the sensitivity issues mentioned for other models that use dynamic correlation graphs. 5. Key Contribution: The papers introduction should clearly highlight its key contribution in relation to existing literature. 6. GNN and Polynomial Graph Filter Discussion: The papers discussion of GNNs and polynomial graph filters should be reorganized to provide a clearer understanding of the differences and their relevance to the proposed method. 7. Empirical Observations: The paper should provide insights into the benefits and limitations of using average coefficients in their model. 8. Data Generation: The authors should clarify the assumption of independent and identically distributed entries in the generated data."], "mowt1WNhTC7": ["Summary: The study examines the errors made by topperforming image classifiers on the ImageNet dataset. Despite achieving high accuracy these models still make mistakes which are analyzed to identify remaining challenges in ImageNet classification. The paper also proposes ImageNetM a curated subset of ImageNet to evaluate future models ability to address these challenges. Strengths:  Provides detailed insights into the nature of errors made by current models highlighting the need to go beyond simple accuracy metrics.  Introduces ImageNetM as a benchmark for assessing models performance on specific challenging cases. Weaknesses:  Concerns about the representativeness of ImageNetM as it is based on the intersection of errors made by a limited number of models.  Questions about the added value of ImageNetM compared to existing evaluation metrics such as multilabel accuracy (MLA).  The qualitative analysis of the 68 ImageNetM samples is not fully explored in the paper. Conclusion: The analysis of remaining errors in current models is valuable. However the effectiveness of ImageNetM as a benchmark for future models needs further verification. The papers contribution could be considered for publication with some revisions to address these concerns."], "prQkA_NjuuB": ["Summary: This paper presents two methods to represent divergencefree vector fields in any dimension. These methods provide an extension to the common practice of using the curl of a vector field as a divergencefree vector field in 3D space. The derivations rely on differential forms and are based on first principles. The methods have distinct characteristics:  Matrix Representation: Scales quadratically with dimension.  Vector Representation: Requires secondorder derivatives but employs a compact vector representation allowing for constraints like positivity. Applications: These parameterizations find use in various areas:  Compressible Fluid Modeling: Reformulating fluid velocity and density as a divergencefree vector field enables building neural networks that conserve mass by design.  Maxwell Equations: Similar construction can be applied to solve Maxwell equations.  Optimal Transport: One parameterization is adapted to enforce density constraints (positivity within 0 and 1) for optimal transport applications. Strengths and Weaknesses:  Significance:  [Medium] Proposes a general parameterization for divergencefree vector fields.  [Medium] Introduces a reformulation of continuity equations that enables finding divergencefree vector fields.  [High] Provides a neural network architecture that inherently conserves mass addressing a challenge in physicsinformed neural networks.  [High] Allows for solving optimal transport problems with enforced mass conservation constraints.  Originality:  The construction of divergencefree vector fields using neural networks is novel.  The reformulation of the mass conservation equation is new to the machine learning community.  Quality:  The approach appears technically sound and complete.  Experiments provide qualitative proof of concept.  The universality proof appears valid but requires familiarity with differential forms.  No code is provided.  Clarity:  The overall structure is wellorganized.  The section on differential forms lacks clarity and detail making it difficult to follow for those unfamiliar with the subject.  Some notation is confusing. Question: In 2D fluid flows the augmented vector field would be 3D resulting in a curl equation. How would this compare to the matrix and vector representations in practice Would the matrix representation remain advantageous in low dimensions"], "xvZtgp5wyYT": ["Summary: The paper presents a neural partial differential equation (PDE) solver that employs a latent global state representation to reduce the computational cost of PDE approximation while maintaining accuracy. The solver can potentially be applied to inverse problem solving. Strengths:  Originality and simplicity of approach.  Competitive or superior performance compared to existing methods.  Thorough experimental evaluation.  Clear writing style (with some exceptions).  Significance of neural PDE solvers. Weaknesses: Minor points:  Typos and confusingly worded sentences.  Lack of clarity in some statements (e.g. line 205 354).  Insufficient explanation of \"neural operators\" (line 107). Major questions:  Ambiguity in the objective function (line 127): Integration over x may be missing.  Large accumulated error in the WEBO5 ground truth method in Table 1. Additional Details: The LEPDE solver is unique in its use of a latent global state to represent the system and reduce the computational cost. The method can also be applied to inverse problems where it aims to uncover unknown parameters that govern the PDE system."], "tPiE70y40cv": ["Paraphrased Summary: The paper presents an algorithm that creates a compressed representation (coreset) from relational data without physically combining all the tables (materialization). It outperforms existing methods by supporting tasks beyond clustering and by limiting coreset size to avoid exponential growth with the number of tables. The resulting coresets guarantee an error bound on predictive models that combines multiplicative and additive components. The algorithm builds a hierarchical tree structure from the individual tables. At each level it applies Gonzalezs clustering algorithm to create local coresets that are then merged in parent nodes (pseudocubes). The weights of points in the final coreset are calculated by counting their inclusion in the intersection of pseudocubes and the original dataset. Sampling helps prevent overcounting due to overlapping pseudocubes. Experimentation and Evaluation: Experiments on three datasets demonstrate the algorithms effectiveness for kmeans clustering SVM and logistic regression. Compared to materialization SVM and LR show one to two orders of magnitude speedup. In clustering it outperforms Rkmeans for more than three tables in terms of time and solution quality. Strengths and Weaknesses: Compared to related work (Rkmeans) the algorithm has clear advantages: it supports multiple tasks avoids exponential coreset growth and yields faster execution. Technical Questions: 1. Concept Definitions:  Diameter of a dataset (\u0394): Maximum distance between any two points in the dataset.  Optimal radius of kcenter clustering: Radius that minimizes the maximum distance between any point and its closest cluster center.  Additive inequalities: Inequalities that allow for errors that increase with the size of the dataset. 2. Proof of Claim 1: Claim 1 states that a coreset with a fixed number of points can approximate the empirical risk of a large dataset within a bounded error. Its proof is based on the properties of Gonzalezs clustering algorithm and concentration inequalities. 3. Combining Nodes from Different Levels: The hierarchical tree structure assumes that each level is built from two nodes at the previous level. However if the number of tables is not a power of 2 combining nodes from different levels does not impact the correctness of the algorithm. 4. Binary Attributes in Home Credit Dataset: Despite having binary attributes the method can still be applied because the clustering algorithm employed (Gonzalezs algorithm) does not require continuous data."], "wFymjzZEEkH": ["Paraphrased Statement: Summary: The paper introduces a personalized Federated Learning (FL) technique designed to enhance robustness fairness and communication efficiency simultaneously. To ensure fairness it employs personalization and regularization techniques similar to previous studies. Additionally it proposes a lowdimensional random projection of local client updates to optimize communication costs. Empirical evidence shows significant improvements in these aspects compared to existing baselines. Strengths:  The theoretical underpinnings of the paper appear valid supported by numerical experiments.  It addresses three crucial challenges in FL settings (communication efficiency fairness robustness) comprehensively addressing a gap in current research. Weaknesses:  The novelty of the method may be somewhat limited with the main contribution being the dimensionality reduction of local updates.  Although the personalized accuracy performance is claimed to be superior to competitors the results in Table 3 (Appendix) suggest mixed outcomes.  The communication performance comparison is incomplete omitting key existing works.  There is no evaluation or discussion of the approachs robustness against attacks like backdoors. Questions:  The poor robustness performance of Ditto on the EMNIST dataset differs from reported results on FEMNIST. Can you explain this discrepancy  Have you optimized the hyperparameters for the Ditto algorithm to ensure optimal performance"], "wjSHd5nDeo": ["Paraphrased Statement: Neural Image Compression (NIC) commonly employs a VAE objective for optimization equating the ratedistortion objective to the Evidence Lower Bound (ELBO). However certain unique aspects arise:  The inference networks posterior is assumed to be uniformly distributed to mimic quantization noise. This paper thoroughly examines:  The impact of this assumption on different gradient estimators.  Adaptations necessary for multisample objectives like IWAE targets. The proposed MSNIC objective yields 25 improvement in BDrate (measured by PSNR) for two wellknown image compression approaches. Strengths and Weaknesses:  The paper provides a detailed analysis of the relationship between NIC objectives and VAEs.  The MSNIC objective appears wellreasoned and shows promising empirical results.  However the trainingonly nature of the multisample objective raises questions about its impact on inference performance.  The interplay between quantization and noise mismatch and its effect on overall analysis need to be clarified.  The training time implications of the proposed objective should be considered. Questions:  Does the \"tighter ELBO\" achieved by MSNIC improve the inference objective  How does the quantization vs. noise mismatch affect the analysis  What are the tradeoffs between the proposed objective and training time"], "kcQiIrvA_nz": ["Original Statement: This paper introduces two methods (UBWC and UBWP) for detecting unauthorized use of opensource datasets. The authors train a watermark generator (G) to minimize a loss function that incorporates the difference between the watermarking and the true label. They claim that the proposed watermarking method is robust against dispersibility and enables the detection of watermarked data by comparing the predictions made by a malicious network on watermarked and clean test data. Strengths:  The method makes malicious networks predict random labels for watermarked data.  The authors provide comparisons to other data poisoning methods on CIFAR10 and an ImageNet subset.  The method is robust against transferability stealthiness and defenses. Weaknesses:  The method reduces benign accuracy on CIFAR10 and ImageNet.  The method has only been tested on ResNet18 and smallscale datasets.  The comparisons do not include similar works or a range of backdoor attackdata poisoning approaches.  The method requires knowledge of the malicious models architecture. After AuthorReviewer Discussion: The authors have addressed the concerns raised in the first review. They have demonstrated the applicability of their method to largescale datasets transferability and stealthiness. Strengths:  The method makes malicious networks predict random labels for watermarked data.  The method is robust against transferability stealthiness and defenses.  The authors have provided extensive experimental results on various backdoor attackdata poisoning approaches. Weaknesses:  The method reduces benign accuracy on CIFAR10 and ImageNet.  The method requires knowledge of the malicious models architecture. Overall: The authors have addressed the weaknesses identified in the first review demonstrating the value and effectiveness of their method. It is an insightful and novel approach that can improve the detection of unauthorized use of datasets."], "uKYvlNgahrz": ["Summary: Researchers have created a new type of neural network layer called a \"monotonic dense layer\" that can learn nonconvex functions. This new layer can be combined to form deeper neural networks that are still monotonic and have better performance than other methods. Strengths:  Easy to use: The new layer uses existing components in popular deep learning frameworks.  Extensible: The layer can be combined to create deeper neural networks.  Strong results: Models using the new layer have fewer parameters and better quality. Weaknesses:  Missing definitions: Some key terms in the paper need to be defined for clarity.  Limited stacking: The new layer can only be stacked with other monotonic layers which could restrict the development of deeper networks. Questions: 1. Can the method be adapted to create monotonic layers other than dense layers (e.g. selfattention or convolutional layers) 2. Can the method be compared to other methods by performing hyperparameter tuning on one of the other methods in the paper to demonstrate the advantage of the new architecture"], "wQVjGP5NbP9": ["Paraphrased Statement: Summary: The authors developed a differentially private algorithm for correlation clustering. They used graph notation to add noise to vertex degrees and edges. They then defined vertices as light or heavy based on the number of noisy edges. Edges were discarded and heavy vertices were outputted as clusters. An approximate method was also presented. Strengths:  The authors provided thorough analysis and time complexity.  The application of differential privacy to correlation clustering is novel. Weaknesses:  The paper contains minor issues such as clarifying negative edge values.  The privacy budget calculation may be incorrect due to the dependence of vertex degrees. The actual budget may be higher than stated.  The arXiv references should be updated for the final submission. Questions:  The term \"(00)differential private\" is not widely used. Consider using \"postprocessing\" or \"privacy budgetneutral\" instead."], "m6HNNpQO8dc": ["Paraphrase: This paper introduces novel activation functions derived from relaxed versions of the logical operators AND OR and XNOR. The authors also propose efficient approximations using simple max min and addition operations. This approach is motivated by the behavior of these operators under logit space equivalence. As the binary operators are reduction operations (converting two inputs to one) the authors link them to the concept of MaxOut. Additionally they offer ensemble alternatives to integrate these logical activation functions into neural network architectures. Empirical results demonstrate the performance of various combinations of these logical activation functions across different datasets and architectures. Strengths and Weaknesses: This paper makes a valuable contribution to the field by introducing new activation functions. The authors acknowledge the potential weakness of making independence assumptions but these do not seem to impact model performance. The empirical evaluation is rigorous and shows that the activation functions perform well sometimes even improving model performance. The authors achieve this without substantially increasing the number of parameters by using ensembles of activation functions. This is a significant improvement over previous methods like MaxOut which required many more weights for the same number of outputs. Questions:  Could these activation functions facilitate explainability in neural networks  Is it possible to extend this approach to compose more complex logical functions such as implication into activation functions"]}