{"wS23xAeKwSN": " Peer Review: 1) Summary: The paper introduces PolarMix a novel data augmentation technique for LiDAR point clouds aimed at improving training data efficiency and performance across various tasks and scenarios. PolarMix consists of scenelevel swapping and instancelevel rotation and paste to enrich point cloud distributions while maintaining fidelity. Extensive experiments demonstrate the superiority of PolarMix in improving network training for semantic segmentation object detection and unsupervised domain adaptation. 2) Strengths and Weaknesses: Strengths:  Novelty: PolarMix introduces a unique approach to data augmentation for LiDAR point clouds addressing the limitations in existing augmentation methods.  Performance: Extensive experiments show the consistent improvement in performance across different perception tasks datasets and domains.  Applicability: PolarMix is shown to work effectively with various 3D architectures and offers a plugandplay solution for LiDAR point cloud networks.  Generalization: The method demonstrates robustness and generalization capabilities across different network architectures and datasets. Weaknesses:  Evaluation Metrics: While the experiments have shown promising results across different tasks more indepth analysis on specific scenarios or failure cases could provide additional insights.  Comparison: Although comparisons with baseline methods are provided a more detailed comparison with stateoftheart methods in data augmentation could strengthen the evaluation. 3) Questions:  How does PolarMix handle outlier points or noise in LiDAR scans during the augmentation process  Can the instancelevel rotation and paste approach introduce any biases in the augmented data especially in cases with limited instances of certain classes  How scalable is PolarMix in terms of computational complexity as the size of LiDAR scans increases 4) Limitations:  The experiments focus on specific tasks like semantic segmentation and object detection. Further evaluation on different tasks such as LiDAR reconstruction or classification could provide a more comprehensive assessment of PolarMixs effectiveness.  The study mainly uses synthetic datasets for unsupervised domain adaptation. Evaluating on realworld datasets could enhance the practical validity of the findings. In conclusion the paper introduces a significant contribution to the field of LiDAR point cloud data augmentation with PolarMix. The novelty applicability and generalization capabilities of PolarMix make it a valuable tool for enhancing the learning from limited LiDAR point cloud data. Further exploration and refinement of PolarMix in diverse scenarios and tasks could lead to broader applications in autonomous systems and related fields.", "l2CVt1ySC2Q": "Peer Review 1) Summary: The paper investigates the excess capacity of deep networks in the context of supervised classification. It extends Rademacher complexity bounds to account for convolutional layers in modern architectures. The study shows that there is a substantial amount of excess capacity per task and the capacity can be maintained at a similar level across tasks. The experiments are conducted on benchmark datasets of varying task difficulties. 2) Strengths:  The paper addresses an important problem of understanding the capacity and generalization of deep networks.  The extension of Rademacher complexity bounds to accommodate convolutional layers and function composition is a significant contribution.  The experiments on benchmark datasets provide empirical evidence supporting the theoretical findings.  The detailed analysis of the relationship between task difficulty capacity and generalization performance is wellsupported by a rigorous theoretical framework. Weaknesses:  The paper could benefit from clearer explanations in certain sections especially regarding the technical details of the Rademacher complexity bounds.  The constraints enforced during optimization are crucial but more details on the practical implementation could enhance the clarity of the methodology.  The paper could include a discussion on the computational complexity implications of the proposed excess capacity analysis. 3) Questions:  How sensitive are the empirical results to the choice of hyperparameters such as learning rate weight decay and batch size  Could variations in the network architecture (besides ResNet18) impact the observed excess capacity effects  Have you explored the impact of data augmentation techniques on the excess capacity analysis 4) Limitations:  The study focuses on Rademacher complexity and weight norms as measures of capacity potentially overlooking other aspects like activation functions or network depth.  The approach to constraining capacity through weight norms and Lipschitz constants might oversimplify the complex dynamics of deep networks.  The evaluation is limited to specific datasets and generalizability to different types of tasks or datasets should be explored. Overall the paper makes a valuable contribution to understanding excess capacity in deep networks and provides insights into the compressibility of networks with respect to weight norms. Addressing the weaknesses and questions raised could further enhance the quality and impact of the research.", "tlUnxtAmcJq": "Peer Review Summary: The paper introduces the SO3KRATES model a selfattention based messagepassing neural network designed to address nonlocal electronic effects in quantum chemistry through the use of spherical harmonic coordinates (SPHCs). The proposed model separates atomic features from SPHC information allowing for efficient modeling of nonlocal effects and achieving stateoftheart performance with reduced complexity and improved data efficiency. Experimental results demonstrate the models capability to describe nonlocal effects over arbitrary length scales outperforming existing benchmarks with significantly fewer parameters and faster training and inference times. The paper presents a wellstructured and thorough study that addresses a crucial challenge in quantum chemistry applications. Strengths: 1. Innovation: The paper introduces a novel approach using SPHCs to model nonlocal effects in quantum chemistry efficiently leading to improved performance and reduced complexity. 2. Performance: The SO3KRATES model shows superior performance compared to existing benchmarks achieving stateoftheart results with fewer parameters and faster computation times. 3. Experimental Rigor: The experiments conducted are comprehensive demonstrating the models ability to capture nonlocal effects improve data efficiency and generalize well across different molecular structures. Weaknesses: 1. Complexity: While the proposed model simplifies geometric representations the utilization of SPHCs might introduce added complexity during implementation and interpretation. 2. Evaluation Metrics: The paper could benefit from a more detailed discussion on the evaluation metrics used and their implications for model performance especially in the context of nonlocal effects. Questions: 1. Can the authors elaborate on the tradeoffs between model complexity and performance as well as the practical implications of using SPHCs in quantum chemistry applications 2. How does the SO3KRATES model compare with existing approaches in terms of interpretability and ability to capture complex molecular interactions beyond nonlocal effects Limitations: 1. The proposed SO3KRATES model may have limitations in cases where nonlocal interactions are less critical or in scenarios with highly dynamic systems where longerrange interactions play a minimal role. 2. The current study focuses primarily on quantum chemistry applications and the generalizability of the SO3KRATES model to other fields could be further explored and validated. Overall Assessment: The paper presents a significant advancement in addressing nonlocal effects in quantum chemistry using the SO3KRATES model. The research is wellstructured and meticulously executed showcasing the models capability to outperform existing benchmarks while reducing complexity. Further clarification on the models interpretability and practical implementation can enhance the papers impact and broaden its applicability in diverse scientific domains.", "nosngu5XwY9": "Peer Review Summary: The paper introduces a novel framework called Dynamic Inverse Reinforcement Learning (DIRL) for understanding animal decisionmaking behavior in complex environments such as mazes. The study proposes a method to infer timevarying reward functions of animals through observed trajectories and demonstrates its application on simulated and real mouse data. The results indicate that DIRL outperforms existing IRL methods in explaining animal behavior offering interpretable reward functions and capturing exploratory behavior. The authors discuss the implications of their work and suggest potential future applications in neuroscience and beyond. Strengths: 1. Novelty: The introduction of DIRL as a method for inferring timevarying reward functions in animal decisionmaking is a novel contribution to the field of neuroscience and reinforcement learning. 2. Applicability: The demonstration of DIRL on both simulated and real mouse data showcases the methods applicability to diverse environments and datasets highlighting its potential for elucidating complex behaviors in animals. 3. Interpretability: The method provides interpretable reward functions such as identifying specific reward maps like \"water\" and \"home\" and capturing exploratory behavior through the inferred \"explore\" map enhancing the understanding of animal decisionmaking. Weaknesses: 1. Data Requirement: The framework necessitates a large number of trajectories for inferring animal reward functions which can limit its applicability to scenarios with limited data availability or highdimensional state spaces. 2. Scalability: The scalability of the approach to highdimensional state spaces may pose a challenge due to the separate reward learning for each state in each goal map. Further exploration on scaling techniques could enhance the methods versatility. 3. Assumption of Policy Form: The reliance on the Boltzmann policy for inferring rewards might limit the generalizability of the framework especially in scenarios where different policy forms are more suitable. Questions: 1. Generalization: How generalizable is the DIRL framework to other animal species or decisionmaking tasks beyond maze navigation Are there specific considerations needed for applying this method to different contexts 2. Cognitive Factors: To what extent does the framework account for cognitive elements like memory learning or external influences on animal decisionmaking processes and how could these factors be incorporated into the modeling approach Limitations: 1. Limited State Space: The studys focus on a 127node labyrinth environment might restrict the generalizability of the findings to more complex or varied environments where animal decisionmaking occurs. 2. Behavioral Dynamics: While the method effectively captures certain behavioral aspects like exploration limitations in understanding more nuanced or complex behaviors especially across different cohorts could be a potential drawback. Overall the paper presents a promising approach in the field of neuroscience by introducing DIRL for inferring timevarying reward functions in animal decisionmaking. The demonstration on realistic data and the discussion of implications and future directions enhance the significance of the study within the scientific community. Further improvements in scalability data requirements and generalizability could strengthen the frameworks utility in diverse research contexts.", "uNYqDfPEDD8": " Peer Review:  1) Summary: The paper proposes a neural mixedsize macro placement and routing pipeline for VLSI chip design. It presents an RLbased model for mixedsize macro placement and a conditional generative routing model. The joint placement and routing network is novel and performs well on benchmarks showcasing its effectiveness.  2) Strengths and Weaknesses: Strengths:  The paper addresses a significant challenge in VLSI chip design.  The proposed neural pipeline integrates placement and routing efficiently.  Experimental results demonstrate competitive performance and costefficiency.  The approach of oneshot generative routing model is innovative and effective. Weaknesses:  The paper lacks detailed comparison with more recent stateoftheart techniques.  Limited datasets for training of the generative routing model.  The optimization for runtime speedup through concurrent routing may impact wirelength and overflow metrics.  3) Questions:  How does the proposed method compare to other deep learningbased approaches in terms of scalability and generalizability  What are the challenges faced in the implementation of the proposed pipeline on realworld largescale chip designs  Is the concurrent routing approach suitable for scenarios where wirelength optimization is a critical factor  4) Limitations:  The training dataset for the generative routing model is limited potentially impacting its generalizability to diverse scenarios.  The initialization of the placement network with random weights may affect the convergence of routing solutions.  The lack of extensive benchmarking against other deep learning algorithms in the field is a limitation in assessing the competitiveness of the proposed approach. In conclusion the paper presents an innovative neural pipeline for VLSI chip design demonstrating promising results in joint placement and routing tasks. However further exploration and validation are required to establish the scalability and robustness of the proposed approach in realworld chip design scenarios.", "q__FmUtPZd9": " Peer Review   1) Summary The paper introduces a new problem of inverse decisionmaking with task migrations in the context of social contagion management. The main aim is to investigate if one decisionmaking task can be solved by using querydecision pairs from another task sharing the same underlying model. The proposed framework called SocialInverse is evaluated using empirical studies on various graph structures. The results demonstrate the effectiveness of SocialInverse compared to other traditional learningbased and graphbased methods.   2) Strengths and weaknesses  Strengths: 1. Novelty: The paper introduces a novel problem of inverse decisionmaking with task migrations in a welldefined context of social contagion management. 2. Proposed Framework: The development of the SocialInverse framework is wellthoughtout and theoretically justified. 3. Empirical Studies: The empirical studies provide valuable insights into the effectiveness of the proposed method across different graph structures and compared to baseline methods. 4. Theoretical Analysis: Theoretical analysis provides a solid foundation for the generalization performance of the framework.  Weaknesses: 1. Dataset: The paper lacks details about the dataset used in the empirical studies such as data sources preprocessing steps and data characteristics. 2. Assumptions: The assumption that the training set contains approximation solutions may not always hold true in practical scenarios limiting the generalizability of the proposed method. 3. Experiments: While the empirical studies demonstrate the effectiveness of the SocialInverse framework more indepth analysis and comparisons with different parametric settings could enhance the robustness of the findings.   3) Questions 1. Generalization Performance: How sensitive is the generalization performance of the framework to parameters such as K \u03b2 or the empirical distribution 2. Effective Use of Samples: Can the paper elaborate on the conditions under which the querydecision pairs of one optimization problem effectively inform the solutions of another problem under the same underlying model 3. Scalability: Have the authors considered the scalability of the framework to graphs of extreme scales in realworld applications 4. Comparison Methods: What are the specific reasons behind the poor performance of learningbased methods in certain scenarios and how can these limitations be addressed   4) Limitations 1. Sample source: The assumption that the training set contains approximation solutions is a strong theoretical requirement that may not always be met in practice. 2. Dataset Details: Lack of information about the dataset used in the empirical studies limits the reproducibility and comparability of the results. 3. Realworld Applicability: The effectiveness of the proposed method in practical realworld scenarios and with largescale graphs remains to be validated. 4. Algorithmic Complexity: Further discussion on the computational complexity and efficiency of the proposed framework especially for large graphs would be beneficial.  In conclusion the paper provides a significant contribution to the field of inverse decisionmaking with task migrations in social contagion management. Addressing the limitations and questions raised could further enhance the credibility and applicability of the proposed SocialInverse framework. Additional clarity on the dataset assumptions and experimental setup would improve the overall impact of the research.  Overall the paper shows promise in addressing complex decisionmaking tasks and presents a valuable framework for future research in the domain of social contagion management.  (298 words)", "pgF-N1YORd": " Peer Review: 1) Summary: This study delves into the challenge of transfer learning in continual reinforcement learning particularly focusing on the factors influencing transfer efficacy within the SAC algorithm. Through a systematic investigation using twotask and full continual learning scenarios the study uncovers the critical role of the critic and provides insights on the impact of different components such as exploration and actor networks on transfer. The proposed method ClonExSAC showcases significant improvements in performance and forward transfer compared to existing benchmarks. 2) Strengths and weaknesses: Strengths:  The paper provides a comprehensive analysis of transfer learning in continual reinforcement learning addressing the specific mechanisms within the SAC algorithm.  Systematic exploration of different components effects such as critic actor and exploration on transfer efficacy.  The study offers practical recommendations and proposes a novel method ClonExSAC demonstrating superior performance in the Continual World benchmark.  Rigorous experimental setup utilizing diverse CL methods and examining various scenarios to draw nuanced conclusions. Weaknesses:  Limited discussion on the generalizability of findings beyond the SAC algorithm and Continual World benchmark.  The study could benefit from a more indepth discussion on the theoretical underpinnings of transfer learning in continual reinforcement learning.  Lack of realworld application examples or demonstrations to provide practical insights for implementation. 3) Questions:  How do the findings of this study align with existing research on transfer learning in the context of continual reinforcement learning  Are the recommendations and insights derived from the study applicable to other RL algorithms and diverse continuous control environments  Can the proposed ClonExSAC method be extended or adapted for transfer learning scenarios in domains outside continual reinforcement learning 4) Limitations:  The study primarily focuses on the SAC algorithm and Continual World suite limiting the generalizability of findings to other RL algorithms and environments.  The need for retaining data from previous tasks in ClonExSAC may pose challenges in scenarios with privacy concerns or data storage constraints.  The study acknowledges certain limitations in the broader spectrum of transfer learning in continual reinforcement learning indicating the need for further research to address these gaps. In conclusion the paper presents valuable insights into transfer learning mechanisms in continual reinforcement learning offering a novel method ClonExSAC that outperforms existing benchmarks. While the study contributes significantly to the understanding of transfer in CRL further research is warranted to explore the scalability and applicability of these findings in diverse RL settings and realworld applications.", "rUc8peDIM45": " Peer Review:  1) Summary: The paper explores the phenomenon of stochastic gradient descent (SGD) favoring flat minima and provides a detailed explanation by relating the noise structure of SGD to its linear stability. Through theoretical analysis and empirical validation on multiple models including linear networks random feature models and nonlinear networks the authors establish a connection between linear stability and flatness of minima accessible to SGD. The key findings include the alignment property of SGD noise and the quantitative characterization of how learning rate and batch size affect the flatness of minima. The research shows how SGD escapes from sharp minima exponentially fast due to noise dependence and noise direction awareness.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important and relevant topic in machine learning about the implicit regularization mechanism of SGD.  The theoretical analysis is rigorous and supported by empirical experiments on various models indicating the validity and practical relevance.  The alignment property of SGD noise and the impact on linear stability provide valuable insights into the behavior of SGD in selecting flat minima.  The paper makes a significant contribution to understanding the noise structure of SGD and its role in training deep learning models. Weaknesses:  The paper could benefit from more indepth discussions on the broader implications and applications of the findings beyond the specific models used in the experiments.  Some parts of the paper may be difficult to follow for readers not wellversed in the technical details of machine learning and optimization.  3) Questions:  How generalizable are the findings to different types of neural network architectures and datasets beyond CIFAR10  Can the alignment property of SGD noise be extended to more complex models and practical scenarios in largescale machine learning tasks  Are there practical implications for hyperparameter tuning and optimization algorithm design based on the insights provided in the paper  4) Limitations:  The paper focuses on specific neural network models and may lack generalizability to a wider range of architectures and datasets.  The theoretical analysis while comprehensive might require a high level of expertise in machine learning theory for full comprehension.  The empirical experiments while supportive may benefit from additional datasets and benchmarks to strengthen the generalizability of the findings. Overall the paper provides substantial insights into the relationship between noise structure linear stability and flatness of minima in SGD optimization. More clarity in the practical implications and broader implications of the research would enhance its impact in the machine learning community. Further exploration of the noise structure across different architectures and datasets could deepen the understanding of SGD behavior in complex learning scenarios.", "nRcyGtY2kBC": " Summary This research paper addresses the problem of heterogeneous transfer learning for the estimation of conditional average treatment effects (CATE) in various domains. By leveraging related information from a source domain with different feature spaces the proposed Heterogeneous Transfer Causal Effect (HTCE) learners aim to improve CATE estimation on a target domain with limited training examples. The study introduces building blocks that handle heterogeneous feature spaces and share information between potential outcome (PO) functions across domains. Extensive experiments on a new semisynthetic data simulation benchmark demonstrate that the HTCElearners outperform traditional CATE learners providing new insights into the differences between these learners from a transfer perspective.  Strengths 1. Innovation: The paper addresses a novel problem of heterogeneous transfer learning for CATE estimation which is crucial for decisionmaking in various domains. 2. Comprehensive Approach: The study introduces building blocks that handle both heterogeneous feature spaces and share information between PO functions across domains providing a flexible transfer learning framework. 3. Experimental Validation: Extensive experiments on a semisynthetic benchmark demonstrate the superiority of HTCElearners over traditional CATE learners providing insights into performance improvements. 4. Clear Presentation: The paper is wellorganized and provides detailed explanations of the proposed methods datasets used and results obtained.  Weaknesses 1. Complexity: The paper introduces several technical terms and concepts potentially making it challenging for readers unfamiliar with the subject area to grasp the content. 2. Evaluation Limitation: The evaluation is primarily focused on the PEHE metric and additional evaluation metrics or realworld case studies could enhance the practical applicability of the proposed approach. 3. Theoretical Guarantees: While the experimental results are promising the paper lacks theoretical guarantees on the performance of the HTCElearners which could strengthen the validity of the proposed methods.  Questions 1. How does the proposed transfer learning framework handle potential biases in the dataset particularly in the context of causal inference 2. Are there additional metrics or methods that could provide further insights into the performance of the HTCElearners beyond the PEHE metric 3. How generalizable are the findings of the study across different domains beyond the healthcare setting considered in the paper 4. Can the HTCElearners be adapted to handle more complex treatment settings such as multiple treatments or temporal aspects of treatments  Limitations 1. Generalizability: The study primarily focuses on the binary treatment setting limiting the generalizability of the findings to more complex treatment scenarios. 2. Theoretical Validation: While extensive experimental results are provided the paper lacks theoretical guarantees on the performance and robustness of the proposed HTCElearners. 3. Evaluation Metrics: The evaluation relies heavily on the PEHE metric and the inclusion of additional metrics could provide a more comprehensive understanding of the proposed transfer learning approach.", "s1yaWFDLxVG": "Abstract: The paper investigates the sensitivity of firstorder optimization to assumptions made on objective functions. It introduces a new class of functions RSI\u2212 and EB which provide interesting geometrical properties and analytical solutions for the performance estimation problem. The study shows gradient descent to be exactly optimal on functions satisfying these conditions among all firstorder algorithms. Strengths: 1. The paper addresses the sensitivity of optimization to assumptions providing a robust alternative in RSI\u2212 and EB. 2. The analytical solutions for the performance estimation problem provide a clear understanding of convergence rates. 3. The proofs and theoretical analysis are detailed and provide a solid foundation for the claims made in the paper. Weaknesses: 1. The paper could benefit from more practical validation beyond theoretical analysis. 2. The experimental results could be expanded to include a wider range of scenarios to reinforce theoretical claims. 3. The assumptions made in RSI\u2212 and EB may pose limitations when applied to realworld datasets. Questions: 1. How do RSI\u2212 and EB conditions compare to existing optimization assumptions like strong convexity and smoothness in terms of practical applicability 2. Can the conditions RSI\u2212 and EB be extended or modified to address potential limitations in certain optimization scenarios 3. Are there any specific implications for RSI\u2212 and EB in solving optimization problems in highdimensional spaces or with complex loss landscapes Limitations: 1. The focus on worstcase analysis may not capture the full practical performance of algorithms in realworld scenarios. 2. The empirical validation is limited to a specific experiment on ResNet18 and CIFAR10 which may not fully represent the diversity of applications. 3. The generalizability of the findings to a wide range of optimization problems may be restricted by the specific conditions imposed by RSI\u2212 and EB.", "plu6AK3qs5T": " Summary The paper introduces an algorithm called AutoClipping which removes the need to tune the clipping threshold for achieving high accuracy in differential private (DP) training for deep learning models. The AutoClipping method is shown to be as private and efficient as existing DP optimizers but does not require DPspecific hyperparameters. The paper provides a rigorous convergence analysis of automatic DPSGD in the nonconvex setting demonstrating that it can match the convergence rate of standard SGD. Experimental results on various tasks show that automatic clipping outperforms or matches the stateoftheart and requires minimal changes to existing codebases.  Strengths 1. Innovative Approach: The introduction of AutoClipping to remove the need for tuning the clipping threshold for DP training is a novel and valuable contribution to the field. 2. Theoretical Analysis: The paper provides a detailed theoretical analysis and convergence guarantees for the proposed automatic DPSGD. 3. Empirical Validation: Experimental results on various language and vision tasks demonstrate the effectiveness of automatic clipping and its superiority over existing approaches. 4. Code Snippets: Inclusion of simple code snippets in the Appendix to demonstrate the ease of implementation and use of AutoClipping.  Weaknesses 1. Privacy Considerations: The paper does not deeply discuss the potential negative societal impacts or ethical considerations of the proposed method. This aspect should be elaborated on for a more comprehensive review. 2. Hyperparameter Considerations: The paper could provide a more detailed discussion on the potential impact of the proposed method on other hyperparameters and training considerations.  Questions 1. Transferability: How transferable is the AutoClipping method to a wide range of deep learning models beyond the ones tested in the paper 2. Scalability: How does the performance of AutoClipping scale with the size and complexity of the models being trained 3. Robustness: Are there scenarios or specific types of tasks where AutoClipping may be less effective or exhibit limitations compared to traditional methods  Limitations 1. Dataset Considerations: The specific datasets used for evaluation are not extensively discussed in terms of diversity and representativeness. Including more details on the datasets could strengthen the paper. 2. Assumptions: The paper might benefit from discussing the limitations of the assumptions made in the theoretical analysis especially in the context of realworld applications. 3. Reproducibility: While the paper includes code snippets a link to a publicly accessible code repository for full reproducibility would enhance the credibility of the findings. Overall the paper presents a significant advancement in the realm of DP training for deep learning models with the introduction of AutoClipping. It provides a compelling argument for the effectiveness of the proposed method through a combination of theoretical analysis and empirical validation. Further discussions on societal impacts scalability and potential limitations could enhance the overall impact and relevance of the work.", "uLYc4L3C81A": " Review: 1) Summary: The paper introduces the Confident Adaptive Language Modeling (CALM) framework for dynamically allocating compute resources during text generation tasks using large language models (LLMs). The framework aims to reduce the computational load by allowing early exits from the model with provably maintained high performance. Different confidence measures are proposed and evaluated empirically across diverse NLP tasks showing significant efficiency gains while preserving quality. 2) Strengths and Weaknesses: Strengths:  Novel Framework: CALM introduces a novel approach to dynamically allocate compute resources during text generation tasks addressing the challenges of computational load in LLMs.  Theoretical Analysis: The paper provides theoretical analyses supporting the proposed framework enhancing the credibility of the approach.  Empirical Validation: The empirical experiments on multiple NLP tasks demonstrate the effectiveness of CALM in reducing complexity and accelerating inference while maintaining performance guarantees.  Efficiency Gains: The paper shows substantial speedup gains of up to three times faster while reliably controlling high performance which is a significant contribution to the field. Weaknesses:  Validation: The paper could benefit from a more detailed discussion on the scalability and generalizability of the proposed CALM framework across a wider range of tasks and model sizes.  Limitation in Baselines: The comparison with static baselines could be further expanded to include a broader range of baseline models to provide a more comprehensive evaluation. 3) Questions:  Model Performance: How does the performance of CALM compare with other stateoftheart methods for adaptive computing in LLMs such as knowledge distillation or layer pruning  Scalability: Is the CALM framework scalable to handle larger models and more extensive datasets while maintaining computational efficiency  Realworld Application: How does the CALM framework perform in practical applications and what are the potential challenges in deploying it in realworld scenarios  Interpretabilty: Can the confidence measures proposed in CALM provide insights into model decisionmaking and enhance interpretability of LLMs 4) Limitations:  Task Specificity: The paper focuses on text generation tasks limiting the generalizability of the CALM framework to other NLP domains.  Assumptions: The performance guarantees and confidence measures rely on specific assumptions which may not hold in all scenarios affecting the applicability of the framework in diverse settings.  Complexity: The complexity of implementing the CALM framework especially in production systems and the requirements for tuning the confidence measures may pose challenges in practical applications. In conclusion the paper introduces a promising framework CALM for dynamically allocating compute resources during text generation with LLMs showing significant efficiency gains while maintaining performance guarantees. Further exploration of scalability realworld deployment and generalizability could enhance the relevance and impact of the proposed framework.", "opw858PBJl6": "Peer Review 1) Summary The paper introduces a new approach to evaluating saliency explanations in deep learning models by focusing on the concepts of completeness and soundness. The authors argue that existing methods may prioritize completeness leading to misleading interpretations of model decisions. They propose new evaluation metrics and a simple saliency method that consider both completeness and soundness. The experiments show that incorporating soundness into saliency evaluations can lead to more accurate and reliable explanations. Overall the paper provides a rigorous framework for evaluating saliency methods based on intrinsic properties of the model. 2) Strengths and Weaknesses Strengths:  The paper addresses an important issue in explainability in deep learning by introducing the concepts of completeness and soundness in saliency evaluations.  Introduces new evaluation metrics and a simple saliency method that consider both completeness and soundness.  Conducts thorough experiments to demonstrate the effectiveness of the proposed approach.  Provides theoretical justification for common heuristic techniques like TV regularization and upsampling in saliency methods. Weaknesses:  The paper focuses on intrinsic evaluations and lacks discussions on how the proposed method could be applied in realworld scenarios where interpretability and human understanding are crucial.  The experiments are limited to smaller datasets and the scalability of the proposed method to larger datasets such as ImageNet needs further exploration.  The paper could benefit from more extensive comparisons with existing saliency methods to provide a comprehensive evaluation of the proposed approach. 3) Questions  How does the proposed method account for outliers or noise in the data particularly in complex datasets like ImageNet  Are there specific scenarios or types of models where the completeness and soundness metrics may be more or less effective  How does the new evaluation framework compare to traditional extrinsic evaluation methods in terms of practical applicability and performance 4) Limitations  The paper does not extensively discuss potential biases or limitations that may arise when applying the proposed approach to diverse datasets and models.  It remains unclear how the proposed method would handle scenarios where the model output is ambiguous or conflicting. Overall the paper introduces a novel perspective on evaluating saliency explanations in deep learning models. The proposed concepts of completeness and soundness provide a valuable contribution to the field of model interpretability. Further exploration and validation on larger datasets and realworld applications would enhance the robustness and applicability of the proposed method.", "pFqgUJxXXz": " Peer Review  Summary The paper introduces Adversarial Task Upsampling (ATU) a novel approach for generating tasks in metalearning that aims to meet the criteria of being taskaware taskimaginary and modeladaptive. ATU utilizes an upsampling network to improve the generalization capability of metaknowledge. The paper provides theoretical analysis experimental validations on regression and classification tasks across multiple datasets and comparisons with existing task augmentation strategies. Results show that ATU consistently outperforms stateoftheart methods in both regression and classification tasks.  Strengths  The paper addresses an important issue in metalearning by proposing a novel approach for generating tasks that are taskaware taskimaginary and modeladaptive which has the potential to improve the generalization capability of metalearners.  The experimental validations cover various datasets and metalearning methods providing a comprehensive evaluation of the proposed methods effectiveness and superiority over existing techniques.  The theoretical analysis and empirical results are wellstructured and clearly demonstrate the benefits of ATU in enhancing the metalearning performance on both regression and classification tasks.  Weaknesses  The paper could benefit from more detailed explanation or empirical evidence regarding the robustness of ATU in different scenarios such as significantly smaller or larger metaknowledge models or varying data distributions.  While the paper extensively evaluates the proposed method on various datasets additional insights into the computational efficiency and scalability of ATU in realworld applications would strengthen the practical applicability of the approach.  The experimental results could be further enriched by including analyses on the interpretability of the generated tasks and the impact of hyperparameter choices on the performance of ATU.  Questions 1. Can you provide more insights into the computational complexity of the ATU approach compared to existing task augmentation strategies How does the training time and resource requirements of ATU scale with the number of tasks and the complexity of the base model 2. The paper mentions tackling withintask overfitting and metaoverfitting separately. How does ATU specifically address these two types of overfitting and are there any limitations or tradeoffs associated with each type of overfitting mitigation strategy 3. How does the proposed method generalize to tasks with highly complex underlying distributions such as those with nontrivial relationships between inputs and outputs Are there any limitations in applying ATU to such scenarios without known task distributions  Limitations  Although the paper provides a thorough exploration of the proposed ATU approach and its benefits the limitations section could be expanded to discuss potential challenges or constraints faced by ATU in practical implementations particularly in diverse and dynamic metalearning environments.  Future work could focus on enhancing the interpretability of the generated tasks by ATU and extending the application of the approach to more complex metalearning problems with heterogeneous task distributions and inputoutput relationships. In conclusion the paper presents a significant contribution to the field of metalearning with the introduction of ATU. By enhancing the task generation process with considerations for taskawareness taskimaginary and modeladaptivity ATU showcases improvements in metalearning performance across various datasets and metalearners. The methodological soundness comprehensive experiments and theoretical justifications make this paper a valuable addition to the metalearning literature.  Rating: 910", "mSiPuHIP7t8": " Peer Review: 1) Summary: The paper introduces GraphDE a probabilistic generative framework for addressing the challenges of debiased learning and outofdistribution (OOD) detection in graph data. The authors mathematically formulate and tackle these problems under a unified model that includes a recognition model to infer the environment variable during training. The proposed model is evaluated on diverse datasets with different types of OOD data showing consistent outperformance over strong baselines for both debiasing and OOD detection tasks. 2) Strengths:  The paper addresses important and underexplored problems in graph data namely debiased learning and OOD detection offering a novel and unified approach.  The theoretical formulations and insights provided about the models effectiveness are well articulated.  The proposed GraphDE framework achieves consistent performance improvements over strong baselines across various datasets and tasks.  Experimental results and comparisons with existing methods support the effectiveness of the proposed model.  The clarity of the writing the logical flow of the paper and the provision of source code for reproducibility enhance the value of the research. 3) Weaknesses:  The paper could benefit from more detailed explanations about the datasets used including statistics and why they were chosen. This would provide a clearer understanding of the experimental setup.  While the theoretical justifications and analysis are thorough more empirical insights or visualizations could strengthen the paper by providing a clearer understanding of the models behavior or the impact of different components.  Some technical terms and methodology details may be challenging for readers not deeply familiar with graph neural networks and probabilistic generative models potentially limiting the accessibility of the paper. 4) Questions:  How sensitive is the GraphDE model to hyperparameters or variations in the dataset distributions  Are there any scalability concerns or computational challenges when applying the model to larger or more complex graph datasets  Can the authors elaborate on any realworld applications or scenarios where GraphDE could have a significant impact in practice 5) Limitations:  The evaluation is primarily focused on synthetic and a limited number of realworld datasets potentially limiting the generalizability of the findings.  The discussion on scaling the proposed model to larger datasets or more complex scenarios could provide additional insights into its practical utility. Overall the paper presents a novel and promising approach to address critical challenges in graph data processing offering a solid theoretical foundation supported by experimental results. Addressing the weaknesses and questions raised could further enhance the clarity and impact of the research.", "vExdPu73R2z": " Summary The paper introduces Robust Referring Video Object Segmentation (RVOS) an extended task from Referring Video Object Segmentation (RVOS) without assuming semantic consensus between the linguistic expression and video content. The proposed method incorporates cycle consistency to improve semantic alignment and discriminates between positive and negative videotext pairs. Extensive experiments on new evaluation datasets show stateoftheart performance in RVOS tasks.  Strengths and Weaknesses Strengths: 1. Innovative approach in addressing the semantic consensus issue in RVOS tasks. 2. Introduction of Robust RVOS task that handles unpaired video and text inputs. 3. Use of relational cycle consistency and semantic consensus discrimination to improve accuracy. 4. Comprehensive evaluation using new datasets and achieving stateoftheart performance. Weaknesses: 1. Lack of discussion on computational efficiency and scalability. 2. Limited comparison with additional existing methods on the proposed Robust RVOS task. 3. The limitations of the dataset and potential biases should be further explored.  Questions 1. Can you provide more insights into the practical implications of addressing the false alarm issue in the RVOS task 2. How does the proposed method handle occlusions and object disappearances during video segmentation 3. What measures were taken to ensure the dataset used for evaluation doesnt introduce biases or inaccuracies in the results 4. Have you considered the broader impact of the proposed method beyond the current research scope  Limitations 1. Lack of detailed information on the negative societal impacts and ethical considerations of the research. 2. Limited discussion on potential biases introduced by the dataset used and generalizability of the proposed method. 3. The absence of error bars in reporting experimental results may affect the reproducibility and certainty of the findings. Overall the paper presents a novel approach to enhancing semantic alignment in video object segmentation tasks but further discussions on broader implications biases and scalability are needed for a more comprehensive evaluation.", "uKYvlNgahrz": " Peer Review:  1) Summary: The paper introduces a simple and elegant solution for building constrained monotonic neural networks that can approximate both convex and concave functions. The proposed method involves the constrained monotone fully connected layer and demonstrates matching or superior accuracy compared to stateoftheart methods. Experimental results on various datasets such as COMPAS Blog Feedback Lending club loan data Auto MPG and Heart Disease showcase the effectiveness of the proposed approach. The paper also includes a discussion on the theoretical guarantees and the computational complexity of the method.  2) Strengths and Weaknesses: Strengths:  The paper provides a clear and detailed explanation of the proposed method for building constrained monotonic neural networks.  The experiments are thorough covering multiple datasets and comparing the proposed approach to existing stateoftheart methods.  The theoretical guarantees and proofs support the claims made in the paper.  The discussion on computational complexity and simplicity of the proposed method adds value to the research. Weaknesses:  The paper can benefit from more detailed explanations of certain sections such as activation function choices and their impact on performance.  Further analysis on the robustness of the proposed method to noise and small datasets could strengthen the claims made regarding its effectiveness in realworld scenarios.  3) Questions:  How does the proposed method perform in scenarios with noisy data and fewer monotonic features compared to the synthetic examples provided  Are there any specific criteria for selecting the optimal number of hidden layers and neurons in the network architecture  Can the proposed method be extended to handle more complex neural network architectures beyond fully connected networks  Have you explored the potential impact of hyperparameter selection on the performance and scalability of the proposed method  4) Limitations:  The paper could delve deeper into the analysis of the impact of various activation functions and their combinations on the performance of the proposed method.  Additional insights into the interpretability and explainability of the resulting monotonic neural networks could enhance the practical utility of the approach.  The generalization of the proposed method to handle a wider range of functions and scenarios could be further explored to enhance the applicability of the approach. Overall the paper presents a significant contribution in the domain of constructing monotonic neural networks addressing key challenges and showcasing promising results. Further enhancements in terms of scalability robustness and interpretability could further elevate the impact of the proposed method.", "qPb0m0NXt4j": " Peer Review:  1. Summary: The paper introduces a novel approach to modeling and simulating the evolution of graphical conventions in a visual communication game where two neural agents communicate using sketches. The agents evolve jointly towards successful communication and abstract graphical conventions using a reinforcement learning method. The study defines and evaluates the fundamental properties of the emergent conventions including iconicity symbolicity and semanticity. Through experiments and controls the authors demonstrate the emergence of successful communication and the preservation of semantics in evolved sketches. The research sheds light on emergent communication using sketches and contributes to the understanding of graphical conventions.  2. Strengths and Weaknesses: Strengths:  The study tackles an interesting and novel research problem of modeling graphical conventions in visual communication.  The paper incorporates a welldefined methodology including a visual communication game learning frameworks and evaluation methods for emergent graphical conventions.  Theoretical foundations experimental design and evaluation criteria are well thoughtout and align with the research objectives.  Results demonstrate the successful evolution of graphical conventions and provide insights into iconicity symbolicity and semanticity of the evolved sketches. Weaknesses:  While the study presents a pioneering approach the generalizability and scalability of the proposed methodology could be further explored.  The paper could benefit from a more indepth discussion of limitations and potential areas for future research to enhance the impact of the study.  The complexity of the methodology and technical terms used may limit the accessibility of the findings to a wider audience.  3. Questions:  Can you elaborate on how the proposed reinforcement learning method effectively facilitates agents awareness of the correlation between complex and simple sketches for abstraction processes  How do the experimental results support the observation that coevolved agents can switch between conventionalized and iconic communication based on their familiarity with referents  Could you discuss the implications of the findings on emergent communication with sketches in realworld scenarios or applications beyond the current experimental setup  4. Limitations:  The study heavily relies on pretrained models and datasets which may introduce biases or limitations in the results. Further exploration on the impact of these pretraining choices could enhance robustness.  The simulated environment may not fully capture the complexity and variability of realworld communication scenarios. Consideration of more diverse and dynamic environments could enhance the external validity of the findings.  The complexity of the model and evaluation methods may hinder easy replication and extension of the study by other researchers. Overall the research presents a promising exploration of emergent graphical conventions in visual communication games. Addressing the limitations and further investigating the broader implications of the findings could enhance the significance and applicability of the study.", "wXNPMS11aUb": " Peer Review  1) Summary: The paper introduces a novel bioinspired method DevFly for improving the performance of FlyLSH neural networks by developing binary connections from input dimensions to code dimensions. The study investigates the impact of developing connections on FlyLSHs ability to find nearest neighbors in various datasets. Experimental results show that the proposed DevFly method outperforms FlyLSH in terms of precision for nearest neighbor search tasks. The method is shown to be efficient fast and stable providing valuable insights for enhancing neural network performance in hashing tasks.  2) Strengths and weaknesses: Strengths:  Introduces a bioinspired method DevFly to improve FlyLSH performance in locality sensitive hashing.  Experimental results demonstrate the effectiveness of DevFly in enhancing precision for nearest neighbor search tasks.  The study provides insights into the impact of developing connections on network performance efficiency and stability.  Comparison with existing methods and datasets analysis enrich the validation of DevFly. Weaknesses:  The paper lacks a comprehensive analysis of the scalability and computational complexity of DevFly compared to existing methods.  Limitations in realworld applications and scalability to larger datasets need to be addressed.  The discussion on the impact on diverse applications beyond hashing could be further developed.  3) Questions:  How does the proposed DevFly method compare to other stateoftheart methods in terms of scalability and computational efficiency on large datasets  Can the DevFly method be adapted to more complex neural network architectures beyond the FlyLSH model  Have the authors considered potential limitations and challenges in implementing DevFly in realworld applications and highdimensional datasets  How does the developmental process used in DevFly affect the interpretability and generalization capabilities of the neural network in different application scenarios  4) Limitations:  The study mainly focuses on the comparison with FlyLSH and lacks comprehensive comparisons with a broader range of stateoftheart methods in hashing tasks.  The impact of developmental processes on network performance in more complex and diverse datasets could have been explored further.  The discussion of potential applications beyond hashing tasks is limited and could be expanded to provide a broader context for the relevance of DevFly. Overall the paper presents a novel and promising method in enhancing network performance for hashing tasks. Future work could further investigate the scalability applicability to diverse datasets and potential extensions to other neural network architectures. Further discussion on realworld applications and implications would also strengthen the papers contributions.", "x0LCDsbJ5JF": " Peer Review  1) Summary: The paper proposes the SpatiallyAdaptive SqueezeExcitation (SASE) module for image synthesis and recognition. The SASE module is designed to reinforce data specificity by incorporating both channelwise attention and spatial attention. Experimental results demonstrate the effectiveness of SASE in lowshot and oneshot image synthesis tasks as well as in image recognition using ResNets. Overall the paper presents a novel approach to enhance deep networks in image generation and classification tasks.  2) Strengths and Weaknesses: Strengths:  The proposed SASE module addresses the challenge of data specificity in deep networks by combining channelwise and spatial attention.  Experimental results showcase improved performance in highresolution image synthesis and image recognition tasks.  Discussion of related work and comparison with existing methods provides context for the proposed approach. Weaknesses:  The paper lacks detailed discussion on the interpretability and explainability of the SASE module.  The limitations section could be expanded to provide a more comprehensive overview of potential drawbacks or areas for future research.  Error bars for the experimental results would enhance the robustness of the findings.  3) Questions: 1. How does the SASE module handle potential biases or unwanted artifacts in the generated images during synthesis tasks 2. Could the authors elaborate more on the implications of the increased training time for models using the SASE module especially in highresolution image synthesis 3. Given that the proposed SASE shows stronger performance in some tasks compared to existing methods are there specific scenarios or datasets where it may not be as effective  4) Limitations:  The paper could benefit from further exploration of potential negative impacts on environmental factors due to the improved performance and efficiency of deep networks.  Clearer delineation of the computational costs and resource requirements associated with using the SASE module would help provide a more comprehensive evaluation. Overall the paper presents a novel and promising approach with the SASE module. Addressing the identified weaknesses and expanding on limitations and potential impacts would strengthen the contribution. The experimental results support the efficacy of the proposed method but further investigation into interpretability and biases would enhance the comprehensiveness of the study.", "kUnHCGiILeU": " Peer Review: 1) Summary (avg word length 100): The paper introduces a novel framework CageNeRF for deforming and animating implicit neural radiance fields without categoryspecific priors. It leverages cagebased deformation demonstrating effectiveness in geometry editing object animation and deformation transfer. 2) Strengths:  The proposed framework addresses the challenge of deforming and animating implicit fields without relying on datadependent priors enhancing generalization.  CageNeRF demonstrates versatility in deforming and animating arbitrary objects showcasing effectiveness in geometry editing object animation and deformation transfer.  Extensive experiments validate the efficacy of the framework across tasks supported by quantitative and qualitative results. 3) Weaknesses:  The paper could benefit from a more detailed discussion on the limitations of the proposed framework especially in handling complex deformations or dynamic objects with changing color and lighting conditions.  Further comparisons with existing methods especially in scenarios requiring intricate deformations or materialcolor editing could strengthen the evaluation.  The impact of uncertainties in geometry estimation on the performance of the framework could be elaborated further. 4) Questions:  How does CageNeRF handle cases where the geometry proxy for cage deformation has inaccuracies as noted in the human animation task evaluation  Could the authors elaborate on the computational efficiency of the proposed sampling strategy and its scalability to larger more complex datasets  Are there plans to extend the framework to address dynamic objects with varying color or lighting conditions considering recent advancements in neural rendering techniques 5) Limitations:  The framework may face challenges in accurately capturing complex deformations such as facial expressions due to the limitations of cagebased deformation.  The stability of color and density in the canonical radiance field restricts the adaptability of the framework to dynamic objects with changing color or lighting conditions.  The paper lacks a comprehensive discussion on the potential implications of inaccuracies in geometry estimation on the overall performance of CageNeRF. Overall the paper presents an innovative approach to deforming and animating implicit radiance fields showcasing promising results in various tasks. Addressing the identified weaknesses and limitations could enhance the robustness and applicability of the framework.", "zfQrX05HzBO": " Peer Review  1) Summary: The paper addresses the challenging problem of Continuous Category Discovery (CCD) where unlabeled data are continuously fed into the system for category discovery. The proposed Grow and Merge (GM) framework alternates between growing and merging phases to balance classification performance on known classes with the discovery of new categories. Extensive experiments show that GM outperforms stateoftheart methods in continuous category discovery scenarios.  2) Strengths and Weaknesses:  Strengths:  The paper addresses a novel and challenging problem of Continuous Category Discovery which is relevant for realworld applications.  The proposed GM framework effectively balances known class classification and novel category discovery tasks through growing and merging phases.  The experimental results demonstrate the effectiveness of GM in reducing forgetting effects and improving category discovery accuracy.  The work provides a thorough evaluation in different scenarios and datasets showcasing the flexibility and performance of the proposed method.  Weaknesses:  The paper lacks a detailed discussion on the computational complexity of the proposed GM framework and how it scales with increasing data size.  While the experimental results are comprehensive more insights could be provided on the interpretability of the features learned by GM and how they aid in category discovery.  The paper could benefit from a more indepth analysis of the limitations and potential areas for future research to enhance the proposed approach.  3) Questions:  Could the authors elaborate on the robustness of the GM framework to noisy or mislabeled data in the continuous stream and how it handles such scenarios  How does the GM framework perform when the distribution of known and novel classes in the unlabeled data stream is imbalanced  Is there any plan to extend the GM framework to handle scenarios where the number of known and novel categories varies dynamically over time  4) Limitations:  The paper primarily focuses on the experimental evaluation of the proposed GM framework without delving much into theoretical insights or algorithmic complexity analysis.  The practical implementation and deployment aspects of the GM framework are not extensively discussed which could be valuable for the adoption of the method in realworld applications.  The method heavily relies on the assumption of continuous unlabeled data which might limit its applicability in scenarios with intermittent data streams or where data updates are less frequent. Overall the paper presents a novel framework for Continuous Category Discovery and demonstrates its effectiveness through extensive experiments. However addressing the highlighted limitations and providing more insights into the practical implications of the proposed method could further strengthen the contribution of this work.", "pfI7u0eJAIr": " Peer Review  1) Summary The paper explores the underexplored aspect of using embeddings for numerical features in tabular deep learning (DL) architectures. It introduces two different embedding building blocks  piecewise linear encoding and periodic activation functions  and demonstrates their impact on model performance. The study shows that proper embeddings can close the gap with gradient boosted decision trees (GBDT) on GBDTfriendly benchmarks highlighting the potential of embeddings for numerical features in improving tabular DL models.  2) Strengths and weaknesses Strengths:  The paper addresses an important and underexplored area of research in tabular DL.  The study introduces novel embedding schemes and empirically demonstrates their effectiveness in improving model performance.  Results show that proper embeddings allow DL models to compete with GBDT on GBDTfriendly benchmarks achieving new stateoftheart performance.  The detailed experimental evaluation and comparisons provide valuable insights into the impact of different embedding modules.  The inclusion of the source code availability enhances reproducibility. Weaknesses:  The paper could benefit from a more extensive comparison with existing stateoftheart embedding methods in tabular DL.  While the observed performance improvements are notable the paper lacks indepth analysis of the mechanisms through which the proposed embeddings enhance model performance.  The study focuses more on demonstrating the performance improvements rather than explaining the underlying reasons for the effectiveness of the new embedding schemes.  Additional experiments on a wider range of datasets and comparisons with more diverse architectures could strengthen the findings.  3) Questions  Could the authors provide more insights into the interpretability of the piecewise linear encoding and periodic activation functions in the context of tabular DL models  How do the proposed embedding schemes compare with more complex and advanced embedding techniques used in other domains such as NLP and CV  Are there specific scenarios or types of tabular datasets where one of the embedding schemes proves to be more advantageous over the other  4) Limitations  The study primarily focuses on performance improvements without delving deep into the interpretability and generalizability of the proposed embedding modules.  The generalizability of the findings to diverse tabular datasets and architectures could be further validated to assess the robustness of the proposed embedding schemes.  The explanation of how the embeddings help in optimization and the underlying mechanisms behind their effectiveness could enhance the theoretical contribution of the study. In conclusion the paper contributes to advancing the understanding of using embeddings for numerical features in tabular DL. By introducing novel embedding schemes and demonstrating their impact on model performance the study opens up new possibilities for improving tabular DL models. Addressing the weaknesses and questions raised could further strengthen the study and its practical implications in the field of deep learning for tabular data.", "xl39QEYiB-j": "Peer Review Summary: The paper introduces a novel approach embodied sceneaware human pose estimation for estimating 3D human poses by controlling a simulated agent in a known environment based on 2D visual observations. The proposed method utilizing a multistep projection gradient for global movement cues achieves accurate pose estimation on the PROX dataset while being trained only on synthetic data. Experimental results demonstrate competitiveness against stateoftheart methods. Strengths: 1. Innovative Approach: The concept of embodied sceneaware human pose estimation is novel and addresses the challenges of estimating global 3D poses in complex scenes. 2. Realtime Performance: The method operates in near realtime and can handle longterm pose estimation challenges. 3. Robustness: The proposed method is robust to occlusion and can recover from consecutive frames of missing body observations. 4. Comprehensive Evaluation: The evaluation on popular datasets demonstrates the effectiveness and competitive performance of the proposed method. 5. Detailed Methodology: The paper provides a thorough explanation of the methodology including the multistep projection gradient and sceneaware kinematic policy. Weaknesses: 1. Complexity: The method involves several intricate components such as physicsbased character control and scene representation which may increase computational complexity and training time. 2. Simulation Simplifications: The method simplifies humanoids and scene geometries as rigid bodies potentially limiting the realism of interactions. 3. Failure Cases: While addressing some failure cases more discussion on potential failure scenarios and mitigation strategies would enhance the papers comprehensiveness. 4. Generalization: The models generalization to different environments or scenarios beyond the provided datasets is not extensively discussed. Questions: 1. How does the proposed method perform in scenarios with dynamic or unpredictable environment changes 2. Can the method adapt to different types of motion such as dance movements or sports actions 3. Is there a plan to extend the evaluation to include more diverse and challenging datasets beyond H36M and PROX 4. Have the authors considered realworld applications and potential limitations of deploying the method in physical environments Limitations: 1. Data Dependency: The reliance on synthetic data for training may limit the models adaptability to realworld scenarios. 2. Simulation Realism: Simplifications in simulating environment geometries and interactions could affect the models ability to generalize to complex realworld settings. 3. Scalability: The computational requirements of the proposed method for complex scenes or largescale applications may present scalability challenges. Overall the paper presents a promising approach to embodied sceneaware human pose estimation offering valuable insights and contributions to the field. Further investigations into realworld applicability scalability and generalization would enhance the impact of the proposed method.", "xL7B5axplIe": "Peer Review 1) Summary: The paper introduces Conservative Dual Policy Optimization (CDPO) a novel ModelBased Reinforcement Learning (MBRL) algorithm aimed at addressing the issue of overexploration and aggressive policy updates present in existing MBRL algorithms such as Posterior Sampling RL (PSRL). CDPO achieves global optimality through a Referential Update and a Conservative Update ensuring stability and efficient exploration. The work includes theoretical analyses empirical validation and comparisons with other MBRL algorithms across various tasks. 2) Strengths:  The paper provides a comprehensive analysis of the limitations of existing MBRL algorithms and presents CDPO as a solution addressing overexploration and aggressive policy updates.  The theoretical analysis demonstrates the statistical equivalence between CDPO and PSRL as well as the iterative policy improvement and global optimality properties of CDPO.  Empirical results validate the exploration efficiency of CDPO across different domains showing competitive performance compared to other MBRL algorithms.  The detailed explanation of the algorithmic framework including the Referential Update and Conservative Update steps adds clarity to the proposed approach. 3) Weaknesses:  The presentation of the paper could be improved by providing more intuitive explanations and examples to aid in understanding the complex concepts especially for readers less familiar with MBRL.  While the empirical evaluation showcases the effectiveness of CDPO additional analysis on scalability and robustness to hyperparameters could enhance the practical applicability of the algorithm.  The paper focuses heavily on theoretical analyses and comparisons with existing algorithms leaving less space for indepth analysis of implementation details and realworld applications. 4) Questions:  Could the authors elaborate on the computational efficiency of CDPO compared to other MBRL algorithms especially in terms of training time and sample complexity  How does CDPO perform in scenarios with highdimensional state and action spaces where model generalization is particularly challenging  Are there any specific assumptions or constraints in the CDPO algorithm that might limit its applicability to certain types of RL problems or environments 5) Limitations:  The paper lacks extensive discussions on the generalizability of CDPO to a wide range of RL tasks and environments beyond the ones tested in the empirical evaluation.  The validation of CDPO primarily relies on comparisons with a limited set of existing MBRL algorithms potentially limiting the breadth of insights into its overall performance. Overall the paper presents a promising MBRL algorithm CDPO with theoretical rigor and empirical validation. By further expanding on the practical implications and addressing potential limitations this work could have a significant impact on the field of RL.", "ptUZl8xDMMN": " Peer Review  1) Summary The paper introduces a novel framework for designing and analyzing graph scattering networks that goes beyond traditional graph wavelet approaches. The framework allows for varying branching ratios nonlinearities and filter banks providing stability guarantees for changes in input signals and graphshift operators. Theoretical results are supported by numerical experiments demonstrating the effectiveness of the proposed approach in social network graph classification and regression of quantumchemical energies on QM7.  2) Strengths  The paper addresses an important problem in graph signal processing and introduces a flexible framework for graph scattering networks extending beyond traditional graph wavelet approaches.  Theoretical contributions on stability guarantees for varying branching ratios nonlinearities and filter banks are significant and wellsupported by numerical experiments.  The proposed methodology shows superior performance in social network graph classification and quantumchemical energy regression tasks compared to existing approaches.  The experiments are well designed and provide concrete evidence of the effectiveness of the proposed method.  3) Weaknesses  The paper could benefit from clearer explanations in some sections especially regarding the technical details of the proposed framework and experimental setups.  The presentation of results in tables could be improved for better readability and comparison with existing methods.  While the theoretical framework and numerical results are strong more discussion on the practical implications and potential applications of the proposed approach would enhance the papers impact.  4) Questions  How does the proposed framework handle realworld graph data with noisy or incomplete information  Are there any limitations or assumptions in the theoretical model that may affect the applicability of the proposed approach in diverse graph signal processing tasks  Could the authors provide more insights into the scalability and computational efficiency of the proposed graph scattering networks in largescale graph datasets  5) Limitations  The paper may benefit from discussing the generalizability of the proposed framework to other types of graph data beyond the specific tasks evaluated in the experiments.  Further investigations or experiments on more diverse datasets could strengthen the generalizability and robustness of the proposed approach in various graph signal processing applications. In conclusion the paper presents a novel and mathematically rigorous framework for graph scattering networks demonstrating promising results in social network graph classification and quantumchemical energy regression tasks. Addressing some minor clarity issues and expanding the discussion on practical implications and generalizability could enhance the impact and relevance of the proposed approach in the field of graph signal processing.", "lXuZaxEaI7": "Peer Review Summary: The paper investigates the concept of batch sizeinvariance in policy optimization algorithms focusing on decoupling the proximal policy from the behavior policy to achieve this property. Through detailed experiments and theoretical analysis the authors introduce PPOEWMA and PPGEWMA variants demonstrating their batch sizeinvariance and efficiency in utilizing stale data. The findings highlight key insights into the optimization process of policybased methods. Strengths: 1. Novel Contribution: The decoupling of the proximal policy from the behavior policy to achieve batch sizeinvariance is a novel and significant contribution to the field of policy optimization algorithms. 2. Thorough Experimental Validation: The paper provides detailed experimental evidence supporting the proposed methodology showcasing its effectiveness in achieving batch sizeinvariance and improving sample efficiency. 3. Clarity and Structure: The paper is wellorganized with clear section divisions and explanations enabling easy comprehension of the concepts and methodologies presented. 4. Relevance: The research addresses a practical concern in reinforcement learning by offering insights and solutions to make policy optimization algorithms more robust and efficient in handling variable batch sizes. Weaknesses: 1. Technical Depth: The paper delves deeply into the technical aspects of policy optimization algorithms which may pose challenges for readers not wellversed in the field. Simplifying certain sections may aid in broader accessibility. 2. Generalization: The experiments are largely focused on specific scenarios and algorithms. Including more diverse datasets or scenarios could strengthen the generalizability of the proposed methods. 3. Discussion of Baselines: While the paper introduces modified algorithms a comparison with traditional approaches without the introduced modifications could further highlight the effectiveness of the proposed method. Questions: 1. Comparison with Existing Algorithms: How does the proposed decoupling methodology compare with existing algorithms in terms of performance convergence and computational efficiency 2. Scalability: To what extent are the findings scalable to larger and more complex environments beyond the current experimental setup 3. Theoretical Underpinnings: Can the authors elaborate on the theoretical foundations supporting the proposed decoupling strategy and batch sizeinvariance in policy optimization algorithms Limitations: 1. Experimental Scope: The experiments are primarily conducted on Procgen Benchmark limiting the scope of the findings to specific environments. Exploring a wider range of environments could enhance the robustness of the conclusions. 2. Computational Overhead: The additional memory requirements and computational costs associated with the EWMA modification could be a practical limitation in resourceconstrained settings. 3. Realworld Applicability: Further investigation on the applicability of the proposed methodology to realworld scenarios with varying constraints and complexities could enhance the practical significance of the research. Overall the paper offers valuable insights into enhancing the batch sizeinvariance of policy optimization algorithms through the decoupling of proximal and behavior policies. Addressing the weaknesses and questions raised could further strengthen the research findings and their applicability in realworld reinforcement learning applications.", "wlEOsQ917F": "Summary: The paper introduces a novel framework for solving bilevel optimization problems focusing on stochastic methods to handle large datasets and complex functions. The proposed framework allows for the development of two algorithms SOBA and SABA which achieve competitive convergence rates. The experiments demonstrate the effectiveness of the proposed method in hyperparameter selection and data hypercleaning tasks. Strengths: 1. The paper addresses an important and challenging problem of bilevel optimization in machine learning. 2. The proposed framework simplifies the derivation of stochastic algorithms for bilevel optimization making it accessible for practical applications. 3. The theoretical analysis provides convergence guarantees for both SOBA and SABA algorithms showing competitive performance. 4. The experimental results showcase the superiority of SABA over other stateoftheart methods in hyperparameter selection and data hypercleaning tasks. Weaknesses: 1. The complexity of the proposed method and theoretical analysis might make it challenging for practitioners without a strong mathematical background to fully grasp. 2. The paper lacks a detailed discussion on the scalability and practical implementation aspects of the proposed algorithms. 3. The results are limited to specific tasks and datasets and more diverse experiments could further validate the superiority of the proposed method. Questions: 1. How would the proposed framework handle more complex optimization problems beyond the scenarios tested in the experiments 2. Are there specific limitations or constraints in realworld applications that might hinder the practical adoption of the proposed algorithms 3. How robust and adaptable are SOBA and SABA algorithms to different learning scenarios noise levels and data distributions 4. Could the proposed method be extended or modified to address other related optimization challenges in machine learning beyond bilevel optimization Limitations: 1. The paper focuses mainly on theoretical analysis and empirical validation on specific tasks limiting the generalizability of the proposed method to diverse realworld scenarios. 2. The practical implementation details and computational efficiency of the proposed algorithms are not extensively discussed potentially hindering adoption in realworld applications. 3. The experimental results are confined to a few tasks and broader benchmarking against more diverse datasets and scenarios could provide a comprehensive evaluation of the proposed methods effectiveness.", "vgIz0emVTAd": " Peer Review: 1. Summary: The paper introduces a novel adversarial defense method called aDversarIal defenSe with local impliCit functiOns (DISCO) that aims to remove adversarial perturbations by projecting images into the image manifold using local implicit functions. DISCO is shown to outperform prior defenses in terms of robust accuracy under both oblivious and adaptive settings. The paper provides a detailed explanation of the methodology architecture training procedure and experimental results showcasing the effectiveness of DISCO across various datasets classifiers and attacks. 2. Strengths and Weaknesses:  Strengths:  The paper proposes a novel adversarial defense method DISCO based on local implicit functions which outperforms prior defenses on various datasets and attacks.  Extensive experiments demonstrate the robustness and effectiveness of DISCO under both oblivious and adaptive settings showcasing its superior defense performance.  The methodology is welldescribed detailing the architecture training procedure and inference process of DISCO providing clarity on its implementation.  The evaluation of DISCO against a wide range of attacks classifiers and datasets shows its robustness and transferability highlighting its practical effectiveness.  The paper addresses the limitations and societal impact of the proposed defense method providing a comprehensive overview of the implications of the research.  Weaknesses:  The paper could benefit from a more indepth discussion on the limitations of DISCO particularly in terms of its susceptibility to novel or diverse attack strategies that were not addressed in the current study.  The peer review could expand on the technical aspects of the method such as the choice of network architecture hyperparameters and training details for a more comprehensive evaluation of the methodology. 3. Questions:  Could the authors elaborate on the computational efficiency of DISCO in terms of inference time and resource requirements compared to other adversarial defense strategies  How does DISCO handle adversarial attacks that involve larger perturbations or complex attack strategies beyond L\\xe2\\x88\\x9e and L2 norms  Are there any specific scenarios or realworld applications where DISCO could be particularly effective and are there any known limitations in such contexts 4. Limitations:  One limitation of the study is the focus on normbounded attacks raising questions about the efficacy of DISCO against more complex or diverse adversarial strategies.  The transferability of DISCO to unconventional or nonstandard attack scenarios could be a limitation that requires further investigation.  The study lacks comprehensive evaluation across a wide range of attack types and datasets limiting the generalizability of the findings. Overall the paper presents a strong and innovative approach to adversarial defense with DISCO supported by thorough experiments and analysis. Further exploration of the limitations and potential extensions of the method could enhance the impact of the research.", "mjVZw5ADSbX": "Peer Review 1) Summary The paper introduces a new Contrastive Neural Text generation framework named CONT to address the limitations of existing methods in applying contrastive learning to neural text generation tasks. CONT proposes improvements in constructing contrastive examples defining the contrastive loss function and integrating the learned sequence similarity score into the decoding process. The framework is evaluated on five generation tasks with ten benchmarks showing significant performance improvements over traditional training frameworks and outperforming previous contrastive learning methods. Overall the paper presents compelling results and contributes a new stateoftheart framework for text generation tasks. 2) Strengths and Weaknesses Strengths  Rigorous evaluation on multiple generation tasks with various benchmarks.  Clear explanation of the proposed CONT framework and its key components.  Significant performance improvements over existing contrastive learning methods.  Stateoftheart results achieved on summarization code comment generation datatotext generation and commonsense generation tasks.  Implementation details provided including code availability for reproducibility. Weaknesses  The paper lacks a detailed analysis of computational complexity or resource requirements for the proposed CONT framework.  The limitations of the framework including scenarios where it might not always outperform traditional methods could be further discussed.  The paper could benefit from a more extensive discussion on the generalizability of the proposed framework to other text generation tasks or datasets. 3) Questions  How does the proposed CONT framework handle scenarios with highdimensional input data or complex sequences in text generation tasks  Could the authors provide insights into potential practical applications of the CONT framework in realworld text generation scenarios beyond the evaluated benchmarks  How does the training efficiency issue impact scalability and deployment in production environments and are there ongoing efforts to address this challenge 4) Limitations  The evaluation of the CONT framework primarily focuses on comparing it to existing contrastive learning methods without extensive benchmarking against traditional noncontrastive approaches.  The paper lacks a detailed discussion on potential biases or ethical considerations that may arise from deploying contrastive learning frameworks in text generation tasks.  The limitations section could be expanded to include scenarios where the CONT framework might underperform or face challenges in complex text generation applications. In conclusion the paper presents a novel and effective Contrastive Neural Text generation framework CONT that demonstrates superior performance on various text generation tasks. By addressing key limitations of existing contrastive learning methods CONT offers promising advancements in the field. However further studies are needed to explore the scalability and generalizability of the framework and address the training efficiency concerns raised in the paper.", "wwWCZ7sER_C": "Peer Review: 1) Summary: The paper explores the utilization of multiple machine learning predictions in algorithm design for various fundamental problems like matching load balancing and scheduling. It presents new algorithms that incorporate multiple predictors and provides theoretical bounds on their performance. The research focuses on cases where having multiple predictors can enhance algorithm performance without significantly increasing costs. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important and emerging research area that could significantly improve algorithm design by leveraging multiple machine learning predictions.  Theoretical analysis and bounds are provided for the new algorithms introduced for fundamental problems like matching load balancing and scheduling.  The paper presents a systematic approach for incorporating multiple predictions in algorithms and analyzing their impact on performance. Weaknesses:  Although theoretical results are provided the empirical validation of the algorithms is limited to only one problem which could have been expanded to provide a more comprehensive evaluation.  The complexity of the algorithms and the detailed theoretical analyses might make it challenging for readers unfamiliar with the field to grasp the key ideas easily.  The paper could benefit from more discussion on potential practical implementations of the proposed algorithms in realworld scenarios. 3) Questions:  How scalable are the proposed algorithms when dealing with a large number of predictions in realworld settings  What are the computational requirements and time complexities of the new algorithms compared to existing approaches when incorporating multiple predictors  How do the algorithmic performance and learning efficiency vary when the predictions exhibit varying degrees of accuracy across different problem instances 4) Limitations:  The empirical validation is limited to only one problem area which constrains the overall generalizability and realworld applicability of the findings.  The paper mainly focuses on theoretical analysis and may benefit from more practical insights or case studies to showcase the effectiveness of the proposed algorithms.  The extensive theoretical nature of the content might hinder accessibility for readers with limited background in the field. 5) Overall Evaluation: The paper presents a rigorous investigation into the incorporation of multiple machine learning predictions in algorithm design offering valuable insights and new algorithms for fundamental problems. While the theoretical contributions are strong more emphasis on practical applications and empirical validations could enhance the impact of the research. Addressing some of the questions raised above and providing more practical illustrations could further enrich the paper and broaden its potential audience. The work represents a significant contribution to the field of algorithms with predictions and sets a strong foundation for further research in leveraging multiple predictors for enhanced algorithmic performance.", "zzDrPqn57DL": " Peer Review:  1) Summary: The paper introduces BEVFusion a novel LiDARcamera fusion framework for 3D object detection. BEVFusion disentangles the current fusion methods dependency on LiDAR data which addresses the limitations of existing approaches. The framework utilizes two independent streams for camera and LiDAR data encoding them into features within the same BEV space leading to improved performance under both normal and robust training settings. Experimental results demonstrate the frameworks superior performance robustness against LiDAR malfunctions and generalization ability on the nuScenes dataset.  2) Strengths:  The paper addresses an important limitation of current LiDARcamera fusion methods and proposes a simple yet effective framework to overcome it.  The disentanglement of camera and LiDAR data streams into a common BEV space is a novel approach.  Extensive experiments demonstrate the superiority of BEVFusion over stateoftheart methods particularly robustness against LiDAR malfunctions.  Availability of code on GitHub enhances reproducibility and adoption by the research community.  Weaknesses:  While the paper presents a novel framework and demonstrates its effectiveness more indepth analysis on the performance of individual components and the impact of hyperparameters could strengthen the evaluation.  The paper could benefit from including a comparative study with more recent fusion methods beyond those mentioned in the Related Works section to provide a broader scope for readers.  Providing insights into computational efficiency and inference time comparison with existing methods would be valuable for practical deployment considerations.  3) Questions:  Can the authors elaborate on the complexity and resource requirements of BEVFusion compared to stateoftheart fusion methods  How sensitive is the framework to variations in the input data quality or distribution  Are there any scenarios or edge cases where BEVFusion might underperform compared to other fusion methods  4) Limitations:  While the paper demonstrates significant improvements the evaluation is primarily focused on the nuScenes dataset extending the experiments to other datasets or realworld scenarios would enhance the generalizability of the proposed framework.  While the robustness against LiDAR malfunctions is highlighted further investigation into the generalization of the framework to diverse environmental conditions or sensor configurations could provide a more comprehensive understanding of its applicability. In conclusion the paper presents a valuable contribution to the field of LiDARcamera fusion for 3D object detection with BEVFusion showing remarkable performance improvements and robustness. Addressing the raised questions and limitations would further strengthen the credibility and applicability of the proposed framework.", "lYHUY4H7fs": " Summary: The paper explores the concept of envyfree policy teaching in a multiagent setting with personalized reward modifications. It introduces fairness through the notion of envyfreeness and investigates the existence of envyfree solutions cost minimization and the price of fairness. The paper shows that envyfree solutions always exist under certain conditions can be computationally efficient and presents tight asymptotic bounds on the price of fairness.  Strengths: 1. Clear Problem Formulation: The paper clearly defines the problem of envyfree policy teaching and provides a structured investigation into this important topic. 2. Theoretical Depth: The paper contributes to the theoretical understanding of fairness in multiagent teaching introducing new concepts and analyzing their implications rigorously. 3. Algorithmic Solutions: The paper presents algorithmic solutions for computing envyfree solutions efficiently and formulates the problem as convex optimization. 4. Asymptotic Bounds: The derivation of tight asymptotic bounds on the price of fairness enhances the understanding of the tradeoffs involved in incorporating fairness.  Weaknesses: 1. Complexity: The paper introduces a number of concepts and mathematical formulations that may be challenging for readers without a strong mathematical background. 2. Limited Practical Validation: While the theoretical results are sound practical validation or experimental results to demonstrate the effectiveness of the proposed solutions would enhance the papers applicability. 3. Limited Discussion on Realworld Applications: The paper could benefit from more discussion on how these concepts and findings could be applied in realworld scenarios to address fairness concerns.  Questions: 1. How do the proposed envyfree solutions compare with existing fairness mechanisms in multiagent settings 2. Could the approach be extended to incorporate other fairness notions beyond envyfreeness to provide a broader view of fairness in policy teaching 3. Are there practical implications or specific applications where the proposed solutions could be immediately useful and impactful 4. What are the key assumptions made in the model and how robust are the results to changes in these assumptions  Limitations: 1. Theoretical Focus: The paper is primarily theoretical and more practical validation or case studies could enhance its applicability. 2. Generalizability: The applicability of the findings to diverse realworld scenarios may be limited without further exploration and validation. 3. Complexity: The complex mathematical formulations may pose a barrier to understanding for readers from nonmathematical backgrounds. Overall the paper makes a valuable contribution to the understanding of fairness in multiagent teaching settings providing a solid theoretical foundation for further research in this area. Further validation and practical exploration would strengthen the significance and applicability of the findings in realworld settings.", "vRwCvlvd8eA": " Peer Review of the Paper 1) Summary: The research paper introduces chefs\u2019 random tables (CRTs) a novel class of nontrigonometric random features designed for approximating Gaussian and softmax kernels. The authors propose the optimal positive random features (OPRFs) the first random feature method for unbiased softmaxkernel estimation with positive and bounded features. By leveraging statistics and orthogonal random projections the paper achieves variance reduction and provides uniform convergence results for softmax attention approximation with positive random features. Extensive empirical evaluations demonstrate new stateoftheart results for lowrank text Transformers. 2) Strengths and Weaknesses: Strengths:  The paper introduces a novel class of random features CRTs which provide positive and bounded random features for unbiased softmaxkernel estimation.  The OPRFs proposed in the paper show exponentially small tails and significantly lower variance compared to existing methods leading to the first uniform convergence results for softmax attention.  The theoretical results and empirical evaluations demonstrate the effectiveness of the proposed methods achieving new stateoftheart results for lowrank text Transformers. Weaknesses:  The paper could provide more detailed explanations of the algorithmic implementations and practical considerations for using CRTs in realworld scenarios especially in Transformers.  The implications of the proposed methods on the computational and memory requirements for largescale applications could be discussed more comprehensively.  The limitations of the research such as potential constraints or failures in specific use cases could be addressed to provide a more nuanced understanding of the applicability of the proposed methods. 3) Questions:  How do the proposed CRTs and OPRFs compare against existing stateoftheart methods extensively used in Transformer models such as the selfattention mechanism in the original Transformer architecture  Are there specific scenarios or data distributions where the benefits of using CRTs outweigh the computational costs or limitations compared to traditional trigonometric random features  Can the paper elaborate on the interpretability and generalizability of the results obtained through the empirical evaluations especially in the context of the practical deployment of lowrank text Transformers 4) Limitations:  The paper lacks extensive discussion on the practical implications and potential challenges of integrating the proposed methods into existing Transformer architectures and other machine learning models.  The paper focuses primarily on textual data applications thus the generalizability of the proposed methods to other domains such as computer vision or audio processing remains unclear.  The empirical evaluations while demonstrating new benchmarks for lowrank text Transformers may not cover a broad enough range of diverse datasets and tasks to establish the scalability and robustness of the proposed methods across different applications. Overall the paper introduces innovative concepts in random feature generation for Gaussian and softmax kernels offering promising results in terms of variance reduction and stateoftheart performance in lowrank attention Transformers. Strengthening the discussions around practical implications scalability to various domains and potential limitations would enhance the significance and applicability of the proposed methods in the field of machine learning and artificial intelligence.", "lTZBRxm2q5": " Peer Review:  Summary: The paper introduces a novel approach of incorporating a generalized white noise into neural Stochastic Differential Equations (SDEs) by proposing an efficient approximation method for noise sample paths. The method is based on classical integration techniques and sparse Gaussian processes. The experimental results demonstrate the capability of the proposed model in capturing noise characteristics and improving model fittings over existing approaches. The paper explores the application of this approach in scorebased generative models and shows improved image generation results. The study provides a comprehensive theoretical framework computational analysis and empirical evaluations to support the proposed approach.  Strengths: 1. Innovative Concept: The integration of a generalized fractional white noise into neural SDE models is a novel and creative approach that extends the capabilities of existing models. 2. Theoretical Foundation: The paper is grounded in wellestablished theories of stochastic processes differential equations and Gaussian processes providing a solid theoretical foundation for the proposed method. 3. Empirical Validation: The experimental results provide strong evidence of the effectiveness of the proposed approach in various scenarios including synthetic data realworld financial data and image generation tasks. 4. Contribution: The paper introduces a threefold contribution \u2013 a new type of neural SDEs with general white noise an efficient noise approximation method and practical applications that showcase the benefits of the proposed technique.  Weaknesses: 1. Complexity: The paper is highly technical and may be challenging for readers without a deep understanding of stochastic processes neural networks and differential equations. Some sections could be simplified for better readability. 2. Generalizability: The paper focuses on specific scenarios and applications limiting the generalizability of the proposed approach. Providing more insights into the scalability and applicability of the method across diverse domains would enhance its impact. 3. Evaluation Metrics: While the empirical results are promising additional evaluation metrics and comparisons with other stateoftheart methods could further validate the effectiveness of the proposed model.  Questions: 1. Scalability: How scalable is the proposed approach in handling large datasets and complex modeling tasks Have you tested the performance of the model on significantly larger datasets 2. Parameter Tuning: How sensitive is the performance of the model to hyperparameter choices especially in the context of the noise approximation method and the neural SDEs 3. Realworld Applications: Beyond the synthetic and financial data used in the experiments what are the potential realworld applications of the proposed model and how adaptable is it to different domains  Limitations: 1. Hurst Exponent Selection: The study focuses on a specific range of Hurst exponents potentially limiting the applicability of the approach to scenarios with different characteristics. 2. Assumptions: The paper makes several assumptions such as fixed step sizes in SDE numerical solvers which might not hold in all practical applications limiting the generality of the findings. 3. Interpretability: While the model demonstrates improved performance the interpretability of learned parameters especially in relation to noise characteristics could be further elucidated. In conclusion the paper presents a significant advancement in the integration of white noise in neural SDEs supported by theoretical rigor and empirical validation. Addressing the limitations and questions raised could further enhance the impact and practical utility of the proposed approach.", "unb1wyXf-aC": "Peer Review: Summary: This paper proposes a novel perceptual super resolution (PSR) method for 3D neuroimaging termed concurrent super resolution and segmentation (CSRS). The study evaluates the effectiveness of CSRS in detecting brain changes associated with neurodegenerative diseases by upsampling both image intensity channels and segmentation labels simultaneously. Various loss functions are compared and CSRS is demonstrated to enhance the ability to detect regional atrophy in five related diseases. The methodology involves training on volumetric brain data and employs a combination of reconstruction total variation perceptual quality and segmentation overlap as loss functions. Strengths: 1. The paper addresses an important problem in neuroimaging by proposing a PSR method specifically designed for 3D neuroimaging. 2. The study provides a detailed methodology that includes training details data sources and evaluation metrics. 3. The realworld evaluation of CSRS in detecting brain changes due to neurodegenerative diseases adds significant value to the field. 4. The inclusion of data from various sources like the Human Connectome Project and Parkinson\u2019s Progression Markers Initiative strengthens the studys applicability. Weaknesses: 1. While the proposed CSRS method shows promise the paper lacks comparative analysis with existing stateoftheart techniques in the field of super resolution and neuroimaging. 2. The paper could benefit from a more detailed discussion on the interpretability of the results and potential implications for clinical application. 3. The limitations of the study such as assumptions made potential biases and generalizability could be further elaborated. Questions: 1. How does the proposed CSRS method compare to other existing 3D super resolution techniques in terms of computational efficiency and accuracy 2. Can the authors elaborate more on the generalizability of the CSRS method across different neurodegenerative diseases and imaging datasets 3. How robust is the approach to variations in input data quality noise levels or imaging hardware differences 4. What considerations were taken into account for the potential biases in the data sources used such as demographic variations and imaging protocols Limitations: 1. The study heavily relies on simulated data and may not fully capture the complexities of realworld neuroimaging scenarios. 2. The lack of a comprehensive discussion on the ethical considerations and societal impacts of the proposed method is a notable limitation. 3. The generalizability of the findings may be constrained by the specific datasets used warranting further validation on diverse datasets for broader applicability. In conclusion the study presents a novel approach to 3D perceptual super resolution for neuroimaging with significant implications for detecting brain changes in neurodegenerative diseases. Addressing the weaknesses and limitations identified would further enhance the impact and validity of the research.", "wk5zDkuSHq": " Peer Review:  1) Summary: The paper introduces a novel algorithm for online agnostic multiclass boosting extending previous work on boosting algorithms in machine learning. The study presents the first weak learning conditions and algorithms for this particular problem with a focus on reducing boosting to online convex optimization. Empirical results showcase the efficiency and competitiveness of the proposed online agnostic boosting algorithm compared to existing stateoftheart multiclass boosting algorithms.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important gap in the literature by developing an online agnostic multiclass boosting algorithm.  The algorithm leverages a reduction to online convex optimization demonstrating a clean and simple approach.  The study provides empirical evidence of the algorithms efficiency and competitive performance.  Extensive theoretical analysis and definitions of various weak learning conditions add rigor to the proposed algorithm. Weaknesses:  The paper could benefit from a more detailed explanation of the experimental methodology including hyperparameter tuning and the rationale behind algorithm selection.  The discussion of future work could be expanded to provide more insights into potential directions for further research.  3) Questions:  How robust is the proposed algorithm to variations in dataset characteristics such as imbalance or noisy labels  Can the algorithm scale effectively to larger datasets and more complex multiclass classification tasks  What are the computational requirements of the algorithm in terms of memory and processing power  How does the advantage parameter impact the algorithms performance and are there guidelines for selecting an optimal value  4) Limitations:  The performance evaluation is limited to benchmarking against a few existing algorithms. Comparing against a wider range of stateoftheart methods could enhance the studys credibility.  The focus on empirical results could be further strengthened by discussing the algorithms interpretability and generalizability across diverse applications.  The generalizability of the proposed algorithm to realworld scenarios with dynamic data streams or concept drifts could be explored further. Overall the paper presents a significant contribution to the field of machine learning by introducing a novel online agnostic multiclass boosting algorithm. The research adds valuable insights into the realm of boosting algorithms and provides a foundation for future advancements in this area.", "voV_TRqcWh": "Peer Review: 1) Summary: The paper introduces a novel \"Poisson flow\" generative model (PFGM) that leverages a physical analogy to map a uniform distribution on a highdimensional hemisphere into any data distribution. By interpreting data points as electrical charges the model uses an augmented space with an additional dimension z to generate a highdimensional electric field. The model demonstrates stateoftheart performance on CIFAR10 with impressive Inception and FID scores. Notably PFGM offers significant acceleration on image generation tasks and robustness to network architecture and step size variations. 2) Strengths and weaknesses:  Strengths:  Innovative approach using a physicsbased analogy for generative modeling.  Achieves stateoftheart performance on CIFAR10.  Offers significant acceleration on image generation tasks.  Demonstrates stability on weaker network architectures and robustness to step size variations.  Comprehensive experimental evaluation and comparison with existing methods.  Weaknesses:  The paper lacks detailed empirical comparisons with a wider range of data sets.  The theoretical discussion is intricate and may be challenging for readers without a physics background.  The paper could benefit from more clarity in presentation especially in the technical details of the proposed model. 3) Questions:  Can the physical analogy used in PFGM be easily understood and interpreted by researchers without a background in physics  How does the performance of PFGM compare to traditional generative models like GANs and VAEs on other datasets beyond CIFAR10  Are there any additional practical implications or applications of the proposed Poisson flow generative modeling beyond image generation tasks 4) Limitations:  The proposed models generalizability to a broader set of datasets beyond CIFAR10 is not thoroughly explored.  The complex nature of the Poisson flow concept might present challenges in practical implementation and understanding for some users.  The performance of PFGM on more challenging datasets with diverse distribution characteristics remains to be investigated. Overall the paper presents a novel and promising approach to generative modeling showcasing impressive performance and offering valuable insights into the fusion of physics and machine learning. Further refinement in the theoretical presentation and a wider range of experimental evaluations can enhance the impact and applicability of the proposed Poisson flow generative model.", "sWNT5lT7l9G": " Peer Review  Summary: The paper proposes a novel approach called constrained GPI to improve zeroshot transfer in reinforcement learning by bounding the value approximation errors of successor features on new target tasks. The authors introduce lower and upper bounds on the optimal values for novel tasks expressed as linear combinations of source tasks and present a constrained training scheme as well as a testtime constrained GPI method. The proposed approach is empirically evaluated in Scavenger and DeepMind Lab environments showing significant performance improvements over prior methods.  Strengths: 1. Innovative Approach: The paper introduces a novel method to enhance zeroshot transfer in reinforcement learning by bounding value approximation errors which is a valuable contribution to the field. 2. Theoretical Contributions: The introduction of the lower and upper bounds for optimal values for novel tasks is a significant theoretical contribution that can benefit various problems in the domain. 3. Empirical Evaluation: The experiments conducted in Scavenger and DeepMind Lab environments provide concrete evidence of the effectiveness of the proposed constrained GPI method. 4. Clear Presentation: The paper is wellorganized providing a thorough explanation of the problem previous approaches the proposed method and experimental results.  Weaknesses: 1. Limited Comparison: The paper could benefit from a more detailed comparison with existing methods or alternative approaches to highlight the advantages and limitations of the proposed constrained GPI method more effectively. 2. Theoretical Depth: While the theoretical foundations of the approach are wellexplained further analysis or discussion on the practical implications of the proposed lower and upper bounds would enhance the paper. 3. Reproducibility: While the paper mentions the availability of code and additional information more details on the reproducibility of the experiments and implementation specifics would be beneficial.  Questions: 1. Generalization Power: How does the proposed constrained GPI method perform in scenarios with highly complex tasks or environments beyond Scavenger and DeepMind Lab where generalization is challenging 2. Hyperparameters: Can the authors provide insights into the tuning of hyperparameters in the constrained training scheme and constrained GPI for optimal performance 3. Impact of Approximation Errors: How sensitive is the performance of constrained GPI to the approximation errors of the successor features approximators especially in scenarios with significant discrepancies between source and target tasks  Limitations: 1. Scope of Environments: The experimental validation is limited to Scavenger and DeepMind Lab environments which may restrict the generalizability of the proposed constrained GPI approach. 2. Computational Complexity: The additional computational cost and potential instability introduced by the constrained training scheme could be a limitation in realworld applications. 3. Assumption of Source Task Quality: The effectiveness of the constrained GPI heavily relies on the accurate approximation of successor features on source tasks which may not always be feasible in practice. In conclusion the paper presents a novel constrained GPI approach for improving zeroshot transfer in reinforcement learning supported by theoretical foundations and empirical evidence. Addressing the weaknesses and limitations identified could further strengthen the impact and applicability of the proposed method in realworld scenarios.", "ok-SB1kz67Z": " Peer Review:  1) Summary: The paper proposes a novel approach called introspective learning which introduces a twostage decisionmaking process in neural networks. The first stage involves sensing patterns in the data and making a quick decision while the second stage involves reflecting on alternative decisions to arrive at a final choice. The authors use gradients of trained neural networks to measure this reflective stage and introduce a simple Multi Layer Perceptron as the second stage. The paper demonstrates the benefits of introspective networks in improving robustness and reducing calibration errors when generalizing to noisy data. Additionally the proposed method is validated in various downstream tasks such as active learning outofdistribution detection and uncertainty estimation. The experiments show promising results in improving performance and calibration in diverse applications.  2) Strengths and weaknesses: Strengths:  The paper introduces a novel concept of introspective learning in neural networks providing a unique perspective on decisionmaking processes in AI systems.  The experiments conducted on various tasks demonstrate the effectiveness of introspective networks in improving robustness and calibration in challenging scenarios.  The utilization of gradients for measuring reflection provides a practical and interpretable way to introspect neural networks decisionmaking processes. Weaknesses:  The paper lacks a detailed analysis of the computational complexity and scalability of the proposed method especially with a larger number of classes.  The explanation of the methodology and implementation details could be further elaborated to enhance reproducibility and understanding for the reader.  While the experiments show promising results a more extensive comparison with existing methods and benchmarks could strengthen the validation of the proposed approach.  3) Questions:  How does the proposed introspective method compare with existing interpretability techniques such as SHAP or LIME in providing insights into neural network decisionmaking processes  Can the introspective approach be generalized to different network architectures beyond the simple Multi Layer Perceptron used in the study  Is there any exploration on the potential biases that may be amplified through the introspection process and how can these biases be mitigated in practical applications  4) Limitations:  The study relies on a relatively simple MLP for the introspective stage. Exploring more complex architectures and their impacts on introspective learning could provide further insights.  The research would benefit from a deeper analysis of the robustness and calibration aspects in realworld scenarios beyond the datasets used in the experiments.  The generalization of introspective learning to highdimensional data and more complex models is an area that requires further investigation to ensure scalability and effectiveness. Overall the paper presents an intriguing concept of introspective learning in neural networks and offers valuable insights into improving robustness and calibration. Further development and validation in diverse scenarios would enhance the applicability and impact of the proposed approach.", "o4neHaKMlse": " Peer Review:  1) Summary The paper introduces FIBER a novel visionlanguage (VL) model architecture capable of handling various tasks ranging from imagelevel understanding to regionlevel perception. FIBER employs twostage pretraining to efficiently leverage both coarsegrained and finegrained data. Comprehensive experiments across multiple VL tasks demonstrate consistent performance improvements over strong baselines. The paper provides insights into the architecture pretraining strategies and adaptation to downstream tasks showcasing FIBERs versatility and effectiveness in diverse VL tasks.  2) Strengths and Weaknesses Strengths:  Innovative Architecture: FIBER introduces a unique approach by embedding crossattention modules within the image and text backbones enabling deep multimodal fusion.  Twostage Pretraining: The twostage pretraining strategy efficiently combines lowresolution imagetext data with highresolution imagetextbox data showing performance gains across tasks.  Performance Improvement: FIBER consistently outperforms strong baselines on a wide range of VL tasks while reducing the reliance on large amounts of annotated data. Weaknesses:  Limited Comparison: Although the paper presents comparisons with some existing models a more detailed comparison with a broader set of stateoftheart models would enhance the robustness of the evaluation.  Scalability: The paper does not extensively discuss the scalability of the proposed approach to larger datasets or more complex tasks which could be a limitation.  3) Questions  Could FIBERs architecture be easily extended to handle additional modalities beyond vision and text such as audio or sensor data  How does FIBER perform in scenarios with limited data availability compared to models that rely heavily on largescale pretrained datasets  Are there any implications of utilizing the twostage pretraining strategy such as potential information loss or overfitting concerns in specific tasks  4) Limitations  Data Bias and Environmental Impact: The paper briefly mentions the potential for inherited societal biases and environmental costs due to pretraining data. Further exploration and mitigation strategies on these aspects could be beneficial.  Scalability Challenges: While the proposed architecture shows promising results addressing scalability challenges for tasks demanding larger datasets or more computational resources would be important for realworld applications. Overall the paper presents a significant advancement in the VL domain with FIBER showcasing superior performance across a range of tasks. Further exploration into scalability generalizability and ethical considerations would enhance the impact and applicability of the proposed framework.", "mamv07NQWk": "Peer Review 1. Summary The paper tackles the theoretical understanding of Multilabel Classification (MLC) by providing upper and lower regret bounds for Hamming loss and Precision\u03ba. The study introduces novel labelwise margin conditions and investigates the interplay between low noise assumptions and label sparsity in improving regret bounds. The research includes nonparametric settings using knearest neighbor classifiers and parametric settings with empirical risk minimization. The results are derived by building on former contributions and extending analyses to MLC scenarios. 2. Strengths  The paper addresses an important gap in the theoretical understanding of MLC providing valuable insights into upper and lower bounds for regret in this context.  The use of both nonparametric and parametric settings adds depth to the analysis covering different algorithmic approaches.  The introduction of novel labelwise margin conditions and the exploration of the impact of low noise assumptions and label sparsity are significant contributions. 3. Weaknesses  The paper is highly technical and may be challenging for general readers especially those not wellversed in statistical and machine learning theory.  The study heavily relies on assumptions that may not always hold in practical scenarios possibly limiting the generalizability of the results.  The paper lacks practical applications or empirical validations of the theoretical findings which could have enhanced the relevance and applicability of the research. 4. Questions  How do the findings in this paper align with existing practical applications of MLC and what are the potential realworld implications of these theoretical results  Can the developed models and approaches be easily extended or adapted to handle more complex MLC scenarios or other loss functions beyond Hamming loss and Precision\u03ba  Are there specific computational or algorithmic considerations that need to be addressed when implementing the theoretical frameworks proposed in this research 5. Limitations  The complexity of the mathematical derivations and assumptions utilized in the study may limit the practical utility of the findings for nonexpert audiences or for direct implementation in industry applications.  The lack of experimental validation or practical case studies may hinder the immediate translation of the theoretical insights into tangible solutions for realworld multilabel classification challenges.  The narrow focus on specific loss functions and assumptions may restrict the generalizability of the results to diverse MLC scenarios or applications with different requirements or constraints. In conclusion the paper provides valuable contributions to the theoretical understanding of MLC through rigorous analyses and novel approaches. However to enhance the impact and practical relevance of the research further empirical validations and exploration of broader application scenarios would be beneficial.", "rWgfLdqVVl_": " Peer Review: 1) Summary: The paper introduces an unsupervised transformerbased Visual Concepts Tokenization framework VCT which aims to extract visual concepts from images by generating a set of disentangled visual concept tokens. The framework utilizes crossattention to extract visual information from image tokens without selfattention between concept tokens. The study shows that VCT achieves stateoftheart results in disentangled representation learning and scene decomposition tasks with experiments demonstrating its effectiveness on various datasets. Notably VCT aligns well with language representations enabling image content control using text input. 2) Strengths and Weaknesses: Strengths:  The framework addresses the challenge of extracting visual concepts from images in an unsupervised manner achieving significant improvements in disentangled representation learning and scene decomposition tasks.  The use of crossattention for extracting visual information and the Concept Disentangling Loss are novel and contribute to the effectiveness of the proposed framework.  Extensive experiments on multiple datasets validate the performance of VCT and showcase its potential advancements in visual concept learning. Weaknesses:  The paper could provide more details on the implementation and architecture of VCT for better understanding by readers.  The evaluation metrics used could be further discussed in terms of their relevance to the tasks and the interpretation of the results.  While the paper highlights the benefits of VCT limitations such as scalability to larger datasets and the need for manual annotation for evaluations could be explored. 3) Questions:  Can the disentangled visual concept tokens derived by VCT be compared or related to interpretable representations derived by humans in any way  How does VCT handle the presence of noise or variations in images that could affect the representation of visual concepts  Are there any considerations for enhancing the interpretability of the extracted visual concept tokens beyond the presented experiments 4) Limitations:  The study focuses on smaller datasets limiting the generalizability and applicability of VCT to largerscale datasets with more complex scenes.  The paper lacks detailed discussions on potential societal impacts and ethical considerations of implementing VCT in realworld applications especially regarding privacy and data security. Overall the paper presents a comprehensive framework for visual concept tokenization and disentangled representation learning showcasing promising results on various datasets. Further clarity on implementation details evaluation methodologies and addressing scalability could enhance the robustness and usability of VCT in practical applications.", "lArVAWWpY3": " Review of the Scientific Paper \"Statistical Robustness and Computational Aspects of Sliced Distances\"  1) Summary: The paper focuses on the scalability of Sliced Wasserstein distances in high dimensions addressing three key aspects: empirical convergence rates robust estimation and computational guarantees. The authors derive sharp empirical convergence rates characterize dimensionfree robust estimation risks and provide formal guarantees for popular computational methods for both the average and maxsliced Wasserstein distances.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important and timely research question on the scalability of sliced Wasserstein distances in highdimensional settings.  The analysis is comprehensive and covers empirical robustness and computational aspects providing a holistic view of the scalability issue.  Derivation of sharp empirical convergence rates with explicit dimension dependence adds depth to the understanding of these distances.  The paper provides formal guarantees for commonly used computational methods for average and maxsliced Wasserstein distances.  The experimental validation supports the theoretical findings and provides insights into the practical implications of the results. Weaknesses:  The paper may benefit from more detailed explanations in certain sections especially for readers with less background knowledge in the field.  While the theoretical analysis is comprehensive additional discussion on the limitations and assumptions made in the study could further enrich the paper.  4) Questions:  How do the findings of the empirical convergence rates impact the application of sliced Wasserstein distances in realworld scenarios particularly in data analysis or machine learning tasks  Is there potential for extending the study to explore the scalability of sliced Wasserstein distances in more complex probability distribution settings or applications with nonEuclidean data structures  How do the derived robust estimation rates compare to existing methods in terms of accuracy and efficiency and are there tradeoffs that researchers should consider when choosing a method for robust estimation  5) Limitations:  The study primarily focuses on logconcave distributions and may not fully capture the behavior of sliced Wasserstein distances in nonlogconcave settings.  The computational complexity bounds derived for the algorithms might require further validation in practical scenarios to assess their performance on diverse datasets.  The study may benefit from exploring the impact of different noise levels dataset sizes or distribution characteristics on the scalability and robustness of sliced Wasserstein distances. Overall the paper makes significant contributions to the understanding of the scalability of sliced Wasserstein distances and provides valuable insights for researchers in the field of statistical inference and machine learning. The thorough analysis and experimental validation enhance the credibility of the findings presented in the paper.", "klElp42K9U0": " Peer Review 1) Summary: The paper introduces a novel pipeline called Split Select Retrain (SSR) for automated algorithmhyperparameter selection for offline reinforcement learning (RL) using historical datasets. The SSR pipeline incorporates repeated random subsampling (RRS) for robust AH selection and policy deployment by training on multiple data splits. The experimentations across various domains demonstrate that SSRRRS consistently outperforms other methods in selecting highperforming policies in small data regimes. The study emphasizes the crucial role of proper algorithmhyperparameter selection in achieving superior policy performance in offline RL scenarios. 2) Strengths and Weaknesses: Strengths:  The paper addresses a significant challenge in offline RL by proposing a systematic approach for AH selection using historical data.  The SSRRRS pipeline is welldocumented with clear algorithmic steps and pseudocode provided for implementation.  The experiments conducted across diverse domains provide comprehensive evidence of the effectiveness of SSRRRS in selecting highperforming policies.  The paper highlights the importance of RRS in reducing the variance introduced by data partitioning and its impact on AH selection.  The proposed approach is compared with relevant baselines enriching the understanding of its performance relative to existing methods. Weaknesses:  The paper could benefit from a more detailed explanation of the implications of realworld applications and deployment scenarios for the SSRRRS pipeline.  While the experiments cover various domains further analysis on the scalability and computational efficiency of the SSRRRS pipeline could enhance the practical applicability of the proposed approach.  A deeper exploration of the limitations of the SSRRRS pipeline such as situations where the method may not be optimal could provide a more nuanced perspective on its generalizability. 3) Questions:  Could the SSRRRS pipeline be adapted or tuned for specific domains or applications to optimize its performance further  How does the computational overhead of implementing the RRS approach compare to traditional methods in AH selection for offline RL  Are there specific challenges or tradeoffs involved in extending the SSRRRS pipeline to handle larger datasets or more complex RL environments 4) Limitations:  The study primarily focuses on AH selection and policy deployment in offline RL with historical data potentially limiting the analysis to specific scenarios and data sizes.  The reliance on specific OPE methods for policy evaluation might introduce bias towards certain types of algorithms which could impact the generalizability of the results.  The performance evaluation of the SSRRRS pipeline could benefit from a deeper investigation into the robustness of the approach under varying dataset characteristics and noise levels. In conclusion the paper presents a valuable contribution to the field of offline RL through the introduction of the SSRRRS pipeline for automated AH selection. Further studies on the scalability applicability to realworld settings and generalizability of the method could enhance its impact and adoption within the research community.", "p0LJa6_XHM_": " Summary The paper introduces the Sleeper Agent backdoor attack a novel method that effectively poisons deep neural networks trained from scratch without directly inserting triggers into training data. By employing gradient matching data selection and adaptive retraining Sleeper Agent successfully backdoors neural networks even in blackbox scenarios. The attack is shown to be effective on datasets like CIFAR10 and ImageNet demonstrating high performance without significant drops in validation accuracy.  Strengths and Weaknesses Strengths: 1. Innovative Attack: Introduces a novel backdoor attack method Sleeper Agent which successfully poisons neural networks trained from scratch without visible triggers. 2. Effectiveness: Demonstrates the effectiveness of Sleeper Agent on various architectures and in blackbox settings. 3. Gradient Matching: Utilizes gradient matching and data selection strategies to enhance the success of the attack. 4. Experimental Results: Provides comprehensive experimental results on CIFAR10 and ImageNet showcasing the efficacy of the attack. Weaknesses: 1. Lack of Realworld Application: While the attack is effective in experimental settings its realworld applicability and generalizability to diverse datasets and applications remain to be investigated. 2. Complexity: The method involves intricate optimization procedures and heuristics which may limit its practical implementation and ease of understanding for nonexperts. 3. Limited Evaluation: The evaluation is primarily focused on standard datasets and the generalizability of the attack to more complex scenarios and domains is not extensively explored.  Questions 1. Generalizability: How transferable is the Sleeper Agent attack to other datasets beyond CIFAR10 and ImageNet 2. Adversarial Robustness: How does the attack perform against adversarial defenses like adversarial training or detection mechanisms 3. Impact: Can the attack be detected by existing security mechanisms and what are the implications for deploying robust defenses against such backdoor attacks  Limitations 1. Dataset Dependency: The study focuses on CIFAR10 and ImageNet limiting the understanding of the attacks behavior on diverse or more complex datasets. 2. Evaluation Scenarios: The evaluation primarily considers controlled experimental setups necessitating further exploration in realworld or industryspecific contexts. 3. Complexity: The complexity of the attack methodology may hinder practical implementation or adoption by practitioners with limited expertise in adversarial machine learning. Overall the paper presents valuable insights into the development of hidden trigger backdoor attacks but further investigations are required to address broader impacts realworld applicability and robustness against advanced defenses.", "sFapsu4hYo": " Peer Review  Summary: The paper investigates dynamic sparse training for sparse models with shuffled block structures aiming to accelerate the training process while maintaining accuracy. The proposed algorithm adapts the sparse model by dropping unimportant weights and adding new parameters in shuffled blocks. Experimental results on various networks and datasets show promising results with dense accuracy achieved at various levels of sparsity. The paper introduces novel concepts of blockaware drop criterion and blockwise grow criterion enhancing the model adaptation process.  Strengths: 1. Innovative Approach: The paper introduces a novel method for dynamic sparse training with shuffled block structures addressing the challenge of accelerating training while maintaining accuracy. 2. Experimental Validation: Extensive experiments on various networks and datasets provide strong evidence of the effectiveness of the proposed algorithm in achieving high accuracy at different levels of sparsity. 3. Detailed Explanations: The paper provides clear explanations of the algorithms and methodologies used making it easy for readers to understand the concepts presented. 4. Comparative Analysis: The comparison with existing sparse training methods and discussions on the tradeoffs between accuracy and hardware efficiency add value to the study.  Weaknesses: 1. Hardware Dependence: The reliance on specific hardware architectures like sparse tensor cores may limit the generalizability of the proposed method to other hardware platforms. 2. Complexity: The shuffled blocking operation is computationally expensive leading to significant overhead. Further optimization strategies could be explored to reduce this overhead. 3. Lack of Realworld Implementation: While the results are promising in experimental settings realworld implementations and practical considerations are not thoroughly addressed.  Questions: 1. Scalability: How does the proposed algorithm perform on even larger models or datasets in terms of scalability and computational efficiency 2. Generalizability: Can the algorithm be adapted to work on different hardware platforms beyond the ones mentioned in the paper 3. Longterm Effects: How does the proposed training method impact the longterm stability and generalizability of the trained models compared to traditional dense training methods  Limitations: 1. Hardware Dependency: The reliance on specific hardware may limit the algorithms applicability to a broader range of hardware platforms. 2. Overhead: The overhead of shuffled blocking poses a challenge in terms of computational resources which could impact the practicality of the algorithm in realworld scenarios. 3. Generalizability: The performance of the algorithm on a diverse set of networks and datasets beyond those tested in the study is not investigated extensively. Overall the paper presents an innovative approach to dynamic sparse training with shuffled block structures showing promising results in terms of accuracy and efficiency. Addressing the weaknesses and limitations highlighted could further enhance the impact and applicability of the proposed algorithm in practical settings.", "nrksGSRT7kX": " Peer Review of \"Robust Adversarial ModelBased Offline RL (RAMBO)\"  1) Summary: The paper introduces Robust Adversarial ModelBased Offline RL (RAMBO) a novel approach to offline reinforcement learning that formulates the problem as a twoplayer zerosum game against an adversarial environment model. The model is trained to minimize the value function while predicting transitions accurately from the dataset for conservative policy optimization. The proposed algorithm is theoretically grounded providing a performance guarantee and a pessimistic value function that lower bounds the true environment value. RAMBO outperforms existing stateoftheart baselines on widely studied offline RL benchmarks.  2) Strengths:  The paper introduces a novel approach to modelbased offline RL addressing the problem as a twoplayer game against an adversarial model demonstrating theoretical grounding and strong empirical results.  RAMBOs adversarial training of the model for conservatism is a promising method to prevent model exploitation and improve policy optimization.  The comparison with stateoftheart algorithms especially COMBO and the ablation study on adversarial training provide a comprehensive evaluation of RAMBO.  Weaknesses:  While the theoretical grounding and empirical results are convincing a more detailed analysis of the algorithms complexity and scalability would strengthen the papers contribution.  Further discussion on the sensitivity of the algorithms performance to hyperparameters and its generalizability to different domains could be beneficial.  The introduction of limitations and potential failure cases of RAMBO would provide a more balanced discussion of its effectiveness.  3) Questions:  Can you provide more insights into the scalability and computationally complexity of the RAMBO algorithm especially in largescale applications or domains with highdimensional state spaces  How sensitive is the performance of RAMBO to hyperparameters and are there any specific settings that are crucial for achieving optimal results in various offline RL datasets  Are there limitations or specific scenarios where RAMBO may not perform as effectively as traditional modelbased offline RL algorithms and how does it compare in online RL settings  4) Limitations:  The paper primarily focuses on empirical results and theoretical justification which may benefit from further explanations on the practical implementation challenges or limitations of RAMBO.  While the experiments cover a diverse set of benchmarks a more extensive evaluation on various realworld scenarios or domains beyond the benchmark datasets would enhance the robustness of the algorithms performance assessment. In conclusion the paper presents a novel and promising algorithm for modelbased offline RL with strong theoretical grounding and empirical performance. Further exploration into scalability sensitivity to hyperparameters and potential shortcomings of RAMBO can enhance the understanding of its efficacy and applicability in broader contexts.", "rOimdw0-sx9": " Peer Review: 1) Summary: The paper introduces a novel framework called DiAMetR for metareinforcement learning that addresses the challenge of distribution shift in testtime task distributions. By training a population of metapolicies with varying levels of robustness to distribution shifts DiAMetR aims to adaptively choose the most appropriate policy for a given test distribution. The paper provides a thorough theoretical framework algorithm description and experimental evaluation on simulated robotics problems. 2) Strengths:  The paper addresses an important challenge in metaRL by proposing a framework to handle distribution shift in testtime task distributions.  The theoretical analysis concerning the properties of the proposed algorithm and the relation between the level of robustness and performance provides valuable insights.  The experimental evaluation on four continuous control environments demonstrates the effectiveness of DiAMetR in adapting to varying levels of distribution shift.  Clear and detailed descriptions of the algorithm tasks and experiments help in understanding the proposed framework. Weaknesses:  The paper could provide more clarity in explaining the algorithmic implementations especially in terms of the optimization process and parameterizations.  The discussion on the limitations of the proposed framework could be further expanded to provide additional insights for future work.  More comparisons with existing stateoftheart methods could enhance the evaluation of DiAMetRs performance against other approaches in handling distribution shift. 3) Questions:  How does the proposed framework compare with other adaptive metaRL algorithms in terms of computational complexity and convergence speed  Are there any specific scenarios where DiAMetR might not perform optimally and how does it handle noisy or sparse reward signals  Can the proposed framework be extended to more complex environments with image observations or highdimensional action spaces 4) Limitations:  The experimental evaluation is limited to simulated robotics problems and the generalizability of DiAMetR to realworld applications needs further validation.  The paper focuses on tasks with known shift levels. How well does DiAMetR perform in scenarios where the shift level is continuously changing or unknown  The paper lacks a detailed explanation of the practical challenges in implementing DiAMetR and deploying it in realworld scenarios. Overall the paper makes a significant contribution to the field of metareinforcement learning by addressing an important challenge of handling distribution shift in testtime task distributions. The proposed DiAMetR framework shows promising results in adapting metapolicies to varying levels of distribution shifts and provides a strong foundation for further research in this area.", "nJt27NQffr": "Title: Maximum Entropy Coding for SelfSupervised Learning: A Principled Approach to Generalizable Representations 1) Summary: The paper proposes Maximum Entropy Coding (MEC) a selfsupervised learning method that aims to alleviate biases introduced by existing pretext tasks and improve generalization to unseen downstream tasks. Inspired by the principle of maximum entropy MEC optimizes the structure of representations by leveraging the minimal coding length in lossy data coding as a computationally tractable surrogate for entropy. Extensive experiments show that MEC achieves stateoftheart performance on various downstream tasks in computer vision surpassing previous methods based on specific pretext tasks. 2) Strengths and weaknesses: Strengths:  Novel approach: Introduces a principled objective (MEC) that explicitly optimizes representation structure for improved generalization.  Theoretical foundation: Builds on the maximum entropy principle from information theory.  Comprehensive experiments: Demonstrates the efficacy of MEC across various downstream tasks datasets and architectures.  Equivalence insight: Shows an interesting equivalence between loworder approximations of MEC and existing SSL objectives. Weaknesses:  Complexity: The method involves a sophisticated formulation based on maximum entropy coding which may be challenging for some readers to grasp.  Computational cost: The original formulation of entropy estimation may be computationally expensive for largescale pretraining although the proposed reformulation addresses this.  Limited comparisons: While MEC outperforms existing methods in experiments a more extensive comparison with a wider range of SSL methods could provide a more comprehensive evaluation. 3) Questions:  How sensitive is the performance of MEC to hyperparameters such as the order of Taylor expansion or the distortion measure \\xcf\\xb5d  Can you elaborate on the practical implications and potential limitations of integrating MEC as a regulation term with other SSL methods like SimCLR and SimSiam  Could you comment on the interpretability of the representations learned by MEC and how they compare to representations acquired through other SSL methods 4) Limitations:  Generalization beyond computer vision: The paper focuses on computer vision tasks hence the applicability of MEC to other domains remains unexplored.  Reproducibility: While code and pretrained models are provided additional information on hyperparameter settings training protocols and additional datasets used in experiments would enhance reproducibility.  Scalability: Although the reformulation of the coding length function in MEC addresses scalability concerns further investigation into the scalability of the method to even larger datasets and architectures would be beneficial. Overall the paper presents a compelling method for selfsupervised representation learning with a strong theoretical grounding. Addressing the questions and limitations outlined above could further strengthen the impact and understanding of the proposed approach.", "uIXyp4Ip9fG": "Peer Review 1) Summary The paper introduces Active Surrogate Estimators (ASEs) for labelefficient model evaluation addressing the challenge of evaluating model performance when labels are costly. ASEs actively learn a surrogate model and propose the XWED acquisition strategy for tailored learning. The authors demonstrate that ASEs outperform existing methods in evaluating deep neural networks for image classification. 2) Strengths and Weaknesses Strengths:  Novel approach: ASEs introduce an innovative method for labelefficient model evaluation.  XWED acquisition strategy: The proposed acquisition strategy is tailored to the task and enhances performance.  Theoretical insights: The paper provides a detailed theoretical analysis of ASE errors breaking them down into components for better understanding. Weaknesses:  Limited empirical evaluation: While the paper provides experimental results additional experiments on diverse datasets could enhance the generalizability of the findings.  Computational complexity: The computational complexity of ASEs is a concern especially in scenarios with large datasets.  Lack of comparison with more active learning methods: Comparing the performance of ASEs with a wider range of active learning strategies could provide a more comprehensive assessment of their effectiveness. 3) Questions  How does the XWED acquisition strategy perform in scenarios with varying dataset sizes and label acquisition budgets  What are the implications of the computational complexity of ASEs in realworld applications with large datasets  Can ASEs be applied effectively to tasks beyond image classification such as natural language processing or reinforcement learning 4) Limitations  Lack of extensive empirical evaluation: While the results are promising more diverse experiments on various datasets and model architectures could further validate the effectiveness of ASEs.  Scalability concerns: The computational complexity of ASEs may limit their applicability to largescale datasets or realtime tasks.  Generalizability limitations: The paper focuses on specific scenarios like image classification which may not fully represent the broad applicability of ASEs. In conclusion the paper presents a compelling new method for labelefficient model evaluation with ASEs and the XWED acquisition strategy. Further empirical validation and exploration of scalability concerns and generalizability to other domains could enhance the impact and applicability of these findings.", "l1WlfNaRkKw": " Peer Review 1. Summary The paper explores PAC learnability under transformation invariances in three settings: invariantly realizable relaxed realizable and agnostic. It evaluates data augmentation (DA) theoretically and proposes optimal algorithms based on combinatorial measures. The main contributions include showing that DA outperforms vanilla empirical risk minimization but is suboptimal in some settings. Through various theorems the optimal sample complexity is characterized by different dimensions depending on the setting. 2. Strengths and Weaknesses Strengths:  Rigorous theoretical analysis on PAC learnability under transformation invariances.  Clear definitions of the settings and theoretical guarantees for different scenarios.  Proposing optimal algorithms based on combinatorial measures.  Providing lower bounds and adaptive algorithms to handle different levels of invariance. Weaknesses:  The theoretical nature of the study may limit the immediate practical applications.  Some definitions and concepts might be complex for readers not deeply familiar with the subject matter.  The paper focuses on deterministic labels which might not fully capture realworld scenarios.  Limited discussion on the practical implications and applications of the theoretical findings. 3. Questions  How do the proposed algorithms perform in realistic scenarios with noisy data or when the data distribution deviates from the assumptions in the theoretical analysis  Are there any plans to validate these theoretical findings through empirical studies or realworld applications  How does the concept of invariance under probabilistic labels relate to the presented framework and can it be incorporated into future research  Can the results of DA and optimal algorithms be extended to handle nongroup transformations effectively 4. Limitations The study focuses primarily on theoretical analysis without deep exploration of practical implications or empirical validation. The assumptions of deterministic labels and Ginvariance may limit the applicability of the findings to realworld scenarios with noisy or complex data distributions. The paper could benefit from more discussion on the practical relevance and potential extensions to handle more diverse scenarios. Overall the paper presents valuable insights into PAC learnability under transformation invariances offering theoretical foundations and optimal algorithms. To enhance the impact and relevance of the work further exploration into practical applications and validations may be beneficial.", "tVbJdvMxK2-": " Peer Review 1) Summary The paper proposes a novel approach Global Pseudotask Simulation (GPS) to address catastrophic forgetting in continual learning by formulating dynamic memory construction as a combinatorial optimization problem. The authors take an offline approach to reduce the search space and provide an approximate solution in an online continual learning setting using simulated pseudotasks. Empirical results on vision benchmarks demonstrate that GPS outperforms existing methods especially in long task sequences. 2) Strengths and Weaknesses Strengths:  The paper addresses an important challenge in continual learning and proposes a unique solution.  The dynamic memory construction problem is wellformulated and the GPS method is innovative.  The paper provides thorough experimental evaluations on commonly used vision benchmarks demonstrating the effectiveness of GPS.  The proposed method is shown to achieve higher accuracy compared to existing baselines especially in long task sequences. Weaknesses:  The paper lacks comparison with more recent stateoftheart continual learning methods besides the selected baselines.  The complexity of the method and its applicability to realworld scenarios could be discussed further.  The paper could benefit from a more indepth exploration of the limitations and future directions of the proposed approach. 3) Questions  Could the authors elaborate on the practical implications of GPS in realworld scenarios outside of the experimental settings  How does the method scale with larger and more complex datasets beyond the ones used in the experiments  Are there any specific conditions or requirements for the successful implementation of the proposed GPS method 4) Limitations  The paper focuses on vision benchmarks limiting the generalizability of the proposed method to other domains.  The experimental evaluations mainly involve offline and synthetic settings realworld experiments or applications could provide additional insights.  The simulation time cost of GPS could be a limitation in largescale scenarios if not carefully managed. Overall the paper introduces a promising approach to dynamic memory construction in continual learning through GPS and demonstrates its efficacy in addressing catastrophic forgetting. Further exploration of realworld applications and scalability considerations could enhance the impact of the proposed method.", "ttQ_3CiZqd3": " Peer Review:  Summary: The paper introduces a novel principle for learning equilibrium systems with a temporally and spatially local rule casting learning as a leastcontrol problem. By incorporating learning signals within a dynamics as optimal control the paper avoids storing intermediate states in memory and does not rely on infinitesimal learning signals. The principle is applied to various neural network architectures including feedforward and recurrent networks demonstrating strong performance matching leading gradientbased learning methods. The results shed light on how the brain might learn and offer new ways to approach a broad class of machine learning problems.  Strengths: 1. Novel and Important Concept: The paper introduces a novel principle for learning equilibrium systems which is a significant contribution to the field of neural network learning. 2. Theoretical Grounding: The paper provides a sound theoretical framework for the proposed principle outlining the conditions under which the leastcontrol solutions correspond to minima of the original objective. 3. Applicability: The principle is applied to various architectures demonstrating competitive performance in both feedforward and recurrent neural networks as well as in metalearning tasks. 4. Comparative Analysis: A thorough comparative analysis with existing methods highlights the effectiveness and competitiveness of the proposed principle.  Weaknesses: 1. Limitation to Equilibrium Systems: The fact that the principle is limited to equilibrium systems is acknowledged as a limitation. Future work could focus on extending the principle to outofequilibrium dynamics. 2. Computational Cost: The study points out that the computational cost of the proposed principle often exceeds that of performing the two phases of recurrent backpropagation which limits its immediate practical adoption in complex scenarios. 3. Generalization of Loss Function: The restriction on the loss function not directly depending on the parameters \\xce\\xb8 could limit the general applicability of the principle. Considering extensions to accommodate this aspect might be valuable.  Questions: 1. Robustness to Noise: How robust is the proposed principle to noise in the learning process especially given the focus on optimal control and activitydependent local learning rules 2. Scalability and Implementation: Are there considerations regarding the scalability and practical implementation of the principle particularly when dealing with largescale neural network architectures 3. Biological Plausibility: How biologically plausible is the proposed principle in the context of neural computations and learning mechanisms observed in the brain  Limitations: 1. Equilibrium Assumption: The limitation to equilibrium systems might restrict the applicability of the principle to dynamic and evolving systems requiring further investigation and development. 2. Computational Cost: The higher computational cost compared to existing methods raises questions about practical feasibility especially in largescale applications. 3. Loss Function Dependency: The restriction on the loss function not directly depending on \\xce\\xb8 limits the generality of the principle and its adaptability to a wider range of learning scenarios. Overall the paper presents a valuable contribution to the field of neural network learning offering a novel perspective and approach. Addressing the limitations and questions raised could further enhance the understanding and practical implementation of the proposed principle.", "mn1MWh0iDCA": " Peer Review  1) Summary: The paper explores the behaviors of Offline Reinforcement Learning (RL) agents through a series of experiments evaluating representations value functions and policies. Surprisingly it reveals that a more performant agent may have lowquality representations and inaccurate value functions. The study introduces a novel offline RL algorithm and investigates the effectiveness of different policy evaluationimprovement methods. Furthermore the research explores the role of learned dynamics models and proposes an uncertaintybased sample selection method to handle model noises.  2) Strengths and Weaknesses: Strengths:  The paper thoroughly examines the behaviors of offline RL agents addressing fundamental questions and gaps in understanding.  The experiments are welldesigned to evaluate representations value functions and policies providing valuable insights.  The introduction of a novel offline RL algorithm and the investigation into the effectiveness of existing methods contribute significantly to the field.  The proposed uncertaintybased sample selection method provides a practical solution to mitigating the challenges posed by model noises. Weaknesses:  While the experiments suggest crucial findings about offline RL agents behaviors more indepth analysis or comparisons with existing literature could enhance the papers impact.  The paper could benefit from more detailed explanations of the proposed methods and algorithms for better understanding by the readers.  Providing more context on the selection of benchmark tasks datasets and evaluation metrics would help readers gauge the significance of the results better.  3) Questions:  How do the findings of this study align with existing research on offline RL and what novel contributions does this work offer  Can the proposed offline RL algorithm outperform existing stateoftheart methods on a wider range of tasks and environments  How generalizable are the insights gained from the experiments conducted in this study to other offline RL settings and scenarios  What implications do the results have for realworld applications of offline RL especially in domains like robotics selfdriving cars and medical treatments  4) Limitations:  The paper might benefit from a more detailed discussion on the potential limitations of the proposed experiments algorithms or methods.  While the provided results are insightful the study could include a broader range of experiments to strengthen the generalizability and robustness of the conclusions.  The paper could elaborate on the scalability and computational efficiency of the proposed methods especially considering realworld applications. Overall the paper offers valuable insights into understanding the behaviors of Offline RL agents and introduces innovative approaches to address challenges in the field. Further discussions comparisons and clarifications could enhance the impact and applicability of the research findings.", "uP9RiC4uVcR": " Summary: The paper introduces the MoralExceptQA challenge set aimed at assessing AI systems ability to understand and predict human moral judgments in scenarios that involve potentially permissible moral exceptions. Using a novel MORALCOT prompting strategy the study demonstrates improved performance by 6.2 F1 over existing large language models (LLMs) in predicting human moral judgments. The work sheds light on the complexity and flexibility of human moral reasoning providing insights into the challenges AI systems face in navigating novel moral situations.  Strengths: 1. Innovative Challenge Set: Introducing the MoralExceptQA challenge set the paper offers a novel framework to evaluate AI models understanding of moral flexibility. 2. Improved Model Performance: The MORALCOT prompting strategy showcases a significant improvement in predicting human moral judgments compared to existing LLMs. 3. Theoretical Grounding: The studys integration of cognitive science theories into AI systems demonstrates a thoughtful approach to capturing human moral reasoning.  Weaknesses: 1. Limited Dataset Size: The paper acknowledges the datasets size limitation which could impact the generalizability of the findings. A larger dataset would provide more robust conclusions. 2. Lack of Demographic Variation: The study is limited to U.S. residents overlooking potential cultural geographic and sociocultural variations in moral judgment that could affect the models performance. 3. Limited Error Analysis: While the paper conducts a detailed error analysis deeper insights into model behavior and potential biases could enhance the studys completeness.  Questions: 1. Scalability: How scalable is the proposed MORALCOT model to larger and more diverse datasets beyond the MoralExceptQA challenge set 2. Interpreting Model Decisions: How transparent is the MORALCOT model in explaining its decisions especially in cases where it deviates from human judgments 3. Generalizability: How transferable are the findings and the MORALCOT approach to realworld applications beyond the MoralExceptQA framework  Limitations: 1. Dataset Size: The smallscale dataset may limit the models ability to capture the full spectrum of human moral reasoning. 2. Demographic Bias: Restricted to U.S. residents the study lacks diversity in demographic representation potentially limiting the models applicability across broader populations. 3. Ethical Considerations: While intended for AI safety research potential ethical implications in automating moral decisions should be carefully monitored and addressed. Overall the paper presents a valuable contribution to AI ethics and safety research showcasing promising advancements in understanding human moral judgment. Addressing the identified weaknesses and limitations would further enhance the studys impact and applicability in realworld AI systems.", "m6DJxSuKuqF": " Peer Review: Summary: The paper introduces a novel KeyPointGuided model by ReLation preservation (KPGRL) for optimal transport (OT) matching. The model leverages annotated keypoints across domains to guide correct matching in OT by preserving keypoint pairs and relations to keypoints in the transport plan. The proposed KPGRL model is extended to partial OT settings and applied to heterogeneous domain adaptation (HDA). Experimental results demonstrate the effectiveness of the KPGRL model in improving matching accuracy. Notably the paper covers detailed derivations and comparisons with existing OT and HDA models. Strengths: 1. Innovative Approach: The integration of keypoints for guiding OT matching is a novel and practical approach aiding correct matching in scenarios where partial or heterogeneous distributions are involved. 2. Thorough Methodology: The paper provides a comprehensive discussion of the proposed KPGRL model including the formulation constraints and extensions. The theoretical derivations and constraints are wellexplained. 3. Experimental Evaluation: The experiments conducted on toy data and HDA tasks demonstrate the superiority of the KPGRL model over existing OT and HDA methods showcasing the practical relevance and effectiveness of the proposed approach. 4. Detailed Analysis: The detailed comparison with existing models both in OT and HDA provides a clear understanding of the advantages of the proposed KPGRL model. The analysis on varying keypoints and labeled target data numbers adds depth to the evaluation. Weaknesses: 1. Complexity: The paper may be challenging to follow for readers unfamiliar with optimal transport or domain adaptation. Simplifying the technical language and concepts where possible could improve accessibility. 2. Clarity in Explanations: Some parts of the paper especially the derivations and formulations may require further clarification to enhance understanding. Additional examples or visual aids could aid in explaining complex concepts. 3. Evaluation Metrics: While the experiments demonstrate the efficacy of the proposed model incorporating more diverse evaluation metrics could provide a wellrounded evaluation addressing aspects like computation time memory requirements and robustness. Questions: 1. Could the paper further elaborate on the computational complexity of the proposed KPGRL model compared to existing OT and HDA models particularly in realworld applications with large datasets 2. How sensitive is the KPGRL model to hyperparameters such as temperature (\u03c4 \u03c4) temperature coefficients (\u03c1) and the number of keypoint pairs Has the impact of these hyperparameters been extensively studied 3. In practical scenarios how feasible is the annotation of keypoints and their relevance in various applications outside the scope of HDA Are there considerations for automated keypoint detection or selection methods Limitations: 1. The paper focuses primarily on mathematical formalism and experimental validation leaving out discussions on realworld implementation challenges or considerations. 2. The application of the proposed KPGRL model is limited to specific tasks like HDA and the generalizability to a wider range of applications and datasets remains unexplored. 3. The realworld scalability and generalizability of the proposed model might be limited by the assumptions made in the keypointguided approach. Overall the paper presents a novel and valuable contribution to the field of optimal transport by leveraging annotated keypoints for guiding correct matching. With some enhancements in clarity additional experiments and considerations for practical applications the proposed KPGRL model could have broader applicability and impact in various domains.  End of Review.", "xWvI9z37Xd": " Peer Review  1) Summary The paper introduces a new efficient unsupervised method for feature selection called WAST based on sparse autoencoders. WAST optimizes the sparse topology during training to quickly detect informative features. Extensive experiments on 10 datasets show that WAST outperforms existing methods in terms of selecting informative features while reducing training iterations and computational costs significantly.  2) Strengths and weaknesses Strengths:  The paper addresses an important problem of feature selection using neural networks.  The proposed method WAST demonstrates superior performance in terms of selecting informative features while reducing computational costs.  The experiments cover a wide range of dataset types and characteristics showcasing the effectiveness of WAST.  The methodology is welldescribed and the experiments are extensive. Weaknesses:  The paper lacks indepth discussion on the theoretical foundations of the proposed method.  While the results are promising more detailed analysis of the comparative performance across different dataset types would enhance the paper.  The paper could benefit from additional insights into the interpretability of the selected features through WAST.  3) Questions  What are the specific criteria used to determine the importance of neurons and connections during training How do these criteria differ from existing methods  Can you provide more insight into the robustness of WAST in extremely noisy environments How does WAST handle noisy features effectively  How do you envision the practical implementation of WAST in realworld scenarios with large and diverse datasets  4) Limitations  One limitation of the study is the lack of exploration into the interpretability of the selected features by WAST. Adding a discussion on this aspect would enhance the value of the paper.  The discussion on the limitations of the hardware support for sparse training methods could be expanded to incorporate potential future implications and developments in the field. Overall the paper presents a strong contribution in the field of feature selection using neural networks. Addressing the questions and limitations highlighted above would further enrich the paper and provide valuable insights for the readers.", "l7aekTjF6CO": " Peer Review  1) Summary: The paper investigates the representation and fairness tradeoff in sortitionbased democracy focusing on the random selection of representatives. It introduces a new representation metric and a selection algorithm aiming to analyze the relationship between representation and fairness. The study provides theoretical analysis algorithm development and empirical evaluation on realworld datasets.  2) Strengths and weaknesses: Strengths:  Novelty: The paper introduces a novel representation metric and selection algorithm to analyze the representationfairness tradeoff in sortitionbased democracy.  Theoretical Rigor: The study provides solid theoretical foundations by defining representation metrics proposing algorithms and proving results under different scenarios.  Empirical Evaluation: The empirical evaluation on realworld datasets adds practical insights to the theoretical findings.  Clarity: The paper is wellstructured and clearly presents the problem formulation methodology and results. Weaknesses:  Complexity: The analysis and mathematical formulations may be challenging for readers without a strong background in algorithmic fairness.  Limited Insight into RealWorld Implementation: While the theoretical and empirical evaluation are thorough a deeper discussion on the practical implications and challenges of implementing the proposed algorithms could enhance the study.  3) Questions:  Generalizability: To what extent can the findings and algorithms proposed in this study be applied to other domains beyond sortitionbased democracy  Scalability: How scalable are the proposed algorithms in realworld settings with large populations and diverse features  Fairness and Representation Metrics: How sensitive are the results to the choice of fairness and representation metrics Are there alternative metrics that could be explored  4) Limitations:  Assumptions and Simplifications: The study makes various assumptions about the population distribution and metric structure that may not always hold in practical scenarios limiting the generalizability of the results.  Applicability: While the study provides valuable insights into the representationfairness tradeoff within the context of sortition the direct applicability of the findings to broader democratic processes may require further investigation. Overall the paper makes a significant contribution to the understanding of representation and fairness in sortitionbased democracy offering novel insights and methodologies for further research in this field. Further clarification on the practical implications and generalizability of the findings would enhance the impact of the study.", "yZ_JlZaOCzv": " Peer Review: 1) Summary: The paper explores the vulnerability of AlphaZero (AZ) agents to adversarial attacks in the game of Go. The authors introduce the concept of adversarial examples in Go and devise an algorithm that efficiently searches for perturbed states that lead to significant mistakes by AZ agents. The results demonstrate the agents susceptibility to such attacks even when using PolicyValue neural networks (PVNN) and Monte Carlo tree search (MCTS). The authors find that their method can consistently mislead the agents with high success rates across multiple datasets. Furthermore the robustness of different AZ agents to adversarial attacks is evaluated showcasing the effectiveness of the proposed attack method. 2) Strengths and Weaknesses: Strengths:  The paper addresses a novel and important research question regarding the vulnerability of AZ agents to adversarial attacks in the game of Go extending the application of adversarial examples to board games.  The proposed adversarial attack method is welldefined and systematically designed showcasing a thorough understanding of the challenges posed by attacking Go agents.  The experimental results provide convincing evidence of the vulnerability of AZ agents to the proposed attack with high success rates observed across various datasets.  The efficient search algorithm developed for finding adversarial states along with the methodology to involve a weaker verifier contributes to the robustness of the study. Weaknesses:  The paper lacks a detailed discussion on the implications of the findings on the broader field of artificial intelligence and reinforcement learning beyond Go AI.  While the success rates of the attacks are welldocumented a more indepth analysis of the specific types of mistakes made by the AZ agents could provide valuable insights into their decisionmaking processes.  The paper would benefit from further discussion on potential defenses against the identified vulnerabilities and suggestions for improving the robustness of AZ agents against adversarial attacks.  The limitations of the study such as constraints in guiding AZ agents to adversarial states could be elaborated upon to provide a more comprehensive understanding of the works scope. 3) Questions:  How do the identified vulnerabilities in AZ agents to adversarial attacks in the game of Go compare to vulnerabilities observed in other domains of artificial intelligence such as computer vision or natural language processing  Could the proposed adversarial attack method be generalized to other board games beyond Go and if so what challenges might arise in adapting the method to different game settings  What considerations were taken into account when selecting the thresholds and criteria for evaluating the success of the adversarial attacks and how do these choices impact the generalizability of the results 4) Limitations:  The study primarily focuses on identifying adversarial states in AZ agents but does not extensively explore avenues for strengthening the agents resilience to such attacks.  The reliance on human verifiers as a reference point for assessing the significance of the identified adversarial examples may introduce biases and limitations in the evaluation process.  While the robustness of PVNNs and PVMCTS agents to adversarial attacks is evaluated comprehensively further investigations into the underlying causes of their vulnerabilities could enhance the depth of the study.  The scalability of the proposed efficient attack algorithm to accommodate more complex game scenarios and larger search spaces remains a potential limitation that could be addressed through future research endeavors. In conclusion the paper presents a significant contribution to the field of reinforcement learning and AI by uncovering vulnerabilities in AZ agents through adversarial attacks in the game of Go. The wellstructured methodology experimental findings and proposed attack algorithm showcase the potential for enhancing the robustness and understanding of AI agents in strategic decisionmaking scenarios. Addressing the identified limitations and expanding the discussion on the broader implications of the study would further strengthen the impact and relevance of the research findings.", "ogNrYe9CJlH": " Peer Review 1) Summary The paper proposes a novel reductiontobinary (R2B) approach for enforcing demographic parity in multiclass classification with nonbinary sensitive attributes. The method is based on the alternating direction method of the multipliers (ADMM) framework and demonstrates improvements over existing baselines on synthetic and realworld datasets from various domains. 2) Strengths The paper presents a wellstructured and comprehensive methodology with a detailed explanation of the proposed R2B algorithm. The use of synthetic and realworld datasets from social science computer vision and healthcare for validation adds credibility to the results. The theoretical guarantees and empirical demonstrations provide strong support for the R2B approach. 3) Weaknesses  The paper primarily focuses on enforcing demographic parity and may benefit from further discussion on how the method could extend to other fairness metrics.  Limited comparison with other stateoftheart fairness techniques for multiclass classification with nonbinary sensitive attributes.  The implications of the proposed method in realworld applications and potential challenges in deployment could be further explored. 4) Questions  What are the implications of the R2B approach in practical settings with data constraints or dynamic environments where bias may evolve over time  How does the R2B approach compare with more complex inprocessing or postprocessing fairness methods for multiclass classification  Are there considerations for model interpretability and explainability when employing the R2B approach in sensitive domains like healthcare 5) Limitations  The study focuses on demographic parity and may not address other fairness metrics like equalized odds which could limit its applicability in certain contexts.  The inability to handle continuous sensitive attributes without binning could restrict the methods generalizability to diverse datasets.  The impact of preprocessing debiasing methods on model accuracy and performance in realworld scenarios may require further investigation. Overall the paper presents a novel approach for tackling bias in multiclass classification tasks with nonbinary sensitive attributes. While the R2B algorithm shows promise in improving fairness additional exploration of practical implications comparative analyses and extension to other fairness metrics would enhance the papers contributions to the field of algorithmic fairness in machine learning.", "pOEN7dDC0d": " Peer Review 1) Summary: The paper explores whether Harriss articulation scheme (HAS) applies to emergent languages by using unsupervised word segmentation. The study shows that although some prerequisites for HAS are met the segmentation boundaries may not always be semantically valid. 2) Strengths and weaknesses: Strengths:  Clear introduction and motivation for the study.  Comprehensive literature review and theoretical background.  Methodology wellexplained and experiments conducted systematically.  Results are wellsupported with visualization and analysis. Weaknesses:  The paper lacks discussion on the potential implications of the findings for the broader field of artificial intelligence or linguistics.  The study could benefit from a more detailed explanation of the experimental findings including implications for future research. 3) Questions:  How do the findings of this study contribute to the broader understanding of artificial communication and language development in AI systems  Were there any unexpected results during the experiments that were not discussed in the paper 4) Limitations:  The study focuses solely on artificially emerging languages limiting direct comparison or application to natural language development.  The paper does not address potential biases or limitations in the experimental setup. The paper provides valuable insights into the application of HAS to emergent languages but additional discussion on the broader implications and limitations of the findings would enhance its impact. Further clarification on unexpected results and potential biases would strengthen the papers contribution to the field.", "tro0_OqIVde": " Peer Review:  1) Summary: The paper introduces the Recursive Gated Convolution (gnConv) as an alternative to selfattention in vision Transformers highlighting its efficiency and effectiveness in achieving highorder spatial interactions. The proposed approach is evaluated on various tasks and datasets showcasing competitive performance compared to existing models.  2) Strengths:  Innovative Contribution: The introduction of gnConv as a convolutionbased approach to model highorder spatial interactions is a novel and significant contribution to the field.  Extensive Experiments: The paper conducts thorough experiments on ImageNet COCO and ADE20K datasets showcasing the effectiveness of HorNet compared to Swin Transformers and ConvNeXt.  Efficiency and Scalability: The paper demonstrates the efficiency and scalability of gnConv along with its compatibility with various convolutionbased models.  3) Weaknesses:  Comparison Metrics: While the paper presents competitive results a more detailed comparison of metrics like inference speed model size and computational complexity could enhance the analysis.  Theoretical Justification: While the practical implementations and results are wellexplained deeper theoretical justifications behind the effectiveness of gnConv could strengthen the paper.  4) Questions:  Generalization: How well does gnConv generalize to other tasks beyond ImageNet classification COCO object detection and ADE20K semantic segmentation  Scalability: Are there limitations to the scalability of gnConv with respect to extremely large datasets or complex models  Comparison with Transformer Models: Could the paper provide a more comparative analysis between gnConvbased models and existing Transformer architectures on different benchmarks  5) Limitations:  Hardware Compatibility: The paper mentions that HorNet is slower than ConvNeXt on GPUs. Addressing hardwarefriendly optimizations for gnConv could be beneficial.  Theoretical Discussions: While practical implications are strong deeper theoretical discussions on the longterm impact of highorder interactions in vision models could be explored further. Overall the paper makes a substantial and practical contribution to vision modeling by introducing gnConv and HorNet. Enhancing theoretical justifications exploring broader generalization capabilities and addressing hardware optimization concerns could further strengthen the findings presented in the paper.", "mfxq7BrMfga": "Peer Review 1) Summary: The paper presents a novel framework for the generalized oneshot GAN adaptation task addressing both style and entity transfer by decoupling the adaptation process. The method uses sliced Wasserstein distance to constrain internal distributions introduces an auxiliary network for entity generation and applies variational Laplacian regularization for crossdomain correspondence. Extensive experiments demonstrate the effectiveness of the proposed framework in various scenarios. 2) Strengths and Weaknesses: Strengths:  The paper addresses a novel and challenging problem of oneshot GAN adaptation comprehensively considering both style and entity transfer.  The proposed method is wellmotivated utilizing innovative techniques like sliced Wasserstein distance and variational Laplacian regularization.  Extensive experiments across various domains demonstrate the effectiveness of the framework both qualitatively and quantitatively. Weaknesses:  The paper lacks a detailed comparison with other stateoftheart methods especially for the specific task of generalized oneshot GAN adaptation.  The paper would benefit from additional theoretical analysis or ablation studies to further justify the effectiveness of the proposed components.  Some limitations of the method such as difficulty controlling complex entities or extreme pose changes are noted but could be explored further. 3) Questions:  How does the proposed framework compare to existing methods in terms of computational complexity and training time  Are there any specific scenarios or datasets where the proposed method may face challenges or limitations  Could further experiments or analysis be conducted to provide insights into the interpretability or robustness of the method 4) Limitations:  The study compares with a limited set of existing works and a more extensive comparison with recent methods might provide a better perspective on the performance.  The implications of the proposed method in practical applications beyond image synthesis and manipulation could be discussed further.  The limitations of the method particularly regarding controlling entities or handling extreme cases could be addressed more explicitly. Overall the paper presents a valuable contribution to the field of GAN adaptation addressing a challenging problem with a novel and comprehensive framework. Further exploration and analysis could enhance the understanding and applicability of the proposed method in various contexts.", "uPdS_7pdA9p": "Peer Review 1) Summary: The paper investigates the robustness of large pretrained foundation models (FMs) to group shifts focusing on improving group robustness without retraining. Through experiments the authors find that existing methods like linear probing and adapters do not consistently enhance group robustness leading to the development of a novel approach called contrastive adapting. This method significantly improves worstgroup accuracy without finetuning and by using only a fixed set of FM embeddings. 2) Strengths and Weaknesses: Strengths:  Thorough analysis of group robustness in foundation models.  Introduction of contrastive adapting as an effective and efficient method to enhance group robustness.  Clear explanation of methodological approaches and excellent organization of experiments.  Extensive evaluation across multiple benchmarks and models showcasing consistent improvements in worstgroup accuracy.  Efficient performance demonstrated with stateoftheart group robustness surpassing existing methods with minimal parameters. Weaknesses:  While the method is described in detail additional clarity on the mathematical notations and experimental setup might be beneficial.  The paper could provide more insights into potential areas for further exploration or limitations of the proposed method. 3) Questions:  Could you elaborate more on the computational complexity of the contrastive adapting method compared to other existing approaches  How does the proposed contrastive adapting method address biases or fairness concerns related to group shifts in data  Have you considered realworld applications or use cases where the improved group robustness can have significant implications 4) Limitations:  The paper primarily focuses on the technical aspects of group robustness in foundation models. It would be valuable to incorporate realworld scenarios or practical implications to enhance the relevance of the research.  The study evaluates the method across various benchmarks but a comparison with realworld datasets or scenarios could strengthen the generalizability of the findings. Overall Assessment: The paper makes a valuable contribution to the field of group robustness in foundation models by introducing an efficient and effective method contrastive adapting to improve worstgroup accuracy. The thorough evaluation and clear presentation of results enhance the credibility of the research. However including more discussion on the practical implications and societal impacts of the proposed method would further enrich the manuscript.", "n4wnZAdBavx": " Peer Review  1) Summary The paper introduces a novel framework MultiAgent communication via Selfsupervised Information Aggregation (MASIA) aiming to enhance coordination in Multiagent Reinforcement Learning (MARL) by aggregating meaningful messages from teammates efficiently. The proposed method aggregates received messages into compact representations using a permutation invariant message encoder and a novel message extraction mechanism. Experimental results demonstrate superior performance compared to strong baselines across various cooperative MARL tasks.  2) Strengths and Weaknesses Strengths:  The paper addresses an important issue in MARL by focusing on efficient message aggregation for coordination.  The proposed MASIA framework is wellmotivated and shows significant performance improvements over baselines across multiple tasks.  The method incorporates selfsupervised learning for efficient message representation which enhances policy learning. Weaknesses:  The paper lacks a detailed comparison with related works in terms of the proposed methods novelty and effectiveness.  The empirical evaluation could benefit from more analysis on the scalability of the proposed approach in largescale multiagent environments.  The paper could further discuss limitations related to realworld applicability and potential challenges in implementation.  3) Questions  How does the MASIA framework perform when faced with noisy or deceptive messages from teammates  Can the proposed method effectively handle scenarios with a large number of agents in complex environments  Are there any computational overheads associated with the aggregation and extraction mechanisms that could impact scalability  4) Limitations  The paper primarily focuses on the message aggregation aspect potentially overlooking other factors influencing coordination in MARL.  The evaluation lacks realworld application scenarios limiting the generalizability of the proposed method.  The generalization of the MASIA framework to various valuebased MARL methods needs further exploration to understand its adaptability. Overall the paper presents a novel approach to enhancing communication efficiency in MARL demonstrating promising results. Further exploration into realworld applications scalability and robustness to noisy inputs could strengthen the impact and relevance of the proposed MASIA framework.", "zb-xfApk4ZK": " Peer Review 1) Summary: The paper introduces an ExploreExploit Markov chain Monte Carlo strategy (Ex2MCMC) that combines local and global samplers to improve sampling efficiency. The authors prove Vuniform geometric ergodicity of Ex2MCMC under realistic conditions and provide convergence guarantees for its adaptive version FlEx2MCMC. Experimental results on classical sampling benchmarks and highdimensional distributions demonstrate the advantages of the combined approach over purely local or global MCMC methods. Theoretical analyses and numerical experiments support the effectiveness of Ex2MCMC and FlEx2MCMC in various sampling scenarios. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem of improving sampling efficiency by combining local and global samplers.  The theoretical analysis provides a solid foundation for understanding the properties and convergence guarantees of Ex2MCMC and FlEx2MCMC.  The numerical experiments demonstrate the effectiveness of the proposed strategy in various sampling scenarios including highdimensional distributions.  The adaptive aspect of FlEx2MCMC involving learning a global proposal with a normalizing flow adds novelty to the approach. Weaknesses:  The paper could benefit from clearer explanations of the theoretical concepts and assumptions especially for readers less familiar with MCMC algorithms.  More indepth comparisons with existing methods or strategies could further highlight the uniqueness and advantages of Ex2MCMC and FlEx2MCMC.  Providing a more detailed discussion on the limitations of the proposed approach and potential areas for improvement would enrich the paper. 3) Questions:  How does the proposed Ex2MCMC strategy compare to other stateoftheart MCMC algorithms in terms of mixing rate and convergence speed  Can the benefits of Ex2MCMC and FlEx2MCMC be generalized to a wider range of sampling scenarios beyond the ones presented in the paper  What are the computational implications of using an adaptive normalizing flow in FlEx2MCMC especially in terms of training time and resource requirements 4) Limitations:  The experimental evaluation focuses on specific sampling scenarios and the generalizability of the results to other types of distributions needs further exploration.  The analysis could include a more detailed discussion on the scalability and computational complexity of the proposed strategies especially in highdimensional settings.  Potential limitations in the assumptions made for the theoretical analysis should be explicitly addressed to provide a clearer understanding of the scope and applicability of the results. Overall the paper makes a significant contribution to the field of MCMC sampling by introducing a novel strategy that combines the strengths of local and global samplers. Further investigations and clarifications in theoretical assumptions experimental comparisons and limitations would enhance the impact and relevance of the research findings.", "zUbMHIxszNp": " Peer Review  1) Summary The paper introduces a new multilevel framework for graph generative models that jointly models nodelevel properties and global graphlevel statistics to enhance graph generation quality. The proposed micromacro objective and its application to GraphVAE show significant improvements in graph quality on benchmark datasets. The approach maintains the generation speed advantage of GraphVAE while enhancing realism and diversity of generated graphs.  2) Strengths  Novel Contribution: The introduction of the micromacro objective for training graph encoderdecoder models represents a novel contribution to the field of graph generative models.  Improved Graph Quality: The experimental results demonstrate significant improvements in graph quality scores compared to stateoftheart approaches enhancing both realism and diversity of generated graphs.  Efficient Training and Generation: The micromacro modeling approach improves graph quality without compromising the fast training and generation time of GraphVAE making it a practical and efficient solution for mediumsized graphs.  Weaknesses  Limited Application: The paper mainly focuses on the application of the proposed micromacro modeling to the GraphVAE architecture. Exploring the applicability of this approach to other graph generative models could strengthen the generalizability of the findings.  Evaluation Metrics: While the paper provides detailed evaluation metrics and comparisons additional insights on the limitations or possible biases of these metrics would enhance the robustness of the evaluation.  Scalability: The scalability of the approach to very large graphs is mentioned as a challenge. Further discussions on strategies to address scalability issues could provide more comprehensive insights for potential realworld applications.  3) Questions 1. Is it possible to provide insights on the interpretability of the learned graph embeddings with respect to the joint micromacro modeling approach 2. Could the paper elaborate on the potential tradeoffs or challenges associated with incorporating a wide range of graph statistics for training the model 3. How does the proposed micromacro objective perform in scenarios with highly sparse or dense graph structures beyond the mediumsized graphs considered in the experiments  4) Limitations  Model Generalizability: The applicability and performance of the micromacro modeling approach across diverse graph datasets and domains need further exploration for a comprehensive understanding of its generalizability.  Data Collection Ethics: While the paper mentions ethical considerations more detailed discussions on potential ethical implications of network modeling are warranted for a thorough understanding of the social impact of the research. Overall the paper presents a valuable contribution to the field of graph generative models through the development of the micromacro modeling framework with improvements in graph quality and efficiency. Further exploration into the scalability interpretability and generalizability of the approach would enhance the impact and relevance of the findings.", "tHK5ntjp-5K": " Peer Review  1) Summary: The paper introduces the Latent Point Diffusion Model (LION) for 3D shape generation which combines a hierarchical variational autoencoder (VAE) framework with deep diffusion models (DDMs) in latent space. LION achieves stateoftheart performance on ShapeNet benchmarks offers flexibility for applications such as multimodal generation and shape denoising and incorporates surface reconstruction for smooth mesh generation. The paper presents experimental results demonstrating LIONs highquality generation and versatility for various tasks.  2) Strengths and weaknesses: Strengths:  LION introduces a novel hierarchical VAE framework with latent DDMs for 3D shape generation achieving stateoftheart performance.  The flexibility of LION allows for various applications such as voxelconditioned synthesis shape interpolation and surface reconstruction.  The combination of point cloud latents with surface reconstruction methods provides highquality and diverse shape synthesis.  The paper is comprehensive detailing the methodology experiments and comparisons with existing models. Weaknesses:  The paper lacks a detailed discussion on the computational efficiency and scalability of LION especially when dealing with largescale datasets.  It would be beneficial to include a more indepth comparison with other recent generative models and highlight specific scenarios where LION outperforms them.  The paper could benefit from providing a clearer explanation of key technical terms and concepts for readers less familiar with 3D shape generation and deep learning methods.  3) Questions:  How does LIONs performance compare to other stateoftheart generative models in terms of computational efficiency and scalability  Can LION be easily extended to incorporate imagebased training for textured shape generation  How does LION handle overfitting and generalization on datasets with varying sizes and complexities  Are there specific scenarios or use cases where LION may face limitations or challenges in comparison to other existing models  4) Limitations:  The study focuses on point cloudbased generation and would benefit from exploring the incorporation of other data representations like voxels or meshes.  The evaluation is primarily based on quantitative metrics additional qualitative assessments through user studies or expert evaluations could further validate LIONs utility for digital artists.  The paper could elaborate on the potential ethical considerations associated with generative modeling and address any biases or risks introduced by using such technologies. Overall the paper presents a significant contribution to the field of 3D shape generation showcasing the effectiveness of LION in achieving highquality diverse shape synthesis. Further investigations into the scalability interpretability and realworld applications of LION would enhance its impact and practicality.", "zkQho-Jxky9": " Peer Review  1) Summary: The paper introduces a formal definition of harm within the context of machine learning and algorithmic decisionmaking emphasizing the importance of integrating harm considerations into algorithmic decisionmaking processes. The authors propose a novel framework for harmaverse decisionmaking using counterfactual reasoning and apply it to the optimization of drug doses. The paper highlights the limitations of standard machine learning algorithms in addressing harm and demonstrates how incorporating harm considerations can lead to less harmful decisions without compromising effectiveness.  2) Strengths and weaknesses: Strengths:  The paper addresses a critical aspect of ethical AI by formalizing the concept of harm and incorporating it into decisionmaking processes.  The proposed framework for harmaverse decisionmaking using counterfactual objective functions offers a novel approach to reducing harm in algorithmic decisions.  The application of the framework to identifying optimal drug doses demonstrates its practical implications and potential benefits in realworld scenarios. Weaknesses:  The paper may benefit from more concrete examples and realworld case studies to further illustrate the practical implications of the proposed framework.  The complexity of the concepts presented especially related to causal models and counterfactual reasoning may pose challenges for readers not familiar with these topics.  The limitations of the proposed framework and its applicability to complex realworld decisionmaking scenarios could be further explored and discussed.  3) Questions:  How does the proposed framework for harmaverse decisionmaking compare to existing ethical guidelines or regulatory frameworks for AI and algorithmic decisionmaking  Are there any specific challenges or considerations that need to be addressed when implementing the proposed framework in realworld applications particularly in highstakes domains such as healthcare or autonomous vehicles  How does the proposed framework account for uncertainties or unknown variables in causal models when making decisions under uncertainty  4) Limitations:  The complexity of the theoretical concepts presented in the paper such as structural causal models and counterfactual reasoning may limit the accessibility and applicability of the proposed framework to a broader audience.  The application of the framework to a specific case study on drug dose optimization may not fully capture the diversity of decisionmaking scenarios in other domains potentially limiting the generalizability of the findings. Overall the paper presents a valuable contribution to the field of ethical AI and algorithmic decisionmaking by introducing a formal definition of harm and a novel framework for harmaverse decisionmaking. Further research and exploration of the practical implications and challenges of implementing the proposed framework in various realworld contexts could enhance the impact of the findings.", "osPA8Bs4MJB": " Peer Review: 1) Summary: The paper introduces the Local Temporalaware Transformerbased Deepfake Detection (LTTD) framework emphasizing lowlevel temporal patterns in deepfake detection by modeling sequences of local patches with Transformers. The LTTD framework achieves stateoftheart generalizability and robustness in deepfake detection through localtoglobal learning. Extensive experiments on popular datasets validate the effectiveness of the proposed approach. 2) Strengths and weaknesses: Strengths:  The paper addresses a crucial and timely issue in deepfake detection focusing on lowlevel temporal patterns.  The proposed LTTD framework stands out by emphasizing local temporal information and achieving stateoftheart performance in generalizability and robustness.  The detailed explanation of the Local Sequence Transformer (LST) and CrossPatch Inconsistency (CPI) loss provides a thorough understanding of the methodology.  The ablation studies and visualization results add depth to the analysis enhancing the credibility of the proposed framework. Weaknesses:  The paper lacks a detailed discussion on the computational complexity and efficiency of the proposed LTTD framework which could impact its practical applicability.  The limitations section could be expanded to include potential future directions for mitigating adversarial enhancements in deepfake creation and ensuring the credibility of predictions in realworld scenarios.  It would be beneficial to provide comparisons with more recent and diverse deepfake datasets to comprehensively evaluate the generalization capabilities of the proposed approach. 3) Questions: 1. Could the authors elaborate on the comparative computational requirements of the LTTD framework compared to existing deepfake detection methods especially given the usage of Transformers 2. How does the LTTD framework handle unseen deepfake techniques or potential adversarial attacks that aim to degrade the detection performance 3. In the visualization results for forgery localization were there any specific insights gained for different forgery types that could inform further model improvements 4) Limitations: The paper shines in proposing an innovative deepfake detection framework however limitations include:  Lack of detailed discussion on computational efficiency.  Limited exploration of scenarios involving adversarial enhancements in deepfake creation.  The need for further validation on diverse and newer deepfake datasets to enhance the generalizability assessment. Overall the paper presents a valuable contribution to the field of deepfake detection with its emphasis on lowlevel temporal patterns and Transformerbased modeling. Addressing the identified weaknesses and limitations would enhance the impact and applicability of the proposed LTTD framework.", "wph_3smhuec": " Peer Review  1) Summary This paper introduces a novel concept of Bilevel Games with Selection (BGS) to address ambiguity in nonconvex bilevel optimization problems. The authors propose algorithms based on iterative differentiation and implicit differentiation to approximate the equilibria of these games. They also present a correction method to mitigate errors introduced by unrolled optimization. The theoretical foundations analytical tools and algorithms are discussed in detail showcasing the efficacy of the proposed approach.  2) Strengths and weaknesses Strengths:  Novel Concept: Introducing BGS to resolve ambiguity in nonconvex bilevel problems is a significant contribution.  Theoretical Foundation: The paper presents a solid theoretical foundation leveraging selection maps and implicit differentiation.  Algorithm Development: The proposed algorithms based on unrolled optimization and correction methods demonstrate improved performance in approximating equilibria.  Rigorous Analysis: Extensive analysis on differentiability convergence and equilibrium point characterization enhances the credibility of the proposed methods. Weaknesses:  Complexity: The paper delves into intricate mathematical concepts which may pose a challenge for readers less familiar with the subject matter.  Practical Implementation: While the algorithms show promise practical implementation and scalability in realworld applications need further validation.  Empirical Validation: The lack of empirical results or experimental validation limits the assessment of the algorithms effectiveness in practical scenarios.  3) Questions  How generalizable are the proposed algorithms to diverse machine learning problems beyond the specific cases discussed in the paper  Can the correction method proposed for unrolled optimization be further optimized for efficiency and computational speed  What considerations should be taken into account when applying these algorithms to largescale and highdimensional problems in machine learning  4) Limitations  Empirical Validation: The lack of empirical results limits the practical assessment of the proposed algorithms performance in realworld scenarios.  Generalizability: The generalizability of the algorithms and theoretical concepts to a broader range of machine learning applications needs further exploration.  Scalability: The scalability of the algorithms to highdimensional and largescale problems should be empirically validated. Overall the paper presents a significant advancement in addressing ambiguity in nonconvex bilevel optimization problems through the introduction of Bilevel Games with Selection. Further empirical validation and exploration of practical implications would enhance the impact of the proposed approaches in the machine learning domain.", "qx51yfvLnE": " Summary: The paper focuses on the development and analysis of greedy Online Contention Resolution Schemes (OCRSs) for various combinatorial constraints including the singleitem setting partition matroids and transversal matroids. It introduces a novel 1e selectable greedy OCRS for the singleitem setting improving upon previous stateoftheart schemes. Theoretical proofs and experimental results demonstrate the efficacy of the proposed schemes in achieving optimal selectability for elements under different constraints.  Strengths: 1. Novel Contribution: The paper introduces new 1e selectable greedy OCRSs for a range of fundamental combinatorial constraints providing optimal solutions. 2. Theoretical Rigor: The theoretical analysis is wellstructured backed by Lemmas and Proofs ensuring the validity of the proposed greedy OCRSs. 3. Experimental Validation: Experimental results showcase the superior performance of the proposed OCRSs over existing schemes highlighting their effectiveness across different constraint settings.  Weaknesses: 1. Limited Discussion on Practical Applications: Although the paper extensively covers the theoretical aspects of OCRSs there is a lack of discussion on realworld applications and practical implications of the proposed algorithms. 2. Complexity of Presentation: The intricate nature of the mathematical proofs and technical terms might hinder the comprehension of readers less familiar with combinatorial optimization and stochastic algorithms.  Questions: 1. How do the proposed greedy OCRSs compare in terms of computational efficiency and scalability when applied to largescale optimization problems 2. Are there any specific scenarios or industries where the newly developed OCRSs could offer significant practical benefits over existing approaches 3. How sensitive are the performance guarantees of the OCRSs to variations in the underlying distributional properties of the constraints  Limitations: 1. Theoretical Focus: The paper predominantly focuses on theoretical developments and proofs potentially limiting the extent of practical insights for practitioners. 2. Generalizability: The applicability of the proposed greedy OCRSs across a wider range of combinatorial constraints and optimization scenarios could be further explored. Overall the paper presents significant advancements in the field of online optimization algorithms offering novel greedy OCRS solutions for constrained combinatorial optimization problems. Further exploration of practical implications and broader applicability could enhance the impact of the research.", "zt4xNo0lF8W": "Peer Review 1) Summary The paper introduces Mask Matching Transformer (MMFormer) a novel framework for the fewshot segmentation task. MMFormer decouples the learning of matching and segmentation modules improving flexibility and reducing training complexity. The method achieves competitive results on COCO20i and Pascal5i benchmarks demonstrating its effectiveness and generalization ability. 2) Strengths  The paper presents a new perspective on fewshot segmentation decoupling matching and segmentation modules.  MMFormer is well explained and the method is systematically evaluated on two benchmarks.  The proposed components such as the Mask Matching Module and Feature Alignment Block are innovative and show performance improvement.  Extensive experiments and ablation studies provide a deep understanding of the models architecture training strategy and components. 3) Weaknesses  Some sections of the paper could be more concise to enhance readability.  The paper lacks a visualization of the training convergence analysis or the evolution of loss values during the training process which could provide additional insights.  It would be beneficial to compare the training and inference times of MMFormer with existing methods to showcase its efficiency. 4) Questions  Could the authors elaborate more on how MMFormer handles cases where a support image contains only parts of an object during segmentation  How does the proposed method handle class imbalance or novel classes with limited training samples  Is there any future work planned to further refine the matching mechanism in MMFormer 5) Limitations  The paper demonstrates performance mainly on the COCO20i dataset and the transferability to Pascal5i is highlighted. However further evaluation on a wider range of datasets could enhance the generalization analysis.  The study focuses on the numerical performance of MMFormer but a more indepth analysis of failure cases or qualitative comparisons with other methods could provide a richer perspective. Overall the paper introduces an innovative approach to fewshot segmentation with promising results. Further enhancements in clarity presentation of results and deeper analysis could strengthen the paper\u2019s impact and contribution to the field.", "qZUHvvtbzy": "Peer Review Summary: The paper introduces a new approach for addressing the quantum manybody problem using neural networks particularly restricted Boltzmann machines (RBM) to achieve stateoftheart results in representing ground states. By combining RBMs symmetry projection and Lanczos recursion the approach offers improved accuracy and efficiency in representing complex quantum states. The method is applied to the Heisenberg J1 \u2212 J2 model on a square lattice demonstrating superior performance compared to existing numerical techniques. Strengths: 1. Innovative Approach: The paper introduces a novel method that effectively combines RBMs symmetry projection and Lanczos recursion to improve accuracy in representing ground states. 2. StateoftheArt Results: The method achieves stateoftheart accuracy in representing ground states surpassing previous numerical techniques including convolutional neural networks. 3. Comprehensive Presentation: The paper provides a detailed explanation of the proposed methodology implementation details and results enhancing the reproducibility of the research. Weaknesses: 1. Computational Cost: The computational cost of the proposed method may be a limitation especially for larger lattice sizes which could hinder practical implementation. 2. Correlation Function Accuracy: While the paper achieves high accuracy in energy calculations correlation functions exhibit larger errors suggesting room for improvement in representation power. Questions: 1. How does the proposed method compare in terms of computational efficiency with existing techniques especially for largescale systems 2. Can the approach be extended to address quantum systems with more complex Hamiltonians beyond the Heisenberg J1 \u2212 J2 model Limitations: 1. Computational Cost: The computational cost could limit the scalability of the method for larger systems impacting its practical utility. 2. Correlation Function Errors: The method exhibits relatively larger errors in correlation functions highlighting a need for further improvement in representation power. In conclusion the paper introduces an innovative approach for addressing quantum manybody problems with neural networks. While achieving stateoftheart results the method faces challenges related to computational cost and accuracy in correlation functions. Addressing these limitations would enhance the applicability and robustness of the proposed methodology for broader quantum physics applications.", "yts7fLpWY9G": " Peer Review  Summary: The paper investigates the potential of adaptive readout functions in graph neural networks relaxing the constraint on permutation invariance in hypothesis spaces. The study focuses on various adaptive readouts evaluates their performance on different datasets and discusses implications for tasks like bioaffinity prediction. Extensive experiments are conducted showing improvements over standard readouts particularly in regression tasks. The approach is novel and offers insights into the benefits of adaptive readouts in graph structured data analysis.  Strengths: 1. Novelty: The study presents a novel approach by exploring adaptive readouts in graph neural networks and provides a comprehensive empirical evaluation across various datasets and tasks. 2. Extensive Experiments: The paper conducts a thorough analysis on more than 40 datasets employing different adaptive readout functions convolutional operators and neighborhood aggregation schemes. 3. Detailed Analysis: The discussion on domainspecific interpretations robustness to node permutations and implications for varied tasks like molecular properties prediction adds depth to the study. 4. Empirical Results: The findings show significant performance improvements with adaptive readouts especially in regression tasks providing valuable insights for future research in the field.  Weaknesses: 1. Complexity: The paper is quite technical and might be challenging for readers not wellversed in graph neural networks. Providing more contextual explanations and simplifying technical terms could make it more accessible. 2. Hyperparameters: The study mentions that hyperparameters of the adaptive readouts were not tuned in experiments suggesting potential performance improvements with optimization. Including more insights into this aspect could strengthen the study. 3. Transferability and Interpretability: The paper briefly touches on these aspects but could benefit from more detailed discussions on the transferability of adaptive readouts in realworld applications and the interpretability of the learned models.  Questions: 1. How would the findings of the study impact realworld applications like drug design or bioaffinity prediction Are there specific scenarios where adaptive readouts would be more beneficial than standard readouts 2. The paper focuses on canonical representations for molecular graphs. How generalizable are the results to noncanonical inputs in practical applications 3. Can the study shed light on the computational efficiency and memory usage of models with adaptive readouts compared to standard readouts Are there tradeoffs in terms of resource requirements  Limitations: 1. Generalization: The study evaluates adaptive readouts on a diverse set of datasets but generalizing the findings to other applications or tasks outside the scope of the study might require further validation. 2. Parameter Tuning: While acknowledging the importance of hyperparameters the study could benefit from exploring the impact of tuning adaptive readout hyperparameters on model performance for comprehensive insights. Overall the paper presents a compelling investigation into adaptive readout functions in graph neural networks showcasing promising results and implications for the field. With some clarifications and further exploration into specific aspects this research could significantly contribute to advancements in graph structured data analysis.", "o762mMj4XK": " Peer Review  1) Summary The paper introduces Balanced Neural Ratio Estimation (BNRE) as a variation of the NRE algorithm to produce more conservative posterior approximations in simulationbased inference. It enforces a balancing condition to increase the reliability of the estimations. The authors provide theoretical arguments supporting the conservativeness of BNRE and evaluate its performance on various benchmarks demonstrating improved reliability without introducing computational overhead.  2) Strengths and weaknesses Strengths:  The introduction of BNRE addresses concerns regarding overconfident posterior approximations in simulationbased inference.  Theoretical arguments supporting the reliability of BNRE are welldocumented.  The extensive validation on diverse benchmarks illustrates the effectiveness of BNRE in producing conservative posterior surrogates.  The code availability on GitHub and clear experimental setup contribute to reproducibility. Weaknesses:  The paper could benefit from more detailed explanations or practical examples to aid readers\u2019 understanding especially regarding the theoretical concepts and assumptions.  While the benefits of BNRE are demonstrated empirically a deeper analysis of its limitations in highdimensional parameter spaces could provide a more comprehensive view.  The reliability of BNRE in practice may vary and the authors acknowledge that the balancing condition is not exact possibly affecting the strict conservativeness.  3) Questions  How does the introduction of BNRE impact computational costs compared to traditional NRE algorithms  Could the application of BNRE in highdimensional parameter spaces potentially lead to increased computational complexity or challenges in practical implementation  Are there specific scenarios or domains where BNRE may not be as effective or where its conservativeness could present limitations  4) Limitations  Theoretical arguments and empirical validations focus primarily on the conservativeness of BNRE but limitations in terms of scalability to highdimensional spaces or potential challenges in practical implementation are acknowledged.  The reliance on a regularization penalty for enforcing the balancing condition might introduce uncertainties in the strict conservativeness of the classifier which should be taken into consideration. Overall the paper presents a valuable contribution in addressing the reliability of posterior approximations in simulationbased inference through BNRE. Further investigations into its practical implications and limitations could enhance the understanding and applicability of this algorithm in diverse scientific domains.", "zBlj0Cs6dw1": " Peer Review of \"RLCG: Reinforcement Learning for Column Generation in LargeScale Linear Programs\"  1) Summary The paper introduces RLCG the first Reinforcement Learning (RL) approach for Column Generation (CG) in largescale linear programs. By framing CG as a sequential decisionmaking problem RLCG leverages Deep RL and Graph Neural Networks to optimize column selection demonstrating significant improvements in convergence speed in Cutting Stock Problem and Vehicle Routing Problem with Time Windows.  2) Strengths and Weaknesses  Strengths:  Innovative Approach: Proposing RLCG as the first RL method for CG introduces a novel way of optimizing column selection in largescale LPs.  Performance Improvement: RLCG outperforms traditional greedy policies reducing the number of CG iterations significantly in CSP and VRPTW.  Experimental Rigor: By conducting extensive experiments on benchmark datasets the study provides compelling evidence of RLCGs effectiveness.  Curriculum Learning: The strategic use of curriculum for varying instance difficulties enhances the generalization ability of the RL agent.  Weaknesses:  Lack of Theoretical Analysis: While extensive experimental results are presented a deeper theoretical analysis of RLCG\u2019s convergence guarantees and behavior would strengthen the paper.  Limited Scope: The focus on adding one column per iteration limits the algorithms potential for further acceleration. Exploring methods for adding multiple columns per iteration could be beneficial.  3) Questions 1. Can the authors elaborate on the selection criteria for the hyperparameters used in the RLCG framework especially with respect to the reward function weight \\xe2\\x86\\xb5 and the exploration probability \\xe2\\x9c\\x8f 2. How does RLCG compare to stateoftheart RL methods in other optimization settings beyond CG Is there potential for adaptation to more diverse mathematical programming problems 3. How sensitive is RLCG\u2019s performance to variations in problem structures or dataset distributions beyond the BPPLIB and Solomon datasets used in the experiments  4) Limitations  Generalization to other problem classes: While the focus on CSP and VRPTW is welljustified exploring RLCGs applicability to a broader set of optimization problems could enhance the paper\u2019s impact.  Scalability with Multiple Columns: The challenge of scaling RLCG to handle selecting multiple columns per iteration should be addressed to improve convergence speed further. Overall the paper presents an innovative approach in enhancing the convergence speed of CG in largescale LPs through RL. Addressing the identified weaknesses and limitations would strengthen the papers contributions to the field of mathematical optimization and reinforcement learning. The wellconducted experiments and compelling results make this study a valuable addition to the intersection of optimization algorithms and machine learning. Rating: Accept with Minor Revisions.", "vt516zga8m": " Peer Review  Summary: The paper introduces a CacheAugmented Inbatch Importance Resampling (IR) method for training recommender retrievers to improve the accuracy of the softmax distribution estimation. The proposed method resamples items from the minibatch training pairs based on certain probabilities enabling the sampling of querydependent negatives. Experimental results demonstrate the superior performance of the proposed IR compared to competitive approaches on realworld datasets.  Strengths:  The paper addresses an important issue in recommender systems and proposes a novel solution to improve the training of retriever models.  The theoretical foundation of the proposed method is well explained and supported by Lemmas and Theorems.  The experimental evaluation is extensive covering multiple datasets and comparison with competitive baselines showcasing the effectiveness of the proposed method.  The paper provides indepth analysis and discussion of the results highlighting the key findings and implications of the proposed method.  Weaknesses:  The paper could benefit from more details on the implementation of the proposed method including hyperparameter tuning rationale and specific parameter settings.  While the theoretical explanations are comprehensive the paper could further clarify how the proposed method addresses the limitations of existing approaches.  More insights into the generalizability of the proposed method across different types of datasets or scenarios would enhance the impact of the research.  Questions: 1. Is there a specific reason for choosing the particular hyperparameter values and settings in the experimental evaluation How sensitive is the methods performance to these hyperparameters 2. Can the proposed CacheAugmented Inbatch Importance Resampling method be applied in scenarios with dynamic item or user features and how would it adapt to changing feature distributions over time 3. How does the proposed method scale in terms of computational complexity and training time when applied to larger datasets with millions of items and users  Limitations: The papers limitations include the lack of detailed information on the hyperparameter tuning rationale the need for further clarification on the generalizability of the proposed method and the scalability of the method to larger datasets. Overall the paper presents a novel approach to training recommender retrievers and provides promising results. Addressing the mentioned weaknesses and limitations would enhance the contributions and impact of the research.", "kyY4w4IgtM8": " Peer Review  1) Summary: The paper presents a novel metalearning method that utilizes feature descriptions written in natural language to improve performance on unseen tasks with limited labeled data. By leveraging a pretrained language model and neural networks the proposed method encodes feature descriptions into embeddings and adapts Gaussian process regression for taskspecific prediction models. The experimental results demonstrate the superiority of the proposed method over existing approaches on realworld datasets from the statistical offices of the EU and Japan.  2) Strengths and Weaknesses: Strengths:  Novel approach leveraging feature descriptions in natural language for metalearning.  Effective use of pretrained language models and neural networks for feature encoding and instance embedding.  Superior performance demonstrated on a wide variety of realworld datasets.  Clear explanation of the proposed method algorithm and results.  Through visualization the methods effectiveness in capturing relationships among tasks is highlighted. Weaknesses:  Limited discussion on the scalability and generalizability of the proposed method to diverse datasets beyond those from the statistical offices of the EU and Japan.  The performance comparison lacks statistical significance testing.  The explanation of the method is dense and may be challenging to follow for readers not wellversed in neural networks and metalearning.  Lack of exploration on the impact of hyperparameters and alternative neural network architectures on model performance.  3) Questions:  How does the proposed method handle noisy or incomplete feature descriptions and how robust is it in such scenarios  Could the authors elaborate on the interpretability of the models predictions and provide insights into the reasoning behind specific predictions  How sensitive is the proposed method to variations in the number of support instances per task and are there specific scenarios where it might perform suboptimally  Given the reliance on pretrained language models like BERT how does the method adapt to domainspecific datasets with distinct jargon or terminology  4) Limitations:  The lack of extensive evaluation on diverse datasets beyond those from specific statistical offices limits the generalizability of the proposed method.  The absence of statistical significance testing in comparing the proposed method with existing approaches raises questions about the reliability of the reported performance improvements.  The dense technical details and the complexity of the method may pose accessibility challenges for readers not deeply familiar with neural networks and metalearning.  The proposed method\u2019s scalability and efficiency for largescale datasets and realworld applications warrant further investigation to assess its practical utility. In conclusion while the paper introduces a promising approach to leveraging natural language feature descriptions for enhanced metalearning addressing the highlighted limitations and questions could strengthen the papers impact and applicability in broader research contexts. The thorough experimental evaluation and results presented are commendable and set a solid foundation for future research directions in this domain. Overall the paper showcases a novel and potentially impactful contribution to the field of metalearning highlighting the importance of incorporating feature descriptions in natural language for improving predictive performance on unseen tasks with limited labeled data.", "lgNGDjWRTo-": " Peer Review  1) Summary: The paper proposes a deep generative model PeriodicalGraph Disentangled Variational Autoencoder (PGDVAE) designed to address the challenges in generating periodic graphs. The model leverages a heuristic encoderdecoder architecture to automatically learn and disentangle local and global patterns ensuring periodicity in generated graphs. Experimental results show the effectiveness of PGDVAE in generating diverse and valid periodic graphs compared to existing models.  2) Strengths:  Novel Contribution: PGDVAE introduces a unique approach to address the challenges in generative modeling of periodic graphs demonstrating advancements in disentangling local and global structures.  Experimental Demonstrations: The paper provides comprehensive experimental evaluations to validate the effectiveness and efficiency of the proposed method.  Scalability Analysis: The complexity analysis and comparison with existing models showcase the scalability and performance improvements of PGDVAE.  Visualization of Disentanglement: Qualitative evaluations effectively showcase the disentanglement of local and global semantics in generated graphs.  3) Weaknesses:  Validation Methods: While the experimental results are comprehensive additional validation methods such as comparative analysis in realworld applications could strengthen the evaluation.  Contextual Background: The paper assumes a high level of familiarity with the subject matter potentially making it less accessible to readers not wellversed in generative models for graphs.  4) Questions:  How does the proposed model handle noise or variations in input data that may affect the periodicity of generated graphs  Have there been any investigations into the interpretability of the latent representations learned by PGDVAE for periodic graph structures  In what ways could PGDVAE be extended to handle dynamic or evolving periodic graph structures  5) Limitations:  Model Generalization: While the experiments demonstrate success on synthetic datasets the generalization of PGDVAE to diverse and complex realworld applications remains to be fully explored.  Assumptions on Data Structure: The fixed node ordering assumption for adjacency matrices may limit the models applicability to datasets with varying permutations. Overall the paper presents a significant advancement in generative modeling for periodic graphs with the proposed PGDVAE model. Further explorations into the models applications in different domains and its adaptability to dynamic graph structures could enhance the impact of the research findings.", "qVtbqSwOxy6": " Peer Review:  1) Summary: The paper introduces a novel approach Fast MultiView AnchorCorrespondence Clustering (FMVACC) to address the AnchorUnaligned Problem (AUP) in multiview anchor graph clustering. By capturing both feature and structure information FMVACC aligns anchor sets across views columnwise to improve clustering performance. The proposed method is validated through extensive experiments on benchmark datasets showing effectiveness and efficiency.  2) Strengths and weaknesses: Strengths:  Novel Approach: Introducing a flexible framework to tackle the AnchorUnaligned Problem in multiview anchor graph clustering is a significant contribution.  Theoretical Analysis: The theoretical analysis provided showing the connections with existing methods adds depth to the study.  Broad Experiments: Conducting experiments on both simulated and realworld datasets demonstrates the applicability and effectiveness of the proposed approach.  Efficiency: The analysis of space and time complexity shows that FMVACC is efficient and scalable compared to other methods. Weaknesses:  Lack of Comparative Analysis: While the proposed method is shown to outperform existing methods a more indepth comparative analysis with a wider range of clustering algorithms could further strengthen the study.  Algorithm Complexity: The complexity of the proposed algorithm may be a limiting factor in practical applications which could be further addressed or optimized.  Code Availability: Although the code is provided more details on its usage and documentation could enhance reproducibility and adoption by the research community.  3) Questions:  Could the proposed alignment module in FMVACC be extended to handle varying numbers of anchors across views  How does the FMVACC framework perform when faced with highly imbalanced datasets in terms of sample sizes and dimensions across views  What are the implications of the theoretical analysis connecting FMVACC with existing multiview strategies in practical clustering scenarios  4) Limitations:  The study primarily focuses on the clustering aspect and does not delve into the potential applications or downstream tasks that could benefit from the aligned anchor graph fusion.  The experiments are limited to a specific set of benchmark datasets and it would be valuable to include more diverse and challenging datasets in future evaluations to assess the generalizability of FMVACC.  Overall the paper presents an innovative solution to the AnchorUnaligned Problem in multiview anchor graph clustering with strong theoretical foundations and promising experimental results. Addressing the identified weaknesses and limitations would further strengthen the study and its impact in the field.", "syU-XvinTI1": " Summary The paper critically evaluates the use of deep learning models in neuroscience focusing on grid cells in the entorhinalhippocampal circuit as a case study. It questions the claims made by recent models that training on path integration tasks results in the emergence of grid cells. Through rigorous experimentation the paper demonstrates that the emergence of gridlike representations is more dependent on the networks specific implementation choices rather than the core task of path integration. It raises concerns about the accuracy and generalizability of deep learning models in predicting neural phenomena and emphasizes the need for caution transparency and biological knowledge when interpreting such models.  Strengths  The paper provides a thorough investigation into the claims made by deep learning models of grid cells highlighting the importance of implementation choices.  The research is wellstructured with clear sections detailing the background experimental approach and discussion of results.  The study offers valuable insights into the limitations and challenges of using deep learning models in neuroscience research.  The authors provide code availability making their work reproducible and transparent.  Weaknesses  The paper raises significant questions about the validity of deep learning models in replicating neural circuit behavior which may lead to concerns about the applicability of deep learning in understanding neuroscience.  The study seems to focus heavily on the limitations of existing models without providing a clear path forward for improving deep learning approaches in neuroscience.  The paper lacks information on the total amount of compute used which could be important for assessing the scale and resources required for such experiments.  Questions  Can the findings of this study be extended to other neural circuits or is it specific to the entorhinalhippocampal circuit  How do the results of this study impact the broader field of neuroscience research that relies on deep learning models  Are there specific aspects of deep learning models that can be improved to make them more suitable for understanding neural circuit mechanisms without biases  Limitations  The study primarily focuses on challenging the current deep learning models without providing alternative solutions or approaches to improve their accuracy in modeling neural circuits.  While the paper highlights the limitations of deep learning models in neuroscience it does not offer concrete recommendations for researchers working in this field.  The study does not explore the potential benefits of integrating deep learning models with other computational or experimental approaches to enhance our understanding of neural circuits.", "rg_yN3HpCp": " Peer Review: 1) Summary: The paper proposes a novel Graph Labelinvariant Augmentation (GLA) algorithm for graph contrastive learning which conducts augmentation in the representation space to address the limitations of current graph contrastive learning methods. The proposed method integrates pretrain and finetuning phases leveraging label information to guide contrastive augmentation. Extensive experiments on eight benchmark datasets demonstrate the effectiveness of GLA in semisupervised graph classification tasks outperforming existing methods. Indepth explorations further validate the benefits of labelinvariant augmentation negative pairs and augmentation strategies. 2) Strengths and Weaknesses: Strengths:  The paper addresses a significant problem in graph contrastive learning and proposes a novel algorithm with promising experimental results.  The integration of label information into the augmentation process is a noteworthy contribution and sets the proposed method apart from existing approaches.  Extensive experiments and indepth explorations provide a comprehensive evaluation of the proposed GLA algorithm.  The detailed methodology and clear explanation of the augmentation strategy enhance the reproducibility and understanding of the proposed method. Weaknesses:  The paper could benefit from more detailed discussions on the implications of the proposed algorithm for realworld applications.  The limitations and potential societal impacts could be elaborated further to provide a more holistic view of the research.  Providing comparisons with stateoftheart methods beyond the ones mentioned could strengthen the evaluation of the proposed algorithm. 3) Questions:  How does the proposed GLA algorithm scale with larger datasets or more complex graph structures  Are there any tradeoffs or considerations to be made when choosing the magnitude of perturbation (\u03b7) in the augmentation process  Can the labelinvariant augmentation strategy be extended to other types of data beyond graphstructured data 4) Limitations:  The performance of the proposed method heavily relies on the quality of the decision boundary indicated by the downstream classifier which may limit its applicability in scenarios with limited labeled data.  The study focuses primarily on algorithmic performance evaluation and may benefit from additional validations on realworld datasets or applications to demonstrate practical utility.  The paper does not explicitly discuss potential negative societal impacts which could be important to consider in future research endeavors. Overall the paper presents a novel approach to graph contrastive learning with significant potential and promising results. By addressing the limitations of existing methods through labelinvariant augmentation the proposed GLA algorithm opens up new avenues for improving model generalization and robustness in graphstructured data tasks. Further discussions on scalability realworld implications and societal impacts could enhance the completeness and impact of the research.", "tJBYkwVDv5": " Peer Review: 1. Summary: The paper presents a novel algorithm Spectrum Thresholding Variance Estimator (STVE) to estimate variance parameters in linear dynamical systems with nonconstant observation operators. The approach leverages the spectrum of the global system operator to derive variance estimators with finite sample complexity guarantees. The proposed algorithm is evaluated on synthetic and realworld data demonstrating promising results. 2. Strengths and Weaknesses: Strengths:  The paper addresses an important problem in dynamic linear regression with practical implications.  The introduction of STVE algorithm with finite sample complexity guarantees is a significant contribution.  The theoretical analysis and experimental validation provide a comprehensive evaluation of the proposed method.  The discussions on related work and the detailed exposition of methodology enhance the clarity of the approach. Weaknesses:  The paper could benefit from more detailed explanations of certain technical aspects especially in the discussion of operator spectra.  While the experiments demonstrate the effectiveness of STVE further comparison with existing methods or additional realworld datasets could strengthen the validation.  The presentation of results and illustrations could be improved for better understanding and visualization of key concepts. 3. Questions:  How does the STVE algorithm handle larger datasets or highdimensional feature spaces in terms of computational efficiency and scalability  Can the method be applied to nonlinear dynamical systems and if so what modifications would be necessary  Are there specific limitations or assumptions of STVE that could affect its generalizability to different application domains 4. Limitations:  The lack of extensive comparison with a broader range of existing methods limits the context for evaluating the novelty and effectiveness of STVE.  The assumptions made such as the independence of noise variables and nonzero observation vectors may restrict the applicability of the algorithm in certain scenarios.  The paper could further elaborate on potential challenges or limitations when applying STVE to complex realworld datasets with nonlinear relationships. Overall the paper presents a promising algorithm with theoretical guarantees for variance estimation in dynamic linear regression systems. Further improvements in explanation experiment diversification and consideration of practical constraints would enhance the impact and applicability of the proposed method.", "wVc4Qg5Bhah": " Peer Review:  Summary: The paper introduces EXTRANEWTON a universal and adaptive secondorder algorithm for minimizing secondorder smooth convex functions. The algorithm adapts to both noisy gradient and Hessian oracle feedback achieving a convergence rate of O(\u03c3g \\xe2\\x88\\x9a T  \u03c3HT 32  1T 3). The algorithm is compared against firstorder and other secondorder methods demonstrating promising performance across various optimization problems.  Strengths: 1. Novelty: The paper introduces a novel universal and adaptive secondorder algorithm that adapts to noisy oracle feedback filling a gap in the secondorder optimization literature. 2. Theoretical Foundation: The paper provides thorough theoretical underpinnings employing principles from online learning and acceleration schemes. The convergence guarantees are wellestablished. 3. Algorithm Construction: The stepbystep explanation of the algorithm design transitioning from implicit to explicit construction is clear and informative. The use of secondorder information in extrapolation steps and the adaptive stepsize mechanism are commendable. 4. Empirical Results: The experimental evaluation demonstrates the algorithms effectiveness in comparison to firstorder and other secondorder methods showcasing its superiority in certain scenarios.  Weaknesses: 1. Complexity: The paper delves deeply into theoretical aspects which may challenge readers not wellversed in optimization theory. Simplifying complex concepts could enhance accessibility. 2. Implementation: While the algorithm shows promise in theoretical analysis practical implementation aspects such as computational efficiency memory usage and robustness to noise could be further explored. 3. Experimental Setup: The experimental section could benefit from additional details on dataset characteristics hyperparameters and implementation nuances to aid reproducibility and result interpretation.  Questions: 1. How does the algorithm handle illconditioned problems especially with small regularization constants and what is its sensitivity to such scenarios 2. How does the runtime performance vary for EXTRANEWTON in largescale optimization tasks requiring extensive computational resources 3. Are there scenarios or problem types where EXTRANEWTON may not perform as effectively compared to other stateoftheart optimization algorithms  Limitations: 1. Theoretical Focus: The heavy emphasis on theoretical foundations may overshadow practical implications and realworld applicability in diverse optimization scenarios. 2. Generalizability: The scalability and adaptability of EXTRANEWTON to a broader range of applications including nonconvex optimization or constrained optimization could be further explored. 3. Implementation Challenges: While the algorithms theoretical aspects are robust addressing potential challenges in implementation such as numerical stability and memory requirements could enhance its practical utility. Overall the paper presents a significant contribution to the field of secondorder optimization algorithms but further investigations into implementation details empirical evaluations on varied datasets and addressing practical challenges could strengthen the impact and applicability of the proposed algorithm.", "oNWqs_JRcDD": "Peer Review: 1) Summary: The paper presents a novel framework for efficient scalable and generalizable optimizer search in AutoML systems. The proposed framework organizes the optimizer search space into a tree structure enabling pathfinding algorithms to discover optimizers effectively. The use of Monte Carlo Sampling rejection sampling and equivalentform detection techniques enhances sample efficiency. Extensive evaluations across various tasks demonstrate the frameworks superiority over humandesigned and prior optimizer search methods. 2) Strengths:  The paper introduces a novel framework for automated optimizer search addressing the limitations of existing methods in terms of efficiency generalization and scalability.  The treebased optimization space and the proposed traversal algorithm offer a systematic and effective approach to search for optimal optimizers.  The integration of rejection sampling and equivalentform detection techniques improves sample efficiency and optimizes the search process.  Extensive evaluations on diverse tasks showcase the frameworks effectiveness in discovering optimizers that outperform humandesigned counterparts and prior methods. 3) Weaknesses:  The paper lacks detailed discussions on the potential challenges or shortcomings of the proposed framework in realworld applications.  Further comparisons with stateoftheart optimizer search methods in terms of resource consumption convergence speed and stability could provide a more comprehensive assessment of the frameworks performance.  The limitations of the proposed techniques such as potential biases in optimizer selection or constraints on the search space should be addressed to ensure the robustness and applicability of the framework. 4) Questions:  How does the proposed framework handle complex optimization landscapes with multiple local optima or nonlinear relationships  Can the framework adapt to dynamic or changing optimization requirements in realtime scenarios  Are there provisions for incorporating domainspecific constraints or taskspecific considerations during the optimizer search process 5) Limitations:  The paper lacks a detailed analysis of the computational costs associated with implementing the proposed framework especially in comparison to existing methods.  The generalizability of the framework to a broader range of tasks models and datasets should be further explored to ascertain its applicability in diverse domains.  The effectiveness of the framework on largescale optimization tasks or scenarios involving highdimensional data remains unclear and requires additional investigation. Overall the paper presents an innovative approach to automated optimizer design with promising results on diverse tasks. Addressing the weaknesses and limitations highlighted above could further enhance the significance and impact of the proposed framework in the field of AutoML research.", "oprTuM8F3dt": " Peer Review 1. Summary: The paper presents a novel methodology CoCoINR for implicit neural 3D representation that utilizes prior information via codebook attention and coordinate attention modules. The approach aims to enhance feature representation for each coordinate leading to improved rendering of 3D scenes with fewer input views. Extensive experiments on various datasets demonstrate the effectiveness and robustness of CoCoINR compared to existing methods. 2. Strengths:  The introduction of codebook attention and coordinate attention modules is innovative and leads to improved feature representation enhancing rendering quality even with sparse input views.  Comprehensive experiments on multiple datasets showcase the robustness and efficacy of CoCoINR outperforming stateoftheart methods under varying numbers of input views.  The ablation studies provide insights into the impact of codebook priors codebook attention and codebook size on the performance of the proposed method.  The detailed methodology implementation details and comparative analysis with baselines contribute to the clarity and reproducibility of the research. 3. Weaknesses:  While the presented results are promising there could be a deeper discussion on the limitations and challenges faced by CoCoINR particularly in scenarios where specific features or geometries are more complex or occluded.  The paper could benefit from further analysis on the computational efficiency and scalability of CoCoINR especially regarding training times and memory requirements for largescale datasets or realtime applications.  The discussion on the interpretability and generalization capabilities of the method could be expanded to provide a more comprehensive evaluation of its practical utility in diverse realworld applications. 4. Questions:  How does the proposed CoCoINR framework handle occlusions and complex geometries in scenes and how does it adapt to scenes with highly variable textures or lighting conditions  Can the codebook attention and coordinate attention modules be optimized further to reduce computational complexity and improve efficiency especially for realtime applications or largescale scene reconstructions  What are the main challenges or limitations of the CoCoINR approach and how does it perform in scenarios where structural details or object boundaries are ambiguous or challenging to capture 5. Limitations:  The paper lacks a detailed discussion on the computational overhead and resource requirements of CoCoINR which may limit its practical adoption in resourceconstrained environments or realtime applications.  The evaluation of CoCoINR is primarily focused on specific benchmarks and datasets which may restrict the generalizability of the results to diverse scenes with unique characteristics or challenges.  The extent of interpretability and explainability of the proposed method is not thoroughly addressed which could be a limitation in applications requiring transparency or model understanding. In conclusion the paper introduces a novel approach CoCoINR for implicit neural 3D representation with codebook attention and coordinate attention mechanisms. While it demonstrates significant improvements in rendering quality and robustness under sparse input views further investigations into scalability interpretability and generalizability aspects would enhance the overall impact and applicability of the proposed method. Further research to address the limitations and questions raised could strengthen the contributions of CoCoINR in the field of implicit neural representations for 3D scene reconstruction.", "qSYVigfakqS": " Summary: The paper introduces SimFormer a dual similarity transfer framework for weakshot semantic segmentation aiming to segment novel classes with cheaper imagelevel labels and base classes with pixellevel masks. SimFormer leverages MaskFormer to disentangle the segmentation task into proposal classification and segmentation subtasks. It transfers proposalpixel and pixelpixel similarity from base classes to novel classes and introduces a complementary loss for more effective learning of novel classes. Extensive experiments on COCOStuff10K and ADE20K datasets showcase the methods effectiveness.  Strengths:  The paper addresses an important problem in weakshot semantic segmentation offering a novel solution with dual similarity transfer framework.  SimFormer leverages existing structures like MaskFormer improving its effectiveness with proposalpixel and pixelpixel similarity transfer.  Detailed methodology descriptions including proposal classification and segmentation tasks offer clear insight into the approach.  Comprehensive experiments on challenging datasets demonstrate the practicality and effectiveness of SimFormer.  Weaknesses:  Some parts of the paper could be complemented with more figures or visualizations to enhance reader understanding especially in complex model parts like proposalpixel similarity transfer.  While the experiments demonstrate the methods effectiveness more comparisons with stateoftheart methods or ablation studies could further strengthen the paper.  The failure case provided doesnt extensively cover various failure scenarios potentially limiting the understanding of model limitations.  Questions: 1. Could the paper provide a deeper insight into the tradeoffs between the proposed method and existing stateoftheart techniques in terms of performance and computation cost 2. How does the proposed method perform in scenarios where novel classes are particularly challenging or have overlapping features with base classes 3. Have the authors considered scalability issues when applying SimFormer to larger datasets with numerous novel classes  Limitations:  While the proposed method shows promise in weakshot semantic segmentation it may still struggle with finegrained or challenging novel classes as evident in the failure case presented.  The models effectiveness heavily relies on strong supervision for base classes and may encounter limitations when novel classes lack clear or distinct features.  Overall the paper presents a compelling approach to weakshot semantic segmentation showcasing innovative techniques like dual similarity transfer within the MaskFormer framework. Further exploration of failure cases scalability concerns and comparative analyses could enhance the papers impact in the field of computer vision research.", "pNEisJqGuei": " Peer Review of the Paper: \"Integrating Value Decomposition into Reinforcement Learning Agents for Improved AgentDesign Process\"  Summary: The paper introduces the concept of integrating value decomposition into actorcritic algorithms for reinforcement learning (RL) agents. It proposes SACD a variant of the soft actorcritic (SAC) algorithm adapted for value decomposition enabling a more detailed understanding of an agents learning process. By decomposing the reward function into distinct components and learning value estimates for each the authors demonstrate how this approach can provide insights into agent behavior assist in diagnosing learning challenges and enable targeted interventions to address issues in agent design. The paper showcases the utility of value decomposition through demonstrations on benchmark environments introducing novel tools like the reward influence metric for measuring each reward components impact on decisionmaking. The results show that SACD when properly optimized matches or exceeds the performance of SAC in various environments. Additionally the paper provides examples of how value decomposition can be effectively used for agent design.  Strengths: 1. Innovative Approach: The paper presents a novel and useful methodology by integrating value decomposition into RL agents offering insights into agent behavior and decisionmaking processes. 2. Clear Demonstration: The paper effectively demonstrates the benefits of value decomposition through illustrations and practical examples making complex concepts more accessible. 3. Experimental Results: The experimental results comparing SACD with SAC on benchmark environments provide tangible evidence of the effectiveness of the proposed approach.  Weaknesses: 1. Complexity: The paper delves into intricate technical details which may pose challenges for readers not deeply familiar with RL concepts potentially limiting the accessibility of the research. 2. Evaluation Metrics: The performance evaluation focuses on specific benchmark environments raising questions about the broader applicability of the proposed approach across different types of tasks or scenarios. 3. Literature Review: While the related work section discusses prior research more direct comparisons and contrasting aspects with existing methods could enhance the papers contribution to the field.  Questions: 1. How does the proposed value decomposition approach compare to existing explainable reinforcement learning (XRL) methods like DrQ and DrSARSA in terms of interpretability and practical utility 2. The paper mentions the cost of a more challenging prediction problem due to learning multiple value functions. How does this added complexity impact computational resources or training time compared to standard RL methods 3. Can the value decomposition technique be easily extended to handle nonlinear reward functions or more complex agentenvironment interactions beyond the linear decomposition discussed in the paper  Limitations: 1. Scope: The paper primarily focuses on linear reward decomposition and may benefit from exploring applications in nonlinear reward settings to expand the generalizability of the approach. 2. Model Dependency: The method relies on Qfunction models which may limit applicability in scenarios where policy optimization relies on empirical returns rather than value predictions. 3. Societal Impact: While the technology has potential benefits ensuring that agent predictions do not reinforce biases or lead to incorrect conclusions remains a critical consideration for realworld applications. In conclusion the paper presents valuable contributions to the field of reinforcement learning by introducing an innovative approach to improve agent design through value decomposition. Addressing the weaknesses and limitations highlighted could further enhance the impact and applicability of the proposed methodology. Overall the research provides insights that could benefit both the academic community and practitioners in the field of RL.", "tbdk6XLYmZj": "Peer Review 1) Summary: The paper introduces a novel approach called Learning the Best Combination (LBC) for achieving N:M finegrained sparsity in neural networks. By presenting N:M sparsity as a combinatorial problem and using a divideandconquer strategy the authors optimize sparsity efficiently during regular training. Comprehensive experiments showcase the superiority of LBC over existing N:M sparsity methods across various network models demonstrating improved accuracy and reduced training burden. 2) Strengths:  The paper addresses an important problem of achieving efficient N:M finegrained sparsity in neural networks.  The divideandconquer approach to solving the combinatorial problem and the introduction of learnable scores demonstrate a systematic and innovative solution.  Extensive experiments on different tasks and models validate the effectiveness of LBC in terms of accuracy and training efficiency.  The detailed methodology algorithm and experimental results provide a clear and comprehensive understanding of the proposed approach. Weaknesses:  The paper lacks a detailed comparison with a wider range of stateoftheart methods beyond N:M sparsity which could provide a broader context for evaluating the proposed approach.  The explanation of the experimental setup and results could be enhanced by including more detailed information on hyperparameters training schedules and convergence behavior.  The paper would benefit from a more indepth discussion on the scalability of LBC to larger and more complex neural networks. 3) Questions:  How does the proposed LBC approach generalize to tasks beyond computer vision such as natural language processing (NLP)  What are the implications of the LBC methodology on hardware implementation and deployment in resourceconstrained scenarios  Are there any limitations or challenges in adopting the LBC approach in scenarios with extremely large neural network models 4) Limitations:  One limitation of the research is the focus on N:M sparsity in the context of computer vision tasks without exploring its application in other domains.  The study mainly concentrates on N:M sparsity for weights and the extension of this approach to gradient and activation sparsity remains unexplored.  Although LBC offers improvements over approaches relying on pretrained models or densegradient calculations earlystage computation demands could be optimized further for enhanced efficiency. The overall contribution of this work in proposing LBC for N:M finegrained sparsity in neural networks is noteworthy. Addressing the weaknesses and limitations through additional comparative analysis broader applications and scalability considerations could strengthen the significance and impact of the proposed approach.", "zK6PjBczve": " Peer Review:  1) Summary: The paper proposes a novel method NeurHap for haplotype phasing of polyploid species and viral quasispecies. The approach combines graph representation learning with combinatorial optimization and demonstrates superior performance on both synthetic and real datasets compared to existing methods. NeurHap addresses the challenge of inferring haplotypes from highthroughput sequencing data by formulating the problem as a graph coloring problem. The authors present extensive experiments and analysis to support the effectiveness of their approach in haplotype reconstruction.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem in genetic research and proposes a novel method to improve haplotype phasing in polyploid species and viral quasispecies.  NeurHap combines graph representation learning with combinatorial optimization providing a comprehensive solution that outperforms existing methods.  The extensive experiments on synthetic and real datasets demonstrate the effectiveness of the proposed approach. Weaknesses:  The paper lacks a detailed discussion on the limitations and challenges of the proposed method. Addressing potential drawbacks and areas for improvement could enhance the robustness of the study.  The paper could benefit from a more indepth comparison with existing methods in terms of computational complexity scalability and applicability to different datasets.  While the experimental results show the superiority of NeurHap a more thorough analysis of the results including statistical significance and generalizability would strengthen the conclusions.  3) Questions:  How does NeurHap handle noise and errors in sequencing data especially in the context of highthroughput sequencing technologies  Can the model automatically adapt to varying population sizes and imbalanced abundances in viral quasispecies or does it require manual tuning  Have you considered the interpretability of the color assignments in the context of biological relevance How do the assigned colors correlate with known genetic patterns or variations  4) Limitations:  The paper lacks detailed discussion on the interpretability and biological relevance of the haplotype reconstructions obtained through NeurHap. Providing insights into how the inferred haplotypes relate to known genetic patterns would enhance the utility of the method.  The study primarily focuses on the effectiveness of NeurHap compared to existing methods but does not delve into the practical implications of the findings in realworld genetic research scenarios.  The method appears to require the initial number of haplotypes (k) to be known a priori. Discussing approaches to automatically determining this parameter would be beneficial for wider applicability. Overall the paper presents a valuable contribution to genetic research introducing an innovative approach to haplotype phasing in polyploid species and viral quasispecies. Further refinement and exploration of the methods limitations and practical implications would enhance the impact of the study.", "mxzIrQIOGIK": "Summary: This paper presents a systematic study of multiobjective online learning introducing the framework of MultiObjective Online Convex Optimization. It proposes the Doubly Regularized Online Mirror Multiple Descent algorithm addressing challenges in composite gradient composition. Theoretical bounds for static and dynamic regrets are derived and validated through extensive simulations and realworld datasets. The proposed algorithm demonstrates effectiveness in tracking the Pareto front and achieving adaptively regularized performance improvements over fixed regularization methods. Strengths: 1. The paper provides a comprehensive study of multiobjective online learning introducing a novel framework and algorithm. 2. The proposed algorithm DROMMD addresses challenges with composite gradient composition and achieves sublinear regrets. 3. The theoretical analysis and experimental validation effectively demonstrate the effectiveness and superiority of the proposed algorithm. 4. The paper is wellstructured with clear explanations and detailed discussions throughout. Weaknesses: 1. The paper focuses mainly on the convex setting limiting the generalizability of the results to nonconvex scenarios. 2. The experimentation details could be further elaborated including the handling of hyperparameters and statistical significance. 3. While the theoretical analysis is thorough some of the proofs are relegated to the appendices potentially impacting accessibility for readers. 4. The limitations highlighted in the conclusion could be more explicitly discussed throughout the paper for increased clarity. Questions: 1. How does the proposed algorithm DROMMD compare to existing stateoftheart algorithms in terms of performance on challenging realworld datasets 2. Can the proposed algorithm be extended to handle nonconvex optimization problems and if so what additional challenges may arise in such scenarios 3. Are there any plans to further investigate the scalability and adaptability of the DROMMD algorithm in more complex and dynamic environments 4. How robust is the DROMMD algorithm to variations in hyperparameters and different problem settings Limitations: 1. The papers focus on the convex setting limits the general applicability of the proposed algorithm. 2. The depth of experimentation and validation while extensive may benefit from further statistical analysis and exploration of additional benchmarks. 3. The dependence on hyperparameter tuning and the choice of regularization parameters may impact the algorithms performance in practice. 4. The limited exploration of potential societal impacts and ethical considerations may warrant further examination in future work. Overall the paper offers significant contributions to the field of multiobjective online learning providing a strong foundation for future research and algorithm development in this area. The comprehensive theoretical analysis and empirical validation ensure the credibility and relevance of the proposed algorithm paving the way for advancements in optimizing performance in multitask settings.", "s_mEE4xOU-m": " Peer Review  Summary The paper presents a study on the network load balancing problem in data centers using a multiagent reinforcement learning framework. It explores the challenges posed by the dynamic environments and limited observability in distributed networking systems. The proposed distributed MARL algorithm is evaluated through simulation and realworld experiments demonstrating its effectiveness in improving load balancing performance.  Strengths  The paper addresses an important problem in network load balancing in data centers using a novel approach of multiagent reinforcement learning.  The formulation of the multiagent load balancing problem as a Markov potential game is a unique and valuable contribution to the field.  The proposed distributed MARL algorithm shows promising results in both simulations and realworld experiments outperforming existing load balancing algorithms.  The thorough experimental evaluations including eventdriven simulations and realworld system tests add credibility to the proposed approach.  The clear and detailed presentation of the method including algorithm descriptions and architecture diagrams enhances the reproducibility of the research.  Weaknesses  The paper lacks a detailed discussion of potential implementation challenges or limitations that might be encountered when deploying the proposed distributed MARL algorithm in realworld data center environments.  While the theoretical formulation and algorithm implementation are wellexplained more insights into the scalability of the approach to larger and more complex data center setups could enhance the practical applicability of the proposed method.  The comparison with existing stateoftheart load balancing algorithms could be further extended to include a broader range of methods to provide a more comprehensive evaluation.  Questions 1. How does the proposed distributed MARL algorithm handle potential issues like training stability convergence speed or scalability when applied to larger data center networks with more LB agents and servers 2. What specific challenges or limitations were encountered during the implementation and experimentation phase of the proposed approach and how were they addressed 3. Can the proposed algorithm adapt to dynamic changes in workload patterns effectively in a realworld data center setting and if so what mechanisms ensure this adaptability  Limitations  The paper would benefit from a more detailed discussion on the practical implications and limitations of the proposed distributed MARL approach. Realworld deployment challenges such as network traffic variability system latency or scalability to larger setups should be thoroughly considered.  The experimental evaluations while comprehensive could be further extended to explore additional performance metrics or scenarios to provide a more holistic assessment of the proposed methods capabilities. Overall the paper presents a significant contribution to the field of network load balancing using multiagent reinforcement learning. Addressing the identified weaknesses and limitations through further analysis and experimentation would strengthen the research findings and enhance the applicability of the proposed approach in realworld data center environments.", "s71h4wo9bFI": "Peer Review Summary: The paper presents a comprehensive study on designing optimal Bayesian data acquisition mechanisms for a platform estimating the mean of a distribution while considering privacyconscious users with heterogeneous sensitivities towards local and central privacy losses. The study establishes minimax lower bounds for estimation errors and proposes linear estimators as nearoptimal solutions. It also formulates a Bayesian setting for designing optimal data acquisition mechanisms and proposes a Polynomial Time Approximation Scheme for finding the optimal mechanism. The paper makes significant contributions by addressing the challenges of privacy concerns in data acquisition mechanisms efficiently. Strengths: 1. Comprehensive Framework: The paper provides a cohesive framework for understanding data acquisition mechanisms in the presence of heterogeneous privacy concerns. 2. Theoretical Contributions: The minimax lower bounds and optimality results for linear estimators contribute to the theoretical foundations of data acquisition mechanisms. 3. Bayesian Approach: The studys Bayesian setting for mechanism design adds depth to understanding the distribution of users privacy sensitivities. 4. Algorithm Development: The proposed Polynomial Time Approximation Scheme is a practical contribution for optimizing data acquisition mechanisms efficiently. Weaknesses: 1. Complexity: The nonconvex optimization problem may limit the scalability of the proposed mechanism in realworld applications if not addressed effectively. 2. Limited Validation: The presented case study with two users might be insufficient to represent the complexity of realworld scenarios with multiple participants. 3. Model Assumptions: Assumptions like users privacy sensitivity being drawn from a specific distribution may limit the generalizability of the proposed mechanisms. Questions: 1. How does the proposed mechanism handle scenarios where users may provide inaccurate reports of their privacy sensitivities 2. How sensitive is the mechanism to variations in the underlying privacy sensitivity distribution assumed for users 3. Could you elaborate on the practical implications of adopting Gaussian mechanisms for delivering privacy guarantees Limitations: 1. Generalizability: The focus on linear estimators and Gaussian mechanisms limits the applicability to a broader range of estimation methods and privacy mechanisms. 2. RealWorld Application: The assumptions made regarding user privacy sensitivities and the Gaussian mechanisms simplification may not fully capture the complexities of realworld data acquisition scenarios. 3. Scalability: The complexity of the proposed optimization scheme may pose challenges for implementation in largescale data acquisition settings. In conclusion the paper offers valuable insights into designing data acquisition mechanisms in the presence of heterogeneous privacy concerns. Addressing the limitations and exploring avenues to enhance the scalability and adaptability of the proposed mechanisms can further strengthen the impact of the study in realworld applications.", "ytnwPTrpl38": " Peer Review of the Scientific Paper:  1) Summary: The paper \"GeneRAlizing by DiscovERing (GRADER): A Causal Reasoning Method for Reinforcement Learning Agents\" proposes a novel method that augments GoalConditioned RL with Causal Graph to enhance generalization. By formulating the GCRL problem into variational likelihood maximization with the causal graph as latent variables the authors present a framework that alternates between causal discovery and policy learning. The method is evaluated on nine reasoningintensive tasks in three environments showcasing improved data efficiency and interpretability compared to existing baselines. The paper demonstrates the importance of causal reasoning in reinforcement learning and provides theoretical guarantees for the proposed framework.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important gap in reinforcement learning by incorporating causal reasoning leading to improved generalization.  The proposed method is welltheorized and backed by sound reasoning providing theoretical performance guarantees.  Comprehensive experiments on designed tasks demonstrate the superiority of the method over existing baselines showcasing data efficiency and interpretability.  The iterative optimization process and alternation between causal discovery and policy learning are innovative and effective. Weaknesses:  The method heavily relies on the assumption of factorized state and action spaces which may limit its applicability in realworld scenarios with complex environments.  The paper lacks a realworld application or physical implementation to validate the proposed method beyond the designed environments.  The scalability of the explicit estimation of causal structures to a large number of nodes is not addressed posing a limitation in practical applications.  3) Questions:  How robust is the proposed method to noise or uncertainties in the causal graph estimates especially in realworld scenarios where the true causal structure may be unknown  Does the paper explore the computational complexity of the proposed method and its implications for scalability to larger and more complex environments  Are there any plans to extend the evaluation of the method to realworld applications or more diverse environments to test its applicability beyond the designed tasks  4) Limitations:  The paper lacks a discussion on the potential negative social implications of the proposed method which is essential as AI systems incorporating causal reasoning may introduce biases or ethical concerns.  The paper could benefit from a more detailed analysis on the limitations of the proposed method such as handling nonfactorized state spaces or addressing uncertainties in the causal graph estimation process. Overall the paper presents a valuable contribution to the field of reinforcement learning by integrating causal reasoning into the learning process. The method shows promise in improving generalization and interpretability but further research is needed to address practical limitations and test the applicability of the method in realworld scenarios.", "nyBJcnhjAoy": "Peer Review 1) Summary: The paper introduces ProtoX a prototypebased posthoc policy explainer designed for explaining blackbox reinforcement learning agents. ProtoX prototypes an agents behavior into scenarios represented by prototypes and explains the agents actions based on these basic patterns. The methodology includes selfsupervised learning for visual similarity scenario similarity integration and behavior cloning for training. Various experiments are conducted on AtariNES games demonstrating high fidelity and interpretability of ProtoX as compared to existing methods. 2) Strengths and weaknesses: Strengths:  Clearly articulated problem statement and proposed solution.  Innovative approach using prototypes for explaining reinforcement learning agents.  Detailed methodology covering visual similarity scenario similarity and behavior cloning.  Empirical evaluation on multiple game environments demonstrating high fidelity and interpretability.  Comparative analysis with existing blackbox and interpretable baselines.  Code availability for reproducibility. Weaknesses:  Lack of discussion on the computational efficiency of ProtoX especially in larger and more complex environments.  Limited discussion on potential realworld applications and impact of ProtoX beyond gaming scenarios.  The complexity of the methodology might hinder widespread adoption by practitioners without a strong technical background.  Interpretation of prototypes requiring a basic understanding of the domain may limit the generalizability of ProtoX. 3) Questions:  How does ProtoX perform in terms of computational resources and efficiency particularly when scaled to larger and more complex environments beyond the tested games  Are there plans to explore the applicability of ProtoX in realworld scenarios outside of game environments and if so what challenges are anticipated in this transition  Given the requirement for a basic understanding of the domain to interpret prototypes how can ProtoX be made more userfriendly for nonexperts in the field  In the prototype merging step are there specific criteria or algorithms used to identify and eliminate redundancy among prototypes 4) Limitations:  The interpretability of prototypes in ProtoX may require a certain level of domain expertise limiting its usability for nonexperts.  The fidelity and interpretability results are empirical and may not generalize to all possible reinforcement learning scenarios.  The paper focuses primarily on blackbox reinforcement learning agents in game environments limiting the scope of the study.  Pruning techniques for addressing similar and redundant prototypes could enhance the quality of explanations provided by ProtoX.", "nP6e73uxd1": " Peer Review  1) Summary The paper presents a novel algorithm for sampling from logconcave distributions on convex bodies with improved runtime bounds in infinitydistance dependent on the error parameter. The algorithm converts continuous samples with totalvariation bounds to infinity bounds demonstrating applications in differentially private optimization.  2) Strengths and Weaknesses Strengths:  The paper introduces an algorithm with improved runtime bounds logarithmically dependent on the error parameter bridging a gap in sampling from logconcave distributions.  The algorithm showcases a novel approach to convert continuous samples to infinitydistance bounds demonstrating practical applications in differentially private optimization.  Theoretical underpinnings and mathematical proofs are robust enhancing the credibility of the proposed algorithm. Weaknesses:  The paper could benefit from more clarity in the explanation of certain technical terms and methodologies to improve accessibility to a wider audience.  The limitations of the algorithm in practical implementations and its scalability to higher dimensions should be addressed.  3) Questions  How does the proposed algorithm compare with existing methods in terms of practical implementation and computational efficiency  Can the algorithm be extended to handle more complex distribution types beyond logconcave distributions  How robust is the algorithm to noise and inaccuracies in the input samples especially in realworld scenarios  4) Limitations  The study primarily focuses on logconcave distributions on convex bodies limiting the generalizability of the algorithm to other distribution types.  The assumptions of Lipschitz continuity impose constraints on the functions potentially affecting the applicability of the algorithm in diverse scenarios.  The scalability of the algorithm to higherdimensional spaces and large datasets remains to be tested for practical feasibility.  5) Future Directions  The authors could explore the extension of the algorithm to handle a broader range of distribution types to enhance its versatility.  Investigating the robustness of the algorithm to noisy and incomplete data could lead to practical applications in data analytics and machine learning.  Conducting empirical evaluations and benchmarking against existing sampling algorithms would provide insights into the algorithms performance in realworld scenarios.", "wt7cd9m2cz2": " Summary: The paper introduces a class of shallow neural networks that can learn singleindex models through gradient flow optimization. The models are relevant for highdimensional data with lowdimensional structure. The study shows that the optimization landscape is benign and provides generalization guarantees matching nearoptimal sample complexity. The research addresses a gap in understanding the benefits of shallow neural networks in highdimensional learning particularly for singleindex models.  Strengths: 1. The paper addresses important challenges in highdimensional learning with computational and statistical guarantees. 2. The theoretical analysis of gradient flow optimization for shallow neural networks is thorough and provides insight into learning singleindex models efficiently. 3. The research bridges the gap in understanding the advantages of nonlinear learning models like neural networks over traditional counterparts. 4. Results show promising generalization guarantees and nearoptimal sample complexity for learning singleindex models.  Weaknesses: 1. The setting with frozen biases at initialization may not fully capture the complexities of realworld neural network training. 2. The results heavily rely on specific assumptions and regularization parameters which may limit the applicability to diverse scenarios. 3. Application to more complex network architectures beyond the proposed nonstandard setup could provide further insights into learning capabilities.  Questions: 1. How sensitive are the results to changes in the assumption regarding the frozen biases and shared inner weights structure of the shallow neural networks 2. What are the practical implications and potential limitations of applying the proposed gradient flow optimization approach to realworld datasets with varying complexities 3. Can the findings be extended to different types of neural network architectures or more complex models to better understand their learning capabilities in highdimensional settings 4. How do the results compare to existing methods for learning singleindex models and what are the unique contributions of using shallow neural networks in this context  Limitations: 1. The frozen biases at random initialization and the shared inner weights structure may not fully represent the dynamics of training neural networks in practice. 2. The results heavily rely on stringent assumptions regularization parameters and specific architectures which may limit the generalizability of the findings to realworld scenarios. 3. The focus on singleindex models may restrict the applicability of the approach to more diverse model structures commonly encountered in machine learning tasks.", "xpdaDM_B4D": " Peer Review:  1) Summary: The paper introduces a novel approach Few shot Learning with hard Mixup (FeLMi) for tackling the data scarcity issue in fewshot learning tasks. The proposed method utilizes mixupbased data augmentation and introduces hard mixup using a marginbased uncertainty measure. The approach is evaluated on standard fewshot benchmarks and demonstrates improvements in both 1shot and 5shot settings. Furthermore crossdomain fewshot settings are also explored showing significant enhancements. Overall the paper presents a comprehensive methodology and promising results for addressing the challenges of fewshot learning.  2) Strengths and Weaknesses: Strengths:  The paper presents a novel approach FeLMi addressing the data scarcity issue in fewshot learning.  The proposed use of hard mixup with uncertaintyguided sample selection is a unique and effective strategy.  Extensive experiments on standard benchmarks and crossdomain settings demonstrate the effectiveness of the approach.  Ablation studies and analysis of mixup strategies provide insights into the performance improvements. Weaknesses:  A potential weakness could be the reliance on base examples during metatesting which may limit the scalability to very large datasets.  While the proposed approach shows improvements a more indepth comparison with existing stateoftheart methods could provide a stronger validation of the approach.  3) Questions:  How does the proposed FeLMi approach compare with other mixup strategies in terms of computational efficiency and performance  Could the reliance on base examples during metatesting be mitigated in future work to enhance the scalability of the approach  Are there specific scenarios or types of fewshot learning tasks where the FeLMi approach may particularly excel or face challenges  4) Limitations:  The paper acknowledges the requirement of access to base examples during metatesting as a limitation. Exploring methods to alleviate this requirement could enhance the practical applicability of the proposed approach.  5) Overall Evaluation: The paper presents a strong and wellthoughtout methodology for addressing the data scarcity issue in fewshot learning. The unique incorporation of hard mixup with uncertaintyguided sample selection sets the approach apart from existing methods. The extensive experimental evaluation on standard benchmarks and crossdomain settings provides convincing evidence of the effectiveness of the approach. Addressing the limitations identified and providing further insights into specific use cases where the FeLMi approach excels would enrich the papers contribution to the field of fewshot learning research.", "tIZtD2kZ6zx": " Summary The paper introduces \"Drawing out of Distribution (DooD)\" a neurosymbolic generative model that can learn generalpurpose representations through strokebased drawing. DooD operates directly on images requires no supervision and efficiently performs inference with a symbolic stroke model. Evaluation across datasets and tasks shows that DooD outperforms baselines demonstrating its ability to capture generalpurpose representations effectively. The model showcases strong performance in generalization exemplar generation and oneshot classification tasks.  Strengths and Weaknesses Strengths:  DooD introduces a neurosymbolic generative model for strokebased drawing addressing the need for efficient reliable interpretable and unsupervised computational approaches for generalization.  The model outperforms baselines in generalization across datasets and tasks showcasing its potential for capturing meaningful representations.  DooD achieves realistic unconditional generation and characterconditioned generation highlighting its versatility and robust performance in different tasks. Weaknesses:  The papers complexity might pose a challenge for readers unfamiliar with neurosymbolic models or strokebased drawing techniques.  The evaluation predominantly focuses on model comparisons and performance metrics potentially requiring additional analysis on interpretability and realworld applicability.  While DooD shows promising results further validation on larger and more diverse datasets could provide a comprehensive understanding of its capabilities.  Questions 1. Could the paper provide more insights into the interpretability of the learned representations by DooD especially concerning stroke sequences and stroke layouts 2. How does DooD handle noise or variability in strokebased inputs and does it exhibit robustness to minor variations in stroke patterns  Limitations 1. The generalization of DooD is tested across specific strokebased datasets raising questions about its adaptability to broader domains beyond strokebased tasks. 2. The paper focuses on comparing DooD with specific baselines warranting a more extensive evaluation against a wider range of stateoftheart models for neurosymbolic or generative tasks. 3. The experimental setup mainly showcases output quality necessitating further exploration of the models efficiency scalability and potential limitations in different realworld scenarios. Overall the paper introduces an innovative neurosymbolic approach to generalpurpose representations in strokebased drawing but further exploration and validation in diverse contexts could enhance its applicability and robustness.", "prQT0gN81oG": " Peer Review:  1) Summary: The paper presents a novel framework named SHAKE which bridges the gap between offline and online knowledge distillation methods. The main contribution is the introduction of a shadow head to serve as a studentaware proxy teacher for bidirectional distillation. Extensive experiments on classification and object detection tasks demonstrate the effectiveness of SHAKE achieving stateoftheart results with various neural networks and Vision Transformer models. The method shows compatibility with multiteacher and augmentation strategies providing significant accuracy gains and training efficiency compared to existing knowledge distillation methods.  2) Strengths:  Innovative Approach: The introduction of SHAKE as a framework that combines aspects of offline and online distillation methods is innovative and addresses the performance gap observed in traditional knowledge distillation techniques.  Extensive Experiments: The paper conducts thorough experiments on various datasets and tasks showcasing the superior performance of SHAKE compared to baseline methods. The results indicate significant accuracy gains and training efficiency.  Effective Knowledge Transfer: By leveraging a shadow head as a proxy teacher SHAKE optimizes the distillation process and demonstrates improved teacherstudent alignment enhancing the transfer of knowledge from pretrained models.  3) Weaknesses:  Complexity: While the concept of SHAKE is promising the paper contains complex technical details and formulations which might make it challenging for readers to grasp the methodology without a deep understanding of knowledge distillation and neural networks.  Lack of Benchmarking: The paper could benefit from comparisons with a broader range of stateoftheart knowledge distillation methods to provide a more comprehensive evaluation of SHAKEs performance.  4) Questions:  Generalization: How well does SHAKE generalize to different types of datasets and tasks beyond classification and object detection Have you considered applications in other domains such as natural language processing or reinforcement learning  Scalability: Are there any scalability concerns or limitations when applying SHAKE to larger models or more complex architectures How does the method perform with extremely deep networks or ensemble models  Practical Implementation: Could you provide insights into the practical aspects of implementing SHAKE in realworld scenarios considering factors such as computational resources training time and model deployment  5) Limitations:  Complexity of Formulation: The complexity of the formulation and technical details in the paper may limit the accessibility of the proposed method to a wider audience. Simplifying explanations or providing intuitive examples could enhance the clarity of the research.  Evaluation Metrics: While the paper demonstrates superior performance through experiments additional insights into the choice of evaluation metrics and comparisons with a diverse set of knowledge distillation approaches could strengthen the validation of SHAKEs effectiveness.  Overall Comments: The paper introduces a novel framework SHAKE which effectively bridges the gap between offline and online knowledge distillation methods. The extensive experiments and promising results highlight the potential of this approach in enhancing the efficiency and performance of knowledge transfer processes in deep learning models. Addressing the complexities in formulation and providing clearer explanations could make the research more accessible and impactful in the field of knowledge distillation. Consideration of scalability generalization and practical implementation aspects will further strengthen the robustness and applicability of SHAKE for various machine learning applications.", "xZmjH3Pm2BK": " Peer Review Summary: The paper introduces a Wavelet Scorebased Generative Model (WSGM) as an acceleration technique for Scorebased Generative Models (SGMs) by synthesizing normalized wavelet coefficients conditioned on lower frequencies. The study mathematically analyzes the time discretization properties of the models proves the acceleration obtained by scale separation with wavelets and provides numerical results on Gaussian distributions physical processes at phase transition and natural image datasets. The key contributions include eliminating the dependency of the number of sampling iterations on image size controlling errors of time discretizations of SGMs and demonstrating the acceleration provided by WSGM in generating different types of data sets. Strengths: 1. The paper provides a comprehensive review and analysis of scorebased generative models addressing the computational complexity issue and introducing an innovative solution with WSGM. 2. The mathematical analysis and theoretical results provide a solid foundation for the proposed WSGM method making a valuable contribution to the field. 3. The empirical evaluations on Gaussian distributions physical processes and image datasets demonstrate the effectiveness of WSGM in terms of time complexity and generation quality. 4. The paper is wellorganized with clear sections and explanations of the concepts making it accessible to readers with varying levels of expertise. Weaknesses: 1. The paper focuses extensively on the theoretical foundation and mathematical analysis potentially making it challenging for readers without a strong background in the subject matter to follow the details. 2. While the numerical results are presented a more indepth discussion and comparison with existing acceleration techniques for SGMs could enhance the practical relevance of the proposed method. 3. The limitations of WSGM particularly its applicability only to multiscale processes with nearly white conditional wavelet probabilities could be addressed or explored further to provide insights into potential extensions or improvements. Questions: 1. How does the proposed WSGM method compare to existing acceleration techniques for SGMs in terms of computational efficiency and generation quality Are there specific scenarios where WSGM outperforms other methods 2. Can the theoretical results and analysis presented in the paper be extended or applied to a broader range of nonGaussian multiscale processes beyond the Gaussian setting Are there any limitations or challenges in doing so 3. In practice how well does the WSGM method generalize to different types of datasets and realworld applications beyond the specific examples discussed in the paper Limitations: 1. The paper is limited by its theoretical focus and may benefit from more practical demonstrations or case studies to illustrate the realworld impact and performance of WSGM. 2. The generalizability of the results and findings to diverse datasets and applications beyond the ones studied in the paper should be further investigated to assess the broader applicability of WSGM. In summary the paper offers a novel approach with WSGM to address the computational complexity issues of SGMs supported by strong theoretical foundations and empirical evaluations. The incorporation of practical insights and comparisons with existing techniques could enhance the overall impact and applicability of the proposed method.", "tmUGnBjchSC": " Peer Review of the Paper on Bayesian Optimization with DecisionTheoretic Entropies:  1) Summary: The paper introduces a novel approach to Bayesian optimization by unifying decisiontheoretic and informationtheoretic paradigms. It proposes a family of decisiontheoretic entropies parameterized by a loss function and action set leading to a customizable acquisition function tailored for downstream use cases. The authors develop gradientbased optimization methods and demonstrate strong empirical performance on various optimization tasks.  2) Strengths and Weaknesses: Strengths:  Innovative Approach: The paper presents a novel framework that unifies two paradigms in Bayesian optimization offering a flexible and customizable approach.  Theoretical Grounding: The incorporation of decisiontheoretic entropies based on loss functions provides a principled basis for acquiring optimal solutions in diverse optimization scenarios.  Empirical Evaluation: Strong empirical results across diverse tasks demonstrate the effectiveness of the proposed method in practical settings. Weaknesses:  Complexity: The theoretical and technical complexity of decisiontheoretic entropies and gradientbased methods may limit accessibility for readers without a strong background in Bayesian optimization.  Limited Comparison: The comparison with baseline methods is relatively general lacking indepth comparison with stateoftheart optimization techniques.  Clarity: Some sections particularly those with mathematical derivations may pose challenges for readers not wellversed in the topic.  3) Questions:  Optimality of EHIG: How does the EHIG acquisition function compare with other widely used acquisition functions like UCB Thompson Sampling or PI in terms of performance and computational efficiency  Generalizability: Can the proposed framework accommodate nonGaussian process models and highdimensional input spaces while maintaining computational efficiency  4) Limitations:  Scalability: The paper focuses on empirical performance but does not thoroughly address scalability issues for largescale optimization tasks or datasets.  RealWorld Applicability: While the paper highlights diverse applications a deeper exploration of realworld scenarios and challenges faced during practical implementation could enhance the relevance of the proposed method.  Verdict: Overall the paper presents a promising and innovative framework for Bayesian optimization using decisiontheoretic entropies. The theoretical foundation coupled with strong empirical results demonstrates the potential of the method. Addressing accessibility scalability and broader applicability could further strengthen the impact of this research in the field of Bayesian optimization.", "sn6BZR4WvUR": " Peer Review of the Scientific Paper  1) Summary The paper introduces a novel optimization framework named multiscale perturbed gradient descent (MPGD) which incorporates controlled chaotic perturbations to the gradient descent (GD) algorithms. The study analyzes MPGD from three angles: theoretical analysis showcasing convergence to a stochastic differential equation (SDE) generalization bounds and implicit regularization effects. The empirical results demonstrate the advantages of MPGD in various optimization tasks.  2) Strengths and Weaknesses Strengths:  The paper introduces a novel optimization framework MPGD with theoretical analysis and empirical results supporting its advantages.  The study provides a rigorous mathematical analysis showcasing convergence of MPGD to an SDE driven by a heavytailed L\u00e9vystable process.  It establishes generalization bounds for MPGD and discusses the implicit regularization effect introduced by the chaotic perturbations.  Empirical results on various tasks including minimizing the widening valley loss Airfoil SelfNoise prediction and CIFAR10 classification show the superiority of MPGD over baseline GD and GD with Gaussian perturbations. Weaknesses:  The paper is very technical and may be challenging for readers with limited background in optimization theory and stochastic processes.  The empirical results could benefit from more detailed discussion and analysis to enhance understanding for readers.  The paper could elaborate more on the practical implications and potential realworld applications of the proposed MPGD framework.  3) Questions  How crucial is the selection of parameters such as \u00b5 and \u03c3 in MPGD and how sensitive is the performance to these parameters  Do the theoretical results on generalization bounds and implicit regularization hold for a wide range of optimization problems or are there specific scenarios where MPGD excels  Could further insights be provided on the computational complexity and scalability of MPGD compared to traditional optimization algorithms  4) Limitations  The paper primarily focuses on theoretical analysis and empirical validations but it could benefit from a deeper discussion on the practical applications and implications of MPGD in realworld optimization problems.  The generalization bounds and regularization effects are demonstrated theoretically and empirically but further investigations on how MPGD performs in scenarios with noisy data or nonstationary environments could enrich the scope of the study.  The study acknowledges the limitations of MPGD but a more extensive comparison with other stateoftheart optimization algorithms could provide a clearer understanding of its competitiveness. In conclusion the paper presents a novel optimization framework MPGD with strong theoretical underpinnings and promising empirical results. It opens up avenues for future research in controlled chaotic perturbations in optimization and could potentially have significant implications in advancing optimization algorithms.", "wwW-1k1ljIg": " Summary The paper proposes two novel methods for detecting backdoor attacks in convolutional neural networks (CNNs) by analyzing preactivation distributions. The methods are based on differential entropy and KullbackLeibler divergence allowing for efficient identification of potential backdoor neurons. Experimental results demonstrate the effectiveness of the proposed defense strategies in eliminating backdoor behaviors with minimal impact on model performance.  Strengths 1. Innovative Approach: The paper introduces novel methods based on preactivation distributions for backdoor attack detection in CNNs offering a unique perspective on defense strategies. 2. Effective Defense: Experimental results show that the proposed methods outperform existing defense strategies achieving superior results in eliminating backdoor attacks with minimal accuracy loss. 3. Efficiency: The proposed methods are demonstrated to be efficient requiring less computational time compared to baseline defense methods enhancing their practical applicability.  Weaknesses 1. Limited Evaluation: The paper could benefit from more extensive evaluation on a wider range of datasets and attack scenarios to further validate the generalizability of the proposed methods. 2. Assumption Dependency: The effectiveness of the proposed methods relies heavily on specific assumptions about the distributional properties of backdoor neurons which may limit their applicability in diverse realworld scenarios.  Questions 1. How do the proposed methods compare against stateoftheart defense mechanisms in terms of robustness against advanced backdoor attacks 2. Are there any potential scenarios where the assumptions made in the paper about the distributional properties of backdoor neurons might not hold and how would the proposed methods perform in such cases 3. Can the proposed methods be generalized to other types of neural networks beyond CNNs and if so what modifications or considerations would be necessary  Limitations 1. Generalization: The proposed methods effectiveness may be limited to specific datasets and attack scenarios requiring careful consideration of generalization to diverse settings. 2. Hyperparameter Sensitivity: While the proposed methods are stated to be less sensitive to hyperparameters more indepth analysis of their robustness to variations in hyperparameters is warranted for comprehensive understanding. Overall the paper presents innovative methods for detecting backdoor attacks in CNNs by analyzing preactivation distributions showing promising results in defense against backdoor attacks. Further research could explore the generalizability and practical implications of the proposed methods in diverse contexts.", "sQ2LdeHNMej": "Summary: The paper introduces FATHOM (Federated AuTomatic Hyperparameter OptiMization) as a oneshot online procedure that adapts hyperparameters in federated optimization particularly for Federated Averaging (FedAvg). It addresses the challenges of communication efficiency and reduction of local computation resources by adaptively optimizing client learning rate number of local steps and batch size. The authors provide theoretical convergence bounds and empirical evaluation on Federated EMNIST62 (FEMNIST) and Federated Stack Overflow (FSO) datasets using FedJAX framework. Strengths: 1. Innovative Approach: FATHOM introduces a novel method for adaptive hyperparameter optimization in federated learning improving communication efficiency and reducing local computation. 2. Theoretical Analysis: The paper provides theoretical convergence results and guarantees for the proposed approach under specific assumptions. 3. Empirical Evaluation: Extensive empirical experiments on real datasets demonstrate the effectiveness of FATHOM over traditional methods. Weaknesses: 1. Limited Comparison: The paper lacks comparisons with other oneshot FL HPO methods due to the absence of a standardized benchmark. 2. Flexibility vs. Generalization: FATHOMs performance advantage is discussed but its reduced flexibility in optimizing a wide range of hyperparameters could be a limitation. 3. Communication Effort: The paper does not deeply explore the potential communication overhead of the proposed algorithm. Questions: 1. How does FATHOM compare in terms of scalability to larger models or datasets in federated learning 2. Are there specific scenarios or datasets where FATHOM may not perform as efficiently as traditional methods 3. How does the inclusion of further complexity such as extra weights and activation functions impact the performance of FATHOM Limitations: 1. Benchmark Suite: The absence of a standardized benchmark suite for FL HPO methods limits comprehensive performance comparisons. 2. Evaluation: While the empirical results are promising further validation on diverse datasets and tasks could enhance the robustness of the findings.", "mhP6mHgrg1c": " Summary: The paper introduces \"ORIENT\" a subset selection framework based on Submodular Mutual Information (SMI) functions for supervised domain adaptation. The aim is to improve the efficiency of existing supervised domain adaptation methods by training on subsets of source data that are similar to the target data. The proposed framework is shown to accelerate training and improve performance on target data when combined with existing SDA methods like CCSA dSNE and standard Crossentropy training.  Strengths: 1. The paper addresses an important issue in machine learning by focusing on the efficiency of supervised domain adaptation methods. 2. The proposed ORIENT framework is based on sound theoretical foundations specifically utilizing SMI functions for subset selection. 3. The empirical evaluation on realworld datasets demonstrates the effectiveness of ORIENT in reducing training time while maintaining or improving prediction accuracy. 4. The paper provides a detailed explanation of the proposed framework algorithm and experimental methodology.  Weaknesses: 1. The paper lacks comparison with a wider range of existing domain adaptation techniques limiting the comprehensive assessment of ORIENT against the stateoftheart. 2. The discussion around the limitations of the proposed approach could be more detailed particularly regarding scenarios where ORIENT may not perform as effectively.  Questions: 1. How does the proposed ORIENT framework scale with larger datasets considering memory requirements for storing similarity kernels 2. Are there considerations for the robustness of ORIENT when dealing with noisy or imbalanced datasets in a realworld scenario 3. How does the choice of the subset size impact the performance of ORIENT and are there optimal strategies for determining this parameter  Limitations: 1. The paper primarily focuses on the efficiency improvement in supervised domain adaptation without exploring other aspects such as transferability to different domains or robustness to various shift types. 2. The evaluation is primarily limited to two datasets potentially lacking generalizability to diverse realworld scenarios. Overall the paper offers a novel approach to enhancing supervised domain adaptation efficiency but would benefit from further investigation into scalability generalization capacity and robustness to diverse data conditions. Further details on the limitations and practical implications of the proposed framework would enhance the completeness of the study.", "xK6wRfL2mv7": " Peer Review:  1) Summary: The paper introduces a novel training algorithm called SharpnessAware Training for Free (SAF) to address the overparameterization issue in deep neural networks. By proposing a trajectory loss to quantify sharpness the authors aim to minimize sharp local minima with minimal computational cost. The SAF algorithm along with its memoryefficient variant MESA is demonstrated to outperform the stateoftheart SharpnessAware Minimization (SAM) method achieving better generalization capabilities on benchmark datasets while maintaining training speed with the base optimizer.  2) Strengths and weaknesses: Strengths:  The paper addresses a critical issue in deep learning by proposing a novel algorithm to reduce generalization error in overparameterized networks.  The introduction of SAF and MESA algorithms provides a solution to the computational inefficiency of existing methods like SAM.  Extensive empirical results on CIFAR10100 and ImageNet datasets support the effectiveness of SAF and MESA in improving generalization capabilities.  The comparison with baselines and visualization of loss landscapes enhance the clarity of the proposed methods benefits. Weaknesses:  The theoretical derivations and methodologies are detailed but the description could be made more accessible to a broader audience by simplifying complex technical terms.  The paper focuses heavily on technical details which might make it challenging for readers not familiar with the topic to grasp the significance of the proposed approach.  The experimental section lacks detailed analysis of failure cases or limitations observed during the evaluation processes.  While the comparison with existing algorithms is comprehensive further discussions on practical implications and realworld applications could enhance the papers impact.  3) Questions:  What are the implications of the proposed SAF and MESA algorithms beyond image classification tasks Have you tested these algorithms on other tasks such as natural language processing or reinforcement learning  How would the proposed algorithms handle dynamic changes in data distribution or adversarial attacks during training  Can you provide insights into the tradeoffs associated with using the trajectory loss instead of directly minimizing the sharpness measure in terms of convergence speed and overall training efficiency  4) Limitations:  The paper primarily focuses on empirical results and lacks indepth theoretical analysis or formal proofs to underpin the proposed algorithms efficacy.  The experiments are conducted on standard benchmark datasets which might limit the generalizability of the findings to diverse tasks or data domains.  The memoryefficient variant MESA is introduced to address scalability issues however the tradeoffs in terms of additional computations and potential performance impacts need further exploration. Overall the paper presents a significant advancement in addressing the overparameterization challenge in deep learning through the introduction of SAF and MESA algorithms. By effectively minimizing sharp local minima while maintaining computational efficiency the proposed methods contribute to the ongoing research on enhancing deep neural network training strategies. Further elucidating the practical implications and investigating the scalability and adaptability of these algorithms to various domains would enhance the papers relevance and impact in the deep learning community.", "w1CF57sLstO": " Peer Review  Summary: The paper investigates the generalization properties of ModelAgnostic MetaLearning (MAML) under overparameterization focusing on the impact of various learning parameters and task distributions. The study utilizes a mixed linear regression model to analyze the excess risk of MAML trained with SGD providing upper and lower bounds for the excess risk. The results highlight the importance of task diversity in influencing generalization ability and show how the adaptation learning rate affects excess risk and early stopping time. Theoretical findings are validated through experiments.  Strengths: 1. Sharp Characterization: The paper presents precise upper and lower bounds for the excess risk of MAML capturing the subtle dynamics that influence generalization bounds. 2. Comprehensive Analysis: The study explores the impact of key learning parameters data and task distributions on the generalization capability of MAML providing valuable insights into the workings of the algorithm. 3. Experimental Validation: The theoretical findings are supported by experimentation strengthening the credibility and applicability of the results. 4. Novel Contribution: The paper addresses an important problem in deep metalearning focusing on overparameterization which contributes significantly to the existing body of knowledge in the field.  Weaknesses: 1. Complexity: The paper involves intricate mathematical derivations and concepts that might make it challenging for readers without a strong background in the subject matter to fully grasp the content. 2. Limitation in RealWorld Application: While the theoretical insights are valuable the practical implications of the findings could be further explored to relate the research to realworld metalearning applications more explicitly. 3. Limited Discussion on Practical Implementation: The paper could benefit from a more detailed discussion on how the theoretical insights could be practically applied or translated into improvements in metalearning algorithms in realworld scenarios.  Questions: 1. Generalization in Diverse Scenarios: How do the results of the study generalize across different task and data distributions beyond the specific cases examined in the research Are there plans to extend the analysis to a broader range of scenarios 2. Practical Relevance: How can the theoretical insights gained from this study be practically applied to enhance the performance of metalearning algorithms in realworld applications 3. Optimization Trajectory: Could you elaborate more on how the optimization trajectory of SGD influences the generalization bounds for MAML and what practical implications this has for training deep metalearning models  Limitations: 1. Theoretical Focus: The study predominantly focuses on theoretical analysis and may benefit from further discussions on the practical implementations and implications of the research findings. 2. Scope of Experiments: While experimental validation is mentioned more extensive experimental results and comparisons with existing methods could strengthen the validation of the theoretical findings. Overall the paper presents a rigorous theoretical investigation into the generalization properties of MAML in the overparameterized regime offering valuable insights into the dynamics and influences that affect the algorithms performance. Further efforts to bridge the gap between theory and practice and extend the analysis to more diverse scenarios would enhance the impact and applicability of the research.", "lCGYC7pXWNQ": " Summary The paper introduces FrameMaker a memoryefficient video classincremental learning approach that aims to reduce memory consumption by condensing multiple frames into one representative frame for each video. FrameMaker consists of two main components: Frame Condensing and InstanceSpecific Prompt. Frame Condensing integrates multiple frames into one condensed frame while InstanceSpecific Prompt compensates for lost spatiotemporal details in the condensed frame. Experimental results on challenging benchmarks show that FrameMaker outperforms existing methods while consuming only 20 of memory.  Strengths and Weaknesses Strengths:  FrameMaker addresses the issue of memory consumption in video classincremental learning.  The proposed InstanceSpecific Prompt complements the drawbacks of Frame Condensing.  Experimental results demonstrate the superior performance of FrameMaker compared to existing methods.  The ablation studies provide insights into the effectiveness of different components. Weaknesses:  The paper lacks a clear discussion on the scalability and applicability of FrameMaker to realworld applications.  The limitations of FrameMaker especially in highly constrained memory budgets need further exploration.  The explanation of the societal impacts is limited and could be expanded.  Visualizations and analyses could be enhanced to make the findings more interpretable.  Questions 1. How does FrameMaker ensure that the condensed frames contain sufficient information for accurate action recognition 2. Could the proposed InstanceSpecific Prompt potentially introduce bias or overfitting in the learning process 3. Are there any implications of FrameMaker on online learning scenarios where the learning process is continuous and dynamic 4. What are the computational requirements of FrameMaker in terms of training time and inference speed compared to existing methods  Limitations The paper provides a comprehensive overview of FrameMaker and its components. However it lacks indepth discussions on the scalability robustness and potential limitations of the proposed approach. Additionally the societal impacts section could be further developed to explore the ethical implications of using FrameMaker in various applications. Overall the paper presents an innovative approach to memoryefficient video classincremental learning but further investigation and analysis are needed to address potential limitations and ensure the practicality of FrameMaker in various settings.", "oWqWiazEb62": " Peer Review:  Summary: The paper addresses the challenge of training machine learning models with datadependent constraints focusing on ensuring fairness and stability goals. It introduces a method for reformulating datadependent constraints to guarantee their expected value counterparts are satisfied with a specified probability. The authors propose a distributionally robust optimization approach and provide theoretical guarantees on exact constraint satisfaction probability. They also develop a dual ascent algorithm for solving the reformulated problem and demonstrate the methods efficacy on fairnesssensitive classification tasks. The paper includes simulations and experiments to validate the theory and method.  Strengths: 1. Novel Contribution: The paper presents a novel method for handling datadependent constraints in machine learning models specifically focusing on fairness issues. 2. Theoretical Framework: The paper provides a strong theoretical foundation including formulation of constraints guarantees on constraint satisfaction probability and development of a dual ascent algorithm. 3. Empirical Validation: The method is demonstrated through simulations and experiments showing promising results in ensuring fairness while controlling costs.  Weaknesses: 1. Complexity: The paper can be challenging to follow due to its technical nature especially for readers not wellversed in the mathematical formulations of optimization problems. 2. Generalizability: The applicability of the proposed method in diverse realworld scenarios beyond fairnesssensitive classification tasks should be further explored. 3. Applicability: The practical implementation and scalability of the proposed method in larger datasets or complex models may require more investigation.  Questions: 1. Scalability: How does the computational complexity of the dual ascent algorithm scale with the size of the dataset or the complexity of the model 2. Robustness: How sensitive is the proposed method to the choice of the uncertainty set radii \\xe2\\x87\\xa2 in realworld applications with varying levels of noise and uncertainty 3. Comparison: How does the proposed method compare with existing fairnessaware machine learning techniques in terms of accuracy fairness and computational efficiency 4. Extension: Can the method be extended to handle constraints on nonlinear functions of expected values as mentioned in the future research direction  Limitations: 1. Limited Application: The paper focuses primarily on fairnesssensitive classification tasks limiting the generalizability of the proposed method to a broader range of machine learning applications. 2. Algorithm Complexity: The complexity of the proposed dual ascent algorithm and its scalability in largescale settings may present challenges for practical implementation. 3. Empirical Validation: While the simulations and experiments show promising results the methods performance in more diverse and complex realworld datasets needs further exploration. In conclusion the paper offers a valuable contribution to the field by addressing the challenges of ensuring fairness through datadependent constraints. Further research and experimentation can help enhance the practical applicability and robustness of the proposed method.", "r6_zHM2POTd": " Summary: The paper investigates how tightly balanced recurrent neural networks (RNNs) improve the encoding of timevarying stimuli. By developing a nonstationary dynamic meanfield theory the authors show that the mutual information rate increases linearly with the tightness of balance in the presence of noise and chaos. The study also demonstrates that training RNNs on timevarying signals leads to the emergence of strong recurrent inhibition enhancing information transmission. Furthermore the emergence of weakly connected subnetworks in networks trained on multiple independent signals is observed. The findings suggest that a tight balance between excitatory and inhibitory currents enhances information encoding in RNNs.  Strengths: 1. The paper provides a novel nonstationary meanfield theory that transparently explains how a tight balance of currents improves information encoding in recurrent networks. 2. The study effectively links the tightness of balance with the mutual information rate demonstrating the importance of balanced networks in reliably transmitting timevarying stimuli. 3. The frequencyresolved mutual information rate analysis along with training results on various tasks strengthens the validity and generality of the findings. 4. The implications for network architecture design and information encoding in RNNs provide valuable insight for the neuroscience and machine learning communities.  Weaknesses: 1. The paper primarily focuses on firing rate networks with thresholdlinear transfer functions limiting the generalizability of the findings to more complex neuron models. 2. Although the theoretical results are supported by numerical simulations the study mainly considers scalar input signals leaving out multidimensional inputs which could be important for a broader understanding. 3. The discussion of biological relevance and energy constraints of tightly balanced networks could be further elaborated to provide a more comprehensive perspective. 4. The study mentions that the theory becomes exact only for large network sizes raising the question of how finitesize effects may impact the results.  Questions: 1. How do the findings from firing rate networks with thresholdlinear transfer functions extend to more complex biophysical neuron models such as spiking neural networks 2. What are the implications of tightly balanced networks for neuromodulatory mechanisms or learning rules in biological systems 3. Can the study be expanded to investigate the effects of memoryrelated tasks where the interactions between task dynamics and network balance could play a significant role  Limitations: 1. The study primarily focuses on firing rate networks and scalar inputs limiting the generalizability to more complex neuron models and multidimensional inputs. 2. The discussion on biological constraints energetic considerations and finitesize effects could provide a more comprehensive understanding of the implications of tightly balanced networks. 3. The research highlights the importance of extending the study to tasks with complex temporal structures to further elucidate the impact of network balance on information encoding.", "pbILUUf_hBN": " Peer Review of the Paper  1) Summary: The paper introduces a novel algorithm EXP3.G for online learning with feedback graphs presenting nearoptimal regret bounds in both stochastic and adversarial environments. The algorithm leverages the structure of the feedback graph to reduce exploration efficiently achieving bestofbothworlds guarantees. The paper extends the results to timevarying feedback graphs and provides detailed analysis and proofs supporting the algorithms performance.  2) Strengths and weaknesses: Strengths:  The novel EXP3.G algorithm offers nearoptimal regret bounds in both stochastic and adversarial environments.  The exploration scheme efficiently utilizes the graph structure reducing exploration while ensuring effective learning.  The extension to timevarying feedback graphs broadens the applicability of the algorithm.  The detailed analysis proofs and comparisons with existing works provide a solid foundation for the presented results. Weaknesses:  The complexity of the algorithm and the dependency on graph characteristics may limit its practical applicability in certain scenarios.  The need for prior knowledge of independence and strong independence numbers for optimal performance could be a practical challenge.  The extensive mathematical and theoretical nature of the paper may limit its accessibility to a broader audience.  3) Questions:  How sensitive is the performance of EXP3.G to variations in the graph structure especially in scenarios where the independence number varies significantly over time  Can the algorithm adapt dynamically to changing graph properties such as an abrupt increase in the independence number  Are there practical implementations or realworld examples where the proposed algorithm has been applied showcasing its effectiveness beyond theoretical analyses  4) Limitations:  The complexity introduced by the need for prior knowledge of graph characteristics may hinder the algorithms usability in scenarios where such information is unavailable or changes frequently.  The theoretical focus and intricate mathematical derivations might challenge wider adoption and practical implementation without more userfriendly tools or simplified explanations.  The paper lacks empirical validation or experimental results on realworld datasets limiting the assessment of the algorithms performance in practical scenarios. Overall the paper presents a comprehensive study on online learning with feedback graphs introducing a promising algorithm with strong theoretical guarantees. However addressing practical implementation challenges validating the algorithm on diverse datasets and enhancing user accessibility could further strengthen the impact and applicability of the proposed work.", "n0dD3d54Wgf": "Peer Review Summary: The paper introduces a novel framework called SparCL which leverages sparsity to enable costeffective continual learning on edge devices. SparCL achieves training acceleration and accuracy preservation through weight sparsity data efficiency and gradient sparsity. The effectiveness of SparCL is demonstrated through experiments on CL benchmarks and real mobile devices showcasing improvements in training efficiency and accuracy preservation compared to existing methods. Strengths: 1. Innovative Framework: SparCL introduces a novel approach to continual learning by leveraging sparsity which is crucial for resourcelimited scenarios and edge devices. 2. Comprehensive Evaluation: The paper provides a thorough evaluation of SparCL through experiments on multiple benchmarks and real mobile devices demonstrating consistent improvements in training efficiency and accuracy preservation. 3. Effectiveness of Components: The ablation study highlights the effectiveness of key components like taskaware dynamic masking dynamic data removal and dynamic gradient masking in enhancing efficiency and accuracy. 4. RealWorld Application: Evaluating SparCL on a real mobile phone showcases the practical potential of the framework making it relevant for realworld deployment on edge devices. Weaknesses: 1. Detailed Implementation: The paper could benefit from providing more details on the implementation of SparCL such as specific algorithms used hyperparameter settings and specific model architectures. 2. Comparison with Sparse Training: While the paper compares SparCL with CL methods and CLadapted sparse training methods a more detailed comparison with traditional sparse training methods could provide further insights. Questions: 1. Generalization: How well does SparCL generalize to different types of tasks and datasets beyond the ones tested in the experiments 2. Scalability: How scalable is SparCL to larger datasets and more complex models especially when deployed on resourceconstrained edge devices 3. Adaptability: How easily can SparCL be adapted to different continual learning scenarios and different types of neural network architectures Limitations: 1. Scope of Evaluation: The evaluation mainly focuses on specific benchmarks and scenarios broader generalization to diverse datasets and realworld applications could provide deeper insights. 2. Complexity: The complexity of implementing all components of SparCL may pose a challenge for users without detailed guidance or extensive computational resources for training. Overall the paper presents a promising framework in the field of continual learning with notable strengths in efficiency and accuracy preservation. Further exploration of generalization scalability and adaptability could enhance the practicality and applicability of SparCL in various realworld scenarios.", "x3JsaghSj0v": "Peer Review 1) Summary The paper introduces Adaptive Node Sampling for Graph Transformer (ANSGT) to address the deficiencies in current graph transformers and achieve competitive performance especially on large graphs. The proposed method involves formulating the optimization strategies of node sampling as an adversary bandit problem and introducing a hierarchical attention scheme with graph coarsening. Extensive experiments on synthetic and realworld datasets demonstrate the superiority of ANSGT over existing graph transformers and popular Graph Neural Networks (GNNs). 2) Strengths and Weaknesses Strengths:  The paper addresses a significant issue in the field of graph representation learning by proposing ANSGT to improve the performance of graph transformers on large graphs.  The proposed method is theoretically sound and is supported by empirical evidence from experiments on both synthetic and realworld datasets.  The hierarchical attention scheme and adaptive node sampling approach demonstrate innovative solutions to tackle the deficiencies in existing graph transformers. Weaknesses:  The paper can provide more detailed explanations on the theoretical underpinnings of the proposed method especially regarding the bandit problem formulation and reward mechanisms.  The computational complexity analysis lacks specificity and may benefit from additional details on the efficiency of the proposed method compared to existing approaches.  More comprehensive comparisons with a wider range of baseline methods including recent approaches beyond Graph Neural Networks could strengthen the evaluation of ANSGTs performance. 3) Questions  Can the proposed hierarchical attention scheme be easily extended or adapted to handle different types of graphs or data structures beyond the ones mentioned in the experiments  How sensitive is the performance of ANSGT to hyperparameters and were any sensitivity analyses conducted to optimize these parameters  Have the authors considered the potential impact of noise or sampling errors in the adaptive node sampling process and its implications for the models robustness in realworld scenarios 4) Limitations  The paper could benefit from a more detailed discussion on how ANSGT performs on extremely large graphs or in scenarios with highdimensional graph structures.  The generalizability of the proposed method to domains beyond graph data such as sequential data or images could be explored further to assess the versatility of ANSGT.  Limited discussions on the potential drawbacks or challenges of implementing ANSGT in practical applications could provide a more wellrounded perspective on the methods limitations. Overall the paper presents a novel approach ANSGT with promising results in improving graph transformer performance on large graphs. The proposed hierarchical attention scheme and adaptive node sampling strategy offer innovative solutions to address existing deficiencies in the field of graph representation learning. Further exploration and validation of the methods efficacy on diverse datasets and scenarios could enhance its impact and applicability in realworld applications.", "u4KagP_FjB": " Peer Review  Summary The paper introduces Spartan a novel method for training sparse neural network models with a predetermined level of sparsity. Spartans leverage an optimal transportationbased topk masking operation to induce parameter sparsity during training. This method aims to balance exploration and exploitation of sparsity patterns during training and achieves competitive results in terms of model accuracy and sparsity levels on both ResNet50 and ViT models trained on ImageNet1K dataset.  Strengths  The paper presents a wellstructured and organized introduction to the problem and solution.  Spartan introduces a novel method for sparse neural network training which yields strong results in terms of model accuracy and sparsity levels.  The experimental evaluation is comprehensive covering ResNet50 and ViT models on ImageNet1K dataset with comparisons to existing methods.  The opensource implementation of Spartan on GitHub enhances reproducibility and allows for further exploration.  Weaknesses  It would be beneficial to provide more insight into the tradeoff between different hyperparameters and their impact on model performance.  The paper could benefit from further discussion on the theoretical and empirical analysis of training dynamics under sparsity constraints.  Questions 1. How does Spartan compare to stateoftheart methods in terms of training convergence speed and final model accuracy under different sparsity levels 2. Can the proposed method handle more complex cost models beyond linear cost models for sparsity allocation 3. Are there any scenarios where Spartan may not be the optimal choice for sparse neural network training and if so what are the limitations  Limitations  The paper may lack a detailed explanation of the implications and tradeoffs of the proposed method when applied to different types of neural network architectures.  The scalability and generalizability of Spartan to other datasets beyond ImageNet1K could enhance the scope and applicability of the method. Overall the paper presents a novel approach to sparse neural network training with promising results across different models. Addressing the questions and potential limitations could further strengthen the papers impact and relevance in the field of neural network optimization and sparsity training.", "ldl2V3vLZ5": " Peer Review  1) Summary The paper introduces S3GC a method for scalable graph clustering with node feature sideinformation. S3GC combines Graph Neural Networks contrastive learning and node features to learn clusterable features effectively. Extensive evaluations demonstrate the superior performance of S3GC over stateoftheart methods particularly on largescale datasets.  2) Strengths and weaknesses Strengths:  S3GC effectively combines graph and node feature information for improved clustering accuracy.  The proposed method outperforms stateoftheart methods in terms of clustering accuracy especially on largescale datasets.  Rigorous experimental evaluation across various benchmarks demonstrates the scalability and effectiveness of S3GC.  The scalability of the method is highlighted with successful application to datasets containing millions of nodes. Weaknesses:  The paper could further explain the computational complexity and efficiency of the proposed method for practical deployment.  Providing insights into the interpretability of the learned representations could enhance the understanding of S3GCs clustering performance.  While extensive empirical evaluation is provided a theoretical analysis of S3GCs performance could strengthen the paper.  3) Questions  Can the authors elaborate on the interpretability of the learned clusterable features by S3GC and how they relate to the underlying graph structure and node attributes  How does the choice of hyperparameters impact the clustering performance of S3GC particularly in scenarios with varying data characteristics  Are there specific realworld applications where S3GCs performance advantages over existing methods would have significant practical implications  4) Limitations  The paper could provide more insights into the memory and computational requirements of S3GC especially when scaling to very large datasets.  While the empirical evaluation is robust further explanation on the generalizability of S3GC to diverse graph types and scenarios can be beneficial.  The paper lacks a theoretical analysis of S3GCs performance guarantees or stability in various clustering scenarios which could strengthen the methods theoretical foundation. In conclusion the paper introduces S3GC as a promising method for scalable graph clustering with node features showcasing superior performance over existing methods. Further exploration of interpretability theoretical analysis and realworld applications could enhance the impact and understanding of S3GC in the graph clustering domain.", "mvbr8A_eY2n": " Summary The paper introduces a novel approach to fair allocation problems by formulating the optimal transport problem with envy constraints. It presents a concise representation and geometric interpretation of the solution structure allowing for efficient tradeoffs between efficiency and envy. The authors propose a stochastic optimization algorithm with provable convergence and analyze the statistical properties of the solution space. Experimental results on artificial and simulated data demonstrate the effectiveness of the proposed approach in largescale allocation problems.  Strengths 1. Innovative Problem Formulation: The paper introduces a unique approach to fair allocation problems by considering envy constraints in the optimal transport framework. 2. Efficient Tradeoffs: The ability to trade off efficiency and envy continuously is a significant strength offering flexibility in achieving fairness. 3. Stochastic Optimization Algorithm: The proposed algorithm with provable convergence provides a practical solution for computing optimal allocations. 4. Statistical Properties Analysis: The study of the space of optimal transportbased allocation policies with a sample complexity bound enhances the theoretical foundation of the approach. 5. Experimental Validation: The experiments conducted on artificial and simulated data validate the effectiveness of the proposed method in practical scenarios.  Weaknesses 1. Limited Realworld Application: While the blood donation matching problem is discussed as motivation the paper could benefit from more diverse realworld application examples to further illustrate the methods versatility. 2. Complexity of the Model: The complexity of the proposed model especially in incorporating envy constraints may limit its practical adoption in certain scenarios. 3. Limited Discussion on Scalability: The scalability of the approach for larger allocation problems or complex environments is not extensively discussed.  Questions 1. Generalization to other Domains: How adaptable is the proposed approach to different allocation problems beyond blood donation matching Are there specific characteristics of problems where this method may not be suitable 2. Robustness to Noisy Data: How robust is the algorithm to noisy or imperfect data in realworld scenarios 3. Comparison with Existing Methods: How does the proposed method compare in terms of performance and computational efficiency against existing fairnessaware allocation algorithms in largescale settings  Limitations 1. The paper focuses on a specific problem domain limiting the generalizability of the approach to diverse allocation scenarios. 2. The scalability and computational complexity of the proposed algorithm for very largescale allocation problems are not extensively addressed. Overall the paper presents an intriguing approach to fair allocation problems with envy constraints offering promising theoretical foundations and practical utility in largescale allocation scenarios. Further exploration into realworld applications and scalability considerations could enhance the impact of the research.", "riIaC2ivcYA": " Peer Review:  1) Summary: The paper introduces INeurAL a novel neuralnetworkbased active learning algorithm for the nonparametric streaming setting. It proposes two regret metrics and incorporates a tailored exploration strategy query decisionmaker and update rules. The algorithm is designed to improve theoretical and empirical performance compared to stateoftheart methods. The regret analysis shows promising results and experimental evaluations on various datasets demonstrate the effectiveness of INeurAL.  2) Strengths and Weaknesses: Strengths:  Innovative Approach: The development of INeurAL introduces novel components tailored for the active learning setting addressing limitations of previous works.  Theoretical Analysis: The regret analysis provides valuable insights into the performance guarantees of the algorithm under different scenarios.  Empirical Evaluation: Extensive experiments conducted on realworld datasets show improved performance over stateoftheart baselines. Weaknesses:  Complexity: The complexity of the proposed algorithm and the theoretical analysis might limit its applicability in simpler scenarios.  Implementation Concerns: Lack of details on practical implementation aspects such as computational efficiency and scalability may raise concerns for realworld deployment.  Limited Comparison: While the paper compares INeurAL with some existing baselines a more comprehensive comparison with a broader range of algorithms could strengthen the evaluation.  3) Questions:  Could the authors elaborate on the practical considerations regarding the computational efficiency of INeurAL especially in scenarios with largescale datasets  How does the algorithm handle concept drift and evolving data distributions in the streaming setting over an extended period  Can the proposed regret metrics be extended or adapted to other active learning settings beyond the nonparametric streaming context  4) Limitations:  Scalability: The effectiveness of INeurAL on largescale datasets and the ability to handle highdimensional input spaces may be limited by scalability concerns.  Generalization: The paper focuses on kclass classification problems investigating the algorithms performance on regression tasks or other problem domains could further validate its versatility.  Overall Comments: The paper presents an innovative approach to active learning in nonparametric streaming scenarios showcasing promising results in both theory and practice. Addressing the identified limitations and expanding the scope of evaluation could enhance the impact and applicability of INeurAL in diverse realworld applications. The clear articulation of the algorithms strengths and weaknesses along with thorough experimental validations facilitates a comprehensive understanding of its contributions in the active learning research domain.  Overall Rating: 45", "q-FRENiEP_d": "Peer Review Summary (95 words): The paper introduces SageMix a saliencyguided Mixup technique for point clouds to preserve salient local structures. The method outperforms existing Mixup approaches in 3D vision tasks demonstrating significant improvements in generalization performance robustness and uncertainty calibration. Extensive experiments on benchmark datasets show consistent performance gains including part segmentation and standard 2D image classification. The proposed method offers a novel approach to data augmentation in point clouds by efficiently preserving discriminative regions and generating continuous samples with smoothly varying mixing ratios. Strengths: 1. Novel approach: Introducing a saliencyguided Mixup technique for point clouds is a significant contribution to the field of 3D vision and deep learning. 2. Extensive experiments: The paper provides comprehensive experiments demonstrating the effectiveness of SageMix across various benchmark datasets and tasks. 3. Performance: The results show consistent and significant improvements over stateoftheart methods in terms of generalization robustness and uncertainty calibration. 4. Code availability: The availability of the code on GitHub enhances reproducibility and adoption of the proposed method. Weaknesses: 1. Lack of comparison: While the paper compares SageMix with existing Mixup methods a broader comparison with other data augmentation techniques or outofdomain datasets could further validate the methods effectiveness. 2. Theoretical insights: More detailed theoretical insights or analysis explaining the underlying mechanisms of SageMix could enhance the understanding of the proposed technique. Questions: 1. How does the bandwidth parameter (\u03c3) in the RBF kernel affect the mixing ratio and the preservation of local structures in SageMix 2. Could the saliencyguided sequential sampling method be further optimized to improve diversity in the selection of query points 3. How does the computational overhead of SageMix compare to other stateoftheart Mixup methods especially in terms of training time and resource utilization Limitations: 1. Domain specificity: The method is tailored for 3D point clouds limiting its generalizability to other types of data or domains. 2. Computational complexity: The paper mentions that solving additional optimization problems can be computationally burdensome raising concerns about scalability and efficiency in realworld applications. Overall the paper presents a valuable contribution to improving the generalization ability of deep learning models in 3D vision tasks through the novel SageMix technique. Addressing some of the weaknesses and limitations highlighted could further strengthen the impact and applicability of the proposed method.", "thgItcQrJ4y": " Peer Review  1. Summary: The paper investigates the phenomena of the Edge of Stability (EOS) in modern neural networks trained by fullbatch gradient descent. It divides the gradient descent trajectory into four phases based on changes in sharpness empirically identifying the norm of the output layer weight as an indicator of sharpness dynamics. The paper provides theoretical and empirical explanations for the dynamics of sharpness loss and output layer norm in each phase of EOS. The proof of the sharpness behavior in twolayer linear neural networks is discussed and related works in the field are highlighted.  2. Strengths and weaknesses: Strengths:  The paper effectively explores the Edge of Stability phenomena.  The concept of dividing the trajectory into four phases for analysis is innovative.  The theoretical and empirical explanations provided are insightful.  The paper contributes to understanding sharpness dynamics in neural networks. Weaknesses:  The theoretical assumptions made may be overly restrictive.  The paper lacks robustness in explaining the highfrequency oscillation of sharpness.  Further exploration on the impact of other network layers on sharpness is needed.  3. Questions:  Can the theory be extended to deeper neural networks with nonlinear activation functions  How generalizable are the assumptions made in the theoretical analysis  What are the implications of the findings on practical applications of deep learning  4. Limitations:  The paper heavily relies on assumptions that may not capture the full complexity of neural networks.  The observed empirical results need further validation and generalization.  The theoretical explanations may not fully encompass all aspects of sharpness dynamics. Overall the paper provides valuable insights into the Edge of Stability phenomena but further work is needed to generalize the findings and refine the theoretical underpinnings. The research adds to the understanding of optimization dynamics in deep learning but could benefit from broader exploration and robustness testing.", "rjbl59Qkf_": " Summary This paper explores the limitations of various approaches like importance weighting and distributionally robust optimization (DRO) in achieving robust generalization in the presence of distributional shift. The research shows that a class of methods termed Generalized Reweighting (GRW) algorithms including popular approaches like importance weighting and DRO variants have similar implicit biases to Empirical Risk Minimization (ERM) when training overparameterized models. Theoretical results are provided for both regression and classification tasks highlighting that GRW algorithms do not significantly improve over ERM. The paper concludes by discussing potential avenues to improve distributionally robust generalization and acknowledging the limitations of the study.  Strengths  The paper provides a comprehensive theoretical analysis of the limitations of GRW approaches in achieving distributionally robust generalization.  The research is wellstructured making it easy to follow the theoretical results and implications.  A wide range of relevant references is included to support the discussion and findings.  The experimental verification adds credibility to the theoretical results presented.  Weaknesses  The paper heavily relies on theoretical analysis and lacks a strong empirical evaluation to complement the theoretical findings.  Some assumptions made in the study such as focusing on linear models or wide neural networks may limit the generalizability of the results to more complex model architectures.  While potential future directions are discussed more concrete suggestions for practical application or experimentation could strengthen the paper.  Questions  Could the theoretical results be further validated or complemented through more extensive empirical evaluations especially on more complex models beyond linear or wide neural networks  How feasible would it be to implement the proposed approaches in realworld scenarios considering potential computational constraints or practical challenges  Are there any specific scenarios or applications where the findings of this study could have immediate practical implications  Limitations  The study focuses on theoretical analysis and lacks detailed empirical evaluation to validate the theoretical claims.  The assumptions made such as the model architectures considered may restrict the applicability of the findings in diverse realworld settings.  While potential future directions are suggested the immediate practical implementation of the proposed approaches would require further exploration and validation.", "q85GV4aSpt": " Peer Review of \"Theoretical Understanding of Square Loss in Training Deep Learning Classifiers\"  1) Summary: This paper explores the theoretical understanding of using square loss in training deep learning classifiers. The study investigates square loss performance in overparameterized neural networks and explores properties such as generalization error robustness and calibration error. The research systematically analyzes separable and nonseparable cases providing theoretical guarantees for robustness and model calibration. Empirical validation on synthetic and real datasets demonstrates the effectiveness of square loss in comparison to crossentropy.  2) Strengths and weaknesses: Strengths:  The paper provides a detailed theoretical analysis of square loss in training deep learning classifiers focusing on various aspects such as generalization error robustness and calibration error.  The study includes empirical evaluations on synthetic and real image data providing practical validation of the theoretical findings.  The research addresses a significant gap in the understanding of alternative loss functions in deep learning classifiers offering insights into the advantages of square loss over crossentropy. Weaknesses:  The paper focuses heavily on theoretical analysis which may make it challenging for readers without a strong statistical background to grasp the key concepts.  While the empirical studies support the theoretical findings a more extensive evaluation on a diverse set of datasets could enhance the generalizability of the results.  More discussion on the limitations and potential practical implications of the theoretical findings could further enrich the paper.  3) Questions:  Could you elaborate on how the theoretical analysis of square loss compares to existing research on alternative loss functions in deep learning classifiers  How do the results on synthetic lowdimensional data extend to more complex real image datasets in terms of model performance and robustness  Are there any assumptions made in the theoretical analysis that may limit the applicability of the findings to practical deep learning scenarios  4) Limitations:  The study primarily focuses on overparameterized neural networks in the NTK regime restricting the generalizability of the theoretical results to broader deep learning architectures.  The empirical evaluation while informative could be expanded to include a wider range of datasets and models to validate the effectiveness of square loss across different scenarios.  The paper does not extensively discuss the computational implications and efficiency of using square loss compared to crossentropy in practical deep learning applications. In conclusion the paper contributes valuable insights into the theoretical understanding of square loss in training deep learning classifiers. By bridging theory and empirical validation the study sheds light on the advantages of square loss and opens up avenues for further research in the field of deep learning classification.", "zSkYVeX7bC4": " Peer Review:  1) Summary The paper investigates length generalization capabilities of transformerbased language models particularly in algorithmic tasks such as parity and variable assignment. Naive finetuning shows deficiencies in length generalization but combining incontext learning with scratchpad prompting results in significant improvement. The study explores failure modes effectiveness of different techniques and interactions among them providing insights into enhancing length generalization abilities.  2) Strengths and Weaknesses Strengths:  Careful empirical studies uncovering length generalization capabilities.  Definition and characterization of the problem of length generalization.  Systematic analysis of different techniques (finetuning scratchpad fewshot prompting).  Identification of failure cases and significant improvements through incontext learning with scratchpad prompting. Weaknesses:  Lack of comparison with other types of models or approaches to length generalization.  Limited discussion on the broader implications or applications of the findings.  While the study is detailed and thorough deeper insights into the underlying mechanisms or reasons for specific results could enhance the paper.  3) Questions  How do the identified failure modes in length generalization relate to the current understanding of language model architectures  Are there any specific patterns observed in the responses of different transformer sizes during the length generalization tasks  Could different types of algorithms or tasks provide additional insights into the length generalization abilities of language models  4) Limitations  The study is limited to algorithmic tasks like parity and variable assignment. Generalizing the findings to a broader range of tasks could strengthen the conclusions.  The focus on transformerbased models might limit the applicability of the results to other types of models.  More detailed analyses or experiments on the reasons behind the failure of certain techniques for length generalization could further enhance the paper. In conclusion the paper provides valuable insights into the length generalization capabilities of transformerbased language models demonstrating the effectiveness of incontext learning with scratchpad prompting. Further exploration of underlying mechanisms and comparison with alternative approaches could deepen the understanding of length generalization in reasoning tasks.", "ySB7IbdseGC": " Peer Review:  1) Summary: The paper introduces a structured recognition framework for variational autoencoders specifically applied to Gaussian Process Factor Analysis (SRnlGPFA) in order to capture latent correlations induced by the \"explaining away\" effect. Through extensive experiments on synthetic and real datasets including neural spike data from hippocampal neurons the authors demonstrate the superior performance of SRnlGPFA over baseline methods. Results show that SRnlGPFA can identify latent signals that correlate with various behavioral covariates without direct supervision showcasing the models ability to capture complex latent structures.  2) Strengths and weaknesses: Strengths:  Novel Contribution: The introduction of the structured recognition framework for VAEs particularly applied to GPFA is a significant contribution that addresses the limitations of existing models by capturing latent correlations.  Extensive Experiments: The paper includes a thorough experimental evaluation covering synthetic data EEG datasets and neuronal firing data demonstrating the effectiveness of SRnlGPFA across various scenarios.  Demonstrated Performance: The results indicate that SRnlGPFA outperforms existing methods in terms of prediction accuracy latent space disentanglement and capturing behavioral correlations in neural activities. Weaknesses:  Complexity: The proposed SRnlGPFA model involves multiple layers of computational processes potentially making it challenging for implementation and interpretation.  Data Sources: While the paper demonstrates the effectiveness of SRnlGPFA on neural data it would be beneficial to include a broader range of datasets to test the models generalizability.  Additional Analysis: Some results are presented qualitatively which may benefit from more quantitative analysis to strengthen the conclusions.  3) Questions:  Scalability: How scalable is the SRnlGPFA model in realworld applications with larger datasets and what are the computational constraints associated with the model  Interpretability: Can the latent signals extracted by SRnlGPFA provide meaningful insights into the underlying structures of complex datasets beyond correlation with observed variables  Generalizability: How transferable is the structured recognition framework to other unsupervised learning tasks beyond GPFA and what are the key factors influencing its performance  4) Limitations:  The paper primarily focuses on the application of SRnlGPFA to neural data. In future work expanding the scope to diverse datasets and domains would enhance the generalizability and robustness of the proposed framework.  The complexity of the model architecture and inference process might pose practical challenges in realworld applications raising concerns about interpretability and computational efficiency. Overall the paper presents a novel structured recognition framework for VAEs applied to GPFA demonstrating significant performance improvements in capturing latent correlations in complex datasets. Further investigations on model generalization interpretability and scalability would enrich the impact and applicability of SRnlGPFA in diverse data analysis tasks.", "zVglD2W0EAS": " Peer Review  Summary The paper introduces DrugRec a causal inference based drug recommendation model that aims to address key factors in drug recommendation such as eliminating recommendation bias utilizing historical health conditions effectively and coordinating drug interactions. The model leverages a causal graphical representation to deconfound recommendation bias and extends the approach to multivisit scenarios. Additionally the paper introduces a method to model drugdrug interactions (DDIs) as a 2SAT problem for better coordination in drug recommendation. Experimental results on MIMICIII and MIMICIV datasets show that DrugRec performs significantly better than existing methods in terms of effectiveness metrics and DDI rates.  Strengths and Weaknesses Strengths: 1. Novel Approach: The paper introduces an innovative causal inference based drug recommendation model that addresses important factors in drug recommendation. 2. Deconfounding Bias: DrugRec effectively eliminates recommendation bias by leveraging causal inference techniques and frontdoor adjustment. 3. MultiVisit Incorporation: Extending the model to multivisit scenarios improves the accuracy of recommendations by considering historical health conditions. 4. 2SAT Method for DDIs: The proposal of modeling DDIs as a 2SAT problem is a clever approach for coordinating drug interactions. Weaknesses: 1. Data Sources: The evaluation is limited to MIMICIII and MIMICIV datasets potentially limiting the generalizability of the proposed model. 2. Complexity: The implementation of the model may be complex and computationally intensive which could impact scalability and realworld applicability. 3. Sensitivity to Parameters: The results provided are based on specific hyperparameters and may not generalize well to different settings. 4. Clarity: Some sections are highly technical and may require additional clarification for readers not wellversed in causal inference methods.  Questions 1. Data Processing: How robust is DrugRec to variations in data preprocessing methods especially the extraction of symptom information and the conversion of ICD codes to text descriptions 2. Scoring Network: Could more insights be provided on the choice of a twolayer MLP for the scoring network in DrugReca and DrugReck 3. Evaluation Metrics: Are there any additional metrics or analyses that could provide further insights into the performance and robustness of DrugRec 4. RealWorld Application: How feasible is the implementation of DrugRec in realworld clinical settings considering the computational complexity and data requirements  Limitations 1. Generalizability: The performance of DrugRec is evaluated on specific datasets and the results may not directly translate to other healthcare settings. 2. Resource Intensiveness: The computational demands of the proposed model are not extensively discussed which could be a limitation in realworld deployment scenarios. 3. Clinical Validation: The paper lacks discussion on the potential clinical validation or integration of DrugRec with existing healthcare systems which is critical for realworld adoption. Overall the paper presents a novel and promising approach to drug recommendation but further validation scalability considerations and realworld applicability need to be addressed in future work. The results show potential for significant impact in healthcare research provided the limitations are adequately addressed.", "wZk69kjy9_d": "Peer Review Summary: The paper introduces Director a hierarchical reinforcement learning method that learns hierarchical behaviors directly from pixels by planning in the latent space of a learned world model. Director demonstrates effectiveness in solving longhorizon tasks with sparse rewards and shows generality across a range of environments. The key contributions are a practical and interpretable algorithm for learning hierarchical behaviors within a world model trained from pixels successful performance on challenging sparse reward benchmarks and broad applicability in standard benchmarks. Strengths: 1. The paper addresses an important problem in reinforcement learning: handling longhorizon tasks with sparse rewards. 2. Director demonstrates impressive performance on challenging sparse reward benchmarks outperforming existing exploration methods. 3. The method is shown to be general and applicable across a wide range of traditional reinforcement learning environments. 4. The paper provides valuable insights into the decisionmaking process of Director through visualization of latent goals. 5. The research methodology is wellstructured and includes detailed explanations of the components of Director. Weaknesses: 1. While the paper discusses the limitations of current hierarchical reinforcement learning methods it could provide a more comprehensive comparison with existing methods in terms of performance metrics. 2. The paper could benefit from a more extensive discussion on the scalability of the proposed method to more complex and diverse environments. 3. The evaluation on a wider range of benchmarks could strengthen the generalizability of the findings. Questions: 1. Could the choice of hyperparameters significantly affect the performance of Director especially when applied to different environments 2. How sensitive is Director to changes in the agents dynamics or the environment structure 3. How does Director compare to other stateoftheart hierarchical reinforcement learning methods in terms of sample efficiency and computational cost Limitations: 1. The evaluation focuses heavily on sparse reward benchmarks potentially limiting the generalizability of the findings to other types of tasks. 2. The interpretability of the learned hierarchical behaviors could be further explored to provide deeper insights into the decisionmaking process of Director. 3. The complexity of the Director algorithm may pose a challenge in practical applications requiring careful tuning and extensive computational resources. In conclusion the paper presents an innovative hierarchical reinforcement learning method with promising results on sparse reward benchmarks. Further research on scalability robustness and interpretability could enhance the impact and applicability of Director in various realworld scenarios.", "mux7gn3g_3": " Peer Review:  1) Summary: The paper investigates metareinforcement learning (metaRL) approaches on visionbased benchmarks such as Procgen RLBench and Atari. The main focus is on comparing the performance of metalearning approaches with multitask pretraining followed by finetuning on novel tasks. The findings suggest that multitask pretraining with finetuning can perform equally or better than traditional metaRL approaches with a lower computational cost. The study advocates for evaluating future metaRL methods on more challenging tasks and considers multitask pretraining as a strong baseline.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important gap in the literature regarding the evaluation of metaRL on a diverse set of tasks.  The study is comprehensive covering three different benchmarks and comparing multiple metalearning algorithms.  The findings provide practical insights into the efficacy of multitask pretraining compared to traditional metaRL approaches.  The methodology and experiments are welldesigned and rigorously conducted. Weaknesses:  The study could benefit from more detailed analysis and discussion on the theoretical underpinnings of why multitask pretraining performs well.  The paper could further explore the limitations of multitask pretraining in scenarios where the tasks are more closely related.  The generalizability of the findings may be limited by the specific benchmarks and algorithms chosen for the study.  3) Questions:  Can the authors elaborate on the specific factors that contribute to the success of multitask pretraining in comparison to traditional metaRL approaches  How do the results of this study align with existing theoretical frameworks in metalearning and reinforcement learning  Are there plans to extend this research to explore the performance of multitask pretraining in other settings or with different benchmarks  4) Limitations:  The study has a limitation in terms of the number of available pretraining tasks especially for RLBench and Atari. How might this impact the generalizability of the findings  The paper focuses on visionbased benchmarks how applicable are the conclusions to other types of reinforcement learning tasks  The study compares multitask pretraining with existing metaRL algorithms but does not delve deeply into the interpretability of the learned representations. Overall the paper presents an important contribution to the field of metaRL by highlighting the effectiveness of multitask pretraining in visionbased environments. The findings have implications for future research directions and highlight the importance of evaluating different learning strategies on diverse and challenging tasks. Further work could explore the underlying mechanisms that contribute to the success of multitask pretraining and extend the study to different domains to validate the generalizability of the findings.", "tNXumks8yHv": " Peer Review: 1) Summary The paper introduces a unifying statistical framework based on the coupling of hidden graphs using cross entropy for dimension reduction methods with a focus on SNElike approaches. The framework links existing pairwise similarity DR methods to a generative probabilistic model revealing limitations in preserving global structure. The paper suggests a novel initialization method ccPCA to improve global structure preservation. 2) Strengths and Weaknesses Strengths:  The paper presents a comprehensive and novel probabilistic framework for understanding and unifying existing dimension reduction methods.  The theoretical analysis provided helps in elucidating the limitations of SNElike methods in conserving largescale dependencies.  The proposed ccPCA initialization method shows promise in addressing the deficiency in preserving global structure.  The empirical evaluation and comparison of ccPCA with PCA and Laplacian eigenmaps provide insights into the enhancement offered by ccPCA. Weaknesses:  The paper can benefit from more clarity in presenting the technical details especially in defining the terminology and concepts.  The experimental results could be further strengthened with a wider range of datasets and evaluation criteria.  Some sections lack coherence making it challenging to follow the logical flow of the methodology. 3) Questions  Can the proposed framework be extended to nonGaussian latent structures for more general applications  How does the computational complexity of ccPCA compare to traditional initialization methods like PCA  Are there plans to explore the impact of different graph priors on the performance of ccPCA in future research  What are the implications of using ccPCA in realworld applications especially in fields like image processing or natural language processing 4) Limitations  The paper focuses heavily on theoretical derivations and may need more emphasis on practical implications for endusers or practitioners.  The performance evaluation of ccPCA could be more comprehensive and include comparisons with additional stateoftheart methods.  The discussion on the computational efficiency of the proposed framework and initialization method could be more detailed. In conclusion the paper presents a significant contribution to the field of dimension reduction by providing a novel probabilistic framework and an innovative initialization method. Addressing the highlighted weaknesses and questions could further enhance the impact and practical applicability of the proposed methods.", "wYGIxXZ_sZx": "Peer Review 1) Summary This paper addresses the issue of generalization guarantees in minimax optimization in the stochastic setting. The authors propose a new metric the primal gap to study generalization of minimax learners aiming to overcome the shortcomings of existing metrics such as the primal risk and primaldual risk. Through theoretical analysis and examples they demonstrate the limitations of current metrics in complex nonconvex settings and establish generalization error bounds for the primal gap. Furthermore the paper compares the generalization behavior of two popular algorithms gradient descentascent (GDA) and gradient descentmax (GDMax) in stochastic minimax optimization. 2) Strengths and weaknesses Strengths:  The paper addresses an important and underexplored issue in the field of minimax optimization proposing a novel metric that can provide better insights into generalization behavior in nonconvex settings.  The theoretical analysis and examples presented help in understanding the limitations of existing metrics and the potential of the proposed primal gap metric.  The comparison of the generalization behavior of GDA and GDMax provides practical insights into the performance of different algorithms in stochastic minimax optimization.  The inclusion of assumptions Lemmas and Theorems enhances the rigor and clarity of the theoretical framework in the paper. Weaknesses:  The paper could benefit from more empirical validations of the proposed primal gap metric on realworld datasets and applications to complement the theoretical analysis.  The complexity of the mathematical derivations and assumptions may limit the accessibility and understanding of the paper to readers outside the specific domain of minimax optimization.  The implications and practical significance of the findings regarding GDA and GDMax could be further elaborated to provide actionable insights for practitioners. 3) Questions  Could the authors provide more insights into the practical implications of the findings for researchers and practitioners in machine learning and optimization  How would the proposed metric the primal gap perform in scenarios with highdimensional data or complex optimization landscapes and how generalizable are the results to such scenarios  Are there specific realworld applications or datasets where the proposed metric could be directly applied to showcase its effectiveness in improving generalization guarantees 4) Limitations  The paper focuses primarily on theoretical analysis and comparisons between algorithms potentially limiting its practical applicability without more extensive empirical validation.  The reliance on theoretical assumptions and mathematical derivations may limit the accessibility of the findings to a broader audience.  The generalization of the results from the presented examples to more diverse minimax optimization problems and applications needs to be further explored and validated. Overall the paper makes a significant contribution to the understanding of generalization in stochastic minimax optimization and introduces a novel metric that could advance the field. Further empirical validation and practical applications would enhance the impact and relevance of the proposed research.", "rP9xfRSF4F": " Peer Review:  1) Summary The paper introduces the concept of Optimally Timed Intervention (OTI) for critical events formulating it as an optimal stopping problem on a hazard rate process. The authors propose a Dynamic Deep Recurrent Survival Analysis (DDRSA) architecture for modeling hazard rate processes and demonstrate its effectiveness with experiments showcasing superior performance compared to traditional intervention methods. The paper presents a comprehensive theoretical framework along with practical implementation using Neural hazard rate processes.  2) Strengths and Weaknesses Strengths:  The paper addresses an important and practical problem of OTI for critical events which has significant implications in various fields.  The theoretical framework developed linking optimal stopping theory with hazard rate processes is novel and provides valuable insights.  The introduction of the DDRSA architecture for modeling neural hazard rate processes is innovative and shows promising results in experiments.  The manuscript is wellstructured clear and logically progresses from theory to implementation and experiments. Weaknesses:  The paper assumes Markovian property in hazard rate processes which might not always hold true in realworld scenarios.  The evaluation of the DDRSA architecture could have been further expanded with more comprehensive benchmarks and comparisons to other stateoftheart methods.  In the experiments section the paper could have included more detailed discussions on the results and the implications of the findings.  3) Questions  How does the proposed DDRSA architecture handle noisy or incomplete data in hazard rate processes  Can the Markovian assumption in hazard rate processes limit the applicability of the proposed approach in realworld scenarios with nonMarkovian behaviors  How scalable is the DDRSA architecture in handling largescale datasets and complex systems with diverse covariates  Have the authors considered any potential biases or limitations in the experimental design or dataset selection process  4) Limitations  The paper heavily relies on the Markovian assumption in hazard rate processes which might not always hold in complex realworld systems.  The proposed OTIDDRSARNN architecture may face challenges in interpretability and explainability due to the complexity of neural networks.  The experiments are conducted on a limited set of datasets potentially limiting the generalizability of the findings to other domains or applications.  Overall the paper presents a novel and intriguing approach towards addressing the critical problem of Optimally Timed Intervention for events providing a strong theoretical foundation and practical implementation using a cuttingedge neural network architecture. Further exploration and validation in diverse contexts could enhance the impact and applicability of the proposed methodology.", "vQzDYi4dPwM": " Peer Review:  1) Summary: The paper focuses on studying the interpolation problem for monotone data sets using monotone neural networks with nonnegative parameters and threshold units. It demonstrates that there are limitations to approximating monotone functions using monotone networks with ReLU activation but finds that for the threshold activation function universal approximation can be achieved at constant depth. The study presents results on expressiveness interpolation and efficiency of monotone neural networks compared to general networks. It also provides lower bounds and separation results between monotone and arbitrary networks contributing valuable insights into the field.  2) Strengths and Weaknesses: Strengths:  Rigorous Analysis: The paper provides a detailed and mathematically rigorous analysis of the interpolation problem for monotone data sets and the performance of monotone neural networks.  Novelty: The study addresses a gap in the literature by focusing on the specific challenges and advantages of monotone neural networks for approximating monotone functions.  Theoretical Contributions: The theoretical results presented including lower bounds and separation results between monotone and arbitrary networks add significant value to the field of neural network research. Weaknesses:  Empirical Validation: The study lacks empirical validation of the theoretical findings. Including experimental results or realworld applications could strengthen the practical relevance of the research.  Clarity: Certain sections of the paper especially those involving mathematical proofs may be challenging for readers without a strong background in neural networks or mathematical analysis. Improvements in clarity and accessibility could enhance the overall impact of the study.  3) Questions:  Generalization Properties: Have you considered exploring the generalization properties of monotone networks in terms of their ability to generalize to unseen data  GradientBased Learning: How might the findings change if gradientbased learning methods are utilized for training monotone networks with positive parameters  Extension to Other Activation Functions: Are there plans to extend the study to investigate the performance of monotone networks with other activation functions such as sigmoids and compare them to threshold activation networks  4) Limitations:  Theoretical Emphasis: The study is primarily theoretical in nature focusing on the mathematical aspects of monotone neural networks. Including practical applications or empirical validations could enhance the impact of the findings.  Complexity: The level of mathematical detail and complexity in some sections may limit the accessibility of the study to a broader audience. Efforts to simplify explanations could aid in reaching a wider readership. In conclusion the paper provides valuable insights into the challenges and advantages of utilizing monotone neural networks for approximating monotone functions. By addressing theoretical aspects and presenting novel results the study contributes significantly to the field. Improving clarity and considering practical applications could further enhance the impact of the research.", "lSfrwyww-FR": "Peer Review 1) Summary: The paper introduces a novel method Blackbox Attacks via Surrogate Ensemble Search (BASES) to perform effective blackbox attacks with a reduced number of queries. The method leverages a perturbation machine and a surrogate ensemble to generate successful attacks on various image classifiers with minimal queries. Extensive experiments demonstrate the effectiveness of BASES in achieving high success rates with significantly fewer queries compared to stateoftheart methods even on Google Cloud Vision API. The perturbations generated are highly transferable and can be adopted for hardlabel blackbox attacks. 2) Strengths:  The paper addresses the challenge of blackbox attacks with a unique and effective method BASES combining transfer and querybased approaches.  Extensive experiments utilizing a diverse set of models demonstrate the superior performance of BASES in achieving high fooling rates with minimal queries.  The proposed methods efficiency demonstrated by requiring as few as 3 queries per image for over 90 success rate in targeted attacks showcases its practical applicability.  The method is welldetailed and the pseudocode for both PM and BASES provides clarity on the proposed approach. Weaknesses:  Limited Scope of Evaluation: The experiments primarily focus on image classification tasks limiting the generalizability of the method to other types of tasks.  Evaluation against Fewer Methods: While the comparison with stateoftheart methods is informative including a broader set of baseline methods could strengthen the papers evaluation.  Computational Intensity: The paper mentions the computational expense of perturbation generation over a large ensemble size which could be a limiting factor in scalability. 4) Questions:  Generalizability: How transferrable is the proposed method to tasks beyond image classification and what are the potential challenges in extending BASES to diverse applications  Scalability: Given the computational cost of perturbation generation how scalable is the method to larger ensemble sizes or more complex models  Robustness: How robust is BASES against defenses such as adversarial training or detection mechanisms and are there known vulnerabilities that could be exploited in realworld applications 5) Limitations:  Ensemble Dependency: The methods reliance on a diverse ensemble for successful attacks could be a limitation in scenarios where obtaining such ensembles is challenging.  Computational Demand: The computational intensity of perturbation generation may limit the scalability of BASES to larger models or tasks with higher complexity. In conclusion the paper presents a promising method BASES for efficient blackbox attacks with impressive results and a novel approach. Addressing the limitations and questions raised could further enhance the impact and applicability of the proposed method in various practical scenarios.", "lTKXh991Ayv": " Review of the Scientific Paper: \"Adversarial Attacks on Spatiotemporal Traffic Forecasting Models\"  1) Summary (avg. word length 100) The paper investigates the vulnerability of spatiotemporal traffic forecasting models to adversarial attacks and proposes a practical framework for adversarial spatiotemporal attacks. The framework includes identifying victim nodes and generating realvalued adversarial traffic states. The study conducts extensive experiments on realworld datasets demonstrating the effectiveness of the proposed attacks in degrading the performance of spatiotemporal forecasting models and improving their robustness through adversarial training.  2) Strengths and weaknesses Strengths:  Extensive experimentation on realworld datasets provides practical insights.  The study addresses an important issue of adversarial attacks on spatiotemporal forecasting models.  The proposed framework is agnostic to model architectures and flexible in attack settings.  The theoretical analysis provides upper bounds on attack performance related to the budget constraints. Weaknesses:  Lack of comparison with more diverse baselines and stateoftheart defense mechanisms.  Limited discussion on the ethical implications of adversarial attacks on traffic systems.  Some parts of the methodology section are complex and may require additional clarifications for a broader audience.  3) Questions  Can the proposed adversarial attacks be extended to incorporate more complex networks or multimodal data sources  How do the adversarial attacks affect model interpretability and trustworthiness in realworld traffic forecasting scenarios  Are there any practical implications or strategies suggested for detecting and mitigating adversarial attacks in spatiotemporal traffic forecasting systems  4) Limitations  The study focuses mainly on the performance degradation caused by adversarial attacks without delving into potential realworld consequences or countermeasures.  The experiments may not completely capture the complexity and diversity of realworld traffic scenarios which could limit the generalizability of the findings.  The theoretical bounds presented might oversimplify the potential impact of adversarial attacks on traffic forecasting models. Overall the paper provides valuable insights into the vulnerability of spatiotemporal traffic forecasting models and offers a practical framework for adversarial attacks. Further exploration on the broader implications of these attacks and potential defense strategies could enhance the impact and applicability of the research.", "oMhmv3hLOF2": " Summary The paper introduces a novel method StreamRF for reconstructing streaming radiance fields to enable novel view synthesis of 3D videos in real time. The method utilizes explicit grid representations and an incremental learning framework to handle video sequences onthefly. By using incremental training narrow band tuning differencebased compression and pilot model guidance the proposed method achieves high training efficiency with competitive rendering quality compared to stateoftheart implicit methods.  Strengths 1. Incremental Learning Framework: The incremental learning paradigm enables onthefly handling of video sequences improving training efficiency significantly. 2. Narrow Band Tuning: The use of narrow bands helps capture model differences between adjacent frames enhancing training convergence. 3. Differencebased Compression: The compression strategy reduces storage overhead by storing model differences with a significantly smaller size compared to complete grid models. 4. Pilot Model Guidance: The optimization strategy inspired by curriculum learning to train from an easier problem further boosts training efficiency.  Weaknesses 1. Rendering Quality Artifacts: The paper acknowledges limitations in reconstructing highfrequency details and transparenttranslucent objects hinting at potential areas for improvement. 2. Lack of Realtime Acceleration: While the method achieves high training efficiency there is room for further accelerating the framework to support realtime training.  Questions 1. Algorithm Efficiency: How does the proposed method compare in terms of energy consumption and computational resources required compared to current implicit methods 2. Generalization: How well does the incremental learning framework generalize to different types of dynamic scenes and varying video characteristics 3. Dependence on Initial Model: How sensitive is the method to the quality of the base model established in the first frame during training 4. Realworld Application: How feasible would it be to implement this method in realworld scenarios such as live streaming or interactive applications  Limitations 1. Artifacts: While the method achieves high training efficiency there are still limitations in accurately reproducing highfrequency details and certain types of objects. 2. Realtime Training: Although the method emphasizes training efficiency it may require further enhancements to support realtime training for immediate video processing applications.", "uzqUp0GjKDu": " Summary The paper introduces unsupervised learning under Latent Label Shift (LLS) where class conditionals remain the same across domains while label marginals can shift. The proposed method DiscriminateDiscretizeFactorizeAdjust (DDFA) leverages domain structure to identify classes. Theoretical analysis establishes identifiability and experiments show DDFAs effectiveness compared to competitive unsupervised classification methods on datasets like CIFAR and FieldGuide.  Strengths and Weaknesses Strengths: 1. The paper introduces a novel approach to unsupervised learning under Latent Label Shift. 2. The theoretical analysis establishes identifiability and the connection to topic modeling providing a solid foundation. 3. DDFA outperforms competitive baselines in semisynthetic experiments demonstrating practical efficacy. 4. Application on realworld datasets like CIFAR and FieldGuide showcases the methods potential applicability. Weaknesses: 1. The complexity of the method may limit its accessibility to a wider audience. 2. The paper lacks clarity in presenting the connection between theoretical results and practical implementation. 3. The paper could benefit from more detailed discussions on the limitations of DDFA and possible future directions. 4. The scalability of DDFA to large realworld datasets needs to be further investigated.  Questions 1. Can you explain the impact of the finite input space assumption on the practical applicability or scalability of DDFA in realworld scenarios 2. How sensitive is DDFA to the choice of hyperparameters especially in complex highdimensional input spaces 3. Are there plans to extend DDFA to handle more complex scenarios beyond the current constraints of LLS such as presence of label noise or data scarcity 4. How do the results of DDFA on synthetic and realworld datasets relate to each other especially in terms of generalizability  Limitations 1. The synthetic nature of some experiments may not fully capture the complexity and variability seen in realworld datasets. 2. Datasetspecific characteristics and biases may influence the performance and generalizability of DDFA. 3. The reliance on specific assumptions like anchor set condition may limit the methods applicability to diverse realworld scenarios. 4. The comparative analysis mostly focuses on unsupervised methods exploring the performance against supervised or weakly supervised methods could provide a more comprehensive evaluation. Overall the paper presents a novel method DDFA with promising results in unsupervised classification under Latent Label Shift. Further clarification on practical implications scalability and extensions will enhance the impact and usability of DDFA in various machine learning applications.", "wSVEd3Ta42m": " Peer Review:  1) Summary: The paper addresses the problem of learning a risksensitive policy based on the CVaR risk measure using distributional reinforcement learning. It highlights the limitations of the standard actionselection strategy by showing that it may not converge to dynamic or static CVaR. The proposed modifications including a new distributional Bellman operator greatly enhance the utility of distributional RL for learning and representing CVaRoptimized policies. The proposed approach extends standard distributional RL algorithms to improve CVaR optimization on both synthetic and real data.  2) Strengths and Weaknesses: Strengths:  The paper identifies limitations in existing methods and proposes a solution that improves learning and representation of CVaRoptimized policies using distributional RL.  The proposed modifications such as the new distributional Bellman operator are theoretically sound and empirically demonstrated to work well on synthetic and realworld data.  The paper utilizes various examples and experiments to illustrate the effectiveness of the proposed approach across different scenarios. Weaknesses:  While the proposed approach shows promising results the general convergence properties of the T\\xcc\\x83\\xcf\\x88 operator in distributional RL settings remain unknown which could be a potential limitation.  The empirical results while comprehensive could benefit from a more extensive evaluation across a wider range of tasks and environments to further validate the algorithms performance.  3) Questions:  How does the proposed approach compare to other risksensitive RL methods in terms of computational efficiency and scalability especially when dealing with largescale problems  Are there specific scenarios or environments where the proposed algorithm may not perform as effectively and if so what are the limitations in those cases  Have you considered incorporating more complex risk measures or multiple risk objectives to further enhance the adaptability and applicability of the proposed approach  4) Limitations:  The paper focuses on CVaR optimization using distributional RL and while the proposed algorithm shows promise it would be beneficial to explore its performance compared to other risksensitive objectives and methods to provide a more comprehensive understanding of its capabilities.  The experimental evaluation although diverse could be expanded to include additional benchmarks and tasks to provide a more thorough analysis of the algorithms robustness and generalizability. Overall the paper presents a valuable contribution to the field of risksensitive RL by addressing an important limitation in existing methods. Further investigations into the convergence properties comparative analysis with other methods and broader empirical evaluations could enhance the impact and understanding of the proposed approach.", "slKVqAflN5": " Peer Review 1. Summary: The paper extends the work of Zhang Lin Jegelka Sra and Jadbabaie on minimizing Lipschitz functions by addressing limitations associated with a nonstandard oracle model. The authors propose algorithms for efficient minimization of Lipschitz functions with and without weak convexity assumptions aimed at achieving improved efficiency in low dimensions. The contributions include modifications to the existing algorithm to eliminate the need for a nonstandard oracle and the introduction of a cuttingplane algorithm for enhanced efficiency in low dimensions. The theoretical results and algorithms are presented with detailed mathematical formulations and proofs. 2. Strengths and weaknesses: Strengths:  The theoretical foundation of the paper is strong with rigorous mathematical formulations and proofs.  The paper addresses important limitations of existing algorithms for minimizing Lipschitz functions.  The proposed algorithms offer efficiency improvements for Lipschitz functions with and without weak convexity assumptions.  The paper contributes to the optimization literature by presenting innovative modifications to existing algorithms. Weaknesses:  The paper is highly theoretical and may be challenging for readers not wellversed in optimization theory.  The complexity of the presented algorithms may limit practical applicability without further optimization for realworld use cases.  The manuscript could benefit from additional discussion on the practical implications and potential applications of the proposed algorithms. 3. Questions:  How do the proposed algorithms compare in terms of convergence rates and efficiency with other stateoftheart optimization methods for Lipschitz functions  Are there realworld applications or datasets where the proposed algorithms have been tested to demonstrate their practical benefits  Can the proposed algorithms be extended to handle nonconvex optimization problems beyond Lipschitz functions 4. Limitations:  The paper primarily focuses on theoretical developments and may lack empirical validation to showcase the practical performance of the proposed algorithms.  The complexity associated with the algorithms may limit their adoption in realworld applications without further simplification or optimization.  The applicability of the proposed algorithms to diverse optimization scenarios beyond Lipschitz functions needs further exploration. In conclusion the paper makes significant contributions to the optimization literature by addressing limitations in the minimization of Lipschitz functions and introducing novel algorithms for enhanced efficiency. Theoretical rigor and innovation mark the primary strengths of the work but practical applicability and empirical validation could be further emphasized to strengthen the impact of the research.", "tq_J_MqB3UB": " Peer Review:  1. Summary: The paper introduces Diverse Weight Averaging (DiWA) a weight averaging strategy designed to enhance the functional diversity across averaged models for improved generalization under distribution shifts in computer vision. The paper presents a new theoretical analysis focused on bias variance covariance and locality highlighting the effectiveness of weight averaging in outofdistribution (OOD) scenarios. Experimental results show the superiority of DiWA over existing methods on the DomainBed benchmark. The approach leverages diverse models obtained from independent training runs to enhance OOD performance without added inference overhead.  2. Strengths and Weaknesses: Strengths:  The paper addresses an important challenge in computer vision and proposes a novel approach DiWA to improve generalization under distribution shifts.  The theoretical analysis provides valuable insights into the effectiveness of weight averaging strategies in handling OOD scenarios.  Empirical results demonstrate the superior performance of DiWA on the DomainBed benchmark compared to existing methods establishing it as stateoftheart.  The approach of averaging weights from independently trained models increases diversity and leads to improved OOD performance without additional inference cost. Weaknesses:  While the theoretical analysis is thorough and insightful the complexity of the presented equations might pose challenges for readers not wellversed in the specific domain.  The paper could benefit from additional clarity in explaining the experimental setup and results to facilitate better understanding and interpretation of the findings.  The limitations of DiWA are briefly discussed further exploration of potential drawbacks or scenarios where the approach may not be applicable could provide a more comprehensive evaluation of the method.  4. Questions:  How does the proposed DiWA approach handle cases where the diversity among independently trained models is not significant Are there strategies to ensure diversity in such scenarios  Can the authors elaborate on the computational overhead associated with training multiple models independently and averaging their weights Are there scalability concerns when applying DiWA to larger datasets or more complex neural network architectures  The paper mentions the importance of shared initialization and mild hyperparameter ranges for DiWAs effectiveness. How sensitive is the approach to variations in these initialization settings and are there specific conditions where DiWA performs suboptimally  In practical applications how transferable is the DiWA approach to other domains or datasets beyond the DomainBed benchmark Are there considerations for adapting DiWA to different types of computer vision tasks or data distributions  5. Limitations:  The theoretical analysis is extensive and may be challenging for readers without a strong background in mathematical models and neural network training concepts.  The experimental validation while promising focuses primarily on a specific benchmark (DomainBed) and could benefit from broader testing on a wider range of datasets and scenarios to assess the generalizability and robustness of DiWA.  The paper briefly addresses limitations but could delve deeper into scenarios where DiWA may not be as effective or where its performance might degrade under certain conditions providing a more comprehensive assessment of the approach. Overall the paper presents a novel and wellthoughtout approach to improving generalization under distribution shifts in computer vision with both theoretical and empirical evidence supporting the effectiveness of DiWA. Further clarification and exploration of the approachs limitations scalability and potential extensions could enhance the impact of the work and its applicability in realworld scenarios.", "nQcc_muJyFB": " Summary The paper investigates the importance of the feature projector in knowledge distillation and proposes a new method based on a projector ensemble to improve feature distillation. The study demonstrates that adding a projector between the student and teacher networks can enhance distillation performance even when their feature dimensionalities are identical. The proposed method outperforms existing feature distillation techniques on various datasets and teacherstudent pairs.  Strengths 1. The paper offers a comprehensive review of existing knowledge distillation methods and categorizes them based on their use of projectors. 2. The study provides empirical evidence showing the positive impact of projectors on feature distillation even with equivalent feature dimensions. 3. The introduction of a projector ensemble for further performance improvement is a novel and valuable contribution. 4. Comparative experiments on different datasets and teacherstudent pairs demonstrate the effectiveness of the proposed method over stateoftheart techniques.  Weaknesses 1. The paper lacks a detailed discussion on the computational complexity and scalability of the proposed projector ensemble method. 2. The explanation of the projector ensemble methodology could be more detailed for better understanding. 3. The empirical results could be strengthened by providing a deeper analysis of the impact of varying hyperparameters on performance.  Questions 1. How do the results of the proposed method compare to stateoftheart distillation methods when considering training time and computational resources 2. Are there any specific criteria or guidelines suggested for selecting the number of projectors in the ensemble to achieve optimal performance 3. Could the paper provide more insights into the interpretability of the feature distillation process using projectors and how it impacts the final classification decisions  Limitations 1. The study primarily focuses on image classification tasks limiting the generalizability of the proposed method to other machine learning applications. 2. The lack of indepth analysis on the impact of varying hyperparameters and projector initialization methods may limit the reproducibility and robustness of the findings. 3. The paper does not address potential challenges or limitations associated with scaling the proposed method to larger or more complex datasets. Overall the paper presents a valuable contribution by highlighting the significance of feature projectors in knowledge distillation and introducing an ensemblebased approach for enhanced feature distillation performance. Further improvements in discussing computational aspects method scalability and interpretability could enhance the overall impact of the study.", "rhdfTOiXBng": " Summary The paper introduces NATURALPROVER a language model designed for generating proofs in mathematics by utilizing background references. NATURALPROVER shows improvements over finetuned GPT3 for suggesting next steps in mathematical proofs and generating full proofs on the NATURALPROOFS dataset. Human evaluations from mathematics students indicate that NATURALPROVER can provide correct and useful nextstep suggestions about 40 of the time and produce short proofs effectively. The models performance in proof correctness and usefulness demonstrates its potential utility in interactive mathematics assistance.  Strengths and Weaknesses Strengths:  NATURALPROVER shows significant improvement over finetuned GPT3 in suggesting next steps and generating proofs in mathematics.  The model demonstrates the capability to generate correct proofs and provide useful suggestions enhancing its utility as an interactive mathematics aid.  Grounding proofs with humanprovided or retrieved references boosts proof correctness and utility.  The paper provides detailed experiments and evaluations including human annotations and automatic metric comparisons to assess the models performance. Weaknesses:  Challenges remain in the logical coherence of longer proofs validity of justifications and multistep symbolic derivations particularly as proof complexity increases.  While the model performs well in short proofs it struggles with reasoning involving references and equations highlighting areas for improvement.  Reference reconstruction objective and constrained decoding are essential for improved performances suggesting the need for a more robust framework.  Questions 1. How does NATURALPROVER handle situations where there may be multiple correct next steps or proofs for a given theorem Does the model have a method to explore these variations 2. Can the model generalize well to unfamiliar theorems or domains not covered in the training dataset How does it adapt to new information 3. How does the retrieval of references impact the overall performance of NATURALPROVER compared to humanprovided references Are there specific scenarios where one outperforms the other 4. Are there plans to expand the models capabilities to handle more complex proofs multistep derivations or diverse mathematical domains beyond what was explored in the current study  Limitations  The performance evaluation heavily relies on human annotations which can introduce subjectivity and potential biases in assessing correctness and usefulness.  The study focuses on short proofs (26 steps) and may not fully capture the models scalability to longer and more complex proofs.  While the model demonstrates promising results there are still challenges in ensuring the logical coherence of proofs especially in longer and more intricate mathematical reasoning. The paper provides valuable insights into the potential of language models like NATURALPROVER for theorem proving in natural language but further research is needed to address the identified weaknesses and explore the models scalability and adaptability in diverse mathematical contexts.", "wsnMW0c_Au": " Summary: The paper explores the equivalence between nonconvex gradient descent and convex mirror descent through a reparameterization technique. It focuses on regret minimization in online nonconvex optimization showing that online gradient descent for nonconvex functions approximates online mirror descent for convex functions after reparameterization under certain conditions. The research provides an O(T23) regret bound for nonconvex online gradient descent answering an open problem. The analysis is based on a novel algorithmic equivalence method.  Strengths: 1. Novelty and Contribution: The paper presents a new algorithmic equivalence method to study the convergence of nonconvex gradient descent filling a gap in the understanding of optimization algorithms. 2. Theoretical Analysis: The research offers a thorough theoretical analysis addressing open questions and providing precise bounds for the considered algorithms. 3. Generalization: The results extend to arbitrary Lipschitz convex loss functions making them applicable across a wide range of scenarios. 4. Rigorous Methodology: The research provides detailed proofs Lemmas and theorems enhancing the credibility of the findings.  Weaknesses: 1. Complexity: The technical nature of the paper and the reliance on advanced mathematical concepts may limit the accessibility to a broader audience. 2. Limited Empirical Validation: While the theoretical analysis is robust empirical validation or practical application of the proposed method could strengthen the research. 3. Future Directions: The paper opens up additional questions for future research without providing concrete steps or insights on addressing them leaving room for ambiguity.  Questions: 1. Can the results of the paper be practically applied in realworld scenarios particularly in machine learning optimization tasks 2. How does the O(T23) regret bound compare to existing bounds in the literature for similar algorithms 3. Are there practical considerations or limitations that may affect the implementation of the proposed algorithmic equivalence method in realworld optimization problems  Limitations: 1. Theoretical Focus: The paper is heavily theoretical lacking substantial empirical validation or practical demonstrations of the proposed technique in applied settings. 2. Complexity of Analysis: The advanced mathematical derivations and assumptions involved might limit the generalizability of the findings to a broader audience or practical implementations. 3. Scope and Generalization: The results are limited to specific geometric and smoothness conditions and the applicability to more general scenarios needs further exploration.", "s7SukMH7ie9": " Summary This paper introduces a novel approach of adversarial training (AT) with complementary labels (CLs) which provide partial label information by specifying a class that a data sample does not belong to. The study explores the challenges of combining AT with CLs and proposes a new learning strategy using gradually informative attacks to address the issues of intractable adversarial optimization and lowquality adversarial examples. Extensive experiments are conducted on benchmark datasets to demonstrate the effectiveness of the proposed method.  Strengths 1. Innovative Approach: The paper introduces a novel concept of AT with complementary labels addressing a gap in existing literature. 2. Theoretical Analysis: The paper provides a theoretical foundation for understanding the challenges of AT with CLs. 3. Empirical Verification: Extensive experiments are conducted to validate the proposed method on multiple datasets providing solid evidence of its effectiveness. 4. Detailed Methodology: The proposed learning strategy using gradually informative attacks is well articulated and supported by a flexible scheduler for controlling the dynamics.  Weaknesses 1. Complexity: The paper introduces a new approach with multiple components that may be challenging to implement and replicate for researchers unfamiliar with adversarial training. 2. Limited Comparison: The performance comparison with existing methods and baselines is somewhat limited which could provide a stronger benchmark for evaluating the proposed approach. 3. Technical Details: Some technical details could be further elaborated for better clarity and reproducibility. 4. Generalization: The generalization of the proposed method to different datasets and scenarios could be further discussed for a broader applicability.  Questions 1. How does the proposed method compare to stateoftheart AT techniques with ordinary labels in terms of adversarial robustness and model performance 2. Can the proposed learning strategy with gradually informative attacks be extended to more complex scenarios such as multiple CLs or noisy labels 3. How does the proposed method perform on datasets with higher dimensionality and more complex class distributions 4. Are there any computational or training efficiency considerations associated with implementing the proposed approach in practical scenarios  Limitations 1. Scope of Study: The focus on a specific setting of AT with CLs may limit the generalizability of the proposed method to diverse weakly supervised learning scenarios. 2. Resource Requirements: The experimental setup and the proposed approach may require significant computational resources and extensive hyperparameter tuning. 3. Evaluation Metrics: The evaluation metrics used in the experiments could be further expanded to include a wider range of adversarial attack scenarios for a comprehensive analysis. 4. Dataset Diversity: Although multiple datasets are considered in the experiments a broader range of datasets covering different domains could enhance the studys robustness. Overall the paper presents an innovative approach to adversarial training with complementary labels supported by theoretical analysis and empirical validation. Further exploration comparative analysis and discussions on scalability and generalizability could enhance the impact of the proposed method in the field of weakly supervised learning and adversarial training.", "kOIaB1hzaLe": " Peer Review  1) Summary The paper introduces a new framework NREC for likelihoodtoevidence ratio estimation in simulationbased inference (SBI) tasks addressing biases inherent in existing frameworks NREA and NREB. The proposed framework allows for more reliable diagnostics and performance improvements by discouraging bias terms. Through thorough experiments in various training regimes and hyperparameter settings the authors demonstrate the effectiveness of NREC compared to existing methods. The paper also introduces a novel metric based on the mutual information bound for model performance evaluation in SBI tasks.  2) Strengths and Weaknesses Strengths:  The paper addresses an important issue in likelihoodtoevidence ratio estimation in SBI tasks and proposes a novel framework.  The thorough experimental evaluation across different training regimes and hyperparameters provides comprehensive insights into the effectiveness of the proposed framework.  The introduction of a new metric based on the mutual information bound for model evaluation is a significant contribution.  The use of simulations and benchmarks adds credibility to the findings and conclusions. Weaknesses:  The paper could benefit from a more concise and clearer presentation of the technical details which are at times overly complex and may be challenging for readers to follow.  While the experiments are extensive the conclusions drawn from them could be more explicitly linked back to the theoretical contributions of the framework.  The paper could provide more context and discussion regarding the practical applications and implications of the proposed framework in realworld SBI tasks.  3) Questions  Can you elaborate on the specific scenarios or domains where the proposed NREC framework would offer the most significant advantages over existing methods NREA and NREB  How do the authors foresee the proposed metric based on the mutual information bound being practically implemented and adopted by researchers in the field of SBI  Are there any plans for future work to further refine or extend the NREC framework potentially addressing any limitations or extending its applicability to other types of SBI tasks  4) Limitations  The paper does not explicitly discuss potential limitations or challenges that users may face when implementing the proposed NREC framework in realworld SBI tasks.  While the experiments cover a range of scenarios it would be beneficial to provide more insights into how the proposed framework may perform under specific challenging conditions or edge cases in SBI tasks. Overall the paper presents a novel framework for likelihoodtoevidence ratio estimation in SBI tasks and provides valuable insights through thorough experiments. Addressing the highlighted weaknesses and questions would further enhance the clarity and applicability of the proposed framework in practical settings.", "vmjckXzRXmh": " Peer Review: 1) Summary: The paper investigates the phenomenon of linear transferability of contrastive representations in unsupervised domain adaptation scenarios. By analyzing the relationships between clusters across domains the authors propose a linear classifier based on average representations within classes for effective transfer. The theoretical analysis is supported by novel assumptions and algorithmic approaches. Empirical experiments on BREEDS dataset demonstrate the effectiveness of the proposed method in outperforming traditional linear probing. 2) Strengths:  The paper provides insightful analysis on contrastive learning and domain adaptation pushing the boundaries of current understanding in representation learning.  The proposed linear transferability concept and algorithmic approach offer a practical solution for unsupervised domain adaptation problems.  The paper combines theoretical analyses with empirical evaluations enhancing the reliability and applicability of the findings. 3) Weaknesses:  The theoretical framework and assumptions used in the study might be complex for readers not wellversed in the field of unsupervised domain adaptation and contrastive learning.  The empirical evaluations while positive could benefit from more comprehensive comparisons with existing stateoftheart methods in domain adaptation. 4) Questions:  How do the proposed assumptions in the paper compare with other works in the domain adaptation literature Are these assumptions practical and generalizable to various scenarios  Could the authors elaborate more on the limitations of the proposed linear transferability concept in realworld applications 5) Limitations:  The detailed technical nature of the paper might limit its accessibility to a wider audience particularly researchers new to the field of contrastive learning and domain adaptation.  The empirical evaluation on BREEDS dataset while promising might lack diversity in experimental setups and datasets to establish the generalizability of the proposed algorithm. Overall the paper presents significant advancements in the understanding and application of contrastive learning in unsupervised domain adaptation. Addressing the aforementioned questions and limitations could further strengthen the impact and applicability of the proposed concepts in realworld scenarios.", "rApvGord7j": " Peer Review: 1. Summary: The paper investigates fair machine learning under predictive parity focusing on the conditions under which fair Bayesoptimal classifiers are groupwise thresholding rules. The authors propose the FairBayesDPP algorithm to ensure predictive parity demonstrate its efficacy through theoretical proofs and experiments on both synthetic and empirical data. The study emphasizes the importance of considering fairness measures carefully to prevent unintended consequences. 2. Strengths:  Novel Contribution: The papers focus on predictive parity and sufficiencybased measures fills a gap in existing literature predominantly centered on independence and separationbased fairness metrics.  Theoretical Analysis: The work provides rigorous theoretical proofs and conditions under which fair Bayesoptimal classifiers align with groupwise thresholding rules enhancing the understanding of sufficiencybased fairness measures.  Algorithm Design: The development of the FairBayesDPP algorithm is a practical and adaptive approach to achieving predictive parity while maintaining a high level of accuracy as validated through experiments.  Empirical Experiments: Both synthetic and empirical data experiments demonstrate the effectiveness of the proposed approach in mitigating bias and preserving model accuracy across diverse datasets. 3. Weaknesses:  Complexity and Interpretability: The papers dense theoretical content might be challenging for readers unfamiliar with the intricacies of fairness measures in machine learning potentially hindering comprehension.  Generalizability: The empirical experiments focus on specific datasets and scenarios limiting the generalizability of the results. Including more diverse datasets and benchmarks could strengthen the external validity of the proposed algorithm.  Discussion of Ethical Implications: While discussing fairness in machine learning the paper could benefit from a more explicit discussion on the ethical implications of bias mitigation and the consideration of various stakeholder perspectives. 4. Questions:  Algorithm Robustness: How does the FairBayesDPP algorithm perform under varying degrees of data skewness or imbalance in protected attributes  Scalability: Has the scalability of the proposed algorithm been explored especially when dealing with largescale datasets or complex feature spaces  FairnessUtility Tradeoff: How does the tradeoff between fairness and utility (accuracy) vary with different fairness constraints and conditions Are there scenarios where sacrificing fairness leads to significant gains in model performance 5. Limitations:  Generalization: The proposed approachs generalizability to realworld applications beyond the studied datasets and contexts may be limited.  Bias in Data and Labeling: Despite the effective mitigation of algorithmic bias realworld datasets often contain inherent biases that may not be completely addressed solely through algorithmic interventions.  Interpretability: The FairBayesDPP algorithms interpretability in terms of explaining decisions and fairnessenhancing processes may require further consideration for deployment in sensitive domains. Overall the paper presents a valuable contribution to the field of fair machine learning offering insights into sufficiencybased fairness measures and proposing an adaptive algorithm for predictive parity enforcement. The empirical validation and theoretical underpinnings strengthen the credibility of the proposed approach opening avenues for further research and practical applications in fair algorithm design.", "ofRmFwBvvXh": "Peer Review: 1) Summary: The paper introduces novel approximation error bounds for Sobolevregular functions using deep convolutional neural networks (CNNs). It presents nonasymptotic excess risk bounds for classification using CNNs addressing the curse of dimensionality by considering input data supported on lowdimensional manifolds. The study explores approximation properties of CNNs in rigorous mathematical formulations providing valuable insights into their performance in classification tasks. 2) Strengths:  The paper contributes significantly by deriving new approximation error bounds explicitly tied to network and model parameters impacting the understanding of CNNs performance.  The discussion on the application of findings to classification methods with various convex loss functions highlights practical implications in machine learning.  The consideration of lowdimensional manifold assumptions showcases the potential of CNNs in circumventing the curse of dimensionality enhancing the applicability of theoretical results in realworld scenarios. Weaknesses:  The paper focuses solely on binary classification problems limiting the generalizability of the findings to other classification and regression tasks. Including a broader range of applications would enhance the relevance of the study.  While the theoretical analysis is robust the practical applicability of the results to diverse datasets and realworld scenarios warrants further validation and empirical testing.  The paper could benefit from more explicit discussions on the limitations and assumptions underlying the theoretical framework providing clarity on the scope and applicability of the proposed methods. 4) Questions:  How do the proposed approximation error bounds compare to existing methods in terms of computational efficiency and practical implementation  Can the findings be extended to multiclass classification problems or regression tasks and if so how would the results differ or evolve in such scenarios  Considering the assumption of data supported on lowdimensional manifolds what implications do these findings have for realworld datasets with complex structural patterns beyond lowdimensional approximations 5) Limitations:  The theoretical focus on specific loss functions and binary classification may restrict the generalizability of the results requiring validation across a broader range of applications and problem domains.  The assumption of data supported on approximate lowdimensional manifolds may not fully capture the complexity of realworld datasets necessitating further investigation into diverse structural patterns and invariances present in different types of data. More robust validation on diverse datasets could provide deeper insights into CNN performance. Overall the paper presents a rigorous theoretical analysis of CNNs approximation properties and their application in classification tasks. Addressing the identified weaknesses and limitations could enhance the impact and practical relevance of the study in the field of deep learning and machine learning research.", "yewD_qbYifc": " Peer Review 1. Summary The paper introduces Priority Convention Reinforcement Learning (PCRL) to address challenges in tackling large discrete action spaces in multiagent systems. By embedding agreed priority conventions PCRL facilitates optimal action selection by considering symmetries and tiebreaking strategies. The effectiveness of PCRL is supported by experiments on multiagent complete coverage planning and cooperative pong showcasing its abilities in dealing with large action spaces. The paper provides theoretical explanations detailed methodology and experimental results to validate the proposed approach. 2. Strengths and Weaknesses Strengths:  The paper presents a novel approach PCRL for handling multiagent systems with large discrete action spaces offering a systematic solution to symmetries and tiebreaking.  The experiments conducted on multiagent complete coverage planning and cooperative pong demonstrate the effectiveness of PCRL compared to alternative methods.  The theoretical backing and detailed methodology provide a comprehensive understanding of PCRLs workings and benefits. Weaknesses:  While the experimental results are promising the comparison to more stateoftheart RL techniques beyond baseline methods like jointaction DQN could further strengthen the evaluation.  The paper lacks a comparative analysis of the computational efficiency and scalability of PCRL in realworld applications with varying agent numbers and complexities. 3. Questions  Could the proposed PCRL approach be further optimized or adapted for more challenging and dynamic multiagent scenarios such as those with changing environments or adaptive opponents  How robust is PCRL to noisy or incomplete observations in the multiagent environment and are there strategies to enhance its performance under such conditions  Can the neural network architectures used in PCRL be finetuned or specialized for specific applications to maximize efficiency and accuracy in action selection 4. Limitations  The experimental evaluations though positive might benefit from more diverse and complex scenarios to showcase the versatility and generalizability of PCRL.  The focus on deterministic transition environments in some experiments might limit the applicability of PCRL to more realworld stochastic or dynamic scenarios.  The scalability of PCRL to extremely large multiagent systems with highdimensional action spaces could be a potential limitation not fully explored in the current work. Overall the paper presents a compelling approach with theoretical grounding and practical validation. Further enhancements in experimentation and scalability considerations could strengthen the impact of PCRL in the broad space of multiagent reinforcement learning research.", "utahaTbcHdP": " Peer Review of the Scientific Paper  1) Summary: The paper investigates the phenomenon of Neural Collapse in deep neural networks (deepnets) and explores how the geometry of classifiers and embeddings changes with class imbalances. The authors introduce the SimplexEncodedLabels Interpolation (SELI) geometry as an invariant characterization of the Neural Collapse phenomenon and prove its properties theoretically. Through extensive experiments on synthetic and real datasets the authors confirm the convergence to the SELI geometry highlighting that convergence is affected by increasing imbalances. The paper concludes with implications for understanding optimization landscapes in deepnets especially in the context of class imbalances.  2) Strengths and Weaknesses:  Strengths:  The paper addresses an important phenomenon (Neural Collapse) in deep neural networks and investigates its behavior in the presence of class imbalances.  The introduction of the SELI geometry as an invariant characterization of the Neural Collapse phenomenon is novel.  The theoretical proofs and extensive experiments provide a comprehensive exploration of the topic.  The paper makes a valuable contribution to understanding optimization landscapes in deepnets particularly in the context of class imbalances.  Weaknesses:  The paper might benefit from providing clearer explanations and visual aids to help readers understand complex theoretical concepts and proofs.  Some sections are densely packed with technical details which could potentially make it challenging for readers unfamiliar with the topic to follow.  3) Questions:  How do the findings of this study impact the development of deep learning models for realworld applications with imbalanced datasets  Can the insights gained from the SELI geometry be utilized to improve model generalization particularly in scenarios with class imbalances  Are there plans to extend the research to explore the implications of the findings on the performance of deep learning models in practical applications  4) Limitations:  The paper primarily focuses on theoretical explanations and experimental validations. It might be beneficial to provide more realworld use cases or applications where the SELI geometry could be applied.  The complexity of the mathematical formulations and proofs could make it challenging for readers without a strong background in deep learning theory to grasp the significance of the findings. Overall the paper presents a thorough investigation into the Neural Collapse phenomenon and its behavior with class imbalances. The novel introduction of the SELI geometry as an invariant characterization adds valuable insights to the field of deep learning optimization. Further clarity in explanations and potential realworld applications could enhance the impact and accessibility of the research.", "m16lH6XJsbb": "Peer Review 1) Summary: The paper presents an innovative approach to imagebased reinforcement learning by introducing Spectrum Random Masking (SRM) augmentation method from a frequency domain perspective. The authors argue based on two principles for augmentations in RL and propose SRM as a novel method to enhance the robustness of agents facing diverse distribution shifts. Extensive experiments on DMControl Generalization Benchmark demonstrate that SRM achieves stateoftheart performance with strong generalization potentials. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important challenge in reinforcement learning i.e. generalization to unseen environments and proposes a novel augmentation method to improve this aspect.  The introduction of SRM from a frequency domain perspective is a unique and insightful contribution.  Comprehensive experiments conducted on DMControl Generalization Benchmark support the effectiveness of SRM and show stateoftheart performance.  The paper is wellstructured providing clear explanations of concepts and methodologies. Weaknesses:  The paper could benefit from more detailed explanations in certain sections especially regarding technical details such as parameter tuning and hyperparameters.  The paper could provide more clarity on the limitations of the proposed method and future directions for research. 3) Questions:  Can the authors provide more insights into how the SRM method can be extended or adapted for more complex or specific RL tasks  How does SRM perform when the observations are incomplete or when only state information is available  Could the authors elaborate on the potential computational overhead introduced by the SRM method during training 4) Limitations:  The paper acknowledges the challenge that the proposed method may face when image observations are incomplete or when only state information is available. This limitation could impact the generalizability of the proposed method in certain scenarios.  The paper could address the potential computational complexity introduced by SRM especially in largescale RL tasks. In conclusion the paper presents a novel approach to imagebased reinforcement learning that addresses the issue of generalization to unseen environments. The proposed SRM method from a frequency domain perspective shows promising results in enhancing robustness and performance. By addressing the limitations and potential future research directions the paper could further strengthen its contributions to the field of RL.", "kgT6D7Z4Xv9": " Peer Review:  1) Summary: The paper investigates the concept of path independence in equilibrium models focusing on their ability to achieve strong upwards generalization by exploiting scalable testtime compute. Utilizing a metric called the Asymptotic Alignment (AA) score to quantify path independence the study demonstrates strong correlations between path independence and accuracy on a perinstance basis as well as upwards generalization performance. Experimental interventions promoting or penalizing path independence are shown to have corresponding effects on generalization. The findings suggest that path independent equilibrium models can effectively utilize additional testtime iterations to achieve higher accuracy showcasing promising potential for building scalable learning systems.  2) Strengths and Weaknesses: Strengths:  The paper introduces an innovative concept of path independence in equilibrium models and emphasizes its role in achieving strong upwards generalization.  The study provides a comprehensive analysis of path independence using the AA score metric showcasing its correlations with accuracy and generalization across various tasks.  Experimental interventions to promote or penalize path independence offer valuable insights into the causal relationship between path independence and generalization performance.  The research findings have significant implications for designing neural network architectures capable of utilizing additional testtime compute effectively. Weaknesses:  Clarifications could be provided on the implications of the concept of path independence for practical applications and realworld use cases.  The paper could benefit from more detailed discussions on the limitations and potential challenges associated with measuring and promoting path independence in equilibrium models.  The manuscript could be improved by including a discussion on the implications of the studys findings for future research directions and applications in the field of deep learning.  3) Questions:  How do the findings of the study contribute to the existing literature on neural network architectures and their ability to generalize to harder instances  What are the potential implications of path independence for improving the scalability and efficiency of deep learning systems in practical applications  Could the authors elaborate further on the practical implications of promoting or penalizing path independence in equilibrium models for different types of tasks and problem domains  4) Limitations:  The study mainly focuses on theoretical discussions and experimental validations within the context of equilibrium models which may limit the generalizability of the findings to other neural network architectures.  The paper could benefit from a more indepth exploration of the broader implications of path independence for enhancing generalization and efficiency in deep learning applications beyond the tasks discussed in the study. Overall the paper presents a novel concept of path independence in equilibrium models and provides valuable insights into its role in achieving strong upwards generalization. Further discussions on the practical implications limitations and future directions of the research could enhance the impact and relevance of the study in the field of deep learning.", "xpR25Tsem9C": " Peer Review: 1) Summary The paper proposes HHVAEM a Hierarchical Variational Autoencoder (VAE) model for handling mixedtype incomplete data utilizing Hamiltonian Monte Carlo (HMC) with automatic hyperparameter optimization for improved approximate inference. The paper presents experiments demonstrating the superior performance of HHVAEM compared to existing baselines in tasks such as missing data imputation and supervised learning with missing features. Additionally a samplingbased approach is introduced for efficiently computing information gain for active feature acquisition. The paper addresses limitations of existing VAEs and demonstrates the effectiveness of the proposed HHVAEM model through quantitative and qualitative experiments. 2) Strengths and Weaknesses Strengths:  The paper addresses an important problem of handling mixedtype incomplete data by proposing a novel hierarchical VAE model.  The utilization of Hamiltonian Monte Carlo (HMC) with automatic hyperparameter tuning for improved approximate inference is a significant contribution.  Extensive experiments on various datasets demonstrate the superior performance of HHVAEM over existing baselines in missing data imputation and supervised learning tasks.  The proposed samplingbased method for information gain computation provides a novel approach for active feature acquisition. Weaknesses:  The paper lacks a detailed discussion on the scalability and computational efficiency of the proposed HHVAEM model especially for largescale datasets.  The limitations and challenges of implementing the proposed model in realworld scenarios are not extensively discussed.  The paper could benefit from a more comprehensive comparison with stateoftheart methods in terms of different metrics such as computation time scalability and robustness to varying data types. 3) Questions  How does the proposed HHVAEM model handle scalability issues when dealing with largescale datasets and complex data structures  Can you elaborate more on the practical implications and challenges of implementing the HHVAEM model in realworld applications  How sensitive is the performance of HHVAEM to variations in hyperparameters especially when applied to diverse datasets with varying characteristics 4) Limitations  The experiments conducted are primarily focused on artificial and benchmark datasets which may not fully represent the complexity of realworld datasets and applications.  The study may benefit from more indepth evaluations on the robustness of HHVAEM under noisy or sparse data conditions.  The proposed method heavily relies on Gaussian approximations which may limit its effectiveness in scenarios where the underlying data distribution deviates significantly from Gaussian assumptions. In conclusion the paper presents a novel approach HHVAEM for handling mixedtype incomplete data with improved approximate inference using Hamiltonian Monte Carlo. While the experimental results showcase the promising performance of HHVAEM in various tasks further exploration and validation on more diverse and challenging datasets would be valuable to establish the models practical utility in realworld applications.", "wZEfHUM5ri": "Peer Review 1) Summary: The paper introduces a novel selfsupervised pretraining task Crossview Completion (CroCo) tailored for 3D vision and lowerlevel geometric downstream tasks. By leveraging pairs of images showing the same scene from different viewpoints the model is trained to predict masked regions in one image using information from a second image. The experimental results demonstrate that CroCo outperforms existing Masked Image Modeling (MIM) models on monocular 3D vision tasks like depth estimation and also achieves competitive results on binocular tasks such as optical flow and relative camera pose estimation. 2) Strengths and weaknesses: Strengths:  Novelty: Introducing CroCo as a selfsupervised pretraining task tailored for 3D vision tasks adds a valuable contribution to the field.  Experimental Results: The results show significant performance improvements on monocular 3D vision tasks compared to existing MIM models.  Applicability: CroCo demonstrates competitive performance on binocular tasks without requiring taskspecific design modifications.  Model Architecture: Detailed explanation of the encoder and decoder components training methodology and pretraining data provide a comprehensive understanding of the proposed approach. Weaknesses:  Lack of HighLevel Semantic Task Performance: The paper acknowledges limitations in highlevel semantic tasks compared to established MIM models suggesting a need for future exploration in more objectcentric datasets.  Dependency on Synthetic Data: The reliance on synthetic renderings for training data raises questions about realworld applicability and generalization.  Performance Comparison: While CroCo outperforms existing models on 3D vision tasks the paper lacks a more extensive comparison with stateoftheart methods for a comprehensive evaluation. 3) Questions:  Could the adaptation of the CroCo model to realworld image pairs potentially impact performance on downstream tasks compared to synthetic data  How does CroCo handle variations in scene content and lighting conditions when transferring to realworld scenarios  Could the models performance on highlevel semantic tasks be improved by incorporating additional taskspecific training or datasets 4) Limitations:  Dataset Dependency: The reliance on synthetic image pairs might limit the models ability to generalize to diverse realworld scenarios with varying content and environments.  TaskSpecific Adaptation: CroCos performance in highlevel semantic tasks suggests the need for additional adaptations or dataset choices to enhance its efficiency.  Transferability: While CroCo shows promise in 3D vision tasks further evaluation with additional benchmark datasets and realworld scenarios could provide a clearer understanding of its generalization capabilities. Overall the paper presents an innovative approach in selfsupervised pretraining for 3D vision tasks showcasing notable performance improvements in specialized tasks. Addressing the identified limitations and expanding the scope of evaluation could further enhance the practical application and understanding of the CroCo model.", "ue4gP8ZKiWb": " Summary The paper introduces a new algorithm Prompt Certified Machine Unlearning (PCMU) that addresses the inefficiency issue of traditional machine unlearning methods. PCMU performs simultaneous training and unlearning operations without the need to retrain models from scratch. The algorithm leverages randomized data smoothing and gradient quantization to achieve certified machine unlearning. The paper includes theoretical derivations and practical frameworks for improving efficiency and performance in handling machine unlearning requests.  Strengths 1. Innovative Approach: PCMU introduces a novel method for efficient machine unlearning by combining randomized smoothing and gradient quantization. 2. Theoretical Foundation: The paper includes detailed theoretical derivations to support the proposed algorithm and provides insights into the certified radius and budget of data removals. 3. Empirical Evaluation: Empirical evaluation on real datasets demonstrates the superior performance of PCMU compared to existing methods showcasing the practical applicability of the proposed algorithm. 4. Ablation Study: The ablation study compares variants of the PCMU model providing insights into the effectiveness of each component. 5. Efficiency: PCMU is shown to be computationally efficient and effective for handling machine unlearning requests in a timely manner.  Weaknesses 1. Complexity: The paper contains highly technical content making it challenging for those unfamiliar with the domain to comprehend. 2. Limited Generalizability: The empirical evaluation is focused on image classification datasets limiting the generalizability of the findings to other domains. 3. Algorithm Details: Some algorithmic details could be further clarified to enhance understanding especially for readers not wellversed in machine unlearning techniques.  Questions 1. How does PCMU handle scenarios where the data removal requests are frequent and continuous 2. Are there specific requirements or assumptions regarding the data distribution that could impact the performance of PCMU 3. Can PCMU be applied to other types of machine learning tasks beyond image classification 4. What are the implications for deploying PCMU in realworld systems considering factors like scalability and integration with existing machine learning pipelines  Limitations 1. Limited Scope: The paper focuses on machine unlearning for image classification tasks limiting the exploration of PCMU applicability to a wider range of machine learning applications. 2. Evaluation Metrics: The evaluation mainly focuses on accuracy and errors and additional metrics related to model robustness or stability could provide a more comprehensive assessment of PCMU performance. 3. Scalability: While the paper mentions efficiency gains further exploration of scalability challenges and solutions in largescale deployment scenarios could provide additional insights.", "vRrFVHxFiXJ": " Peer Review  1) Summary The paper introduces chemCPA a novel encoderdecoder architecture aimed at studying perturbational effects of unseen drugs on single cells by leveraging bulk RNA HTS datasets for transfer learning. The model outperforms existing methods in predicting unobserved drugcovariate combinations demonstrating improved generalization performance. Through experiments on sciPlex3 and L1000 datasets the authors show the effectiveness of chemCPA in singlecell responses for both shared and extended gene sets. Additionally the proposed uncertainty measure provides insights into the models generalization ability to unseen drugs.  2) Strengths and Weaknesses Strengths:  The paper addresses an important challenge in singlecell transcriptomics by proposing a method to predict cellular responses to unseen drugs.  The introduction of chemCPA and the incorporation of transfer learning from bulk RNA HTS datasets showcase novel contributions to the field.  The experimental evaluation on both shared and extended gene sets demonstrates the models performance superiority over existing methods.  The uncertainty measurement provides valuable insights into the model\u2019s generalization ability. Weaknesses:  While the proposed method is innovative additional experimental validation on diverse datasets and scenarios would further strengthen its credibility.  The limitation of the proposal to unseen compounds needs to be carefully addressed to ensure the robustness of predictions in realworld applications.  More detailed discussion on potential biases or limitations in the approach and results should be included.  3) Questions  How does the performance of chemCPA compare to existing methods when applied to larger more diverse datasets beyond sciPlex3 and L1000  Can the uncertainty measure be further validated on external datasets to assess its consistency and reliability in predicting generalization ability  Are there specific drug classes or molecular characteristics where chemCPA demonstrates superior performance or limitations  4) Limitations  The paper lacks extensive external validation on diverse datasets which could provide a more comprehensive evaluation of chemCPA\u2019s applicability across different experimental setups.  The focus on unseen compounds introduces uncertainties that need to be thoroughly investigated and discussed to ensure the model\u2019s reliability in drug discovery applications.  The study may benefit from including a more indepth analysis of potential biases assumptions or biases in the proposed approach. Overall this paper presents a valuable contribution to the field of singlecell transcriptomics by introducing chemCPA a novel model for predicting cellular responses to unseen drugs. Further validation and exploration of the proposed method on varied datasets and realworld scenarios could enhance its impact and practical relevance in drug discovery applications.", "lXUp6skJ7r": "Peer Review: Summary: The paper introduces an adversarial style augmentation approach (AdvStyle) to address domain generalization in semantic segmentation. AdvStyle dynamically generates stylized images during training improving the models robustness to style variations in unseen domains. The proposed method is compared with existing augmentation techniques on synthetictoreal benchmarks and achieves stateoftheart results. Additionally AdvStyle is shown to enhance image classification performance in a single domain generalized setting. Strengths: 1. Novel Contribution: The proposed AdvStyle method introduces a unique approach to adversarial style augmentation for domain generalization demonstrating significant improvements in semantic segmentation and image classification tasks. 2. Experimental Results: The paper provides extensive experimental evaluations on synthetictoreal benchmarks and single domain generalized image classification showcasing the effectiveness of AdvStyle in improving model performance. 3. Model Agnostic: AdvStyle is shown to be easily implementable across different models without requiring modifications to network structures enhancing its applicability and versatility. 4. Thorough Analysis: The paper includes indepth discussions on the impact of style features comparison with existing methods parameter analysis and visualization of style features providing comprehensive insights into the proposed approach. Weaknesses: 1. Theoretical Support: The paper could benefit from additional theoretical explanations or analyses to support the effectiveness of AdvStyle explaining how the adversarial style augmentation contributes to improved domain generalization. 2. Comparative Benchmarking: While comparisons with existing methods are provided benchmarking against a broader range of stateoftheart techniques in diverse scenarios could strengthen the validity and generalizability of the results. 3. Complexity: The complexity of the proposed method may hinder its adoption in practical applications and further discussions on computational efficiency and scalability could be beneficial. 4. Visualization: While tSNE visualization of style features is informative additional qualitative and visual comparisons of segmentation results with ground truth annotations could enhance the manuscript. Questions: 1. How sensitive is the AdvStyle method to hyperparameters such as the learning rate for adversarial style learning Are there any tradeoffs or challenges in selecting optimal values for these parameters 2. Considering the potential practical applications are there any computational constraints when implementing AdvStyle on realworld semantic segmentation tasks with large datasets 3. How does AdvStyle compare when applied to more complex models or datasets with higher levels of variability in style features and domain shifts 4. Are there any theoretical insights or future research directions suggested by the experimental findings that could further enhance the understanding and application of adversarial style augmentation in domain generalization Limitations: 1. Experimental Settings: While the experiments demonstrate the effectiveness of AdvStyle the limitations of using synthetic datasets and specific benchmarks could restrict the generalizability of the findings to realworld scenarios. 2. Scalability: The scalability of AdvStyle to diverse datasets and complex scenarios may require further investigation to understand its adaptability and performance under varying conditions. 3. Interpretability: Further insights into the interpretability of model decisions and the impact of AdvStyle on semantic segmentation results could provide a deeper understanding of the methods mechanisms. Overall the paper presents a novel approach to domain generalization with promising results providing valuable insights into the role of style features in enhancing model robustness. Strengthening the theoretical foundation expanding comparative analyses and addressing potential scalability and interpretability issues could further elevate the impact and applicability of AdvStyle in semantic segmentation and image classification tasks.", "mUeMOdJ2IJp": " Peer Review: 1) Summary: The paper focuses on recovering linear subspaces in a Federated Learning setting using Principal Component Analysis (PCA) while addressing irregular noise. The researchers propose an estimator that leverages multiple data points from each user to recover the subspace efficiently without restrictive assumptions on noise distributions. They analyze the upper bound error of the estimator proving it matches an informationtheoretic lower bound even under nonspherical noise. Additionally they extend their findings to linear regression problems. The methodology and results are detailed in three main sections: introduction PCA analysis and linear models analysis. 2) Strengths and weaknesses: Strengths:  The paper addresses an important problem in Federated Learning with welldefined methodology and rigorous analysis.  The researchers provide clear explanations of their techniques including the design of the estimator and the error bounds.  The approach of leveraging multiple data points per user to evade informationtheoretic impossibility is a novel and effective strategy.  The work shows promising results in overcoming challenges posed by nonspherical and userdependent noise distributions. Weaknesses:  The paper could benefit from more discussion on the practical implications of the findings and potential realworld applications.  Some sections are highly technical potentially limiting accessibility for readers not wellversed in the topic.  The limitations of the proposed estimator in more complex scenarios or with larger datasets could be further explored. 3) Questions:  Can you elaborate on how the proposed estimator performs in scenarios with a large number of users or data points per user  How robust is the estimator to variations in the level of noise or changes in the underlying data distributions  Have you considered extending your analysis to more diverse noise models beyond the subGaussian assumption 4) Limitations:  The generalizability of the results to practical Federated Learning scenarios with varying levels of heterogeneity should be further investigated.  The assumption of subGaussian noise may not always align with realworld data raising concerns about the estimators applicability in diverse settings.  A comprehensive evaluation on the scalability and computational efficiency of the proposed estimator in largescale Federated Learning scenarios is warranted. Overall the paper presents a valuable contribution to the field of statistics and machine learning addressing a critical problem in Federated Learning settings. The methodology analysis and results are robust but further exploration of practical implications scalability and generalizability is necessary.", "qk1qpCN-k6": " Summary The paper explores the concept of overfitting in machine learning by employing information theory specifically the information bottleneck method. It quantifies overfitting through residual and relevant information and studies the tradeoff between these two aspects in linear regression and randomized ridge regression. The research sheds light on the information complexity of learning a linear map in high dimensions and reveals an informationtheoretic phenomenon similar to multiple descent. The study provides insights into the fundamental properties of optimal algorithms and Gibbs posterior regression showcasing the efficacy of various learning strategies in extracting information efficiently.  Strengths and Weaknesses Strengths: 1. The paper takes a novel approach by applying information theory to the study of overfitting offering a fresh perspective on the issue. 2. The research methodically analyzes the tradeoff between residual and relevant information providing a deeper understanding of machine learning algorithms. 3. The comparison between optimal algorithms and Gibbs regression provides valuable insights into information efficiency and the impact of regularization. 4. The exploration of highdimensional learning and the effects of anisotropy on learning strategies are significant contributions to the field. Weaknesses: 1. The paper is highly technical and may pose challenges for readers not wellversed in information theory and machine learning. 2. The study mainly focuses on linear regression limiting the generalizability of the findings to more complex machine learning models. 3. The use of extensive mathematical formulations might make it difficult for practitioners to extract practical implications from the research. 4. The analysis might benefit from empirical validation to demonstrate the realworld applicability of the theoretical results.  Questions 1. How do the findings of the study translate to more complex machine learning models beyond linear regression 2. Can the results be extended to practical applications and if so what are the implications for improving the generalization performance of neural networks and deep learning models 3. Are there key assumptions made in the study that deserve further scrutiny particularly when applying the proposed methods to realworld datasets 4. How do the identified tradeoffs between residual and relevant information inform the design of machine learning algorithms and how can these insights be practically implemented in industry settings  Limitations 1. The paper primarily focuses on theoretical analyses and simulations lacking empirical validation using realworld datasets. 2. The findings are limited to linear regression models potentially restricting the applicability of the results to more diverse machine learning scenarios. 3. The technical complexity of the study might hinder the broad dissemination and understanding of the information presented. 4. The studys reliance on certain assumptions may limit the direct transferability of the results to diverse machine learning contexts and industry applications.", "mq-8p5pUnEX": "Peer Review 1) Summary The paper proposes a novel architecture that combines the strengths of Transformers and recurrent neural networks to learn temporally compressed representations. By introducing a temporal latent bottleneck the model divides computation into fast and slow streams improving sample efficiency and generalization performance. The approach is evaluated across various domains showing superior performance compared to competitive baselines. Notably the model demonstrates effectiveness in tasks requiring longrange dependencies and sequential decision making. 2) Strengths and weaknesses Strengths:  The proposed model addresses the challenge of learning temporally compressed representations effectively.  The division of computation into fast and slow streams along with the introduction of a temporal latent bottleneck leads to improved sample efficiency and generalization.  The experiments across vision reinforcement learning and natural language tasks demonstrate consistent outperformance of competitive baselines.  The study undertakes a comprehensive evaluation in various scenarios and domains showcasing the versatility and effectiveness of the proposed approach. Weaknesses:  The paper could provide more insights into the interpretability of the model and the latent representations learned.  Lack of comparison with more recent stateoftheart models in specific domains could limit the comprehensive evaluation of the proposed approach. 3) Questions  How does the proposed model handle noisy inputs or variations in the dataset for tasks like image classification or sequential decision making  Can the model adapt to dynamic environments or changing data distributions without retraining  What are the implications of the fixed chunk size on the models scalability and performance across different tasks or datasets  Is there a tradeoff between the models sample efficiency and its computational complexity in realworld applications 4) Limitations  The fixed chunk size may limit the models adaptability to varying sequence lengths in different tasks.  The lack of extensive comparison with recent stateoftheart models in specific domains may restrict the evaluation and benchmarking of the proposed approach.  The generalization of the proposed model beyond the studied domains and tasks warrants further investigation. Overall the paper presents a promising architecture that effectively leverages the benefits of both Transformers and recurrent neural networks. The experimental results showcase the potential of the proposed approach in enhancing sample efficiency and generalization performance across a diverse set of tasks. Further exploration and validation in realworld applications and comparisons with cuttingedge models could strengthen the findings of the study.", "o4uFFg9_TpV": "Peer Review: 1) Summary: The paper introduces the concept of visual prompting inspired by NLP prompting techniques utilizing image inpainting to adapt pretrained models to new downstream tasks without finetuning. A curated dataset from academic papers is used for training masked autoencoders showing promising results across various imagetoimage tasks. The experiments demonstrate the effectiveness of visual prompting and the importance of the Figures dataset in improving model performance. 2) Strengths:  The concept of visual prompting is novel and shows potential for adapting pretrained models to diverse tasks without additional training.  Clear explanation of the proposed method dataset generation process model architectures and downstream tasks evaluation.  Comprehensive experiments showcasing the effectiveness of visual prompting across different computer vision tasks.  Indepth analysis of the results exploring the impact of dataset choice number of examples and prompt engineering on model performance. Weaknesses:  While the paper extensively discusses the results obtained a more comparative analysis against baseline methods or related works could provide a better understanding of the methods performance.  There is limited discussion on the limitations of the proposed method apart from dataset dependency and ambiguous task definitions.  The models efficiency and scalability with larger datasets or more complex tasks are not thoroughly addressed. 3) Questions:  How does the performance of the proposed visual prompting method compare to traditional finetuning approaches across a wide range of computer vision tasks  Are there plans to explore the scalability and generalization of the visual prompting method on larger and more diverse datasets beyond the Figures dataset  Can the model be extended to handle more complex imagetoimage translation tasks beyond the ones considered in the paper 5) Limitations:  The reliance on a specific dataset (Figures) for training could limit the generalizability of the method to a broader range of tasks and datasets.  The effectiveness of the method may diminish when faced with more complex or varied tasks requiring finer details or context understanding.  The models interpretability in handling diverse inputoutput examples and queries while maintaining consistency needs further investigation. Overall the paper presents an intriguing concept of visual prompting leveraging image inpainting for adaptability in computer vision tasks. Further exploration into comparative evaluations scalability and addressing the limitations could enhance the impact and applicability of the proposed method in realworld scenarios.", "rYkGxHPnCIf": " Peer Review:  1) Summary: This paper introduces a method called nonequilibrium importance sampling (NEIS) to compute expectations with respect to complex highdimensional distributions with unknown normalization constants. The method involves generating samples from a simple base distribution transporting them with a velocity field and performing averages along the flowlines. The paper presents theoretical analysis of NEIS discusses methods to tailor the velocity field to the target distribution and establishes conditions for a zerovariance estimator. The computational aspect involves using deep learning to represent the velocity field using neural networks and training them to optimize the estimator. Numerical experiments demonstrate significant variance reduction compared to traditional methods like annealed importance sampling (AIS).  2) Strengths and Weaknesses:  Strengths:  The paper presents a novel method (NEIS) for computing expectations in highdimensional distributions.  The theoretical analysis is thorough establishing conditions for zerovariance estimators.  Use of deep learning to optimize the velocity field is a significant and innovative approach.  The numerical experiments demonstrate substantial improvements in variance reduction compared to existing methods.  The topic is timely and relevant to computational sciences and statistical inference.  Weaknesses:  The theoretical aspects could be challenging for readers not wellversed in the subject matter. More explanation or intuitive insights may aid in understanding.  The detailed mathematical formulations and derivations might be overwhelming for some readers particularly those without a strong background in mathematics.  The practical implications and realworld applications of NEIS could be further elaborated to enhance the papers impact and relevance.  3) Questions:  Could the paper provide more insights into potential realworld applications where NEIS could have a significant impact  How does the performance of NEIS compare to other stateoftheart methods beyond AIS in terms of computational efficiency and accuracy  Are there any limitations or challenges encountered in applying NEIS to more complex or largerscale problems and how might these be addressed  4) Limitations:  The paper could benefit from additional intuitive explanations or examples to aid in understanding the complex theoretical concepts presented.  The focus on relatively lowdimensional benchmark examples (up to 10 dimensions) may limit the generalizability of the method to higher dimensions or more complex distributions.  Consideration of different types of target distributions and scenarios could provide a more comprehensive evaluation of NEISs performance and applicability. Overall the paper presents a significant contribution to the field of computational sciences and statistical inference with the development of NEIS. The theoretical foundation computational approach using deep learning and promising numerical results make it a valuable addition to the research landscape. To enhance its impact providing more accessible explanations expanding the discussion on realworld applications and exploring diverse scenarios would be beneficial.", "w0O3F4cTNfG": " Peer Review  1. Summary: The paper delves into the problem of causal discovery in systems plagued by measurement error and unobserved latent causes within linear structural equation models (SEMs). By establishing a mapping between SEMs with measurement error (SEMME) and SEMs with unobserved roots (SEMUR) the study presents identifiability results and proposes structure learning algorithms for both models. The main contributions include demonstrating the connection between the two models characterizing identifiability under a twopart faithfulness assumption and introducing the concepts of ancestral ordered grouping (AOG) and direct ordered grouping (DOG) for structural identifiability.  2. Strengths and weaknesses: Strengths:  The paper offers a novel perspective by linking SEMME and SEMUR presenting a unique mapping between the models.  Theoretical results provide insights into identifiability under different faithfulness assumptions.  The introduction of AOG and DOG concepts for structural identifiability enriches the methodology proposed.  Detailed formulations proofs and descriptions of models enhance the understanding and robustness of the findings. Weaknesses:  The paper might benefit from more explicit explanations in certain sections for easier comprehension by readers.  While the theoretical contributions are substantial practical implications and applicability of the proposed algorithms could be discussed in more depth.  Limitations of the research methodology or assumptions made could be further elaborated to provide a comprehensive view.  3. Questions:  How do the proposed methods and findings compare with existing causal discovery algorithms especially in realworld applications with complex data  Are there scenarios where the assumptions made in the study might not hold and how would such cases impact the results and applicability of the proposed algorithms  Can the mapping between SEMME and SEMUR models be extended to nonlinear systems and what challenges might arise in such extensions  4. Limitations:  Applicability: While the theoretical results are robust the practical applicability and generalizability of the proposed structure learning algorithms to realworld datasets with diverse structures need to be validated.  Assumption Sensitivity: The study heavily relies on assumptions such as separability and faithfulness and deviations from these assumptions might affect identifiability and algorithm performance.  Model Complexity: Addressing the complexity of realworld systems with nonlinear relationships and multiple latent variables remains a challenge not directly tackled in the current scope of the research.  Overall Evaluation: The paper contributes significantly to the field of causal discovery by exploring connections between SEMME and SEMUR models and providing identifiability results under specific assumptions. Theoretical foundations are strong but further validation and application of the proposed algorithms in practical settings are essential for full comprehension of their impact and efficacy in realworld scenarios. Additional discussions on limitations extensions to nonlinear models and empirical validations would enhance the robustness and practical utility of the research findings.", "nZRTRevUO-": " Peer Review of the Paper \"Bayesian optimization over the latent spaces of deep autoencoder models with LOLBO\"  1) Summary: The paper proposes a novel approach called LOLBO for improving Bayesian optimization in highdimensional latent spaces of deep autoencoder models (DAEs). The key innovation lies in adapting trust regions concepts to improve local optimization in structured input spaces. LOLBO is evaluated across six realworld benchmarks and shows up to a 20x improvement over existing methods. The paper highlights the importance of joint design of latent space modeling and local Bayesian optimization strategies.  2) Strengths and Weaknesses: Strengths:  Novel Contribution: The proposed LOLBO method addresses the challenges of optimization in highdimensional latent spaces and demonstrates significant improvements over stateoftheart methods.  Empirical Results: The empirical evaluation on realworld benchmark tasks showcases the effectiveness of LOLBO compared to existing approaches.  Integration of Trust Regions: By incorporating trust regions in the structured setting LOLBO aligns local optimization in latent space with input space which is a valuable contribution to improving optimization strategies. Weaknesses:  Theoretical Explanation: The paper could benefit from a more indepth theoretical explanation of the trust region concept and its adaptation to highdimensional latent spaces for a better understanding.  Comparative Analysis: While the empirical results are promising a more detailed comparison with other stateoftheart methods in terms of computational efficiency scalability and robustness would strengthen the paper.  Reproducibility: It would be helpful to provide more details on the implementation of LOLBO hyperparameters selection and the datasets used to facilitate reproducibility by other researchers.  3) Questions:  Generalization: How do you ensure the generalizability of the LOLBO method across different types of optimization tasks beyond the ones considered in the study  Computational Efficiency: Could you provide insights into the computational cost and scalability of LOLBO especially when dealing with largescale optimization problems  Impact of Hyperparameters: How sensitive is the performance of LOLBO to hyperparameter choices and how were these parameters optimized during experimentation  4) Limitations:  Sample Efficiency: While LOLBO shows improvements in optimization performance the paper does not delve into the sample efficiency of the method. Analyzing how LOLBO performs in lowbudget scenarios would provide a more comprehensive evaluation.  Evaluation Metrics: The paper primarily focuses on objective values as the main metric for comparison. Including additional metrics related to convergence speed robustness or diversity of solutions could offer a more holistic assessment. Overall the paper introduces a promising method in the field of Bayesian optimization over highdimensional latent spaces. Further clarification on theoretical underpinnings additional comparative analysis and an emphasis on reproducibility would enhance the impact and credibility of the presented research.", "omI5hgwgrsa": " Peer Review:  Summary: The paper focuses on decentralized stochastic variational inequalities over fixed and timevarying networks. Lower complexity bounds are presented for communication and local iterations along with optimal algorithms. The algorithms outperform existing literature in decentralized stochastic deterministic and nondistributed stochastic cases. Experimental results confirm the effectiveness of the presented algorithms.  Strengths: 1. The paper addresses an important and increasingly relevant area of decentralized settings for variational inequalities. 2. The work presents lower complexity bounds and optimal algorithms for decentralized stochastic variational inequalities over fixed and timevarying networks. 3. Theoretical contributions include introducing optimal algorithms that surpass existing methods in decentralized settings. 4. The experimental results provide practical validation of the proposed algorithms effectiveness.  Weaknesses: 1. The paper lacks a clear overview of the experimental methodology including dataset details metrics and evaluation criteria used in performance comparisons. 2. While the theoretical contributions are welldetailed the practical implications and realworld applicability could be elaborated further. 3. The paper could benefit from a more comprehensive discussion on how the proposed algorithms can be applied to specific machine learning problems or realworld applications. 4. Detailed explanations of key terms such as variance reduction decentralized methods and specific algorithms would enhance the readability for readers less familiar with the subject matter.  Questions: 1. Can you provide more context on the scalability of the proposed algorithms in terms of handling larger datasets or more complex network topologies 2. How do the proposed algorithms perform in scenarios with noisy data or highdimensional feature spaces 3. Have you considered the impact of different network topologies on the convergence and efficiency of the algorithms 4. Can you discuss any tradeoffs or limitations of the proposed algorithms in terms of computational resources or convergence speed  Limitations: 1. The paper may benefit from providing more extensive explanations and visual aids to enhance reader understanding especially for those less familiar with decentralized variational inequalities. 2. The experimental evaluation could be strengthened by providing more comprehensive details on dataset characteristics and benchmarking against a wider range of existing methods. 3. It would be valuable to explore the sensitivity of the proposed algorithms to hyperparameters and variations in problem settings. 4. Further discussion on the implications of the findings in practical machine learning applications would add value to the papers impact and relevance. Overall the paper contributes significantly to the decentralized optimization field by introducing novel algorithms and theoretical foundations. Addressing the weaknesses and questions raised could further enhance the clarity robustness and practical utility of the work.", "q0XxMcbaZH9": " Peer Review  1) Summary The paper introduces a novel training framework for querybased instance segmentation models by enhancing discriminative query embedding learning through datasetlevel uniqueness and transformation equivariance. The approach significantly improves the performance of existing methods on COCO and LVISv1 datasets showcasing impressive gains in AP. The proposed framework offers a paradigm shift in training strategies contributing to a more robust and effective instance segmentation process.  2) Strengths and Weaknesses Strengths:  The paper presents a wellorganized and clear explanation of the proposed training framework highlighting the importance of datasetlevel uniqueness and transformation equivariance.  Extensive experimental results on COCO and LVISv1 datasets demonstrate significant performance improvements over existing querybased methods.  The approach shows flexibility by being applicable to various backbone networks and querybased models without requiring architectural modifications.  The discussion on key components loss functions sampling strategies and memory capacity provides valuable insights into the algorithms effectiveness. Weaknesses:  The paper lacks a detailed analysis of potential failure cases or situations where the proposed framework might not be as effective.  The abstract and introduction could be improved to provide a more concise overview of the papers contribution.  While the ablation studies offer valuable information a deeper discussion on the implications of different loss functions and sampling strategies would enhance the papers completeness.  4) Questions  Could the paper elaborate on how the proposed framework can handle complex and densely populated scenes where instance separation is more challenging  Does the methodology consider realworld scenarios where instances are occluded or partially visible and how does the training framework address these cases  Are there any insights into the scalability of the proposed approach to larger datasets or different domains beyond COCO and LVISv1  5) Limitations  The study could benefit from a thorough analysis of computational costs and training time especially for larger datasets or more complex models.  While the experiments demonstrate impressive gains in performance the generalizability of the proposed framework to a wider range of datasets and instances would be of interest.  The discussion could be expanded to include potential future research directions such as adapting the framework for video instance segmentation or incorporating semantic information in the training process. Overall the paper presents a novel and effective training framework for querybased instance segmentation models showcasing significant performance improvements on benchmark datasets. Further exploration into the frameworks robustness scalability and applicability to diverse scenarios would enhance the papers impact and relevance in the field of computer vision research.", "pUPFRSxfACD": " Peer Review:  1) Summary: The paper addresses the challenge of learning invariant models from heterogeneous data without explicit environment indexes. It demonstrates the impossibility of learning invariant features under such circumstances and proposes a new framework called ZIN that jointly learns environment partition and invariant representation with the help of auxiliary information. The paper provides theoretical guarantees and experimental results to support the efficacy of the proposed method across synthetic and realworld datasets.  2) Strengths and Weaknesses: Strengths:  Clear problem statement and motivation for the study.  Theoretical analysis provides insightful insights into the challenges of learning invariant models without environment partition.  The proposed ZIN framework is novel and addresses the limitations of existing methods.  Empirical results demonstrate the effectiveness of ZIN in comparison to other stateoftheart methods.  The inclusion of auxiliary information and its impact on learning invariant features is a valuable contribution. Weaknesses:  The paper could benefit from more indepth discussion on the practical implications of the findings and how the proposed framework could be applied in realworld scenarios.  The limitations of the study are briefly mentioned but could be further elaborated to provide a comprehensive understanding of the proposed method.  Further analysis on the scalability and computational complexity of the ZIN framework would enhance the practicality of the proposed approach.  3) Questions:  Can the proposed ZIN framework be extended to handle nonlinear feature extraction methods and if so what challenges are anticipated in doing so  How robust is the ZIN framework to noisy or irrelevant auxiliary information and are there strategies to mitigate the impact of such variables on the learning process  What are the implications of the findings from this study for realworld applications in fields such as healthcare finance or autonomous systems  4) Limitations:  The study lacks a detailed discussion on the generalizability of the proposed ZIN framework to diverse domains and datasets.  While the empirical results show promising performance further evaluation on larger and more diverse datasets would strengthen the validity of the proposed method.  The focus on linear feature extractors limits the generalizability of the theoretical guarantees provided. Overall the paper presents a valuable contribution to the field of invariant learning from heterogeneous data. The proposed ZIN framework offers a novel approach to address the challenges of learning invariant models without environment partition. Further exploration and extension of the proposed method could uncover additional insights and applications in various domains.", "yI7i9yc3Upr": " Peer Review:  1) Summary: The paper proposes a framework called NeurAllyDecomposed Oracle (NADO) for controlling autoregressive language models in text generation tasks. By decomposing a sequencelevel oracle into tokenlevel guidance and training an auxiliary neural model NADO efficiently guides the base model to satisfy specified attributes without the need for labeled data. Experimental results on tasks including text generation with lexical constraints and machine translation with formality control demonstrate that NADO effectively guides the base model while maintaining highquality generation. The theoretical analyses provided further support the frameworks effectiveness across different tasks.  2) Strengths and weaknesses:  Strengths:  Innovative Framework: The NADO framework introduces a novel approach to controlling autoregressive language models in text generation tasks.  Efficient and Flexible: NADO efficiently guides the base model towards specified attributes without requiring additional labeled data offering flexibility in handling various control factors.  Theoretical Analysis: The paper provides theoretical analyses regarding the approximation quality of NADO and how it affects controllable generation.  Experimental Results: Results on both lexical constrained generation and machine translation tasks demonstrate the frameworks effectiveness in maintaining high generation quality while guiding the model towards specific attributes.  Weaknesses:  Theoretical Rigor: While the paper provides comprehensive experimental results and theoretical analyses further elaboration on the limitations of the proposed framework would enhance the discussion.  Generalization: The paper primarily focuses on the proposed frameworks performance in specific tasks exploring its generalization to a broader range of applications could strengthen the validity of the framework.  3) Questions:  How does the proposed NADO framework compare to existing methods in terms of computational efficiency and training time  Can NADO be easily integrated into existing text generation models or does it require significant modifications  Have the authors considered the potential biases that might arise from relying on a sequencelevel oracle for guiding the text generation process  4) Limitations:  Evaluation on Diverse Datasets: The evaluation is primarily conducted on specific tasks such as lexically constrained generation and formality control in machine translation. Further evaluation on a diverse set of datasets with varying constraints would strengthen the generalizability of NADO.  Scalability: The scalability of the NADO framework to larger models and datasets warrants further investigation to assess its performance under more complex scenarios.  Robustness to Noisy Data: How does NADO perform in scenarios where the sequencelevel oracle or training data may contain noise or errors and what strategies are in place to mitigate such issues Overall the paper presents a novel and efficient framework for controlling autoregressive language models in text generation tasks. The theoretical analyses and experimental results provide compelling evidence of NADOs effectiveness in guiding models towards specific attributes while maintaining high generation quality. Further exploration of its broad applicability and robustness to various scenarios would enhance the frameworks utility in realworld applications.", "mmzkqUKNVm": " Peer Review 1. Summary The paper introduces a novel semantic diffusion network (SDN) to enhance semantic boundary awareness in deep semantic segmentation models. The SDN is designed to address the issue of convolutional operators smoothing local detail cues making it challenging for deep models to generate accurate boundary predictions. The approach is based on an anisotropic diffusion process guided by semantics resulting in improved boundary quality in segmentation results. 2. Strengths and Weaknesses Strengths:  The paper addresses an important challenge in semantic segmentation: improving boundary quality.  The proposed SDN is a novel and efficient module that can be easily integrated into existing segmentation models.  Extensive experiments show consistent improvements over baseline models on challenging benchmarks.  The theoretical background of anisotropic diffusion and the formulation of the SDN are well explained. Weaknesses:  The paper could provide more clarity in explaining how the proposed approach compares with existing boundaryaware segmentation methods.  While the results show improvement in boundary quality a more detailed comparison with stateoftheart methods could strengthen the papers claims.  The paper lacks a detailed discussion on the computational complexity of the SDN compared to other methods. 3. Questions  How does the proposed SDN compare with other methods that enhance semantic boundary awareness in terms of computational efficiency and accuracy  Can the SDN be generalized to other tasks beyond semantic segmentation and are there any limitations to its applicability in different domains  Have the authors considered any potential limitations or challenges in implementing the SDN in realworld applications 4. Limitations  The paper focuses on the effectiveness of the SDN in improving semantic segmentation boundary quality but lacks a thorough analysis of the generalizability of the approach to different datasets and scenarios.  The discussion on the computational overhead of integrating the SDN into deep networks could be further elaborated to provide a comprehensive understanding of its practical implications.  The qualitative evaluation could be enhanced by providing more detailed visualizations and comparisons with ground truth annotations. In conclusion the paper presents a novel approach to enhance semantic boundary awareness in semantic segmentation models. While the experimental results demonstrate the effectiveness of the proposed SDN further analysis and comparisons with stateoftheart methods would strengthen the papers contributions and provide a more comprehensive understanding of its implications in the field of computer vision.", "uzn0WLCfuC_": " Peer Review  1) Summary The paper introduces the Expected Improvement (EI) technique in the contextual bandit setting and proposes two novel EIbased algorithms for linear bandits and neural contextual bandits. The algorithms are analyzed theoretically and validated through experiments on benchmark datasets. The paper provides insights into the tradeoff between exploration and exploitation in both linear and neural bandit problems.  2) Strengths and Weaknesses Strengths:  Novel Contribution: Introducing EI in the contextual bandit framework is a significant contribution.  Theoretical Analysis: The theoretical analysis provides regret bounds for the proposed algorithms enhancing the understanding of their performance.  Empirical Validation: Experiments on benchmark datasets demonstrate the effectiveness of the proposed algorithms especially in high dimensions.  Clear Presentation: The paper is wellstructured and presents the material in a logical sequence. Weaknesses:  Limited Comparison: While the comparison with existing approaches is insightful additional comparisons with more algorithms could provide a broader perspective.  Assumption Clarity: Some assumptions especially those related to parameter selection or algorithm components could be elaborated for better understanding.  3) Questions  Can you elaborate on the practical implementation aspects of the proposed algorithms especially in terms of computational complexity and convergence speed  How sensitive are the proposed algorithms to hyperparameter choices and are there guidelines for selecting optimal hyperparameters  Could you provide more insights into the scalability of the algorithms to larger datasets or more complex reward functions  4) Limitations  Assumption Dependency: The performance of the algorithms may heavily depend on the assumptions made about the reward functions or contexts affecting their generalizability.  Generalization: While the paper focuses on linear and neural reward functions it would be interesting to explore the applicability to a broader range of reward function types. Overall the paper provides valuable insights into leveraging the EI technique for contextual bandits offering promising new algorithms and theoretical results. Further exploration and validation on diverse datasets and scenarios could enhance the impact of the research.", "upuYKQiyxa_": "Peer Review 1) Summary The paper introduces a method to enhance the robustness of visual classification models by focusing on foreground objects rather than spurious cues from the background. This is achieved through finetuning Vision Transformer (ViT) models using a relevancebased supervised learning approach involving foreground masks. The method shows significant improvement in the models resistance to distribution shifts leading to more accurate classification of various datasets. It provides detailed experiments comparisons with baseline models and ablation studies to validate the effectiveness of the proposed approach. 2) Strengths and weaknesses Strengths:  The paper addresses an important issue of relying on background cues in visual classification models.  The proposed method is wellmotivated and demonstrated with comprehensive experiments.  The ablation study and comparison with baseline methods help in understanding the effectiveness of the approach.  Extensive evaluation on various datasets supports the claims about improved robustness. Weaknesses:  While the method shows promising results on robustness a deeper analysis of potential failure cases or limitations could enhance the paper.  The approach heavily relies on foreground masks which may not be feasible or available for all datasets or applications.  Further insights into the computational overhead of the proposed method especially in largescale applications could be beneficial. 3) Questions  How does the method perform on more complex datasets with intricate background objects or cluttered scenes  Can the proposed approach be extended or adapted for different modalities beyond vision models  Is there a risk of bias introduced through the finetuning process that might affect the models overall performance on specific datasets or classes 4) Limitations  The reliance on foreground masks for finetuning could limit the practical applicability of the method in scenarios where obtaining such masks is challenging.  The drop in accuracy on the original training dataset raises concerns about possible tradeoffs between robustness and generalization.  The methods dependency on hyperparameters and potential sensitivity to variations in the input data could impact its reproducibility and scalability. Overall the paper presents a novel and valuable contribution to improving the robustness of visual classification models by focusing on foreground objects. Further investigation into the methods adaptability to different scenarios and potential biases introduced during finetuning is recommended to enhance the papers impact and applicability.", "q_AeTuxv02D": " Review of the Scientific Paper 1. Summary: The paper introduces the concept of inductive outofdistribution (OOD) link prediction tasks where test graphs are larger than training graphs and studies the ability of graph Message Passing Neural Networks (gMPNNs) in this context. The theoretical framework developed shows that link predictors based on structural node embeddings from gMPNNs may fail in OOD link prediction tasks as test graphs grow larger. A novel family of structural pairwise embeddings gMPNNs\u2022\u2022 is proposed to address this challenge. The work includes nonasymptotic bounds on convergence and empirical validation on random graphs. 2. Strengths:  The paper addresses an important and novel research question concerning link prediction tasks in the context of OOD scenario with test graphs larger than training graphs.  The theoretical framework and proposed gMPNNs\u2022\u2022 for pairwise embeddings offer new insights into the limitations and possibilities in OOD link prediction tasks.  The empirical validation using stochastic block models and realworld data (ogblddi dataset) strengthens the credibility of the theoretical findings. 3. Weaknesses:  The paper lacks realworld application examples limiting the generalizability of the findings to practical scenarios.  The theoretical and empirical results could be further supported by comparing with more baseline methods and datasets to provide a comprehensive evaluation.  The text is technical making it difficult for nonexperts to understand the key concepts easily. 4. Questions:  How do the proposed gMPNNs\u2022\u2022 compare with other stateoftheart methods in OOD link prediction tasks in terms of performance and computational efficiency  Can the theoretical insights on link prediction in OOD scenarios be extended to other types of graphs such as temporal or spatial networks  Are there any assumptions made in the theoretical framework that might affect the generalizability of the findings to diverse graph structures 5. Limitations:  The study focuses on a specific aspect of link prediction tasks which may limit the broader applicability of the findings to different types of graphs and applications.  The empirical evaluation is primarily done on synthetic and limited realworld datasets potentially restricting the generalizability of the results to other scenarios.  The complexity of the theoretical framework and technical language used in the paper may hinder accessibility to a wider audience of researchers. Overall the paper presents valuable insights into OOD link prediction tasks offering a strong theoretical foundation and empirical validation of the proposed methods. Further research could explore the practical implications of the findings in realworld scenarios and expand the scope of applications for these novel approaches.", "t4vTbQnhM8": " Peer Review of the paper \"Assessing the Quality of Synthetic Data Generators using NonParametric Kernel Stein Discrepancy Tests\" Summary: The paper introduces a novel method NonParametric Kernel Stein Discrepancy (NPKSD) for assessing the quality of synthetic data generators in cases where explicit probability distributions are not available particularly for deep generative models. The method is based on estimating conditional score functions and offers theoretical guarantees for quality assessment. Experimental results on synthetic and real datasets show that NPKSD outperforms existing approaches in terms of power performance. Strengths:  The paper addresses a relevant and important problem in machine learning focusing on assessing the quality of synthetic data generators.  Introduces a novel method NPKSD which provides theoretical guarantees for assessing the quality of synthetic data generators especially for implicit generative models in high dimensions.  The experimental results demonstrate the effectiveness of NPKSD in comparison to existing approaches showcasing improved power performance.  The theoretical underpinnings provided in the paper are robust and the methodological approach is well explained.  The use of synthetic and real datasets for experimentation adds credibility to the findings. Weaknesses:  While the theoretical basis of NPKSD is sound the practical implications of the method could be further elaborated. How does NPKSD compare in terms of computational complexity and scalability Are there any limitations in terms of applicability to realworld datasets or scenarios  The paper could benefit from a more detailed discussion on the choice of the summary statistic and its influence on the assessment results. Clarity on how different summary statistics impact the performance of NPKSD would enhance the understanding of the method.  The limitations of the proposed method especially in scenarios where the dimensionality of the data is extremely high need to be explicitly addressed.  More insights into the interpretability and generalizability of the findings on the realworld datasets specifically in the context of potential implications for decisionmaking processes would be valuable. Questions: 1. How does the choice of summary statistic impact the performance and interpretability of the NPKSD method Are there guidelines or recommendations for selecting an appropriate summary statistic 2. Can NPKSD be easily extended or adapted for different types of datasets or generative models beyond the ones considered in the experiments 3. How does NPKSD handle scenarios where the dimensionality of the data is extremely high and are there any scalability issues with large datasets Limitations:  The paper focuses primarily on the theoretical and experimental aspects of the proposed NPKSD method without delving deeply into the practical implications and limitations in realworld applications.  The experiments while providing valuable insights are limited to synthetic and two realworld datasets. Further validation on a broader range of datasets and complex generative models could enhance the generalizability of the findings. Overall the paper makes a significant contribution to the field of synthetic data generation and quality assessment methods. However addressing the weaknesses and limitations mentioned along with the raised questions would strengthen the paper and its applicability in practical scenarios.", "p3w4l4nf_Rr": " Peer Review  1) Summary This paper investigates Nashregret minimization in congestion games proposing centralized and decentralized algorithms with finitesample guarantees. Central to the study are congestion games which have broad realworld applications and pose challenges due to the curse of exponential action sets. The authors introduce a new problem class Markov congestion games enabling modeling of nonstationarity. The proposed algorithms achieve sample complexity polynomial in the number of players and facilities. They establish a connection between congestion games and potential games addressing shortcomings of existing algorithms by offering sampleefficient solutions.  2) Strengths and weaknesses Strengths:  The paper addresses an important problem in game theory with practical applications.  The proposed algorithms achieve sublinear Nash regrets with polynomial sample complexity.  The study introduces a novel problem class Markov congestion games broadening the scope of research.  The approach of combining centralized and decentralized algorithms for congestion games is innovative. Weaknesses:  The paper lacks empirical validation of the proposed algorithms.  Detailed explanations of certain technical aspects could be helpful for readers less familiar with the topic.  The complexity of the presented algorithms may limit their practical applicability without further optimizations.  3) Questions  How do the proposed algorithms compare to existing solutions in terms of computational efficiency  Is there a plan to validate the algorithms through simulations or realworld applications  How sensitive are the algorithms to variations in the parameters of the congestion games  Can the decentralized algorithm be further optimized to reduce sample complexity while maintaining performance  4) Limitations  The lack of empirical validation limits the understanding of algorithm performance in realworld scenarios.  The theoretical focus may hinder practical applicability without further refinement and optimization.  The assumption of specific reward structures and transition kernels may limit generalizability to realistic game scenarios. In conclusion the paper offers valuable contributions to the field of congestion games by providing sampleefficient algorithms for Nashregret minimization. The inclusion of empirical validation and further optimization efforts would enhance the practical relevance and impact of the proposed solutions.", "wQVjGP5NbP9": "Peer Review 1) Summary: The paper introduces a new differentially private algorithm for correlation clustering on unweighted complete graphs. The algorithm achieves a constant multiplicative approximation and an optimal O(n log\u00b2n) additive approximation with fixed privacy parameters. The authors provide theoretical results on privacy guarantees and approximation bounds building upon previous work. The paper also discusses potential future research directions. 2) Strengths and weaknesses: The paper presents novel research on differentially private correlation clustering addressing an important problem in unsupervised learning. The algorithm proposed offers strong privacy guarantees and nearoptimal approximation bounds enhancing the understanding of privacypreserving clustering methods. The theoretical analysis is rigorous with clear technical details on the algorithm and its privacy and approximation properties. The paper cites relevant prior research in the field providing context for the current work. One weakness is the lack of experimental validation which limits the practical assessment of the proposed algorithms performance. Additionally the complexity of the proposed algorithm may hinder its scalability especially in realworld scenarios with large datasets. The paper could benefit from discussing potential challenges in implementing the algorithm in practice. 3) Questions:  The paper mentions that the additive error is far from matching the lower bound in some cases. Could the authors elaborate on the potential reasons for this discrepancy and suggest possible strategies for reducing the gap in future work  Given the limitations of the current algorithm in terms of scalability and practical implementation are there any plans to explore optimizations or modifications to improve efficiency in realworld applications  The paper discusses the theoretical aspects extensively. Could the authors provide insights into how the algorithm might perform on realworld data scenarios considering the complexities involved in privacypreserving clustering tasks 4) Limitations:  The lack of experimental evaluation limits the practical relevance of the proposed algorithm. Future work should include empirical studies to demonstrate the performance of the algorithm on real datasets.  The algorithms complexity and scalability could be a significant limitation in realworld applications where processing large datasets efficiently is crucial. In conclusion the paper makes a significant contribution to the field of differentially private correlation clustering by introducing a novel algorithm with strong theoretical guarantees. However addressing practical implementation challenges and conducting empirical validations would enhance the impact and applicability of the research findings.", "kImIIKGqDFA": " Peer Review  1) Summary: The paper introduces the Adaptive Gradient Variance Modulator (AGVM) algorithm designed to enable largebatch training of dense visual predictors with consistent gradient variances across different modules. The method aligns gradient variances among network components thereby improving training stability and reducing convergence time. Extensive experiments on COCO and ADE20K datasets demonstrate AGVMs superior performance enabling training of models with one billion parameters in significantly reduced timeframes while maintaining high accuracy.  2) Strengths and Weaknesses: Strengths:  Novel Contribution: AGVM introduces a unique approach to address gradient variance misalignment in largebatch training enabling efficient training of complex visual prediction pipelines.  Generalizability: AGVM is shown to perform well across various architectures (CNNs Transformers) and tasks (object detection segmentation) making it applicable to diverse scenarios.  Superior Performance: The experimental results demonstrate AGVMs capability to train models rapidly without sacrificing performance achieving stateoftheart results.  Theoretical Analysis: The paper provides a theoretical foundation for AGVM adding depth to its understanding. Weaknesses:  Lack of Comparison: While AGVM is compared against standard largebatch optimization methods a more comprehensive comparison with a wider range of approaches could strengthen the evaluation.  Complexity: The paper introduces intricate technical details regarding gradient variance which may pose challenges for readers not familiar with the domain.  Limited Abstractions: The paper could benefit from more highlevel explanations or visualizations to help readers grasp the core concepts without delving into detailed mathematical formulations.  3) Questions:  Scalability: Can AGVM be adapted for even larger batch sizes beyond 10k and what challenges may arise in scaling further  Robustness: How does AGVM perform under noisy or sparse datasets where gradient estimation may be less reliable  Computational Overhead: What is the computational cost associated with implementing AGVM especially on resourceconstrained systems  4) Limitations:  Module Partitioning: The paper acknowledges challenges in effective batch size estimation for some pipelines and poses a potential limitation in models without explicit modularity.  Societal Impact: The potential negative use of AGVM for training malicious models highlights ethical considerations that should be addressed carefully. In conclusion the paper presents a valuable contribution to largebatch training in dense visual prediction tasks. Improvements in readability broader comparative analysis and addressing scalability concerns would further enhance the significance of this research.  Overall Rating: 45", "mwIPkVDeFg": "Peer Review 1) Summary: The paper addresses the issue of communication complexity in decentralized optimization for overparameterized machine learning models. Researchers conduct a comprehensive analysis to determine the minimum communication overhead required to achieve optimal training loss. They introduce algorithms that utilize linear compression and adaptive quantization to reduce communication complexity focusing on both PolyakLojasiewicz (PL) problems and overparameterized models. The results are promising demonstrating dimensionindependent communication complexity for certain models. Extensive theoretical analysis and algorithms are provided for decentralized optimization showcasing advancements in this field. 2) Strengths:  Comprehensive Analysis: The paper provides indepth theoretical analysis and algorithms to address the communication complexity challenges in decentralized optimization for overparameterized models.  Novel Algorithms: The development of algorithms incorporating linear compression and adaptive quantization is a valuable contribution to the field.  Clear Presentation: The structure of the paper is organized and the content is presented in a logical order facilitating understanding for readers. 3) Weaknesses:  Lack of Experimental Validation: While the paper provides theoretical analysis and preliminary numerical experiments further experimental validations on realworld datasets are recommended to verify the practical effectiveness of the proposed algorithms.  Complexity: The paper is highly technical and may be challenging for readers without a strong background in optimization theory. Simplifying some technical aspects for a broader audience could enhance the accessibility of the paper. 4) Questions:  Experimental Results: Have you planned additional experiments on diverse datasets to further evaluate the performance of the proposed algorithms in practical scenarios  Scalability: How do the proposed algorithms scale with respect to the size of the network (number of agents) and the complexity of the models  Sensitivity Analysis: Have you conducted a sensitivity analysis on algorithm parameters to evaluate the robustness and stability of the proposed methods 5) Limitations:  Theoretical Focus: The heavy emphasis on theoretical analysis may limit the practical applicability of the proposed algorithms. Incorporating more practical considerations could enhance the relevance of the work.  Simplification: To cater to a wider audience simplifying complex concepts and providing more intuitive explanations could make the paper more accessible to researchers from diverse backgrounds. In conclusion the paper makes significant contributions to the field of decentralized optimization for overparameterized models. However additional experimental validation and a focus on practical implications could further strengthen the impact of the work. Overall the paper provides valuable insights into communication complexities in decentralized optimization offering novel algorithms and theoretical analysis. Further empirical validation and considerations for practical implementation would enhance the applicability and impact of the research.", "v1bxRZJ9c8V": " Peer Review Summary: The paper introduces a novel uncertaintyaware model for continuoustime dynamics of interacting objects using latent Gaussian process ordinary differential equations. The model decomposes the dynamics into independent and interaction components allowing for reliable uncertainty estimation. Through variational sparse Gaussian process inference techniques the model efficiently handles complex networks of variables. The empirical findings demonstrate improved reliability of longterm predictions over neural network alternatives and successful handling of missing information. Strengths: 1. Innovative Model: The proposed model for uncertaintyaware continuoustime dynamics of interacting objects is innovative and addresses a critical gap in the field. 2. Effective Decomposition: The decomposition of dynamics into independent and interaction components allows for interpretable predictions and improved extrapolation behavior. 3. Efficient Inference: The use of variational sparse Gaussian process inference techniques enables efficient inference for complex networks of variables showcasing scalability to large datasets. 4. Superior Performance: The model consistently outperforms neural network alternatives and noninteracting dynamical systems demonstrating its effectiveness in various scenarios. 5. Handling Missing Information: The models ability to handle missing dynamic or static information is a significant strength enhancing its practical applicability. Weaknesses: 1. Detailed Implementation Discussion: The paper lacks a detailed discussion on the practical implementation aspects of the proposed model such as computational requirements hyperparameter tuning guidelines and potential challenges in realworld applications. 2. Comparative Analysis: While the empirical findings demonstrate the superiority of the proposed model a deeper comparative analysis with existing stateoftheart models especially in specific application domains would enrich the discussion on the models strengths and limitations. Questions: 1. Scalability: How does the proposed model scale in terms of computational complexity as the number of objects or interactions increases Are there limitations in handling extremely large datasets 2. Interpretability: Could the authors provide more insights into the interpretability of the disentangled representations of independent kinematics and interaction effects in practical applications 3. Generalization: How does the model perform in terms of generalizability when applied to realworld datasets with inherent complexities and uncertainties beyond the controlled experimental setups Limitations: 1. Kernel Choice: The paper mentions the limitations imposed by the choice of the kernel used. Exploring alternative kernel functions or composite kernels could enhance the models flexibility and performance. 2. Approximation Errors: Acknowledging the approximation errors and potential numerical inaccuracies should be further discussed to provide a comprehensive understanding of the models robustness. Overall Assessment: The paper presents a highly novel and promising model for uncertaintyaware continuoustime dynamics of interacting objects. The empirical results showcase the models superior performance and ability to handle missing information. Addressing the raised questions and limitations could further strengthen the papers contribution to the field of dynamical systems modeling. Further clarification on practical implementation details and realworld applicability would enhance the papers impact and relevance. Final Recommendation: With some additional discussions on implementation specifics comparative analysis and addressing the raised questions and limitations the paper has the potential to make a significant contribution to the field and should be considered for publication with minor revisions.", "lAN7mytwrIy": " Peer Review of the Paper \"ElasticMVS: A Novel Approach for SelfSupervised MultiView Stereopsis\" 1) Summary: The paper introduces ElasticMVS a novel approach for selfsupervised multiview stereopsis which leverages geometric correlations inferred from images to guide pixelwise multiview correspondence estimation. It presents an elastic part representation and a comprehensive selfsupervised MVS framework that significantly outperforms stateoftheart methods in terms of reconstruction completeness accuracy efficiency and scalability. ElasticMVS demonstrates remarkable performance gains especially in challenging largescale reconstruction benchmarks. Experimental results on DTU and Tanks and Temples datasets validate the efficacy and robustness of ElasticMVS compared to supervised and existing selfsupervised approaches. 2) Strengths and Weaknesses: Strengths:  The paper addresses a critical issue in selfsupervised MVS by leveraging geometric correlations for accurate pixelwise multiview correspondence estimation.  The introduction of the elastic part representation is a novel and effective way to encode physically connected part segmentations improving both accuracy and completeness in reconstruction.  The proposed framework outperforms stateoftheart methods in challenging largescale scenes demonstrating superior performance and generalization ability.  Extensive evaluations and comparisons on DTU and Tanks and Temples datasets validate the effectiveness of ElasticMVS in terms of accuracy efficiency and generalization. Weaknesses:  The computational complexity of calculating the patch similarity for each pixel in ElasticMVS may limit realtime application and scalability suggesting potential constraints in practical implementations.  The discussion on the impact of parameters highlights the importance of certain thresholds but further exploration or optimization of these parameters could enrich the papers analysis.  The paper could benefit from a more extensive analysis of the limitations and possibilities for future work particularly in addressing computational complexity and realtime implementation challenges. 3) Questions:  How does the proposed elastic part representation compare with traditional segmentation networks in terms of efficiency and performance  Could the authors provide more insights into the computational complexity constraints of ElasticMVS and potential strategies for overcoming them in future work  How does the selfsupervised training process of the elastic part representation handle noisy initializations and prevent undesired identifications  In the ablation study what additional experiments could be valuable to further highlight the importance of the partaware patchmatch components 4) Limitations:  The computational complexity of ElasticMVS might hinder practical realtime implementation without further optimization strategies.  The limitations section could benefit from a more extensive discussion on scalability robustness in various scenarios and potential future directions for addressing the identified challenges. Overall the paper presents a novel and effective approach for selfsupervised MVS with significant performance gains. By addressing the limitations and computational complexities the authors can further enhance the applicability and scalability of ElasticMVS. Further investigation into parameter optimization and computational efficiency could provide valuable insights for future research and practical implementations.", "tUH1Or4xblM": " Peer Review  1) Summary The paper introduces an objectcentric segmentation model using a layered representation and a transformer architecture that processes optical flow data to discover track and segment multiple moving objects in videos. The model is trained on synthetic data and achieves stateoftheart performance on various video segmentation benchmarks. The approach shows promise in handling occlusions and inferring depth ordering of object layers.  2) Strengths  The paper addresses a challenging problem of segmenting multiple moving objects with occlusions in videos.  The introduction of an ObjectCentric Layered Representation (OCLR) model showcases innovative use of a transformer architecture for object segmentation.  Training on synthetic data for Sim2Real generalization is a noteworthy contribution.  Thorough ablation studies and performance evaluation on standard benchmarks demonstrate the effectiveness of the proposed model.  3) Weaknesses  The paper focuses on synthetic data and may not fully account for realworld complexities potentially limiting the models performance in challenging scenarios.  The reliance on optical flow alone poses limitations in scenarios where objects are static or obscured.  The use of a global depth ordering may restrict the models ability to handle more complex layer interactions and order alterations.  Further exploration of incorporating RGB information into the model for enhanced performance could be beneficial.  4) Questions  How does the proposed model handle cases of heavy object deformations and complex mutual interactions in realworld scenarios  What strategies can be employed to overcome the limitations of relying on optical flow data exclusively for object segmentation  Is there a plan to extend the model to dynamically adapt the layer ordering based on pairwise relationships between objects in a sequence  How does the model perform when presented with highly articulated objects or 3D sprites in the data simulation pipeline to reduce the Sim2Real gap  5) Limitations  The heavy reliance on synthetic data may not fully capture the complexity of realworld scenarios potentially limiting the models performance in challenging conditions.  The use of optical flow as the primary information stream may constrain the models capability in scenarios where objects are stationary or visually obscured.  The global depth ordering approach may restrict the models ability to handle more intricate layer interactions and order alterations in complex scenes. Overall the paper presents a novel approach to multiobject video segmentation but could benefit from further exploration into adapting the model to realworld challenges and incorporating RGB information for improved performance. The limitations should be carefully considered in future developments to enhance the models applicability in more diverse scenarios.", "pHdiaqgh_nf": " Summary: The paper discusses the reconstruction of complex image stimuli from fMRI signals using a multimodal approach that incorporates textual descriptions and pretrained visionlanguage latent spaces. The study aims to decode brain signals more naturally and faithfully by mapping fMRI signals into a shared latent space. The results show successful reconstruction of complex images considering both naturalness and fidelity and include a demonstration of the potential utility of the pipeline through microstimulation experiments.  Strengths: 1. Innovative Approach: The paper introduces a novel method to reconstruct complex images from fMRI signals incorporating additional text modality and leveraging pretrained models. 2. Data Scarcity Addressed: By utilizing pretrained latent spaces and incorporating more modalities the study effectively tackles the challenge of data scarcity in brain datasets. 3. Comprehensive Experimental Setup: The paper provides a detailed experimental setup including model architectures training procedures and evaluation metrics. 4. Microstimulation Experiments: The inclusion of microstimulation experiments provides insights into understanding brain regions functionalities and responses.  Weaknesses: 1. Limited Comparison: The paper lacks a comprehensive comparison with existing methods for brain signal decoding making it difficult to assess the methods advancements over previous approaches. 2. Complexity: The intricate pipeline involving multiple modalities and pretrained models may be challenging for replication and practical implementation in other studies. 3. Lack of Generalization: While the results are promising the studys generalizability beyond the specific dataset and subjects utilized is not thoroughly explored. 4. Ethical Considerations: The potential implications of brain signal decoding on privacy and security are briefly acknowledged but warrant further discussion.  Questions: 1. Generalization: How applicable is the proposed method to datasets beyond the specific NSD dataset and subjects used in the study 2. Performance Comparison: How does the method compare in terms of computational efficiency and reconstruction quality against existing brain imaging decoding approaches 3. Clinical Applications: Are there potential realworld applications of the pipeline in clinical neuroimaging or cognitive studies  Limitations: 1. Dataset Specificity: The study relies on a specific dataset which may limit the generalizability of the findings to other imaging data. 2. Complexity: The complexity of the pipeline and reliance on pretrained models may hinder broader adoption and implementation in practical settings. 3. Interpretation Challenges: While the reconstructed images are photorealistic the interpretability of the results in the context of neural processes or visual perception remains a challenge. 4. Ethical Implications: The paper briefly addresses ethical considerations but would benefit from a more indepth discussion on potential societal impacts and regulatory aspects. Overall the paper presents an innovative approach to reconstructing images from fMRI signals addressing data scarcity challenges and leveraging multimodal information. Further investigations into generalizability efficiency and realworld applications would enhance the studys impact and relevance in the field of neuroscience.", "ob8tk9Q_2tN": " Peer Review of the Scientific Paper 1. Summary: The paper introduces MinAM a novel variant of Anderson mixing (AM) method that addresses the memory bottleneck issue by storing only one vector pair. MinAM is proven to be equivalent to the fullmemory TypeI AM for solving strongly convex quadratic optimization. The paper establishes the convergence properties of MinAM for general nonlinear optimization problems and extends it to solve stochastic programming problems. Experimental results validate the effectiveness of MinAM in logistic regression network training CIFAR and ImageNet datasets. 2. Strengths and Weaknesses:  Strengths:  The paper introduces a novel variant MinAM addressing the memory requirement bottleneck in AM for largescale optimization problems.  The theoretical analysis of MinAMs convergence properties under various optimization scenarios is comprehensive.  Experimental results on logistic regression network training CIFAR and ImageNet datasets validate the effectiveness of MinAM.  The proposed method shows competitive performance and reduced memory overhead compared to existing methods.  Weaknesses:  The paper could emphasize more on the practical implementation aspects and computational complexity of MinAM.  The discussion on theoretical convergence properties can be further strengthened with more detailed analysis.  More comparison with existing stateoftheart methods could enhance the robustness and significance of MinAM. 4. Questions:  Is there a tradeoff between memory efficiency and convergence speed in MinAM compared to existing memoryintensive methods like AM  How does the computational complexity of MinAM compare with other optimization methods especially for largescale optimization problems  Are there any specific scenarios or problem types where MinAM may not be as effective or practical 5. Limitations:  The paper could benefit from a more detailed explanation of the experimental setup hyperparameters and comparison metrics used in the experiments.  While the theoretical analysis is sound the paper could provide more insights into the practical implications and limitations of MinAM in realworld applications.  The limitations of MinAM in terms of scalability and applicability to diverse optimization problems could be better addressed. Overall the paper presents a significant contribution with the introduction of MinAM to address memory constraints in AMbased optimization methods. However further clarity and elaboration in terms of practical implementation and comparison with existing methods would enhance the impact and applicability of MinAM in various optimization scenarios.", "xjXN3wEvCGG": "Peer Review 1) Summary: The paper presents a novel approach called Demonstration Informed Specification Search (DISS) for learning temporal task specifications from expert demonstrations. The method efficiently identifies tasks by alternating between conjecturing labeled examples and sampling tasks consistent with the conjectured labeled examples. The study empirically demonstrates the effectiveness of DISS in learning Probabilistic Finite Automata from expert demonstrations. 2) Strengths and Weaknesses: Strengths:  The paper introduces a new algorithm DISS for learning temporal task specifications effectively from expert demonstrations.  The approach is novel and presents a concrete implementation in the context of Deterministic Finite Automata showcasing its efficiency in identifying tasks from one or two expert demonstrations.  The experiments demonstrate the effectiveness of DISS in outperforming baseline methods in terms of search efficiency.  The paper includes detailed theoretical foundations algorithmic descriptions and empirical results contributing to the advancement of the field of learning from demonstrations. Weaknesses:  The paper lacks a detailed discussion on the scalability of the proposed approach to larger and more complex tasks.  While the experiments demonstrate the comparative efficiency of DISS a more extensive comparison with a broader range of existing methods would strengthen the evaluation.  The paper could benefit from a more indepth discussion on the tradeoffs or limitations of the proposed method in dealing with noisy or sparse demonstrations. 4) Questions:  How does the DISS algorithm scale with increasing complexity or size of the concept class especially in scenarios with large and diverse tasks  Can the proposed approach handle noisy or incomplete expert demonstrations effectively and are there strategies in place to improve robustness in such scenarios  Have you considered incorporating additional domain knowledge or constraints into the learning process to enhance the efficiency of task identification 5) Limitations:  The study primarily focuses on learning Probabilistic Finite Automata and may have limited generalizability to more complex task representations.  The empirical evaluation lacks realworld application scenarios limiting the practical implications of the proposed method.  The paper mainly discusses the efficiency of task identification but a more indepth analysis of the interpretability and generalization capabilities of the learned models could provide a more comprehensive understanding of the approachs limitations. Overall the paper presents a promising approach for learning task specifications from expert demonstrations with notable strengths in efficiency and effectiveness. However addressing the identified weaknesses and limitations could enhance the robustness and applicability of the proposed method in realworld settings.", "mhQLcMjWw75": " Peer Review  1) Summary The paper introduces a novel lightweight framework for Federated Learning (FL) that leverages multiple fixed pretrained models to replace learnable feature extractors for each client. The framework also incorporates Federated Prototypewise Contrastive Learning (FedPCL) to share knowledge among clients via class prototypes and contrastive learning. Extensive experiments are conducted to validate the proposed approachs ability to fuse representations effectively integrating knowledge from diverse backbones achieving better performance and showing robustness under varying levels of heterogeneity and different numbers of clients.  2) Strengths and weaknesses Strengths  Novelty: Introducing a lightweight FL framework integrating pretrained models is innovative and promising.  Performance: The proposed FedPCL demonstrates superior performance over baseline methods especially in heterogeneous settings and with varying numbers of clients.  Robustness: FedPCL shows stable convergence and robustness across different runs indicating its reliability.  Scalability: The framework adapts well to a large number of clients without a decline in performance demonstrating scalability. Weaknesses  Scope Limitation: The focus on vision tasks limits the generalizability of the findings to other domains like natural language processing.  Evaluation Metrics: While the results show improved performance further investigation into additional evaluation metrics beyond test accuracy could provide a more comprehensive assessment of the proposed framework.  3) Questions  Framework Extension: Have you considered extending the framework beyond vision tasks to domains like natural language processing  Privacy Concerns: How do you address potential privacy concerns when sharing class prototypes among clients  Realworld Applicability: Have you tested the proposed framework in realworld scenarios to assess its practical applicability and performance  4) Limitations  Task Scope: The narrow focus on vision tasks limits the broader implications and generalizability of the proposed approach.  Evaluation Metrics: The reliance on test accuracy as the primary evaluation metric may not capture the full spectrum of model performance.  Scalability Tests: While scalability is demonstrated more comprehensive scalability tests under different conditions could provide deeper insights into the models adaptability.  Realworld Validation: Lack of realworld validation could limit the practical utility of the proposed framework.  5) Overall Impression The paper presents a novel lightweight FL framework with FedPCL that shows promising results in improving model performance robustness and scalability. However there is a need for further exploration beyond vision tasks integration of additional evaluation metrics addressing privacy concerns and validation in realworld scenarios to enhance the impact and applicability of the proposed framework.", "nzuuao_V-B_": " Peer Review of the Scientific Paper  1) Summary The paper systematically reevaluates stateoftheart attack methods for input recovery in federated learning focusing on batch reconstruction. The study includes implicit and explicit model variations proposing an angular Lipschitz constant as a measure to explain model vulnerability against recovery attacks. The proposed measure shows a strong correlation with loss drop and reconstruction quality aiming to aid in developing clientside defensive strategies in a blackbox setting.  2) Strengths and Weaknesses  Strengths:  Thorough evaluation of attack methods on a broad spectrum of models in the context of batch reconstruction.  Introduction of the angular Lipschitz constant as a measure for vulnerability in a realistic federated learning setting.  Clear presentation of experimental results and correlations between proposed measure and quality of reconstructed samples.  Weaknesses:  Limited discussion on potential negative societal impacts of the proposed measure and its implications.  Lack of detailed information on participant consent and data privacy considerations especially in the context of privacy threats in federated learning.  3) Questions  Is the proposed angular Lipschitz constant applicable to various types of models and data distributions  How sensitive is the measure to variations in model architectures and input data characteristics  Can the proposed measure be extended to address more complex privacy concerns in federated learning environments  4) Limitations  While the paper mentions limitations related to the potential impact of its work and future directions more explicit discussions on broader societal implications and ethical considerations would be beneficial.  The reliance on certain assumptions such as knowledge of private labels and batch statistics might limit the generalizability of the proposed measure to realworld scenarios.  Overall Assessment The paper presents a rigorous reevaluation of gradient inversion attacks in federated learning offering a novel angular Lipschitz constant measure to assess model vulnerability. The experimental results and correlations provide valuable insights into the effectiveness of the measure. However further exploration into the generalizability and practical implications of the proposed measure would enhance the significance of the research in the broader context of privacy protection in federated learning.", "xT5rDp5VqKO": " Peer Review  1) Summary: This paper explores the performance of coincidence detection a classic neuromorphic signal processing method in comparison to a deep learning method for pattern recognition. By applying coincidence detection with distributed transmission times the authors find that it can be competitive with stateoftheart deep learning for specific tasks. The study emphasizes the potential of employing more advanced continual learning methods on spiking neural network hardware to surpass the accuracy of taskspecific deep learning methods.  2) Strengths and Weaknesses:  Strengths:  The paper effectively presents the historical context and relevance of coincidence detection in neuromorphic signal processing.  The comparison between coincidence detection and a deep learning method is welldefined.  The method of coincidence detection through normalized logistic regression is clearly explained.  Results show that the proposed method achieved higher accuracy than the deep learning approach within a short training time.  Weaknesses:  The paper lacks detailed insights into the experimental setup such as specific hyperparameters and data splits that could provide more context for the results.  The implications of the accuracy improvement over the deep learning method should be further discussed.  The discussion on the limitations of the study should be more elaborate to provide a comprehensive view of the research.  3) Questions:  How does the proposed method of coincidence detection outperform deep learning in this specific pattern recognition task Is there a particular aspect of this task where coincidence detection excels  Can the authors elaborate on the potential drawbacks of using coincidence detection compared to deep learning methods especially in scenarios beyond the study\u2019s scope  What are the implications of the study for the broader field of neuromorphic computing and artificial intelligence applications in realworld scenarios such as infection detection as mentioned in the Introduction  4) Limitations:  The paper would benefit from a more detailed discussion on the generalizability of the findings to other datasets or tasks beyond the pathogenic bacteria classification considered.  The study lacks an indepth analysis of the computational efficiency and scalability aspects of coincidence detection compared to deep learning approaches which are essential considerations for practical deployment. In conclusion the paper provides valuable insights into the potential of coincidence detection for pattern recognition tasks but further clarification on experimental details broader implications and limitations would strengthen the overall impact of the study. Would you like to make any modifications or further explanation to any of the sections outlined in the peer review", "zbuq101sCNV": " Peer Review: 1. Summary: The paper introduces TANGO a framework for textdriven photorealistic 3D style transfer. By disentangling appearance style into reflectance properties geometric variation and lighting conditions TANGO learns to stylize 3D meshes based on text prompts achieving photorealistic results without specific task training. Extensive experiments show TANGO outperforms existing methods in terms of quality consistency and robustness. The ablation study and comparisons with Text2Mesh demonstrate the effectiveness and superiority of TANGO in various scenarios. 2. Strengths:  Innovative Approach: TANGOs disentanglement of appearance style components and utilization of CLIP supervision for style transfer is innovative and shows promising results.  Photorealistic Quality: The paper showcases TANGOs ability to generate photorealistic renderings of 3D meshes surpassing existing methods in quality and consistency.  Robustness: TANGO demonstrates robustness when stylizing lowquality meshes which is a significant advantage compared to other approaches.  Ablation Study: The conducted ablation study provides valuable insights into the importance of each component in TANGO demonstrating their impact on stylization performance.  Comparative Analysis: Comparisons with Text2Mesh and user evaluation provide a comprehensive understanding of TANGOs strengths and superiority in various aspects of 3D style transfer. 3. Weaknesses:  Limitation in Shading Model: The simplified shading model may limit TANGOs capability to represent complex lighting effects in large scenes indicating a potential area for improvement.  Vertex Displacement Comparison: The paper acknowledges the geometry capacity limitation compared to vertex displacement methods highlighting a difference in performance capacity that needs further exploration.  Junior Artist Impact: The potential impact on junior artists due to TANGOs ability to automate part of the manual work should be discussed further considering implications for practitioners in the field. 4. Questions:  Complex Lighting Effects: How feasible would it be to extend TANGO to simulate more complex lighting scenarios beyond the current shading model and what steps could be taken to achieve this  Vertex Displacement vs. Normal Displacement: Can the paper provide more insights into the specific scenarios where normal displacement excels over vertex displacement and how these methods can complement each other for diverse stylization tasks  Social Implications: How does the potential automation impact on artists and practitioners in the 3D graphics field and what considerations should be taken to ensure a balance between automation and human creativity 5. Limitations:  Simplified Shading Model: The paper mentions limitations in representing complex lighting effects which could impact TANGOs applicability in scenarios requiring advanced lighting simulations.  Comparison with Vertex Displacement: The study acknowledges a difference in geometry capacity compared to vertex displacement methods potentially affecting performance in certain stylization tasks.  Social Implications: The potential impact on the employment landscape in 3D graphics creation due to TANGOs automation features may need further exploration and discussion. Overall the paper presents a significant advancement in textdriven 3D style transfer with TANGO demonstrating remarkable results in photorealistic quality and robustness. Further exploration into complex lighting representations and societal implications of automation in the field could enhance the papers contribution and relevance.", "lJx2vng-KiC": "Peer Review Report Summary: The paper investigates the impact of prior knowledge on learning dynamics in deep linear networks by deriving exact solutions to the learning dynamics for rich prior knowledge settings. The authors provide analytical expressions for network function evolution hidden representational similarity and neural tangent kernel through training under various initializations and tasks. The study uncovers taskindependent initializations leading to drastic changes in learning dynamics dissociating learning trajectories from initial representations structure. The results offer insights into how network weights align with task structure and present implications for continual learning reversal learning and structured knowledge learning. The findings aim to contribute to a better understanding of the role of prior knowledge in deep learning. Strengths: 1. Theoretical Contributions: The paper makes significant theoretical contributions by providing explicit solutions for learning dynamics in deep linear networks with rich prior knowledge. The results offer insights into how network weights evolve with different initializations and tasks. 2. Rigorous Analysis: The study presents exact mathematical solutions that enable a thorough understanding of the impact of prior knowledge on learning trajectories in deep linear networks. 3. Relevance: Given the importance of prior knowledge in deep learning the results presented in the paper address a crucial gap in understanding the influence of initial network state on learning dynamics. Weaknesses: 1. Complexity: The theoretical formulations and derivations presented in the paper may be challenging for readers without a strong mathematical background to grasp fully. The paper could benefit from providing more intuitive explanations along with the mathematical derivations. 2. Scope: While the study focuses on deep linear networks it would be valuable to discuss the implications of the findings on more complex deep learning architectures such as nonlinear networks to enhance the studys applicability beyond the linear context. Questions: 1. Could the authors provide more insights into how the findings of this study can be translated to practical applications in deep neural network training particularly in nonlinear networks 2. Considering the extensive mathematical derivations in the paper how accessible is the content to readers with varying levels of mathematical expertise 3. Have the authors considered potential extensions or future research directions based on the results obtained in this study Limitations: One of the limitations of the study is the focus on deep linear networks which might restrict the generalizability of the findings to more complex neural network architectures. Additionally the reliance on certain assumptions in the theoretical derivations could limit the applicability of the results to realworld scenarios. Further discussion on the practical implications and potential challenges of applying the results in more diverse deep learning settings could enhance the papers impact. In conclusion the paper presents a rigorous theoretical analysis of learning dynamics in deep linear networks with rich prior knowledge offering valuable insights into the interplay between initializations and learning trajectories. Addressing the weaknesses and limitations identified could enhance the papers clarity and relevance to a broader audience.", "zKBbP3R86oc": "Peer Review 1) Summary (avg word length: 100) The paper introduces SMOL a method for training deep networks with varying precisions for weights and hidden state activations to reduce energy consumption and inference time. They show significant improvements in quantization for CNNs trained on CIFAR and ImageNet and GANs. The approach is based on optimizing noise magnitudes which represent precisions allowing for a more efficient model construction through the entire training process. 2) Strengths and Weaknesses Strengths:  Novel and comprehensive approach to quantizing weights and activations with varying precisions.  Improvements in model efficiency and inference time demonstrated across different datasets and tasks.  The method is modular allowing for easy integration with different optimization algorithms or architectures. Weaknesses:  While the results are promising more extensive comparative experiments with other stateoftheart methods could provide a stronger case for the effectiveness of SMOL.  The paper could benefit from a more detailed explanation of the exact computational overheads involved in implementing the proposed method on specialized hardware. 3) Questions  How does SMOL compare in terms of computational complexity during training compared to traditional quantization methods  Are there constraints or limitations to the size or complexity of models that can effectively benefit from the heterogeneous precision allocation proposed by SMOL  Can the method be applied to other types of neural networks beyond CNNs and if so how might the results differ 4) Limitations  The paper lacks extensive analysis on the generalizability and robustness of the proposed method across a wider range of tasks and datasets.  The energy efficiency claims could be further substantiated with empirical evidence on real hardware implementations. Overall the paper presents an innovative approach to network quantization with promising results. Extending the evaluation and addressing the points raised could further strengthen the impact and applicability of the proposed method.", "lQ--doSB2o": " Peer Review Title: FeatureLevel Adversarial Attacks in Computer Vision: Robustness and Interpretability  1) Summary: The paper introduces a novel approach to adversarial attacks in computer vision by focusing on featurelevel perturbations rather than pixellevel perturbations. The authors demonstrate that these featurelevel attacks are versatile robust and interpretable providing insights into deep network representations. The study presents three main contributions: the use of featurelevel attacks for studying network representations the introduction of versatile and robust featurelevel attacks at the ImageNet scale and the practical application of adversarial images as an interpretability tool for identifying network bugs. Through experiments and analysis the authors show the effectiveness of these featurelevel attacks in generating targeted disguised physicallyrealizable and blackbox attacks. Additionally they validate the interpretability benefits of these attacks through copypaste attacks which aid in understanding network vulnerabilities. 2) Strengths and weaknesses: Strengths:  The paper addresses an important issue in computer vision research by exploring featurelevel adversarial attacks which offer robustness and interpretability.  The approach presented in the paper is innovative and welldocumented with clear methodology and detailed experiments.  The use of ImageNet scale models and the practical application of adversarial images for interpretability make the study relevant to realworld applications.  The validation of interpretability through copypaste attacks showcases the practical utility of the proposed method. Weaknesses:  Although the study demonstrates the effectiveness of featurelevel attacks a more extensive comparison with existing methods could provide a clearer understanding of the novelty of the proposed approach.  The limitations of the approach such as the challenges in optimizing multiple desiderata simultaneously could be further elaborated to provide a more comprehensive view of the methods practical usability.  The paper could benefit from a clearer discussion on potential ethical implications and considerations when using adversarial attacks in practical applications. 3) Questions:  How does the proposed method compare in terms of computational efficiency and scalability when generating featurelevel attacks on ImageNet scale models compared to existing pixellevel perturbations  Can the interpretability benefits of featurelevel attacks extend to other domains beyond computer vision such as natural language processing as suggested in the study  What are the potential implications of deploying featurelevel attacks in realworld scenarios especially in safetycritical applications where interpretability and robustness are paramount 4) Limitations:  The studys evaluation method while promising may be limited in its proofofconcept nature for copypaste attacks. Further investigation and validation of interpretability tools are warranted.  The tradeoffs and challenges in simultaneously optimizing multiple desiderata for featurelevel attacks may hinder largescale adversarial training and require more stringent screening.  The reliance on the efficiency and quality of the generator in the proposed method could limit the scalability and generalizability of the approach warranting future work in leveraging advancements in generative modeling. In conclusion the paper presents a compelling study on featurelevel adversarial attacks in computer vision offering valuable insights into robustness and interpretability. The innovative approach and practical applications showcased in the study contribute significantly to the field of adversarial attacks and deep network understanding.  This peer review provides an overview of the papers strengths weaknesses potential questions and limitations offering valuable feedback to enhance the studys clarity and depth.", "um2BxfgkT2_": " Peer Review:  1) Summary: The paper introduces Tokenized Graph Transformer (TokenGT) showcasing that standard Transformers applied directly to graphs can yield promising results both theoretically and empirically. The method involves treating nodes and edges as independent tokens augmenting them with appropriate embeddings and feeding them to a Transformer. Theoretical analysis demonstrates that TokenGT is at least as expressive as specific graph network architectures and can approximate equivariant linear operations on graphs. Empirical evaluation on the PCQM4Mv2 dataset reveals significant improvement over baseline GNN models and competitive performance compared to Transformer variants with graphspecific modifications.  2) Strengths and Weaknesses: Strengths:  Theoretical Analysis: The paper provides a solid theoretical foundation showing the expressive power of TokenGT in comparison to existing graph neural network architectures.  Empirical Results: Demonstrates superior performance of TokenGT on the PCQM4Mv2 dataset compared to GNN baselines and competitive results against Transformer variants with graphspecific inductive bias.  Novel Approach: Introduces a straightforward yet powerful method of applying Transformers to graphs without complex graphspecific modifications.  Code Availability: Sharing the implementation on GitHub promotes reproducibility and facilitates future research. Weaknesses:  Scalability Concerns: The quadratic nature of selfattention could limit scalability for large graphs though addressed with kernelization further exploration into more efficient Transformers would be beneficial.  Performance Gap: While competitive there is room for improvement to match or surpass stateoftheart results in graph learning tasks.  Complexity: The method may lack interpretability due to the blackbox nature of selfattention and further research on explaining the learned structures would be valuable.  3) Questions:  How does the choice of token embeddings impact the models performance and interpretability  Could TokenGT benefit from incorporating additional structural information from graphs beyond node and type identifiers to further enhance performance  What are the implications of applying TokenGT to tasks requiring highlevel interpretability or decisionmaking based on graph inputs  4) Limitations:  The paper focuses on theoretical guarantees and empirical results primarily on the PCQM4Mv2 dataset limiting generalizability to other graph learning tasks or domains.  Despite the improved performance TokenGT still falls slightly short of the stateoftheart methods suggesting further optimizations are required.  The evaluation could benefit from more diverse and challenging datasets to assess the robustness and applicability of TokenGT across various graph structures. Overall the paper presents an innovative approach with strong theoretical underpinnings and promising empirical results in graph learning. Addressing scalability concerns and improving interpretability could further enhance the utility and impact of Tokenized Graph Transformer in realworld applications.", "wTp4KgVIJ5": " Peer Review 1. Summary The paper introduces a new algorithmic framework called SAGDA for federated minmax learning to address the challenge of high communication complexity. The proposed method aims to achieve a linear speedup in terms of the number of clients and local update steps leading to significantly lower communication complexity compared to existing approaches. The authors also provide theoretical results and conduct numerical experiments to demonstrate the effectiveness and efficiency of SAGDA. 2. Strengths and weaknesses Strengths  The paper addresses an important challenge in federated minmax learning namely high communication complexity which is crucial for practical applications.  The proposed SAGDA algorithm is wellmotivated and backed by theoretical analysis demonstrating its potential to achieve significant improvements in communication efficiency.  The incorporation of numerical experiments using realworld datasets adds empirical validation to the theoretical findings enhancing the credibility of the proposed approach. Weaknesses  Although the proposed SAGDA algorithm shows promising results in terms of communication complexity more detailed discussions on the limitations of the method and potential scenarios where it may not perform optimally could provide a more comprehensive understanding.  The paper could benefit from a more detailed comparison with existing stateoftheart methods in terms of convergence rates scalability and robustness to various settings and datasets to highlight the uniqueness of SAGDA. 3. Questions  How sensitive is the performance of SAGDA to the choice of hyperparameters such as the learning rates and local update steps Have you explored the robustness of the algorithm to different hyperparameter settings in your experiments  Are there specific scenarios or dataset characteristics where SAGDA may not exhibit the desired improvements in communication complexity compared to existing methods How does SAGDA perform under nonideal conditions  Can the proposed SAGDA algorithm be easily implemented and scaled in realworld federated learning systems Are there any practical challenges or computational overhead associated with deploying SAGDA in largescale distributed environments 4. Limitations  The paper primarily focuses on the communication complexity aspect of federated minmax learning and while the proposed SAGDA algorithm shows promising results in this regard a more comprehensive analysis of other factors such as convergence speed model accuracy and robustness could provide a more holistic evaluation of the approach.  The experimental validation is conducted on a limited set of datasets and scenarios. Further investigation on a more diverse range of datasets with varying characteristics could provide additional insights into the generalizability and applicability of SAGDA across different settings. In conclusion the paper presents a novel algorithmic framework for addressing communication complexity in federated minmax learning showing promising results through theoretical analysis and experimental validation. It contributes valuable insights to the field of federated learning but could benefit from further exploration of potential limitations and more extensive empirical evaluations.", "kS5KG3mpSY": "Peer Review: 1) Summary: This paper introduces a novel method for learning an energybased model (EBM) towards more expressive prior models in the latent space of a generator model. The proposed adaptive multistage density ratio estimation involves breaking density estimation into multiple stages enabling more efficient and accurate learning of the EBM. The method is demonstrated through experiments showcasing strong performance in image generation reconstruction and anomaly detection tasks. 2) Strengths and Weaknesses: Strengths:  The proposed adaptive multistage density ratio estimation method for learning latent space EBM prior is innovative and addresses the limitations of existing methods.  The experiments conducted cover a wide range of tasks including image synthesis reconstruction and anomaly detection demonstrating the versatility and effectiveness of the proposed method.  Detailed explanations and comparisons with related works provide a comprehensive understanding of the novel approach.  The paper presents both qualitative and quantitative results enhancing the rigor of the study. Weaknesses:  The paper could benefit from more indepth analysis of limitations and potential challenges in implementing the proposed method in realworld scenarios.  Providing a discussion on the scalability and computational efficiency of the proposed method would be beneficial for practical applications.  The paper could improve by discussing the generalizability of the method to different datasets and domains beyond image data. 3) Questions:  Could you elaborate on the computational efficiency and scalability of the proposed method especially in larger datasets or different domains  How does the adaptive multistage density ratio estimation approach compare to existing techniques in terms of parameter efficiency and training complexity  Have you explored the robustness of the proposed method to noisy or incomplete data and if so what were the findings 4) Limitations:  The study primarily focuses on image data limiting the generalizability of the proposed method to other types of data such as text or graphs.  The paper lacks discussion on potential challenges or limitations in implementing the proposed method in practice such as data distribution assumptions or noise in realworld applications.  The discussion on the scalability and computational efficiency of the method could have been more detailed to provide insights for practical implementation. Overall the paper presents a novel and effective method for learning energybased models in the latent space of a generator model with strong experimental results supporting the proposed approach. Further exploration of scalability generalizability and potential limitations would enhance the impact and applicability of the method in various domains.", "tbId-oAOZo": " Peer Review of \"QueryPose: Sparse EndtoEnd MultiPerson Pose Regression\"  Summary: The paper proposes QueryPose a sparse endtoend multiperson pose regression framework that directly predicts multiperson keypoint sequences from input images. It introduces Spatial Part Embedding Generation Module (SPEGM) and Selective Iteration Module (SIM) to encode spatial details and structural information for precise keypoint regression. The proposed method achieves impressive results outperforming existing dense endtoend methods on MS COCO and CrowdPose datasets.  Strengths: 1. Innovative Approach: The introduction of sparse learnable spatialaware part queries to encode local spatial features is a novel and promising approach for multiperson pose regression. 2. Effectiveness of Modules: The SPEGM and SIM modules enhance the spatial details and structural information leading to improved performance without the need for complex postprocesses. 3. Comprehensive Experiments: The detailed ablation studies and comparison with existing methods on MS COCO and CrowdPose datasets demonstrate the effectiveness and superiority of QueryPose. 4. Efficient Inference: QueryPose eliminates timeconsuming postprocesses and achieves faster inference speeds compared to dense counterparts making it practical for realtime applications.  Weaknesses: 1. Comprehensibility: The paper is highly technical and may be challenging for readers unfamiliar with the field to grasp the intricacies of the proposed method and modules. 2. Assumptions and Limitations: The effectiveness of QueryPose is largely demonstrated with HRNetW48 and SwinLarge backbones. The generalizability of the method across different architectures should be further explored.  Questions: 1. How adaptable is QueryPose to complex realworld scenarios with occlusions varying poses and background clutter 2. Can QueryPose handle instances with irregular shapes or uncommon poses effectively 3. Are there specific limitations or failure cases that need to be addressed for QueryPose to be more robust in practical scenarios  Limitations: 1. Network Dependency: The performance of QueryPose is extensively evaluated with specific network architectures (HRNetW48 SwinLarge). Further evaluation on a wider range of architectures could enhance the generalizability of the proposed method. 2. Scalability: The scalability of QueryPose to handle largescale datasets and realworld scenarios with a high number of individuals in the image remains to be investigated. 3. External Factors: The impact of external factors such as light conditions diverse environments and image quality on QueryPoses performance needs to be explored for realworld deployment. Overall \"QueryPose: Sparse EndtoEnd MultiPerson Pose Regression\" presents a novel approach to multiperson pose regression with promising results. Further exploration and validation under diverse conditions are recommended to enhance the robustness and applicability of QueryPose in realworld scenarios.", "q4IG88RJiMv": " Peer Review: 1) Summary (avg word length 100) This paper introduces a novel methodology linking fdivergences to ROC curves by demonstrating the arc length of optimal ROC curve as an fdivergence. The study formulates a variational objective for estimating arc length and presents results with high convergence rates. The research also investigates the surface area between the ROC curve and the diagonal using similar techniques. Practical applications and experiments on CIFAR10 datasets suggest promising AUC performance in imbalanced binary classification tasks. 2) Strengths: The paper demonstrates a novel connection between fdivergences and ROC curves providing new insights and methodologies in classification tasks. The theoretical derivations and algorithmic procedures for estimating arc length and maximizing AUC are welldetailed. The numerical experiments on CIFAR10 datasets showcase the efficacy of the proposed approach in imbalanced classification scenarios. The paper is wellstructured and engages in a systematic exploration of the topic. Weaknesses:  Lack of Clarity: Some sections in the paper may be challenging to follow due to the complex mathematical derivations and technical terms. Simplifying some explanations may enhance understanding for readers.  Completeness of Experiments: While the experiments on the CIFAR10 datasets show promising results a comparison with additional stateoftheart methods or benchmarks would further strengthen the evaluation of the proposed approach. Providing more insights into the limitation of the approach would be beneficial for the readers. 3) Questions:  Could the methodology proposed in the paper be extended to other datasets beyond CIFAR10 and how generalizable is it across different classification problems  Are there any specific restrictions or assumptions in the study that might limit the applicability of the approach to realworld scenarios especially in cases of highdimensional datasets or diverse data distributions  How does the proposed twostep procedure compare in terms of computational efficiency and scalability particularly when applied to larger datasets or in realtime classification tasks 4) Limitations:  The generalizability of the proposed methodology to diverse datasets and complex classification scenarios should be further investigated to identify potential limitations.  The assumptions made in the theoretical derivations and experiments may not entirely represent realworld data settings indicating a need for additional validation on different datasets with varying characteristics.  The computational complexity of the proposed algorithms should be carefully considered especially in scenarios where realtime processing or largescale datasets are involved. In conclusion the paper presents an innovative approach linking fdivergences and ROC curves offering significant contributions to the field of classification and performance evaluation. Further refinement in the clarity of presentation expanded experimentation in varying scenarios and addressing potential limitations will enhance the impact and practical applicability of the research.", "v2es9YoukWO": " Peer Review  1) Summary: The paper introduces the Super Kernel Flow Network (SKFlow) for optical flow estimation emphasizing handling occlusions to improve accuracy. SKFlow leverages super kernels with enlarged receptive fields to recover occluded motions efficiently. The proposed architecture demonstrates compelling performance ranking 1st on the Sintel benchmark and showcasing significant improvements in occluded areas. The paper includes detailed experiments ablation studies and comparisons with existing methods providing a thorough analysis of model efficiency occlusion handling and generalization.  2) Strengths and weaknesses: Strengths:  Clear problem statement and motivation.  Novel approach with the introduction of super kernels for optical flow estimation.  Comprehensive experiments demonstrating the effectiveness of SKFlow especially in handling occlusions.  Detailed analysis with ablation studies visualization of results and comparisons with stateoftheart methods.  Availability of code and acknowledgements for funding support. Weaknesses:  Limited discussion on the potential challenges or drawbacks of the proposed method.  Further elaboration on the scalability of SKFlow to realworld applications could provide more context.  3) Questions:  Can you provide insights into the computational efficiency of SKFlow in realtime applications  How does the generalization performance of SKFlow compare across different datasets beyond Sintel and KITTI  Are there any specific scenarios where SKFlow may struggle or exhibit limitations in optical flow estimation tasks  4) Limitations: The paper presents valuable contributions to the field of optical flow estimation especially in handling occlusions. However there are few limitations that could be addressed for further improvement:  Realworld applicability: Further validation on realworld datasets or scenarios could strengthen the practicality of the proposed method.  Interpretability and robustness: Insights into the interpretability of SKFlows predictions and robustness to noise or extreme conditions could enhance its reliability.  Scalability and deployment: Discussion on the scalability of SKFlow for deployment in realtime systems or resourceconstrained environments would add practical value. Overall the paper offers a significant contribution to the field of optical flow estimation particularly in addressing occlusions. Addressing the identified limitations could further enhance the impact and practicality of SKFlow in various applications.", "vjKIKdXijK": " Peer Review:  1) Summary: The paper presents a novel implementation of the Hessian approach for certifying convexity of functions and compares it with the disciplined convex programming (DCP) approach. The Hessian approach is applied to a class of functions with vector input and is shown to be at least as powerful as the DCP approach for certifying convexity. The study demonstrates the implementation of the Hessian approach for a rich class of functions showcases algorithm examples and discusses the limitations of existing generic approaches for certifying convexity.  2) Strengths and weaknesses: Strengths:  The paper addresses an important problem in optimization and machine learning  certifying convexity.  The study provides a detailed comparison between the Hessian and DCP approaches showcasing their respective strengths and limitations.  The implementation of the Hessian approach is demonstrated through relevant examples and algorithm descriptions providing clarity to readers.  The proposed method is shown to cover functions that cannot be certified as convex by traditional approaches highlighting its potential applicability in various scenarios. Weaknesses:  The paper lacks practical applications or case studies to demonstrate the realworld relevance of the proposed Hessian approach.  The complexity of the mathematical and computational methods discussed may limit accessibility for readers without a strong background in optimization and machine learning.  Further experimental validation on a diverse set of problems would strengthen the credibility and generalizability of the proposed approach.  3) Questions:  How does the proposed Hessian approach compare in terms of computational efficiency and scalability when applied to largescale optimization problems  Are there specific types of functions or domains where the Hessian approach may outperform other existing convexity certification methods  Can the proposed approach be extended to handle nonconvex optimization problems or other complex optimization scenarios  4) Limitations:  The paper primarily focuses on theoretical aspects and algorithmic descriptions lacking empirical evaluations or practical demonstrations.  The study assumes a strong mathematical background from the readers which might limit the accessibility to a broader audience.  It would be beneficial to explore the robustness and reliability of the Hessian approach under noisy or uncertain conditions typically encountered in practical optimization tasks. Overall the paper presents a valuable contribution to the field of convex optimization and machine learning by introducing a novel approach for certifying convexity using the Hessian of functions. Addressing the questions and limitations highlighted could enhance the clarity applicability and impact of the proposed method.", "pIYYJflkhZ": " Peer Review: 1) Summary: The paper introduces SoftPatch a memorybased unsupervised anomaly detection method that addresses labellevel noise in image sensory anomaly detection. The proposed method efficiently denoises data at the patch level using noise discriminators resulting in improved anomaly detection performance in the presence of noisy data. Extensive experiments demonstrate that SoftPatch outperforms existing methods on benchmark datasets and maintains strong modeling ability of normal data. The paper highlights the significance of studying noisy data problems in unsupervised anomaly detection and offers contributions in the field. 2) Strengths and weaknesses: Strengths:  The paper identifies and addresses the issue of noisy data in unsupervised anomaly detection.  The proposed SoftPatch method effectively denoises data at the patch level leading to improved anomaly detection performance.  Extensive experiments showcase the superior performance of SoftPatch compared to existing methods in various noise scenarios.  The paper provides indepth explanations of noise discriminators and soft weight mechanisms contributing to a clear understanding of the proposed method. Weaknesses:  The paper could benefit from a more comprehensive comparison with a broader range of existing anomaly detection methods.  The discussion on the limitations of the proposed method could be further expanded to provide a more holistic view of the research area.  Consideration of realworld deployment challenges and scalability of SoftPatch in practical settings could be explored further. 3) Questions:  Could the authors elaborate on the computational efficiency of SoftPatch compared to existing methods especially in largescale datasets  How does the proposed method generalize across different types of anomalies beyond the ones studied in the experiments  Are there any plans to extend the research to address additional challenges in unsupervised anomaly detection such as dynamic or evolving anomalies 4) Limitations:  The paper primarily focuses on noise in image sensory anomaly detection potentially limiting the scope to other types of data or applications.  The generalizability of SoftPatch to diverse datasets with varying noise characteristics might be a limitation that needs further investigation.  The evaluation on only a few benchmark datasets may restrict the broader applicability and robustness assessment of SoftPatch in realworld scenarios. Overall the paper makes a significant contribution by addressing the issue of noisy data in unsupervised anomaly detection and introducing an effective method SoftPatch. The results presented in the paper demonstrate promising advancements in improving anomaly detection performance especially in the presence of noise. Further exploration of the practical implications and scalability of SoftPatch would enhance the impact and relevance of the research in the field of anomaly detection.", "zuL5OYIBgcV": " Peer Review:  1) Summary: The paper presents a novel approach to address the issue of latency in neural networks by introducing \"ParNet\"  a nondeep network architecture using parallel substructures. The research demonstrates that highperforming networks can be designed with a low depth of just 12 layers. ParNet achieves impressive accuracies on ImageNet CIFAR10 CIFAR100 and MSCOCO datasets while maintaining a reduced depth. Various architectural choices scaling rules and optimization strategies are discussed in detail providing insights into the performance and efficiency of ParNet.  2) Strengths and Weaknesses: Strengths:  The paper introduces a unique concept of parallel subnetworks to reduce depth while maintaining high performance.  ParNet achieves remarkable accuracies on multiple benchmark datasets with significantly lower depth compared to existing networks.  Detailed analysis of scaling rules architectural choices and performance comparisons with existing models.  Ablation studies and ensemble comparisons provide valuable insights into the effectiveness of ParNet.  The exploration of nondeep networks opens up new possibilities for lowlatency recognition systems in safetycritical applications. Weaknesses:  The paper lacks a comprehensive analysis of the computational complexity and memory requirements of ParNet which is crucial for practical implementations.  More insights into the tradeoffs between depth reduction and parameter efficiency could add depth to the discussion.  The limitation section could be expanded to include potential challenges in realworld deployment and generalizability to diverse datasets.  Further discussion on the scalability and applicability of ParNet to different domains could enhance the relevance of the research.  3) Questions:  How does the efficiency of ParNet in terms of inference time compared to existing deep networks especially in scenarios with limited computational resources  Are there specific use cases or applications where the nondeep architecture of ParNet offers a significant advantage over deep networks other than latency reduction  How does the performance of ParNet vary with different types of datasets especially those with high variability or complexity  Could the authors elaborate on the potential impact of ParNet on emerging technologies such as edge computing or IoT devices  4) Limitations: An important limitation of the paper is the lack of detailed exploration into the computational and memory efficiency of ParNet particularly in resourceconstrained environments. Realworld deployment scenarios may require a more comprehensive evaluation of the energy efficiency and scalability aspects of ParNet. Additionally the generalizability of ParNet to diverse datasets and tasks remains a limitation that could be addressed through further experimentation and comparative analysis with a wider range of benchmarks. The focus on specific datasets like ImageNet CIFAR and MSCOCO limits the broader applicability of the findings and including more diverse datasets could provide a more robust assessment of ParNets performance in varied contexts. Overall the research presents a novel approach to designing highperforming nondeep neural networks with the potential to address latency concerns in safetycritical systems. Further exploration of the practical implications scalability and generalizability of ParNet would enhance the impact and relevance of the findings in the field of neural network architecture design.", "yP0vpghGoLF": "Peer Review Summary: The paper introduces a novel analysis through potential functions to show that lastiterate convergence rates of the Past Extragradient (PEG) method can be achieved without assumptions on the Jacobian of the operator. The study extends the results to the constrained case and addresses the last iterate convergence of PEG for monotone variational inequalities. The work provides theoretical insights using proofs and numerical experiments to support the findings. Strengths: 1. Innovative Approach: The paper introduces a novel analysis through potential functions to address the lastiterate convergence rates of PEG filling a gap in the literature. 2. Theoretical Contributions: The study provides rigorous proofs and theoretical analysis to demonstrate convergence rates without assumptions on the Jacobian of the operator. 3. Extensive Analysis: The thorough discussion on constrained and unconstrained cases along with numerical experiments enhances the credibility and depth of the study. 4. Clarity: The paper is wellstructured with clearly delineated sections making it easy to follow the methods and results. Weaknesses: 1. Numerical Experiments: While the numerical experiments support the theoretical findings some readers might seek a more detailed explanation of the experimental methodology and assumptions made. 2. Complexity: The study involves advanced mathematical concepts and techniques which may make it less accessible to readers with limited expertise in optimization methods. 3. Simplified Proof: The authors acknowledge that the derived upper bounds are worse than those obtained numerically implying room for further refinement of the proofs. Questions: 1. Could the authors provide more insights into the practical implications of the findings especially in the context of machine learning applications 2. How do the convergence rates and results of PEG in the unconstrained and constrained cases compare with existing optimization methods in practical scenarios 3. Are there specific assumptions or requirements regarding the problem structure or operator characteristics for the derived convergence rates to hold Limitations: 1. The study primarily focuses on theoretical analysis and less on practical applications limiting the generalizability of the findings. 2. The complexity of the mathematical derivations and methods used may pose a barrier to understanding for readers without a strong background in optimization theory. 3. The paper mainly addresses lastiterate convergence rates of PEG leaving scope for future research on broader optimization perspectives and applicationspecific analyses. Overall the paper makes a significant contribution to the optimization community by addressing lastiterate convergence rates of the PEG method and showcasing the value of potential functions in theoretical analysis. Further clarity on practical implications and additional insights into the broader relevance of the findings could enhance the papers impact.", "nX-gReQ0OT": " Peer Review:  Summary: The paper presents a novel deeplearning architecture combined with Monte Carlo methods to solve the Schr\u00f6dinger equation with high accuracy and moderate computational cost. The architecture achieves significant improvements in energy error reduction and computational cost compared to previous approaches. The paper systematically analyzes the improvements focusing on the impact of increasing physical prior knowledge. The results show the potential for achieving highly accurate variational ground state energies for various atoms and molecules.  Strengths: 1. Innovative Approach: The paper introduces a novel deeplearning architecture that significantly improves accuracy and computational cost in solving the Schr\u00f6dinger equation. 2. Systematic Analysis: The work provides a comprehensive breakdown of the improvements made highlighting the impact of increasing physical prior knowledge. 3. Benchmark Results: The achieved results in terms of accuracy and computational efficiency set a new benchmark for variational ground state energies in electronic structure calculations.  Weaknesses: 1. Complexity: The paper delves deep into technical details potentially making it challenging for nonexperts to grasp the concepts presented. 2. Prior Knowledge Impact: The effect of increasing physical prior knowledge on model accuracy seems to have mixed results raising questions on the optimal level of prior knowledge incorporation.  Questions: 1. Can you elaborate on the reasons behind the counterintuitive finding that increasing physical prior knowledge can decrease accuracy 2. How does the proposed architecture compare to existing methods in terms of scalability for large systems 3. What are the potential implications of the observed reduction in energy errors for practical applications in computational chemistry  Limitations: 1. Computational Requirements: Despite improvements the method still requires significant computational resources limiting its applicability for largescale simulations. 2. Model Bias: The incorporation of extensive physical prior knowledge can introduce biases that hinder optimization and accuracy posing a challenge for model training. Overall the study presents a significant advancement in solving the Schr\u00f6dinger equation using deep learning and Monte Carlo methods. Addressing the identified weaknesses and limitations could further enhance the impact and applicability of the proposed architecture in computational chemistry research.", "p9zeOtKQXKs": " Peer Review: 1) Summary (108 words) The paper introduces a unified framework for studying metagradient estimations in Gradientbased Meta Reinforcement Learning (GMRL). It identifies and analyzes biases in existing stochastic metagradient estimators used in GMRL methods including compositional bias and multistep Hessian bias. The paper conducts empirical studies on tabular MDPs Iterated Prisoner\u2019s Dilemma and Atari games to validate theoretical insights and proposed bias correction methods. The results demonstrate the impact of biases on metagradient estimations and the effectiveness of bias correction techniques like offpolicy learning and lowbias estimators in improving RL performance. 2) Strengths and Weaknesses Strengths:  The paper addresses an important issue of biases in metagradient estimators in GMRL methods offering a unified framework for analysis.  The theoretical insights provided on compositional bias and multistep Hessian bias are thorough and supported by empirical studies on various RL environments.  The empirical validation of bias correction methods like offpolicy learning and lowbias estimators demonstrates the practical impact of bias in GMRL algorithms. Weaknesses:  The paper could benefit from clearer presentation of the experimental results potentially with more visual aids to enhance the understanding of the empirical evaluations.  The impact of biases on specific algorithms or in different learning scenarios could be further explored to provide a more nuanced understanding of the problem.  The paper could elaborate more on how the proposed bias correction methods impact the overall learning process and how they compare to existing techniques in the field. 3) Questions  What are the implications of the identified biases on the overall performance of GMRL algorithms and how do they influence the generalizability of such methods to diverse RL environments  How do the proposed bias correction methods compare to existing techniques used in RL for mitigating estimation biases and are there specific scenarios where one method might be more effective than others  Could the paper delve deeper into the implications of biases on the convergence sample efficiency and stability of metaRL algorithms especially in complex realworld applications 4) Limitations  The paper focuses on theoretical analysis and empirical validation of bias in metagradient estimation however the impact of biases on practical applications and realworld RL problems may vary.  The study primarily assesses bias correction techniques in controlled RL settings and further evaluation in complex multiagent environments or with more challenging RL tasks may be needed for a comprehensive analysis.  The paper lacks an indepth discussion on the potential tradeoffs or computational costs associated with implementing bias correction methods in GMRL algorithms which could be important factors for adoption in practical systems. Overall the paper offers valuable insights into bias in metagradient estimators in GMRL algorithms and presents effective bias correction methods. Further exploration of the practical implications of biases and a comparison with existing techniques could enhance the relevance and applicability of the proposed framework. Additional clarity in presenting experimental results and addressing potential limitations will strengthen the papers contributions to the field of RL research.", "mCzMqeWSFJ": " Peer Review  1. Summary The paper proposes ComENet a novel and efficient graph neural network for 3D molecular graph learning. The method achieves global and local completeness of 3D information through a novel message passing scheme that operates within a 1hop neighborhood. ComENet is shown to be orders of magnitude faster than existing methods achieving similar or better performance on large datasets. Experimental results demonstrate the effectiveness and efficiency of ComENet on various datasets.  2. Strengths and Weaknesses Strengths:  The proposed ComENet addresses the important issue of incorporating complete 3D information efficiently in graph neural networks.  The paper provides a rigorous proof of completeness and analysis of time complexity for the proposed method.  Experimental results show that ComENet outperforms existing methods in terms of efficiency and performance on largescale datasets.  The novel message passing scheme and focus on global and local completeness are significant contributions to the field of 3D molecular graph learning. Weaknesses:  The paper lacks a detailed comparison with stateoftheart methods in the field of graph neural networks for 3D molecular graph learning.  The limitations of the proposed method and potential failure cases are not adequately discussed.  The paper does not provide extensive analysis of the impact of hyperparameters or sensitivity analysis of the proposed method.  3. Questions  How does the proposed ComENet compare with other graph neural network models for 3D molecular graph learning that incorporate 3D information differently  Are there specific scenarios or types of molecular structures where ComENet may not perform as effectively in terms of distinguishing conformers  Have you considered the transferability of the proposed method to other related tasks or domains beyond molecular graph learning  4. Limitations  The paper could benefit from a more comprehensive evaluation on various data distributions including noisy or sparse datasets to demonstrate the robustness of ComENet.  The discussion of limitations and challenges faced by ComENet in realworld applications should be expanded to provide a more holistic view of the methods applicability.  Consideration of interpretability of the models decisions and representations would enhance the understanding of the proposed algorithm. Overall the paper presents a novel and efficient method for 3D molecular graph learning with significant contributions to the field. Further exploration and validation on diverse datasets and extension of the method to other related tasks can enrich the impact of ComENet in practical applications.", "xTYL1J6Xt-z": " Peer Review  Summary The paper introduces an innovative approach FasterRisk for efficiently producing a collection of highquality risk scores learned from data. The authors address the limitations of existing methods and propose a threestep framework that leverages beamsearch algorithms and a novel star ray search technique with multipliers to produce accurate risk scores. The paper showcases superior solution quality and runtime efficiency of FasterRisk compared to existing methods like RiskSLIM emphasizing its potential impact on various fields requiring predictive models.  Strengths and Weaknesses  Strengths: 1. Innovative Approach: The introduction of the FasterRisk algorithm addresses the limitations of existing methods for creating risk scores and provides a unique solution framework. 2. Performance Comparison: The paper provides a thorough comparison of FasterRisk with existing methods showcasing superior performance in terms of solution quality and runtime efficiency. 3. Theoretical Foundation: The theoretical underpinnings and guarantees presented in the paper provide a solid basis for the proposed algorithms efficacy. 4. Applicability: The study demonstrates the broader applicability of FasterRisk in various domains requiring predictive models such as healthcare finance and criminal justice.  Weaknesses: 1. Complexity: The complexity of some algorithms and theoretical frameworks may present challenges for practitioners without a deep understanding of the underlying concepts. 2. Lack of RealWorld Implementation: While the experiments are comprehensive the paper lacks realworld case studies or practical implementations to demonstrate the algorithms effectiveness in real scenarios. 3. Validation: The validation of FasterRisk on more diverse and realworld datasets could strengthen the generalizability of the proposed approach.  Questions 1. Interpretability: How does FasterRisk ensure the interpretability of the produced risk scores particularly in complex and highdimensional datasets 2. Generalizability: Has FasterRisk been tested on a wider range of datasets with varying characteristics to assess its generalizability outside of the datasets considered in the study 3. Runtime Optimization: Are there any plans or ongoing work to further optimize the runtime efficiency of FasterRisk particularly for larger datasets or realtime applications  Limitations 1. Dataset Limitation: The study could benefit from testing FasterRisk on a more extensive and diverse set of datasets to demonstrate its effectiveness across different domains and data types. 2. Scalability Concerns: While the paper highlights the scalability of FasterRisk additional experiments on very large datasets could help evaluate its performance in scenarios with complex features and highdimensional data. Overall the FasterRisk algorithm presents a promising approach for creating highquality risk scores efficiently. Further validation realworld implementations and scalability testing could enhance the practical implications and applicability of this novel method in predictive modeling applications.  This review provides an overview of the strengths weaknesses questions and limitations of the FasterRisk algorithm as presented in the scientific paper. It aims to provide constructive feedback to improve the clarity robustness and applicability of the proposed approach.", "peZSbfNnBp4": "Peer Review 1) Summary The paper investigates the chaotic behavior of models in Domain Generalization (DG) settings due to stochasticity in optimization proposing a model averaging protocol that boosts outdomain performance and improves reliability of model selection. The authors use a moving average model protocol ensembling moving averages further boosts performance. Theoretical explanations drawing on BiasVariance tradeoff are provided. Experimental results demonstrate the effectiveness of the proposed method outperforming existing approaches on benchmark datasets. 2) Strengths and Weaknesses Strengths:  The paper addresses an important problem in DG and proposes a simple and efficient solution.  The model averaging protocol is hyperparameterfree and computationally efficient.  The experimental results demonstrate significant improvements in outdomain performance compared to existing methods.  Theoretical explanations and ablation studies provide a rigorous understanding of the proposed method. Weaknesses:  Limited discussion on the generalizability of the proposed method to other architectures or datasets.  Some sections of the paper could benefit from clearer explanations or visual aids to enhance understanding.  More comparisons with a wider range of existing methods would strengthen the evaluation section. 3) Questions  Can the proposed method be easily adapted to different architectures beyond the ones tested in the experiments  How does the performance of the proposed method change with variations in the size of the pretraining dataset  Is there a limit to the number of models that can be averaged in the ensemble without diminishing returns in performance 4) Limitations  The paper mainly focuses on improving DG performance in specific settings with limited discussions on the wider applicability of the proposed method.  Although the method shows promise more extensive experiments on diverse datasets and architectures are needed to validate the generalizability of the approach.  Theoretically there could be other factors influencing the performance of the ensemble of averages that may need to be explored further. In conclusion the paper presents a promising approach to improving domain generalization through model averaging offering significant performance gains and a more reliable selection of models. Further research and validation on different datasets and architectures could enhance the impact and usability of the proposed method.", "maSvlkPHc-k": "Peer Review 1) Summary: The paper introduces a novel theoretical framework to study the discrimination risk stemming from graph feature imputation algorithms and its impact on machine learning model fairness. The authors present the discrimination risk metric and propose a mean aggregation imputation framework. They theoretically and empirically analyze the discrimination risk across different graphs and provide a solution for ensuring low discrimination risk during feature imputation. 2) Strengths:  The paper addresses an important and timely issue of fairness in machine learning models applied to imputed data specifically focusing on discrimination risk.  The theoretical foundation of the discrimination risk metric is welldeveloped and offers a solid contribution to the field.  The proposed mean aggregation imputation framework is clear simple and demonstrates potential for addressing discrimination risk in machine learning applications.  The empirical evaluation on synthetic and realworld credit networks provides valuable insights into the practical implications of the proposed solution.  The detailed analysis figures tables and theoretical explanations enhance the understanding and credibility of the research. Weaknesses:  Although the theoretical framework is solid the practical implications of the proposed solution are limited by the assumption of known and static group membership which may not reflect realworld scenarios accurately.  The paper primarily focuses on discrimination risk associated with imputed features but does not explore other facets of fairness such as intersectional biases or dynamic group memberships.  The empirical evaluation on realworld datasets did not demonstrate significant improvements in fairness raising questions about the generalizability of the proposed solution to complex realworld scenarios. 4) Questions:  How do the assumptions of known and static group membership impact the practical application of the proposed solution in dynamic and evolving social contexts  Could the paper benefit from exploring the intersectional aspects of fairness beyond binary group memberships to account for more nuanced identity categories  The inability to reproduce similar unfairness results from previous work on realworld datasets is intriguing could this indicate limitations in the proposed discrimination risk metric or the chosen datasets  How might the proposed mean aggregation imputation framework be extended to address fairness constraints beyond statistical parity such as intersectional fairness metrics 5) Limitations:  One limitation is the focus on binary group memberships which might overlook the complexities of group identities in realworld scenarios.  The lack of significant fairness improvements on realworld datasets indicates the need for further investigation into the practical applicability of the proposed solution in diverse and nuanced settings.  The assumptions made in the theoretical framework may not fully capture the complexity of realworld social structures which could limit the generalizability and effectiveness of the proposed solution in dynamic contexts.", "rG0jm74xtx": "Summary The paper introduces a time series forecasting model called DVAE which utilizes a bidirectional variational autoencoder and a coupled diffusion probabilistic model. It aims to address the common issue of uncertainty in time series forecasting by augmenting the data reducing noise and disentangling latent variables. Extensive experiments on synthetic and realworld datasets demonstrate that DVAE outperforms competitive algorithms significantly. Strengths 1. The paper addresses a critical issue of uncertainty in time series forecasting by proposing a novel approach. 2. The model integrates generative modeling denoising and disentanglement techniques to improve time series forecasting. 3. Extensive experiments on synthetic and realworld datasets validate the effectiveness of the proposed model showing remarkable performance improvements. Weaknesses 1. The complexity of the proposed model may hinder its reproducibility and practical implementation. 2. The paper lacks a discussion on computational efficiency and scalability of the model. 3. The introduction of biases in the series due to the diffusion process needs more thorough exploration and potential mitigation strategies. Questions 1. How does the proposed model handle longterm dependencies in time series data 2. Can the model adapt to varying levels of noise and data characteristics in different time series applications 3. What are the implications of the diffusion process on the interpretability and reliability of the generated predictions Limitations 1. The proposed model introduces biases in the data during the diffusion process potentially affecting the generalizability to diverse time series datasets. 2. Lack of exploration on the computational efficiency and scalability of the model limits its practical applicability. 3. The unsupervised disentanglement learning used in the model may lead to interpretation challenges particularly in labeling latent factors in time series data. Overall the paper presents a novel approach to time series forecasting that addresses uncertainties through generative modeling denoising and disentanglement. However further investigation is needed to address potential biases and enhance the practicality and interpretability of the proposed model.", "sGugMYr3Hdy": " Summary The paper introduces a Bayesian Goal Inference model to implement pedagogical and pragmatic mechanisms in an artificial teacherlearner setup. The mechanisms are inspired by cognitive science concepts of pedagogy and pragmatism aiming to reduce goal ambiguity and improve learning from demonstrations in multigoal environments. The study demonstrates that leveraging these mechanisms accelerates learning reduces goal ambiguity and enhances goal inference especially in scenarios with few demonstrations.  Strengths and Weaknesses Strengths: 1. The paper offers a novel approach by integrating pedagogy and pragmatism concepts from cognitive science into artificial intelligence systems. 2. The Bayesian Goal Inference model is well explained and effectively implemented to improve learning outcomes in a teacherlearner setup. 3. The experiments conducted in multigoal environments demonstrate the effectiveness of pedagogical demonstrations and pragmatic learning. Weaknesses: 1. The paper could provide further explanation on the practical implications and realworld applications of the proposed model. 2. The presentation of results could be more concise and structured for clarity. 3. While the theoretical framework is robust more empirical validation and comparisons with existing methods could strengthen the papers contribution.  Questions 1. How does the proposed Bayesian Goal Inference model compare to traditional goal inference methods in terms of computational efficiency and performance 2. Is there a plan to conduct user studies or realworld applications to validate the effectiveness of the pedagogical and pragmatic mechanisms 3. How generalizable is the approach to different domains beyond block manipulation tasks and what are the key considerations for transferring the model to new applications  Limitations 1. The study focuses primarily on simulated environments limiting the generalizability of the findings to realworld scenarios. 2. The paper lacks extensive comparison with existing literature on learning from demonstrations and Bayesian inference which could provide further context and validation. 3. The proposed mechanisms heavily rely on specific assumptions and may require significant adaptation for diverse learning scenarios or more complex tasks.", "vdxOesWgbyN": " Peer Review: 1. Summary: The paper investigates the vulnerability of Split Federated Learning (SFL) to model extraction attacks proposing five novel model extraction (ME) attacks under a specific threat model. It evaluates the performance of these attacks in different settings including consistent and inconsistent gradient access nonIID data adversarial attacks and with different surrogate model architectures. Defensive methods against these attacks are also explored along with an ablation study and an analysis of data privacy. The paper concludes by discussing the implications and potential impacts of the findings. 2. Strengths:  The paper addresses an important and timely issue of model extraction attacks in federated learning schemes specifically Split Federated Learning.  The proposed ME attacks are welldescribed and cover various scenarios including consistent and inconsistent gradient access nonIID data different architectures and defensive methods.  The experiments are wellstructured and provide detailed results that support the claims made in the paper.  The inclusion of an ablation study adversarial attack performance and defensive measures add depth to the analysis.  The broader impact discussion and ethical considerations are commendable and contribute to the responsible presentation of the research. 3. Weaknesses:  The paper lacks a detailed discussion on potential realworld implications of the proposed attacks and the practical feasibility of the defenses in operational settings.  While the experiments cover a wide range of scenarios the paper could benefit from more detailed comparisons with existing methods or additional sets of experiments to strengthen the findings.  The limitations of the study are mentioned briefly a more comprehensive exploration of the limitations and areas for future research could enhance the paper.  The paper should provide a roadmap or actionable insights for mitigating the identified vulnerabilities in federated learning systems. 4. Questions:  How do the proposed ME attacks compare with existing methods in terms of effectiveness computational complexity and practical applicability  Have you considered the potential impact of these attacks on realworld systems and how the proposed defenses could be integrated into production environments  In the nonIID data setting how do the ME attacks perform in scenarios with varying levels of dataset bias or distribution mismatches  What are the key considerations for organizations looking to implement Split Federated Learning to protect model IP in light of the vulnerabilities identified in this study 5. Limitations:  The experiments are primarily focused on specific settings and may not fully capture the diversity of realworld scenarios.  The generalizability of the findings to other federated learning architectures or datasets may be limited warranting further exploration.  The practical feasibility and implementation nuances of the proposed defenses need to be validated in realworld scenarios to assess their effectiveness. Overall the paper provides valuable insights into the vulnerability of Split Federated Learning to model extraction attacks and offers a comprehensive analysis of novel ME attack strategies. Further work on extending the research to diverse scenarios and practical implementations could enhance the impact of the findings on the field of federated learning security. Note: The review is structured to align with the specified format and provides a balanced assessment of the paper. Additional questions areas for improvement and future research suggestions have been included to provide constructive feedback to the authors.", "vaxPmiHE3S": " Peer Review of \"Eventbased GRU: A Scalable and Efficient Recurrent Neural Network Model\" 1. Summary: The paper introduces the Eventbased GRU (EGRU) model aiming to improve the scalability and efficiency of recurrent neural networks (RNNs) by introducing activitysparsity and eventdriven computation. The EGRU model demonstrates competitive performance on various realworld tasks such as language modeling and gesture prediction. The paper covers the theoretical foundations training algorithms and practical applications of the EGRU showing promising results comparable to stateoftheart models. 2. Strengths:  The introduction of the EGRU model is a novel contribution with the potential to significantly improve the efficiency of RNNs.  The theoretical derivations and training algorithms for the continuous and discretetime versions of EGRU are wellexplained.  The experimental results on tasks like gesture prediction sequential MNIST and language modeling show promising performance and significant computational savings through activitysparsity. 3. Weaknesses:  While the EGRU model shows competitive performance a more indepth comparison with a wider range of stateoftheart models could strengthen the evaluation.  The implications for different model hyperparameters and configurations such as network size and learning rates could be further discussed to provide insights into model behavior under various settings. 4. Questions:  How robust is the EGRU model to changes in hyperparameters and network sizes across different tasks  Can the training algorithm for the EGRU be easily adapted to handle more complex tasks or larger datasets  How does the EGRU model perform on tasks involving longterm dependencies or sequential data with irregular time intervals 5. Limitations:  The paper mainly focuses on demonstrating the performance of the EGRU model on specific tasks but a more extensive analysis of its generalizability across a wider range of applications and datasets would be beneficial.  The evaluation of the EGRU on highly challenging benchmarks or realworld scenarios with complex data distributions could provide further insights into its effectiveness and limitations. Overall the paper presents a promising new direction in the field of recurrent neural networks with the Eventbased GRU model showing potential for improved efficiency and scalability. Further exploration and validation across diverse tasks and datasets could enhance the impact and generalizability of this model. Please let me know if you need further information or clarification.", "oR5WIUtsXmx": "Peer Review 1) Summary: The paper investigates the behavior of coordinateMLPs in comparison to regular MLPs in the context of implicit regularization for deep neural networks used in regression tasks. It highlights the unique spectral characteristics and challenges of coordinateMLPs and proposes a simple regularization technique to address issues related to the suppression of lower frequencies. The study provides theoretical insights based on a Fourier perspective and validates the findings with empirical experiments. 2) Strengths:  The paper offers a detailed analysis of coordinateMLPs shedding light on their behavior and challenges compared to regular MLPs.  The use of Fourier analysis to understand the networks frequency response is a novel and insightful approach.  The proposed regularization technique to enforce smooth interpolations and preserve spectral balance in coordinateMLPs is practical and valuable.  Empirical experiments support the theoretical findings demonstrating their relevance in realworld scenarios.  The thorough discussion and exploration of the effects of hyperparameters depth and capacity on the networks behavior provide a comprehensive understanding of the topic. Weaknesses:  The paper is highly technical which may make it challenging for readers without a strong background in neural networks and Fourier analysis to comprehend fully.  The manuscript lacks a clear comparison or benchmarking with existing regularization methods or frameworks limiting the assessment of the proposed techniques effectiveness against other approaches.  While the theoretical derivations are robust the practical implementation and scalability of the proposed regularization method in larger more complex networks are not thoroughly discussed.  The paper overlooks a more extensive discussion on the potential implications and applications of the findings in realworld scenarios beyond the presented experimental results. 3) Questions:  How does the proposed regularization technique compare to traditional regularization methods like L1L2 regularization dropout or batch normalization in terms of performance and efficacy  Could the findings from coordinateMLPs be extended to other types of neural networks beyond regression tasks such as classification or generative tasks  In realworld applications what computational overhead does the proposed regularization technique introduce and how does it scale with network complexity 4) Limitations:  The study mainly focuses on the behavior of coordinateMLPs within a shallow setting which may limit the generalizability of the findings to deeper networks commonly used in practical applications.  The manual tuning of regularization parameters required by the proposed technique may pose challenges in terms of scalability and automation especially in largescale models. 5) Conclusion: Overall the paper presents a valuable investigation into the unique characteristics and challenges of coordinateMLPs in comparison to regular MLPs. The theoretical insights derived from a Fourier perspective coupled with empirical validations provide a substantial contribution to understanding neural network behavior in signal representation tasks. Addressing the limitations and questions raised could further enhance the impact and practical applicability of the proposed regularization technique.", "wfKbtSjHA6F": " Peer Review:  Summary: The paper investigates the effectiveness of sparse networks specifically \"winning tickets\" identified through magnitude pruning in datalimited regimes demonstrating their superiority over dense networks. Extensive experiments on CIFAR10 datasets and other diverse datasets show that sparse winning tickets combined with augmentations outperform dense networks substantially in lowdata settings. The study also examines the robustness of winning tickets to synthetic noise and domain shifts and their generalizability to longtailed classification. The research highlights the importance of network capacity and connectivity in enhancing data efficiency.  Strengths: 1. Extensive Experiments: The paper conducts comprehensive experiments on various datasets showcasing the superiority of sparse winning tickets in datalimited regimes augmented with finetuning strategies. 2. Novelty: Demonstrates practical validation of sparse networks in handling image recognition tasks in low data settings filling a gap in previous literature. 3. Robustness Analysis: Thorough analysis on synthetic noise and domain shifts enhances the understanding of winning tickets robustness. 4. Generalizability: Investigation into the generalizability of sparse networks on various datasets and longtailed classification provides valuable insights.  Weaknesses: 1. Clarification on Implementation Details: The paper lacks detailed explanations on certain implementation aspects such as the specific augmentation strategies used and the finetuning process from selfsupervised backbones. 2. Discussion on Sample Complexity Reduction: Further discussion on how pruning reduces sample complexity in theory and practice would enhance the understanding of sparse networks in data efficiency.  Questions: 1. Network Pruning Process: How does the iterative magnitude pruning process specifically enhance the performance of winning tickets in dataefficient learning 2. Impact of Connectivity in Generalization: Could the paper elaborate more on the role of network connectivity in improving the generalizability of sparse networks considering both capacity and connectivity are crucial  Limitations: 1. Dataset Dependency: The studys reliance on CIFAR10 datasets restricts the generalizability of findings. Including datasets from diverse domains could strengthen the papers impact. 2. Validation Methods: More detailed insights on the validation methods used to assess the performance of winning tickets in different scenarios would provide a clearer understanding of the results. In summary the paper presents a valuable contribution by showcasing the effectiveness of sparse winning tickets in datalimited regimes. By addressing the weaknesses and answering the questions raised the research can further solidify its findings making a significant impact on the field of neural network optimization in low data settings.  Overall the paper presents a robust investigation into the performance of sparse networks in datalimited regimes showcasing their superiority over dense networks. The thorough experimental setup and analysis provide valuable insights into the practical application of winning tickets in image recognition tasks. Further clarity and elucidation on certain aspects would enhance the papers impact and contribute to the scientific communitys understanding of sparse network optimization.", "muvlhVKvd4": " Summary The paper presents a fundamental unified convergence theorem that establishes expected and almost sure convergence results for a variety of stochastic optimization methods. The theorem is applicable to a broad class of problem structures and algorithms providing a novel theoretical framework for convergence analysis. Specifically applications of the theorem include recovering and establishing convergence results for stochastic gradient method (SGD) random reshuffling (RR) stochastic proximal gradient method (proxSGD) and stochastic modelbased methods for nonsmooth nonconvex optimization problems.  Strengths and Weaknesses Strengths: 1. Novel unified convergence theorem that applies to a wide range of stochastic optimization methods. 2. The theorem provides a plugintype convergence analysis tool simplifying the convergence analysis process. 3. Applications of the theorem show new and improved convergence results for wellknown stochastic optimization methods. 4. The proof of the theorem is deemed elementary providing accessibility to a broader audience. Weaknesses: 1. While the paper presents strong theoretical results practical applications and empirical validations are lacking. 2. Some assumptions in the convergence framework might be restrictive and further discussion on the applicability of these assumptions in realworld settings would be beneficial.  Questions 1. How do the assumptions made in the convergence framework directly translate to realworld condition requirements for ensuring convergence of the stochastic optimization methods 2. Are there practical implications or applications of the theoretical results presented in the paper that have been tested in realworld scenarios 3. What additional experiments or case studies could further validate the effectiveness and applicability of the proposed unified convergence framework  Limitations The limitations of the paper include: 1. Lack of empirical validation or practical application of the theoretical results in realworld scenarios. 2. Theoretical assumptions might not fully capture the complexities of realworld stochastic optimization problems potentially limiting the generalizability of the findings. 3. The paper focuses primarily on theoretical developments without exploring practical implementation challenges or considerations.  Overall Assessment The paper introduces a novel and promising unified convergence framework for stochastic optimization methods. While the theoretical results are significant and lay a strong foundation for convergence analysis further validation through empirical experiments and realworld applications would strengthen the practical utility and impact of the research. Additional clarity on the practical implications and applicability of the proposed framework would enhance the overall impact of the findings in the field of stochastic optimization.", "u6GIDyHitzF": " Peer Review  1) Summary The paper introduces a batched algorithm C2B for the Karmed dueling bandit problem focusing on scenarios where sequential updates may be infeasible. The algorithm achieves regret bounds that match those of the best sequential algorithms under the Condorcet condition with computational experiments validating its performance.  2) Strengths and weaknesses Strengths:  The paper addresses an important problem in the field of machine learning and provides a novel solution through the C2B algorithm.  The theoretical analysis is thorough providing highprobability and expected regret bounds under the Condorcet condition.  Computational experiments on realworld datasets validate the theoretical results showing promising performance of the C2B algorithm. Weaknesses:  The paper could provide more detailed explanations on the implementation of the C2B algorithm making it easier for readers to understand the practical aspects.  Clarification on the selection of parameters and assumptions in the computational experiments would enhance the reproducibility of the results.  3) Questions  Can the C2B algorithm be extended or adapted to handle scenarios where the Condorcet condition does not hold  How sensitive is the C2B algorithm to variations in dataset characteristics or parameters like the comparison confidence level (alpha)  4) Limitations  The evaluation of the C2B algorithm is focused on scenarios with existing Condorcet winners limiting generalizability to broader dueling bandit problem instances.  The paper could discuss potential limitations or failure cases of the C2B algorithm in realworld applications and how they might be addressed. Overall the paper presents a significant contribution to the field with the development of the C2B algorithm for batched dueling bandit problems. The theoretical analysis is comprehensive and the computational experiments provide valuable insights into the algorithms performance. Clarifications on practical implementation aspects and further exploration of algorithm extensions would strengthen the papers impact and applicability.", "mT18WLu9J_": " Summary This paper investigates data poisoning attacks to increase the precision of membership inference attacks aiming to amplify privacy leakage in machine learning models. The study proposes both dirtylabel and cleanlabel poisoning attacks evaluating them across various datasets and architectures. The attacks showcased substantial increases in membership exposure with minimal impact on model performance. Moreover potential countermeasures were explored to mitigate the negative effects of these attacks.  Strengths  The paper addresses a relevant and timely topic highlighting the risks associated with data poisoning attacks and privacy leakage in machine learning models.  The proposed attacks are welldescribed covering both dirtylabel and cleanlabel strategies showcasing thorough research and experimentation.  Comprehensive evaluations across multiple datasets and architectures provide a holistic view of the attacks effectiveness and impact.  The exploration of potential countermeasures adds value by offering insights into defense mechanisms against privacy threats.  Weaknesses  While the experiments are meticulous error bars could enhance the statistical robustness of the results.  The paper could benefit from a more detailed discussion on the potential realworld implications and applications of the presented attacks.  The study lacks a detailed analysis of the costeffectiveness of implementing the proposed countermeasures.  Questions 1. Can you elaborate on the scalability of the proposed attacks and countermeasures to realworld scenarios with largescale datasets and models 2. How do the proposed cleanlabel poisoning attacks perform in a blackbox setting where the attacker does not have access to the feature extractor 3. Could the authors delve deeper into the potential implications of these attacks on industries or domains relying heavily on machine learning such as healthcare or finance  Limitations 1. The papers scope could be expanded to explore the interplay between various attack methodologies and their combined impact on model integrity and privacy. 2. The study may benefit from a more comprehensive analysis of the practicality and feasibility of implementing the proposed countermeasures in different machine learning applications and environments.", "vdh62914QR": " Peer Review  Summary The paper presents an indepth analysis of generalization error bounds for blackbox learning using the Zerothorder Stochastic Search (ZoSS) algorithm. The results indicate that ZoSS achieves similar generalization error bounds as SGD even for nonconvex losses with the learning rate being slightly decreased. The analysis is extended to cover minibatch ZoSS as well. The study contributes to the understanding of gradientfree learning algorithms and their generalization capabilities.  Strengths 1. Novelty: The paper fills an important gap in understanding generalization error in blackbox learning. 2. Theoretical Rigor: The analysis is thorough and wellstructured providing detailed insights into the stability and generalization error bounds of the ZoSS algorithm. 3. Relevance: The implications of the findings for various realworld applications such as adversarial learning and federated learning are highlighted enhancing the practical value of the research.  Weaknesses 1. Complexity: The theoretical nature of the paper might make it challenging for nonexperts to fully grasp the content without additional clarification or examples. 2. Lack of Empirical Validation: While the theoretical analysis is robust empirical validation on real datasets or scenarios could enhance the practical relevance of the study. 3. Clarity: Some parts of the paper are quite technical which may require clarification or simplification for a broader audience to understand the key contributions.  Questions 1. Practical Applicability: How transferable are the theoretical findings to realworld scenarios Have there been any practical implementations or experiments to validate the theoretical claims 2. Algorithm Parameters: How sensitive is the ZoSS algorithm to variations in parameters like K \u00b5 and batch size Are there guidelines for optimal parameter selection in different settings 3. Comparison: Could the paper benefit from a more detailed comparison with existing gradientbased methods or other gradientfree approaches in terms of generalization performance 4. Scalability: How does the ZoSS algorithm scale with higher dimensions or larger datasets particularly in scenarios where the assumption of Lipschitz and smooth losses may not hold  Limitations 1. Theoretical Emphasis: The focus on theoretical analysis without empirical validation may limit the practical insights for practitioners. 2. Complexity Assumptions: The reliance on assumptions like Lipschitz and smooth losses may restrict the applicability of the findings to realworld settings where such assumptions may not always hold. Overall the paper presents a significant contribution to the understanding of generalization error in blackbox learning but would benefit from addressing the weaknesses and limitations highlighted above to enhance its impact and practical relevance.", "yZgxl3bgumu": " Peer Review 1) Summary The paper introduces PhysGNN a datadriven framework leveraging graph neural networks for approximating soft tissue deformation in neurosurgical procedures addressing the limitations of the finite element method (FEM). PhysGNN shows promising results in accurately predicting tissue deformation and showcases enhanced computational feasibility compared to stateoftheart approaches. The experimental validation demonstrates PhysGNNs capability to accurately approximate tissue deformation in neurosurgical settings. 2) Strengths and weaknesses Strengths:  The paper addresses a crucial issue in neurosurgery regarding accurate tissue deformation approximation.  PhysGNN introduces a novel framework that leverages graph neural networks to incorporate mesh structural information.  Empirical results demonstrate the effectiveness and computational feasibility of PhysGNN in predicting tissue deformation with high accuracy.  The authors provide detailed experimental setups methodologies and comparative analysis to showcase the efficacy of PhysGNN. Weaknesses:  The comparisons made with similar studies were empirical in nature due to limited access to data and models which might introduce biases.  The baseline FEM method has limitations depending on MRI quality and constitutive laws warranting future validation against more advanced methods like Meshless Total Lagrangian Explicit Dynamics. 3) Questions  Can PhysGNN be generalized to account for patientspecific variations in tissue properties considering the variability in brain tissue behaviour  How scalable is PhysGNN to handle realtime application in surgical environments where rapid decisionmaking is critical  Are there plans to validate PhysGNN against advanced methods like MTLED for more accurate soft tissue deformation simulations 4) Limitations  The study is limited in its scope of comparison with prior works due to empirical comparisons.  The use of FEM as the baseline method may have inherent limitations that affect the accuracy of PhysGNN validations.  Future studies might encounter challenges in integrating PhysGNN with patientspecific data and scaling for realtime surgical applications. 5) Recommendations  The authors could consider expanding their empirical comparisons with more studies to provide a broader perspective on PhysGNNs performance.  Validation against more advanced methods like MTLED and patientspecific data sets could enhance the credibility and accuracy of PhysGNN.  Addressing the methods computational efficiency and scalability for realtime implementation could further solidify PhysGNN as a viable solution in neurosurgical settings. Overall the paper presents a novel and promising approach in approximating tissue deformation using PhysGNN and the results indicate its potential utility in neurosurgery. Further validations and scalability considerations would strengthen the study and its implications.", "y5ziOXtKybL": " Peer Review:  1) Summary: This paper focuses on the application of Bayesian neural networks with spikeandslab prior and shrinkage prior in capturing the uncertainty of predictions and achieving near minimax optimal convergence rates when dealing with unstructured data. The study extends previous results to the Besov space highlighting the posterior consistency and optimal convergence even in scenarios where the smoothness of the true regression function is unknown. The paper provides theoretical insights demonstrating the efficiency and practicality of the proposed methodologies for neural network model estimation.  2) Strengths and Weaknesses: Strengths:  The study addresses an important topic in the field of neural networks by delving into Bayesian methods and demonstrating practical applications.  The paper extends previous results to the Besov space broadening the understanding of neural network models convergence rates and posterior consistency.  The theoretical framework and algorithms proposed provide valuable insights for researchers and practitioners working with neural networks.  The comparison between spikeandslab prior and shrinkage prior showcases the effectiveness and computational feasibility of the latter. Weaknesses:  The theoretical nature of the study may limit its immediate practical applicability without more extensive validation on realworld datasets.  The complexity of the proposed prior distributions and hyperparameters may hinder their implementation in largescale applications.  The paper lacks empirical validation and realworld case studies to demonstrate the effectiveness of the proposed methodologies in practical scenarios.  3) Questions:  Can you elaborate on the computational complexity associated with implementing the spikeandslab prior versus the shrinkage prior for large neural network models  How do the theoretical results presented in the paper translate to realworld scenarios especially when dealing with highdimensional unstructured data sets  Are there scalability challenges when applying the proposed Bayesian neural network methods to largescale industrial applications such as image analysis or natural language processing  4) Limitations:  The study primarily focuses on theoretical derivations and lacks empirical validation on realworld datasets to demonstrate the practical effectiveness of the proposed Bayesian neural network methods.  The stringent conditions required for achieving theoretical optimality in neural network estimation may pose challenges in practical implementations with large datasets and complex architectures.  The paper does not address the interpretability of the Bayesian neural network models and how uncertainty estimates can be practically integrated into decisionmaking processes. Overall the paper provides valuable theoretical insights into Bayesian neural networks with spikeandslab and shrinkage priors showcasing the potential for achieving posterior consistency and optimal convergence rates. Addressing the limitations regarding empirical validation and practical applicability could further enhance the impact of the study in the field of neural network research.", "rcMG-hzYtR": "Peer Review: Summary: The paper presents a novel offpolicy actorcritic approach MaxMin Twin Delayed Deep Deterministic Policy Gradient algorithm (M2TD3) to optimize worstcase performance in reinforcement learning scenarios with uncertainty parameters. The researchers demonstrate the effectiveness of M2TD3 using experiments in MuJoCo environments comparing it to several baseline approaches. The proposed method outperformed the baselines in terms of worstcase performance and average performance in various scenarios. Strengths: 1. Innovative Approach: The introduction of M2TD3 for optimizing worstcase performance in uncertain settings is commendable and addresses a significant issue in reinforcement learning. 2. Detailed Explanation: The paper provides a comprehensive explanation of the proposed methodology including mathematical formulations algorithmic framework and experimental setup. 3. Experimental Validation: The study includes extensive experiments using MuJoCo environments to evaluate the effectiveness of M2TD3 and the comparison with baseline methods helps in understanding the superiority of the proposed approach. 4. Public Implementation: Sharing the implementation code publicly on GitHub enhances the reproducibility and transparency of the research. Weaknesses: 1. Complexity: The proposed M2TD3 approach is complex and involves multiple technical components which might make it challenging for practitioners to implement and apply in realworld scenarios. 2. Theoretical Insights: While the paper demonstrates empirical performance additional theoretical insights to explain the working principles and guarantees of the proposed method would strengthen the contribution. 3. Limited Application Scope: The study focuses on a specific setting of reinforcement learning with uncertainty parameters limiting the generalizability of the approach to broader RL problems. Questions: 1. Training Stability: Could the authors provide insights into the training stability and convergence properties of M2TD3 compared to baseline methods 2. Scalability: How does the computational complexity of M2TD3 compare to other approaches especially as the number of uncertainty parameters or dimensions increases 3. RealWorld Applications: Are there any plans to test the proposed method in realworld robotic control scenarios to demonstrate practical usability and effectiveness outside simulation environments Limitations: 1. Small Uncertainty Set: The study acknowledges potential suboptimal results of M2TD3 in scenarios with small uncertainty parameter sets indicating a limitation in maximizing worstcase performance in such conditions. 2. Adversarial Settings: Although the paper briefly extends the approach to handle adversarial forces further exploration of adversarial settings and additional experimentation could strengthen the validity of the proposed method. Overall the research paper provides a valuable contribution to the field of reinforcement learning by proposing a method to optimize worstcase performance under uncertain environments. Addressing the noted weaknesses and limitations through further analysis and practical applications could enhance the impact and relevance of the proposed M2TD3 algorithm.", "m6HNNpQO8dc": " Peer Review  1) Summary: The paper presents a novel approach to activation functions in neural networks by introducing logitspace probabilistic Boolean operations namely ANDAIL ORAIL and XNORAIL. These functions are designed to address the limitations of traditional activation functions and have been applied to various tasks like tabular classification image classification transfer learning abstract reasoning and compositional zeroshot learning. The study demonstrates the effectiveness of these functions and ensembling strategies across diverse tasks and architectures.  2) Strengths and weaknesses: Strengths:  Introduces novel activation functions based on logitspace Boolean operations expanding the neural network activation function landscape.  The paper covers a wide range of tasks to demonstrate the effectiveness of the proposed activation functions.  Detailed derivations and approximations are provided for the logitspace Boolean operations.  Ensembling strategies and normalization techniques are explored contributing to a comprehensive study of activation functions.  The methodology and experiments are welldocumented with code repositories for reproducibility. Weaknesses:  The paper lacks a clear comparison with existing stateoftheart activation functions apart from ReLU and variants limiting the context for the proposed functions.  The evaluation on some tasks lacks statistical significance testing which may impact the reliability of the results.  While the theoretical derivations are detailed the practical implications and computational costs of implementing these functions in largescale applications are not adequately discussed.  4) Questions: 1. How do the proposed activation functions compare to stateoftheart functions like GELU Swish or Mish particularly in terms of performance and computational efficiency 2. Did you observe any particular patterns or trends in the behavior of the activation functions across different tasks or datasets 3. What are the implications of using these activation functions in deeper networks or more complex architectures beyond the experiments conducted in this study  5) Limitations:  The paper primarily focuses on theoretical derivations and experimental demonstrations without delving deeply into the practical scalability and generalizability of the proposed activation functions.  Some experiments lack statistical rigor potentially impacting the robustness of the conclusions drawn from the results.  The computational complexity and memory requirements of implementing these new activation functions at scale are not thoroughly discussed which could be a significant limitation in realworld applications. Overall the paper introduces innovative activation functions based on logitspace Boolean operations and provides compelling evidence of their effectiveness across various tasks. However further investigation is required to address scalability and compare the proposed functions against a wider range of stateoftheart activations to establish their versatility and general applicability in neural networks.", "luGXvawYWJ": " Summary: The paper introduces a novel dataset factorization approach called HaBa for dataset distillation (DD). HaBa decomposes a dataset into data hallucination networks and bases allowing the synthesized data to encode intersample relationships. An adversarial contrastive constraint is employed to enhance data diversity. Extensive experiments demonstrate that HaBa outperforms stateoftheart methods in downstream classification tasks while reducing storage costs.  Strengths: 1. Novel Approach: Introduces a unique dataset factorization perspective through HaBa improving data efficiency for dataset distillation. 2. Versatility: Compatible with existing DD baselines offering a plugandplay strategy for improved performance. 3. Performance Improvement: Consistent and significant enhancement in downstream model training performance shown across multiple benchmarks. 4. Adversarial Constraint: Introduction of adversarial contrastive constraints contributes to increased data diversity and informativeness.  Weaknesses: 1. Computational Cost: The process of online pairwise combination between hallucinators and bases may increase time and GPU memory costs slightly. 2. Limited Performance Gain: Potential limitations in achieving substantial performance gain when the number of images is large.  Questions: 1. Does HaBas approach lead to any significant changes in model training convergence time compared to baseline methods 2. How does HaBa handle cases of highly imbalanced datasets in terms of classwise relationship encoding 3. Are there any insights on the scalability of HaBa to larger datasets and more complex architectures 4. Can the generalization capabilities of HaBa be tested on tasks beyond image classification such as object detection or semantic segmentation  Limitations: 1. Time and Memory Cost: The increased computational cost of online pairwise combination may hinder scalability to larger datasets. 2. Performance Gain Limitation: Limited enhancement observed when increasing the number of images substantially potentially constraining the practical impact. 3. ClassWise Relationship Encoding: The current approach might lack explicit mechanisms for classwise relationships encoding which could be a limitation in some scenarios. Overall the paper presents a promising approach to dataset distillation with notable improvements in data efficiency and performance. Further investigations into scalability classwise relationships and generalization to different tasks could enhance the applicability and robustness of HaBa in realworld scenarios.", "pZtdVOQuA3": " Peer Review of the Scientific Paper  Summary: The paper proposes an alternative rendering algorithm for neural radiance fields based on importance sampling aiming to improve efficiency in view synthesis. The algorithm involves reparameterization of the sampling process enabling endtoend learning. The experimental results demonstrate competitive rendering quality with reduced computational costs.  Strengths and weaknesses: Strengths: 1. The reparameterization of the sampling algorithm is a novel and effective approach to optimize neural radiance fields with reduced computational costs. 2. The experiments show promising results in terms of competitive rendering quality and efficiency compared to baseline methods. 3. The paper addresses the limitations and potential societal impacts demonstrating ethical considerations. Weaknesses: 1. Lack of code availability and detailed instructions for reproducing the experimental results may hinder the reproducibility of the findings. 2. The paper could benefit from providing error bars for the experimental results to convey the variability and robustness of the algorithm.  Questions: 1. How does the proposed reparameterization of the sampling algorithm affect the interpretability and generalization of the neural radiance fields 2. Can the algorithm handle complex scenes with varying levels of opacity and density gradients effectively 3. Are there specific scenarios or applications where the proposed importance samplingbased rendering algorithm outperforms existing methods significantly  Limitations: 1. The paper lacks detailed training data splits and hyperparameter selection process which could impact result reproducibility and algorithm optimization. 2. The absence of error bars in reporting experimental results may not fully capture the variability and reliability of the algorithms performance. 3. The limited discussion on the broader impact of the proposed algorithm beyond efficiency improvements could be enhanced to address potential wider implications. Overall the paper presents an innovative approach to optimizing neural radiance fields for efficient view synthesis. Addressing the mentioned weaknesses and limitations could further strengthen the papers contributions and applicability in the field of 3D scene reconstruction and rendering.", "wFymjzZEEkH": "Peer Review 1) Summary The paper introduces a personalized Federated Learning (FL) method named lpproj focusing on communication efficiency robustness against adversarial attacks and performance fairness. The method projects local models into a lowdimensional subspace and uses Lpregularization demonstrating convergence for smooth objectives. The theoretical analysis is supported by empirical experiments highlighting the methods superiority over existing stateoftheart approaches. The paper also addresses related works methodology theoretical analyses of convergence and numerical experiments on robustness and fairness. 2) Strengths and Weaknesses Strengths: a) The paper addresses important challenges in FL showcasing a method that prioritizes communication efficiency robustness and fairness. b) The theoretical analysis provides a solid foundation for the proposed method showcasing convergence guarantees and benefits of lowdimensional projection. c) Extensive empirical experiments demonstrate the methods superiority over existing approaches in terms of performance metrics. d) The approach is welljustified and integrates advanced techniques like infimal convolution and subspace projection effectively. Weaknesses: a) The complexity of the proposed method may pose challenges for practical implementation and adoption in realworld scenarios. b) The paper lacks a thorough discussion on scalability and computational efficiency especially with large datasets and complex models. c) More detailed comparisons with existing methods could provide a clearer understanding of the contributions and limitations of the proposed lpproj method. 3) Questions a) Can the lpproj method be easily extended to accommodate more complex deep learning models and nonlinear problems b) How sensitive is the method to hyperparameter choices especially in scenarios with varying data distributions or network configurations c) How does the proposed method perform in scenarios with a high degree of data heterogeneity or extreme adversarial attacks that go beyond the simulated experiments 4) Limitations a) The generalization of the proposed method to diverse FL settings and applications needs to be further validated through additional experiments and comparisons. b) The paper could benefit from more indepth discussions on the practical implications and limitations of the lpproj method in realworld FL implementations. Overall the paper presents a comprehensive study on personalized FL offering a welltheorized and empirically validated approach. Addressing the raised questions and limitations can further enhance the clarity and applicability of the proposed lpproj method.", "stAKQ6vnFti": "Peer Review: 1) Summary: The paper presents a novel framework Contrastive Learning with Lowdimensional Reconstruction (CLLR) to address the issue of highly scattered instances in the highdimensional feature space caused by existing Contrastive Learning (CL) techniques. CLLR introduces a sparse projection layer to reduce the dimensionality of the feature embedding effectively capturing the similarity between pairwise instances. Theoretical proofs and empirical results across various domains validate the superiority of CLLR in learning lowdimensional contrastive embeddings. 2) Strengths and weaknesses: Strengths:  The paper addresses a critical issue in CL by proposing CLLR introducing a novel approach to reduce the dimensionality of feature embeddings effectively.  Theoretical analyses and empirical results demonstrate the effectiveness of CLLR across multiple domains showcasing its superiority over existing methods.  Extensive experimentation and comparison with stateoftheart models on realworld datasets validate the significance of lowdimensional contrastive embeddings. Weaknesses:  The paper could elaborate more on the specific datasets used hyperparameters and any potential limitations in the experiments conducted.  While the theoretical analyses are thorough the practical implications of the proposed method on different types of datasets could be further discussed.  The paper could provide more insights into the computational complexity of CLLR compared to existing methods. 3) Questions:  How does CLLR perform on datasets with varying sizes and complexities compared to existing CL techniques  Can CLLR be easily integrated into different CL models and are there any specific limitations in terms of scalability or adaptability  What potential challenges or tradeoffs might arise when implementing CLLR in practical scenarios especially in terms of computational resources and training time 4) Limitations:  The paper lacks indepth discussions on the generalizability of CLLR across diverse datasets and tasks which could impact its applicability in realworld scenarios.  The evaluation and comparison are predominantly focused on traditional image and text classification tasks limiting the exploration of CLLRs effectiveness in other domains.  The practical implications of CLLR in scenarios where negative pairs are not available could be an interesting future direction but remains unexplored in this study. Overall the paper presents a significant contribution in proposing CLLR for learning lowdimensional contrastive embeddings backed by strong theoretical foundations and compelling empirical results. Further exploration and validation on diverse datasets and tasks could enhance the applicability and robustness of CLLR in various machine learning applications.", "uLhKRH-ovde": " Peer Review  1) Summary The paper introduces a novel communicationefficient distributed stochastic local gradient clipping algorithm aimed at accelerating training deep neural networks in a distributed setting. The algorithm takes advantage of relaxed smoothness conditions and skips communication rounds to achieve parallel speedup. The theoretical analysis provides insights into the algorithms convergence properties and communication complexity which are further validated through experiments on various benchmark datasets.  2) Strengths and Weaknesses Strengths:  The paper addresses an important issue in distributed training of deep neural networks by proposing a novel algorithm that leverages relaxed smoothness conditions and infrequent communication.  The algorithm is theoretically analyzed showcasing linear speedup and reduced communication complexity supported by empirical results demonstrating its efficiency.  The use of unique techniques such as estimating truncated random variables and skipping communication rounds enhances the novelty of the proposed algorithm. Weaknesses:  The proposed algorithms applicability is limited to homogeneous data settings excluding the analysis for heterogeneous data distributions.  The paper lacks discussion on potential challenges or limitations that may arise during the practical implementation of the algorithm.  The implications of the algorithms performance on realworld applications or complex datasets could be further elaborated to provide a broader perspective.  4) Questions  Can the proposed algorithm be easily adapted to handle or be extended for dealing with heterogeneous data distributions in a distributed setting  How robust is the algorithm to noise or variations in hyperparameters especially in scenarios with largerscale datasets or complex neural network architectures  Are there any practical constraints or overhead associated with implementing the algorithm in realworld distributed training environments  5) Limitations  The theoretical analysis and empirical evaluation focus on homogeneous data settings limiting the algorithms practical applicability in scenarios with heterogeneous data distributions.  The absence of detailed discussion on potential challenges performance tradeoffs or scalability issues of the proposed algorithm might hinder its adoption in complex distributed training setups.  The experiments should be extended to include more diverse and challenging datasets to validate the algorithms performance across various scenarios. Overall the paper presents a promising approach to enhancing distributed training efficiency in deep neural networks. However further research addressing the mentioned limitations and providing insights into the algorithms robustness and scalability in realworld distributed training setups would strengthen the papers contributions.", "xONqm0NUJc": "Peer Review: Summary: The paper introduces Relational Proxies a novel approach for encoding semantic labels in finegrained visual categorization by leveraging the relationships between global and local views of an object. The theoretical framework developed in the paper formalizes the notion of distinguishability in finegrained categories and identifies necessary conditions for model learning. Experimental results on benchmark datasets demonstrate stateoftheart performance gains in FGVC tasks. Strengths: 1. The paper provides a rigorous theoretical formulation of the FGVC task outlining necessary conditions for learners to capture finegrained information effectively. 2. The introduction of Relational Proxies demonstrates significant accuracy gains on benchmark FGVC datasets surpassing existing stateoftheart results by a notable margin in some cases. 3. The ablation studies showcase the importance of different components such as relationaware representations relational proxies and summary of local attributes contributing to the overall performance of the model. 4. The experimental evaluation across various FGVC datasets and comparison with existing approaches highlight the effectiveness and superiority of the proposed method. Weaknesses: 1. The paper could provide more detailed explanations and insights into the choice of hyperparameters model architecture and experimental setups to enhance reproducibility and understanding. 2. While the theoretical framework is well developed the practical implications and realworld applications of the proposed Relational Proxies approach could be further elaborated upon for broader context. 3. It would be beneficial to include more analysis on the limitations of the model and potential challenges in realworld scenarios to provide a comprehensive understanding of the applicability of the proposed method. Questions: 1. How do the relationships between global and local views translate to realworld examples beyond the datasets used for evaluation Have such concepts been validated in practical scenarios 2. Are there specific use cases or domains in which the Relational Proxies approach might exhibit limitations or require further enhancements for optimal performance 3. What considerations were taken into account for the computational efficiency of the model especially in terms of scalability to larger datasets or realtime applications Limitations: 1. The paper lacks indepth discussions on the generalizability of the proposed method beyond the benchmark datasets which could limit its applicability in diverse FGVC tasks. 2. The interpretability and explainability of the Relational Proxies approach need further exploration to ensure transparency and trustworthiness in realworld applications. 3. The reliance on a predefined cropping methodology for obtaining local views may introduce biases or limitations in capturing diverse object parts potentially affecting the models adaptability to different scenarios.", "nDemfqKHTpK": " Peer Review  1) Summary The paper proposes an innovative approach named GraB for improving data ordering in stochastic gradient descent. It introduces the concept of herding for data ordering with theoretical justifications and empirical validations. GraB leverages online Gradient Balancing to achieve faster convergence rates compared to Random Reshuffling and other baseline algorithms on various machine learning tasks. The experimental results demonstrate the effectiveness of GraB in terms of both training and validation performance.  2) Strengths and Weaknesses Strengths:  The paper introduces a novel algorithm GraB based on theoretical foundations concerning data ordering in SGD.  GraB is shown to outperform Random Reshuffling and stateoftheart greedy ordering in empirical experiments across various machine learning tasks.  The paper provides extensive theoretical justifications for the proposed approach.  The experimental evaluation is thorough covering multiple machine learning applications. Weaknesses:  The theoretical derivations could be complex for readers not wellversed in optimization theory.  The relationship between the proposed algorithm and practical scalability on large datasets could be explored further.  The paper could benefit from a more detailed discussion on the limitations of the proposed approach.  3) Questions  How does GraB perform with varying batch sizes in comparison to the baseline algorithms  Are there scenarios where GraB might not be as effective as Random Reshuffling or other algorithms  What are the computational overheads associated with implementing GraB in realworld ML pipelines  4) Limitations  The paper primarily focuses on theoretical analysis and empirical evaluations without delving into the interpretability of the models trained using GraB.  The comparison with other recent stateoftheart algorithms could provide further insights into the performance of GraB.  The memory and computational efficiency gains highlighted by GraB could be put into perspective with respect to industryscale applications. Overall the paper presents a strong case for the effectiveness of GraB in improving data ordering for SGD showcasing both theoretical rigor and empirical superiority. However further exploration on practical scalability interpretability and comparison with recent approaches could enhance the impact and relevance of the proposed algorithm in machine learning research and applications.", "n7Rk_RDh90": " Peer Review 1) Summary (avg word length 100): The paper proposes a novel approach 3D Concept Grounding (3DCG) which utilizes neural descriptor fields and neural operators to perform 3D semantic and instance segmentation through question answering supervision. The framework outperforms existing models on 3D aware visual reasoning tasks and can generalize well to unseen shape categories and real scans. Experimental results on the collected PARTNETREASONING dataset demonstrate the effectiveness of the proposed approach. 2) Strengths and weaknesses: Strengths:  Introduces a novel approach 3DCG leveraging neural descriptor fields for 3D semantic and instance segmentations.  Demonstrates superior performance on 3D aware visual reasoning tasks and generalization to unseen categories and real scans.  Utilizes continuous differentiable neural fields for segmentation and concept learning providing flexibility and differentiability.  Offers qualitative examples and experimental results supporting the effectiveness of the proposed framework. Weaknesses:  The models training data is limited to synthetic scenes and questions raising concerns about realworld generalization.  The methods complexity might pose challenges for adoption in practical applications.  The paper lacks a detailed analysis of the computational complexity and runtime efficiency of the proposed framework. 3) Questions:  How does the proposed approach compare in terms of computational efficiency with existing 3D concept grounding methods  Can you explain how the neural operators defined on the neural field operate during the reasoning process in more detail  What strategies are in place to address potential biases or limitations in the neural descriptor fields representations during reasoning tasks 4) Limitations:  The primary limitation lies in the absence of training and evaluation on realworld scenes making the generalization to complex real scenarios uncertain.  The proposed frameworks complexity could hinder its adoption in practical applications that demand efficient and lightweight solutions.  The lack of comprehensive analysis regarding the computational requirements and runtime efficiency of the model is a notable limitation. Overall the paper presents an innovative approach to address the 3D concept grounding problem demonstrating promising results on synthetic datasets. However ensuring the frameworks practical utility in realworld scenarios and addressing concerns related to computational efficiency and complexity will be crucial for its broader impact. Further investigations into generalization on real scenes and potential biases in the neural descriptor fields are recommended for future work.", "uFSrUpapQ5K": " Peer Review  1) Summary The paper addresses the OffPolicy Evaluation (OPE) problem in the contextual bandit setting focusing on scenarios where the full support assumption is not valid. Two estimators leveraging side information about actions are proposed and evaluated theoretically and experimentally. The estimators were shown to outperform traditional approaches particularly in the context of deficient support.  2) Strengths and Weaknesses Strengths:  The paper addresses an important problem in OPE with a clear focus on deficient support scenarios.  The theoretical analysis is rigorous providing insights into the design and performance of the proposed estimators.  The experimental evaluation with realworld data adds practical relevance to the proposed methods.  Detailed explanations of assumptions proposed estimators and experimental setups enhance the papers clarity. Weaknesses:  Experimental Design Clarity: The paper could benefit from a more detailed explanation of the experimental setups including parameter choices and result interpretation.  Further Analysis: While the theoretical and experimental results are insightful a deeper analysis of limitations extensions and practical implications could enhance the papers impact.  3) Questions  Generalization: How well do the proposed estimators generalize to different domains or datasets with varying characteristics  Scalability: Have the proposed estimators been tested for scalability with larger datasets or more complex contextual bandit scenarios  Practical Implementation: What are the practical challenges in implementing the proposed estimators in realworld systems and how can these be addressed  4) Limitations  Assumption Dependency: The performance of the estimators relies heavily on the assumptions made. How sensitive are the estimators to violations of these assumptions in practical scenarios  Complexity: The proposed estimators involve some complexity in design and implementation. Are there simplifications or improvements that could enhance their usability without compromising performance Overall the paper presents valuable contributions to the OPE problem especially in deficient support scenarios. Addressing the limitations and further exploring practical implications could enhance the papers impact and applicability in realworld settings.", "mXP-qQcYCBN": "Peer Review: 1) Summary: The paper presents a novel selfsupervised method AutoLink for learning structured representations from unlabeled image collections by disentangling object structure from appearance using a graph of 2D keypoints. The method outperforms existing selfsupervised methods in keypoint and pose estimation benchmarks paving the way for structureconditioned generative models on diverse datasets. The approach is demonstrated on multiple benchmarks and showcases significant improvements in keypoint localization accuracy and applicability across various image domains. 2) Strengths and weaknesses: Strengths:  Novel selfsupervised method for learning structured representations from unlabeled images.  Effective disentanglement of object structure from appearance using a graph of 2D keypoints.  Outperforms existing selfsupervised methods in keypoint and pose estimation benchmarks.  Applicability demonstrated across diverse image domains and datasets.  Interpretability of the resulting graph with applications spanning portraits persons animals hands and flowers. Weaknesses:  The method relies on masking input images which may limit its performance in scenarios with complex backgrounds or occlusions.  The model may struggle with differentiating between left and right sides of objects.  Some limitations in handling occlusions and fully capturing structured backgrounds are acknowledged suggesting future directions for improvement.  The potential risks of misuse in applications like deep fakes and surveillance are highlighted but not extensively addressed within the paper. 3) Questions:  How does the performance of AutoLink compare to supervised methods for keypoint detection and pose estimation benchmarks  Could the models robustness be further evaluated in scenarios with varying levels of occlusions and structured backgrounds  Are there plans to address the limitations in handling occlusions and differentiating between object sides in future iterations of the method  How does the model generalize when applied to datasets with highly diverse object classes and variations in appearance features 4) Limitations:  The paper acknowledges limitations in the models handling of occlusions distinguishing object sides and capturing highly structured backgrounds.  Potential risks of misuse for deep fakes and surveillance applications are recognized but not fully elaborated within the paper.  The reliance on masking input images may limit the models performance in scenarios with complex backgrounds or occlusions. Overall the paper presents a promising selfsupervised method AutoLink for learning structured representations from unlabeled image collections showcasing significant improvements in keypoint localization accuracy and applicability across various image domains. Addressing the acknowledged limitations and exploring further robustness in handling complex scenarios could enhance the models efficacy and applicability in diverse realworld settings.", "xbhsFMxORxV": " Peer Review  1) Summary The paper \"Hyphen: A DiscourseAware Hyperbolic Spectral CoAttention Network\" proposes a novel approach to socialtext classification by incorporating public discourse through a hyperbolic manifold and Fourier coattention mechanism. The model Hyphen performs well on various socialtext classification tasks and produces explanations for the predictions. Extensive experiments on ten datasets show that Hyphen outperforms stateoftheart models especially in tasks like fake news detection. Overall the paper presents a comprehensive framework with promising results.  2) Strengths and Weaknesses Strengths:  Novel approach: The incorporation of public discourse through hyperbolic manifold and Fourier coattention is innovative.  Performance: Hyphen achieves stateoftheart results on multiple socialtext classification tasks across different datasets.  Explainability: Hyphen excels in providing explanations for the predictions which is crucial for model interpretability.  Experimental Evaluation: Extensive experiments and ablation studies provide empirical evidence supporting the effectiveness of the proposed model. Weaknesses:  Tangent Space Limitation: The paper mentions using tangent spaces which might limit accuracy as they are local approximations of the manifold.  Model Complexity: The models complexity especially with hyperbolic representation learning and Fourier coattention may make it challenging to interpret and implement by practitioners.  DataSpecific Baselines: The utilization of dataspecific baselines may limit the generalizability of the results.  Curvature Consistency: The need for consistent curvatures in the hyperbolic manifolds could potentially restrict the flexibility of the model.  3) Questions  How does the utilization of hyperbolic spaces impact the scalability of the model in terms of both computing resources and training time  Could the proposed framework be extended to handle multilingual or crosslingual socialtext classification tasks  What are the potential challenges in applying Hyphen to realtime social media monitoring scenarios considering the processing requirements of hyperbolic manifold representations  4) Limitations  Tangent Spaces: Dependency on tangent spaces for computing Fourier coattention could limit the models capability in capturing complex hierarchical structures.  Generalizability: The models performance and explainability may vary across different socialtext classification tasks and datasets.  Interpretability: The complexity of the proposed model might hinder its interpretability and adoption in practical applications.  Curvature Constraint: The requirement for consistent curvatures in hyperbolic manifolds could restrict the models flexibility and adaptability to diverse datasets. Overall the paper presents a promising approach in the field of socialtext classification but further investigations are needed to address scalability generalizability and interpretability concerns. The models performance across various scenarios and its realworld applicability should be thoroughly evaluated in future studies.", "q-snd9xOG3b": " Summary The paper explores canonical noise distributions (CNDs) for fdifferential privacy (fDP) in both onedimensional and multivariate settings. It investigates the existence and construction of logconcave CNDs and multivariate CNDs showing their link to group privacy and mechanism composition. The study provides new insights into CND properties and fDP addressing questions on constructing logconcave CNDs and multivariate CNDs.  Strengths 1. Innovative Approach: The paper introduces new properties of CNDs and fDP offering a fresh perspective on addressing privacy concerns. 2. Theoretical Contributions: The study establishes theoretical connections between tradeoff functions group privacy and mechanism composition enhancing our understanding of privacy mechanisms. 3. Detailed Analysis: The paper provides detailed proofs technical details and constructions for various scenarios enhancing the rigor of the research.  Weaknesses 1. Technical Complexity: The paper features intricate mathematical arguments and advanced concepts making it challenging for readers without a deep background in differential privacy. 2. Limited Practical Applications: While the theoretical contributions are valuable the practical implications or realworld applications of the findings could be further discussed. 3. Definition Clarity: The definition and implications of fDP CNDs and related concepts could be clarified for easier comprehension by a broader audience.  Questions 1. How do the findings of this research contribute to the development of improved privacy mechanisms in practical scenarios 2. Can the concepts explored in the paper be practically implemented in existing privacy protection frameworks in tech companies or government agencies  Limitations 1. Theoretical Focus: The study is highly theoretical with a limited discussion on practical implementation or empirical validation of the proposed concepts. 2. Complexity: The technical complexity of the content may limit accessibility to a broader audience. 3. Scope: The study may benefit from expanding on the implications of the findings in broader contexts beyond differential privacy theory. Overall the paper presents valuable theoretical insights into CNDs and fDP but could benefit from enhancing clarity and discussing practical implications in more depth. Further research could focus on bridging the gap between theoretical developments and practical applications in privacy protection frameworks.", "qfC1uDXfDJo": "Summary The paper investigates how overparameterization affects the loss landscape of ReLU neural networks by studying the optimization problem associated with fitting twolayer networks. Analyzing symmetry structures the researchers develop new tools to understand spurious minima and the effectiveness of overparameterization in minimizing them. They provide analytic estimates for loss and Hessian spectrum proving that adding neurons can transform spurious minima into saddles and create descent directions. The work addresses fundamental questions in deep learning optimization utilizing techniques from algebraic geometry and representation theory. Strengths and Weaknesses Strengths:  The paper addresses a fundamental problem in deep learning concerning nonconvex optimization landscapes and the role of overparameterization.  The use of symmetry structures and novel tools provides valuable insights into the mechanism of spurious minima annihilation.  Analytic approach and sharp estimates offer rigorous analysis of the optimization problem.  The work contributes to the development of new methods and ideas for studying overparameterization in neural networks. Weaknesses:  The paper is highly technical and may be challenging for readers without a deep background in algebraic geometry and representation theory.  The experimental validation is lacking and there is no concrete demonstration of the proposed methods on real datasets or models.  The implications of the findings for practical applications in deep learning are not explicitly discussed. Questions 1. How do the results and findings of the paper compare to existing methods and approaches for analyzing optimization landscapes in deep learning 2. What are the practical implications of the identified descent directions and their role in escaping spurious minima in neural network optimization 3. Can the proposed tools and techniques be generalized and applied to more complex neural network architectures beyond the studied twolayer ReLU networks Limitations 1. The paper lacks experimental validation on real datasets and models limiting the direct applicability of the findings. 2. The technical complexity of the approaches used may hinder broader understanding and adoption by researchers outside the specific field. 3. The focus on symmetric structures may restrict the generalizability of the results to more diverse and asymmetric neural network structures.", "uPyNR2yPoe": "Peer Review Summary: The paper introduces ELIGN (Expectation Alignment) a novel selfsupervised intrinsic reward for multiagent systems inspired by the selforganization principle in Zoology. ELIGN encourages agents to act predictably to their teammates and unpredictably to adversaries aiming to improve coordination and collaboration in decentralized training under partial observability. The efficacy of ELIGN is demonstrated across various tasks in the multiagent particle and Google Research football environments showcasing its superiority over sparse and curiositybased intrinsic rewards. The experiments highlight ELIGNs scalability taskagnostic nature and performance in both cooperative and competitive settings. Strengths: 1. Novel Concept: ELIGN proposes a unique intrinsic reward based on the selforganization principle offering a fresh perspective on multiagent reinforcement learning. 2. Experimental Rigor: The paper conducts thorough experiments across different tasks environments and scenarios demonstrating the effectiveness of ELIGN. 3. Comparative Analysis: Comparisons with existing baselines like SPARSE and CURIOselfteam provide a comprehensive evaluation of ELIGNs performance. 4. Scalability: ELIGNs ability to scale well with an increasing number of agents is a significant strength showcasing its applicability to larger multiagent systems. 5. Clear Presentation: The paper is wellstructured with detailed explanations of key concepts algorithms and experimental setups. Weaknesses: 1. Limitations in Generalizability: The evaluation is limited to specific environments (multiagent particle and Google Research football) and tasks restricting the generalizability of ELIGNs performance to other domains. 2. Absence of RealWorld Application: The paper lacks realworld applications or implementations which could enhance the practicality and relevance of ELIGN in practical scenarios. 3. Limited Exploration of HumanAgent Interaction: While discussing legibility and humanAI interaction the paper could delve deeper into how ELIGN could influence these aspects. Questions: 1. How does ELIGN perform in environments with more complex state spaces and action spaces such as those involving language or complex human interactions 2. Can ELIGN be applied in scenarios where predictable actions may not be optimal such as in adversarial settings with sophisticated opponents 3. Considering ELIGNs ability to handle partial observability how would it perform in environments with varying levels of agent observability Limitations: 1. The limited evaluation of ELIGN in realworld or more complex scenarios hinders the assessment of its practical utility in diverse settings. 2. The evaluation is primarily focused on specific benchmark environments possibly overlooking the variability in multiagent tasks across different domains. Overall the paper introduces a promising concept in ELIGN showcasing its potential to improve multiagent coordination in decentralized settings. Further exploration in diverse environments and realworld applications could enhance the impact and relevance of ELIGN in multiagent systems.", "r5rzV51GZx": " Peer Review  Summary: The paper introduces a novel certifiably robust collaborative inference framework called CoPur for defending against various attacks during inference in a distributed prediction setting. The proposed method purifies embedded features based on the blocksparse nature of adversarial perturbations and redundancy across features. The method is theoretically shown to recover true features despite adversarial corruptions. Extensive experiments on ExtraSensory and NUSWIDE datasets demonstrate the superiority of CoPur over existing defenses against targeted and untargeted adversarial attacks.  Strengths: 1. The paper addresses a relevant problem in collaborative inference with a novel defense method providing theoretical guarantees and extensive empirical validation. 2. The theoretical analysis and experiments are comprehensive showcasing the robustness of CoPur against various attacks. 3. The proposed method outperforms existing defenses emphasizing its effectiveness and relevance in realworld applications.  Weaknesses: 1. Some sections of the paper are highly technical and may hinder the understanding of readers not wellversed in the field. 2. The limitations or potential future challenges of the proposed method could be further discussed to provide a more comprehensive insight into its applicability.  Questions: 1. Could the authors elaborate on the scalability of CoPur in handling a larger number of agents with varying degrees of compromise 2. How generalizable is the proposed defense method to different types of datasets and inference scenarios beyond the ones tested in the study 3. Are there any specific assumptions made in the theoretical analysis that may limit the practical applicability of CoPur in diverse realworld settings  Limitations: 1. The papers technical nature may limit its accessibility to a broader audience potentially impacting its dissemination and practical adoption. 2. The evaluation is predominantly focused on specific datasets and attack scenarios limiting the generalizability of the findings to other contexts and applications. In conclusion the paper presents a valuable contribution to the field of collaborative inference defense mechanisms offering a robust and certifiably secure approach. Addressing the identified weaknesses and questions could further enhance the papers impact and applicability in realworld scenarios.", "xvlaiSHgPrC": " Peer Review: 1. Summary: The paper presents a comprehensive study on the composition properties of differential privacy in concurrent compositions of interactive mechanisms. It generalizes and extends previous work providing optimal parallel composition properties for major notions of differential privacy including approximate DP R\u00e9nyi DP and zeroconcentrated DP. The main results confirm that the privacy guarantees hold even when multiple mechanisms are run concurrently offering valuable insights into privacypreserving data analysis. 2. Strengths:  The paper addresses an important and timely research problem in the domain of differential privacy focusing on concurrent compositions of interactive mechanisms.  The work significantly advances the understanding of differential privacy in concurrent settings providing optimal composition theorems for various notions of privacy showcasing the depth and breadth of the research.  The rigorous proofs and technical developments in the paper demonstrate strong theoretical foundations and novel methodologies to analyze the privacy properties of concurrent compositions.  The paper offers valuable insights and implications for designing new differentially private algorithms and ensuring privacy in practical deployments of interactive mechanisms. 3. Weaknesses:  The paper primarily focuses on theoretical aspects of differential privacy in concurrent compositions potentially lacking direct practical applications or case studies to demonstrate the realworld impact of the findings.  The technical nature of the paper and the complexity of the proofs may limit the accessibility and understanding of the material for readers outside the field of differential privacy or theoretical computer science.  While the proposed composition theorems are shown to hold for various notions of differential privacy the paper could benefit from discussing potential limitations or scenarios where the theorems may not apply. 4. Questions:  How do the findings and composition theorems presented in the paper apply to practical scenarios involving realworld data analysis applications that require differential privacy guarantees  Are there specific types of interactive mechanisms or data analysis tasks where the results of the paper would be particularly impactful and can you provide examples to illustrate this  What are the key assumptions or conditions under which the optimal composition properties for differential privacy in concurrent compositions are guaranteed to hold and are there scenarios where these assumptions may not be met 5. Limitations:  The paper focuses predominantly on theoretical developments and proofs potentially lacking direct empirical validations or practical case studies to demonstrate the applicability and effectiveness of the proposed composition theorems in realworld settings.  The technical complexity of the material might limit the accessibility and understanding of the findings for a broader audience particularly individuals without a strong background in differential privacy or theoretical computer science.  The paper does not explicitly discuss potential ethical implications or considerations associated with differential privacy in concurrent compositions leaving room to explore the ethical dimensions of the research in future work. Overall the paper provides significant contributions to the field of differential privacy offering novel insights into privacy guarantees in concurrent compositions of interactive mechanisms. The theoretical advancements and composition theorems established in the paper lay a strong foundation for future research in this area. Further exploration of the practical implications and broader impacts of the findings could enhance the relevance and applicability of the work in realworld data analysis scenarios.", "yZcPRIZEwOG": " Peer Review:   1) Summary: The paper presents a novel algorithm LCP for policy optimization under Linear Temporal Logic (LTL) constraints in reinforcement learning. The key contributions include a systematic framework for LTLconstrained policy optimization a modelbased approach with guarantees of both task satisfaction and cost optimality and an empirical evaluation in two domains. The algorithm is shown to outperform existing methods and offer efficient sample complexity. The theoretical analysis and experimental results demonstrate the effectiveness of the proposed approach.  2) Strengths and weaknesses: Strengths:  The paper addresses an important problem in reinforcement learning by introducing an innovative approach for policy optimization under LTL constraints.  The algorithm is wellmotivated systematically developed and supported by rigorous theoretical analysis.  The experimental evaluation in diverse domains demonstrates the efficacy and competitiveness of the proposed method.  The sample complexity analysis provides valuable insights into the efficiency of the algorithm. Weaknesses:  The assumptions made in the paper particularly related to the lower bound on transition probabilities and bound on costs may limit the generalizability of the algorithm.  The complexities of the LTL expressions and the need for strong assumptions on model dynamics and cost functions could raise concerns about scalability and applicability in more complex scenarios.  The experiments while insightful are limited to specific domains and may benefit from broader evaluation to showcase the algorithms versatility.  3) Questions:  Can the algorithm handle more complex and diverse LTL expressions beyond the examples provided in the paper  How sensitive is the performance of the algorithm to variations in the assumptions made such as the lower bound on transition probabilities  Are there plans to extend the experimental evaluation to additional domains or scenarios to further validate the algorithms effectiveness and generalizability  4) Limitations:  The paper could benefit from discussing the limitations and potential challenges in realworld applications of the proposed algorithm especially considering the simplifying assumptions made in the theoretical analysis.  The robustness of the algorithm to noise or uncertainties in the environment dynamics and transition probabilities could be a limitation worth exploring further. Overall the paper presents a wellstructured and theoretically grounded approach to a challenging problem in reinforcement learning. The contributions are significant and the empirical results are promising. Addressing the limitations and potential future directions will enhance the impact and applicability of the proposed algorithm in practical settings.  This peer review provides an analysis of the papers content strengths weaknesses questions raised by the content and potential limitations that could affect the practical application of the proposed algorithm. It aims to provide constructive feedback that can guide further development and improvement of the research.", "pluyPFTiTeJ": " Peer Review: Summary: The paper focuses on a reformulation of penaltybased domain generalization by proposing an approach that minimizes the penalty under the constraint of optimality of the empirical risk. This reformulation aims to address the issue of excessive risk associated with joint optimization of the empirical risk and penalty in existing methods. The paper introduces a novel method called Satisficing Domain Generalization (SDG) and demonstrates its effectiveness by applying it to existing domain generalization models such as CORAL FISH and VRex on benchmark datasets like WILDS and DomainBed. The method employs tools from ratedistortion theory to design an iterative algorithm that solves the proposed optimization problem efficiently. Strengths: 1. Novel Approach: The paper introduces a novel reformulation of penaltybased domain generalization addressing the issue of excessive empirical risk. 2. Theoretical Foundation: The paper is wellsupported by theoretical analysis drawing connections to ratedistortion theory and gradientbased methods. 3. Empirical Validation: Extensive empirical evaluations on benchmark datasets demonstrate the effectiveness of the proposed SDG method across different domain generalization models. 4. Generality and Applicability: The paper showcases the generality of the SDG method by applying it to multiple existing domain generalization models beyond CORAL and across different benchmark datasets. Weaknesses: 1. Computational Complexity: A major drawback highlighted is the additional computational complexity of the SDG method which requires gradients per domain potentially impacting training time significantly. Suggestions on mitigating this limitation in future research would strengthen the paper. 2. Limited Explanation of Limitations: While the paper details the limitations of the proposed method more indepth discussion or analysis of how these limitations could impact realworld applications would enhance the completeness of the study. Questions: 1. How does the proposed method handle scenarios where there is inherent contradiction between indistribution and outofdistribution performance as mentioned in the reformulation 2. Can the paper provide more insight into potential adaptations or approximations that could be made to reduce the computational complexity associated with the proposed SDG method 3. How does the SDG method compare with existing methods in terms of scalability to larger datasets or more complex domain generalization tasks Limitations: 1. While the paper covers a wide range of domains and presents strong empirical results the scalability and generalizability of the SDG method to even larger and more diverse datasets or domains could be a potential limitation. 2. The additional computational complexity introduced by the method as noted might limit its practical applicability in realworld settings with constraints on computing resources. Overall the paper presents a wellstructured and insightful contribution to the domain generalization research field with a novel reformulation and method that shows promising results. Addressing the computational complexity issues and providing further insights into the practical applications and limitations of the proposed method could enhance the impact and applicability of the study.", "q-tTkgjuiv5": " Summary The paper investigates fair allocation of graphical resources based on maximum matchings in induced subgraphs. It explores Maximin Share (MMS) fairness and EnvyFreeness up to one item (EF1) for homogeneous and heterogeneous agents. For homogeneous agents constantapproximation polynomialtime algorithms for MMS fairness and EF1 allocations are designed with a focus on social welfare guarantees. However for heterogeneous agents strong impossibility results are shown for MMS fairness while EF1 allocations offer limited social welfare guarantees except for special cases.  Strengths and Weaknesses Strengths: 1. The paper addresses an important realworld problem of fair resource allocation in graphical scenarios. 2. Clear and structured presentation of the theoretical frameworks algorithms and their theoretical guarantees. 3. The paper provides detailed explanations of the proposed algorithms theorems and related works in the field. Weaknesses: 1. Lack of empirical evaluation or practical application of the proposed algorithms on real datasets or scenarios. 2. The paper predominantly relies on theoretical analysis potentially limiting its direct impact on practical implementations. 3. The absence of detailed proofs and explanations in the main body might hinder the readability for nonexperts in the field.  Questions 1. Could the proposed algorithms be extended to handle more complex scenarios such as different types of valuations or additional constraints 2. How scalable are the polynomialtime algorithms proposed for fair allocation especially when applied to larger graphs or a higher number of agents 3. Is there any consideration for the computational complexity or scalability of the algorithms especially in terms of space or memory requirements 4. Are there any considerations or discussions on the robustness of the algorithms to potential changes in the graph structure or agent preferences over time  Limitations 1. Lack of empirical validation or realworld application might limit the practical utility of the proposed algorithms. 2. Theoretical guarantees provided for homogeneous agents may not directly translate to realworld scenarios with diverse agent types and preferences. 3. The scope is restricted to specific scenarios like binaryweight functions or twoagent cases potentially overlooking the broader applicability of the proposed methodologies. Overall the paper offers valuable insights into fair resource allocation in graphical contexts but further empirical validation and broader applicability considerations would enhance its impact.", "xDaoT2zlJ0r": " Summary This paper introduces a novel approach called First IntegralPreserving Neural Differential Equation (FINDE) to identify first integrals of unknown dynamical systems by leveraging neural networks. The proposed method finds and preserves first integrals even in the absence of prior knowledge about the underlying structures. Experimental results demonstrate the efficacy of FINDE in predicting future states and identifying various quantities consistent with wellknown first integrals of target systems.  Strengths 1. Innovative Method: FINDE offers a unique approach to identify first integrals even in situations where underlying structures are unknown. 2. Experimental Validation: The paper provides experimental evidence showcasing the effectiveness of FINDE in predicting future states and capturing first integrals. 3. Theoretical Foundation: Theoretical background and proofs provided for the proposed method add credibility to the approach.  Weaknesses 1. Computational Complexity: The methods computational complexity especially for training is highlighted as a limitation which could hinder realworld applications. 2. Scope Limitations: The study is constrained to finitedimensional Euclidean spaces limiting the generalizability of the method. 3. Limited Benchmarking: The paper focuses on specific datasets potentially limiting the general applicability of the proposed method.  Questions 1. Can the findings from the experimental evaluations be generalized to a broader range of dynamical systems beyond the specific datasets used in this study 2. Have there been comparisons made with other stateoftheart methods for identifying first integrals to demonstrate the superiority or uniqueness of FINDE 3. How does the computational complexity of FINDE compare with other methods in the field and are there strategies proposed to mitigate the identified computational challenges  Limitations 1. The study is limited to finitedimensional Euclidean spaces potentially restricting the applicability of the proposed method to more complex systems. 2. Computational overhead during training could hinder the scalability and practicality of the approach in realworld applications. 3. Generalizability to a broader range of dynamical systems needs further exploration to establish the methods robustness across various domains. Overall the paper presents an innovative approach in identifying first integrals using neural networks supported by theoretical foundations and experimental validations. Further research could focus on addressing the challenges related to computational complexity and expanding the scope of the method to more complex systems.", "xOqqlH_E5k0": " Summary The paper introduces an augmented deep unrolling neural network for snapshot compressive hyperspectral imaging aiming to reconstruct hyperspectral images from snapshot measurements. The network incorporates innovative modules for gradient update and proximal mapping including a memoryassistant gradient update and a crossstage selfattention mechanism. Additionally a spectral geometry consistency loss is proposed for better reconstruction. Extensive experiments demonstrate the performance advantage of the proposed approach over existing methods on various datasets.  Strengths and Weaknesses Strengths: 1. Innovative Architecture: The incorporation of memoryassistant gradient update and crossstage selfattention modules is a unique approach. 2. Performance: Extensive experiments show significant performance improvement over existing methods on different datasets. 3. Ablation Studies: Ablation studies help demonstrate the importance of each component of the proposed architecture. Weaknesses: 1. Limited Explanation: The paper lacks detailed explanations in some sections making it challenging for readers to fully grasp the methodology. 2. Reproducibility: The absence of code data and instructions to reproduce the experimental results and the promise to release them upon acceptance might hinder reproducibility.  Questions 1. Implementation Details: Can you provide more insights into the key hyperparameters used for training the network such as the learning rate batch size and optimizer settings 2. Comparison Metrics: In addition to PSNR and SSIM did you consider other evaluation metrics to assess the reconstruction performance comprehensively 3. Generalization: How does the proposed network architecture perform on unseen datasets or under different imaging conditions 4. Complexity vs. Performance: How does the proposed MadcapNet strike a balance between model complexity and reconstruction performance compared to existing methods  Limitations 1. Reproducibility: The lack of code data and detailed training instructions may prevent reproducibility and hinder the broader adoption of the proposed method. 2. Model Interpretability: While the performance is highlighted more insights into the interpretability of the model and its decisionmaking processes could enhance the papers quality. 3. Ethical Considerations: While some ethical aspects are briefly mentioned a more detailed discussion on potential societal impacts would strengthen the paper. Overall the paper presents a novel approach to hyperspectral imaging reconstruction with promising results. Addressing the identified weaknesses and limitations would further enhance the papers contribution to the field.", "toR64fsPir": " Summary This paper presents a novel tensorbased latent space model (TLSM) for structurepreserving embedding in multilayer networks with community structure. The model aims to capture heterogeneity among vertices and layerspecific effects while preserving the original vertex relations. The paper establishes theoretical results on asymptotic consistencies and provides extensive numerical experiments on both synthetic and reallife multilayer networks.  Strengths  The paper introduces a novel and flexible TLSM model dedicated to preserving the structure of multilayer networks.  The theoretical results on asymptotic consistencies add rigor to the proposed method.  Extensive numerical experiments on various synthetic and reallife datasets demonstrate the effectiveness of TLSM compared to competitors.  Clear explanations of model components the optimization algorithm and theoretical proofs are provided.  Weaknesses  The paper lacks a discussion on potential negative societal impacts and could benefit from including such considerations.  The presentation could be further improved by enhancing the readability and organizing the content for easier comprehension.  The details on the selection and performance of hyperparameters could be elaborated to provide more insights for readers.  Questions 1. How robust is the TLSM model to variations in the network characteristics such as sparsity community sizes and layer heterogeneity 2. Are there specific scenarios or types of networks where TLSM may not perform as effectively as in the demonstrated experiments 3. How does the proposed communityinducing regularizer impact the convergence and performance of TLSM in practical applications 4. Can the theoretical results on asymptotic consistencies be further extended to incorporate more complex network structures or conditions  Limitations  The paper could further discuss the generalizability and scalability of TLSM to larger and more diverse multilayer network datasets.  While the numerical experiments demonstrate TLSMs superior performance a more detailed analysis of tradeoffs and potential failure cases could provide deeper insights.  The assumptions made in the theoretical analysis may not fully capture the complexities of realworld multilayer networks suggesting a need for further validation and exploration in diverse settings. Overall the paper offers a solid contribution to the field of structurepreserving embedding in multilayer networks with theoretical backing and empirical validation. Addressing the identified weaknesses and limitations could strengthen the impact and clarity of the research.", "rHnbVaqzXne": " Peer Review  1) Summary: The paper introduces a novel rotated hyperbolic wrapped normal distribution (RoWN) as an alteration of the hyperbolic wrapped normal distribution (HWN) to address limitations in representing hierarchical data in hyperbolic space. The study provides a thorough analysis of the geometric properties of the diagonal HWN highlighting its shortcomings. By proposing RoWN the authors demonstrate improvements in representation learning on hierarchical datasets through empirical verification. The experiments cover synthetic binary tree WordNet and Atari 2600 Breakout datasets showcasing the effectiveness of RoWN compared to existing distributions.  2) Strengths:  Indepth Analysis: The paper provides a detailed analysis of the geometric properties of HWN exposing limitations and motivating the need for RoWN.  Novel Proposal: Introducing RoWN as a remedy to the shortcomings of HWN is a significant contribution to probabilistic modeling in hyperbolic space.  Empirical Verification: The empirical results on various datasets validate the efficacy of RoWN emphasizing its superiority over existing distributions.  Clear Description: The paper offers a wellstructured explanation of concepts algorithms and experiments facilitating understanding for readers in the field.  3) Weaknesses:  Generalization Limitation: The study is confined to hyperbolic space lacking exploration of generalization methods for other Riemannian spaces.  Optimization Challenges: Optimization difficulties with the full covariance HWN are discussed but not thoroughly addressed suggesting a need for further investigation into better optimization algorithms.  Experiments Scope: While experiments cover different datasets the scope could be extended to more diverse datasets to enhance the applicability of RoWN.  4) Questions:  How does RoWN compare to other existing distributions in hyperbolic space beyond the ones mentioned in the paper  Are there specific scenarios where RoWN may not perform as well as expected and if so what factors influence its performance  Can the proposed RoWN be extended or adapted to work efficiently in different Riemannian spaces apart from hyperbolic space  5) Limitations:  The study focuses solely on hyperbolic space limiting the generalizability of RoWN to other Riemannian spaces.  Optimization challenges with the full covariance HWN suggest the need for further exploration of better optimization techniques for hyperbolic distributions.  The experiments while insightful could benefit from a broader range of datasets to showcase RoWNs versatility and robustness across various applications. In conclusion the paper presents a valuable contribution to probabilistic modeling in hyperbolic space through the introduction of RoWN. The thorough analysis novel proposal and empirical verification demonstrate the potential of RoWN in enhancing representation learning for hierarchical datasets. Addressing the raised questions and limitations can further strengthen the study and its implications for future research in this domain.", "qw3MZb1Juo": " Summary: The paper investigates the issue of forgetting in federated learning and its impact on data heterogeneity. By drawing an analogy to continual learning the study suggests that the global model forgets knowledge from previous rounds resulting in data heterogeneity problems. To address this the authors propose a novel algorithm called Federated NotTrue Distillation (FedNTD) which aims to prevent forgetting by preserving global knowledge outside of the local distribution. Experimental results demonstrate that FedNTD achieves stateoftheart performance across various setups without compromising data privacy or increasing communication costs.  Strengths: 1. Innovative Approach: The paper introduces a new perspective on the issue of forgetting in federated learning and proposes a novel algorithm to address it. 2. Thorough Analysis: The study conducts a systematic investigation into the phenomenon of forgetting providing detailed empirical observations and theoretical analyses. 3. Empirical Validation: The experimental results support the effectiveness of the proposed FedNTD algorithm showcasing improved performance in mitigating data heterogeneity issues. 4. Relevance: The research has practical implications for improving the performance and robustness of federated learning systems in realworld scenarios.  Weaknesses: 1. Complexity: The paper delves into technical details and formalisms which may make it challenging for readers less familiar with the field to grasp the content easily. 2. Generalizability: While the proposed FedNTD algorithm shows promising results more extensive experiments on a broader range of datasets and scenarios could further validate its generalizability.  Questions: 1. How does the FedNTD algorithm compare to existing methods addressing data heterogeneity and forgetting in federated learning 2. Are there specific scenarios or datasets where FedNTD may not be as effective and if so how could its performance be improved in such cases 3. Can you elaborate on the computational and training efficiency of FedNTD compared to other federated learning algorithms that tackle the data heterogeneity problem 4. How can the insights from this study be applied to practical federated learning systems and industry applications  Limitations: 1. Dataset Dependency: The experiments focus on specific datasets like CIFAR10 and MNIST which may limit the generalizability of the findings to other types of data. 2. Simplification: The study assumes uniform distribution for certain analyses which may oversimplify the complexity of realworld data distributions. 3. Algorithm Complexity: The implementation and practical considerations of applying FedNTD in largescale federated learning systems need to be further explored and evaluated. 4. Scalability: The study does not extensively address the scalability of the FedNTD algorithm to larger federated learning setups with a higher number of clients and diverse data distributions. Overall the paper presents a valuable contribution to understanding and addressing the issue of forgetting in federated learning with the FedNTD algorithm showing promise in improving model performance under data heterogeneity constraints. Further investigation and validation across diverse datasets and scenarios could enhance the applicability and impact of the proposed algorithm in federated learning settings.", "oUigTwc7Cw5": " Peer Review  Summary The paper presents a comprehensive account of retinal ganglion cell (RGC) organization in relation to the efficient coding theory using a model optimized for natural videos. The study investigates the emergence of new RGC cell types and mosaic organization as channel capacity is varied. The results suggest that the number and characteristics of RGC cell types depend on channel capacity with new cell types focusing on higher temporal frequencies and larger spatial areas. The paper also explores the transition from mosaic alignment to antialignment across cell types with changing noise levels offering insights into the retinal functional diversity.  Strengths 1. The paper effectively links efficient coding theory to explain the formation of RGC cell types and mosaic organization. 2. Comprehensive account of how changing channel capacity influences the emergence of new cell types and their specialization in encoding temporal features. 3. Clear illustration of the shift from mosaic alignment to antialignment based on noise levels in the system. 4. The models ability to replicate experimentallyobserved shapes and spatial RF tiling is a significant strength.  Weaknesses 1. The paper could benefit from a more detailed discussion on the practical implications of the findings in understanding retinal circuit function. 2. The study focuses mainly on theoretical aspects without direct experimental validation of the models predictions. 3. The paper could address potential limitations and challenges in translating the findings to realworld applications or experimental validation.  Questions 1. Are there plans to validate the models predictions or findings experimentally to confirm the emergence of new cell types and mosaic organization as channel capacity varies 2. Could the authors elaborate on potential applications or implications of the studys results in understanding retinal processing or visual system function beyond the theoretical framework 3. How does the study account for biological complexities or factors not explicitly modeled in the retinal encoding for natural videos 4. Could the authors discuss how the identified phase change in mosaic arrangement from alignment to antialignment could impact visual processing or perception in realworld scenarios  Limitations 1. The study primarily focuses on theoretical modeling and simulationbased results lacking direct experimental validation. 2. The simplifications and assumptions in the model such as linear filtering and separable filters may limit the generalization of the findings to complex biological systems. 3. The practical transferability of the results to realworld retinal processing mechanisms may require additional confirmation and validation. Overall the paper presents a novel and informative exploration of retinal ganglion cell organization and mosaic formation in response to changing channel capacity within the framework of efficient coding theory. Addressing the questions and potential limitations raised could enhance the relevance and impact of the study within the field of neurosciences.", "zfo2LqFEVY": "Peer Review: 1) Summary: The paper introduces a novel Multimodal Grouping Network (MGN) for weaklysupervised audiovisual video parsing. MGN addresses modality and temporal uncertainties by explicitly grouping semanticaware multimodal contexts. The proposed framework outperforms prior baselines on various metrics and showcases efficiency gains in terms of parameter usage. Experimental validation on the LLP Dataset demonstrates the effectiveness and generalizability of MGN especially in contrastive learning and label refinement scenarios. 2) Strengths and weaknesses: Strengths:  Introduces a novel framework (MGN) explicitly focusing on semanticaware grouping in multimodal contexts for audiovisual video parsing.  Outperforms existing baselines in weaklysupervised AVVP tasks demonstrating the superiority of MGN.  Efficiency gains with reduced parameter usage compared to prior methods.  The proposed framework showcases the generalizability to contrastive learning and label refinement methods.  Extensive experimental results and ablation studies provide detailed analysis supporting the effectiveness of MGN.  Visualization aids in demonstrating the learned classaware features and the separation of classes. Weaknesses:  Certain limitations are acknowledged such as lower gains in audio events compared to visual events suggesting potential areas for improvement.  The models performance worsens with the increase in the depth of transformer layers signaling challenges in the weaklysupervised setting with only videolevel annotations.  Addressing rare but significant events might be a challenge indicating potential biases in data capture that could affect realworld applications.  Potential solutions and future directions are suggested but could be further elaborated for clarity and depth. 3) Questions:  How does the MGN framework handle scenarios with nonaligned audio and visual data in realworld videos where temporal alignment is not guaranteed  Can you elaborate on the generalization performance of MGN to different datasets beyond the LLP Dataset particularly with varying event categories and video types  What are the computational costs associated with implementing the proposed MGN especially concerning inference time in realworld applications  How does MGN perform in scenarios where there are overlapping or complex audiovisual events that may pose challenges in event parsing 4) Limitations:  The paper presents limitations related to the relative performance gains between audio and visual events the challenge of increasing transformer layer depth and potential biases in data capturing rare events. Addressing these limitations and providing more comprehensive solutions in future research will enhance the papers impact and applicability in realworld scenarios. Overall the paper presents a compelling approach with the MGN framework for improving weaklysupervised audiovisual video parsing showcasing effectiveness and efficiency gains. Further exploration and refinement in addressing identified limitations and questions will strengthen the contribution of this research work.", "lfe1CdzuXBJ": " Peer Review  1) Summary: The paper introduces the concept of group meritocratic fairness in the context of linear contextual bandits aiming to address the challenge of selecting candidates fairly when they belong to different sensitive groups. The proposed FairGreedy policy is designed to achieve sublinear fair pseudoregret and demographic parity while approximating a fair policy. The analysis includes regret bounds simulation experiments with diverse reward distributions and an extension to scenarios where multiple candidates from different groups are considered.  2) Strengths and weaknesses: Strengths:  The paper addresses an important and timely issue of fairness in decisionmaking within the context of linear contextual bandits.  The concept of group meritocratic fairness and the FairGreedy policy are innovative and offer a strong notion of fairness.  The theoretical analysis provides rigorous results and bounds enhancing the credibility of the proposed approach.  Simulation experiments and US Census data analysis provide practical validation of the proposed FairGreedy policy. Weaknesses:  The simulations and experiments could be expanded to include more diverse scenarios and datasets to further validate the generalizability of the proposed approach.  The assumptions made such as independence of contexts and continuously distributed rewards limit the applicability in some realworld scenarios. Investigating the impact of relaxing these assumptions could be beneficial.  The discussion on future work is insightful but could have been further elaborated to provide a more comprehensive roadmap for future research directions.  3) Questions:  How robust is the proposed FairGreedy policy to deviations from the assumptions made in the analysis such as noncontinuous rewards or correlated contexts  Can the FairGreedy policy be extended or adapted to handle scenarios where the sensitive groups are not fixed in advance but change dynamically over time  How well does the FairGreedy policy perform when the sensitive groups have imbalanced sample sizes or varying importance in decisionmaking  4) Limitations:  The paper focuses on a specific setting with restrictive assumptions which may limit the generalizability of the proposed FairGreedy policy to more complex and realistic scenarios.  The practical utility of the FairGreedy policy in diverse realworld applications needs to be further explored and validated through extensive empirical studies. Overall the paper presents a novel and intriguing approach to addressing fairness in linear contextual bandits with promising theoretical results and initial empirical validations. The insights provided pave the way for future research in fair decisionmaking algorithms.", "wmsw0bihpZF": " Review:  1) Summary: The paper proposes a novel approach to optimizing data collection workflows in machine learning applications by framing it as an optimal data collection problem. The authors introduce LearnOptimizeCollect (LOC) a learningandoptimizing framework that minimizes future collection costs. They compare their framework to conventional methods showing significant improvements in meeting performance targets while keeping costs low. The paper covers various aspects including neural scaling laws active learning statistical learning theory and optimal experiment design. Experimental results over classification segmentation and detection tasks demonstrate the effectiveness of LOC in reducing failure rates and total collection costs.  2) Strengths and weaknesses: Strengths:  The paper addresses an important and practical problem in machine learning by formalizing the data collection process and offering a systematic optimization approach.  The proposed LOC framework shows promising results in reducing failure rates and total collection costs for various machine learning tasks.  The integration of theoretical analysis with empirical experiments strengthens the papers contributions.  The exploration of multivariate data collection scenarios adds depth to the research considering different types of data sources with varying costs. Weaknesses:  While the theoretical framework and empirical results are comprehensive more detailed discussions on the realworld applicability and scalability of the proposed approach could enhance the paper.  The paper could benefit from a more detailed description of the computational complexity and scalability of the proposed framework especially in scenarios with multiple data sources.  The limitations section could be expanded to discuss potential challenges or scenarios where the proposed framework may not be as effective.  3) Questions:  How does the LOC framework handle scenarios where the underlying data distribution changes over time or the data quality deteriorates gradually  Are there specific constraints or assumptions in the optimization model that might limit its applicability to certain types of machine learning tasks or data sets  Could the proposed approach be extended to address uncertainties related to model performance degradation over time considering factors like concept drift or dataset shift  4) Limitations:  The paper focuses mainly on classification segmentation and detection tasks in computer vision. It would be beneficial to discuss the generalizability of the LOC framework to other machine learning domains.  The practical implementation and deployment challenges of the proposed framework especially in realworld industrial applications could be further elaborated on.  The evaluation of the LOC framework on a broader range of data sets and tasks could provide additional insights into its effectiveness across different scenarios. Overall the paper presents a novel and potentially impactful approach to optimizing data collection workflows in machine learning. By addressing important costperformance tradeoffs the LOC framework offers a valuable contribution to the field. Further exploration and validation in diverse settings could enhance the understanding of its applicability and effectiveness.", "zGvRdBW06F5": " Peer Review:  1) Summary: The paper proposes an algorithmsystem codesign framework to enable ondevice training on tiny IoT devices using only 256KB of memory. The proposed framework addresses challenges related to quantized graphs of neural networks and limited hardware resources introducing innovations like QuantizationAware Scaling and Sparse Update. The Tiny Training Engine is presented as a lightweight training system to implement these algorithm enhancements. Experimental results demonstrate that the framework achieves comparable accuracy to cloud training while drastically reducing memory footprint and training time making it feasible for deployment on microcontrollers.  2) Strengths and Weaknesses: Strengths:  The paper addresses a significant challenge in enabling ondevice training on tiny IoT devices providing a practical solution with innovative algorithm enhancements and system optimizations.  The framework reduces memory consumption significantly while maintaining high accuracy enabling lifelong learning and privacy preservation.  The proposed techniques such as QuantizationAware Scaling and Sparse Update are well explained and show promising results in experimental evaluations.  The Tiny Training Engine is a notable contribution that offloads autodifferentiation to compile time and achieves substantial memory and speed improvements. Weaknesses:  The paper lacks a detailed discussion on the scalability of the proposed framework to different types of models beyond CNNs and to other modalities beyond vision recognition.  The experimental evaluation focuses mostly on downstream accuracy and memory savings with limited exploration of potential challenges or drawbacks of the proposed methods.  The ablation studies and analysis could be more exhaustive providing deeper insights into the effectiveness and limitations of the proposed algorithmic enhancements.  The societal impacts section could be expanded to discuss potential ethical considerations and implications of ondevice training for personalization and privacy.  3) Questions:  Can the proposed framework be adapted to support training for different types of models beyond CNNs such as RNNs or Transformers  How do the algorithmic enhancements such as QuantizationAware Scaling and Sparse Update affect training convergence and stability under realworld scenarios  What are the criteria for selecting the parameters in the algorithm such as the update ratio for Sparse Update and how generalizable are these criteria to different models and datasets  4) Limitations:  The study focuses heavily on vision recognition tasks with CNNs and may benefit from broader applicability testing across various modalities and model architectures.  The paper lacks an indepth analysis of potential drawbacks or failure modes of the proposed framework which could be important for practical deployment considerations.  The environmental impact of the study in terms of electricity consumption during dataset experimentation should be acknowledged and discussed in more detail. Overall the paper presents a valuable contribution to the field of ondevice training for tiny IoT devices with innovative algorithmic solutions and system optimizations. Further exploration and validation across different models and modalities as well as a more extensive analysis of potential limitations and societal impacts would enhance the robustness and relevance of the proposed framework.", "z9CkpUorPI": " Peer Review:  1) Summary: The paper introduces NeuroSchedule a GNNbased scheduling method for highlevel synthesis (HLS) that aims to enhance performance by efficiently arranging operations for circuitlevel implementations. Scheduling is a critical step in HLS and existing methods suffer from either high runtime or poor solution quality. NeuroSchedule proposes an efficient GNNbased approach that achieves nearoptimal solutions with a significant runtime improvement compared to ILPbased methods. Pretraining models are employed to enhance scalability across different scheduling problems. Experimental results demonstrate superior performance over stateoftheart methods making NeuroSchedule a promising advancement in HLS scheduling.  2) Strengths and Weaknesses: Strengths:  Introduces a novel GNNbased scheduling method NeuroSchedule addressing the challenges of HLS scheduling.  Formulates the scheduling problem as a learning task for the first time proposing a new machine learning framework.  Demonstrates impressive nearoptimal solutions and substantial runtime improvement over ILPbased methods.  Utilizes pretraining models to enhance scalability for various scheduling problems with different settings.  Comparative experiments show NeuroSchedule outperforms stateoftheart entropydirected methods. Weaknesses:  The pretraining dataset could be further discussed detailing the selection criteria and potential bias.  The paper lacks a detailed discussion on potential limitations or challenges in implementing NeuroSchedule in realworld scenarios.  Further insights into the implications of the results in practical HLS applications would be valuable.  Methodological considerations such as hyperparameter tuning and training convergence could be elaborated for improved reproducibility.  3) Questions:  What specific features of GNN make it wellsuited for solving the complications of operation dependencies in scheduling for HLS  How does NeuroSchedule handle scalability considerations when faced with scheduling problems of varying complexity and resource constraints  Can NeuroSchedule effectively adapt to dynamic changes in scheduling requirements within HLS workflows  What are the key factors influencing the decision to use MSE loss versus Rank loss during training and how does this impact the models predictive performance  4) Limitations:  Limited discussion on the generalizability and robustness of NeuroSchedule beyond the experimental setup.  The absence of a thorough analysis of potential biases or limitations introduced by the dataset used for pretraining.  Lack of realworld case studies or practical implementations to validate NeuroSchedules effectiveness in industrial HLS applications.  Unclear discussion on any potential tradeoffs or constraints when deploying NeuroSchedule in diverse HLS scenarios. In conclusion the paper presents a significant advancement in HLS scheduling by introducing NeuroSchedule a GNNbased method with notable improvements in both solution quality and runtime efficiency. However addressing the highlighted limitations and further exploring the implications of NeuroSchedule in realworld HLS applications would strengthen the papers impact and practical relevance.", "w_jvWzNXd6n": " Peer Review: 1) Summary (avg word length 100): The paper introduces SBMTransformer an efficient Transformer model that can adaptively adjust attention sparsity based on input data without the need to explicitly compute the full attention score matrix. The model leverages a Stochastic Block Model (SBM) to sample attention masks and achieve linear forward and backward costs. Theoretical proofs and empirical evaluations on LRA and GLUE benchmarks demonstrate the models effectiveness compared to previous efficient Transformer variants. 2) Strengths:  The paper addresses the issue of quadratic cost in Transformers by introducing a flexible dataadaptive method for adjusting attention sparsity.  The use of SBM for sampling attention masks enables the model to choose between linear to full attention leading to more efficient computations.  The theoretical foundation and empirical evaluations on LRA and GLUE benchmarks provide strong evidence of the models superiority over existing efficient Transformer variants.  Detailed explanations of the models architecture sampling algorithms and training procedures enhance understanding and reproducibility. Weaknesses:  The paper could benefit from more thorough discussions on the implications of the proposed model in realworld applications to provide clearer practical insights.  Additional comparisons with other stateoftheart models beyond the selected baselines would strengthen the evaluation and demonstrate the models effectiveness in a broader context. 4) Questions:  How does the model adapt to varying complexities in different tasks and does it require taskspecific tuning for optimal performance  Can the proposed SBMTransformer handle scalability issues when trained on larger datasets or higherdimensional inputs  Are there any implications of the sparse attention mechanisms used in SBMTransformer on interpretability and explainability of the models decisions 5) Limitations:  The paper mentions limitations related to sparse tensor operations being less optimized on GPU kernels which could impact realtime performance in practical applications.  The models adaptability to varying computational requirements based on the number of edges may introduce challenges in resource management and training stability. In conclusion the paper presents a novel and promising approach to efficient Transformers through adaptive attention sparsity. Further exploration of practical implications and scalability considerations could enhance the significance of SBMTransformer in realworld applications.", "xs9Sia9J_O": "Peer Review 1) Summary The paper addresses the issue of lossy decomposition in Individual Global Max (IGM) decomposition in cooperative multiagent reinforcement learning. It introduces the IGMDA framework combining imitation learning with IGM decomposition to prevent error accumulation. The proposed method is theoretically proven and empirically validated on challenging benchmark problems outperforming stateoftheart IGMbased approaches in zero sight view scenarios. The paper also provides indepth analyses experimental results and comparisons with existing methods. 2) Strengths and weaknesses Strengths:  The paper addresses a critical issue of lossy decomposition in IGM in cooperative multiagent reinforcement learning.  The proposed IGMDA framework is novel and theoretically justified demonstrating improved performance in challenging scenarios.  The paper provides detailed proofs methodological explanations and empirical evaluations to support the proposed framework.  Empirical results show significant improvements over existing stateoftheart methods in zero sight view environments. Weaknesses:  Lack of analysis on the scalability of the proposed method to largerscale environments with varying levels of complexity.  While empirical results demonstrate superior performance the generalizability of the IGMDA framework to diverse multiagent tasks could be further explored.  More discussion on the computational complexity and training efficiency of the proposed method could enhance the paper. 3) Questions  How does the proposed IGMDA framework handle scalability issues in largerscale environments with multiple agents and more complex tasks  Are there specific limitations or constraints of the IGMDA framework that might hinder its applicability in realworld scenarios  How flexible is the IGMDA framework in adapting to different types of cooperative multiagent tasks beyond the mentioned benchmark problems  Can the imitation learning component in IGMDA lead to overfitting or bias in learning from expert agents strategies 4) Limitations  The paper primarily focuses on zero sight view scenarios limiting the generalizability of the findings to broader multiagent settings.  The study lacks realworld application demonstrations or experiments on varied environments which would provide a more thorough evaluation of the proposed framework.  The theoretical underpinnings and empirical validations are mainly focused on specific benchmark tasks raising questions about the general applicability and robustness of the IGMDA framework in diverse scenarios. In conclusion the paper presents a novel approach to address lossy decomposition in IGM in multiagent reinforcement learning yielding promising results in challenging environments. It opens avenues for further research on improving cooperative multiagent strategies. Addressing the raised questions and limitations could enhance the impact and applicability of the proposed IGMDA framework.", "qYc8VnmUwbv": " Peer Review of the Scientific Paper: \"Efficient Algorithms for Realizable Smoothed Online Learning\"  1) Summary: The paper addresses the statistical and computational challenges in online learning by introducing computationally efficient algorithms for realizable smoothed online learning. It establishes optimal regret bounds for various function classes including Kwise linear classification and piecewise regression under smoothed online learning settings. The authors highlight the importance of the Johns ellipsoid in achieving geometric decay and log(T ) regret. They propose novel techniques involving Ville\u2019s inequality geometric measure theory and convex geometry to achieve logarithmic regret both in the horizon and smoothness parameter.  2) Strengths and weaknesses:  Strengths:  The paper presents novel computationally efficient algorithms achieving statistically optimal regret bounds for realizable online learning in various function classes.  The theoretical foundation and technical results are welldeveloped including the use of Johns ellipsoid and abstract decay analysis.  The work addresses the statisticalcomputational gap and provides insights into the structure of function classes allowing efficient learning.  Weaknesses:  The paper lacks clarity in some parts especially in the technical details of the algorithms and proofs. More explanations or examples could enhance understanding.  The theoretical results could benefit from more extensive empirical validation or practical applications to showcase the algorithm performance in realworld scenarios.  3) Questions:  How do the proposed algorithms compare with existing methods in terms of empirical performance and scalability in realworld applications  Can the introduced algorithms be generalized to accommodate nonrealizable adversaries or more complex function classes beyond the studied cases  4) Limitations:  The paper focuses on theoretical analyses and technical results without extensive empirical validation or experiments to demonstrate the algorithm efficacy.  The complexity of the presented algorithms may hinder their practical implementation or integration into existing online learning systems.  5) Overall Assessment: The paper presents significant contributions to the field of online learning by introducing computationally efficient algorithms with optimal regret bounds. The theoretical findings especially regarding smoothed online learning and function class structure are valuable for advancing the understanding of statisticalcomputational tradeoffs in online learning settings. However to enhance the impact and applicability of the work further empirical validation and practical demonstrations of the proposed algorithms are recommended. Additionally addressing the potential scalability and complexity concerns of the algorithms could broaden their usability in realworld applications. The insights provided by this research pave the way for future advancements in the field of online learning and optimization.", "sFQJ0IOkHF": " Summary The paper proposes DivBO a diversityaware framework based on Bayesian optimization to enhance the ensemble performance in Automated Machine Learning (AutoML) by injecting explicit search of diversity into Combined Algorithm Selection and Hyperparameters optimization (CASH) problems. DivBO introduces a diversity surrogate to predict pairwise diversity between configurations and uses a temporary pool and weighted acquisition function to guide the search for both performance and diversity. Empirical results on 15 public datasets show that DivBO achieves the best average ranks on both validation and test errors among the compared methods including posthoc designs in recent AutoML systems and stateoftheart baselines for ensemble learning on CASH problems.  Strengths and weaknesses Strengths: 1. Introduction of diversityaware framework for enhancing ensemble performance in AutoML. 2. Novel approach using a diversity surrogate to predict pairwise diversity between configurations. 3. Empirical results show that DivBO outperforms existing methods on public datasets in terms of validation and test errors. 4. Combining of performance and diversity considerations in the optimization process. Weaknesses: 1. Lack of detailed explanation on the computational complexity of the proposed method. 2. Limited discussion on the generalizability of DivBO to other AutoML problems beyond classification tasks. 3. The paper could benefit from a more detailed comparison with other stateoftheart methods in the field.  Questions 1. Could you explain the choice of LightGBM as the diversity surrogate over other treebased alternatives like XGBoost and probabilistic random forest 2. How does the proposed method handle the balance between exploration and exploitation in the optimization process 3. Did the authors consider the impact of dataset characteristics such as imbalance and dimensionality on the performance of DivBO  Limitations The paper lacks detailed information on the computational complexity of DivBO generalizability to other AutoML problems and a thorough comparison with other stateoftheart methods. The discussion on dataset characteristics influencing the performance of DivBO could be more comprehensive.  Overall Assessment The paper presents a novel approach DivBO to address the diversity in ensemble learning for CASH problems in AutoML. The empirical results indicate that DivBO outperforms existing methods on various datasets. However the paper could benefit from additional details on computational complexity generalizability and comparison with other methods to strengthen the findings.", "nxw9_ny7_H": " Peer Review  1) Summary The paper introduces a novel method called AugNet to learn data invariances directly from training data through learnable augmentation layers embedded in a neural network. AugNet is compared to stateoftheart automatic data augmentation (ADA) methods and demonstrates superior performance in learning invariances across different tasks including image recognition and sleep stage classification. The experiments show that AugNet achieves high generalization power and performance while avoiding the complex bilevel optimization structure of ADA methods.  2) Strengths and Weaknesses Strengths:  Novel Method: AugNet introduces a novel approach to learning data invariances directly from training data using learnable augmentation layers in neural networks demonstrating versatility and improved performance.  Versatility: The method is shown to be applicable to a broad range of learning problems beyond computer vision tasks highlighting its adaptability.  Performance: Empirical results show that AugNet achieves comparable or superior performance compared to stateoftheart ADA methods in various experiments.  EndtoEnd Approach: AugNet offers an endtoend training process avoiding the need for complex and computationally demanding bilevel optimization structures. Weaknesses:  Computational Complexity: The method may require a large number of copies for optimal performance increasing computational complexity at test time. This tradeoff between performance and speed needs to be carefully considered.  Limitations in Augmentations: AugNet is limited to augmentations that can be relaxed into differentiable surrogates and needs a predefined set of augmentations T which might restrict the variety of augmentations that can be learned.  Dependency on Hyperparameters: The approach requires tuning of hyperparameters specifically the regularization parameter \u03bb which may pose challenges in practical implementations.  3) Questions  Generalization: Have tests been conducted to evaluate the generalization capability of AugNet across various datasets and tasks beyond those mentioned in the paper  Robustness: How robust is AugNet to dataset characteristics such as noise levels imbalanced data or different modalities  Comparison Methods: How does AugNet compare to other emerging methods in the field of automatic data augmentation beyond those mentioned in the paper  Scalability: What are the implications of scaling AugNet to larger datasets or more complex tasks in terms of computational resources and training time  4) Limitations  Computational Complexity: The computational complexity of AugNet could be a limitation in realtime applications or resourceconstrained environments.  Augmentation Variability: The reliance on a set of predefined augmentations may limit AugNet\u2019s ability to adapt to unforeseen variations in input data.  Hyperparameter Sensitivity: The need for hyperparameter tuning especially the regularization parameter \u03bb could pose challenges in practical deployment scenarios. In conclusion the paper presents a significant contribution in the field of learning data invariances using AugNet showcasing promising results in various experiments. Addressing the identified limitations and questions could further enhance the robustness and applicability of AugNet in realworld settings.", "lkrnoLxX1Do": " Summary The paper presents a novel method SelfIR that leverages blurry longexposure images and noisy shortexposure images for image restoration in lowlight conditions. It introduces a selfsupervised learning approach that jointly utilizes shortexposure noisy images for deblurring and longexposure blurry images for denoising. The method enhances restoration performance by learning in a collaborative manner and benefits from the complementary information provided by the different exposure settings. Experimental results on synthetic and realworld images demonstrate the effectiveness and practicality of SelfIR in comparison with stateoftheart methods.  Strengths and Weaknesses Strengths: 1. The proposed SelfIR method addresses a challenging problem in lowlight imaging by combining shortexposure noisy and longexposure blurry images for restoration. 2. The selfsupervised learning approach removes the need for collecting paired clean images making it more practical for realworld applications. 3. The collaborative learning framework enhances restoration performance by leveraging the complementary information from different exposure settings. 4. Extensive experiments on synthetic and realworld images validate the effectiveness and superiority of SelfIR over existing methods. Weaknesses: 1. The paper lacks a detailed analysis of the computational complexity and resource requirements of the proposed method. 2. The limitation of the proposed method in handling specific types of noise or blur artifacts is not discussed. 3. The generalizability of the SelfIR model to diverse imaging scenarios and noise distributions should be further investigated. 4. The evaluation metrics used may not fully capture the perceptual quality of the restored images.  Questions 1. Could you provide more insights into the selection criteria for the hyperparameters \u03bbreg and \u03bbaux in the loss function 2. How does the proposed method perform when dealing with realworld scenes with varying levels of noise and motion blur 3. Are there plans to extend the SelfIR model to address other challenges in image restoration beyond lowlight conditions 4. Can you elaborate on the computational efficiency and inference speed of the SelfIR model especially for deployment on mobile or edge devices  Limitations 1. The evaluation of the SelfIR method primarily focuses on synthetic and limited realworld image datasets potentially limiting the generalizability of the results. 2. The lack of a detailed comparison with more diverse stateoftheart methods in lowlight imaging and image restoration limits the comprehensive evaluation of the proposed approach. 3. The reliance on specific hardware configurations or sensor models in the datasets used for experiments may restrict the applicability of the SelfIR method to a wider range of devices and imaging conditions. 4. The absence of qualitative assessments based on user studies or perceptual metrics may overlook the subjective aspects of image quality perceived by viewers. Overall the paper presents an innovative approach to lowlight image restoration through selfsupervised learning but requires further investigation and validation to address potential limitations and ensure its practical utility across diverse imaging scenarios.", "v6CqBssIwYw": " Peer Review of the Scientific Paper 1. Summary The paper introduces InstanceBased Uncertainty estimation for Gradientboosted regression trees (IBUG) a method for extending any GBRT point predictor to produce probabilistic predictions by leveraging the knearest training instances and tree ensemble kernel. Empirical results across 22 benchmark regression datasets demonstrate that IBUG achieves similar or better performance than existing stateoftheart methods in terms of probabilistic predictions and model flexibility. 2. Strengths and Weaknesses Strengths:  Clear and welldefined problem statement and proposed solution.  Extensive experimental evaluation across multiple datasets demonstrating the effectiveness of IBUG.  Flexibility in modeling output distributions and the ability to improve probabilistic performance by using different base GBRT models.  Novel approach that fills a gap in providing probabilistic estimates for GBRT models. Weaknesses:  While the paper presents significant empirical results more insight into the theoretical underpinnings of IBUG and how it compares to existing methods in that context would enhance the paper.  Limited discussion on the practical implications and potential applications of IBUG in realworld scenarios.  The paper could benefit from a thorough explanation of the tradeoffs in computational efficiency when using IBUG. 3. Questions  Can the paper provide more details on how the affinity between training examples is computed and whether this process is sensitive to the size or complexity of the dataset  How does the choice of k impact the performance of IBUG and is there any tradeoff between k and prediction accuracy or efficiency  Are there specific types of regression problems or datasets where IBUG may perform exceptionally well and are there cases where it might not be as effective 4. Limitations  The paper focuses primarily on empirical results without delving much into the theoretical or conceptual aspects of IBUG which could limit the understanding of the methods inner workings.  Limited exploration of the robustness of IBUG to noisy or unbalanced datasets which could be a potential limitation in practical applications.  The discussion on scalability especially with larger and more complex datasets is somewhat limited and more insight into potential challenges in scaling IBUG would be insightful. In conclusion the paper presents a novel and effective method for extending GBRT point predictors to probabilistic predictions. Further theoretical insights and practical considerations could enhance the overall quality and applicability of the proposed method. The empirical results are promising showcasing the potential of IBUG for probabilistic regression tasks. Overall the paper is wellstructured and presents a valuable contribution to uncertainty estimation for GBRT models and it is wellsuited for publication pending some clarifications and enhancements as suggested above.", "xqYGGRt7kM": " Peer Review 1) Summary The paper focuses on streaming algorithms for pure exploration in Stochastic Multi Armed Bandits (MABs) extending previous work on this topic. It introduces lower bounds that illustrate the challenges of achieving instancesensitive sample complexity with limited memory. Furthermore it proposes an algorithm that maintains a singlearm memory while achieving the instancesensitive sample complexity under specific assumptions. The results shed light on the tradeoffs between sample and space complexity in this domain. 2) Strengths and Weaknesses Strengths:  The paper presents novel results that contribute to the understanding of streaming algorithms in stochastic MABs.  The use of lower bounds provides valuable insights into the limitations and complexities of the problem.  The proposed algorithm that achieves instancesensitive sample complexity with minimal memory under certain conditions is a significant advancement. Weaknesses:  The paper contains complex technical details that may be challenging for readers unfamiliar with the topic to grasp.  The practical implications of the proposed algorithm and theoretical findings could be further elaborated.  Detailed explanations of the technical aspects and algorithms could enhance the accessibility of the paper. 3) Questions  Could you elaborate on the practical applications of the proposed algorithm and how it could be applied in realworld scenarios  In the case where random arm arrival and the knowledge of all gap parameters \\xe2\\x88\\x86[i]ni2 are provided does the algorithm exhibit significant improvements in performance compared to other existing algorithms  How sensitive is the algorithms performance to deviations from the assumed conditions such as random arm arrival or knowledge of certain parameters 4) Limitations  The complexity and technical nature of the paper may limit its accessibility to a broader audience especially those outside the specific research domain.  The paper focuses heavily on theoretical analysis and may benefit from additional discussion on practical implications and potential future directions for research.  The assumptions made in the algorithm design and analysis could restrict its applicability to certain scenarios raising questions about its robustness in realworld applications. Overall the paper provides valuable contributions to the field of streaming algorithms for pure exploration in Stochastic Multi Armed Bandits highlighting both theoretical insights and practical challenges in achieving instancesensitive sample complexity. Further clarification on the applicability and robustness of the proposed algorithm could enhance the impact and relevance of the research findings.", "sr0289wAUa": " Peer Review: 1. Summary: The paper introduces ASPiRe (Adaptive Skill Prior for RL) a novel approach that accelerates reinforcement learning by leveraging multiple primitive skill priors extracted from specialized datasets. This method aims to guide policy learning for new downstream tasks by combining these skill priors adaptively. ASPiRe includes an Adaptive Weight Module to infer weights for the skill priors and demonstrates improved learning efficiency and task success rates over competitive baselines in challenging environments with sparse rewards. 2. Strengths and weaknesses: Strengths:  Clear and detailed explanation of the proposed method and its components.  Wellstructured experiments testing ASPiRes effectiveness in various complex tasks with sparse rewards.  Demonstrated the benefits of using multiple primitive skill priors compositional skills as regularization and adaptively assigning weights.  Comprehensive comparison with alternative approaches showcasing ASPiRes superiority in learning efficiency and task success. Weaknesses:  The approach requires labeled and separated datasets for extracting primitive skill priors which might limit its applicability in realworld scenarios where such datasets may not be readily available.  The need for manual separation and labeling of datasets for primitive skills can be cumbersome and subjective if not automated.  The paper could have included more discussion on potential realworld applications and scalability of the proposed method. 3. Questions:  How does ASPiRe handle situations where the primitive skill priors overlap or the distinction between tasks for each primitive skill is not clear  Can the method automatically adapt to new tasks without requiring manual intervention in selecting and combining skill priors  How sensitive is the performance of ASPiRe to the number and variety of primitive skills extracted from datasets  Are there any considerations or mechanisms in place within ASPiRe to handle noisy or incomplete data when learning the primitive skill priors 4. Limitations:  One limitation is the requirement for labeled and separated datasets for extracting distinct primitive skill priors which may not always be feasible or practical.  The manual intervention needed to define and label primitive skill priors could introduce bias or inconsistency in skill extraction.  The reliance on multiple skill priors for downstream task learning might introduce complexity in understanding the interactions between different priors and their impacts on policy learning. In conclusion the paper presents a promising approach with ASPiRe for accelerating reinforcement learning through adaptive skill priors. The method shows significant improvements over existing baselines but it also raises important questions regarding the scalability and practicality of the proposed technique. Automating the process of extracting primitive skill priors and further exploring the adaptability of ASPiRe to new tasks could enhance its utility in diverse realworld applications.", "tZUOiVGO6jN": " Peer Review: 1) Summary: The paper \"Efficient Parallel DNN Training via Dependent Sampling Algorithm\" proposes a new data loading method JOADER for training parallel DNNs with overlapped datasets. The proposed method aims to improve training efficiency by reusing data preparation work and enhancing sampling locality with the use of a dependent sampling algorithm (DSA) and a domainspecific cache policy. A novel tree data structure is also introduced to efficiently implement the DSA. Experimental results show significant improvements in training speed especially in asynchronous cases. The paper addresses an important and practical problem in deep learning model training. 2) Strengths:  The paper addresses a significant issue in parallel DNN training and presents a novel solution.  The proposed method shows substantial improvements in training efficiency without compromising accuracy as evidenced by the experimental results.  The incorporation of the dependent sampling algorithm domainspecific cache policy and the treebased data structure demonstrates a comprehensive approach to tackling data loading challenges.  The evaluation conducted on ImageNet with various ResNet models provides valuable insights into the effectiveness of the proposed method.  The paper is wellstructured with clear explanations of the proposed techniques and their implementation. Weaknesses:  The paper might benefit from providing more detailed comparisons with existing methods particularly in scenarios where strong constraints are not present.  While the proposed method shows promising results more indepth analysis of the potential limitations and edge cases could further strengthen the paper.  The evaluation section could be expanded to include more diverse datasets and models to better demonstrate the generalizability of the proposed method.  The paper could discuss potential caveats or challenges in implementing the proposed method in realworld applications. 3) Questions:  Can the proposed method be easily integrated into existing deep learning frameworks beyond PyTorch  How does the performance of JOADER vary on datasets with different characteristics or sizes  Are there specific conditions or scenarios where the proposed method may not be as effective or may require additional tuning  Has the proposed method been tested on datasets with high class imbalances or complex structures to assess its robustness  What are the potential tradeoffs or drawbacks associated with the efficient training speed improvements achieved using the proposed method 4) Limitations:  The paper could provide a deeper discussion on potential limitations or challenges in realworld deployment of the proposed method.  The generalizability of JOADER to diverse datasets and applications warrants additional investigation.  The effectiveness of the proposed method under various hardware configurations or constraints could be further explored.  Discussion on the scalability of the proposed method to largescale distributed training scenarios could enhance the papers impact. Overall the paper presents a valuable and innovative approach to improving parallel DNN training efficiency. Further insights into the algorithms limitations performance on diverse datasets and practical application considerations could enhance the papers contribution to the field of deep learning research.", "wN1CBFFx7JF": " Peer Review  1) Summary The paper investigates the theoretical properties of deep neural networks in modeling nonlinear time series data with temporal dependence among observations. The theoretical results establish consistency rates for prediction errors of deep feedforward neural networks under mixingtype assumptions demonstrating the effect of temporal dependence on neural network performance. The study includes numerical simulations and real data application to macroeconomic data showcasing the practical implications of the theoretical findings.  2) Strengths and weaknesses Strengths:  The paper addresses an important gap in theoretical developments by focusing on deep neural networks for modeling temporally dependent time series data.  The theoretical results are supported by numerical simulations and application to real macroeconomic data enhancing the practical relevance of the study.  The incorporation of composite regression functions and consideration of various time series models add depth and breadth to the analysis. Weaknesses:  The paper heavily focuses on the theoretical aspects possibly limiting the accessibility to a wider audience who may not have a strong background in the mathematical underpinnings of neural networks.  The application of ReLU as the only activation function could be too restrictive potentially missing out on exploring the impact of other activation functions on model performance.  While the simulations and real data application are included a more detailed discussion and analysis of the results could enhance the practical implications section of the study.  3) Questions  How generalizable are the findings to other types of time series data beyond the specific setups considered in the paper  Can the study be extended to explore the impact of different activation functions or network architectures on prediction error consistency  What are the implications of the additional logarithmic factors in the convergence rates due to temporal dependence for practical model implementation and interpretation  4) Limitations  The paper heavily focuses on theoretical developments potentially limiting its accessibility to a broader audience.  Restricting the study to ReLU activation function might overlook insights that could be gained from exploring other activation functions.  The practical implications of the theoretical findings could be further emphasized and analyzed in more depth especially in the context of the real data application. Overall the paper makes a valuable contribution to the understanding of how deep neural networks can be used to model nonlinear time series data with temporal dependencies. By addressing theoretical properties and providing empirical validation the study offers insights that can benefit both the theoretical and practical aspects of time series analysis with neural networks.", "m67FNFdgLO9": " Peer Review: 1) Summary: The paper introduces the Dense Interspecies Face Embedding (DIFE) approach which aims to understand faces of various animals by extracting common features shared among different species including humans. The method leverages multiteacher knowledge distillation of CSE and StyleGAN2 along with semantic matching loss to overcome challenges such as data scarcity ambiguous connections between species faces and extreme shape variations. DIFE is evaluated through interspecies facial keypoint transfer tasks on datasets like MAFL and AP10K demonstrating promising results in interspecies face image manipulation and dense keypoint transfer tasks. The study showcases the potential of DIFE in enabling various crossdomain applications beyond human faces. 2) Strengths and Weaknesses: Strengths:  Novel approach addressing interspecies face understanding through DIFE.  Utilization of multiteacher knowledge distillation technique.  Synthesis of pseudopaired images and semantic matching loss to enhance performance.  Comprehensive evaluation on different datasets and crossdomain keypoint transfer tasks.  Applicability in image manipulation face parsing and dense keypoint transfer tasks demonstrated. Weaknesses:  Dependency on the performance of pretrained CSE and StyleGAN2 models.  Lack of detailed comparison with more existing methods and benchmarks.  The approachs generalization to multiple animal domains beyond the evaluated three domains is not thoroughly explored.  Limited discussion on failure cases and challenges in practical scenarios like occlusion and illumination variations.  Code availability mentioned but details on implementation complexity or challenges are missing. 3) Questions:  How sensitive is the performance of DIFE to variations in data quality or the pretrained models used  Could the method be extended to generalize across multiple animal species beyond the three domains evaluated  How does DIFE perform in scenarios with partial occlusion or under different lighting conditions  Are there any considerations for applying the method in realworld applications such as wildlife monitoring or pet identification  How scalable is the approach for potential future applications and more extensive datasets 4) Limitations:  The study primarily focuses on three animal species without exploring a broader range of animal domains.  Absence of indepth analysis on failure cases and limitations of the proposed method.  Limited discussion on scalability and practical applicability in realworld scenarios.  The generalization capability of the approach to handle diverse environmental conditions and occlusions is unclear.  Lack of thorough comparison with more existing stateoftheart methods in interspecies face understanding. Overall the paper presents an intriguing approach to interspecies face understanding through DIFE demonstrating promising results across various crossdomain tasks. Further exploration into extending the method to diverse animal species addressing realworld challenges and robustness testing could enhance the applicability and impact of the proposed approach. Additional discussions on failure cases and practical considerations would further strengthen the studys contributions and relevance in the field of face understanding across different animal species.", "m97Cdr9IOZJ": " Peer Review: 1) Summary (108 words): This paper introduces ParaCFlows a novel neural network model and proves its Ckuniversality for approximating diffeomorphisms. The paper showcases the application of ParaCFlows in contextual Bayesian optimization tasks demonstrating superior optimization performance and gradient approximations compared to other neural surrogate models. The authors provide theoretical proofs empirical results and comparisons with other models to support the effectiveness of ParaCFlows. Additionally the paper addresses the limitations of existing models and presents a comprehensive analysis of the models capabilities. 2) Strengths:  The demonstration of the Ckuniversality of ParaCFlows for approximating diffeomorphisms is a significant theoretical contribution.  The paper presents a structured proof that connects the models architecture to its expressiveness.  The empirical results especially in the contextual Bayesian optimization tasks clearly showcase the superior performance of ParaCFlows.  The clear explanation of the model architecture and comparison with other models provides a comprehensive understanding of model capabilities. 3) Weaknesses:  The paper could benefit from more detailed explanations of the experimental setup and evaluation metrics used in the empirical studies.  While the theoretical proofs are thorough a more detailed discussion on the practical implications of the theoretical results could enhance the papers impact.  The paper lacks a discussion on the limitations and potential challenges faced in implementing ParaCFlows in realworld scenarios.  The abstract and introduction sections could be more concise to improve readability and clarity. 4) Questions:  How does the computational complexity of ParaCFlows compare to other neural surrogate models especially in terms of training time and inference speed  Are there scenarios where ParaCFlows may not be the most suitable model and if so how could these limitations be addressed  How does the model generalize to realworld optimization problems with highdimensional and dynamic contexts  Are there any practical considerations or tradeoffs that need to be made when implementing ParaCFlows in complex optimization tasks 5) Limitations:  The generalizability of the model to a broader range of optimization tasks and datasets could be a potential limitation which may require further exploration and validation.  The reliance on theoretical guarantees and empirical results from synthetic benchmarks may not fully represent the challenges and complexities of realworld optimization problems.  The scalability of ParaCFlows to very highdimensional contexts or actions may pose challenges that need to be addressed before practical deployment. In conclusion the paper presents a novel neural network model with promising theoretical and empirical results. Further exploration and validation in diverse optimization scenarios could enhance the models applicability and impact in realworld applications.", "mhe2C2VWwCW": " Peer Review 1. Summary: The paper introduces a novel typology for predictive queries in neural autoregressive sequence models which aims to make complex queries tractable by representing them with elementary building blocks. The authors develop new query estimation methods based on beam search importance sampling and hybrids demonstrating their effectiveness across various largescale sequence datasets and language models like GPT2. The work explores the computational challenges of answering predictive queries beyond traditional onestepahead predictions. 2. Strengths:  The paper addresses an important and challenging problem in neural sequence modeling providing a systematic approach to handle predictive queries beyond traditional predictions.  The proposed typology and query estimation methods demonstrate significant improvements over naive forward simulation showcasing the efficacy of the approach.  The work is supported by detailed experiments across multiple datasets providing a comprehensive evaluation of the proposed methods.  The discussion on the tradeoffs between search and sampling methods as well as the impact of model entropy on query estimation adds valuable insights to the field. 3. Weaknesses:  While the experiments are thorough the evaluation is primarily focused on hitting time queries potentially limiting the generalizability of the findings to more diverse query types.  The restrictions on the datasets used particularly the limitation to datasets with small vocabularies might limit the applicability of the results to models with larger vocabularies typically seen in natural language processing tasks.  The practical implications of the proposed methods in realworld applications especially in terms of scalability and interpretability could be better discussed. 4. Questions:  Could the proposed typology and query estimation methods be extended to handle more complex queries beyond those specifically discussed in the paper  How do the proposed methods perform in scenarios with noisy or incomplete data and are there any strategies to mitigate such challenges  What are the potential implications of the findings for improving the efficiency and interpretability of neural autoregressive models in realworld applications 5. Limitations:  The papers reliance on a select few datasets and models with small vocabularies may limit the generalizability of the findings to more diverse and complex scenarios.  The focus on hitting time queries may restrict the applicability of the proposed methods to a broader range of predictive queries warranting further investigation into their performance on diverse query types.  The lack of detailed discussion on potential negative societal impacts and ethical considerations associated with predictive querying in neural models could be a limitation in addressing broader implications of the research. Overall the paper presents a significant contribution to the field of neural sequence modeling by introducing a novel typology and query estimation methods for handling predictive queries. By addressing computational challenges and providing empirical evaluations the work provides valuable insights and opens up avenues for future research in enriching the capabilities of autoregressive models.", "rO6UExXrFzz": " Peer Review of Paper  1) Summary: The paper introduces a novel approach to control the capacity of the composition of function classes by adding Gaussian noise to their output. Through a series of theorems and experiments the paper demonstrates how this noise addition can effectively bound the uniform covering number of composite classes leading to improved generalization bounds. The focus is on neural networks particularly in the context of deep learning where the proposed method shows promising results for reducing the NVAC metric while maintaining model performance.  2) Strengths and Weaknesses: Strengths:  The paper addresses a significant problem in machine learning  controlling the capacity of composite function classes.  The theoretical foundation provided by new definitions and theorems is thorough and wellreasoned.  Empirical experiments on the MNIST dataset support the theoretical claims providing practical validation.  The proposed method of adding Gaussian noise offers a simple yet effective way to manage capacity with minimal impact on model performance. Weaknesses:  The general applicability of the method to other types of neural networks beyond those with bounded activation functions is not directly addressed.  The experimental validation is limited to one dataset a broader range of datasets could further strengthen the empirical results.  3) Questions:  Could the proposed method of noise addition be extended to activation functions beyond bounded activation functions such as ReLU  How sensitive is the method to the choice of noise level particularly in scenarios where larger noise amounts may be required  Are there any implications or considerations for applying this method to neural networks with different architectures or for different tasks  4) Limitations:  The assumption of bounded activation functions limits the direct applicability to neural networks with unbounded activations like ReLU.  The empirical validation is restricted to a single dataset which may not fully capture the methods performance across various domains and tasks. Overall the paper presents an innovative approach to controlling the capacity of composite function classes in deep learning with both theoretical and empirical support. Further exploration into the methods robustness and applicability to diverse neural network architectures would enhance the papers impact and generalizability.", "zGPeowwxWb": " Peer Review:  Summary: The paper presents an innovative approach to modeling diffusion models using a deep equilibrium (DEQ) framework aiming to address the challenge of long sampling chains typically required for highfidelity image generation. By formulating the generative process of denoising diffusion implicit models as a DEQ the authors demonstrate improvements in single image sampling and model inversion tasks. The proposed method shows promising results on various datasets including CIFAR10 CelebA and LSUN Bedroom and Churches.  Strengths: 1. Innovative Approach: The paper introduces a novel perspective on modeling diffusion models through a DEQ framework providing a unified and efficient solution for image generation tasks. 2. Performance Demonstrations: The methods performance is extensively evaluated across different datasets showcasing improvements in single image generation and model inversion tasks. 3. Technical Depth: The paper provides thorough explanations of the proposed DEQ formulation the benefits it offers and the experimental results obtained contributing to a comprehensive understanding of the research.  Weaknesses: 1. Complexity: The technical nature of the paper might pose challenges for readers not wellversed in deep learning concepts and diffusion models. 2. Comparison: While the results demonstrate the efficacy of the proposed method a more extensive comparison with existing stateoftheart methods could strengthen the evaluation.  Questions: 1. The paper mentions utilizing Anderson acceleration as the default fixed point solver. Could the authors elaborate more on the choice of this solver over other alternatives and its impact on the results 2. How does the proposed DEQ formulation specifically address the limitations related to long sampling chains in diffusion models Could the authors provide more insights into the mechanism behind this improvement 3. In the model inversion experiments what were the key factors that contributed to the superior performance of DEQ over the baseline method Was there any tradeoff in terms of computational complexity  Limitations: 1. Generalizability: The paper focuses on specific datasets and scenarios potentially limiting the generalizability of the proposed DEQ formulation to broader applications. 2. Complexity: The DEQ formulation while innovative may introduce additional complexity in implementation and interpretation warranting careful consideration in practical applications. 3. Evaluation Metrics: While the FID scores provide a quantitative evaluation of image quality additional perceptual quality metrics or user studies could offer a more comprehensive assessment of the generated images. In conclusion the paper presents a novel DEQ formulation for diffusion models showing promising results in enhancing image generation tasks. Addressing the weaknesses and questions raised could further strengthen the papers contribution and impact in the field of generative models.", "z64kN1h1-rR": " Summary The paper introduces a new offline reinforcement learning (RL) algorithm Model Standarddeviation Gradients (MSG) which leverages ensembles of Qfunctions to estimate lower confidence bounds (LCB) for policy optimization. The algorithm addresses a critical flaw identified in existing methods that use shared pessimistic target values leading to effectively optimistic value estimates. Through theoretical analyses and experiments on challenging RL benchmarks including D4RL and RL Unplugged MSG with deep ensembles outperforms stateoftheart methods significantly. The paper also examines ensemble size explores the role of separate networks per ensemble member and compares the performance of efficient ensemble approximations from supervised learning literature.  Strengths 1. Innovative Algorithm: MSG proposes a novel approach by training Qfunction ensembles with independently computed targets overcoming the optimistic estimates issue found in shared target methods. 2. Theoretical Rigor: The paper provides comprehensive theoretical analyses including mathematical characterizations and empirical validations to support the proposed algorithms effectiveness. 3. Experimental Results: The experiments conducted on challenging offline RL benchmarks demonstrate that MSG outperforms existing methods by a wide margin particularly in domains like antmazes. 4. Ensemble Ablations: The study on the significance of ensembling approaches particularly the comparison between independent and shared targets sheds light on the critical aspects of ensemble training in offline RL. 5. Efficient Ensemble Evaluation: By examining the transfer of efficient ensemble approaches from supervised learning to RL the paper encourages future research directions and highlights the need for better uncertainty estimation techniques.  Weaknesses 1. Lack of RealWorld Applications: The paper focuses primarily on theoretical analysis and benchmark experiments lacking demonstrations in realworld RL tasks. 2. Limited Attention to Hyperparameter Sensitivity: While some hyperparameters are discussed a more indepth analysis of sensitivity to hyperparameters could strengthen the evaluation. 3. Computational Cost Not Extensively Addressed: Although the paper mentions the computational costs of deep ensembles a more detailed discussion of the computational requirements and scaling issues could provide additional insights.  Questions 1. Generalization to Continuous Domains: How does MSG perform in continuous control tasks especially those with complex dynamics or continuous action spaces 2. Scalability Concerns: Given the computational overhead with increasing ensemble sizes how does MSG handle scalability concerns when applied to larger neural network architectures  Limitations 1. Generalization to RealWorld Scenarios: The experiments are primarily conducted on benchmark tasks further evaluation on realworld RL scenarios could validate the algorithms practical applicability. 2. Sensitivity to Hyperparameters: The impact of different hyperparameter choices on MSGs performance and robustness warrants further investigation for better understanding and generalization. 3. Limited Comparison: While the comparison with existing methods is robust a more extensive comparison with a wider range of stateoftheart algorithms could enhance the papers evaluation. Overall the paper presents a novel offline RL algorithm with strong theoretical foundations and promising experimental results while also calling attention to the need for more efficient uncertainty estimation techniques in reinforcement learning.", "r__gfIasEdN": " Peer Review  1) Summary: The paper presents a novel approach named Generalized Autoregressive ParaphraseIdentification (GAP) to address biases introduced by negative examples in paraphrase identification models. The authors propose training separate models for positive and negative pairs ensembling them based on perplexity for outofdistribution prediction. Strong empirical results demonstrate significant improvements over existing methods in outofdistribution scenarios while maintaining performance in indistribution predictions.  2) Strengths and Weaknesses:  Strengths:  Novel Approach: Introducing separate positive and negative models as well as the perplexitybased outofdistribution detection is innovative and addresses biases effectively.  Robust Empirical Results: Strong empirical results showing a substantial improvement over stateoftheart models in outofdistribution scenarios validate the efficacy of the proposed approach.  Auto Ensembling: The automatic determination of how much to rely on negative models during inference based on perplexity is a notable strength.  Weaknesses:  Neglect of Interaction: The paper acknowledges that GAP neglects the interaction between positive and negative pairs which could be important for indistribution predictions.  Limited Performance Analysis: The paper lacks a more detailed analysis of model performance under varying conditions such as sensitivity analysis or overfitting issues.  Validation of Perplexity Metric: While the paper proposes perplexity as an outofdistribution metric a more comprehensive validation against existing metrics could further strengthen the approach.  3) Questions:  Interplay of Positive and Negative Pairs: Have you explored methods to incorporate the interplay between positive and negative pairs during training rather than only in the ensembling stage  Generalizability: How does the performance of GAPX compare to OODP in cases where the distribution shift is less severe  Scalability: How does the proposed method scale with larger datasets and more complex models in terms of training time and computational resources  4) Limitations:  Limited Dataset Exploration: While the paper utilizes multiple datasets for evaluation a deeper analysis of dataset characteristics and their impact on model performance could provide more insights.  Realworld Applicability: How does the proposed approach fare in realworld scenarios with noisy or unlabeled data where distribution shifts may be more subtle Overall the paper presents a compelling approach to address biases in paraphrase identification models and achieves impressive results in outofdistribution scenarios. Addressing the weaknesses highlighted and further exploring the questions raised could enhance the impact and understanding of the proposed method.", "vkhYWVtfcSQ": " Summary: The paper introduces EMIX an energybased intrinsic motivation framework for surprise minimization in MultiAgent Reinforcement Learning (MARL) scenarios. The approach utilizes a temporal EnergyBased Model (EBM) to estimate and minimize surprise jointly across all agents. The authors validate their theoretical claims through empirical studies on multiagent tasks involving collaboration under fastpaced dynamics demonstrating improved performance over existing MARL methods.  Strengths: 1. Innovative Approach: The integration of energybased surprise minimization in the multiagent learning framework is novel and addresses a critical issue in MARL settings. 2. Theoretical Foundation: The paper provides a solid theoretical foundation for surprise minimization using freeenergy concepts and demonstrates the convergence properties of their proposed EMIX framework. 3. Empirical Validation: The empirical studies conducted on challenging multiagent tasks showcase the effectiveness of EMIX in improving task performance and handling sudden environmental changes. 4. Ablation Studies: The ablation studies provide insights into the effectiveness of key components of the proposed method emphasizing the importance of the energybased scheme.  Weaknesses: 1. Clarity: The paper is dense with technical terminology and complex mathematical formulations potentially limiting accessibility to a broader audience. 2. Empirical Evaluation: While the experimental results are promising a more comprehensive comparison with a wider range of existing MARL methods could further strengthen the validity of the proposed approach.  Questions: 1. Scalability Concerns: How does the proposed EMIX framework handle scalability issues when the number of agents and complexity of the environment increase 2. Sampling Techniques: Could the authors elaborate on the methods used for sampling from the surprise distribution in the context of EMIX 3. Realworld Applications: How applicable and transferable is the EMIX framework to realworld scenarios especially those with highly dynamic multiagent interactions such as autonomous systems or robotics  Limitations: 1. Complexity: The complexity of the proposed EMIX framework may pose challenges in its practical implementation and deployment in realworld settings. 2. Generalizability: The evaluation primarily focuses on specific MARL tasks raising questions about the generalizability of the approach to a broader range of scenarios. Overall the paper presents a promising direction in leveraging energybased surprise minimization for enhancing collaborative multiagent learning. Addressing scalability concerns and conducting further evaluations on diverse scenarios would strengthen the impact and practical applicability of the proposed approach.", "xILbvAsHEV": " Peer Review:  Summary: The paper introduces an efficient approach for learning ExtensiveForm Games (EFGs) by converting them to NormalForm Games (NFGs) using the \\xce\\xa6Hedge algorithm. This enables the direct translation of techniques from NFGs to learning EFGs. The paper addresses the computational intractability caused by the exponential blowup of the game size due to conversion by implementing \\xce\\xa6Hedge to learn Nash Equilibria NormalForm Coarse Correlated Equilibria (NFCCE) and ExtensiveForm Correlated Equilibria (EFCE) in EFGs. It presents an improved Balanced EFCEOMD algorithm achieving a sharp \\xce\\x98(\\xe2\\x88\\x9aXAT) EFCEregret under banditfeedback.  Strengths: 1. Novel Approach: The paper presents an innovative and thorough approach for learning EFGs by converting them to NFGs using the \\xce\\xa6Hedge algorithm. 2. Efficiency: The developed algorithms demonstrate efficiency in learning Nash Equilibria and EFCE by direct translation from NFGs. 3. Regret Bounds: The analysis of regret bounds under both full and bandit feedback settings provides valuable insights into the performance of the algorithms. 4. Balanced EFCEOMD: The introduction of the Balanced EFCEOMD algorithm and its nearoptimal trigger regret bound is a significant contribution.  Weaknesses: 1. Complexity: The paper involves complex mathematical concepts and algorithms making it challenging to understand for readers without a solid background in game theory and optimization. 2. Lack of Simplicity: The dense technical descriptions and mathematical formulations may hinder the accessibility and clarity of the paper to a broader audience. 3. Assumptions: Some of the assumptions made in the paper may limit the practical applicability of the proposed algorithms in realworld scenarios. A discussion on these assumptions and their implications would be beneficial.  Questions: 1. Applicability: How applicable are the proposed algorithms and approaches to realworld scenarios involving complex multiplayer games with imperfect information 2. Computational Efficiency: Can the \\xce\\xa6Hedge algorithm be further optimized for increased computational efficiency or scalability in handling larger and more complex game structures 3. Comparison: How do the presented algorithms compare to existing stateoftheart methods for learning equilibria in EFGs Are there specific advantages or limitations in comparison to other approaches  Limitations: 1. Technical Complexity: The intricate mathematical derivations and algorithm implementations may limit the practical adoption of the proposed methods by researchers lacking expertise in the field. 2. Realworld Validation: The lack of empirical validation or case studies demonstrating the effectiveness of the algorithms in practical scenarios raises concerns regarding the generalizability of the findings. 3. Assumption Sensitivity: The theoretical developments heavily rely on certain assumptions about the game structures and players behavior potentially limiting the general applicability and robustness of the proposed algorithms. The paper provides a significant contribution to the field of game theory and optimization with its innovative approach and efficient algorithms for learning equilibria in EFGs. However addressing the weaknesses and limitations highlighted above would enhance the clarity applicability and robustness of the research findings.", "vWUmBjin_-o": "Peer Review: 1) Summary: The paper introduces a novel approach for learning equivariant representations using invariants of a given linear representation of a transformation group. The proposed approach named Symmetry Regularization (SymReg) enforces invariance of geometrical quantities such as distances inner products and angles in the latent space to achieve equivariance to different transformation groups. Through experiments on various tasks including world modeling and reinforcement learning the paper demonstrates the effectiveness of SymReg in learning disentangled representations and improving sample efficiency. The work addresses the limitations of existing equivariant networks by providing a simple and generalizable solution for learning equivariant representations without knowing the specific group actions in the input space. 2) Strengths and weaknesses: Strengths:  The paper introduces a novel approach SymReg for learning equivariant representations without the need for knowing the group actions in the input space.  The use of invariants for symmetry regularization of the latent space is a unique and effective method for achieving equivariance.  The experiments on various tasks including world modeling and reinforcement learning provide qualitative and quantitative evidence supporting the efficacy of SymReg for learning disentangled representations and improving sample efficiency.  The detailed theoretical background formulation of the objective functions and application to different transformation groups are wellexplained in the paper. Weaknesses:  The paper focuses heavily on the technical aspects of the proposed SymReg approach potentially making it challenging for readers less familiar with the concepts of group theory and representation learning.  It would be beneficial to provide more comparative analyses with existing methods and additional insights into the limitations of SymReg compared to other approaches especially in different types of environments or datasets.  The paper could benefit from clearer explanations of the practical implications and potential realworld applications of the proposed approach beyond the presented experiments. 3) Questions:  How does SymReg compare to existing methods in terms of computational efficiency and scalability especially when dealing with largescale datasets or complex transformation groups  Is there any limitation to the generalizability of SymReg across different types of datasets or tasks and are there scenarios where SymReg may not be the optimal choice for learning equivariant representations  Could the paper provide more insights into the interpretability of the learned representations and how they can be applied in realworld AI applications beyond the experimental setups presented 4) Limitations:  The paper focuses primarily on the theoretical framework and experimental results of SymReg without discussing potential challenges or limitations that may arise in practical implementations or realworld scenarios.  The generalizability of SymReg to diverse domains or environments is not extensively explored leaving room for understanding how well the approach performs in different contexts.  The evaluation metrics used in the experiments while informative could be expanded to include more diverse benchmarks or tasks to provide a comprehensive assessment of SymRegs performance in various scenarios.", "oiztwzmM9l": " Summary The paper introduces two novel algorithms STOBNTS and STOBNTSLinear that address the limitations of existing neural bandit algorithms by sidestepping the need to invert large parameter matrices and supporting batch evaluations while preserving theoretical guarantees. The algorithms leverage the strong representation power of neural networks (NNs) to improve sample efficiency and performance in blackbox optimization tasks demonstrating competitive empirical effectiveness in automated machine learning (AutoML) reinforcement learning (RL) and optimization over image inputs.  Strengths and Weaknesses Strengths: 1. The paper addresses important limitations of existing neural bandit algorithms and proposes novel solutions. 2. The algorithms leverage the expressive power of neural networks allowing for more effective modeling of complex realworld functions. 3. The theoretical guarantees provided for the algorithms offer valuable insights into their performance. 4. Empirical results across various experiments demonstrate the superior performance of the proposed algorithms compared to baseline methods. 5. The computational advantages of the algorithms are highlighted showing the potential for practical adoption. Weaknesses: 1. The paper may benefit from further discussion on the scalability of the proposed algorithms to larger datasets. 2. The limitations of the algorithms in specific scenarios or tasks could be more thoroughly explored and discussed. 3. The computational complexity of the algorithms and potential finetuning requirements are not extensively discussed.  Questions 1. How do the algorithms compare in terms of computational efficiency compared to existing methods 2. Can the algorithms handle noisy or uncertain environments effectively 3. Are there specific types of tasks or data distributions where the proposed algorithms may underperform 4. Could the algorithms be extended to address other types of optimization problems beyond the ones considered in the paper 5. How sensitive are the algorithms to hyperparameters and initialization choices  Limitations 1. The paper primarily addresses the theoretical and empirical performance of the proposed algorithms but lacks indepth discussions on realworld application challenges. 2. The limitations of the algorithms in highly complex or unpredictable environments are not extensively explored. 3. The experiments focus primarily on specific optimization tasks leaving room for further validation in diverse problem domains. Overall the paper presents innovative solutions to enhance the performance of neural bandit algorithms in blackbox optimization tasks. The theoretical guarantees and empirical results provide compelling evidence of the effectiveness of the proposed algorithms highlighting their potential for practical adoption in various optimization scenarios. Further exploration of scalability robustness and generalizability of the algorithms would be valuable for future research directions.", "xp5VOBxTxZ": " Summary The paper explores the impact of normalization layers and weight decay on neural network training dynamics. It introduces the concept of sharpness reduction bias and exhibits how weight decay interacts with normalization layers to reduce the sharpness of the loss landscape during training. The study provides theoretical analysis and supporting experiments demonstrating the phenomenon of reduced sharpness and its benefits to generalization. The paper specifically focuses on the continuous sharpnessreduction flow induced by the interplay between normalization layers weight decay and gradient descent.  Strengths and Weaknesses Strengths: 1. The paper provides a rigorous theoretical analysis supported by experiments to explain the concept of sharpness reduction bias during training neural networks with normalization layers and weight decay. 2. The study sheds light on how the combined effect of normalization layers and weight decay influences the training dynamics and leads to improved generalization. 3. The application of the theory to linear regression with batch normalization provides a concrete example to understand the sharpening dynamics induced by weight decay and normalization layers. 4. The experimental validation of the sharpnessreduction phenomenon on CIFAR10 datasets and matrix completion tasks reinforces the theoretical findings. Weaknesses: 1. The paper is highly technical and might be challenging for readers not wellversed in mathematical analysis and neural network optimization theory. 2. The reliance on specific assumptions and ideal settings such as scale invariance and specific architectures may limit the generalizability of the findings to all neural network configurations. 3. The paper focuses primarily on the analysis of fullbatch gradient descent and the extension of results to stochastic gradient descent could provide broader insights into training dynamics. 4. It would be beneficial to include discussions on the practical implications of the sharpnessreduction bias and how it can be leveraged to improve training efficiency in deep learning tasks.  Questions 1. How does the sharpnessreduction bias induced by the interplay between normalization layers and weight decay compare to other regularization techniques in terms of generalization performance 2. How does the sharpnessreduction flow established in the study impact convergence speed and convergence to global minima in deep neural network training 3. Could the results and observations from linear regression with batch normalization be extended to more complex architectures and tasks to further understand the benefits of sharpness reduction bias in neural network training  Limitations 1. The paper focuses on theoretical analysis and experiments on specific configurations which may limit the generalizability of the findings to a broader range of neural network architectures and tasks. 2. The study assumes certain ideal conditions such as scale invariance and specific network setups which may not always hold true in practical deep learning scenarios. 3. The complexity and technical nature of the analysis may hinder the accessibility of the findings to a wider audience potentially limiting the practical applications of the sharpnessreduction bias concept.", "z9cpLkoSNNh": " Peer Review: 1) Summary: The paper delves into the domain of continual learning offering a framework via feature extraction and proposing an algorithm DPGrad for efficient performance in linear feature scenarios while highlighting the impracticability of continual learning in the presence of nonlinear features. The authors establish theoretical foundations and present practical experiments validating the effectiveness of DPGrad against comparable methods in synthetic datasets and realworld benchmarks. 2) Strengths:  Theoretical Groundwork: The paper constitutes a solid theoretical foundation by formulating the continual learning problem through a feature extraction approach and proving the efficacy of the proposed DPGrad algorithm for linear features.  Novelty: The research fills a crucial gap in formalizing continual learning guarantees particularly in situations with changing environment distributions making it a valuable contribution to the field.  Simulation and Experiment Validation: The conducted simulations on synthetic datasets and realworld benchmarks demonstrate the practicality and superior performance of DPGrad compared to conventional methods corroborating the theoretical claims. Weaknesses:  Complexity: The intricate theoretical and algorithmic formalisms might limit the accessibility of the research to a broader audience potentially necessitating more elucidation or simplification in explanations.  Empirical Experiments: While the experiments showcase the effectiveness of DPGrad a more comprehensive evaluation on various datasets and scenarios could further validate the algorithms versatility and robustness. 3) Questions:  How does DPGrad handle scenarios with highdimensional data or complex feature spaces beyond the simulated and benchmark scenarios discussed  Can the algorithm be extended to incorporate adaptive learning rates or regularization techniques to enhance adaptability to diverse environments and prevent overfitting in practical use cases  Given the focus on linear features what challenges might arise when transitioning to nonlinear feature scenarios and what adaptations or approaches could be explored to address these challenges 4) Limitations:  Generalization to NonLinear Features: The impossibility of continual learning in the presence of nonlinear features is a significant limitation calling for alternative strategies to address continual learning in more complex feature spaces.  Simplistic Benchmarks: While the experiments with synthetic datasets and benchmarks are insightful additional experimentation with a wider range of datasets and varying complexities could provide more comprehensive insights into the algorithms performance across diverse scenarios. In conclusion the paper offers a substantial contribution to the continual learning landscape through its feature extraction framework and DPGrad algorithm for linear features though further exploration on tackling nonlinear feature challenges and broader empirical evaluations would enhance the researchs impact and applicability.", "kjR8GiwqCK": " Peer Review: 1) Summary: The paper introduces IMEDRL a reinforcement learning algorithm optimized for discrete undiscounted infinitehorizon Markov Decision Problems under the average reward criterion. The algorithm aims to minimize regret in scenarios where rewards and transitions are unknown. The study extends popular bandit strategies to the MDP setup focusing on proving asymptotic optimality and achieving competitive performance against stateoftheart algorithms particularly in tabular MDPs. The research provides theoretical guarantees of optimality for ergodic MDPs and illustrates competitive numerical results. 2) Strengths and weaknesses: Strengths:  The paper offers a novel algorithm IMEDRL addressing reinforcement learning in MDPs with unknown rewards and transitions.  The study provides theoretical guarantees of asymptotic optimality and competitive performances against stateoftheart algorithms particularly in tabular MDPs.  Detailed mathematical formulation of the algorithm and proofs offer a comprehensive understanding of the proposed methodology.  The numerical experiments provide empirical validation of the algorithms efficacy and performance in practical scenarios. Weaknesses:  The complexity analysis indicates a high computational cost which might limit scalability on larger MDPs.  While the focus is on ergodic MDPs the algorithms performance in nonergodic MDPs or more complex environments is not discussed.  The paper could benefit from additional analysis or comparison against more benchmark RL algorithms to further assess its effectiveness.  The practical implementation details including hyperparameter tuning and convergence rates could be elaborated upon for better understanding. 3) Questions:  How does the proposed IMEDRL algorithm compare in terms of convergence rates and sample efficiency with existing RL algorithms on standard benchmark problems  How sensitive is IMEDRL to the selection of hyperparameters and how do these choices impact its performance  Are there any known limitations or failure modes of the IMEDRL algorithm that were not discussed in the paper  Could the proposed algorithm be extended to handle nonergodic MDPs or more complex environments with different characteristic features 4) Limitations:  The analysis focuses predominantly on ergodic MDPs limiting the generalizability of the results to other types of MDPs.  The paper lacks a comparative analysis with a broader range of stateoftheart RL algorithms limiting the assessment of IMEDRLs performance in various scenarios.  The computational complexity of the algorithm may pose challenges in scalability to larger MDPs or more complex environments. In conclusion the paper presents a novel contribution to the field of reinforcement learning with promising results. The insights provided through theoretical analysis and empirical experiments lay a solid foundation for future extensions and improvements in the domain of MDPbased reinforcement learning. Additional benchmarks and analysis across a wider range of scenarios would enhance the robustness and applicability of the proposed IMEDRL algorithm.", "oWx_9VJgyV7": "Peer Review Summary The paper proposes a novel unsupervised paradigm named SNAKE for 3D keypoint detection from point clouds entangling shape reconstruction with keypoint detection. The method achieves superior performance on various benchmarks and demonstrates advantages in repeatability semantic consistency and geometric registration. The study includes extensive experiments and an ablation study to validate the effectiveness of the proposed approach. Strengths 1. The paper addresses an important and underexplored question of joint shape reconstruction and 3D keypoint detection. 2. The proposed method SNAKE achieves stateoftheart performance in key aspects such as repeatability semantic consistency and geometric registration. 3. Extensive experiments on various datasets and benchmarks demonstrate the effectiveness and superiority of the proposed approach. 4. The paper provides a detailed explanation of the network architecture and loss functions enhancing clarity for readers. 5. An insightful ablation study is conducted to analyze the impact of various design choices on the models performance. Weaknesses 1. The paper lacks a clear discussion on the limitations of the proposed method in realworld applications and potential failure modes. 2. Although the ablation study is beneficial it could be further strengthened by providing more indepth analysis and discussions of the results. 3. It would be valuable to discuss the computational complexity and efficiency of the proposed method especially during the keypoint extraction optimization step. Questions 1. Can the proposed method handle highly complex and deformable objects such as those with intricate geometries or organic shapes 2. How does the SNAKE model perform with varying levels of noise or occlusion in the input point cloud data 3. Are there specific scenarios or datasets where the proposed method may underperform compared to traditional methods or other deep learningbased approaches Limitations 1. The optimization step for explicit keypoint extraction during inference is computationally intensive limiting realtime applicability. 2. The potential negative social impact of using the method for pose estimation in autonomous robots is briefly mentioned but a more detailed discussion on ethical considerations and safety implications would be beneficial. Overall the paper presents a novel and promising method for joint shape reconstruction and 3D keypoint detection demonstrating significant improvements in key aspects. Addressing the weaknesses and questions raised could further enhance the value and applicability of the proposed approach.", "scfOjwTtZ8S": " Peer Review:  Summary: The paper presents a method called EAGER for automated reward shaping in reinforcement learning tasks involving long horizons and sparse rewards using languageconditioned framework. EAGER leverages question generation and answering systems to create auxiliary objectives for the agent based on language goals. The method does not require expert intervention unlike existing approaches and shows improved sample efficiency. Experimental results on the BabyAI platform demonstrate the effectiveness of EAGER compared to stateoftheart methods.  Strengths: 1. Innovative Approach: The use of question generation and answering for automated reward shaping is novel and wellmotivated. 2. Sample Efficiency: EAGER shows improved sample efficiency without the need for expert knowledge making it a promising direction for reinforcement learning tasks. 3. Experimental Rigor: The experiments conducted on the BabyAI platform are thorough comparing EAGER against existing methods and providing insights into its advantages.  Weaknesses: 1. Limited Evaluation: While the experiments are extensive additional evaluation on diverse environments or benchmarks could further validate the effectiveness of EAGER. 2. Generalization: The paper could elaborate more on the generalization capabilities of the proposed method to different task types or environments. 3. Complexity: The method relies on pretraining the questionanswering system which may introduce a level of complexity and dependency on external data sets.  Questions: 1. Scalability: How does EAGER scale with more complex tasks or environments beyond the BabyAI platform 2. Robustness: How robust is EAGER to variations in the language instructions or changes in the environment structure 3. RealWorld Applications: Are there plans to test EAGER in realworld scenarios or more complex environments involving human instructions  Limitations: 1. QA PreTraining: The reliance on pretraining the QA system with example trajectories could limit applicability in scenarios where such data is not available. 2. Generalization: The generalization of EAGER to different tasks and environments might need further validation to ensure robust performance. 3. Complexity: Implementing EAGER may introduce additional complexity especially in environments without predefined example trajectories. Overall the paper presents a novel and impactful approach to automated reward shaping in reinforcement learning. The methods potential for improved sample efficiency and reduced expert intervention make it a valuable contribution to the field. Further work in evaluating generalization and scalability aspects could enhance the practical applicability of EAGER.", "lKFOwaYNQlb": "Peer Review Summary: The paper introduces Distributed InfluenceAugmented Local Simulators (DIALS) a method for training large networked multiagent systems efficiently using distributed simulators with periodic training of approximate influence predictors (AIPs). By factorizing the system into multiple subregions DIALS enables parallel training and mitigates nonstationarity issues. Empirical results demonstrate accelerated training times and scalability to systems with many agents. Strengths:  The paper proposes a novel method DIALS to efficiently train large networked multiagent systems.  The theoretical framework of InfluenceBased Abstraction (IBA) and the extension to MARL are well articulated.  Empirical results demonstrate the effectiveness of DIALS in reducing training times and scaling to systems with many agents.  The clear explanation of the algorithm experimental setup and results enhances the readability and comprehensibility of the paper.  The paper addresses realworld applications and discusses the potential impact of DIALS across various domains. Weaknesses:  The paper lacks a detailed discussion on the limitations and potential challenges of implementing DIALS in realworld scenarios.  The theoretical framework could be further elaborated including a deeper analysis of the assumptions and conditions under which DIALS is most effective.  The paper could benefit from more detailed comparison with existing methods and a broader discussion on the implications of the results. Questions: 1. How does the choice of hyperparameters such as the AIPs training frequency impact the overall performance and convergence of DIALS in different environments 2. Are there specific requirements or challenges in implementing DIALS in domains with highly interconnected and interdependent agents where influence sources may be more complex 3. Can the authors provide insights into the computational costs associated with training the AIPs and the tradeoffs between computational efficiency and memory usage Limitations:  The paper focuses on specific scenarios like traffic control and warehouse commissioning which may limit the generalizability of the results to other types of multiagent systems.  The experiments are conducted with a limited number of seeds and the scalability of DIALS may need further validation with larger sample sizes and more diverse scenarios.  The theoretical foundation of DIALS could be strengthened with additional formal analysis and comparisons with related methods in the literature. Overall the paper presents a promising approach for training multiagent systems efficiently but further research is needed to explore the broader applicability and robustness of DIALS across various settings and scenarios.", "mTra5BIUyRV": "Peer Review 1) Summary: The paper introduces a fairranking framework that addresses errors in sociallysalient attributes affecting the fairness and utility of existing fairranking algorithms. It considers a model of random and independent errors in these attributes and presents a new framework that can output rankings with high fairness and utility. The framework works for a general class of fairness criteria and provides provable guarantees on fairness and utility. Empirical evaluations on synthetic and realworld datasets show promising results compared to existing baselines. 2) Strengths:  The paper addresses an important issue of errors in sociallysalient attributes impacting fairness in ranking algorithms.  The proposed fairranking framework incorporates probabilistic information about perturbations in attributes and provides provable guarantees on fairness and utility.  The empirical evaluations on synthetic and realworld data demonstrate the superiority of the proposed framework over existing baselines showing higher fairness and similar or better fairnessutility tradeoffs.  The theoretical results on the fairness and utility guarantees of the proposed framework are comprehensive and wellexplained. Weaknesses:  The paper assumes random and independent errors in sociallysalient attributes which may not always hold in realworld scenarios limiting the generalizability of the framework.  The empirically observed results need to be further validated on larger and more diverse datasets to ensure the robustness of the framework across different settings.  The paper focuses on the technical and algorithmic aspects of the fairranking framework providing limited discussion on the broader implications of the research and potential ethical considerations. 3) Questions:  Can the proposed framework be adapted or extended to handle correlated errors in socially salient attributes  Have realworld use cases been explored to validate the effectiveness of the framework in practical applications  How does the performance of the framework scale with larger datasets and more complex fairness constraints  Are there specific industries or domains that could benefit most from the proposed fairranking framework 4) Limitations:  The assumptions made in the model of errors in sociallysalient attributes may not fully capture the complexities of realworld data.  The generalizability of the proposed framework to diverse datasets and applications needs further exploration.  The paper lacks a detailed discussion on potential biases or unintended consequences that may arise from implementing the fairranking framework in realworld scenarios. In conclusion the paper presents a valuable contribution to the area of fairranking algorithms under the presence of errors in sociallysalient attributes. Further research and validation on practical applications and diverse datasets would strengthen the impact of the proposed framework in mitigating biases in ranking algorithms.", "snUOkDdJypm": " Peer Review  1) Summary: The paper investigates the lastiterate convergence rate of the extragradient and optimistic gradient algorithms in multiplayer games providing tight results using the tangent residual as a new potential function. The study addresses an open question in the constrained setting offering significant contributions to the field of learning in games.  2) Strengths:  The paper introduces new concepts such as the tangent residual providing a novel approach to measuring proximity to a Nash equilibrium.  The formal proof techniques and mathematical rigor of the study are commendable leading to tight convergence rate results for the algorithms in constrained settings.  The approach of utilizing SOS programming to search for potential functions is novel and can potentially have broader applicability in related studies.  The analysis is thorough covering both unconstrained and constrained settings for the two algorithms.  3) Weaknesses:  The paper assumes a high level of mathematical sophistication potentially limiting the accessibility to a wider audience of researchers.  Although the study is focused on lastiterate convergence it might benefit from including practical implications or realworld applications of the findings.  The complexity of the SOS programming approach and the mathematical formalism may pose challenges for replication and implementation by other researchers.  4) Questions:  Could the paper include more discussion on the practical implications of the lastiterate convergence rates in multiplayer games How could these theoretical results impact realworld applications or algorithm design  The paper mentions computeraided proofs based on SOS programming. Can the authors elaborate on the practical aspects of implementing and verifying these proofs in a research environment  5) Limitations:  The study is highly theoretical focusing primarily on mathematical formalism and proofs. Future research could potentially explore the practical consequences of these convergence rates in realworld game settings.  The use of complex mathematical techniques like SOS programming may limit the accessibility and reproducibility of the findings for researchers with less expertise in this area. Overall the paper presents valuable insights into lastiterate convergence rates in multiplayer games offering novel contributions to the field. However the high level of mathematical detail may present barriers for readers unfamiliar with advanced mathematical concepts. Addressing potential practical implications and providing further context for the results could enhance the impact and relevance of the study.", "zqQKGaNI4lp": "Peer Review 1) Summary: The paper proposes an ensemble classification method using the manifoldHilbert kernel for data distributed on a Riemannian manifold. Through theoretical analysis and proofs the study establishes consistency and interpolation properties for the proposed method. The main findings include the development of weighted random partition kernels and demonstrating that the manifoldHilbert kernel can be regarded as a weighted random partition kernel on the ddimensional sphere Sd. The paper aims to contribute to the theory of interpolating ensembles by showcasing consistent classifiers for a broad class of distributions in various dimensions thus filling a gap in understanding ensemble methods in the interpolating regime. 2) Strengths and Weaknesses:  Strengths:  The paper is wellstructured with clear objectives and a detailed methodology.  The research fills a gap in the theory of ensemble methods by providing generalization guarantees in the interpolating regime.  The study introduces novel concepts such as the manifoldHilbert kernel and weighted random partition kernels which offer theoretical insights into interpolating classifiers.  The theoretical analysis and proofs are rigorous and wellsupported.  Weaknesses:  Lack of empirical validation or practical implementation of the proposed method.  Limited discussion on the applicability of the findings in practical machine learning scenarios.  The paper may benefit from elaborating on the computational complexity and scalability of the proposed ensemble method. 3) Questions:  How does the proposed ensemble method compare to existing ensemble methods in terms of prediction performance on realworld datasets  Can the weighted random partition kernels be extended to other types of data distributions beyond the sphere 4) Limitations:  The lack of empirical evaluation limits the practical applicability of the proposed ensemble method.  The focus on theoretical analysis without practical validation may hinder the adoption of the method in realworld machine learning applications. 5) Suggestions for Improvement:  Provide an empirical evaluation of the ensemble method on benchmark datasets to demonstrate its effectiveness.  Include comparisons with existing ensemble methods to showcase the advantages and limitations of the proposed approach.  Discuss the computational efficiency and scalability of the proposed method for large datasets. Overall the paper offers valuable insights into ensemble methods in the interpolating regime and introduces novel concepts that can contribute to the theoretical understanding of such methods. However further empirical validation and discussions on practical implications are needed to enhance the impact of the research.", "ofwkaIWFqqv": " Peer Review:  1) Summary: The paper introduces GRASP a novel goaldriven retrosynthetic planning approach aiming to improve the efficiency and quality of synthetic pathway planning in the field of synthetic chemistry. GRASP leverages reinforcement learning to navigate through vast singlestep reaction spaces enhancing explorationexploitation tradeoffs and enabling goaldriven retrosynthesis navigation towards userdefined objectives. Experimental results on academic and industrial benchmark datasets demonstrate the superiority of GRASP over existing stateoftheart approaches in both general retrosynthetic planning metrics and goaldriven planning. 2) Strengths and weaknesses: Strengths:  GRASP introduces a innovative goaldriven framework for retrosynthetic planning addressing limitations of existing methods by improving search efficiency and quality.  The paper is wellstructured providing detailed explanations of the proposed framework experimental setup and results.  The thorough evaluation on benchmark datasets including comparisons with various baselines and ablation studies lends credibility to the effectiveness of the GRASP approach. Weaknesses:  The paper would benefit from more indepth discussions on specific experimental results such as detailed analysis of success rates pathway lengths and costs to provide deeper insights into the performance of the GRASP framework.  While the paper highlights the advantages of GRASP in goaldriven retrosynthetic planning further insights into the scalability and generalizability of the approach to diverse chemical spaces could enhance the comprehensiveness of the study. 3) Questions:  Can you provide more details on the specific metrics used to evaluate the quality and efficiency of the retrosynthetic pathways generated by GRASP and the comparative baselines  How does the GRASP framework handle cases where the synthetic pathways involve complex or unconventional reactions that may not be wellrepresented in the training datasets  Could you elaborate on the user studies conducted to validate the goaldriven planning capabilities of GRASP and provide insights into the feedback and reactions from the participating chemists 4) Limitations:  The reliance on simulated datasets for training value networks and planning algorithms may introduce biases or limited generalization to realworld scenarios. It would be valuable to explore the robustness of GRASP in handling novel or outofdistribution molecules.  The success of GRASP in goaldriven planning is crucially dependent on the accuracy and diversity of the available building block materials data. Future work could focus on enhancing the adaptability of the framework to different chemists requirements and preferences.  Overall the paper presents a compelling contribution to the field of synthetic chemistry by introducing GRASP a novel approach that excels in both general and goaldriven retrosynthetic planning. The thorough experimental evaluations and user studies validate the effectiveness of the proposed framework highlighting its potential to streamline and enhance the synthetic pathway design process. Further investigations into the scalability robustness and realworld applicability of GRASP could further consolidate its position as a valuable tool for chemists in synthetic chemistry research and drug discovery.", "v_0F4IZJZw": " Peer Review of the Scientific Paper  1) Summary The paper systematically explores domainadaptive training to reduce the toxicity of language models focusing on three dimensions: training corpus model size and parameter efficiency. The study introduces SelfGeneration Enabled domainAdaptive Training (SGEAT) and conducts a comprehensive analysis of detoxifying LMs with parameter sizes ranging from 126M to 530B. The findings highlight the tradeoff between detoxification effectiveness and language model quality showcasing the benefits of using selfgenerated datasets for detoxification and leveraging adapter layers for more parameterefficient training.  2) Strengths and Weaknesses  Strengths:  Comprehensive exploration of domainadaptive training for detoxifying language models across different dimensions.  Introduces a novel approach SGEAT which demonstrates significant detoxification efficiency improvements over existing methods.  Systematic study on detoxifying LMs with a wide range of parameter sizes including largescale models like 530B.  Findings regarding the impact of model size on detoxification effectiveness provide valuable insights for researchers in the field.  Weaknesses:  While the paper discusses the tradeoff between detoxification and LM quality a more detailed analysis of potential biases introduced during detoxification could provide further depth to the study.  Issues related to the utilization of existing hate speech classifiers may raise concerns regarding the bias within the detoxification process.  3) Questions  Can the study elaborate more on the potential biases that might evolve within the domainadaptive training process and suggest methods to mitigate such biases  Are there considerations or approaches discussed in the paper to address the bias present in the hate speech classifiers used in the detoxification process  How do the findings of the study correlate with realworld applications of largescale language models and their ethical deployment given the toxicity issues  4) Limitations  The study acknowledges limitations related to potential biases within the hate speech detectors and existing biases within pretrained models. Efforts to address these biases through improvements in the detoxification process would enhance the researchs applicability.  Further validation of the detoxification effectiveness of the proposed SGEAT method in realworld scenarios or diverse datasets may provide additional insights and strengthen the research outcomes.  Overall Assessment: The paper presents a thorough investigation into domainadaptive training for detoxifying language models with a focus on novel methodologies like SGEAT and insights regarding model size impact and parameterefficient training. The findings contribute significantly to the field of NLP research although further exploration of biases and validation in practical applications would enhance the studys utility and relevance.", "y--ZUTfbNB": "Peer Review 1. Summary The paper explores algorithmic design tailored for distance matrices focusing on fast computation for fundamental linear algebraic primitives. It introduces efficient algorithms for computing matrixvector products for a variety of distance matrices such as the `1 metric along with upper and lower bounds. The work also covers fast algorithms for matrix multiplication and constructing distance matrices. Theoretical findings are supported by empirical evaluations on real and synthetic datasets. 2. Strengths and weaknesses  Strengths:  The paper addresses an important topic in algorithm design tailored specifically for distance matrices and related matrices.  The research provides a comprehensive theoretical analysis with rigorous proofs and bounds for various algorithmic tasks.  Empirical evaluations on real and synthetic datasets validate the theoretical findings and demonstrate the practical relevance of the proposed algorithms.  Clear and structured presentation of results with detailed explanations and examples provided throughout the paper.  Weaknesses:  The complexity of the algorithms may limit their practical applicability to very large datasets due to high computational demands as indicated by the infeasibility of handling large datasets in the empirical evaluation.  The paper lacks a discussion on potential challenges or limitations in implementing the algorithms in realworld scenarios including considerations for scalability and memory constraints.  The empirical evaluation could benefit from a broader range of datasets and different types of queries to enhance the generalizability of the results. 3. Questions  How do the proposed algorithms compare with existing stateoftheart methods for distance matrix computation in terms of efficiency and scalability  Can the algorithms be extended or modified to address the challenges associated with handling extremely large datasets that cannot fit in memory  Are there any specific characteristics of the distance matrices or datasets where the proposed algorithms might exhibit superior performance compared to traditional approaches 4. Limitations  The focus on theoretical analysis and idealized scenarios might limit the practical utility of the proposed algorithms in handling realworld datasets with diverse properties and requirements.  The empirical evaluation while insightful is restricted to a limited set of datasets and scenarios possibly overlooking potential challenges that might arise in other practical applications.  The paper could benefit from a more indepth discussion on the implications of the theoretical findings and algorithmic complexities for realworld implementations and scalability. Overall the paper presents significant contributions to the algorithmic design for distance matrices but further exploration of practical implications and scalability considerations could enhance the impact and applicability of the research findings.", "wjqr6aqkLUV": " Peer Review:  1) Summary: The paper explores the phenomenon of weight condensation in neural networks with small initialization specifically focusing on the impact of the multiplicity of the activation function on weight alignment. The authors conduct experiments on various network architectures and activation functions to demonstrate the relation between activation function multiplicity and weight condensation. The theoretical analysis provided in the paper elucidates how the multiplicity affects the initial condensation dynamics. Overall the paper provides valuable insights into how small initialization influences neural network regularization through weight condensation dynamics.  2) Strengths:  The paper addresses an important aspect of neural network behavior related to weight condensation with small initialization contributing to understanding network regularization.  The experimental results and theoretical analysis are thorough providing a comprehensive examination of the condensation phenomenon.  The paper is wellstructured with clear sections outlining the introduction preliminary concepts experiments and theoretical analysis.  The use of both synthetic and real dataset experiments strengthens the validity of the findings.  The discussion about the implications of weight condensation for network training and generalization is insightful and provides a good basis for further research in the field.  3) Weaknesses:  The paper could benefit from more explicit discussions of the practical implications of the findings for realworld applications and how these insights could be leveraged for improving neural network training strategies.  While the theoretical analysis is detailed some parts of it may be challenging for readers without a strong mathematical background to follow suggesting a need for more accessible explanations.  The paper could discuss potential limitations or challenges of the proposed method and highlight areas for future research or refinement.  4) Questions:  Can the authors provide more insight into how the findings regarding weight condensation can be practically applied in training neural networks for specific tasks or domains  How do the results of weight condensation dynamics align with existing regularization techniques commonly used in neural network training  Could the authors elaborate on the computational complexity implications of weight condensation and whether it could affect training efficiency or scalability in largescale networks  5) Limitations:  The paper mainly focuses on the initial condensation dynamics due to small initialization. Exploring the impact of weight condensation during different stages of training or in the context of transfer learning could enhance the completeness of the study.  The analysis predominantly centers on the multiplicity of activation functions. Consideration of other factors influencing weight condensation such as network depth or architecture could further enrich the discussion.  The generalizability of the findings may be limited by the specific experimental setups used. Addressing the applicability of the results across a wider range of network architectures and datasets would strengthen the research impact. In conclusion the paper presents a valuable investigation into weight condensation in neural networks with small initialization offering a blend of empirical evidence and theoretical analysis. Addressing the suggested points could enhance the clarity and applicability of the research findings for the scientific community.", "sOVNpUEgKMp": "Peer Review 1. Summary: The paper introduces an Adaptive MultiDistribution Knowledge Distillation (AMDKD) scheme to address the crossdistribution generalization issue in deep models for Vehicle Routing Problems (VRPs). The AMDKD leverages knowledge distillation from multiple teachers trained on exemplar distributions to train a more generalizable student model. Experimental results demonstrate the effectiveness of AMDKD in achieving competitive results on both indistribution and outofdistribution instances while consuming fewer computational resources. 2. Strengths:  Novel Contribution: The introduction of the AMDKD scheme is a novel approach to enhancing the generalization of deep models for VRPs.  Experimental Results: The extensive experimental results showcase the competitive performance of AMDKD compared to baseline methods on both unseen indistribution and outofdistribution instances.  Specificity: The paper is wellstructured and provides detailed information on the methodology results and comparison with existing methods. 3. Weaknesses:  Lack of Comparison: While the paper provides a comprehensive comparison with existing methods there could be more detailed comparisons with various scenarios such as different backbone models and problem sizes.  Missing Discussion: The paper lacks indepth discussion on the limitations and potential areas for future research in the conclusion section.  Reproducibility: More details on the implementation and hyperparameters used in the experiments could enhance reproducibility. 4. Questions:  Can AMDKD be further extended to handle more complex VRP variants or other combinatorial optimization problems  How sensitive is AMDKD to the choice of exemplar distributions used for training the teacher models  Are there any specific strategies employed in the adaptive strategy to track the realtime learning performance of the student model on each exemplar distribution 5. Limitations:  Generalization Improvement: While AMDKD shows promising results the paper could elaborate more on scenarios where the method may not lead to substantial improvements in generalization.  Scalability: How well does AMDKD perform with increasing problem sizes or more complex problem instances  Interpretability: Further clarification on the interpretability of AMDKD would enhance the understanding of the knowledge distillation process. Overall the paper introduces a promising approach with the AMDKD scheme to enhance the generalization of deep models for VRPs. The experimental results validate the efficacy of the method and further investigations into scalability and interpretability could provide deeper insights. Further revisions addressing the weaknesses and limitations would strengthen the paper.", "xLnfzQYSIue": "Peer Review 1) Summary: The paper provides a comprehensive analysis of Top Two algorithms in the context of best arm identification in multiarmed bandit models particularly focusing on bounded distributions. The study introduces new variants of Top Two algorithms including the use of the Empirical Best leader and Transportation Cost Improved challenger and proves their asymptotic performance for nonparametric distributions. Experimental results on realworld data and Bernoulli instances illustrate the effectiveness of these algorithms compared to benchmarks like KLLUCB and uniform sampling. 2) Strengths and weaknesses: Strengths:  The paper offers a novel analysis of Top Two algorithms beyond Gaussian distributions focusing on bounded distributions and showing asymptotic \u03b2optimality.  Introduces new variants of Top Two algorithms that outperform existing benchmarks on realworld data and Bernoulli instances.  Provides detailed proofs and explanations of key concepts including the sampling rules leaderchallenger choices and sample complexity analysis.  Offers a strong theoretical foundation combined with practical experiments. Weaknesses:  The paper could benefit from a more concise presentation as the extensive theoretical details may be overwhelming for nonexperts.  While the experimental results are promising a deeper analysis of scalability and computational efficiency for larger datasets could be beneficial.  Additional clarification on the practical implications and potential limitations of the proposed algorithms would enhance the discussion. 3) Questions:  Can the proposed Top Two algorithms be easily extended to handle more complex distribution classes beyond bounded and Bernoulli distributions such as heavytailed distributions  How sensitive are the algorithms to hyperparameter choices such as the threshold function in the stopping rule or the sampling probability \u03b2  Have the authors considered the computational complexity of the algorithms in realworld applications that involve a large number of arms or datasets with high dimensionality 4) Limitations:  The generalizability of the experimental results may be limited due to the focus on specific distributions and scenarios. Further validation on diverse datasets is needed for robustness.  The paper does not explore the potential tradeoffs between empirical performance and computational efficiency which could be critical in practical applications.  While theoretical guarantees for the asymptotic \u03b2optimality are established the study does not address the nonasymptotic performance for a broader range of \u03b4 values which could be more informative for practical use. Overall the paper presents a valuable contribution to the field of best arm identification in bandit models offering insights into Top Two algorithms for bounded distributions and showcasing their effectiveness through experiments. Addressing the highlighted questions and limitations could further enhance the impact and applicability of the proposed algorithms.", "sL7XH6-V21e": " Peer Review of the Paper  Summary: The paper presents a theoretical analysis of the Deep Forest (DF) algorithm a multilayer ensemble learning method comparing it with the Random Forest (RF) algorithm. The authors analyze the role of the prediction concatenation (PreConc) operation in DF and show that DF can achieve faster convergence rates than RF with increasing the number of trees. The paper explores the consistency and power of deep forests in contrast to random forests. The theoretical analysis is supported by simulation experiments validating the proposed results.  Strengths: 1. The paper provides a comprehensive theoretical analysis of the Deep Forest algorithm offering insights into the advantages of depth over width in ensemble learning. 2. The study offers rigorous mathematical proofs and propositions to support the theoretical findings enhancing the understanding of the algorithms properties. 3. The simulation experiments help validate the theoretical results and provide practical implications for algorithm design and optimization. 4. The paper addresses an important research gap by focusing on the theoretical analysis of the prediction concatenation operation in deep forests.  Weaknesses: 1. The paper could benefit from further discussion on the practical implications of the theoretical findings. How can the theoretical advantages of DF be leveraged in realworld applications 2. The text can be dense with technical details which may limit accessibility to readers not wellversed in ensemble learning theory. Consider simplifying the presentation to improve readability. 3. The assumptions made in the theoretical analysis might not fully capture the complexity of realworld datasets and scenarios. Discuss the limitations of the assumptions and how they might impact the generalizability of the results.  Questions: 1. Can you elaborate on the implications of the priority of the new feature in the secondlayer decision tree splitting in DF How does this impact the overall predictive performance of the algorithm 2. How do the theoretical findings on the convergence rates of DF and RF with increasing trees align with practical considerations for model selection and optimization 3. Are there specific characteristics or types of datasets where the benefits of deep forests over random forests would be more pronounced  Limitations: 1. The study focuses on asymptotic consistency potentially limiting the applicability of the results to realworld finite sample scenarios. 2. The paper relies on certain assumptions that might not fully capture the complexity of real datasets. Consider discussing the implications of these assumptions on the applicability of the results in practice. Overall the paper offers valuable insights into the theoretical advantages of Deep Forest over Random Forest in ensemble learning. Enhancing the discussion on practical implications and addressing some of the technical complexities could improve the accessibility and impact of the research. Further investigations could focus on validating the theoretical findings on diverse datasets and exploring the scalability of the proposed algorithms.", "u4dXcUEsN7B": " Review of the Paper \"Exploring Example Influence for StabilityPlasticity Dilemma in Continual Learning\"  1) Summary: The paper addresses the StabilityPlasticity (SP) dilemma in Continual Learning (CL) by exploring the impact of example influence on achieving better SP through sequential learning. The proposed MetaSP algorithm efficiently computes example influence on S and P and fuses them towards SP Pareto optimality. Empirical results demonstrate the superiority of the proposed method over stateoftheart approaches on both task and classincremental benchmark CL datasets.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important issue in CL by focusing on the impact of example influence on SP.  The proposed MetaSP algorithm offers a simple yet effective solution to compute and fuse example influence towards SP Pareto optimality.  Experimental results show significant performance improvements over existing methods on various CL datasets. Weaknesses:  The paper lacks a detailed discussion on the computational complexity of the proposed algorithm and the potential scalability issues especially when dealing with largescale datasets.  It would be beneficial to provide deeper insights into the underlying mechanisms of how the computed example influence impacts the learning process and the model\u2019s behavior.  3) Questions: 1. How does the proposed MetaSP algorithm compare in terms of training time efficiency with existing methods especially when applied to larger datasets 2. Can the effectiveness of the proposed approach be further validated on more diverse and challenging CL datasets to ensure its generalizability 3. Are there any specific scenarios or types of tasks where the proposed MetaSP algorithm might not perform as effectively and if so can you provide insights into the limitations in those cases  4) Limitations:  The paper could elaborate on the practical applicability of the proposed algorithm in realworld CL scenarios and discuss how it can be adapted to different domains.  The generalizability of the results could be further examined by including a broader range of evaluation metrics to assess various aspects of model performance. Overall the paper presents a novel approach to addressing the SP dilemma in CL through the exploration of example influence. The proposed MetaSP algorithm shows promising results in improving SP performance although further research is needed to address the identified limitations and to validate the algorithms applicability in diverse CL scenarios.", "rBCvMG-JsPd": " Summary: The paper compares fewshot incontext learning (ICL) and parameterefficient finetuning (PEFT) methods for adapting pretrained language models to new tasks. The proposed PEFT method named (IA)3 scales activations by learned vectors achieving better performance with fewer new parameters. The study introduces a recipe called TFew based on a T0 model that outperforms ICL approaches achieving superhuman performance on the RAFT benchmark.  Strengths: 1. Novel Approach: Introducing (IA)3 and the TFew recipe provides a unique and promising alternative to fewshot learning methods. 2. Comprehensive Evaluation: The paper rigorously compares different methods across various experiments datasets and tasks providing a thorough analysis of performance and computational costs. 3. High Performance: Achieving stateoftheart accuracy on the RAFT benchmark and outperforming human baselines demonstrates the effectiveness of the proposed approach. 4. Transparency: Providing detailed explanations of the methods datasets hyperparameters and results enhances the reproducibility and understanding of the study.  Weaknesses: 1. Validation: While the performance metrics are impressive more analysis on generalizability beyond the RAFT benchmark could strengthen the studys conclusions. 2. Realworld Implementation: Assessing the practical implications deployment challenges and realworld applicability of TFew could enrich the studys impact. 3. Code and Documentation: While the authors mention that the code is publicly available further details on implementation setup and usability could improve reproducibility and accessibility.  Questions: 1. How does the study address potential biases or limitations in the datasets used for evaluation 2. Are there specific scenarios or tasks where the proposed TFew recipe may not perform as effectively compared to conventional finetuning or ICL methods 3. Can the (IA)3 method be applied to different types of models or tasks beyond language models and are there limitations to its applicability in diverse domains 4. How does the study handle interpretability and explainability aspects of the proposed PEFT method particularly when applied to critical or sensitive tasks  Limitations: 1. TaskSpecific Evaluation: The study focuses on classification tasks limiting insights into the generalizability of TFew across diverse NLP applications or tasks. 2. Complexity: The proposed PEFT method and TFew recipe involve additional parameters and computational costs potentially raising concerns about scalability and efficiency in largescale deployment. 3. External Validity: While the results are promising the study may benefit from validation on a wider range of datasets tasks or models to confirm the effectiveness across diverse scenarios.", "p6hArCtwLAU": "Review of the Paper Summary: The paper introduces TreeMoCo a selfsupervised learning framework based on TreeLSTM network to represent neuron morphology. It addresses the need for a quantitative representation of neuron morphology and demonstrates its effectiveness in classifying celltypes and subtypes. The method is tested on 2403 3D neuron reconstructions and shows promising results providing a new approach for quantitative neuron morphology analysis. Strengths: 1. Innovative Methodology: The use of TreeLSTM network and selfsupervised learning for neuron morphology representation is novel and addresses an essential need in neuroscience. 2. Effective Classification: TreeMoCo shows success in both classifying major brain celltypes and identifying subtypes demonstrating its potential in neuron morphology analysis. 3. Comprehensive Evaluation: The experiments and results are welldetailed showing visual inspection and quantitative evaluation to assess the performance of TreeMoCo. 4. Thorough Background: The paper provides a strong background on the problem of neuron morphology representation and related works giving context to the proposed TreeMoCo framework. Weaknesses: 1. Data Quality Variation: The paper notes performance fluctuations in certain datasets indicating potential issues with data quality. Further investigation into the impact of data quality on the results could enhance the robustness of the proposed method. 2. Lack of Early Stopping Criterion: The absence of an effective early stopping criterion especially in the absence of label information could lead to challenges in training convergence and model performance stability. 3. Limited Discussion on Limitations: While the paper mentions limitations related to global sensitivity of TreeLSTM more detailed discussion on potential drawbacks and challenges would offer a more comprehensive analysis. Questions: 1. Generalization: How well does TreeMoCo generalize to datasets with different characteristics or species beyond mouse brain neuron reconstructions 2. Scalability: Have scalability considerations been evaluated for applying TreeMoCo to larger datasets with potentially millions of neuron reconstructions 3. Biological Validation: Are there plans for biological validation of neuron classification results obtained using TreeMoCo to ensure the biological relevance of the identified celltypes 4. Implementation Details: Could more details be provided on the hyperparameters used and the training process to facilitate replication and extension of the proposed method Limitations: 1. Data Quality and Variations: Fluctuations in results across datasets hint at potential data quality issues or variations that could impact the methods generalizability and robustness. 2. Training Stability: The observed oscillations in performance during training on small datasets raise concerns about the stability and consistency of the method over extended training periods. 3. Dependency on Topology Information: The sensitivity of TreeLSTM to global changes due to topologybased augmentation suggests limitations in scenarios where topology structure may vary significantly. Overall the paper presents a valuable contribution to the field of neuroscience through the development of TreeMoCo for neuron morphology representation. Addressing the highlighted weaknesses and limitations could further strengthen the robustness and applicability of the proposed method in quantitative neuron morphology analysis.", "znbTxnBPlx": " Peer Review:  1) Summary: The paper presents a novel method for training stabilized supralinear networks (SSNs) a model of cortical neural circuits that aims to bridge the gap between biological realism and network performance in neural networks. The method involves gradually expanding the network size while training to avoid dynamical instabilities and achieve successful training on challenging tasks such as MNIST classification and probabilistic inference under a Gaussian scale mixture generative model. The results demonstrate the effectiveness of the approach in training realistic corticallike neural networks on diverse tasks. Overall the method contributes to advancing the understanding of neural circuit mechanisms at a singlecell resolution.  2) Strengths and Weaknesses:  Strengths:  Uniqueness of the proposed method: The gradual network expansion approach via dynamicsneutral addition of neurons during training addresses the challenge of training SSNs on complex tasks while maintaining stability.  Successfully trained SSNs on challenging tasks: The paper demonstrates the ability to train SSNs on MNIST classification and on probabilistic inference tasks under a Gaussian scale mixture generative model tasks that were previously unattainable. This showcases the effectiveness of the method.  Importance for biological relevance: The paper emphasizes the importance of maintaining biological realism in neural networks contributing to deeper insights into neural circuit mechanisms.  Weaknesses:  Lack of comparison with existing methods: While the paper demonstrates the effectiveness of the proposed method a comparison with existing methods for training SSNs could provide a clearer understanding of its advantages and limitations.  Complexity of the method: The method involves a detailed process of network expansion during training which may raise concerns about computational costs and scalability.  Limited discussion on generalizability: The paper focuses on specific tasks like MNIST classification and probabilistic inference. Discussion on the generalizability of the method to a broader range of tasks would enhance the impact of the research.  3) Questions: 1. How does the proposed method compare to existing techniques for training biologically realistic neural networks particularly in terms of scalability and computational efficiency 2. Can the method be extended to train SSNs on tasks beyond MNIST classification and Gaussian scale mixture inference and if so what potential challenges or limitations might arise 3. How does the proposed training method impact the interpretability of the trained SSNs in understanding neural circuit mechanisms at a singlecell resolution  4) Limitations:  The paper focuses on specific tasks without extending the discussion to broader applications limiting the demonstration of the versatility of the proposed method.  The detailed process of network expansion during training may raise concerns about its computational efficiency and practical implementation in largescale networks.  The lack of comparison with existing methods may inhibit a comprehensive assessment of the methods advantages and generalizability. In conclusion the paper presents a novel and effective method for training biologically realistic SSNs on challenging tasks showcasing the potential for deeper insights into neural circuit mechanisms. Addressing scalability broader applications and comparative analysis with existing methods could further enhance the impact and applicability of the proposed method.  [End of Review]", "zLVLB-OncUY": " Peer Review  1) Summary: The paper proposes a novel approach Contrastive Optimal Positive Generation (COPGen) for selfsupervised contrastive learning. The approach aims to generate optimal positive pairs for each instance by learning instancespecific latent transformations in the latent space of pretrained generative models. Through theoretical analysis and empirical experiments the paper demonstrates the effectiveness of COPGen in producing contrastive positives that outperform other methods in selfsupervised contrastive learning settings.  2) Strengths and weaknesses: Strengths:  The paper addresses an important issue in contrastive learning by focusing on generating instancespecific optimal positives.  The theoretical foundation provided for COPGen is robust and wellexplained.  The empirical experiments support the effectiveness of COPGen in improving contrastive learning performance.  Leveraging pretrained generative models in latent space navigation is a novel and promising approach. Weaknesses:  While the theoretical analysis is detailed the realworld implications and practical significance could be further elaborated.  The paper could benefit from a more comprehensive comparison with other stateoftheart methods in selfsupervised contrastive learning.  The computational overhead and training time associated with COPGen are mentioned but could be further discussed in terms of practicality for realworld applications.  3) Questions:  Can you provide more insights into how the COPGen approach could be further applied or extended beyond the current scope of selfsupervised contrastive learning  How does the instancespecific nature of latent transformations impact generalization to unseen data or different downstream tasks  Could the COPGen approach be sensitive to variations in the pretrained generative models used as data sources  4) Limitations:  The primary limitation of the paper lies in the potential practicality and scalability of the proposed COPGen approach especially in realworld applications with largescale datasets.  The reliance on welltrained generative models as data sources could introduce biases or limitations that may affect the generalizability and transferability of the learned representations.  The impact of synthetic data learning on realworld tasks and domains needs to be further explored to understand its implications fully. Overall the paper presents a valuable contribution to the field of selfsupervised contrastive learning with the introduction of COPGen. However further investigation into the practical implications of the approach and its broader impact on representation learning is recommended. The theoretical underpinnings and experimental results provide a strong foundation for future research in this area.", "tadPkBL2gHa": " Peer Review  1) Summary: The paper addresses the issue of academic recruitment under uncertainty proposing a new formulation to optimize candidate selection while considering the target number of positions as a soft constraint. The authors develop algorithms under different penalty functions conduct theoretical analyses and provide empirical evaluations. The results suggest that the proposed algorithms especially LOWVALUEL1 offer competitive performance compared to traditional greedy heuristics XGREEDY and XPGREEDY.  2) Strengths and Weaknesses: Strengths:  The paper addresses a relevant and practical problem in academic recruitment settings.  The formulation of the problem with soft constraint target positions is a novel and promising approach.  Theoretical analyses provide insights into the efficiency and optimality of various algorithms.  Empirical evaluations on different correlation scenarios add to the robustness of the proposed algorithms.  The paper provides a nuanced comparison of different heuristics and a constantfactor approximation algorithm. Weaknesses:  The paper could benefit from a clearer discussion on the practical implications and potential limitations of the proposed algorithms.  It would be useful to include a discussion on the scalability of the algorithms to larger instances or realworld recruitment scenarios.  The paper lacks a direct comparison with existing recruitment practices or tools which could enhance the relevance and applicability of the proposed algorithms.  3) Questions:  Can the proposed algorithms be easily integrated into existing recruitment systems or processes in academic institutions  How sensitive are the algorithmic performance and theoretical guarantees to variations in the assumptions made about candidate values and acceptance probabilities  Are there specific scenarios or types of academic recruitment settings where the proposed algorithms are expected to outperform traditional methods significantly  4) Limitations:  The experimental evaluations are based on synthetically generated data additional studies using realworld recruitment datasets would be valuable.  The paper does not explicitly address issues related to diversity bias or fairness in academic recruitment which are important considerations in modern recruitment practices.  The focus on negative correlation scenarios in experiments may limit the generalizability of the findings to other recruitment contexts with different characteristics. In conclusion the paper offers valuable insights into optimizing academic recruitment decisions under uncertainty. The theoretical analyses and empirical evaluations support the efficacy of the proposed algorithms. Addressing the limitations and potential extensions of the research could further enhance its impact and applicability in academic recruitment settings.", "vhKaBdOOobB": " Peer Review:  1) Summary: The paper introduces a novel hardwarefriendly attention mechanism called DFC attention and presents the GhostNetV2 architecture designed for mobile applications. The DFC attention mechanism is based on fully connected layers and efficiently captures longrange spatial information in a lightweight convolutional neural network. GhostNetV2 demonstrates superior performance over existing architectures achieving 75.3 top1 accuracy on ImageNet with 167M FLOPs while maintaining a good tradeoff between accuracy and inference speed.  2) Strengths and Weaknesses: Strengths:  Introduces a novel DFC attention mechanism that efficiently captures longrange spatial information in lightweight CNNs.  GhostNetV2 architecture improves over existing architectures showing enhanced accuracy with similar computational costs.  Extensive experiments validate the superiority of GhostNetV2 on benchmark datasets.  The paper addresses the importance of efficiency and speed in neural network models for mobile applications. Weaknesses:  The paper could provide more detailed comparisons with other stateoftheart models including a broader range of datasets and tasks.  Some sections in the paper are very technical and may be challenging for readers unfamiliar with deep learning concepts.  The paper could include more information on the generalizability of GhostNetV2 to a wider variety of tasks beyond image classification and object detection.  3) Questions:  How does the DFC attention mechanism differ from other attention mechanisms in terms of computational efficiency and model performance  Can the hardwarefriendly DFC attention be applied to other neural network architectures beyond GhostNetV2 with comparable benefits  What are the implications of the proposed approach for realworld deployment on various mobile devices in terms of speed memory usage and flexibility  Are there any limitations in terms of scalability or applicability of the GhostNetV2 architecture to larger and more complex datasets or tasks  4) Limitations:  The paper focuses primarily on image classification performance on ImageNet and object detection on COCO. It could benefit from exploring the application of GhostNetV2 in other fields of computer vision.  The practical implications of deploying GhostNetV2 on different mobile devices and the potential tradeoffs between accuracy and speed need to be further investigated.  The use of DFC attention in enhancing model performance could have limitations in scenarios with highly dynamic or nonstatic datasets. Overall the paper presents a novel attention mechanism and architecture design for lightweight convolutional neural networks showcasing improved performance and efficiency. Further exploration into the generalizability practical deployment and potential limitations of GhostNetV2 could enhance the significance of this research in the field of mobile AI applications.", "sZAbXH4ezvg": " Peer Review  1) Summary The paper introduces and justifies the limitations of implicit graph neural networks (GNNs) concerning their ability to capture longrange dependencies and multiscale information on graphs. A theoretical analysis is provided to show the constrained expressiveness of previous implicit GNNs in capturing distant information. To address these limitations the paper proposes a Multiscale Graph Neural Network with Implicit Layers (MGNNI) that can model multiscale structures and expand the effective range for capturing longrange dependencies. Comprehensive experiments on both node and graph classification tasks demonstrate the superior performance of MGNNI over representative baselines.  2) Strengths and Weaknesses Strengths:  The paper clearly identifies and addresses limitations in existing implicit GNNs related to capturing longrange dependencies and multiscale information.  The theoretical analyses provided are thorough and provide a solid foundation for the proposed MGNNI model.  The comprehensive experiments on synthetic and realworld datasets showcase the effectiveness of MGNNI in capturing both longrange and multiscale information.  The ablation study and attention visualization enhance the understanding of the models performance and behavior. Weaknesses:  The paper lacks a detailed discussion on the potential implications or applications of MGNNI beyond the experimental results.  The paper focuses heavily on technical details and experimental results potentially making it less accessible to readers unfamiliar with graph neural networks.  Further comparisons with more stateoftheart models or methods could strengthen the papers contributions.  3) Questions  How does the proposed MGNNI model compare to other recent approaches that aim to address the limitations of implicit GNNs in capturing longrange dependencies and multiscale information  Are there specific realworld applications or domains where MGNNI is expected to provide the most significant improvements over existing models  Can the novel insights provided by the paper on effective range and multiscale modeling have implications for other areas of graphbased machine learning  4) Limitations  The paper could provide more insights into the scalability and computational efficiency of MGNNI especially for largescale graphs and complex datasets.  There is a need for more discussion on potential challenges or tradeoffs associated with integrating multiscale modeling into implicit GNNs.  The paper might benefit from addressing the generalizability and robustness of MGNNI across a wider range of datasets and tasks. Overall the paper presents a significant contribution to the field of graph neural networks by addressing critical limitations of implicit GNNs and introducing a novel approach (MGNNI) to enhance the modeling of longrange dependencies and multiscale information. Further work could focus on refining the models computational efficiency scalability and applicability to diverse realworld scenarios.", "tYAS1Rpys5": "Peer Review: 1) Summary: The paper introduces Simulationguided Beam Search (SGBS) as a novel beamsearch method in combinatorial optimization (CO) problems. The authors propose a hybrid scheme using SGBS and efficient active search (EAS) to enhance the quality of solutions found in neural CO approaches. The study evaluates these methods on wellknown CO benchmarks like the traveling salesperson problem (TSP) capacitated vehicle routing problem (CVRP) and flexible flow shop problem (FFSP) showing significant improvements in solution quality under reasonable runtime assumptions. The research highlights the importance of effective inference methods for neural CO and presents SGBS as an efficient search mechanism. Additionally SGBS combined with EAS demonstrates superior performance over traditional CO methods. 2) Strengths:  The paper presents a novel approach SGBS which offers an effective search mechanism for neural CO methods especially in realworld CO problems.  The hybridization of SGBS with efficient active search (EAS) shows promising results in enhancing solution quality.  The study is wellstructured providing detailed explanations of the proposed methods algorithms experiments and results.  The experimental evaluations on popular CO benchmarks demonstrate the effectiveness of SGBS and the SGBSEAS hybrid approach in producing highquality solutions.  The paper addresses a gap in the field by focusing on effective inference methods in neural CO settings which is crucial for industrial applications. Weaknesses:  The paper could benefit from a more detailed discussion on the limitations and potential challenges of the proposed methods.  While the study showcases impressive results a more extensive comparison with existing stateoftheart neural CO methods would strengthen the evaluation of SGBS and SGBSEAS.  The explanation of the SGBS algorithm while informative might be too detailed for readers not familiar with CO and neural optimization techniques. 4) Questions:  What additional experiments or validations could further support the generalizability and robustness of the SGBS and SGBSEAS methods across different CO problem domains  How does the runtime complexity of SGBS compare to traditional CO solvers and other neural CO methods especially for largescale instances  Can the authors provide insights into the interpretability of the solutions generated by SGBS and how they compare to results obtained from other inference techniques in neural CO 5) Limitations:  The research focuses on the efficacy of SGBS and the SGBSEAS hybrid in neural CO optimization potentially limiting the scope of the studys applicability to a broader range of optimization problems.  The lack of indepth analysis on the performance tradeoffs computational overheads and scalability aspects of SGBS may restrict its practical deployment in resourceconstrained environments.  The impact of hyperparameters tuning in SGBS and EAS on solution quality and convergence rates is not extensively explored raising concerns about the robustness of these methods across different problem instances. Overall the paper presents a valuable contribution to the field of neural combinatorial optimization with the introduction of SGBS and the demonstration of its effectiveness in improving solution quality. Further work on addressing the limitations and expanding the experimental evaluations would enhance the impact and applicability of the proposed methods in practical CO scenarios.", "qqHMvHbfu6": " Peer Review of \"Analyzing Learning Dynamics in Lewis Signaling Games\"  1) Summary: This research paper delves into the dynamics of language evolution through Lewis signaling games and investigates the emergence of structured communication protocols. The authors decompose the loss function in Lewis games into two components  an information loss and a coadaptation loss. By manipulating the coadaptation loss they demonstrate that controlling for overfitting leads to the development of more compositional languages that generalize better. Through empirical experiments and analytical insights the authors shed light on the critical role of coadaptation dynamics in shaping emergent language properties.  2) Strengths and Weaknesses:  Strengths:  The paper provides a clear and wellstructured description of Lewis signaling games the motivation behind the study and the methodology employed. The theoretical framework and empirical experiments are welldesigned.  The decomposition of the loss function into information and coadaptation losses offers a novel perspective on the learning dynamics in Lewis games.  The experimental results effectively demonstrate the impact of controlling overfitting in the coadaptation loss on language properties and generalization.  The authors provide thorough discussions on the implications of their findings such as the relationship between coadaptation overfitting generalization and compositionality.  Weaknesses:  While the paper presents clear experimental results additional statistical analyses could strengthen the significance of the findings.  The paper could benefit from a more detailed discussion on the implications of the research for the field of linguistics and machine learning.  3) Questions:  How did the authors determine the optimal balance between the information loss and the coadaptation loss in the Lewis games Is this balance specific to each game scenario or generalizable across different settings  Can the findings of this study be applied to other areas of machine learning beyond language emergence such as reinforcement learning in complex environments  4) Limitations:  The research primarily focuses on Lewis signaling games with specific constraints and assumptions. Generalizing the findings to more complex or varied scenarios may require further investigation.  The study heavily relies on empirical experiments with synthetic environments. Validating the results in realworld settings or more diverse datasets could enhance the robustness of the findings. Overall this paper provides valuable insights into the learning dynamics of language emergence in Lewis signaling games. The thorough experimental analyses and theoretical framework contribute significantly to the understanding of how coadaptation overfitting impacts the development of structured communication protocols. Further research building upon the current findings could potentially advance the field of computational linguistics and machine learning.", "pD5Pl5hen_g": "Peer Review: Summary: This paper addresses the minimax optimization problem with smooth and stronglyconvexstronglyconcave functions by proposing an optimal algorithm with lower gradient evaluation complexity. By reformulating the problem applying the proximal point algorithm and solving monotone inclusions the authors develop an algorithm that matches the lower bound of the gradient evaluation complexity. Strengths: 1. The paper addresses a fundamental issue in minimax optimization by providing an algorithm with optimal gradient evaluation complexity. 2. The methodology is wellstructured with detailed explanations of the algorithm and theoretical analysis. 3. The paper presents a thorough review of related works elucidating the contributions in the context of existing literature. 4. The proposed Algorithm 4 is welldesigned using insights from other algorithms and adaptations for solving the minimax problem effectively. Weaknesses: 1. The paper can be challenging for readers not wellversed in optimization theory due to its high level of technicality. 2. While the proposed algorithm shows improved complexity empirical validation or comparison with other stateoftheart methods on benchmark problems would strengthen the analysis. 3. The paper lacks practical examples or realworld applications to demonstrate the effectiveness of the proposed algorithm in practical scenarios. Questions: 1. Could the authors provide a comparison with existing algorithms on benchmark problems to showcase the practical superiority of their proposed algorithm 2. Have the authors considered the computational overhead involved in implementing the proposed algorithm particularly in terms of memory usage and computational time 3. Can the proposed Algorithm 4 handle noise or uncertainties in the objective functions and has robustness been considered in the algorithm design Limitations: 1. The paper focuses on theoretical analysis and complexity bounds lacking empirical validation on real datasets. 2. The high complexity of the proposed algorithm may limit its applicability to largescale problems. 3. The paper assumes specific properties of the objective functions which may not always hold in practical applications. Overall this paper makes a significant contribution to the field of minimax optimization by providing an optimal algorithm. Addressing the weaknesses and limitations highlighted would enhance the clarity and applicability of the research.", "msFfpucKMf": "Peer Review 1) Summary: The paper presents a framework for training robust options to perform arbitrary sequences of subtasks by formulating the problem as a twoagent zerosum game. The proposed algorithms ROSAC and AROSAC are designed to maximize worstcase performance across all tasks. Experimental evaluations on two multitask environments show the superiority of the proposed algorithms over existing approaches. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem in reinforcement learning by focusing on compositional reinforcement learning and training robust options.  The use of adversarial reinforcement learning adds a novel perspective to the problem.  The proposed algorithms ROSAC and AROSAC are welldesigned and demonstrate superior performance over existing baselines.  The experiments are thorough evaluating the algorithms on two diverse multitask environments. Weaknesses:  The paper lacks a detailed discussion on the computational complexity and scalability of the proposed algorithms.  The limitations section could be expanded to include more detailed insights into potential drawbacks of the approach. 3) Questions:  What is the computational overhead of the asynchronous value iteration algorithm proposed in the paper  How sensitive are the proposed algorithms to hyperparameters tuning  How practical are the proposed algorithms for realworld applications with complex and dynamic environments 4) Limitations:  The option discovery process in the proposed algorithms relies on a fixed set of subtasks provided by the user. This may limit the adaptability of the algorithms to unknown or dynamic environments.  The complexity of implementing the proposed algorithms in realworld robotic applications especially considering the need for finetuning and training times could be a potential limitation. 5) Overall Impression: The paper presents a novel and wellstructured framework for training robust options in compositional reinforcement learning. The theoretical foundations algorithms and experimental validations provide a comprehensive analysis of the proposed approach. Addressing some scalability issues providing more insights into potential drawbacks and further discussing the realworld applicability could improve the completeness of the work. Overall the paper makes a significant contribution to the field of reinforcement learning.", "vy7B8z0-4D": " Peer Review  1) Summary The paper introduces a novel method Fused Unbalanced Gromov Wasserstein (FUGW) for intersubject alignment of cortical surfaces using optimal transport. FUGW aligns functional signatures while penalizing large deformations addressing challenges in functional alignment. The method significantly increases betweensubject correlation leading to precise mapping at the group level.  2) Strengths and Weaknesses Strengths:  Introduces a novel method FUGW addressing intersubject variability in neuroimaging data.  Demonstrates improved betweensubject correlation and more precise mapping at the group level.  Wellstructured experiments and numerical results providing comprehensive evaluation. Weaknesses:  Lack of comparison with more functional alignment approaches in the experiments.  Limited discussion on the computational complexity and scalability for larger datasets.  Clearer explanation of the significance of certain hyperparameters and their impact needed.  3) Questions  Can the FUGW method be extended or adapted to address interspecies alignment challenges  How robust is the FUGW method to noise or variability in the functional data used for alignment  Are there any plans to integrate FUGW into existing neuroimaging software or pipelines for broader usage  4) Limitations  The study relies on a relatively small cohort of individuals validation on larger datasets could strengthen the findings.  The interpretation and utility of some hyperparameters could be further explored and explained.  Exploration of FUGWs performance on different types of neuroimaging data and tasks would enhance the methods generalizability. Overall the paper presents an innovative solution to an important challenge in neuroimaging research. Further validation on larger datasets broader comparisons with existing methods and clarity on hyperparameter choices would enhance the impact and understanding of the FUGW method.", "x_HUcWi1aF1": " Summary This paper introduces a novel approach for offline multiagent reinforcement learning by proposing the strategywise concentration principle. The study focuses on twoplayer zerosum Markov games and multiplayer generalsum Markov games. The proposed algorithms show improvements in sample complexity compared to prior methods and can find nearoptimal strategies. The paper also highlights the significance of unilateral coverage for learning Nash equilibrium strategies in offline MARL scenarios.  Strengths and Weaknesses Strengths: 1. Innovative Approach: The strategywise concentration principle introduces a novel strategywise bonus that reduces the exponential dependence on the number of players in offline MARL. 2. Improved Sample Complexity: The algorithms proposed in the paper show better sample complexity and computational efficiency especially in twoplayer zerosum games and multiplayer generalsum games. 3. Theoretical Contributions: The paper contributes valuable theoretical insights into offline MARL highlighting the importance of unilateral coverage for learning NE strategies. Weaknesses: 1. Complexity in Algorithm 4: The surrogate minimization algorithm for multiplayer generalsum games (Algorithm 4) may face computational challenges due to the need to explore the entire strategy class \\xe2\\x87\\xa7. 2. Limited Experimental Validation: The paper lacks empirical validation of the proposed algorithms which could offer insights into the practical performance of the methods. 3. Theoretical Assumptions: The sample complexity guarantees are based on specific assumptions the practical feasibility of these assumptions in realworld scenarios needs to be further investigated.  Questions 1. Practical Applicability: How well do the proposed algorithms perform in realworld scenarios with complex multiagent interactions and large action spaces 2. Computational Efficiency: Can the algorithms proposed for multiplayer generalsum games scale effectively to scenarios with a large number of players 3. Extension to Function Approximation: Are there plans to extend the study to incorporate function approximation techniques for MARL scenarios with highdimensional state and action spaces  Limitations 1. The absence of empirical validation of the algorithms limits the practical understanding of their performance. 2. The study focuses primarily on theoretical aspects how would the proposed methods fare in scenarios with noisy or incomplete information 3. The assumptions made in the theoretical analyses may not always hold in realworld settings potentially impacting the effectiveness of the proposed algorithms. Overall the paper presents valuable contributions to the field of offline MARL but further empirical validation and exploration of practical applications are needed to enhance its impact and relevance.", "rnJzy8JnaX": "Peer Review 1. Summary: The paper investigates the utilization of lowresolution frames for efficient video recognition. It reveals that the poor recognition accuracy on lowresolution frames is mainly due to the mismatch between network architecture and input scale rather than information loss in the downsampling process. The proposed method CrossResolution Knowledge Distillation (ResKD) bridges this gap by transferring knowledge from highresolution frames to guide the learning of the network on lowresolution frames. Extensive experiments demonstrate the effectiveness of ResKD on benchmark datasets showcasing superior efficiency and accuracy compared to existing methods. The paper presents a detailed analysis of the causes of performance degradation on lowresolution videos introduces ResKD as a simple yet powerful solution and provides comprehensive results demonstrating its efficacy across different datasets and architectures. 2. Strengths and Weaknesses: Strengths:  Indepth Analysis: The paper provides a thorough analysis of the issues affecting recognition accuracy on lowresolution frames offering insights into the importance of network architecture and input scale coordination.  Novel Method: The introduction of ResKD as a solution to address the mismatch between network design and input resolution is novel and promising.  Extensive Experiments: Extensive experiments on largescale datasets and comparison with stateoftheart methods demonstrate the superiority of ResKD in terms of efficiency and accuracy.  Clear Presentation: The paper is wellstructured with clear explanations of the problem methodology and experimental results. Weaknesses:  Evaluation Metrics: While the paper evaluates the proposed method on various datasets and architectures it would be beneficial to include additional metrics or analyses to further validate the effectiveness of ResKD.  Comparison: Providing more detailed comparisons with a wider range of existing methods could strengthen the paper\u2019s contribution and highlight its uniqueness.  Reproducibility: While the code availability is mentioned additional details on implementation and reproducibility could enhance the practical utility of the proposed method. 3. Questions:  Generalization: How generalizable is ResKD across different video recognition tasks and datasets beyond the ones mentioned in the paper  Potential Applications: Have you considered specific realworld applications or scenarios where ResKD could significantly impact video recognition efficiency and accuracy  Scalability: How scalable is ResKD to handle extremely lowresolution frames and are there any limitations or challenges in extending its applicability to such scenarios 4. Limitations: The limitations of the study are as follows:  Scope: The paper focuses on the efficacy of ResKD in addressing the network and input scale mismatch but the broader implications or potential drawbacks in practical implementation could be further explored.  Complexity: While ResKD is presented as a relatively simple and effective solution the complexity of integrating it into existing video recognition systems or workflows could pose challenges that need to be addressed. In conclusion the paper offers valuable insights into leveraging lowresolution frames for efficient video recognition and presents a novel method ResKD as a solution to enhance accuracy in such scenarios. Further exploration of the generalizability practical applications and scalability of ResKD could enhance the impact and relevance of the proposed approach in the field of video recognition research.", "zofwPmKL-DO": " Peer Review: 1) Summary: The paper presents quantum algorithms for logconcave sampling and estimating their normalizing constants. The authors develop quantum algorithms based on underdamped Langevin diffusion and Metropolisadjusted Langevin algorithms achieving significant speedups compared to classical algorithms. They establish a quantum lower bound for estimating normalizing constants. The work explores classical techniques in a quantum context and provides substantial advancements in quantum sampling strategies. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem in logconcave sampling and provides quantum solutions with significant speedups.  The use of underdamped Langevin diffusion and Metropolisadjusted Langevin algorithms showcases innovative quantum adaptation of classical methods.  The quantum algorithms achieve polynomial speedups in key parameters enhancing efficiency over classical approaches.  The theoretical analysis and technical details are thorough providing a solid foundation for the proposed quantum algorithms. Weaknesses:  The paper could benefit from more detailed explanations or examples to aid in understanding the quantum algorithms for readers less familiar with the quantum computing concepts.  Some sections contain technical details that might be challenging for readers without a strong background in quantum computing or mathematical optimization. 3) Questions:  Could the proposed quantum algorithms be further optimized or generalized to handle a broader range of logconcave distributions beyond the specified conditions  How sensitive are the quantum algorithms to noise or errors in the quantum computation and have these aspects been considered in the analysis  Are there realworld applications or datasets where the proposed quantum logconcave sampling algorithms could be practically applied and if so what performance improvements might be expected 4) Limitations:  The paper mainly focuses on theoretical advancements and quantum speedups in logconcave sampling. Realworld implementation challenges or experimental validations of the quantum algorithms are not discussed.  The complexity and mathematical nature of the paper might limit its accessibility to a broader audience particularly those with limited background in quantum computing or mathematical optimization. Overall the paper presents significant advancements in quantum algorithms for logconcave sampling and normalizing constant estimation demonstrating promising speedups over classical approaches. The theoretical analysis and technical details are comprehensive laying a solid foundation for future research in quantum sampling methods. Additional discussions on practical applications and potential experimental validations could enhance the impact of the study.", "yipUuqxveCy": "Peer Review 1) Summary The paper introduces a novel framework for offline multiagent reinforcement learning (offline MARL) utilizing previously collected data. The method reformulates offline MARL as a sequence modeling problem based on the Transformer architecture. Central to the framework is training a teacher policy with access to all agents data to identify and recombine \"good\" behavior. This knowledge is then distilled into separate student policies enabling decentralized execution and better credit assignment across agents. Experimental results demonstrate improved performance convergence rate sample efficiency and robustness compared to existing baselines across various tasks. 2) Strengths and Weaknesses Strengths:  The paper addresses an important issue in MARL by proposing a novel framework for offline MARL that leverages precollected data without online interactions.  The methodology is wellmotivated and introduced with clear explanations of the components involved.  Experimental results demonstrate the effectiveness of the proposed method outperforming existing baselines in terms of performance convergence rate and sample efficiency.  The incorporation of structural relation distillation in policy learning is novel and shows benefits in multiagent tasks. Weaknesses:  The paper lacks a detailed analysis of the computational complexity and scalability of the proposed method especially in scenarios with a large number of agents.  The limitations section could be expanded to provide insights into potential challenges that the method might face in realworld applications.  Clarity could be improved in some sections to facilitate understanding especially regarding the mapping networks role in distillation. 3) Questions  How does the proposed method handle situations where the teacher policy may overlook important aspects of the agents behavior during the distillation process  Can the method be extended to handle dynamic environments where agent behaviors change over time  Has the proposed framework been tested on more complex multiagent environments to assess its scalability and generalizability 4) Limitations  The paper focuses on the performance and efficacy of the proposed method in controlled environments realworld application may present additional challenges.  The results are limited to a set of specific tasks and environments broader evaluation across a wider range of scenarios is needed.  The scalability of the centralized decision transformer in larger multiagent systems remains a potential limitation. 5) Recommendations for Improvement  Provide a more indepth discussion on the scalability and computational efficiency of the proposed method especially in scenarios with numerous agents.  Explore the application of the framework in more complex and dynamic environments to gauge its practical utility.  Consider expanding the discussion on potential ethical implications or negative consequences of deploying such algorithms in realworld multiagent systems. Overall the paper presents a wellmotivated and novel framework for offline MARL with promising results. With some clarifications expansions and further experimentation the method could have significant implications for advancing multiagent reinforcement learning research.", "lhLEGeBC-ru": " Peer Review 1) Summary Avg word length: 131 The paper investigates the BurerMonteiro method for solving largescale semidefinite programs (SDP) in a smoothed analysis setting. It presents the first polynomial time guarantees for the BurerMonteiro method for both SDP and least squares SDP (SDPls). The main technical contribution is the utilization of algebraic geometry for concentration bounds. The paper provides endtoend guarantees without structural assumptions and improves existing results. Experimental results support the theoretical findings highlighting the methods efficacy and phase transitions. The paper is wellstructured providing thorough theoretical analysis alongside experimental validation. 2) Strengths and Weaknesses Strengths:  Clear formulation of problem statements and theoretical results.  Novel application of algebraic geometry for concentration bounds.  Extensive experimental validation supporting theoretical findings.  Addressed an important open question in the context of SDP.  Theoretical results have potential implications in practical optimization algorithms. Weaknesses:  The technical nature of the paper may limit accessibility to a broader audience.  Statistical significance of experiments could be further discussed.  The application of the theoretical findings to practical scenarios and realworld datasets could have added value. 3) Questions  Could the paper provide more insights into how the theoretical findings may be practically applied or extended beyond the theoretical setting  How do the experimental results align with the theoretical predictions and are there any discrepancies worth discussing  Is there a discussion on the computational complexity of implementing the proposed method in comparison to existing methods 4) Limitations  The narrow focus on theoretical guarantees and smoothed analysis may limit the practical applicability of the findings.  The complexity of the theoretical methods proposed may pose challenges for widespread adoption in optimization applications.  Further discussions on the generalization of the results to other classes of SDPs and the implications of structural bounds on the theoretical results could enhance the papers depth. In conclusion the paper presents significant contributions to the field of optimization particularly in the context of SDP solving techniques. While the theoretical findings are robust and strong further discussions on practical implications computational complexities and generalizability could enhance the utility and impact of the research. I recommend publication with minor revisions focusing on practical implications and discussions on generalizability.", "sADLRl2STMe": "Peer Review 1) Summary: The paper investigates the interplay between implicit and explicit regularization in deep linear neural networks using an explicit penalty that mimics the implicit bias observed in gradient trajectories during optimization. The study proposes a novel penalty that in combination with specific adaptive optimizers like Adam enables a degenerate singlelayer network to achieve lowrank approximations with comparable generalization error to deep networks. The experiments demonstrate that the penalty under Adam yields depthinvariant solutions in matrix completion tasks outperforming various approaches and achieving nearperfect rank recovery. The findings suggest a nuanced role for explicit regularization in designing efficient learning dynamics in deep networks. 2) Strengths:  The paper addresses an important topic by exploring the interactions between implicit and explicit regularization in deep networks.  The experiments are welldesigned and provide clear insights into the effectiveness of the proposed penalty under different optimization settings.  The comparative analysis with other penalties and optimizers enhances the robustness of the studys conclusions.  The analysis extends beyond theoretical considerations to realworld data demonstrating practical applicability. 3) Weaknesses:  The paper lacks a detailed discussion on the computational complexity or scalability of the proposed method especially in comparison to traditional regularization techniques.  The evaluation on realworld data while valuable could benefit from a more comprehensive comparison with stateoftheart methods in recommendation systems.  The limitations of the study such as the generalizability of the findings to nonlinear networks or classification tasks could be explicitly discussed in more detail. 4) Questions:  Can the proposed explicit penalty demonstrate superiority over traditional regularization methods in scenarios beyond matrix completion tasks  How does the proposed penalty compare to stateoftheart deep learning architectures for matrix completion or factorization tasks  Are there considerations for adapting the proposed penalty to more complex neural network structures or tasks requiring nonlinear representations 5) Limitations:  The study primarily focuses on linear neural networks for matrix completion limiting the generalizability of the findings to other deep learning architectures or tasks.  The lack of a comprehensive analysis of the tradeoffs between the penaltys effectiveness and computational overhead may limit its practical adoption.  While the results are promising further validation on diverse datasets and benchmarks is needed to establish the broad applicability of the proposed penalty. In conclusion the paper presents an insightful exploration of the interplay between implicit and explicit regularization in deep linear networks highlighting the potential for explicit penalties to enhance learning dynamics. Addressing the identified weaknesses and limitations could further strengthen the papers contributions and implications for the field of deep learning regularization techniques.", "kvtVrzQPvgb": " Peer Review:  1) Summary: The paper introduces a novel aggregation procedure based on the social choice theory to rank systems in a multitask setting addressing the limitations of meanaggregation. Through extensive numerical experiments the authors demonstrate the superiority of their approach showcasing its reliability and robustness on both synthetic and real scores from various benchmarks in Natural Language Processing (NLP). The study reveals that the proposed method yields different rankings of stateoftheart systems compared to meanaggregation while being more reliable and robust.  2) Strengths and weaknesses: Strengths:  The paper addresses a crucial issue in benchmarks of NLP systems and proposes an innovative and theoretically grounded solution.  The thorough numerical experiments on synthetic and real scores validate the effectiveness of the proposed method.  The research adds significant value to the field by enhancing the reliability and robustness of ranking systems across different tasks. Weaknesses:  The paper lacks a detailed discussion on the computational complexity of the proposed method compared to traditional aggregation approaches.  The assumptions and limitations of the proposed aggregation procedure should be explicitly stated for a more comprehensive understanding.  More detailed comparisons with existing aggregation methods could enhance the significance of the proposed approach.  4) Questions: 1. Could you elaborate on the computational complexity of the proposed Kemeny consensus aggregation procedure in practical settings and how it compares to other aggregation methods 2. How sensitive is the proposed method to outliers or noisy data within the scores of multiple tasks in realworld scenarios 3. Have you considered conducting qualitative analyses or user studies to assess the interpretability of the rankings obtained through the proposed aggregation procedure  5) Limitations:  The research focus is primarily on numerical experiments potentially lacking a more comprehensive qualitative analysis or user studies.  The generalization of the proposed method to other domains outside of NLP should be further explored and discussed in the paper.  The scalability and practical applicability of the aggregation procedure on largescale datasets need to be addressed for realworld usage. Overall the paper presents a valuable contribution to the domain of NLP benchmarks offering a novel aggregation procedure to rank systems in multitask settings. Further improvements in computational complexity analysis robustness to outliers and generalization to other domains would enhance the impact and applicability of the proposed method.", "pyLFJ9TBZw": " Peer Review:  1) Summary: The paper proposes Generative Multitask Learning (GMTL) a method for causal machine learning in the multitask setting. By modifying the inference objective of conventional multitask learning GMTL aims to mitigate targetcausing confounding and improve robustness to target shift. Through empirical validation on two datasets Attributes of People and Taskonomy the results show improved robustness across multiple multitask learning methods.  2) Strengths and weaknesses: Strengths:  The paper introduces a novel method GMTL for causal machine learning in multitask settings.  The theoretical foundation of the proposed approach is wellexplained focusing on addressing targetcausing confounding.  Empirical validation on two datasets showcases improved robustness to target shift across multiple multitask learning methods.  The approach is flexible offering a single hyperparameter to adjust the removal of spurious dependencies between input and targets.  The paper extensively discusses the interpretation of alpha the severity of target shift and its relevance in the context of CML. Weaknesses:  The paper could benefit from a clearer distinction between existing approaches and how GMTL differs or improves upon them.  While the theoretical background is wellestablished the practical implications and realworld applications could be further emphasized.  The explanation of the experimental setup especially the models used could be more detailed to aid in reproducibility.  The limitations and potential challenges in implementing GMTL in complex scenarios or with larger datasets could provide valuable insights.  3) Questions:  Can you elaborate on how GMTL compares to existing methods that address target shift and unobserved confounders in multitask learning  How do the results of GMTL generalize to different types of datasets or domains beyond the ones tested in this study  Are there specific scenarios or types of multitask learning tasks where GMTL is particularly effective or are there limitations to its applicability  Could you provide more insights into the computational complexity or scalability of GMTL when applied to larger datasets or a higher number of tasks  4) Limitations:  The paper focuses mainly on two datasets which might limit the generalizability of GMTL to diverse datasets and domains.  The experimental evaluation while comprehensive could be extended to include additional benchmark datasets or tasks to further validate the effectiveness of GMTL.  The assumptions made in GMTL such as dealing with targetcausing confounders might not be applicable in all multitask learning scenarios raising potential challenges in practical implementation.  The interpretation and setting of the hyperparameter alpha could pose challenges in scenarios where the true test distribution is unknown or dynamically changing. In conclusion the paper presents a promising approach in the realm of causal machine learning for multitask settings with the introduction of GMTL. The theoretical foundation is solid and the empirical results show potential improvements in robustness to target shift. Further exploration and validation across diverse datasets and scenarios would enhance the applicability and impact of GMTL in realworld settings. Clarifying the distinctions from existing methods addressing scalability concerns and providing a more diverse set of experimental evaluations will strengthen the papers contribution to the field.", "pkfpkWU536D": " Peer Review:   Summary: The paper introduces Neural Shape Deformation Priors a novel method for shape manipulation by predicting mesh deformations of nonrigid objects based on userprovided handle movements. The approach leverages transformerbased deformation networks to learn continuous deformation fields anchored in 3D space enabling manipulation of meshes with varying vertices and poses. Experimental validation on the DeformingThing4D dataset showcases the methods effectiveness in producing accurate and natural shape deformations compared to optimizationbased and recent neural networkbased techniques.  Strengths: 1. Innovative Methodology: The introduction of transformerbased local deformation field networks for shape manipulation is a novel approach that improves generalization to new deformations. 2. Clear Exposition: The paper effectively outlines the problem methodology and experimental results making it accessible to readers. 3. Robust Evaluation: Comprehensive experiments and comparisons against classical and modern methods support the claims of the proposed techniques superiority in handling shape deformation.  Weaknesses: 1. Data Limitation: While the DeformingThing4D dataset is utilized for validation the limitations in the available models and poses may affect the generalization to diverse scenarios. Expanding the dataset could strengthen the models performance in realworld applications. 2. Handle Interactivity: The restriction to handle selection from a set of candidate parts limits the users interaction regions. Addressing this limitation by allowing free handle placement during training could enhance usability.  Questions: 1. Generalization to Unseen Models: How does the proposed method perform when applied to completely novel models outside the training dataset Are there plans to extend the evaluation to cover a wider range of shapes 2. Computational Complexity: Could the authors provide insights into the computational requirements of the proposed method particularly in terms of training time and inference speed 3. Handling Rotation: Is there ongoing research to incorporate rotations alongside displacements to allow for more diverse shape manipulations  Limitations: 1. Dataset Constraints: The models generalization may be constrained by the limited diversity in the training dataset impacting its performance on unseen deformations. 2. User Interaction: The restriction to handle placement from predefined parts may limit the users creative control in shape manipulation. Overall the paper presents a promising methodology for shape manipulation demonstrating superior performance compared to conventional and recent techniques. Addressing the limitations and potential future research directions could further enhance the impact of the proposed approach in practical applications.  This review provides a detailed evaluation of the strengths weaknesses questions and limitations of the paper offering constructive feedback for further improvement and exploration in the field of shape manipulation research.", "lxdWr1jN8-h": " Summary The paper introduces Symmetric Planning (SymPlan) framework utilizing group symmetry to enhance data efficiency and generalization in endtoend differentiable planning algorithms for 2D robotic path planning problems. It formalizes the concept from Value Iteration Networks (VINs) and demonstrates the representation of value iteration in a convolutional form named Symmetric Planner (SymPlan) achieving significant improvements over nonequivariant counterparts.  Strengths 1. Novel Contribution: The paper introduces a novel framework SymPlan to incorporate symmetry into value iteration for path planning demonstrating substantial improvements in training efficiency and generalization. 2. Theoretical Foundation: The paper provides a solid theoretical foundation by proving equivariance properties in the path planning context and connecting it with steerable convolution networks. 3. Detailed Experiments: Detailed experiments showcase the effectiveness of the proposed framework on both given and learned maps providing a comprehensive evaluation. 4. Clear Structure: The paper is wellorganized with clearly defined sections covering related work method experiments and discussions.  Weaknesses 1. Code Availability: The absence of code data and instructions for reproducing main experimental results is a drawback. Providing access to these resources would enhance reproducibility. 2. Mapper Network Bottleneck: In the experiments on learned maps the mapper networks impact as a bottleneck is highlighted indicating potential limitations in certain scenarios.  Questions 1. Can you elaborate on the scalability of the SymPlan framework to higherdimensional or continuous Euclidean spaces beyond 2D grids 2. How sensitive is the SymPlan framework to changes in hyperparameters particularly kernel sizes or representation choices 3. Are there plans to release the code and data to enable reproducibility and further exploration by the research community  Limitations 1. Ethical and Societal Impact: The paper does not discuss potential negative societal implications or ethical considerations related to the proposed SymPlan framework. Including such discussions is essential for responsible research. 2. Explicit Code and Data Availability: Although the paper plans to release code and data before final publication providing access to these resources in the initial submission would enhance transparency and reproducibility.  Overall Evaluation The paper provides a novel contribution with a wellstructured presentation of the Symmetric Planning framework. It offers a solid theoretical foundation detailed experiments and valuable insights into enhancing endtoend planning algorithms through symmetry. Addressing limitations such as ethical considerations explicit code and data availability and further clarifying practical implications will enhance the impact and credibility of the study.", "tvDRmAxGIjw": " Summary: The paper introduces a posttraining quantization (PTQ) method called modulewise reconstruction error minimization (MREM) for pretrained language models (PLMs). MREM aims to reduce reconstruction errors by partitioning PLMs into modules and optimizing them jointly. The proposed method allows fast memorysaving and dataefficient quantization of PLMs with superior performance compared to existing PTQ approaches.  Strengths: 1. Innovative Approach: MREM introduces a novel modulewise reconstruction error minimization technique offering a new perspective on PTQ for PLMs. 2. Efficiency: The paper demonstrates significant reductions in training time memory overhead and data consumption compared to previous quantizationaware training methods. 3. Experimental Results: Empirical results on GLUE and SQuAD benchmarks indicate the effectiveness of MREM in improving quantized PLMs performance.  Weaknesses: 1. Complexity: The MREM method is quite intricate and the paper could benefit from providing a clearer and more intuitive explanation of the proposed approach. 2. Comparison with Existing Methods: While the paper does compare MREM with existing PTQ methods a more comprehensive comparison with a wider range of techniques could provide a better perspective on the proposed methods performance.  Questions: 1. Scalability: How does the performance of MREM scale with different sizes of PLMs and datasets 2. Generalizability: Can MREM be applied to other types of neural network architectures beyond the Transformerbased PLMs 3. Finetuning Tradeoffs: What are the potential tradeoffs in using MREM for finetuning quantized PLMs compared to fullprecision finetuning 4. Extension: Are there any plans to extend MREM to address additional challenges in PLM quantization such as improved compression ratio or robustness to noise  Limitations: 1. Complexity of Methodology: The detailed methodology and parallel training strategy might be challenging for practitioners to implement and replicate. 2. Experimental Setup: While the experimental results are promising a more extensive comparison with a wider range of PTQ methods and datasets could further validate the effectiveness of MREM. Overall the paper presents an intriguing approach to PTQ for PLMs with promising results. Further exploration and clarification of the methods implementation and scalability could enhance the practical utility of MREM.", "u_7qyNFwkP8": "Peer Review of Paper Titled \"Query Complexity of Cake Cutting\" Summary: The paper investigates the query complexity of cake cutting focusing on envyfree perfect and equitable allocations. It provides lower and upper bounds for computing these allocations with the minimum number of cuts. The study covers various fairness notions simulations of moving knife procedures and analysis of different query models. By introducing new results and formalizations the paper contributes to the existing literature on cake cutting and fair division. Strengths: 1. Originality: The paper addresses an interesting and relevant problem in fair division algorithms related to cake cutting. 2. Theoretical Contributions: The paper provides tight lower and upper bounds for computing fair allocations with minimal queries contributing valuable insights to the field. 3. Comprehensive Analysis: The study thoroughly investigates different fairness notions moving knife procedures and various query models offering a comprehensive understanding of the problem space. 4. Clear Structure: The paper is wellstructured and organized making it easy to follow the logical flow of the research. Weaknesses: 1. Complexity: The paper contains complex theoretical results and formal definitions which may be challenging for readers with limited background knowledge in mathematics or computer science. 2. Technical Jargon: The use of technical terms and symbols could hinder the accessibility of the material for a broader audience. 3. Limited Practical Implications: While the study provides valuable theoretical insights practical applications or realworld implications of the findings are not explicitly discussed. Questions: 1. Could the paper provide more context on the practical significance of the query complexity results in cake cutting algorithms 2. How do the results presented in the paper contribute to the broader field of fair division algorithms and computational complexity 3. Were any assumptions made about the valuations or the behavior of the players that might impact the generalizability of the findings to realworld scenarios Limitations: 1. Theoretical Focus: The paper primarily focuses on theoretical aspects of cake cutting algorithms potentially limiting its immediate applicability in practical scenarios. 2. Complexity Limitation: The high level of technical complexity in the paper may restrict its accessibility to a wider audience or interdisciplinary researchers. In conclusion the paper provides valuable theoretical insights into the query complexity of cake cutting algorithms. Clarifying the practical implications and addressing potential limitations could strengthen the impact of the research in both the academic and applied communities.", "w4X7GLThiuJ": " Summary: The paper introduces the alternating mirror descent algorithm for twoplayer zerosum games with constrained strategy spaces. The algorithm is analyzed as an alternating discretization of a skewgradient flow in the dual space. The paper establishes an O(K 23) bound on the average regret after K iterations showing better performance compared to the simultaneous version of the mirror descent algorithm. The analysis highlights the connection between game theory alternating mirror descent dynamics and symplectic integrators.  Strengths: 1. Novel Algorithm: Introduces the alternating mirror descent algorithm for constrained minmax games and establishes its effectiveness in achieving a better regret bound. 2. Theoretical Depth: Analyzes the algorithm as an alternating discretization of a skewgradient flow providing a theoretical framework for understanding its performance. 3. Interdisciplinary Connection: Links game theory optimization methods and symplectic integrators paving the way for crossfertilization between different areas.  Weaknesses: 1. Complexity: The paper involves advanced mathematical concepts and may be challenging for readers without a strong background in convex optimization and game theory. 2. Applicability: The theoretical analysis may limit the practical applicability of the algorithm in realworld scenarios unless simplified versions or practical implementations are explored.  Questions: 1. Generalization: How applicable are the findings and results of the alternating mirror descent algorithm to other types of games beyond twoplayer zerosum games 2. Practical Implications: Have there been any practical implementations or experiments conducted to validate the performance of the algorithm in realistic settings 3. Computational Complexity: How does the computational complexity of the algorithm scale with larger problem sizes and constraints Are there any scalability concerns 4. Extension to Asynchronous Settings: How would the algorithm perform or need to be adapted in asynchronous settings where players make moves in decentralized or randomized fashions  Limitations: 1. Theoretical Focus: While the theoretical analysis is thorough practical applicability in realworld settings might be limited without further exploration and simplification. 2. Scalability: The scalability of the algorithm to larger and more complex games as well as its computational requirements could be a limitation that needs further investigation.", "tPiE70y40cv": " Peer Review:  Summary: The paper presents a novel approach called \"aggregation tree with pseudocube\" for constructing coresets from relational data. The authors address the challenge of constructing coresets for relational data sets where the data is decoupled into multiple tables making it challenging to access the entire data in one matrix. They propose a bottomup tree structure that forms pseudocubes in each subspace for building the coresets. Additionally they provide theoretical proofs and empirical evaluations on machine learning tasks like clustering logistic regression and SVM to demonstrate the effectiveness of their method.  Strengths: 1. Innovative Approach: The paper introduces a novel method the \"aggregation tree with pseudocube\" to construct coresets for relational data addressing a significant challenge in data processing. 2. Theoretical Analysis: The authors provide theoretical proofs and claims to support the effectiveness and efficiency of their approach. 3. Empirical Evaluation: The experimental evaluation on realworld data sets and popular machine learning tasks adds credibility to the proposed method by demonstrating its superior optimization quality and acceptable running time compared to baseline methods.  Weaknesses: 1. Complexity: While the proposed method shows promising results the complexity of the approach especially in handling overlap issues in pseudocubes and random sampling could potentially be a bottleneck for practical implementations. 2. Limited Scope: The paper focused on specific machine learning tasks and relational data types. It would be beneficial to discuss potential applications extensions and limitations in more diverse scenarios.  Questions: 1. Has the proposed method been tested on larger and more diverse relational data sets to assess scalability and generalizability 2. How does the proposed method handle cases where the assumption of low intrinsic dimensions in realworld data sets does not hold 3. Are there plans to explore the application of the aggregation tree with pseudocube approach to other machine learning problems beyond clustering logistic regression and SVM  Limitations: 1. The evaluation mainly focuses on theoretical performance and empirical results on specific data sets and machine learning tasks which may limit the broader applicability and robustness of the proposed method across various domains and data types. 2. The paper could benefit from a more detailed discussion on practical implementation challenges potential tradeoffs and realworld deployment considerations of the proposed technique. Overall the paper presents a novel approach to constructing coresets for relational data sets supported by theoretical analysis and promising empirical results. Addressing practical implementation complexities and broader application scenarios can further enhance the impact and relevance of the proposed method in realworld settings.", "ylila4AYSpV": " Peer Review  Summary: The paper addresses the issue of designing a truthful auction mechanism for modern ad auctions with rich ad formats. The main focus is on maximizing social welfare by selecting a set of ads that fit within the given total space. The authors propose a simple greedy and monotone allocation rule that guarantees a 3 approximation to the optimal social welfare. The paper includes theoretical analysis lower bounds on approximation ratios and empirical evaluation on realworld data.  Strengths: 1. Novelty: The paper tackles an important and complex problem in the design of modern ad auctions which is relevant to the growing field of online advertising. 2. Theoretical Contributions: The proposed monotone allocation rule for ad auctions and the associated approximation guarantees are theoretically sound and provide insights into the design of truthful mechanisms. 3. Empirical Evaluation: The experimental results on realworld data provide practical insights into the performance of the proposed algorithms. 4. Clear Organization: The paper is wellstructured with detailed explanations of the problem theoretical results algorithms and empirical evaluation.  Weaknesses: 1. Over Reliance on Technical Detail: The paper contains extensive technical detail which might make it challenging for nonexperts to grasp the key contributions easily. 2. Assumption of Knowledge: The paper assumes a certain level of knowledge in auction theory and mechanism design which could limit understanding for readers less familiar with the field. 3. Limited Discussion on Practical Implications: While the empirical evaluation is valuable more discussion on the practical implications of the results could enhance the paper. 4. Complexity: The complexity of the proposed algorithms might limit their practical implementation and adoption in realworld systems.  Questions: 1. Could the practical implementations of the proposed algorithms face scalability challenges in realworld systems with a large number of advertisers and rich ad formats 2. How sensitive are the algorithms to variations in the types of rich ad formats or the characteristics of the advertisers 3. How do the proposed algorithms compare to other existing ad auction mechanisms such as the Generalized Second Price rule in terms of efficiency and effectiveness 4. Are there any specific considerations or modifications needed for the algorithms to address potential adversarial behaviors or strategic responses from advertisers  Limitations: 1. Technical Accessibility: The paper\u2019s technical complexity might limit its accessibility to a broader audience including industry practitioners and researchers from related fields. 2. Scalability: The scalability of the proposed algorithms to largescale ad auction platforms with diverse advertisers and rich ad formats needs further investigation. 3. Assumption of Truthful Behavior: The assumption of truthful behavior from advertisers might not always hold in practice potentially impacting the efficacy of the proposed mechanisms. Overall the paper presents valuable contributions to the field of ad auctions and mechanism design offering both theoretical insights and practical implications for optimizing ad allocations in modern online advertising environments. Further clarification on the practical implementation and robustness of the proposed algorithms would enhance the impact of the research.", "pZsAwqUgnAs": " Peer Review of the Scientific Paper: \"Understanding the Dynamics of Stochastic Gradient Descent in HighDimensional Convex Quadratics\" 1. Summary: This paper focuses on investigating the dynamics of Stochastic Gradient Descent (SGD) in highdimensional convex quadratic functions. The authors introduce Homogenized Stochastic Gradient Descent (HSGD) as a stochastic differential equation that characterizes the behavior of multipass SGD. The paper presents asymptotic predictions for learning and risk trajectories establishes an equivalence between SGD and HSGD and explores the impact of noise on generalization performance. The results suggest that SGD does not offer implicit regularization benefits in this context but provide insights into the efficiency of SGD relative to fullbatch methods. 2. Strengths and Weaknesses: Strengths:  The paper takes a rigorous mathematical approach to analyze the dynamics of SGD in highdimensional convex quadratics.  The establishment of the equivalence between SGD and HSGD facilitates a deeper understanding of the efficiency of SGD.  The results provide concrete formulas for learning and risk trajectories shedding light on the performance of SGD relative to fullbatch methods.  The analysis of the impact of noise on generalization performance contributes valuable insights to the field of machine learning optimization. Weaknesses:  The paper heavily focuses on theoretical aspects and may benefit from more practical implications or empirical validations of the proposed approaches.  The paper assumes quasirandom properties of the data which may limit the generalizability of the findings to realworld datasets that do not meet these assumptions.  The discussion on the implications for nonconvex problems seems limited and further exploration in this area could strengthen the papers contributions. 3. Questions:  Can the findings of the study be extended to more complex nonconvex optimization problems commonly encountered in machine learning applications  How applicable are the results to realworld datasets that may not exhibit the quasirandom properties assumed in the analysis  Are there considerations for different variations of SGD (e.g. adaptive learning rates) that could impact the observed dynamics in highdimensional convex quadratics 4. Limitations:  The reliance on quasirandom data assumptions may limit the practical relevance of the findings for datasets that do not meet these strict criteria.  The focus on convex quadratic problems may restrict the generalizability of the results to a broader range of optimization scenarios.  The absence of empirical validations or realworld applications may hinder the practical impact of the theoretical insights presented in the paper. Overall the paper offers a significant contribution to understanding the dynamics of SGD in highdimensional convex quadratics through a rigorous mathematical framework. Further exploration of the implications for nonconvex problems and the consideration of realworld datasets could enhance the practical implications of the research.", "nrOLtfeiIdh": " Peer Review  1) Summary: The paper proposes a novel model RECOURSENET which addresses the problem of poor performance of machine learning models on instances sampled from adverse environments. RECOURSENET consists of a classifier a recourse recommender and a recourse trigger and is trained using a threelevel training process without access to the underlying physical process. The model aims to recommend alternative environment settings so that recoursed instances lead to better predictions. The paper includes theoretical characterizations and experimental results on synthetic and realworld datasets to demonstrate the efficacy of the proposed approach.  2) Strengths and Weaknesses: Strengths:  Novel proposed model RECOURSENET addressing the issue of poor performance in adverse environments.  Introduces a threelevel training strategy that overcomes the challenge of not having access to the underlying physical process.  Theoretical characterizations provided to identify the circumstances under which recourse would enhance prediction accuracy.  Comprehensive experimental evaluation on synthetic and realworld datasets to showcase the effectiveness of the approach. Weaknesses:  The theoretical descriptions could be more concise for better readability.  The experimental section lacks a direct comparison with stateoftheart approaches in the field of instancedriven models.  The paper could benefit from deeper discussions on the generalizability and scalability of the proposed model to different domains.  3) Questions:  How does the proposed model handle complex datasets with continuous environment variables  Are there plans to extend the model to incorporate multiple views of the same object during the recourse process for improved decisionmaking  How does the model address potential bias or fairness concerns in the environment recommendation process  4) Limitations:  Limited benchmark datasets that record different environment settings may affect the generalizability of the model.  The proposed model assumes discrete environment settings which might not fully capture the complexity of realworld scenarios.  The evaluation on realworld datasets could be strengthened by adding more diverse and challenging datasets to assess the models robustness.  Overall Evaluation: The paper presents a compelling approach RECOURSENET to address the challenge of poor ML performance in adverse environments. The theoretical foundations experimental evaluations and proposed strategies contribute to the advancement of instancedriven models. However addressing the mentioned limitations and expanding on the practical implications of the model could further enhance the impact and relevance of the research.", "l5UNyaHqFdO": " Peer Review   Summary: The paper presents a comprehensive study on the convergence and divergence behavior of the Adam optimization algorithm. The authors address the gap between theoretical understanding and practical performance of Adam particularly focusing on the relationship between the choice of hyperparameters (\u03b21 \u03b22) and the convergence behavior. Through theoretical analysis and experiments on various functions and datasets the authors demonstrate that Adam can converge to critical points without any modifications to its update rules provided certain conditions are met such as the size of \u03b22 and the relationship between \u03b21 and \u03b22. The paper uncovers the phase transition from divergence to convergence as \u03b22 increases and provides guidelines for hyperparameter tuning.  Strengths: 1. Novel Contribution: The paper makes a significant contribution by showing that Adam can converge without modifications to its update rules debunking the common belief that Adam may diverge. This is a crucial finding that enhances the understanding and applicability of Adam. 2. Thorough Analysis: The theoretical analysis conducted in the paper is rigorous and provides clear insights into the convergence and divergence behaviors of Adam under various scenarios. The experiments conducted on different datasets and functions validate the theoretical findings. 3. Detailed Methodology: The paper provides detailed explanations of the theoretical proofs and methodologies used to investigate the convergence properties of Adam under different settings. The comprehensive analysis of the relevance of hyperparameters to convergence adds depth to the study.  Weaknesses: 1. Complexity: The paper delves into intricate theoretical analyses and introduces novel concepts which might make it challenging for the general audience to grasp the core ideas presented. Simplifying the presentation of complex concepts could enhance the accessibility of the study. 2. Limited Practical Application: While the theoretical findings are insightful the practical implications for machine learning practitioners or researchers may be limited due to the highly technical nature of the study. Providing more practical examples or implications for realworld applications could increase the relevance of the research.  Questions: 1. Robustness of Results: How robust are the convergence results of Adam under different scenarios such as varying dataset sizes or functions with different characteristics Have the experiments been conducted with a wider range of datasets to validate the findings 2. Comparison with Other Optimization Algorithms: How does the convergence behavior of Adam compare with other popular optimization algorithms such as SGD or RMSProp especially in terms of convergence rates and effectiveness in different scenarios  Limitations: 1. Generalization: The study mainly focuses on Adams convergence behavior under specific conditions and settings. The generalizability of the findings to a broader range of applications or scenarios may need further exploration. 2. Practical Relevance: While the theoretical analysis is robust the direct practical implications and benefits for machine learning practitioners may require clearer elucidation to bridge the gap between theory and application. Overall the paper provides valuable insights into the behavior of the Adam optimization algorithm but enhancements in presentation and practical relevance could further elevate its impact in the machine learning community.  This peer review summarizes the strengths weaknesses questions and limitations of the paper offering constructive feedback for potential improvements.", "p-56bnzZhQ7": " Peer Review: 1) Summary: The paper investigates the complexity of PAC learning halfspaces in the presence of Massart noise. It presents a reduction from the Learning with Errors (LWE) problem to the computational hardness of learning Massart halfspaces demonstrating that no polynomialtime algorithm can achieve an error better than \\xce\\xa9(\u03b7) under the assumption of LWEs subexponentialtime hardness. The main result establishes a lower bound on the error rate for efficient learning algorithms in this setting. 2) Strengths and Weaknesses: Strengths:  The paper provides a detailed and rigorous analysis of the computational complexity of learning halfspaces in the presence of Massart noise.  It establishes a novel hardness result for the learning of Massart halfspaces showcasing the limitations of current efficient learning algorithms.  The reduction from the LWE problem to learning Massart halfspaces is welldefined and technically sound. Weaknesses:  The paper is highly technical and may be challenging for readers unfamiliar with the specific domain of research.  The implications of the results on practical applications or realworld scenarios could be further elaborated. 3) Questions:  Could the assumptions made about the hardness of the LWE problem affect the generalizability of the results to other domains or problems  Are there potential avenues to improve the efficiency of algorithms for learning Massart halfspaces given the established lower bound on error rates  Can the findings of this study be extended to analyze the computational complexity of learning tasks in noisy environments beyond the realm of halfspaces 4) Limitations:  The paper focuses primarily on theoretical aspects of computational complexity possibly limiting its immediate applicability to practical machine learning scenarios.  The technical depth of the paper may hinder its accessibility to a broader audience potentially limiting its impact beyond specialized research communities.", "r70ZpWKiCW": " Peer Review: 1) Summary: The paper introduces a novel framework Gentle Teaching Assistant (GTASeg) for semisupervised semantic segmentation aiming to address the challenges posed by incorrect pseudo labels in existing semisupervised frameworks. The proposed method employs a gentle teaching assistant to disentangle the effects of pseudo labels on feature extraction and mask prediction ultimately improving performance. Through extensive experiments on benchmark datasets the authors demonstrate the effectiveness of GTASeg in leveraging unlabeled data and enhancing segmentation performance. 2) Strengths:  The paper presents a wellmotivated approach to addressing the challenges of incorrect pseudo labels in semisupervised semantic segmentation through the GTASeg framework.  The introduction of a gentle teaching assistant to transfer beneficial feature knowledge to the student model effectively protects it from unreliable pseudo labels showcasing thoughtful design.  Extensive experiments on benchmark datasets demonstrate competitive performance against previous methods validating the effectiveness of GTASeg in leveraging unlabeled data.  The inclusion of ablation studies hyperparameter sensitivity analysis and visualization results strengthens the empirical evaluation of the proposed method. 3) Weaknesses:  While the paper addresses the challenges of incorrect pseudo labels in the context of semisupervised semantic segmentation more discussion on potential realworld applications would enhance the practical significance of the proposed framework.  The inclusion of additional experiments showcasing the scalability of GTASeg with larger datasets or different model architectures would further enhance the generalizability of the proposed method. 4) Questions:  How does the proposed GTASeg framework compare in terms of computational efficiency and training time compared to existing semisupervised segmentation methods  Could the authors provide insights into the interpretability of the models decisions and the potential impact of the GTASeg framework on model explainability  How generalizable is the proposed method to different datasets with varying characteristics or to other computer vision tasks beyond semantic segmentation 5) Limitations:  The paper lacks a detailed discussion on the potential limitations or drawbacks of the proposed GTASeg framework such as scenarios in which the method might underperform or practical constraints in realworld applications.  While the methodology is comprehensively described more indepth analysis on the theoretical foundations supporting the design choices in GTASeg would further enhance the academic and scientific rigor of the paper.", "lNokkSaUbfV": "Peer Review 1) Summary The paper introduces a novel selfsupervised pretraining method for reinforcement learning called Masked Decision Prediction (MaskDP). MaskDP utilizes a masked autoencoder to reconstruct stateaction trajectories enabling the model to infer maskedout states and actions for learning generalizable and scalable RL agents. The study demonstrates that MaskDP outperforms existing methods in zeroshot transfer tasks and offline RL showing promising scalability and adaptability. The method achieves competitive results with prior pretraining methods based on autoregressive pretraining. 2) Strengths and Weaknesses Strengths: The paper provides a comprehensive exploration of MaskDP offering a clear motivation detailed methodology and thorough experimental evaluation. The study demonstrates the efficacy of MaskDP in diverse downstream RL tasks showcasing its generalization capabilities and performance superiority over existing methods. The methods scalability and zeroshot transfer abilities are highlighted as key strengths. Weaknesses: Some weaknesses include the lack of comparison with a wider range of baselines and the absence of an indepth analysis of possible failure cases or scenarios where MaskDP may not perform well. The paper could benefit from a more detailed discussion on the interpretability of the learned representations and the computational complexity of MaskDP in realworld applications. 3) Questions  How does the computational complexity of MaskDP compare to existing pretraining methods in terms of training time and resource requirements  Could the authors elaborate on the interpretability of the learned representations by MaskDP and how they contribute to the models performance in downstream tasks  Are there specific scenarios or environments where MaskDP may not generalize well and if so how can these limitations be addressed or mitigated in future research 4) Limitations The limitations of this study include the narrow focus on specific downstream RL tasks potentially limiting the generalizability of MaskDP to a broader range of problems. The reliance on synthetic datasets for pretraining could introduce biases or artifacts that may not be present in realworld applications. Additionally the lack of realworld deployment and extensive comparison with diverse datasets may limit the applicability and robustness of MaskDP in practical scenarios. Overall Evaluation: The paper presents a wellstructured and detailed investigation of Masked Decision Prediction (MaskDP) for reinforcement learning showcasing its effectiveness in pretraining and its performance in various downstream tasks. The study offers valuable insights into the scalability transferability and generalizability of MaskDP in the RL domain. Further exploration of interpretability scalability and realworld deployment scenarios could enhance the impact and applicability of the proposed method.", "rZalM6vZ2J": " Peer Review 1. Summary: The paper introduces a novel approach DPPCA for computing the principal component from i.i.d. data under (\u03b5 \u03b4)differential privacy overcoming limitations of existing methods. It proposes a singlepass algorithm based on private minibatch gradient ascent and private mean estimation. The algorithm achieves optimal error rates for subGaussian data providing significant improvements over existing private PCA algorithms. The theoretical analysis and empirical results demonstrate the effectiveness of DPPCA in achieving privacypreserving PCA. 2. Strengths and Weaknesses: Strengths:  The paper addresses a significant problem of ensuring privacy in PCA computations.  Introduces a novel and efficient algorithm DPPCA that overcomes existing limitations.  Theoretical analysis is comprehensive providing rigorous proofs and lower bounds.  Empirical results demonstrate the effectiveness of DPPCA in achieving optimal error rates for subGaussian data. Weaknesses:  The paper is highly theoretical and practical implementation details could be further elaborated.  The complexity of the algorithms and assumptions involved may limit the immediate applicability in realworld scenarios.  The analysis heavily relies on specific assumptions such as subGaussian data which may not always hold in practical datasets. 3. Questions:  How does DPPCA perform compared to existing stateoftheart nonprivate PCA algorithms concerning computational complexity and runtime  Are there any implications or tradeoffs in adopting DPPCA in practical largescale data analysis settings considering its theoretical foundations  Can DPPCA be extended to handle more complex scenarios or different privacy constraints beyond (\u03b5 \u03b4)differential privacy 4. Limitations:  The focus on subGaussian data assumptions might limit the generalizability of DPPCA to datasets with nonGaussian distributions.  The algorithmic implementation details and scalability aspects in realworld applications are not extensively discussed.  Practical considerations such as robustness to outliers or noisy data are not explicitly addressed in the current analysis. Overall the paper presents an innovative solution to the privacypreserving PCA problem offering significant improvements over existing methods. A more practical perspective and extensive empirical validations could further enhance the understandability and applicability of the proposed DPPCA algorithm. Considerations about the impact of assumptions and limitations on realworld adoption are essential for future research in this area.", "lUyAaz-iA4u": " Summary The paper discusses the limitations of stochastic gradient descent with stochastic Polyak stepsize (SPS) introduced by Loizou et al. The authors propose new variants of SPS to overcome these limitations particularly focusing on the nonoverparameterized regimes. They introduce a modified version called DecSPS which addresses the drawbacks of the original SPS providing solutions without needing knowledge of optimal minibatch losses and guaranteeing convergence to the exact minimizer. The paper explores the dynamics and convergence properties of SGD equipped with these new variants showcasing their efficacy in various settings.  Strengths and Weaknesses Strengths: 1. The paper addresses an important issue in stochastic optimization and proposes practical modifications to enhance convergence. 2. The experiments provide empirical evidence of the efficacy of the proposed DecSPS compared to existing methods. 3. The theoretical analysis is detailed and provides insights into the behavior of the algorithm in different scenarios. Weaknesses: 1. The paper assumes strong convexity which may limit the applicability of the proposed algorithm to a wider range of optimization problems. 2. The experimental evaluation could benefit from a more comprehensive comparison with a broader range of optimization methods to showcase the superiority of DecSPS more convincingly. 3. The paper lacks a detailed discussion on the computational complexity and practical implementation aspects of the proposed algorithm.  Questions 1. How sensitive is the performance of DecSPS to the hyperparameters like the upper bound `b` and the scaling constant `c0` 2. Given that strong convexity assumptions are made how practical is it to extend the proposed algorithm to nonconvex and possibly nonsmooth optimization problems 3. Can the proposed DecSPS algorithm handle largescale optimization tasks efficiently considering factors like memory usage and computational burden  Limitations The study heavily relies on strong convexity assumptions limiting its applicability to a broader class of optimization problems that may not exhibit such properties. Additionally the experiments lack a comprehensive comparison with stateoftheart optimization methods across a diverse set of datasets and problem domains. The focus on theoretical analysis and strong convexity constraints may restrict the generalizability of the proposed DecSPS algorithm to realworld scenarios where such assumptions may not hold. Further investigations on the scalability and robustness of DecSPS in complex optimization landscapes are essential to validate its practical utility.", "wO53HILzu65": " Peer Review:  1) Summary: The paper provides an insightful investigation into the challenges of selecting the right algorithm and hyperparameters for recommender systems. It introduces RecZilla a metalearning approach that predicts the best algorithm and hyperparameters for new datasets. The study compares 24 algorithms across 85 datasets and 315 metrics to demonstrate the dependency of algorithm performance on dataset specifics. The RecZilla pipeline is shown to effectively reduce human involvement in selecting algorithms for recommender systems.  2) Strengths and Weaknesses: Strengths:  The study conducts a comprehensive comparison of 24 algorithms across a large number of datasets and performance metrics.  RecZillas metalearning approach shows promise in predicting highperforming algorithms and hyperparameters for new datasets.  The release of code pretrained models and raw experimental results promotes reproducibility and further research in the field.  The paper addresses an important challenge in recommender system research and offers a potential solution through automated algorithm selection. Weaknesses:  The study acknowledges the limitations of being in a lowdata regime suggesting room for improvement with more training data.  The direct prediction of hyperparameters is identified as a limitation although this can be a focus for future work.  The largescale evaluation conducted may introduce biases due to the sheer volume of models trained and further attention to failure cases could be beneficial for addressing uncertainties.  3) Questions:  How does RecZilla account for outliers or anomalies in dataset metafeatures that might impact the algorithm selection process  Can the study provide insights into the scalability of RecZilla when applied to extremely large datasets with high dimensionality  How does RecZilla handle cases where multiple algorithms perform equally well on a given dataset based on metafeatures  4) Limitations: The study acknowledges several limitations including:  The lowdata nature of the metalearning problem due to the limited number of datasets considered.  The need for continuous improvement by including new datasets and algorithms in RecZillas repertoire.  The potential biases in the metadataset due to the sheer volume of experiments conducted which may impact the generalizability of results. Overall the paper presents a wellstructured investigation into algorithm selection for recommender systems with RecZilla showing promise as an automated solution. Addressing the identified limitations through further research and refinements could enhance the impact and applicability of RecZilla in realworld recommender system applications.", "lC5-Ty_0FiN": " Peer Review: 1) Summary: The paper investigates the phenomenon of \"benign overfitting\" in deep neural networks (DNNs) by studying the effects of wide last hidden layer representations in various stateoftheart convolutional neural networks. The authors find that in wide DNNs trained with regularization redundant neurons appear in the last hidden layer leading to improved generalization performance. Through experiments and analyses they show that redundant neurons can be viewed as independent estimators of the same features resulting in a decay of test error with the network width suggesting a mechanism for benign overfitting. 2) Strengths:  The paper addresses an important and challenging problem in deep learning.  The experimental design and analyses are rigorous and detailed.  The findings provide valuable insights into the beneficial effects of wide representations in DNNs and offer a potential explanation for the observed phenomena of benign overfitting.  The inclusion of code and data for reproducibility enhances the credibility of the study.  The discussion of the implications for model quality and robustness is insightful. Weaknesses:  The paper could benefit from a clearer structure as the content is dense and complex.  Some parts of the paper particularly the methods section may be challenging for readers not wellversed in deep learning and neural networks.  While the paper is thorough in its analysis the significance of the findings could be more explicitly highlighted.  The link between the proposed mechanism and practical applications or realworld impact could be further emphasized. 3) Questions:  Could the authors elaborate on the potential realworld applications or implications of the findings beyond the theoretical insights provided  How does the proposed mechanism for benign overfitting in wide DNNs relate to existing theories on model generalization and neural network training dynamics  Are there specific types of tasks or datasets for which the observed phenomenon of benign overfitting through redundant neurons is particularly advantageous or relevant 4) Limitations:  The study predominantly focuses on theoretical analysis and empirical observations in controlled settings. Generalizing the findings to more diverse or complex scenarios may require further validation.  The definition and characterization of \"clones\" in the context of redundant neuronal representations could potentially vary based on the specific network architectures or learning tasks.  The extent to which the proposed mechanism of benign overfitting via redundant neurons can be extended to different types of neural networks or applications remains to be explored. In conclusion the paper presents a detailed investigation into the phenomenon of \"benign overfitting\" in deep neural networks through the lens of redundant neuronal representations in wide networks. The findings contribute to our understanding of the biasvariance tradeoff and generalization in DNNs though further exploration and clarification on practical implications and broader implications could enhance the impact of the study.", "pn5trhFskOt": " Peer Review  1) Summary: The paper focuses on the problem of audiovisual source localization highlighting the challenges of overfitting in weaklysupervised methods and the lack of effective evaluation metrics for negative samples. The authors propose a novel approach Simultaneous Localization and AudioVisual Correspondence (SLAVC) that addresses these issues by leveraging extreme visual dropout and momentum encoders. By extending evaluation protocols to include negative samples the paper provides a comprehensive analysis of prior methods and demonstrates the superior performance of SLAVC on benchmark datasets. Overall the paper presents a thorough investigation into the shortcomings of existing methods and introduces a promising solution.  2) Strengths and weaknesses:  Strengths:  Clear identification of existing weaknesses in weaklysupervised audiovisual source localization methods.  Proposal of a novel approach SLAVC that effectively combats overfitting and improves performance.  Comprehensive evaluation of prior methods and the introduction of a new evaluation protocol.  Extensive experiments and detailed analysis to support the proposed approach.  Weaknesses:  The evaluation on a limited set of benchmark datasets may restrict the generalizability of the proposed method.  A more indepth discussion on the limitations and potential failure cases of SLAVC could further enhance the paper.  3) Questions:  How does the proposed SLAVC approach handle complex sound source scenarios such as overlapping or multiple sources in the same video  Could the authors elaborate on the computational efficiency and scalability of SLAVC especially when deployed on larger datasets  In what practical scenarios or applications could SLAVC have significant realworld impact  4) Limitations:  The papers reliance on certain benchmark datasets may limit the applicability of SLAVC to broader audiovisual source localization scenarios.  The evaluation metrics while addressing the need for negative sample assessment could still benefit from further validation in diverse settings.  The generalizability of SLAVC to realworld scenarios beyond the evaluated benchmarks remains to be explored. In conclusion the paper offers valuable insights into the challenges of audiovisual source localization and introduces a promising solution in the form of SLAVC. Further experiments and realworld applications could provide a more comprehensive understanding of the methods effectiveness. Further revisions addressing the limitations and potential extensions of SLAVC could strengthen the overall impact of the research.", "u5oLvX8x4wH": "Peer Review: 1) Summary: The paper focuses on studying the convergence of stochastic gradient descent (SGD) based optimization algorithms specifically the lastiterate convergence of momentumbased SGD (mSGD) under relaxed conditions on step sizes. The authors relax the requirement of the decay on step size and provide theoretical analysis on the performance of mSGD under different step size settings. Experimental results on regression and classification tasks validate the theoretical findings demonstrating faster convergence with larger step sizes. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important topic in the field of machine learning optimization.  Theoretical analysis is rigorous and wellsupported.  Clear and structured presentation of the study with detailed derivations and theorems.  Experimental validation provides empirical support for the theoretical findings. Weaknesses:  The paper lacks a comparison with existing stateoftheart methods for convergence analysis.  The experiments could be extended to more diverse datasets to enhance the generalizability of the findings.  The paper could benefit from more detailed discussions on the practical implications of the results for realworld optimization tasks. 3) Questions:  How do the findings of the relaxed step size conditions in mSGD compare with other similar optimization algorithms in terms of convergence speed and computational efficiency  Are there any specific scenarios or types of problems where the proposed relaxed step size conditions would be particularly beneficial or detrimental  How would the theoretical results change if different types of loss functions or neural network architectures were considered in the experiments 4) Limitations:  The experimental section focuses on relatively simple regression and classification tasks limiting the generalizability of the results.  The study lacks a comprehensive discussion on the potential limitations and challenges of applying the proposed relaxed step size conditions in realworld machine learning applications.  The conclusions drawn from the experiments should be cautiously interpreted due to the limited scope and depth of the experimental validations. Overall the paper provides valuable insights into the convergence properties of mSGD under relaxed step size conditions. Further exploration and validation in more diverse and complex scenarios would enhance the practical relevance and impact of the findings.", "sipwrPCrIS": " Peer Review of \"Feature Learning in InfiniteWidth Neural Networks: A SelfConsistent Dynamical Mean Field Theory Approach\"  Summary: The paper presents a novel approach utilizing selfconsistent dynamical mean field theory to study feature learning in infinitewidth neural networks. The authors develop a path integral formulation to describe the dynamics of network training and provide a numerical procedure to solve the resulting selfconsistent equations. The theory is compared to various approximation schemes and experiments on CIFAR classification tasks are conducted to validate the approach.  Strengths: 1. Novel Approach: The utilization of selfconsistent dynamical mean field theory to study feature learning in neural networks is a novel and promising approach. 2. Theoretical Rigor: The paper presents a detailed theoretical framework supported by mathematical derivations and analytical derivations of selfconsistent equations. 3. Comparison and Validation: Comparisons with various approximation schemes and experimental results on CIFAR classification tasks enhance the credibility of the proposed approach. 4. Methodological Contributions: The development of a computational procedure to solve the selfconsistent equations and the extension to deep linear networks provide valuable contributions to the field.  Weaknesses: 1. Complexity: The theoretical framework and derivations presented in the paper are highly complex and may be challenging for readers not wellversed in the field of neural networks and dynamical systems. 2. Limited Practical Applications: While the theoretical insights are valuable the practical implications of the proposed selfconsistent dynamical mean field theory for realworld applications are not extensively discussed.  Questions: 1. Applicability: How transferrable is the proposed selfconsistent dynamical mean field theory approach to practical deep learning scenarios beyond the theoretical framework presented in the paper 2. Generalization: How well does the theory generalize to different types of datasets architectures or applications outside the scope of the CIFAR classification tasks 3. Scalability: What are the computational challenges associated with applying this approach to larger datasets or more complex network architectures 4. Comparison Metrics: Are there specific quantitative metrics used to compare the performance of the proposed approach with other approximation schemes or traditional training methods 5. Impact: What are the potential implications of this research on the field of deep learning theory and practice and are there specific areas where this approach could have significant impact  Limitations: 1. Heuristic Techniques: The reliance on heuristic physics techniques in the methodology could introduce uncertainties and limitations in the obtained results. 2. Computational Complexity: The computational requirements particularly the time complexity of the proposed numerical procedure could limit its practical applicability to larger datasets or networks. 3. Theoretical Boundaries: The restriction of the method to certain asymptotic regimes and the need for further exploration of its application in diverse scenarios are notable limitations. Overall the paper makes significant theoretical contributions to the understanding of feature learning in neural networks. Addressing the questions raised and potential limitations could further enhance the clarity applicability and impact of the proposed approach in the field of deep learning research.  End of Review.", "yKDKNzjHg8N": " Peer Review  1) Summary The paper introduces a new metric SecondSplit Forgetting Time (SSFT) that characterizes hard examples by tracking how neural networks forget examples during finetuning on a second dataset split. The study demonstrates that SSFT effectively identifies mislabeled examples and distinguishes between different types of hard examples such as mislabeled rare and complex. The research showcases the benefits of using SSFT in noisy data settings dataset cleansing and identifying failure modes. The paper incorporates experimental results across various datasets theoretical analyses and practical implications for machine learning models.  2) Strengths and weaknesses Strengths:  Introduces a novel metric SSFT for characterizing example hardness in neural networks.  Comprehensive experimental study across multiple datasets and modalities.  Theoretical analysis provides insights into why certain examples are forgotten.  Provides practical applications for noisy data and model improvement.  The study showcases the complementarity of firstsplit and secondsplit metrics. Weaknesses:  Theoretical explanations could be further elaborated for better clarity.  Certain ablation experiments and theoretical results are described informally more formal details would enhance credibility.  Limited comparison with existing stateoftheart methods for label noise detection.  Further discussion on the generalization of findings across different architectures could add value.  3) Questions  Could the study expand on the practical implementation challenges of using SSFT in realworld scenarios  How does the SSFT metric compare with other existing forgetting metrics in terms of robustness and applicability  Are there plans to evaluate SSFT on more diverse and complex datasets to further validate its effectiveness  Could the authors elaborate on potential extensions or modifications to the SSFT metric for broader applications in machine learning  4) Limitations  The limited comparison with existing stateoftheart methods for label noise detection might impact the studys completeness in realworld applicability.  Lack of detailed exploration of SSFTs performance across various neural network architectures and different learning rates.  The SSFT metrics limitations in scenarios involving highly complex or rapidly changing datasets could impact its utility in dynamic environments.  The absence of a direct comparison with other forgetting metrics might limit the studys holistic evaluation of SSFTs effectiveness. In conclusion the paper makes a significant contribution by introducing the SSFT metric and showcasing its potential in characterizing hard examples in neural networks. Further refinement and validation across diverse datasets and settings could enhance the utility of SSFT in practical machine learning applications. The theoretical analyses and practical implications presented provide a solid foundation for future research in the field of example hardness characterization.", "ypXcTtbBsnZ": " Peer Review 1. Summary The paper introduces UniCLIP a Unified framework for Contrastive LanguageImage Pretraining that integrates contrastive loss of both interdomain and intradomain pairs into a single universal space. The paper addresses the limitation of existing works where contrastive losses for different pairs were defined independently. UniCLIP outperforms previous visionlanguage pretraining methods on various downstream tasks by addressing inherent differences in similarity measures between interdomain and intradomain pairs. 2. Strengths  UniCLIP addresses the issue of independent contrastive loss definitions for interdomain and intradomain pairs providing a more unified framework for contrastive learning.  The three key components of UniCLIP  augmentationaware feature embedding MPNCE loss and domaindependent similarity measure  are wellexplained and shown to contribute significantly to the final performance.  Extensive experiments demonstrating the superiority of UniCLIP on various single and multimodal downstream tasks are included. 3. Weaknesses  The paper could provide more insight into the practical implications and potential challenges of implementing UniCLIP in realworld scenarios.  While experimental results are promising a more comprehensive comparison with a wider range of existing methods could enhance the papers validity.  Further clarity could be provided on how UniCLIP could be applied to other types of multimodal datasets beyond visionlanguage datasets. 4. Questions  How scalable is UniCLIP to larger more diverse datasets beyond the ones mentioned in the experiments  Can UniCLIP be adapted for realtime or online learning scenarios considering its potential resource requirements and computational efficiency  Have the authors considered the interpretability and explainability of the internal representations learned by UniCLIP 5. Limitations  The paper focuses primarily on visionlanguage multimodal datasets limiting the generalizability of UniCLIP to other types of multimodal data.  The experimental evaluations could benefit from more detailed comparisons with existing stateoftheart methods to showcase the relative strengths of UniCLIP.  There is a need for further investigation into the robustness and generalization capabilities of UniCLIP across different domains and applications.", "rQAJmrLmGC6": "Summary: The paper introduces a novel Twostream Universal Referring localization Network (TURN) for zeroresource sounding object localization bridging acoustic and visual modalities. It employs transfer learning and adaptive sampling to enhance knowledge transfer. TURN achieves competitive performance without training data. Experiments on benchmarks demonstrate feasibility and effectiveness. Strengths: The paper proposes an innovative approach for zeroresource sounding object localization addressing the lack of training data with transfer learning. TURN shows competitive performance on benchmarks without data in the field validating the proposed mechanisms. The adaptive sampling strategy enhances knowledge transfer flexibility. The thorough experiments on various datasets validate the models effectiveness. The ablation studies demonstrate the importance of different components in TURN. Weaknesses: While the proposed TURN model shows promising results the paper could benefit from a clearer explanation of the model architecture and training process. Providing more insights into the limitations and generalizability of the proposed approach could further strengthen the paper. Moreover a discussion on the computational efficiency and scalability of TURN would be valuable for practical application considerations. Questions: 1. How does the proposed TURN model compare to existing models in terms of computational complexity and inference speed 2. Can the adaptive sampling strategy be further optimized to enhance the performance of TURN on datasets with different characteristics 3. How does TURN handle challenges such as dataset bias and noise especially when scaling to larger and more diverse datasets 4. Are there any plans to extend the proposed TURN framework to address other related tasks or modalities in future research Limitations: One of the limitations of the paper is the lack of detailed discussion on the realworld applications of TURN and the potential challenges in deploying the model in practical scenarios. Additionally the paper could provide more insights into the transferability of the proposed model across various domains and datasets.", "n7XbkHOwKn6": "Peer Review Summary: The paper presents CogVideo a largescale pretrained transformer for texttovideo generation tackling challenges faced by previous models. CogVideo leverages a multiframerate hierarchical training strategy and incorporates a dualchannel attention mechanism. Experimental results demonstrate the superiority of CogVideo over existing models in both machine and human evaluations. Strengths: 1. Addressed a significant gap in texttovideo generation and proposed novel solutions to improve performance. 2. Extensive experiments on popular benchmarks demonstrate the effectiveness of CogVideo in generating highquality videos. 3. The incorporation of CogView2s weights and the hierarchical multiframerate generation strategy show clear advantages in terms of model performance. 4. The introduction of dualchannel attention enables better utilization of pretrained texttoimage models. Weaknesses: 1. Lack of clarity in reporting specific training details such as random seed handling total compute used and storage infrastructure. 2. Absence of code data and instructions needed for reproducibility limits the transparency and reproducibility of the research. 3. While the paper mentions potential societal impacts a more indepth discussion on ethical considerations and potential negative implications of the work could strengthen the study. 4. The limitations section could be expanded to discuss further constraints or challenges faced in implementing CogVideo. Questions: 1. Could the authors elaborate on the specific steps taken to mitigate potential risks associated with misinformation that could arise from using CogVideo for creating deepfake videos 2. How does CogVideo handle cases of bias in texttovideo generation particularly in scenarios where the underlying dataset might be skewed or lack diversity 3. Could the authors provide insights into the computational requirements for training and inference using CogVideo particularly in terms of time memory and energy consumption Limitations: 1. The lack of released code data and detailed instructions for reproducing the experimental results hinders the overall transparency and replicability of the study. 2. The absence of error bars in the reported results may raise concerns about the statistical robustness of the findings. 3. The discussion on potential negative societal impacts could be more comprehensive addressing implications beyond misinformation. Overall the paper presents a valuable contribution to the field of texttovideo generation showcasing innovative approaches to improve model performance. Enhancing transparency reproducibility and ethical considerations would further strengthen the impact and credibility of the research.", "yam42JWePu": " Peer Review Paper Title: LOUPE: Learning Finegrained Semantically Aligned VisionLanguage Pretraining Authors: Anonymous JournalConference: 36th Conference on Neural Information Processing Systems (NeurIPS 2022)   Summary: The paper introduces LOUPE a framework for finegrained semantically aligned visionlanguage pretraining. LOUPE approaches the problem from a gametheoretic perspective modeling interactions between visual regions and textual phrases. The proposed uncertaintyaware neural Shapley interaction learning module efficiently computes game interactions. Experimental results demonstrate that LOUPE achieves stateoftheart performance on multiple visionlanguage tasks including object detection and visual grounding without human annotations.  Strengths: 1. Novel Approach: The paper proposes a novel framework for finegrained semantic alignment in visionlanguage tasks using gametheoretic interactions which is a unique and promising perspective. 2. Efficiency: The integration of an uncertaintyaware neural Shapley interaction learning module provides an efficient strategy to compute interactions reducing computational costs while maintaining accuracy. 3. Experimental Validation: The paper provides comprehensive experiments demonstrating the effectiveness of LOUPE on various downstream tasks showcasing its superior performance compared to existing methods.  Weaknesses: 1. Lack of Comparisons: While the paper extensively evaluates LOUPEs performance on different tasks a more detailed comparison with existing methods in terms of computational efficiency and general applicability could further strengthen the contribution. 2. Data Quality Concerns: It is mentioned that the training data includes pairs from the Internet which may contain noise or inappropriate content. Further analysis or strategies to mitigate potential privacy or ethical concerns should be discussed.  Questions: 1. How does the proposed uncertaintyaware neural Shapley interaction learning module handle potential noise or biases in the data during training 2. Is there a scalability issue with the proposed LOUPE framework when applied to larger datasets or more complex visionlanguage tasks  Limitations: 1. Dataset Quality: The usage of Internet data raises concerns about data quality and bias. Addressing and discussing potential biases in the dataset would enhance the papers transparency. 2. Generalization: The robustness and generalization of LOUPE across diverse datasets and domains should be further explored to validate its effectiveness in realworld applications.  Conclusion: The paper presents a novel framework LOUPE for finegrained semantic alignment in visionlanguage tasks. The proposed approach shows promising results and opens avenues for future research in the field of visionlanguage pretraining. However a more indepth analysis of potential biases in the training data and further exploration of the models scalability and generalization capabilities would make the paper more robust.  This peer review aims to provide constructive feedback to enhance the clarity impact and credibility of the research findings presented in the paper.", "wxWTyJtiJZ": " Peer Review of the Paper \"MultiplePurchasewithBudget UCB Algorithms for Revenue Maximization in Online Retailing Scenarios\" 1. Summary The paper introduces a novel approach to address the product ranking problem in online retailing focusing on multiplepurchase scenarios. The authors propose the MPBUCB algorithms designed to maximize revenue by modeling consumers behaviors effectively. Experimental results showcase the effectiveness of the proposed algorithms in both synthetic and semisynthetic datasets. 2. Strengths  Innovative Consumer Choice Model: The paper introduces a unique consumer choice model that considers multiple purchases attention span and purchase budget showcasing a step forward from previous research.  Algorithmic Contributions: The MPBUCB algorithms designed in the paper achieve O\\xcc\\x83( \\xe2\\x88\\x9a T ) regret balancing exploration and exploitation for revenue maximization.  Thorough Experiments: Extensive experiments on both synthetic and semisynthetic datasets validate the effectiveness of the proposed algorithms enhancing the credibility and applicability of the research. 3. Weaknesses  Complexity of Model: The proposed model involves several assumptions and parameters which might make it challenging to implement in realworld online retailing platforms.  Lack of RealWorld Validation: While the experiments on synthetic and semisynthetic datasets are thorough realworld validation of the proposed algorithms on actual online retailing data would strengthen the papers practical relevance.  Limited Comparison: The comparison with existing baselines could be further elaborated to provide deeper insights into the performance of the proposed algorithms. 4. Questions  How scalable are the MPBUCB algorithms proposed in the paper Can they handle larger datasets and realtime interactions in online retailing platforms  Could the authors provide more insights into the interpretability of the consumer choice model introduced in the paper How can retailers effectively leverage the models insights for decisionmaking  Is there any consideration of dynamic consumer behaviors or external factors that might impact consumers purchase decisions in the proposed model 5. Limitations  Generalizability: The proposed model and algorithms may have limitations in accommodating the dynamic nature of consumer behaviors and external factors which are prevalent in realworld online retail environments.  Complexity: The complexity of the model and algorithmic approach may limit adoption by practitioners in the online retail industry especially those lacking expertise in machine learning and algorithm design.  Data Dependency: The reliance on estimated parameters from datasets in the semisynthetic experiments raises concerns about the generalizability and robustness of the proposed approach in practical settings. Overall the paper presents a novel and promising approach to optimizing product ranking for revenue maximization in online retailing scenarios. Addressing the weaknesses and limitations highlighted could further enhance the impact and applicability of the proposed algorithms in realworld settings.", "v7SFDrS44Cf": " Peer Review  Summary The paper proposes FLEXSUBNET a family of flexible neural models for submodular and \u03b1submodular functions. It introduces a novel permutation adversarial training method for differentiable subset selection and performs experiments on both synthetic and real datasets to demonstrate the superiority of FLEXSUBNET over several baselines.  Strengths  The paper tackles the problem of learning submodular and \u03b1submodular functions using neural networks providing a novel and flexible approach.  Introduces a unique permutation adversarial training method for subset selection overcoming the issues of sensitivity to permutations in training.  The experiments conducted on synthetic and real datasets provide evidence of the efficacy of FLEXSUBNET compared to stateoftheart methods.  The paper is wellstructured clearly explaining the methodology models and evaluation protocols.  Weaknesses  Though the methodology is sound the complexity of the models and training techniques might limit the practical adoption in scenarios where computational resources are constrained.  The experiments could benefit from more detailed analyses such as parameter sensitivity or optimization strategy comparisons for a deeper understanding of the proposed method.  The practical implications and scalability of the approach in largescale or realtime scenarios could be addressed for a more comprehensive evaluation.  Questions 1. How does the proposed FLEXSUBNET model scale with respect to the number of elements in the universal set for training and inference 2. Can the paper provide insights into the interpretability of the learned neural models for submodular functions to enhance understanding and trust in the results 3. Were there any particular challenges encountered in the training process due to the integration procedures or the differentiable nature of the neural models used 4. How sensitive is the performance of FLEXSUBNET to hyperparameter choices and were there any strategies employed to ensure robustness in hyperparameter tuning 5. Considering the tradeoffs between model complexity expressiveness and computational efficiency how does FLEXSUBNET fare in scenarios with limited computational resources  Limitations  The paper focuses on synthetic and limited real datasets potentially limiting the generalizability of the results to broader applications.  The computational demands of the proposed method especially in training with adversarial permutations could present scalability challenges in resourceconstrained environments.  The lack of a detailed analysis of the interpretability and explainability of the neural models may raise concerns about the blackbox nature of the approach. Overall the paper presents an innovative approach to learning submodular functions using neural networks offering significant advancements in the field. Further exploration into scalability interpretability and efficiency considerations could enhance the impact and adoption of the proposed FLEXSUBNET model.", "vDeh2yxTvuh": " Review of the Scientific Paper: 1) Summary: The paper presents a thorough investigation and comparison of two popular flatminima optimizers Stochastic Weight Averaging (SWA) and SharpnessAware Minimization (SAM) which aim to improve the generalization performance of deep neural networks. The research fills the gap in understanding these optimizers by analyzing their properties loss surfaces and benchmarking across various domains including computer vision natural language processing and graph representation learning tasks. The findings offer insights into the geometric properties performance and effectiveness of SWA and SAM in comparison to traditional optimizers. 2) Strengths and weaknesses: Strengths:  Comprehensive investigation of SWA and SAM providing insights into their properties and effectiveness.  Detailed analysis of the loss landscapes geometric properties and generalization performance across different domains.  Benchmarking across diverse tasks in computer vision NLP and graph representation learning.  Systematic comparison and rigorous evaluation of SWA and SAM filling the gap in understanding these optimizers.  Clear and detailed descriptions of methodologies and findings with references to related works. Weaknesses:  The paper could benefit from a clearer structure with more defined subsections to aid readability.  Some sections are quite technical and may require a strong background in deep learning optimization to fully understand.  Limited discussion on potential future directions or implications of the results beyond the findings presented. 3) Questions:  How do the findings related to the loss landscapes and geometric properties of SWA and SAM contribute to the broader understanding of deep learning optimization  In what ways could the methodology be improved or expanded to further enhance the analysis and comparison of SWA and SAM across different domains  Are there specific areas or tasks within computer vision NLP or graph representation learning where SWA or SAM showed significant performance gains or limitations that could be explored further 4) Limitations:  The study relies heavily on existing hyperparameter values from prior works potentially limiting the optimization and performance of the flatminima optimizers.  While a broad range of tasks are considered there may still be unexplored domains or sensitive areas where flatminima optimizers could be studied.  The effectiveness of SWA and SAM may be impacted by the fixed hyperparameters shared across all methods suggesting a need for more tailored optimization strategies.  Some technical sections may be challenging to grasp for researchers without a deep understanding of neural network optimization. Overall the paper presents a valuable contribution to the understanding and comparison of SWA and SAM offering insights into their performance and properties across varied tasks in deep learning optimization. Further exploration and refinement of the methodologies used and potential extensions into new areas of study may enhance the impact and relevance of the findings presented.", "pF5aR69c9c": " Summary The paper introduces MemoryConstrained Policy Optimization (MCPO) a constrained optimization method for policy gradient reinforcement learning. MCPO utilizes two trust regions: one is the old policy and the other is a virtual policy representing a memory of past policies. The virtual policy is dynamically learned through an attention network allowing for more efficient learning in diverse environments such as robotic locomotion control navigation with sparse rewards and Atari games.  Strengths 1. Innovative Approach: MCPO introduces a novel method of utilizing a virtual policy to regulate policy updates addressing the issue of suboptimal old policies common in trustregion optimization. 2. Performance: The experiments demonstrate competitive performance of MCPO across a diverse set of environments compared to recent onpolicy constrained policy gradient methods. 3. Attention Mechanism: The attention network to dynamically learn relevant past policies adds a new dimension to trustregion policy optimization.  Weaknesses 1. Complexity: The method introduces additional hyperparameters and components that may require extensive tuning potentially affecting ease of implementation and generalization to new domains. 2. Compute Requirements: The computational demands of MCPO including the attention mechanism and dynamic virtual policy may limit scalability to largescale applications. 3. Evaluation: While the paper discusses various experiments a deeper analysis of the limitations and failure cases of MCPO in comparison to other methods could provide a more comprehensive evaluation.  Questions 1. How does the proposed method handle environments with rapidly changing dynamics or nonstationarity where historical policies may not be useful 2. Could the memory size N affect the robustness of MCPO in learning tasks with varying complexities 3. How sensitive is MCPO to the choice of initial policy and are there strategies to mitigate the impact of suboptimal starting policies 4. Can MCPO be adapted to incorporate domainspecific knowledge or constraints to enhance learning efficiency further  Limitations 1. The generalizability of MCPO to realworld applications beyond simulation environments may be limited without extensive testing and adaptation. 2. The performance improvements demonstrated in the experiments may not fully capture the practical challenges and nuances of realworld reinforcement learning tasks. 3. The paper acknowledges the potential misuse of the method for unethical purposes signaling a need for responsible AI development and deployment practices beyond the research scope.  Conclusion Overall the MemoryConstrained Policy Optimization (MCPO) proposed in the paper shows promise in enhancing constrained policy gradient reinforcement learning through the introduction of novel mechanisms like the virtual policy and attention network. Further exploration and validation of MCPO in diverse and complex environments coupled with considerations for practical scalability and ethical implications would strengthen the utility of the method in the field.", "wfel7CjOYk": " Peer Review   Summary The paper proposes a novel mechanism AllInOne Neural Composition for resourceadaptive federated learning by enabling the construction of models at different complexities using a unified neural basis shared among clients. The proposed mechanism overcomes the challenges posed by system heterogeneity in federated learning allowing for efficient and effective training. Extensive experiments across various datasets and system setups demonstrate the superiority of the proposed approach over existing methods.  Strengths 1. Innovative Solution: The paper introduces a novel mechanism AllInOne Neural Composition to address the challenges of system heterogeneity in federated learning providing a creative approach to training complexityadjustable models. 2. Comprehensive Experiments: The authors conducted extensive experiments across different datasets and system setups to evaluate the proposed approach thoroughly demonstrating its effectiveness and superiority over existing methods. 3. Clarity and Structure: The paper is wellstructured with clear sections that explain the problem proposed solution method details experiments and related works. The inclusion of algorithms and figures aid in understanding the proposed mechanism.  Weaknesses 1. Evaluation Metrics: While the experiments show consistent performance gains a more detailed discussion on the choice of evaluation metrics and comparison with standard federated learning metrics could enhance the papers impact. 2. Realworld Applicability: Further discussion on the practical applicability scalability and potential limitations of implementing the proposed mechanism in realworld federated learning scenarios would provide valuable insights.  Questions 1. Scalability: How does the proposed mechanism scale with an increasing number of clients and varying degrees of system heterogeneity 2. Robustness: Have the authors considered scenarios where clients may have fluctuating resource capacities during training and how does the proposed mechanism handle such dynamics  Limitations 1. Generalizability: The paper focuses on specific experiments with vision and language tasks. Discussing the generalization of the proposed mechanism to other domains or tasks could strengthen the papers applicability. 2. Complexity: The complexity of the AllInOne Neural Composition method may pose challenges in practical implementation. Addressing the computational overhead and training stability concerns could be beneficial.  Overall the paper presents a novel mechanism for resourceadaptive federated learning showcasing promising results in addressing system heterogeneity challenges. Further exploration of scalability aspects robustness under dynamic conditions and practical implications could enhance the papers contribution to the field of federated learning.", "n3lr7GdcbyD": " Peer Review  1) Summary: The paper introduces a novel variant of the MonteiroSvaiter acceleration framework for convex optimization which eliminates the need to solve an implicit equation at each iteration. By developing an MS subproblem solver that does not require problem parameter knowledge the paper achieves complexity improvements matching a lower bound for Lipschitz pth derivative optimization. Experimental results show outperformance of previous acceleration schemes on logistic regression albeit underperformance when compared to Newtons method. Theoretical and empirical comparisons demonstrate the effectiveness and limitations of the proposed algorithms.  2) Strengths and Weaknesses: Strengths:  Rigorous theoretical framework and complexity analysis.  Development of a novel acceleration method with practical applicability.  Comparison with existing methods and insightful experiments.  Adaptive nature of the proposed oracle implementations.  Clear presentation and detailed algorithmic descriptions. Weaknesses:  The paper could enhance clarity by defining certain terms explicitly.  The discussion on the empirical results could be more extensive.  Further justification for the adaptability of the methods could be beneficial.  Incorporating visual aids to explain complex concepts could enhance understandability.  4) Questions:  Could more insights be provided on the adaptability and generalizability of the proposed algorithms across different optimization problems  How do the proposed methods perform when applied to realworld largescale datasets  Are there any inherent properties of the logistic regression problem that make Newtons method significantly more efficient than the proposed algorithms  5) Limitations:  The limitations mainly lie in the clarity of certain sections and the depth of empirical analysis.  The extent to which the proposed methods generalize to diverse problem domains and datasets remains unclear.  The paper could benefit from a discussion on the reproducibility of the experimental results to ensure the robustness of the findings. Overall the paper presents a promising advancement in optimization algorithms providing a novel approach to acceleration with detailed theoretical foundations. Further validation and exploration of the practical implications and scalability on diverse datasets would strengthen the impact of the research contribution.", "pBz3h8VibKY": " Summary The paper introduces a new algorithm ActionInclusive InteractionGrounded Learning (AIIGL) to address the limitations of existing approaches in scenarios where the feedback vector contains information about the action. AIIGL treats the latent reward as a latent state and employs a contrastive learning approach to learn a nearoptimal policy under the assumption of weaker conditional independence. The algorithm is evaluated using largescale experiments on OpenML supervised datasets and a simulated BCI experiment with real human fMRI data demonstrating its effectiveness in expanding the applicability of InteractionGrounded Learning.  Strengths and Weaknesses Strengths: 1. Introduces a novel algorithm AIIGL to handle scenarios where feedback contains information about the action. 2. Theoretical guarantees provided for learning a nearoptimal policy under weaker conditional independence assumptions. 3. Empirical evaluations on a diverse set of datasets and a simulated BCI experiment with real human fMRI data demonstrate the effectiveness of the proposed approach. 4. Addresses the challenge of identifying the semantic meaning of the decoded class with a symmetrybreaking procedure. Weaknesses: 1. The paper focuses on binary latent rewards extending to more general latent reward spaces may be challenging. 2. Potential instability in performance especially in risksensitive applications could be a concern. 3. The symmetrybreaking procedure is currently based on binary latent rewards and one policy per action broader applications may require more sophisticated solutions.  Questions 1. How does the performance of AIIGL compare to existing methods in complex tasks with highdimensional latent reward spaces 2. Can the approach handle dynamic feedback environments where the relationship between action and feedback may change over time 3. How scalable is the AIIGL algorithm in terms of computational resources and training time 4. Are there any practical limitations or challenges in implementing AIIGL in realworld interactive learning scenarios  Limitations 1. The proposed algorithm focuses on binary latent rewards and may not generalize well to higherdimensional latent reward spaces. 2. The symmetrybreaking procedure is limited to one policy per action and binary rewards potentially restricting its applicability in more complex scenarios. 3. The performance stability of AIIGL especially in sensitive applications remains a concern and needs further investigation. Overall the paper introduces a promising approach in the field of InteractionGrounded Learning addressing key limitations and opening up possibilities for broader applications. Further research and experimentation will be crucial to validate the effectiveness and scalability of the AIIGL algorithm.", "odOQU9PYrkD": " Review of \"FactorBased Structuring of Uncertainty in Stochastic Segmentation Networks\"  Summary: The paper introduces a novel approach to structuring uncertainty in stochastic segmentation networks (SSNs) by breaking down predicted uncertainty into smaller components called factors. The key idea is to use factor rotations to control and finetune segmentations by manipulating individual uncertainty components. This is achieved through factorspecific flow probabilities which quantify deviations from the mean prediction and aid uncertainty visualization. Experimental results demonstrate the effectiveness of rotation criteria based on factorspecific flow probabilities in achieving finegrained sample control.  Strengths: 1. Innovative Approach: The paper proposes a novel method of leveraging factor rotations and flow probabilities to provide finegrained sample control in SSNs. This approach addresses the need for structured uncertainty and better interpretability. 2. Clear Presentation: The paper is wellorganized with sections logically flowing from the problem statement to the proposed solution and experimental validation. The explanation of key concepts like factor rotations and flow probabilities is detailed and coherent. 3. Experimental Validation: The experiments conducted using various datasets demonstrate the efficacy of the proposed method showing improvements in factor relevance separability and sparsity. The comparison against classic rotation criteria provides valuable insights into the effectiveness of the new approach. 4. Potential Impact: The application of this method extends beyond SSNs and could have implications for image classification and latent space modeling in other domains. The use of factor models and flow probabilities for uncertainty quantification is a significant contribution to the field of computer vision.  Weaknesses: 1. Computational Complexity: The paper notes that the computation of flowprobability based rotations may be timeconsuming limiting the interactive utility of the proposed method. Addressing this limitation to ensure realtime usability could be beneficial.  Questions: 1. While the paper explains the theoretical underpinnings of factor rotations and flow probabilities can the authors provide more insights into the practical implications of these approaches in realworld applications 2. How generalizable are the findings and conclusions of this study beyond the specific domains of medical imaging earth observation and traffic scenes Have the authors considered applications in other fields 3. Are there any considerations or tradeoffs to be made in choosing the most appropriate rotation criteria based on different datasets and segmentation tasks  Limitations: 1. Limited Dataset Representation: Although experiments span multiple domains the findings are based on a select set of datasets. Including a more diverse range of datasets could enhance the generalizability and robustness of the proposed method. 2. Interactive Use Case: The potential delay in computing flowprobability based rotations may hinder the practicality of realtime interactive use. Finding ways to optimize this process could help overcome this limitation. 3. Evaluation Metrics: While the paper provides a comprehensive evaluation of factor relevance separability and sparsity additional metrics or benchmarks could further validate the effectiveness and efficiency of the method. Overall the paper presents a novel and promising approach to structuring uncertainty in stochastic segmentation networks. Addressing the identified limitations and expanding the scope of the study could enhance the impact and applicability of the proposed method.", "yJEUDfzsTX7": "Peer Review 1) Summary: The paper introduces a novel theoretical framework for risksensitive reinforcement learning focusing on objectives that account for tail outcomes. Specifically the paper presents regret bounds for risksensitive reinforcement learning under a general class of risksensitive objectives such as CVaR. The proposed approach utilizes an optimistic MDP construction and upper confidence bound strategy to achieve regret bounds. Key results include the characterization of the CVaR objective and the discussion of regret bounds in the episodic setting. The paper also explores optimal policies for risksensitive objectives and provides insights into nonMarkov policies. Empirical evaluations on a frozen lake problem demonstrate the effectiveness of the proposed algorithm in minimizing cumulative regret. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important and emerging topic in reinforcement learning  risksensitive objectives.  The theoretical framework is wellstructured presenting novel results and insights into regret bounds for risksensitive objectives.  The proposed algorithm demonstrates promising performance in the empirical evaluations on the frozen lake problem.  The paper offers detailed explanations of the methodology and derivations making it accessible to the target audience. Weaknesses:  The paper could benefit from further clarification on how the proposed methods compare to existing stateoftheart algorithms in risksensitive reinforcement learning.  While the theoretical analyses are thorough the empirical validation is limited to a single frozen lake problem raising questions about the generalizability of the proposed approach to diverse environments and applications.  The paper lacks discussions on the scalability and computational complexity of the proposed algorithm which are crucial for realworld implementations. 3) Questions:  How does the proposed algorithm scale with the size of the problem space particularly in highdimensional environments  Can the bounds and theoretical results be extended to more complex scenarios beyond the episodic setting considered in the paper  How sensitive is the algorithms performance to hyperparameters and the choice of confidence level in the regret bounds 4) Limitations:  The study primarily focuses on theoretical developments and lacks an extensive empirical evaluation on diverse benchmarks.  The frozen lake problem used in the experiments may not fully capture the complexity of realworld applications in healthcare and robotics limiting the generalizability of the findings.  The discussion on the computational efficiency of the proposed algorithm and its practical feasibility in safetycritical applications is lacking. In conclusion the paper makes significant contributions to risksensitive reinforcement learning by providing regret bounds and insights into optimizing risksensitive objectives. However further empirical validations and discussions on scalability and computational aspects would enhance the completeness and applicability of the proposed framework.", "prKLyXwzIW": "Peer Review of Scientific Paper 1) Summary: The paper introduces a novel computationally efficient Generalized Method of Moments (GMM) estimator that is robust against a constant fraction of adversarially corrupted samples. The research extends the algorithmic robust statistics framework to GMM providing recovery guarantees and demonstrating improved performance compared to existing methods in instrumental variables linear regression with heterogeneous treatment effects. 2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem of developing a robust GMM estimator against corrupted samples.  The research introduces a novel algorithmic approach that combines GMM with robust statistics showing strong theoretical guarantees and practical improvements over existing methods.  The experimental results demonstrate the effectiveness of the proposed algorithm in handling corrupted data in instrumental variables regression. Weaknesses:  The paper is highly technical which might make it less accessible to readers without a strong background in algorithmic robust statistics.  The paper lacks a detailed comparison with a wider range of existing robust estimators for similar problems in econometrics and causal inference which could provide a more comprehensive evaluation of the proposed method. 3) Questions:  Can the proposed algorithm be generalized to handle other types of econometric models beyond instrumental variables regression  How sensitive is the algorithms performance to the choice of hyperparameters in practical applications  Are there any limitations or assumptions in the theoretical framework that may restrict the applicability of the algorithm in realworld scenarios 4) Limitations:  The paper focuses primarily on the development and theoretical analysis of the algorithm with fewer discussions on potential practical challenges and limitations in realworld applications.  The experiments are conducted on synthetic datasets and a single real dataset limiting the generalizability of the results to a broader range of practical situations. Overall the paper presents a significant contribution to the field of robust econometric inference by introducing a computationally efficient and robust GMM estimator. Further investigations into the algorithms applicability to diverse econometric models and realworld datasets would strengthen the research.", "u6p_NvZ23qt": " Summary The paper introduces a new generalization bound that controls test error for linear predictors in Gaussian space using Rademacher complexity and training error. It extends the optimistic rate for linear regression with square loss and handles model misspecification. The Moreau envelope theory is applied to noisy interpolation of maxmargin classifiers and spikedcovariance settings proving consistency results. The results hold for Lipschitz and smooth nonnegative losses and handle interpolation learning in highdimensional generalized linear model settings.  Strengths and weaknesses Strengths:  The paper introduces a novel generalization bound for linear predictors in Gaussian space providing insights into the mechanisms underlying generalization performance.  It addresses the issue of model misspecification extending the optimistic rate for linear regression with square loss.  The application of Moreau envelope theory to noisy interpolation and spikedcovariance settings demonstrates consistency results and expands understanding in these areas.  Results hold for a wide range of loss functions including Lipschitz and smooth nonnegative losses in highdimensional models. Weaknesses:  The application of Gaussian data assumption constrains the generality of the framework. Extending the results to nonGaussian distributions would strengthen the papers applicability.  The paper focuses on linear predictors in Gaussian space limiting its scope with respect to multiclass classification or twolayer networks.  Questions  How do the findings of this paper compare to existing theories on generalization and interpolation in machine learning models  Are there any plans to extend the framework to handle nonGaussian data distributions or explore applications in more complex neural network architectures  Can the results be generalized to scenarios beyond highdimensional linear predictors in Gaussian space such as more complex models or nonlinear setups  Limitations  The reliance on the Gaussian data assumption limits the generalizability of the results to nonGaussian distributions.  The focus on linear predictors in Gaussian space restricts the applicability of the findings to more diverse machine learning models and architectures.", "x2WTG5bV977": " Peer Review  Summary The paper investigates the use of transfer learning compared to ModelAgnostic MetaLearning (MAML) in the context of fewshot learning benchmarks. It introduces a novel metric the diversity coefficient to measure the task diversity in fewshot learning benchmarks and compares MAML and transfer learning under fair conditions using this metric. The study finds that in low task diversity scenarios MAML and transfer learning exhibit equivalent metatest performance. The authors suggest a problemcentric approach to metalearning inspired by Marrs levels of analysis emphasizing the importance of understanding the problem being solved. Through experiments on MiniImagenet and Cifarfs benchmarks the paper challenges the prevailing notion that sophisticated metalearning performs better than transfer learning in fewshot scenarios.  Strengths 1. Novel Metric and Methodology: The introduction of the diversity coefficient to assess task diversity in fewshot learning benchmarks is a novel contribution that provides a unique perspective on the effectiveness of metalearning algorithms. 2. Fair Comparison: Conducting the comparison between MAML and transfer learning under fair conditions with consistent architectures and optimization settings enhances the credibility of the study. 3. Clear Exposition: The paper is wellstructured with detailed explanations of concepts methodologies and experimental results aiding in understanding the research approach and findings.  Weaknesses 1. Missing Code and Data Availability: The absence of information regarding the availability of code data and instructions to reproduce the experimental results limits the reproducibility and transparency of the study. 2. Ethical Implications: While the paper discusses technical aspects comprehensively it lacks discussion on potential negative societal impacts of the research which could be an important aspect to consider. 3. Incomplete Checklist Items: Some checklist items related to code data and participant instructions are left partially answered which should be completed for clarity and completeness.  Questions 1. Are there plans to release the code data and instructions to reproduce the experimental results to ensure transparency and reproducibility 2. Can you elaborate on the potential negative societal impacts or ethical considerations of the research especially concerning the implications of different learning algorithms 3. Could the study benefit from further analysis or validation on additional datasets or benchmarks to corroborate the findings  Limitations 1. The lack of code data and instructions for reproducing the results limits the transparency and verifiability of the study. 2. The absence of ethical implications discussion might overlook potential societal impacts of the research. 3. The study focuses primarily on MiniImagenet and Cifarfs benchmarks potentially limiting the generalizability of the findings. Overall the paper provides a valuable perspective on the comparison between MAML and transfer learning in fewshot scenarios but addressing the mentioned weaknesses and limitations would enhance the credibility and comprehensiveness of the research.", "nLKkHwYP4Au": " Peer Review  1) Summary: The paper presents a novel twostage fully sparse convolutional 3D object detection framework named CAGroup3D. The proposed method focuses on generating highquality 3D proposals by leveraging a classaware local grouping strategy and a fully sparse convolutional RoI pooling module for proposal refinement. The model achieves stateoftheart 3D detection performance on ScanNet V2 and SUN RGBD datasets outperforming existing methods in terms of mAP0.25. The proposed framework demonstrates effectiveness in addressing the challenges of 3D object detection from unordered sparse and irregular point clouds.  2) Strengths and weaknesses:  Strengths:  Introduces a novel classaware 3D proposal generation strategy that considers semantic consistency within local groups.  Presents RoIConv pooling module for efficient 3D proposal refinement outperforming existing pooling strategies.  Achieves remarkable gains in 3D detection performance on challenging indoor datasets.  Memory and computationefficient design allows for better encoding of geometryspecific features in 3D proposals.  Weaknesses:  The paper lacks a thorough discussion on the broader impact and applications of the proposed framework in the field of 3D object detection.  The limitations and challenges of the proposed method are not extensively discussed leaving room for further exploration and improvement.  The experimental section could benefit from more detailed comparisons with existing methods on additional metrics and datasets to further validate the proposed approach.  3) Questions:  How does the proposed classaware local grouping module handle the scalability and generalizability of the model across different datasets and scenes  Can the RoIConv pooling module adapt to varying levels of complexity and density in 3D point clouds without compromising performance  What are the potential implications of the proposed methodology for realworld applications such as autonomous driving robotics and augmented reality  4) Limitations:  The paper is limited in discussing the complete range of challenges and limitations of the proposed method in practical deployment beyond the presented datasets.  The generalizability of the model to diverse indoor and outdoor environments as well as its robustness to noise and occlusion could be further explored.  The evaluation of the proposed framework could be strengthened by considering more diverse datasets and benchmarks to assess performance under varying conditions and scenarios. Overall the paper introduces a promising approach for 3D object detection with novel contributions in classaware proposal generation and efficient pooling strategies. Further analysis and experimentation will be valuable to validate the robustness and scalability of the proposed framework in realworld applications.", "wu1Za9dY1GY": " Peer Review: 1. Summary: The paper explores the connection between graph neural networks (GNNs) and dynamic programming (DP) by using methods from category theory and abstract algebra. The authors aim to demonstrate a deeper alignment between GNNs and DP beyond the initial observations and how this alignment can lead to bettergrounded GNN architectures for edgecentric tasks. They propose a novel approach based on integral transforms using a polynomial span framework. Empirical results on a CLRS algorithmic reasoning benchmark are presented to support the proposed improvements. Overall the paper sheds light on the theoretical and practical implications of the GNNDP connection for algorithmic reasoning. 2. Strengths:  The paper delves into a novel and intricate theoretical analysis of the alignment between GNNs and DP using category theory and abstract algebra providing a fresh perspective on neural algorithmic reasoning.  The authors successfully connect GNNs and DP through integral transforms and polynomial span diagrams offering a theoretical foundation for betteraligned GNN architectures.  Empirical results on the CLRS benchmark provide tangible support for the proposed approach demonstrating improvements in algorithmic alignment for edgecentric tasks. 3. Weaknesses:  The paper is highly specialized and technical which may limit its accessibility to a broader audience especially those unfamiliar with category theory and abstract algebra.  While the theoretical framework is robust more discussion on the practical implications and realworld applications of the proposed GNN architectures could enhance the papers relevance.  The complexity of the integral transform approach and the polynomial span diagrams might pose challenges for implementation and practical usage in realworld scenarios. 4. Questions:  Could the authors elaborate on how the proposed integral transform approach can be practically implemented and scaled for realworld applications beyond the CLRS benchmark  Can the findings of this study be extended to explore the alignment between GNNs and other algorithmic problemsolving strategies apart from DP and if so what are the potential implications  How do the newly proposed GNN variants align with existing algorithms and structures and what are the criteria for determining the optimal alignment for different tasks 5. Limitations:  The paper primarily focuses on the theoretical analysis of the GNNDP connection with limited discussion on broader implications and realworld applications.  The practical feasibility and scalability of the proposed GNN architectures based on integral transforms and polynomial spans may need further validation and implementation insights.  The studys relevance outside the specific domain of neural algorithmic reasoning and algorithmic tasks may be limited constraining the generalizability of the findings. Overall the paper presents a significant advancement in understanding the alignment between GNNs and DP using a sophisticated theoretical framework. Addressing the practical implications scalability and broader applications of the proposed approach could further enhance the impact of the study in the field of algorithmic reasoning and deep learning.", "ymAsTHhrnGm": "Peer Review Summary: The paper introduces the concept of inverse game theory in the context of Stackelberg games aiming to learn the underlying utility of agents based on their equilibrium behaviors. The research considers the impact of bounded rationality on learning scenarios and presents the PURE learning algorithm. The theoretical analysis and empirical experiments demonstrate the effectiveness of PURE in recovering follower utilities in different game scenarios. Strengths: 1. Novelty: The paper explores an important topic in game theory by addressing the inverse game theory problem under bounded rationality which is a unique and significant contribution to the field. 2. Rigorous Analysis: The theoretical results are wellsupported with formal proofs and logical reasoning providing a clear theoretical foundation for the proposed learning algorithm. 3. Practical Relevance: The introduction of the PURE algorithm addresses the challenges posed by bounded rationality in learning game parameters offering a practical approach for realworld applications. 4. Empirical Validation: The empirical experiments provide insight into the performance of the proposed algorithm under different game settings adding credibility to the theoretical findings. Weaknesses: 1. Complexity: The paper contains intricate mathematical formulations and proofs which might hinder the comprehension for readers less familiar with advanced game theory concepts. 2. Experimental Design: While the synthesized game instances offer controlled environments for testing the absence of realworld data limits the generalizability of the empirical results. 3. Terminology: The paper incorporates technical terminology that might be challenging for readers outside the specific research domain to fully grasp the content. Questions: 1. How does the PURE algorithm handle situations where the followers responses exhibit significant variability or noise due to bounded rationality 2. Can the approach presented in the paper be extended to more complex game scenarios beyond Stackelberg games incorporating different decisionmaking models 3. How does the selection of different parameters such as the level of bounded rationality impact the learning efficiency and accuracy of the PURE algorithm Limitations: 1. Limited Realworld Data: The reliance on synthesized game instances limits the generalizability of the findings to realworld scenarios where complexities and nuances might not be fully represented. 2. Complexity of Theory: The complex mathematical analysis might deter readers less familiar with game theory or statistical models potentially reducing the accessibility of the paper to a broader audience. Overall the paper offers valuable insights into inverse game theory under bounded rationality presenting a novel approach with promising theoretical results and empirical validation. Clarifications on the practical implications and generalizability of the findings would enhance the relevance and impact of the research.", "o-mxIWAY1T8": " Summary The paper introduces a Semantic Probabilistic Layer (SPL) for structured output prediction tasks providing a clear interface for integrating intricate logical constraints into neural networks. SPL outperforms existing neurosymbolic approaches on challenging tasks ensuring consistency and accuracy. The experiments demonstrate SPLs effectiveness on neuralsymbolic SOP benchmarks including pathfinding and hierarchical multilabel classification.  Strengths 1. Novel Approach: Introduces SPL for incorporating logical constraints into neural networks addressing the limitations of existing neurosymbolic approaches. 2. Experimental Results: Demonstrates superior performance of SPL on challenging tasks compared to stateoftheart methods. 3. Clear Explanations: Provides detailed explanations of SPL components constraints and implementation procedures. 4. Opensource Code: Code availability on Github enhances reproducibility and facilitates further research.  Weaknesses 1. Complexity: The paper may be challenging to understand for readers unfamiliar with neurosymbolic approaches and circuit representations. 2. Evaluation Metrics: While the paper reports performance metrics such as exact matches and consistency additional metrics or comparisons with more baselines could strengthen the evaluation.  Questions 1. What methods were used to ensure the efficiency of SPL during training and inference 2. Can SPL be easily adapted to handle dynamic constraints or evolving environments in realworld applications 3. How does the scalability of SPL compare to other neurosymbolic approaches when dealing with larger datasets or more complex constraints 4. Are there any theoretical guarantees or formal proofs regarding the consistency and accuracy of SPL predictions  Limitations 1. Limited Benchmark Diversity: The experiments focus mainly on structured output prediction tasks and the generalizability of SPL to a broader range of tasks remains to be explored. 2. Complexity of Logical Constraints: While SPL effectively handles complex logical constraints the scalability and efficiency of compiling these constraints into circuits for larger and more diverse tasks need further investigation. Overall the paper presents a promising approach with SPL for addressing neurosymbolic problems but further research is needed to address scalability challenges and evaluate SPL in more varied scenarios.", "zTQdHSQUQWc": "Peer Review Summary: The paper introduces a Frequency improved Legendre Memory model (FiLM) for longterm time series forecasting. The model utilizes Legendre Polynomials projections and Fourier analysis to capture historical information while reducing noise. Experimental results demonstrate significant improvements in multivariate and univariate forecasting tasks compared to stateoftheart models. The proposed FiLM also shows enhanced computational efficiency through dimensionality reduction. Strengths: 1. The paper addresses a crucial challenge in longterm forecasting  balancing historical information preservation and noise reduction. 2. The proposed FiLM model incorporates Legendre Polynomials projections Fourier analysis and lowrank approximation effectively. 3. Experimental results on benchmark datasets illustrate significant improvements in forecasting accuracy for both multivariate and univariate scenarios. 4. The paper includes detailed theoretical explanations and empirical validations to support the effectiveness of the proposed model. Weaknesses: 1. While the paper presents a novel approach more indepth analysis could be provided on the limitations of the proposed model and potential areas for further improvement. 2. The paper could benefit from further discussion on the generalizability of the FiLM model across different types of time series data and forecasting tasks. 3. The experimental setup and evaluation metrics used can be more comprehensive providing a deeper understanding of the models performance compared to existing methods. 4. It would be beneficial to include a discussion on the computational complexity of the proposed model and its scalability to larger datasets. Questions: 1. How does the FiLM model perform on outofsample data compared to insample data indicating the generalizability of the model 2. Could the authors elaborate on the choice of Legendre Polynomials and Fourier analysis over other potential techniques for data representation in time series forecasting 3. How does the FiLM model handle irregularities or anomalies in the time series data and is there a mechanism in place to adapt to changing patterns over time 4. Can the authors provide insights into the interpretability of the FiLM model and how users can extract meaningful information from the generated forecasts Limitations: 1. The experiments conducted focus on specific benchmark datasets potentially limiting the generalizability of the results to diverse realworld scenarios. 2. The model may face challenges when applied to highly dynamic or highly irregular time series datasets where historical patterns do not provide significant predictive power. 3. The paper could provide more discussion on the computational resources required for training and deploying the FiLM model especially in the context of largescale applications. In conclusion the paper presents a novel approach to longterm time series forecasting with the FiLM model showing promising results in accuracy improvement and computational efficiency. However further investigations and discussions are necessary to fully understand the potential implications and limitations of the proposed model.", "oOte_397Q4P": "Peer Review: 1) Summary: The paper introduces Sparse Structure Search for Delta Tuning (S3Delta) which automates the process of constructing delta modules in pretrained models for efficient finetuning. By leveraging differentiable search techniques and explicit sparsity control S3Delta aims to optimize the trainable structure while reducing the number of trainable parameters. Through comprehensive experiments the authors demonstrate that S3Delta outperforms manually designed structures with considerably fewer parameters maintaining high performance across different datasets. Transferability and efficiency of the search process are also highlighted. 2) Strengths and weaknesses: Strengths:  Original Contribution: The paper introduces a novel approach S3Delta which automates the construction of delta modules in pretrained models addressing the challenge of efficient finetuning.  Experimental Rigor: The extensive experiments conducted to evaluate the performance of S3Delta demonstrate strong empirical results showcasing the effectiveness of the proposed method.  Transferability: The ability of the searched structures to transfer across tasks highlights the practical applicability and robustness of the proposed approach.  Efficiency: The authors analyze the computational resources required for the search process shedding light on the efficiency of the proposed methodology. Weaknesses:  Sparse structure search space: While the paper discusses the challenges of finding optimal structures in the search space more insights into the potential limitations of the search space could provide valuable context.  Discussion on failure cases: It would be beneficial to include discussions on potential failure cases or scenarios when S3Delta may not perform optimally to provide a balanced view of the approach. 3) Questions:  What are the criteria used to determine the optimal structure during the search process  How does the efficiency and performance of S3Delta compare to other stateoftheart methods for finetuning pretrained models  Can the proposed method be generalized to different types of pretrained models beyond Transformers 4) Limitations:  Search space complexity: The complexity of the search space may limit scalability and generalizability in realworld applications.  Dependency on initial parameters: The effectiveness of S3Delta may be impacted by the initial set of parameters used for the search necessitating robust initialization strategies. Overall the paper presents a compelling methodology for enhancing the efficiency of finetuning pretrained models through automated structure search. Addressing the highlighted weaknesses and limitations could further strengthen the papers impact and applicability in the field of natural language processing research.", "pgBpQYss2ba": " Peer Review  1) Summary This paper investigates the statistical complexity in adversarial decisionmaking scenarios focusing on the DecisionEstimation Coefficient (DEC). The study establishes that the convexified DEC is essential for achieving low regret in adversarial decision making and draws connections to other complexity measures. The research develops upper and lower bounds on regret and provides a characterization of learnability in adversarial settings.  2) Strengths and Weaknesses Strengths:  Thorough investigation of statistical complexity in adversarial decisionmaking environments.  Establishing the importance of the convexified DecisionEstimation Coefficient for low regret in adversarial scenarios.  Providing upper and lower bounds on regret in these settings.  Development of Algorithm 1 and showing its performance in controlling regret.  Drawing connections between the DecisionEstimation Coefficient ExplorationbyOptimization and the Information Ratio.  Showing structural results linking the Complexity Measures. Weaknesses:  The computational efficiency aspect is mentioned as a potential limitation requiring further exploration for practical scenarios.  The paper could benefit from more realworld application examples to illustrate the practical implications of the theoretical findings.  3) Questions  Can the Algorithm 1 be implemented and tested on realworld decisionmaking problems to validate the theoretical findings  Are there specific scenarios or domains where the convexification of the DecisionEstimation Coefficient is known to have a significant impact on the performance of decisionmaking algorithms  How generalizable are the results to different types of adversarial decisionmaking settings beyond the ones explored in the paper  In terms of computational requirements are there strategies to enhance the efficiency of Algorithm 1 for larger decision spaces  4) Limitations  The study primarily focuses on theoretical analysis and may benefit from empirical validation to demonstrate the practical applicability of the proposed methodologies.  The paper acknowledges the issue of computational efficiency for large decision spaces suggesting potential limitations in scalability for realworld applications. Overall the paper presents a rigorous analysis of statistical complexity in adversarial decision making offering valuable insights into the role of the DecisionEstimation Coefficient. The findings contribute significantly to the understanding of optimal decisionmaking strategies in challenging environments.", "lCGDKJGHoUv": " Peer Review:  Summary: The paper investigates the generalization performance of stochastic gradient descent (SGD) in the context of the stochastic convex optimization framework. Surprisingly the study reveals instances where SGD exhibits substantial empirical risk and generalization gap discrepancies. This challenges conventional wisdom and existing generalization bounds. The paper extends the analysis to different variants of SGD showcasing results for both withreplacement and withoutreplacement settings. Notably the work introduces the novel concept of benign underfitting where SGD underfits training data but still generalizes well. The analysis sheds light on the stability and generalization ability of SGD in various scenarios.  Strengths: 1. Novel Contribution: The paper presents compelling findings that challenge traditional understanding of generalization performance in machine learning algorithms especially in the context of convex optimization. 2. Rigorous Analysis: The mathematical analysis and construction of counterexamples provide a robust foundation for the results presented. 3. Significant Impact: The insights on the behavior of SGD in different settings along with the introduction of the concept of benign underfitting add valuable perspectives to the field of machine learning theory.  Weaknesses: 1. Complexity: The technical nature of the paper may limit the accessibility to a broader audience particularly those without a strong mathematical background. 2. Lack of Practical Implications: While the theoretical insights are profound the practical implications and applications of the findings could have been further discussed to make the research more tangible. 3. Reproducibility: Detailed information on the datasets algorithms and experimental setup would enhance the reproducibility of the results.  Questions: 1. Could the authors elaborate on the implications of benign underfitting in practical machine learning applications 2. How do the findings of the paper impact current practices in model training and generalization strategies in machine learning 3. Are there plans to extend the research to explore the implications of these results on deep learning models and nonconvex optimization scenarios  Limitations: 1. Theoretical Focus: The paper primarily focuses on theoretical analysis without delving into practical implications and applications. 2. Limited Context: The discussion on the broader context industry relevance and potential future research directions is relatively brief. In conclusion the paper presents groundbreaking results challenging the conventional understanding of generalization in machine learning. However enhancing the accessibility of the findings to a wider audience and elaborating on the practical implications could further strengthen the impact of the research.  Recommendation: The research provides significant contributions to the theoretical understanding of SGD in convex optimization frameworks. Addressing the limitations and expanding on the practical relevance of the findings could increase the broader impact of the study on the machine learning community.", "p4xLHcTLRwh": " Peer Review  1) Summary The paper introduces SALSA a novel machine learning attack on Learning With Errors (LWE) based cryptographic schemes which are considered as quantumresistant postquantum cryptosystems in the face of potential attacks by quantum computers. The authors propose training transformers to perform modular arithmetic and develop techniques to recover secrets from smalltomid size LWE instances with sparse binary secrets. By combining machine learning models with statistical cryptanalysis techniques they demonstrate successful cryptanalysis of LWEbased cryptosystems. The paper presents the details of data generation model training secret recovery algorithms and evaluation results to support the effectiveness of SALSA in attacking LWE problems of medium dimension. The authors also highlight the sample efficiency of SALSA and discuss its future scalability to larger lattice dimensions.  2) Strengths and weaknesses Strengths:  Innovative Approach: Introducing a novel machine learning attack on latticebased cryptography is a significant contribution to the field.  Detailed Methodology: The paper provides a thorough explanation of the proposed attack methodology including data generation model training secret recovery techniques and evaluation.  Practical Significance: Successfully demonstrating the attack on smalltomid size LWE instances with sparse binary secrets adds practical value to the research.  Sample Efficiency: Discussion on sample efficiency and potential scalability are commendable for realworld applicability. Weaknesses:  Limited Scope: The study focuses on LWE instances with specific characteristics (e.g. binary secrets) and certain dimension ranges limiting the generalizability of the findings.  Realworld Applicability: The authors acknowledge the current limitations of SALSA in attacking realworld cryptosystems which may reduce the immediate impact of the research.  Future Directions: While the study lays a foundation further work on scaling SALSA to larger lattice dimensions and broader secret distributions is essential for broader applicability.  3) Questions  Scalability: How does the proposed SALSA approach plan to scale up to attack higher dimensional lattices beyond n  128 as stated in the conclusion  Sample Reuse: Could more insights be provided on the implications and limitations of sample reuse in the context of the proposed attack methodology  Algebraic Attacks: How does the proposed machine learning attack compare to traditional algebraic attacks on LWE in terms of efficiency accuracy and potential tradeoffs  4) Limitations  Model Generalization: The study focuses on specific instances of LWE with binary secrets limiting the generalization to diverse secret distributions in realworld implementations.  Resource Intensive: The utilization of largescale models and training procedures may lead to resourceintensive computations which could hinder practical deployment.  Assumptions and Constraints: Certain assumptions such as the need for specific error distributions and secret characteristics may restrict the applicability of SALSA. In conclusion the paper presents a novel machine learning attack on latticebased cryptography showcasing promising results in attacking LWE instances with specific characteristics. Addressing the noted limitations and expanding the scope of application could enhance the impact and relevance of the proposed methodology in realworld scenarios.", "qHGCH75usg": " Peer Review of the Paper: \"BYOLExplore: Curiositydriven Exploration in Visuallycomplex Environments\"  1) Summary: The paper introduces BYOLExplore a curiositydriven exploration algorithm that learns a world model and an exploration policy concurrently through selfsupervised learning. The algorithm is evaluated on challenging tasks in 3D environments and Atari games showcasing superior performance compared to other exploration methods. BYOLExplore demonstrates effectiveness in driving exploration without the need for human demonstrations achieving superhuman performance in various domains.  2) Strengths and Weaknesses: Strengths:  Conceptually simple and effective curiositydriven exploration approach.  Achieves humanlevel and superhuman performance in complex environments.  Combines learning a world model and exploration policy in a single joint framework.  Outperforms prominent baselines such as RND and ICM in both Atari games and DMHARD8 tasks.  Impressive robustness and performance even in the presence of noise in the environment. Weaknesses:  Lack of indepth theoretical analysis explaining the methods performance.  Limited discussion on the impact of hyperparameters on algorithm performance.  Simplification of the algorithm design may raise questions about adaptation to more diverse environments.  The paper could benefit from more detailed comparisons with existing stateoftheart methods.  Limited exploration of the algorithms scalability and efficiency with larger datasets.  3) Questions:  How does the design of BYOLExplore handle unexpected or highly stochastic events in the environment  What is the theoretical justification for why BYOLExplore can achieve superhuman performance in the Atari games  Could the algorithms performance be further improved by incorporating additional model enhancements or adaptive mechanisms  How does BYOLExplore compare in terms of sample efficiency and computational requirements with other curiositydriven exploration methods  Are there any limitations or challenges faced by BYOLExplore that could hinder its applicability to realworld complex environments  4) Limitations:  Limited exploration of the scalability and applicability of BYOLExplore in realworld highly stochastic environments.  Lack of comprehensive theoretical analysis to support the observed empirical results.  The paper could provide more detailed insights into the interpretability and generalization capabilities of the algorithm.  The evaluation primarily focuses on challenging game environments broader domain adaptability considerations are needed.  The impact of computational resources and training time on the algorithms performance should be addressed. Overall the paper presents an intriguing and effective approach to curiositydriven exploration in visuallycomplex environments. Further studies should delve into the algorithms theoretical foundations scalability and adaptation to realworld applications to enhance its impact and practical significance.", "xz-2eyIh7u": " Peer Review: 1. Summary: The paper introduces novel algorithms for collaborative linear bandit problems in the presence of adversarial agents. Key contributions include the Robust Collaborative Linear Bandit Algorithm (RCLB) and extensions to Generalized Linear Bandits and Contextual Bandits. Theoretical results provide tight nearoptimal regret bounds for these settings demonstrating the benefits of collaboration despite adversaries. Simulation results confirm the theoretical findings showcasing the robustness and effectiveness of the proposed algorithms. 2. Strengths and weaknesses:  Strengths:  The paper addresses an important and timely problem of multiagent sequential decisionmaking under adversarial conditions.  The proposed algorithms offer a principled approach to balance explorationexploitation tradeoffs in collaborative settings demonstrating robustness to adversarial behavior.  The theoretical results are welldeveloped providing tight regret bounds fundamental limits and extension to more complex bandit models.  Clear delineation of algorithms proofs lower bounds and extensions demonstrates a comprehensive exploration of the problem space.  Weaknesses:  The evaluation is primarily based on synthetic data necessitating further validation on realworld datasets to establish practical relevance.  The paper can benefit from providing additional insights into the computational complexity and scalability of the proposed algorithms in largescale scenarios.  While the proposed algorithms show promising results in simulation practical deployment considerations and challenges should be discussed. 3. Questions:  How do the proposed algorithms scale in terms of computational complexity with respect to the number of agents M model dimension d and time horizon T  Can the algorithms handle dynamic changes in the adversarial behavior during operation or are they optimized for a fixed adversarial model  Are there specific scenarios or realworld applications where the proposed algorithms have been tested or can be applied to demonstrate their effectiveness in practical domains 4. Limitations:  The evaluation relies heavily on synthetic experiments warranting further validation on realworld datasets to assess the generalizability and applicability of the algorithms.  The discussion of practical challenges deployment considerations and robustness to varying environmental conditions could strengthen the relevance of the proposed algorithms.  Overall Assessment: The paper presents a significant contribution to the field of multiagent sequential decisionmaking under adversarial conditions. The development of robust and effective algorithms for collaborative bandit problems showcases novel approaches to address practical challenges in largescale computing systems. The theoretical results are wellsupported and the simulation results provide empirical validation of the proposed algorithms. Further exploration of realworld applications and considerations for practical deployment would enhance the impact and relevance of the work.", "qtZac7A3-F": " Peer Review 1) Summary: The paper introduces Discrete Adversarial Training (DAT) a novel approach that transfers the merits of NLPstyle adversarial training to improve both robustness and generalization in vision models. By leveraging VQGAN to discretize image data into visual words the proposed DAT method achieves significant improvements on various vision tasks including image classification object detection and selfsupervised learning ultimately establishing a new stateoftheart in robust image classification. The paper provides detailed explanations experimental results and analyses to support the effectiveness of DAT. 2) Strengths:  The paper addresses a significant issue in adversarial training and presents a novel approach that demonstrates promising results in improving both robustness and generalization in vision models.  The proposed DAT method leverages VQGAN and discrete visual representations effectively showcasing stateoftheart performance in tasks like image classification object detection and selfsupervised learning.  The experimental setup is thorough encompassing various benchmarks and tasks to validate the effectiveness of the DAT method across different scenarios.  The paper provides detailed explanations of the proposed DAT method ablation experiments performance comparisons with existing methods and indepth analyses enhancing the understanding of the proposed approach.  The paper establishes a robust theoretical foundation and justifies the methodology through empirical validation and visualizations enhancing the credibility of the findings. 3) Weaknesses:  The paper would benefit from more detailed comparisons with existing approaches in the discussion section to highlight the specific advantages of the DAT method over other stateoftheart techniques.  While the paper provides extensive experimental results some sections could be condensed for better readability without compromising the key findings.  The limitations of DAT such as increased training time and the assumption of an ideal discretizer could be further elaborated along with suggestions for future research to address these challenges. 4) Questions:  Can you provide more insights into the computational cost and scalability of the proposed DAT method when applied to largescale vision tasks  How does the performance of DAT compare to other robust training strategies in scenarios with limited computational resources or smaller datasets  Could the paper discuss potential realworld applications and implications of the DAT method in practical settings or industrial use cases 5) Limitations:  The paper primarily focuses on the effectiveness and performance of DAT in experimental settings but it lacks a thorough discussion on realworld applications and implications.  The limitations of DAT such as increased training time and reliance on an ideal discretizer assumption could restrict the methods practical adoption in resourceconstrained environments or applications with stringent computational constraints.  The transferability of the DAT method to other domains beyond image processing is not explored limiting the generalizability of the proposed approach. Overall the paper presents a novel and noteworthy approach in adversarial training for vision models showcasing strong potential in enhancing robustness and generalization. Addressing the identified weaknesses and limitations could further strengthen the papers contribution and applicability in diverse research and industrial contexts.", "xbgtFOO9J5D": "Peer Review 1) Summary The paper presents theoretical advancements in addressing the closest fair ranking and fair rank aggregation problems with a focus on ensuring minority protection and restricted dominance in the ranking process. It introduces novel algorithms for computing fair rankings under the Kendall tau and Ulam metrics and metaalgorithms for fair rank aggregation under a generalized qmean objective applicable to various distance measures. The study provides exact or approximation algorithms for different metrics and fairness criteria and explores implications for practical applications. 2) Strengths and Weaknesses Strengths:  The paper addresses important issues of algorithmic fairness in ranking and rank aggregation.  The introduction of novel algorithms for finding fair rankings under various metrics and fairness constraints is a significant contribution.  The theoretical framework established for fair rank aggregation under a generalized qmean objective is comprehensive and adaptable to different distance measures.  The paper presents clear and structured details of the algorithms and their complexities. Weaknesses:  While the theoretical advancements are valuable the paper lacks empirical validation or realworld applications to demonstrate the practical effectiveness of the proposed algorithms.  The paper focuses heavily on theoretical analysis more practical relevance and application scenarios could enhance the impact of the research.  The novelty of the proposed algorithms could be further demonstrated by comparing with existing stateoftheart algorithms for fairness in ranking and rank aggregation. 3) Questions  How do the proposed algorithms perform in terms of runtime efficiency and solution quality compared to existing stateoftheart algorithms for fair ranking and rank aggregation  Have the proposed algorithms been implemented and tested on realworld datasets to showcase their effectiveness  How do the algorithms handle cases where elements can belong to multiple groups and is there a plan to extend the algorithms to address such scenarios 4) Limitations  The paper lacks empirical validation or applications on real datasets to demonstrate the practical utility of the proposed algorithms.  The scope of the study is limited to theoretical advancements in fairness in ranking practical implications and wider applications could be explored further.  The theoretical complexity of the algorithms may pose challenges for implementation in largescale or realtime ranking scenarios. Overall the paper presents valuable theoretical contributions to the field of fair ranking and rank aggregation algorithms. However to enhance the impact and relevance of the research future work could focus on empirical validation practical applications and handling more complex scenarios in fairnessaware ranking algorithms.", "qq84D17BPu": " Peer Review  1) Summary: The paper introduces an \"Equation of Motion\" (EoM) for deep neural networks (DNNs) to precisely describe the learning dynamics of gradient descent (GD). By addressing discretization error between gradient flow (GF) and GD the authors derive a differential equation that accounts for the discrete learning dynamics more accurately. The study examines two specific cases scale and translationinvariant layers highlighting the importance of the counter term in improving the description of GDs learning dynamics. Experimental results support the theoretical findings demonstrating the efficacy of the proposed EoM.  2) Strengths and Weaknesses: Strengths:  Novel Contribution: The derivation of the EoM to describe the discrete learning dynamics of DNNs fills a critical gap between GF and GD.  Theoretical Rigor: The paper provides thorough mathematical derivations and proofs to support the proposed EoM and counter term.  Experimental Validation: The experimental results support the theoretical findings enhancing the credibility of the studys conclusions.  Application to Specific Cases: By applying the EoM to scale and translationinvariant layers the study demonstrates the practical implications of the proposed framework. Weaknesses:  Complexity: The mathematical formalism and derivations may be challenging for readers without a strong background in differential equations and optimization theory.  Limited Scope: The study focuses solely on GD and GF excluding consideration of stochastic variants acceleration methods or adaptive optimizers which could provide additional insights.  Lack of Comparison: While the paper presents the benefits of EoM a comparison with existing methods or alternative approaches could further demonstrate the effectiveness of the proposed framework.  3) Questions: 1. How does the proposed EoM compare to existing methods that aim to bridge the gap between continuous and discrete learning dynamics in neural networks 2. Are there potential applications or extensions of the EoM framework beyond the specific cases of scale and translationinvariant layers examined in the study 3. Can the derived conclusions from this study be generalized to different types of neural network architectures or optimization algorithms beyond gradient descent 4. How sensitive is the performance of the EoM model to the choice of hyperparameters such as the learning rate and counter term coefficients  4) Limitations:  Generalization: The focus on scale and translationinvariant layers may limit the generalizability of the results to more diverse neural network architectures and optimization scenarios.  Computational Complexity: The inclusion of higherorder counter terms was omitted due to memory constraints potentially affecting the completeness of the analysis.  External Factors: The study does not account for external factors like stochasticity which are prevalent in practical machine learning settings and could influence the applicability of the proposed EoM. Overall the paper presents a rigorous theoretical framework for describing the discrete learning dynamics of DNNs using an EoM. Addressing the identified weaknesses and limitations could strengthen the impact and applicability of the proposed methodology.", "xqyDqMojMfC": " Peer Review  1) Summary The paper presents a study on stochastic optimization algorithms for constrained nonconvex stochastic optimization problems with Markovian data. The focus is on cases where the transition kernel of the Markov chain is statedependent as found in machine learning applications like strategic classification and reinforcement learning. Both projectionbased and projectionfree algorithms are investigated with an emphasis on the number of calls to the stochastic firstorder oracle needed to obtain an \u03b5stationary point. The study establishes novel oracle complexity results for the stochastic conditional gradient algorithm. Empirical demonstration of the algorithms performance on strategic classification with neural networks is also provided.  2) Strengths and weaknesses Strengths:  The paper addresses an important problem in the area of stochastic optimization for constrained nonconvex problems with Markovian data.  The study introduces novel oracle complexity results for the stochastic conditional gradient algorithm in the context of statedependent Markov chains.  Empirical evaluation on strategic classification with neural networks adds practical relevance to the theoretical findings. Weaknesses:  The paper is highly technical and may be challenging for readers without a strong mathematical background.  The experimental validation could be expanded to include more diverse datasets and comparison with other stateoftheart algorithms.  3) Questions  Could the paper provide more insights into the practical implications and applications of the proposed algorithm in realworld scenarios beyond the strategic classification example  How robust is the algorithm to noise and other uncertainties in the data especially in a dynamic realtime environment where Markovian assumptions may not hold perfectly  4) Limitations  The complexity and technical nature of the paper may limit its accessibility to a broader audience of researchers and practitioners.  The generalizability of the findings to other types of data distributions beyond the specific Markovian setting remains unexplored.  Overall Assessment The paper makes a significant contribution to the field of stochastic optimization with novel oracle complexity results for constrained nonconvex optimization with Markovian data. The technical rigor of the study and the empirical validation enhance the credibility of the results. However efforts should be made to present the findings in a more accessible manner to cater to a wider audience of readers.", "wKf5dRSartn": " Peer Review: 1) Summary: The paper introduces a new teaching complexity parameter STDmin which is the first to provide an upper bound of the VapnikChervonenkis dimension (VCD) on a batch teaching complexity parameter. The significance lies in its ability to satisfy certain desirable properties of teaching models while being upperbounded by VCD. The study also explores the challenges of including ambiguous teaching models and discusses how STDmin can be ambiguous. The theoretical analysis comparison with existing teaching models and demonstration of properties like classmonotonicity and domainmonotonicity are welldetailed. 2) Strengths and weaknesses: Strengths:  The paper presents a significant contribution by introducing STDmin which is a novel batch teaching complexity parameter bounded by VCD.  The theoretical analysis proofs and discussions on desirable properties in teaching models are informative and comprehensive.  The study demonstrates the shortcomings of existing teaching models in terms of ambiguity and provides insights on the applicability of STDmin in addressing these issues. Weaknesses:  While the theoretical analysis is thorough the practical implications of the findings could be further explored.  More discussion on the potential realworld applications or experimental validations of the proposed teaching model could enhance the practical relevance of the study.  The clarity and accessibility of some parts of the paper especially the technical proofs and relationships between different teaching complexity parameters could be improved for a broader audience. 3) Questions:  How do you envision the practical implementation and validation of the STDmin teaching model in realworld machine teaching scenarios  Given that STDmin is ambiguous are there strategies or modifications that could be employed to enhance its unambiguity while preserving its desirable properties 4) Limitations:  The lack of empirical validation or application of the proposed STDmin model in realworld scenarios limits the immediate practical impact of the study.  The focus on theoretical and abstract concepts may make it challenging for practitioners to grasp the practical implications and applicability of the findings. Overall the paper provides a novel contribution to the field of machine teaching by introducing the STDmin complexity parameter which is conceptually sound and theoretically wellfounded. Further exploration of practical applications and addressing the ambiguity issue could strengthen the impact of the research. The comprehensive theoretical analysis and exploration of teaching model properties make this paper a valuable addition to the literature.", "qqIrESv4f_L": " Peer Review  1) Summary The paper proposes the Implicit Neural Signal Processing Network (INSPNet) framework which enables direct modification of Implicit Neural Representations (INRs) without the need for explicit decoding. By incorporating differential operators on INR the framework approximates continuous convolution filters and builds the first Convolutional Neural Network (CNN) operating directly on INRs named INSPConvNet. Experimental results show the effectiveness of the approach in lowlevel image processing and highlevel tasks like image classification.  2) Strengths and Weaknesses  Strengths:  Novel approach: The idea of processing INRs without explicit decoding is innovative.  Theoretical foundation: The theoretical analysis concerning group invariance and expressiveness provides a solid foundation for the proposed framework.  Experimental validation: The experiments demonstrate the frameworks effectiveness in tasks like edge detection image denoising and image classification.  Extensive evaluation: The evaluation covers a wide range of tasks on different datasets showcasing the versatility of the framework.  Weaknesses:  Complexity: The frameworks reliance on highorder derivatives may lead to computational inefficiency and numerical stability issues.  Scalability: The recursive computation of derivatives required for INSPConvNet could be a limitation in scaling up the approach.  Limitations in theory and practice: The paper acknowledges limitations in terms of expressiveness of convolution and practical challenges in memory efficiency and scalability.  3) Questions  How does the proposed framework compare in terms of computational efficiency with traditional convolutional methods especially for large datasets  Could there be potential optimizations or approximations to improve the scalability and memory efficiency of the framework when dealing with highorder derivatives  Have you explored the tradeoffs or potential drawbacks of using highorder derivatives for processing INRs especially in realworld scenarios  How does the framework generalize to more complex tasks beyond the ones evaluated in the experiments  4) Limitations  The paper provides a comprehensive overview of the frameworks capabilities and limitations. However further exploration of practical implications and tradeoffs in realworld applications could enhance the impact of the research.  Addressing the theoretical limitations and proposing potential solutions or workarounds could strengthen the frameworks applicability in broader contexts.  Investigating the frameworks robustness to noise data perturbations or domain shifts would provide insights into its realworld utility. Overall the research presents an intriguing approach to signal processing on INRs with the INSPNet framework. Addressing the questions and limitations highlighted could enhance the frameworks practical relevance and applicability in diverse computer vision tasks.", "vfCd1Vt8BGq": " Peer Review:  1) Summary: The paper presents a novel information theoretic bound for supervised learning algorithms using leaveoneout conditional mutual information (looCMI). The proposed bound is computationally efficient interpretable and provides nonvacuous generalization bounds specifically for deep learning tasks like image classification. The work showcases theoretical derivations empirical validations and potential applications in understanding generalization in deep learning models.  2) Strengths and Weaknesses: Strengths:  The introduction of looCMI bound addresses computational efficiency and interpretability issues in existing conditional mutual information bounds.  The empirical validation on deep learning tasks especially image classification demonstrates the practical applicability and effectiveness of the proposed bound.  The paper effectively connects the theoretical derivations to practical implications such as stability of optimization algorithms and the geometry of loss landscapes.  The exploration of different scenarios including dataset size variations and synthetic randomization adds depth to the analysis and validation of the proposed bounds. Weaknesses:  While the paper addresses the limitations of existing bounds there could be more comparative analysis with other informationtheoretic methods or empirical studies with alternative approaches for a comprehensive evaluation.  The discussion on limitations and potential extensions of the proposed bound could be more detailed to provide a clearer roadmap for future research directions.  The paper focuses heavily on theoretical developments and empirical validations but additional discussions on the practical implications of the results in realworld applications could enhance the impact of the study.  3) Questions:  Can the proposed looCMI bound be extended or modified to handle other types of machine learning tasks beyond supervised learning such as reinforcement learning or unsupervised learning  How does the computational efficiency of the looCMI bound compare to traditional methods in terms of both training time and prediction performance on largescale datasets  Are there specific hyperparameters or conditions under which the looCMI bound performs exceptionally well or poorly and how can these be identified or optimized in practice  4) Limitations:  The empirical validation is limited to specific deep learning tasks which might restrict the generalizability of the proposed looCMI bounds to a wider range of machine learning applications.  The assumption of Gaussian noise for synthetic randomization may not fully capture the complexities of realworld data and learning scenarios potentially impacting the generalizability of the results.  The paper could provide more insights into the interpretability of the looCMI bound in practical machine learning scenarios especially exploring its utility in guiding model development or optimization strategies. Overall the paper presents a sound and innovative approach to informationtheoretic generalization bounds for supervised learning algorithms. Further explorations into the scalability adaptability and practical implications of the proposed looCMI bound could enhance its value for the machine learning community.", "uvE-fQHA4t_": "Peer Review 1) Summary The paper introduces a novel PACBayesian generalization bound that depends on the gradientnorm of the loss function relaxing the assumptions of uniform loss bound in traditional bounds. The methods are applied to linear and deep neural network models and the empirical analysis shows tighter generalization performance. The paper presents theoretical developments experimental results and comparisons with existing bounds. 2) Strengths and weaknesses Strengths:  The paper introduces a novel generalization bound that relaxes traditional assumptions.  Theoretical developments are sound leveraging logSobolev inequalities in a PACBayesian framework.  Empirical analysis shows tighter generalization compared to baseline methods.  The paper addresses an open problem raised by Bartlett et al. [2017a]. Weaknesses:  The theoretical derivations are complex and might be challenging for readers unfamiliar with advanced probability theory.  The empirical evaluation lacks indepth analysis of the impact of different neural architectures.  The practical implications of the proposed bounds in realworld applications are not extensively discussed. 4) Questions  Could the authors elaborate on the practical significance of the proposed bound in realworld deep learning applications beyond the theoretical implications  How sensitive is the new bound to model hyperparameters or different data distributions  Are there computational challenges associated with implementing the techniques proposed in the paper especially considering the complexity of the theoretical derivations  What are the limitations in applying the proposed bound to more complex deep learning models beyond the linear and nonlinear cases considered in the experiments 5) Limitations  The paper primarily focuses on theoretical developments and empirical analysis on limited datasets and architectures potentially limiting the generalizability of the findings.  The complexity of the theoretical derivations may pose a barrier to the accessibility of the proposed techniques to the wider ML community.  The practical implications and applicability of the new bound in realworld scenarios need further exploration and validation. Overall the paper presents a novel approach to generalization bounds with important theoretical developments. However further investigation is needed to explore the practical implications robustness and scalability of the proposed methods in diverse deep learning applications.", "yfrDD_rmD5": " Peer Review  1) Summary: The paper introduces a novel technique called TracInWE to compute data influence in largescale Natural Language Processing (NLP) models focusing on word embedding layers instead of the last layer. This is to address the \"cancellation effect\" observed in existing techniques that operate on the last layer and suffer from reduced discriminative power. TracInWE leverages word gradients similarity to calculate data influence and outperforms previous methods in case deletion evaluation. The paper demonstrates the effectiveness of TracInWE through experiments on multiple NLP classification tasks.  2) Strengths and Weaknesses: Strengths:  The paper addresses a significant issue in current data influence computation techniques by proposing an innovative approach focused on word embedding layers.  TracInWE outperforms existing methods in case deletion evaluation showcasing its practical impact in NLP applications.  The paper is wellstructured providing clear explanations of the methodology and results. Weaknesses:  The paper lacks a thorough comparison with more recent techniques or benchmarks in the field of explainable machine learning.  While the experiments demonstrate the effectiveness of TracInWE in certain scenarios a broader evaluation across a wider range of datasets and models would further strengthen the findings.  The theoretical justification for certain design choices and their implications could be further elaborated to enhance the clarity of the methodology.  3) Questions:  Are there specific use cases or scenarios where the TracInWE method may not be as effective or suitable compared to other existing techniques  How does the performance of TracInWE scale with the size and complexity of NLP models and are there any limitations in its application to extremely large models  Could the authors provide more insights into the interpretability and practical implications of the wordlevel decomposition feature of TracInWE in realworld applications  4) Limitations:  The paper focuses primarily on the NLP domain limiting the generalizability of the findings to other machine learning domains.  The experiments while robust could benefit from more extensive validation across different datasets and model architectures to validate the generalizability of TracInWE.  The paper could elaborate more on the computational efficiency and scalability of TracInWE especially in terms of realworld deployment and performance in production settings. Overall the paper presents an insightful and innovative technique for computing data influence in NLP models. Further research could explore the broader applicability of TracInWE across diverse machine learning tasks and shed light on its realworld implications.", "t0VbBTw-o8": " Peer Review  Summary: The paper introduces novel graybox certificates for Graph Neural Networks (GNNs) that provide stronger guarantees against strong adversaries who can manipulate features of multiple nodes in the graph. The proposed method called messageinterception smoothing randomly intercepts messages by deleting edges and masking features. The certificates consider the underlying graph structure which improves certifiable robustness by applying graph sparsification leading to more efficient and accurate results. Experimental evaluations demonstrate the effectiveness of the method on various datasets and architectures.  Strengths: 1. Innovative Approach: The concept of messageinterception smoothing is unique and addresses a critical aspect of adversarial robustness in GNNs. 2. Stronger Guarantees: The proposed graybox certificates offer stronger guarantees against adversaries manipulating entire nodes enhancing the robustness of GNNs. 3. Efficiency: The method is more sampleefficient and computationally faster compared to existing smoothingbased certificates for GNNs making it more practical and scalable. 4. Experimental Validation: Comprehensive experiments are conducted on multiple datasets and architectures showcasing the effectiveness of the proposed method and its benefits.  Weaknesses: 1. Complexity: The theoretical depth and complexity of the method may pose challenges for practitioners without a strong background in graph theory and machine learning. 2. Threat Model Limitations: The proposed certificates are limited to feature perturbations and may not encompass all possible threat models limiting the generalizability of the approach. 3. Evaluation Settings: The reliance on transductive learning settings may not fully capture practical scenarios raising concerns about realworld applicability.  Questions: 1. Generalizability: How adaptable is the method to handle different types of attacks beyond feature perturbations such as structural modifications or relational attacks 2. RealWorld Deployment: Can the proposed method be practically implemented in scenarios with dynamic evolving graphs or does it require fixed graph structures for certification 3. Certifiable Radius: How does the certifiable radius scale with larger and more complex graphs in terms of computational requirements and practical feasibility 4. Comparison to Alternative Methods: How does the performance of the proposed graybox certificates compare to existing whitebox and blackbox certificates in terms of robustness and efficiency  Limitations: 1. Practical Applicability: The methods reliance on graph sparsification and assumption of full graph knowledge may limit its applicability in realworld dynamic environments with incomplete or evolving graph structures. 2. Threat Model Assumptions: The focus on only feature perturbations may overlook other potential adversarial strategies hence limiting the comprehensiveness of the proposed approach. Overall the paper presents a promising method for enhancing the robustness of GNNs against adversarial attacks. Addressing the raised concerns can further strengthen the contribution and practical utility of the proposed graybox certificates.", "sj9l1JCrAk6": " Peer Review  1) Summary: The paper investigates federated learning in practical recommender systems and natural language processing scenarios focusing on the differentiation of hot and cold data features that involve different numbers of clients. The authors propose a novel algorithm FedSubAvg to address feature heat dispersion which significantly outperforms conventional FedAvg and its variants. The study includes theoretical analysis algorithm design and empirical evaluations on public and industry datasets demonstrating the effectiveness and efficiency of FedSubAvg.  2) Strengths and Weaknesses:  Strengths:  The paper addresses an important issue of feature heat dispersion in federated learning which has practical implications in realworld scenarios.  The proposal of FedSubAvg algorithm is theoretically sound supported by rigorous analysis and convergence proofs.  Extensive evaluation on public and industrial datasets demonstrates the superior performance of FedSubAvg over existing algorithms showcasing its effectiveness.  The detailed theoretical analysis provides insights into the challenges and improvements in federated learning concerning feature heat dispersion.  Weaknesses:  The paper lacks a detailed comparison with other stateoftheart federated learning algorithms beyond the selected baselines which could provide a more comprehensive assessment of FedSubAvgs performance.  The empirical evaluation could benefit from more diverse datasets and scenarios to generalize the findings and understand the algorithms robustness.  The paper could provide more insights into the practical implementation aspects of FedSubAvg such as computational complexity or communication overhead.  3) Questions:  How does the proposed FedSubAvg algorithm handle cases where there are extreme imbalances in the distribution of feature heat among clients Are there any mitigation strategies in such scenarios  Could the authors elaborate on the potential challenges or limitations in deploying FedSubAvg in largescale federated learning systems with a massive number of clients  Has there been any consideration of privacy and security implications in the design and implementation of FedSubAvg particularly in scenarios involving sensitive data from multiple clients  4) Limitations:  Limited Comparison: The paper could expand the comparison to include more stateoftheart federated learning algorithms and provide a more comprehensive evaluation framework.  Validation on Diverse Datasets: The empirical evaluation could be enhanced by testing FedSubAvg on a wider range of datasets to assess its performance across various scenarios and domains.  Implementation Considerations: While the theoretical analysis is extensive the paper could benefit from discussing practical implementation considerations of FedSubAvg to facilitate realworld deployment. Overall the paper presents a novel and effective approach to address feature heat dispersion in federated learning showcasing promising results. Further exploration of practical implications scalability issues and comparison with a broader set of algorithms could strengthen the research findings.", "mxnxRw8jiru": " Peer Review  1) Summary: The paper introduces a novel approach for differentially private data generation by optimizing a small set of samples for downstream tasks under discriminative guidance aiming to improve sample utility. The method demonstrates significant enhancements in sample utility over existing stateoftheart approaches providing practical benefits for realworld applications. Experimental results on MNIST and FashionMNIST datasets validate the effectiveness and superiority of the proposed method showcasing improvements in downstream test accuracy and memorycomputation consumption. The paper discusses scalability issues tradeoffs between visual quality and task utility generalization abilities across architectures and application in private continual learning. The broader impact section highlights the significance of the work in privacypreserving data generation for advancing machine learning applications.  2) Strengths and Weaknesses: Strengths:  The paper introduces an innovative approach that addresses the challenge of data privacy in highdimensional data generation.  The method demonstrates considerable improvements in sample utility over existing methods enhancing downstream task performance.  Experimental results on benchmark datasets validate the efficacy of the proposed approach.  The discussion on scalability tradeoffs generalization and application in private continual learning adds value to the contribution.  Broader impacts on promoting privacy in data generation for societal benefits are acknowledged. Weaknesses:  The paper could provide more insights into the limitations and constraints of the proposed method in terms of scalability to complex datasets.  Further details on the computational complexity and efficiency of the method would enhance the technical evaluation.  Comparison with additional stateoftheart methods beyond those discussed could provide a comprehensive evaluation of the proposed technique.  The discussion on potential risks or downsides of the approach could further enrich the analysis.  3) Questions:  Could the methods efficacy vary significantly when applied to datasets with a larger number of label classes or more diverse characteristics than MNIST and FashionMNIST  How does the computational overhead of the proposed method compare to existing techniques especially when dealing with largescale datasets  Are there any limitations or challenges in extending the approach to realworld applications with more complex data structures or regulatory requirements  Could the method be adapted for use in scenarios where privacy concerns intersect with different types of downstream tasks beyond classification  4) Limitations:  The scalability of the proposed method to datasets with numerous label classes or extensive diversity may raise computational challenges and limit its applicability in certain scenarios.  The efficiency and computational overhead of the method when dealing with largescale datasets remain unexplored potentially affecting its feasibility in realworld applications.  Generalization to a broader range of downstream tasks and datasets beyond classification tasks could present limitations or additional complexities that need to be addressed. Overall the paper presents a novel approach for private highdimensional data generation with promising results. Addressing the highlighted weaknesses and limitations could further strengthen the papers contribution and applicability in diverse research and realworld settings.", "w-Aq4vmnTOP": " Peer Review  1) Summary The paper presents a comprehensive study on learning and refutation tasks in noninteractive local differential privacy (LDP) protocols providing a thorough characterization of sample complexity for agnostic PAC learning. The researchers propose equivalence in sample complexity between learning and refutation in the agnostic setting under noninteractive LDP shedding light on the relationship between the two tasks.  2) Strengths and weaknesses Strengths:  The paper offers a detailed analysis of learning and refutation in LDP protocols including a complete characterization of sample complexity.  The study contributes to the understanding of agnostic PAC learning under strong privacy constraints particularly in the noninteractive LDP model. Weaknesses:  The paper lacks practical application examples or experimental validation to support the theoretical findings.  The complexity of the mathematical formulations might make the content less accessible to a wider audience.  3) Questions  How do the results obtained from this study contribute to improving data privacy methodologies in practical scenarios  Are there specific concept classes or scenarios where the proposed equivalence between learning and refutation in noninteractive LDP is particularly significant or challenging  4) Limitations  The paper primarily focuses on theoretical aspects and may benefit from empirical validation or case studies to demonstrate the applicability of the results in realworld settings.  The complexity of the mathematical proofs and formulations might limit the understanding of readers not deeply familiar with the topic. Overall the paper presents a rigorous study on agnostic PAC learning in the context of noninteractive local differential privacy offering valuable insights into the sample complexity of learning and refutation tasks. Further practical validation and clarity in presentation could enhance the overall impact of the work.", "tQRoZ9nRgM": "Peer Review: 1) Summary: The paper addresses a sequential matching problem in large centralized platforms focusing on matching jobs to workers under uncertainty about worker skill proficiencies. It introduces the concept of an unlimited supply of workers and proposes a novel rateoptimal learning algorithm to maximize cumulative payoffs over a planning horizon. The research provides lower bounds on regret for any matching algorithm in this setting and emphasizes the importance of adaptable algorithms that perform well even in the face of distributional shifts among the worker population. 2) Strengths:  The paper tackles an important and practical problem in modern operational settings like the gig economy.  It introduces a novel algorithm that adapts to uncertainties and achieves rateoptimal performance.  The research establishes lower bounds on regret providing a theoretical foundation for algorithmic performance.  The findings suggest that the algorithms performance depends only on workertype distributions in sublogarithmic terms which is a significant contribution.  The paper is wellorganized with clear explanations and formal problem formulations. Weaknesses:  The complexity of the problem and the mathematical analyses involved might limit the accessibility of the paper to nonexperts in the field.  While the proposed algorithm achieves rateoptimal performance the paper could benefit from realworld experiments or case studies to demonstrate the algorithms effectiveness in practical scenarios.  The paper could elaborate more on the applicability of the proposed algorithm in realworld gig economy platforms and how it compares to existing methods. 3) Questions:  How does the proposed algorithm handle practical challenges such as noisy data or biased sampling in the worker population  Are there any assumptions made in the model that might oversimplify the realworld gig economy scenario  Can the algorithm be easily implemented and scaled for use in largescale operational platforms 4) Limitations:  The paper largely focuses on the theoretical aspects of the matching problem and the lack of empirical validation might limit its immediate practical applicability.  The assumptions about worker distributions and mean payoffs while necessary for the algorithm may not fully represent the complex and dynamic nature of worker populations in gig economy platforms.  The algorithms performance in highly dynamic environments with rapid changes in worker distributions remains to be explored. Overall the paper presents a thorough investigation into a challenging matching problem in the gig economy context offering a novel algorithmic solution with strong theoretical underpinnings. Further exploration and validation of the algorithms performance in realworld scenarios could enhance the practical relevance of the research.", "pk1C2qQ3nEQ": " Summary: The paper proposes a new uncertainty measure Balanced Entropy Acquisition (BalEntAcq) for active learning applications in machine learning. The measure is designed to capture the information balance between the underlying softmax probabilities and label variables. The authors use Beta approximation and propose a new learning principle based on balanced entropy. They demonstrate through experiments that BalEntAcq consistently outperforms other linearly scalable active learning methods across different datasets.  Strengths: 1. Innovative Approach: Introducing a new uncertainty measure BalEntAcq based on the concept of balanced entropy is a novel and interesting approach in the realm of active learning. 2. Theoretical Foundations: The paper provides a detailed theoretical foundation for their proposed method including mathematical derivations and analyses. 3. Experimental Results: Comprehensive experimental results on various datasets support the claim that BalEntAcq outperforms other active learning methods consistently.  Weaknesses: 1. Limited Comparison: The paper lacks a detailed comparison with a wider range of existing active learning methods which could provide a more comprehensive understanding of the proposed methods strengths and limitations. 2. Societal Impact Discussion: The paper could benefit from discussing potential societal impacts or ethical considerations arising from the deployment of the proposed approach.  Questions: 1. Generalization: How well does the proposed BalEntAcq method generalize to different types of datasets beyond the ones tested in the experiments 2. Scalability: Can the BalEntAcq method scale well to larger datasets and more complex neural network architectures 3. Robustness: Were sensitivity analyses conducted to test the robustness of the proposed method to variations in hyperparameters or dataset characteristics  Limitations: 1. Experimental Scope: The experimental scope is limited to specific datasets and scenarios which may not fully capture the realworld diversity of active learning applications. 2. Ethical Considerations: While the paper mentions certain theoretical and technical aspects it lacks a detailed discussion on broader ethical implications or potential biases that could arise from the proposed approach. Overall the paper presents a promising contribution in the field of active learning with the introduction of BalEntAcq. However addressing the mentioned weaknesses and limitations could enhance the robustness and application potential of the proposed method.", "owZdBnUiw2": "Peer Review 1) Summary The paper introduces the Ample and Focal Network (AFNet) as a solution to the challenge of reducing computational costs while maximizing the use of frames in action recognition. AFNet comprises two branches the Ample Branch processing all input frames with lightweight computation and the Focal Branch focusing on salient frames guided by a Navigation Module. Results from these branches are fused to prevent information loss. The method demonstrates superior accuracy with fewer frames and efficient computation on various datasets. 2) Strengths  The paper presents a novel approach for action recognition addressing the tradeoff between computation and frame utilization.  AFNets twobranch design effectively processes more frames with less computational cost resulting in higher accuracy.  The dynamic selection strategy at intermediate features improves temporal modeling leading to enhanced accuracy with fewer frames.  Navigation Module and the fusion strategy contribute to efficient and effective frame selection.  Comprehensive experiments on multiple datasets validate the effectiveness and efficiency of AFNet compared to existing methods. 3) Weaknesses  While the experimental results are promising the paper could benefit from more detailed comparisons with stateoftheart methods especially in terms of computational efficiency.  The explanation of the GumbelSoftmax approach and the impact of different parameters could be elaborated for better understanding.  A clearer explanation of how the proposed method generalizes to different types of action recognition tasks and datasets could strengthen the papers impact. 4) Questions  How does AFNets performance compare to methods specifically designed for realtime action recognition scenarios  Can the proposed dynamic frame selection approach be extended to handle more complex action recognition tasks involving finegrained actions or subtle movements  How does the performance of AFNet vary with different network architectures besides ResNet50 and with varying complexities of the datasets 5) Limitations  The paper does not discuss the potential challenges or limitations of deploying AFNet in realworld applications such as hardware constraints or transferability to new datasets.  More insights into the generalization of AFNet across diverse action recognition tasks and dataset characteristics could further strengthen the proposed methods applicability.  Discussion on the scalability of the proposed navigation module and its computation intensity as datasets expand or tasks become more complex would be beneficial. Overall the paper introduces a promising approach in action recognition with AFNet showcasing efficiency and effectiveness in utilizing frames for improved accuracy. Addressing the questions and limitations raised could enhance the clarity and robustness of the proposed method for practical deployment and broader applicability in the field of action recognition.", "kpSAfnHSgXR": " Peer Review  1) Summary The paper introduces binary code box embeddings a novel generalized box embedding method for modeling directed graphs. The model aims to capture the salient information from directed graphs containing cycles and transitivity. Theoretical and empirical results demonstrate the models ability to represent arbitrary directed graphs outperforming other baseline methods in graph reconstruction and link prediction tasks across various scenarios.  2) Strengths and Weaknesses Strengths:  Innovative Approach: Introducing binary code box embeddings is a novel and innovative approach to modeling directed graphs.  Theoretical Foundation: The paper provides a strong theoretical foundation proving the models capacity to represent arbitrary directed graphs.  Empirical Results: Empirical evaluations on both synthetic and realworld graphs show superior performance of the proposed models.  Comparison: Comprehensive comparison with baseline methods demonstrates the effectiveness and superiority of the proposed models in various scenarios. Weaknesses:  Complexity: The paper is dense with technical details and requires a strong background in graph embeddings and neural networks to fully grasp.  Model Interpretability: The interpretability of the binary code box embeddings may pose challenges for certain users in understanding the inner workings of the model.  Limitations: Although the models perform well in most scenarios the discussion on limitations could have been more detailed to provide a clearer understanding of potential weaknesses.  3) Questions  How does the choice of hyperparameters like temperature (\\xe2\\x8c\\xa7 \\xe2\\x8c\\xab) impact the performance and capacity of the binary code box embeddings  Can the binary code box embeddings be extended to handle dynamic graphs or graphs with evolving structures over time  How does the dimensionality of the binary codes affect the models ability to represent different types of graphs especially those with varying levels of cycles and transitivity  4) Limitations  Model Complexity: The complexity of the binary code box embeddings and the need for tuning parameters may limit widespread adoption and practical implementation.  Scalability: The scalability of the model to largescale graphs or dynamic networks may be a potential limitation especially considering the computational requirements.  Overall Comments The paper presents a strong contribution to the field of graph embeddings by introducing binary code box embeddings with proven theoretical foundations and promising empirical results. The work addresses the challenging task of modeling directed graphs with cycles and transitivity showcasing the superiority of the proposed models across various experiments. Clearer explanations of limitations and potential future directions could enhance the impact and practical applicability of the research.", "r-6Z1SJbCpv": " Peer Review:  Summary: The paper introduces OPTFORMER a textbased Transformer Hyperparameter Optimization (HPO) framework capable of learning policies and function priors across diverse search spaces. The framework is trained on vast tuning data from Googles Vizier database and demonstrated to imitate various HPO algorithms provide accurate function predictions and improve optimization performance. Extensive experiments on public and private datasets showcase OPTFORMERs competitive abilities suggesting it as a promising approach for metalearning HPO algorithms.  Strengths: 1. Novel Approach: Introducing a textbased Transformer framework for HPO is an innovative and promising direction in the field. 2. Extensive Experiments: The paper conducts comprehensive experiments on both public and private datasets to demonstrate the capabilities of OPTFORMER. 3. Competitive Performance: OPTFORMER shows competitive results when compared to existing HPO algorithms especially in terms of accuracy and calibration of predictions. 4. Generalization: The framework shows good generalization performance beyond the optimization horizon it is trained for indicating its robustness.  Weaknesses: 1. Dataset Limitation: The study heavily relies on Googles proprietary Vizier database potentially limiting accessibility and reproducibility for researchers without access to such resources. 2. Complexity: The paper introduces several novel concepts such as function uncertainty estimates and prior over objective functions which may require more detailed explanations for better understanding. 3. Inference: While the results are promising a deeper investigation into the inference mechanisms and computational complexity of OPTFORMER could provide valuable insights.  Questions: 1. The paper mentions imitating multiple HPO algorithms  how does OPTFORMER prioritize learning from diverse algorithms and does it show any preference for a particular type of algorithm 2. Could the authors elaborate on how the OPTFORMER framework handles the challenge of varying search spaces and how this aspect affects its generalizability across different tasks 3. Regarding the training datasets how sensitive is OPTFORMER to the nature of the data it is trained on and are there specific characteristics of training data that significantly influence its performance  Limitations: 1. Accessibility: The reliance on Google\u2019s Vizier database may limit the broader adoption of the OPTFORMER by researchers without access to proprietary datasets. 2. Complexity: The advanced concepts introduced may pose a barrier to understanding for readers unfamiliar with Transformerbased architectures and HPO algorithms. 3. Scope: The paper focuses on singleobjective functions and sequential optimization extending the framework to support multiobjective functions and parallel optimization could enhance its applicability to a wider range of problems.  Overall Assessment: The paper presents a novel approach in the field of HPO with the introduction of OPTFORMER a textbased Transformer framework. While the framework shows promising results in imitating various HPO algorithms and providing accurate function predictions there are several considerations regarding dataset accessibility complexity of concepts and scope for future extensions that could be addressed to enhance the impact and applicability of OPTFORMER in diverse HPO scenarios.", "yNPsd3oG_s": " Peer Review:  1) Summary: The paper addresses the significant issue of backdoor attacks on deep neural networks (DNNs) by introducing a novel training method called NONE which effectively mitigates both injected and natural Trojans. The research identifies the fundamental differences between backdoorrelated neurons and benign neurons and proposes a training algorithm that forces training to avoid generating hyperplanes associated with backdoors. Extensive experiments on five datasets demonstrate that NONE outperforms existing defense methods against various Trojan attacks. The paper provides valuable insights into DNN Trojans and offers a robust defense mechanism.  2) Strengths:  The paper thoroughly investigates the problem of DNN Trojans and provides a clear theoretical framework to understand their nature.  The proposed training method NONE is innovative and effective in defending against both injected and natural Trojans showcasing superior performance compared to existing defenses.  Extensive experiments on multiple datasets and Trojan attacks validate the effectiveness and efficiency of the NONE method.  The inclusion of detailed discussions methodology and experimental results enhances the papers credibility and contribution to the field of adversarial machine learning.  3) Weaknesses:  While the paper provides a comprehensive analysis of DNN Trojans and proposes a novel defense strategy the technical complexity and depth of the content may be challenging for readers with limited expertise in the field of deep learning.  The paper would benefit from clearer explanations and illustrations to aid in understanding complex concepts related to DNN Trojans and the proposed defense mechanism.  4) Questions:  Can the paper elaborate further on how the NONE method specifically detects and addresses natural Trojans compared to injected Trojans  How does the proposed training algorithm balance defense against Trojans with preserving the models performance on benign tasks  Are there considerations for the scalability and computational cost of implementing the NONE method on larger or more complex DNN models and datasets  5) Limitations:  The paper primarily focuses on image classification tasks in the context of DNN Trojans limiting the generalizability of the proposed defense method to other domains such as natural language processing or reinforcement learning.  The robustness of the NONE method against adaptive attacks and evolving threat models remains a potential area for further investigation and validation. Overall the paper presents significant contributions to the field of adversarial machine learning by addressing the critical issue of DNN Trojans and proposing a novel defense mechanism. Further clarity in explanations considerations for broader applications and additional validation under diverse attack scenarios would enhance the researchs impact and relevance.", "ocViyp73pFO": " Peer Review  1) Summary The paper addresses the challenge of disentangling the effects of single interventions from jointly applied interventions considering the presence of unobserved confounders. They demonstrate identification under certain conditions for nonlinear continuous structural causal models with additive multivariate Gaussian noise. The proposed algorithm learns causal model parameters by pooling data and maximizing likelihood with empirical validation on synthetic and realworld data.  2) Strengths and weaknesses Strengths:  Clear motivation and problem statement highlighted at the beginning.  Comprehensive literature review situating the work in the context of existing research.  Theoretical contributions with proofs on identifiability in specific causal model classes.  Detailed methodology for SCM estimation addressing noisy parameters.  Empirical validation on synthetic and real data for validation. Weaknesses:  The paper could provide more explicit comparisons with existing methods in terms of performance metrics.  The empirical validation could benefit from more detailed discussion and analysis of results.  The paper may be strengthened by further exploration or validation on additional realworld datasets.  3) Questions  Can the algorithm handle more complex scenarios with higherdimensional observational data and interventions  How sensitive is the method to the assumptions made about the structural causal models  Are there any scalability issues when dealing with larger datasets or more intricate causal structures  4) Limitations  The study focuses on specific classes of nonlinear continuous structural causal models with additive Gaussian noise limiting generalizability to other types of models.  The empirical validation although with real data could benefit from more diverse datasets to see the methods performance in various domains.  The papers conclusions highlight identifiability in certain scenarios but could delve deeper into practical implications and future research directions. Overall the paper contributes to the field of causal inference by addressing a challenging problem and providing theoretical insights along with a practical algorithm for disentangling intervention effects. Further validation and exploration could enhance the impact and applicability of the proposed method.", "qf12cWVSksq": " Peer Review: Summary: The paper introduces a novel architecture called Inception Transformer (iFormer) that integrates the advantages of Convolutional Neural Networks (CNNs) with Transformers to capture high and lowfrequency information in visual data. The key components Inception mixer and frequency ramp structure aim to address the limitations of existing Vision Transformers (ViTs) in capturing highfrequencies. The iFormer is evaluated on image classification object detection and segmentation tasks showcasing superior performance compared to stateoftheart models. Strengths: 1. Innovation: The Inception Transformer architecture introduces a unique approach to combining CNNs with Transformers to address the limitation of ViTs. 2. Efficiency: The proposed channel splitting mechanism and frequency ramp structure optimize the tradeoff between high and lowfrequency components. 3. Performance: Extensive experiments demonstrate that the iFormer outperforms existing ViTs and CNNs on various vision tasks indicating its effectiveness. 4. Clarity: The paper is wellstructured providing detailed descriptions of the architecture experimental setup and results. Weaknesses: 1. Validation: While the paper presents significant improvements in performance additional analysis on the robustness of the iFormer across different datasets or under adversarial settings could strengthen the findings. 2. Generalization: The evaluation is focused on image classification object detection and segmentation tasks. It would be beneficial to explore the applicability of iFormer in other computer vision tasks. 3. Scalability: The scalability of iFormer with respect to larger datasets and higherresolution images is not thoroughly discussed. How the architecture performs when trained on diverse datasets could be explored for a comprehensive evaluation. 4. Baseline Comparison: The comparison with some recent stateoftheart hybrid architectures could provide a more comprehensive understanding of the iFormers performance. Questions: 1. Robustness: How does iFormer perform under noisy or adversarial conditions Have there been any tests conducted in such scenarios 2. Scalability: How does the iFormer scale with larger datasets for tasks like object detection and segmentation Are there any scalability limitations observed during experiments 3. Generalization: Have there been any experiments performed on datasets other than ImageNet COCO and ADE20K to assess the generalization capability of iFormer across diverse visual tasks Limitations: 1. Manual Design:  The requirement for manual definition of channel ratios in the iFormer blocks might limit its applicability across different tasks without domainspecific expertise. 2. Dataset Size:  The limitation in training on largescale datasets like ImageNet21K could potentially affect the generalizability of the model. In conclusion the paper presents a novel Inception Transformer architecture that effectively addresses the limitations of existing ViTs in capturing high frequencies. With promising results in image classification object detection and segmentation tasks additional analysis on robustness scalability and generalization could further enhance the understanding of iFormers capabilities.  This peer review provides a detailed assessment of the strengths weaknesses questions and limitations of the presented paper on Inception Transformer (iFormer). It highlights the innovation efficiency and performance of the iFormer architecture while suggesting areas for further exploration and validation.", "q2nJyb3cvR9": " Peer Review: 1) Summary: The paper introduces an algorithm Single Seed Randomization (SSR) that utilizes randomized value functions for exploration in reinforcement learning. The algorithm is shown to achieve a nearoptimal regret bound of \\xc3\\x95 ( H \\xe2\\x88\\x9a SAT ) in episodic timeinhomogeneous Markov Decision Processes matching the lower bound up to logarithmic factors. SSR incorporates a novel approach of using a single random seed for each episode leading to improved exploration efficiency and stability. The analysis involves a new clipping method for bounding the estimated value function as well as a comparison of Hoeffdingtype and Bernsteintype noise magnitudes. 2) Strengths:  The paper presents a wellstructured and detailed analysis of the proposed algorithm presenting the theoretical foundations and technical contributions clearly.  SSR introduces innovative algorithmic ideas such as the use of a single random seed and a new clipping strategy which improve exploration efficiency and maintain the desired properties of the estimated value function.  The theoretical results are rigorous and provide insights into achieving nearoptimal regret bounds in randomized exploration algorithms for reinforcement learning. Weaknesses:  The paper could benefit from a more concise presentation of the algorithm and analysis with focus on the key contributions and results to enhance readability.  While the theoretical analysis is thorough the practical implications and experimental validation of SSRs performance could be explored further to strengthen the algorithms applicability.  The complexity of the regret analysis and technical proofs might limit the accessibility of the paper to a broader audience. 3) Questions:  How does the Single Seed Randomization algorithm compare to existing stateoftheart reinforcement learning algorithms especially in scenarios with more complex environments or large actionstate spaces  Could the proposed algorithm be extended or adapted to function approximation settings and are there specific challenges or considerations in applying SSR to more practical realworld scenarios  Is there any insight into how the proposed clipping strategy would behave in dynamic environments or scenarios with nonstationary distributions 4) Limitations:  The focus on theoretical analysis and regret bounds in tabular MDPs may limit the generalizability and practical applicability of the algorithm to more complex and diverse reinforcement learning scenarios.  The experimental validation provided in the paper is limited to the deepsea environment overlooking a broader evaluation in various environments or benchmark tasks to assess the algorithms robustness and performance across different settings.  The reliance on theoretical proof outlines and analyses without extensive empirical validation may pose challenges in translating the algorithm into practical implementations and realworld applications. Overall the paper presents a novel algorithm for randomized exploration in reinforcement learning with a rigorous theoretical foundation. However further exploration into practical applications comparative evaluations and extensions to diverse RL scenarios could enhance the papers impact and relevance in the field.", "mhp4wLwiAI-": " Peer Review: 1) Summary: The paper investigates the trainability issues of deep Graph Convolutional Networks (GCNs) from the perspective of gradient flow and proposes novel methods to improve gradient flow including a topologyaware isometric initialization scheme and gradientguided dynamic rewiring with skip connections. Extensive empirical evidence across multiple datasets demonstrates that these methods significantly enhance the performance of deep vanillaGCNs making them competitive with stateoftheart methods. 2) Strengths:  The paper addresses an important problem of poor gradient flow in deep GCNs and proposes innovative solutions to improve trainability.  The proposed methods are theoretically grounded and empirically validated across multiple datasets showcasing significant performance improvements.  The extensive analysis and visualization of gradient flow and Dirichlet Energy provide valuable insights into the training dynamics of deep vanillaGCNs.  The paper is wellstructured with clear explanations of methodologies experimental setups and results. 3) Weaknesses:  The paper could benefit from a more detailed discussion on the computational complexity of the proposed methods particularly in practical largescale settings.  While the empirical results are compelling additional insights into the behavior of the methods under different hyperparameters or network architectures would strengthen the study.  The paper lacks a comparative analysis of the proposed methods with recent related works that address the trainability of deep GCNs which could provide additional context for the significance of the contributions. 4) Questions:  How do the proposed topologyaware isometric initialization and dynamic rewiring methods generalize across different types of graph datasets with varying properties and structures  Are there specific scenarios or network configurations where the proposed methods may not be as effective and if so what are the potential limitations  Could the paper provide more insights into the interpretability of the gradient flow analysis such as how the gradient flow patterns correspond to the topology or characteristics of the input graph 5) Limitations:  The generalizability of the proposed methods to diverse graph datasets and architectures could be further explored to understand their robustness in realworld applications.  The paper focuses on improving trainability and performance metrics but a more indepth analysis of interpretability and robustness to noise or adversarial settings could enrich the study.  Additional experiments or case studies on the scalability and efficiency of the methods in largescale graph settings would enhance the practical applicability of the proposed techniques. Overall the paper presents novel and promising approaches to address the trainability issues of deep GCNs by improving gradient flow. Further exploration into the scalability generalizability and interpretability aspects of the proposed methods would strengthen the impact and relevance of the study in the field of graph neural networks.", "pHd0v8W30O": "Peer Review: Summary: The paper introduces a novel method LatentVariable AdvantageWeighted Policy Optimization (LAPO) for addressing the challenge of learning policies from heterogeneous offline datasets in the field of reinforcement learning. The work proposes to leverage a latentvariable generative model to represent highadvantage stateaction pairs aiming to improve adherence to data distributions and maximize reward via a policy over the latent variable. Through empirical evaluation on simulated tasks LAPO is shown to outperform existing offline reinforcement learning methods especially on heterogeneous datasets achieving an average performance improvement of 49. Strengths: 1. Novel Method: LAPO introduces a unique approach that effectively tackles the challenges posed by learning from heterogeneous datasets in offline reinforcement learning. 2. Empirical Evaluation: The paper provides thorough experimental results on a variety of tasks showcasing the superior performance of LAPO compared to existing methods. 3. Clear Explanation: The paper effectively details the methodology algorithm and experimental setup enabling readers to understand the proposed approach. 4. Importance of Latent Policy: The study includes ablation studies that highlight the significance of the latent policy training in achieving optimal performance. Weaknesses: 1. Lack of RealWorld Evaluation: The experiments are conducted on simulated tasks and it would be beneficial to include realworld robotic applications for a more comprehensive evaluation. 2. Comparative Metrics: While the paper mentions improvements in performance compared to existing methods additional comparative metrics or analysis could provide a more detailed understanding of the results. Questions: 1. How does the proposed LAPO method handle issues such as overfitting or generalization when applied to realworld scenarios with complex robotic tasks 2. Are there any specific limitations or challenges encountered during the implementation of the LAPO algorithm on the simulated locomotion navigation and manipulation tasks 3. Can the authors elaborate on the scalability of the LAPO method in terms of computational complexity and training time especially when dealing with larger datasets or more complex environments Limitations: 1. Generalizability: The findings are primarily based on simulated tasks and the generalizability of the proposed LAPO method to realworld robotic applications remains to be confirmed. 2. Experimental Scope: The experiments are focused on offline reinforcement learning and exploring the applicability of LAPO in online or semionline scenarios could further enhance the studys impact. Overall the paper presents a notable contribution to the field of offline reinforcement learning with the introduction of LAPO. Further exploration and validation in realworld settings could strengthen the credibility and applicability of the proposed method.", "ouXTjiP0ffV": "Peer Review: 1) Summary: The paper introduces Neural Correspondence Prior (NCP) a novel unsupervised paradigm for computing correspondences between 3D shapes. The authors demonstrate that untrained neural networks can produce pointwise features that are on par with complex local descriptors leading to higher quality correspondences. The proposed twostage algorithm significantly improves the accuracy of maps especially for nonrigid and point cloud matching scenarios. The approach leverages the neural correspondence prior effect to denoise maps and achieve stateoftheart results. Results showcase the effectiveness and generality of the algorithm across various shapematching tasks. 2) Strengths and weaknesses: Strengths:  The paper proposes a novel unsupervised paradigm for 3D shape correspondence leveraging the neural correspondence prior effect.  The twostage algorithm improves the accuracy of correspondences especially in challenging nonisometric matching scenarios.  Extensive experiments on different datasets demonstrate the effectiveness and generality of the proposed approach.  The paper provides detailed explanations of the neural correspondence prior effect and the methodological approach.  The results show significant improvements over stateoftheart methods in various shapematching tasks. Weaknesses:  The paper could benefit from further discussions on the limitations and potential failure cases of the proposed algorithm.  While the method achieves stateoftheart results a more detailed analysis of the computational complexity and scalability would be beneficial.  The discussion of the impact of hyperparameters and potential pitfalls during the training process could provide additional insights for the readers.  The paper could include a more detailed comparison with existing related works to highlight the unique contributions of the proposed approach. 3) Questions:  How does the proposed method perform in cases where ground truth maps are not available or where the ground truth contains errors  What are the key factors influencing the performance differences observed between the proposed method and existing stateoftheart methods in certain tasks  How robust is the proposed algorithm to variations in dataset sizes and shapes  Have you explored the generalizability of the algorithm to unseen datasets not captured in the experiments 4) Limitations:  The described approach assumes the presence of some structure in the initial pointtopoint maps used for training which may limit its effectiveness in scenarios where the initial matches are entirely random or lack structure.  The paper could elaborate on the potential challenges in realworld applications such as dealing with noisy input data or ambiguous correspondences which could impact the performance of the method.  The scalability and computational requirements of the proposed algorithm in more extensive datasets or realtime applications remain to be addressed. Overall the paper presents a significant contribution to the field of 3D shape matching through the introduction of the Neural Correspondence Prior paradigm. The approach shows promise in addressing challenging scenarios like nonrigid and point cloud matching and the extensive experimental results provide strong support for the proposed algorithm. Further discussions on potential limitations and thorough comparisons with existing methods would enhance the clarity and impact of the research findings.", "wlrYnGZ37Wv": " Peer Review  1) Summary: The paper proposes a novel architecture called Sequencer as an alternative to Vision Transformer (ViT) for computer vision tasks. It utilizes the Long ShortTerm Memory (LSTM) for sequence modeling instead of selfattention layers. The proposed architecture demonstrates impressive performance on ImageNet1K with good transferability and robust resolution adaptability. It outperforms other architectures with a comparable number of parameters and shows promise for practical image recognition models.  2) Strengths and Weaknesses: Strengths:  Proposes a novel architecture (Sequencer) that leverages LSTM for computer vision tasks.  Demonstrates competitive performance on ImageNet1K with good transferability and robust resolution adaptability.  Outperforms other architectures with comparable sizes on benchmarks.  Provides insightful ablation studies and analysis of the proposed architecture along with experiments on transfer learning and semantic segmentation. Weaknesses:  Limited comparison with a broader range of stateoftheart models beyond the selected few.  The paper could benefit from further analysis on scalability computational efficiency and realworld applicability.  The discussion on the generality of the proposed architecture beyond specific datasets and tasks could be more detailed.  Lack of indepth comparison with other recent LSTMbased architectures in computer vision.  3) Questions:  How does the Sequencer architecture compare to other LSTMbased models in computer vision especially considering different types of RNNs or attention mechanisms  Are there specific limitations or challenges faced during training or implementation of the Sequencer model that need to be addressed  How does the proposed architecture perform on other diverse computer vision tasks beyond image recognition  Can additional experiments or validations be performed to further demonstrate the versatility and scalability of the Sequencer architecture  4) Limitations:  The study may have a bias towards highlighting the strengths of the proposed Sequencer architecture without delving deeply into potential limitations or failure cases.  The evaluation and comparison with only a few selected architectures may narrow the scope of understanding the broader landscape of computer vision models.  The lack of demonstration on largescale datasets beyond ImageNet1K may limit the generalizability of the findings to varied realworld applications in computer vision. Overall the paper presents an intriguing approach with the Sequencer architecture for computer vision tasks. However addressing the mentioned weaknesses and limitations along with conducting further experiments and analysis can enrich the understanding and impact of the proposed architecture in the field of computer vision research.", "vfR3gtIFd8Y": "Peer Review Summary: The paper introduces a new method of forward variable selection for the KarhunenLo\u00e8ve decomposed Gaussian processes specifically focusing on the Bayesian Smoothing Spline ANOVA kernel (BSSANOVA). The method effectively limits the number of terms in the expansion balancing model fidelity and complexity. The approach is demonstrated on dynamic datasets showing competitive accuracies and inference speeds compared to other methods. The results show promising outcomes for dynamic systems modeling outperforming neural networks and other scalable Gaussian processes. Strengths: 1. The paper addresses an important issue of variable selection in KarhunenLo\u00e8ve decomposed Gaussian processes offering a new forward selection method. 2. The inclusion of detailed comparisons across static and dynamic datasets with various models provides a comprehensive evaluation of the proposed methods performance. 3. The speed and accuracy in modeling dynamic systems is emphasized as a strong point of the algorithm showcasing its potential utility in practical applications. 4. The use of Bayesian criteria for model complexity and regularization is a strong methodological choice enhancing the credibility of the results and the approach. Weaknesses: 1. The paper could benefit from more detailed information on the limitations and assumptions made in the study particularly in the context of scalability to larger datasets. 2. While the comparisons with other methods are informative additional analysis on the robustness of the proposed approach under different scenarios or noise levels could add depth to the evaluation. 3. Clearer explanations of the computational burden and resource requirements for implementing the proposed method would be valuable for readers assessing feasibility in practical applications. 4. Including a discussion on the interpretability of the model results and the tradeoffs made in terms of accuracy versus model complexity could enhance the practical implications of the study. Questions: 1. Can you elaborate on the generalizability of the forward variable selection method proposed in the paper to other types of datasets or applications beyond the ones tested 2. How does the proposed approach handle potential overfitting in dynamic systems modeling compared to other models especially in situations with limited training data 3. In practical implementation how sensitive is the proposed method to hyperparameter tuning and are there any strategies suggested for effective hyperparameter optimization 4. Can you provide insights into any potential challenges or hurdles faced during the development and validation of the new algorithm Limitations: 1. The evaluation of the proposed method is mainly based on two specific dynamic datasets potentially limiting the generalizability of the findings to a broader range of applications. 2. The analysis lacks a thorough discussion on the interpretability of the model results making it challenging for practitioners to understand the implications of the models decisions in realworld scenarios. 3. The study might benefit from a more extensive exploration of the scalability of the proposed method to larger datasets or more complex problems providing a clearer understanding of its limitations in practical applications. 4. The lack of realworld case studies or applications beyond toy problems limits the applicability and generalizability of the proposed approach to more diverse scenarios. Overall the paper presents an innovative approach for variable selection in KarhunenLo\u00e8ve decomposed Gaussian processes with promising results in dynamic systems modeling. Addressing the highlighted strengths weaknesses questions and limitations could further enhance the clarity and impact of the research.", "vNrSXIFJ9wz": "Peer Review: 1) Summary: The paper presents a novel approach called Vertically Federated Participant Selection (VFPS) to address the challenges of scalability and diminishing returns in Vertical Federated Learning (VFL). The focus is on selecting a subset of participants that jointly preserve the most mutual information with the label optimizing the process using VFMINE and a group testingbased framework. The experiments demonstrate the efficiency and effectiveness of VFPS compared to traditional methods. 2) Strengths:  The paper addresses an important issue in VFL by proposing VFPS for participant selection providing a new perspective on model training.  The technical contributions VFMINE and VFPS are well explained with clear algorithm descriptions and illustrations.  The paper provides detailed security analysis which is crucial for the privacypreserving nature of federated learning.  Experimental results are comprehensive comparing VFPS with baselines bruteforce strategy and ablation studies showcasing the effectiveness and efficiency of the proposed method. 3) Weaknesses:  While the paper explains the algorithms in detail more visual aids could enhance the readers understanding of complex concepts especially in the implementation sections.  The limitations of the proposed VFPS framework could be expanded upon to provide a clearer idea of potential challenges or drawbacks.  The paper could benefit from a more elaborate discussion on the generalizability and potential applications of VFPS beyond the scope of the current study. 4) Questions:  How does VFPS handle scenarios where the importance of participants changes dynamically over different stages of model training  Can the proposed VFPS framework be extended to address other types of metrics besides mutual information for participant selection in VFL  In realworld largescale federated systems how scalable is the VFPS approach and are there any strategies to mitigate performance degradation in such scenarios 5) Limitations:  The study focuses primarily on mutual informationbased participant selection potentially limiting the generalizability of the approach to other types of federated learning tasks or scenarios.  The performance of VFPS may degrade in extremely largescale federated networks warranting further investigation into scalability and optimization strategies in such environments.  The reward assessment and fairness considerations in participant selection could benefit from more indepth exploration and analysis to ensure the integrity and equity of the selection process. Overall the paper presents a wellstructured study on participant selection in Vertical Federated Learning introducing a novel approach with significant potential implications in the field. Further investigations into scalability generalizability and potential extensions of the VFPS framework could enhance the impact and relevance of the proposed method in federated learning applications.", "pBJe5yu41Pq": " Peer Review of the Scientific Paper 1) Summary: The paper explores the importance of explicitly incorporating aleatoric uncertainty in Bayesian neural networks for improved predictive performance. It introduces the concept of aleatoric uncertainty in the context of classification problems and discusses how standard Bayesian approaches may misrepresent this uncertainty. The paper proposes techniques like likelihood tempering and the noisy Dirichlet model to better capture aleatoric uncertainty and mitigate the cold posterior effect. Experimental results on image classification tasks validate the effectiveness of these methods and highlight the impact of data augmentation on predictive performance. 2) Strengths and Weaknesses: Strengths:  The paper addresses a critical aspect of uncertainty in neural networks focusing on aleatoric uncertainty.  It provides a comprehensive theoretical analysis clearly explaining the concepts and methodologies proposed.  Experimental validation on image classification tasks demonstrates the practical relevance and effectiveness of the proposed techniques.  The paper offers a novel perspective on data augmentations impact on aleatoric uncertainty providing valuable insights for practitioners and researchers. Weaknesses:  The paper could benefit from more detailed explanations of the experimental setups hyperparameters and model architectures used in the image classification experiments.  While the theoretical framework is welldeveloped some sections may require simplification for broader understanding by readers not deeply familiar with Bayesian methods.  Discussion on potential limitations or challenges faced in implementing the proposed methods in realworld scenarios would enhance the practical applicability of the research. 3) Questions:  How do the proposed techniques for incorporating aleatoric uncertainty compare with existing methods in terms of computational complexity and scalability  Can the findings from the paper be extrapolated to other domains beyond image classification such as natural language processing or reinforcement learning  Have you considered conducting user studies or validation experiments with domain experts to assess the practical utility and interpretability of the proposed models in realworld applications 4) Limitations:  The paper primarily focuses on image classification tasks limiting the generalizability of the findings to a broader range of applications.  The complexity of the theoretical framework and mathematical derivations might pose challenges for readers without a strong background in Bayesian methods.  While the experiments showcase the effectiveness of the proposed techniques further exploration on diverse datasets and tasks could strengthen the research outcomes. In conclusion the paper makes a significant contribution to the understanding of aleatoric uncertainty in Bayesian neural networks and offers valuable insights for improving predictive performance. Addressing the identified weaknesses and limitations could enhance the papers impact and applicability in the field of machine learning.", "tfkeJG9yAX": " Summary: The paper discusses the fundamental limits of the expressive power of neural networks by providing lower bounds on the approximation error in Lp norm for different sets of functions. It introduces a generic lower bound theorem and applies it to specific cases such as H\u00f6lder balls and multivariate monotonic functions. The study aims to understand the limitations of neural network approximation properties in different norm settings shedding light on similarities and differences between approximation in Lp norm and sup norm.  Strengths: 1. Rigorous Analysis: The paper presents a rigorous analysis of the lower bounds on the approximation error providing theoretical foundations for understanding the expressive power of neural networks. 2. Theoretical Contributions: The paper introduces a generic lower bound theorem and applies it to specific sets of functions providing insights into the approximation properties in Lp norm. 3. Clear Structure: The paper is wellstructured with clear sections definitions and notations making it easy to follow the arguments and results presented.  Weaknesses: 1. Limited Empirical Validation: The paper lacks empirical validation or practical examples to demonstrate the implications of the theoretical results on realworld neural network applications. 2. Complexity: The paper involves complex mathematical derivations and concepts which may be challenging for readers without a strong mathematical background. 3. Future Directions: While the paper lays a solid theoretical foundation it could benefit from discussing potential future research directions or practical applications of the results presented.  Questions: 1. How do the proposed lower bounds on the approximation error in Lp norm compare with existing methods or bounds in neural network approximation theory 2. Are there specific practical applications or scenarios where the theoretical results presented in the paper could have significant implications for designing neural network architectures 3. What are the implications of the theorem and corollaries presented in the paper for the design and optimization of neural networks in realworld applications particularly in terms of network architecture and training strategies  Limitations: 1. Theoretical Emphasis: The paper heavily focuses on theoretical derivations and proofs lacking empirical validation or practical examples to illustrate the realworld impact of the results. 2. Complexity: The mathematical complexity and theoretical nature of the paper may limit its accessibility and applicability to broader audiences particularly those without a strong mathematical background. 3. Scope: The paper primarily focuses on lower bounds for neural network approximation error leaving room for further exploration of upper bounds practical implementations or extensions to more diverse network architectures or sets of functions.", "oDoj_LKI3JZ": " Summary: The paper introduces a novel algorithm for sparse Gaussian process regression that leverages MCMC to sample from the hyperparameter posterior within the variational inducing point framework. It compares the proposed scheme against baseline methods and stochastic variational GPs. The study focuses on Bayesian treatment of hyperparameters in sparse GPs and its practical benefits. The methodology is evaluated on various datasets showcasing improved performance and computational efficiency.  Strengths: 1. Novel Algorithm: Introducing a new algorithm for sparse Gaussian process regression is a significant contribution. 2. Theoretical Foundation: Wellgrounded in theory using Bayesian inference and MCMC methods. 3. Detailed Experimentation: Extensive computational analysis and comparison with baseline methods on various datasets. 4. Practical Relevance: Addressing a practical need for handling hyperparameter uncertainty in Gaussian processes. 5. Clear Presentation: Wellorganized sections and detailed explanations enhance understanding. 6. Quantitative Evaluation: Providing detailed performance metrics and comparisons for the proposed method.  Weaknesses: 1. Complexity: The methodology may be complex for readers less familiar with Bayesian inference and MCMC methods. 2. Experimental Setup: The description of experimental setups and datasets could be more detailed. 3. Clarity in Results: Some sections particularly regarding results may benefit from clearer interpretation or visual aids. 4. Generalizability: The study could benefit from discussing the generalizability of the proposed algorithm to different scenarios or datasets.  Questions: 1. How do the computational costs of the proposed scheme compare to baseline methods especially in large dataset scenarios 2. Do the findings suggest any specific recommendations for practitioners working with sparse GPs and hyperparameter uncertainty 3. What are the implications for realworld applications of Gaussian processes considering the results and implications of this study 4. How sensitive is the proposed algorithm to changes in hyperparameter settings or inducing point selection  Limitations: 1. Limited Generalizability: The study focuses on specific scenarios and datasets raising questions about the general applicability of the proposed algorithm. 2. Complexity of Implementation: The MCMC sampling approach may pose challenges in practical implementation especially for users inexperienced with these techniques. 3. Lack of Robustness Analysis: While extensive experiments are conducted a deeper analysis of the robustness of the proposed method could strengthen the study. 4. Scalability: Any limitations related to the scalability of the proposed scheme especially in highdimensional or largescale dataset settings need to be addressed.", "yW5zeRSFdZ": " Peer Review 1) Summary: This paper delves into the issue of outlier suppression in transformer quantization focusing on addressing outliers in LayerNorm structures within NLP models. The study identifies the role of the scaling parameter \u03b3 as an amplifier for outliers and proposes two components Gamma Migration and TokenWise Clipping to effectively suppress outliers. Experimental results demonstrate the frameworks superiority in pushing 6bit posttraining BERT quantization to fullprecision levels. 2) Strengths and Weaknesses: Strengths:  Thorough investigation of outliers inducement and impact on quantization performance.  Innovative approach using Gamma Migration and TokenWise Clipping to suppress outliers effectively.  Extensive experiments across various NLP models and tasks demonstrating the frameworks superiority.  Code availability on GitHub enhances reproducibility. Weaknesses:  Some sections contain dense technical details that might be challenging for nonexperts to understand.  The paper lacks comparative analysis with stateoftheart outlier suppression methods in other domains beyond NLP. 3) Questions:  Can the outlier suppression framework be easily integrated with existing quantization techniques in realworld applications  How sensitive is the framework to variations in model architectures apart from BERT RoBERTa and BART  Are there any plans to extend the research into outlier suppression in different domains beyond NLP models 4) Limitations:  The study focuses primarily on NLP models limiting the generalizability of the outlier suppression framework.  The indepth technical nature of the paper may limit its accessibility to a wider audience.  Limited discussion on the practical implications and potential challenges of implementing the proposed outlier suppression framework in resourceconstrained environments. Overall the paper presents a novel approach to tackle outliers in transformer quantization within NLP models. The experimental results are promising showcasing the efficacy of the proposed framework in enhancing quantization performance. However the paper could benefit from broader discussions on the implications of outlier suppression in diverse domains and insights into realworld deployment challenges.", "ttC9p-CtYT": " Summary The paper introduces a new family of quadratic functions based on the normalized advantage functions (NAF) algorithm aimed at solving reinforcement learning problems obtained through the timediscretization of optimal control problems. The authors prove the approximation properties of this new family of functions and present ways to enhance the NAF algorithm. Experimental results demonstrate the efficiency of the proposed improvements.  Strengths  Novelty: The paper introduces a new family of quadratic functions and provides theoretical results to support their effectiveness in solving optimal control problems.  Experimental Validation: The authors conduct experiments to validate the efficiency of the proposed algorithms.  Clarity of Presentation: The paper is wellstructured and clearly explains the theoretical concepts and experimental findings.  Weaknesses  Limited Scope: The focus on a specific class of reinforcement learning problems obtained through timediscretization may limit the generalizability of the proposed solutions.  Lack of Comparison: While the experiments show the efficiency of the proposed algorithms a more comprehensive comparison with existing stateoftheart methods would strengthen the paper.  Questions  Generalization: How applicable are the proposed algorithms and the new family of functions to a broader range of reinforcement learning problems beyond the specific class considered in the paper  Scalability: Have the authors considered the scalability of the proposed algorithms to larger and more complex environments  Limitations  Applicability: The focus on timediscretization of optimal control problems may restrict the applicability of the proposed solutions to a narrow set of scenarios.  Evaluation: The experimental evaluation while demonstrating efficiency could be enhanced by evaluating the proposed algorithms across a wider range of tasks. Overall the paper presents a valuable contribution by introducing a new family of quadratic functions for enhancing the NAF algorithm in solving reinforcement learning problems derived from optimal control scenarios. Further exploration of the generalizability and scalability of the proposed solutions would strengthen the papers impact in the field.", "zbt3VmTsRIj": " Peer Review:  Summary: The paper introduces a novel singleloop algorithm for Inverse Reinforcement Learning (IRL) that aims to recover the reward function and associated optimal policy. The algorithm is designed to avoid the computational burden of nested loop structures maintains accuracy in reward estimation and provably converges to a stationary solution in a finite time. Experimental evaluations demonstrate the superior performance of the proposed algorithm in policy estimation and reward recovery compared to existing IRL and imitation learning benchmarks especially in transfer learning settings.  Strengths: 1. Novelty: The paper introduces a unique singleloop algorithm for IRL addressing computational efficiency and maintaining reward estimation accuracy. 2. Theoretical Rigor: The algorithm is theoretically wellgrounded with finitetime convergence guarantees especially notable under nonlinear reward parameterization. 3. Experimental Validation: Extensive experiments are conducted on various tasks and settings demonstrating the superiority of the proposed algorithm over existing methods. 4. Transfer Learning: The algorithm performs well in transfer learning settings showcasing its robustness and potential applicability in realworld scenarios.  Weaknesses: 1. Complexity: The complexity of the proposed algorithm especially in terms of sampling trajectories and utilizing stochastic gradient estimators might limit its practical scalability and ease of implementation. 2. Stability Concerns: The potential instability observed in existing IRL methods like IQLearn raises concerns about the stability of the proposed algorithm particularly in realworld applications. 3. Experimental Design: The paper lacks detailed discussion on hyperparameter settings and sensitivity analysis which are crucial for algorithm optimization and performance evaluation.  Questions: 1. How sensitive is the proposed algorithm to hyperparameter choices and are there any guidelines provided for tuning these parameters 2. Can the algorithm be adapted for offline IRL settings to facilitate training on preexisting datasets without the need for realtime data collection 3. Are there any specific scenarios or tasks where the proposed algorithm might not perform as well and what are the potential limitations in practical applications 4. Could further insights be provided on the interpretability of the recovered reward functions and policies especially in complex and highdimensional environments  Limitations: 1. The paper lacks a detailed discussion on the interpretability of the recovered reward functions which is crucial for understanding the learning process and ensuring accurate reward recovery. 2. The practical implications of potential negative biases in expert demonstrations and methods to mitigate such biases are not extensively addressed warranting further investigation for safe deployment in realworld applications. Overall the paper presents significant advancements in the field of IRL with a welldefined algorithm and rigorous theoretical analysis. Addressing the identified weaknesses and limitations could further enhance the clarity reliability and applicability of the proposed algorithm in various domains.", "m8YYs8nJF3T": "Peer Review Summary: The paper develops a unified approach for analyzing Wasserstein distances in highdimensional contexts by introducing the Sliced Wasserstein Process leading to distributional limit theorems and statistical guarantees for various Wasserstein distances based on onedimensional projections. The work provides insights and results where no distributional limits were previously known demonstrating improvements in statistical performance. Strengths: 1. Novel Approach: The unified approach for analyzing different Wasserstein distances is a significant contribution and provides a comprehensive framework for statistical analysis. 2. Theoretical Rigor: The paper builds on solid theoretical foundations incorporating concepts from empirical process theory and demonstrating mathematical rigor in the proofs and derivations. 3. Simulation Studies: The simulation studies effectively demonstrate the applicability of the proposed method and provide insight into practical implementations. 4. Implications: The work has important implications for statistical inference and demonstrates the applicability of the methodology across various Wasserstein distances. Weaknesses: 1. Complexity: The paper may be challenging to digest for nonexperts in the field given the technical nature of the content and the reliance on advanced mathematical concepts. 2. Generalizability: The assumptions made particularly regarding compact support may limit the generalizability of the approach. Further exploration of scenarios where these assumptions are relaxed could strengthen the applicability of the method. 3. Clarity: Certain sections of the paper especially those involving derivations and mathematical proofs could benefit from improved clarity and explanatory details for better comprehension by a wider audience. Questions: 1. Extension Possibilities: Are there plans to extend this research to address scenarios where the compact support assumption is relaxed 2. Practical Implementation: How feasible is the application of the proposed methodology in realworld scenarios especially considering the computational complexity in highdimensional settings 3. Comparison to Existing Methods: How does the proposed unified approach compare to existing methods in terms of computational efficiency and statistical performance 4. Future Directions: What are the potential future research directions that could stem from this work in the context of Wasserstein distances and highdimensional analysis Limitations: 1. Assumption Dependency: The results heavily rely on assumptions like compact support which may limit the broad applicability of the proposed methodology. 2. Complexity in Implementation: Translating the theoretical findings into practical applications may pose challenges due to the complexity involved in highdimensional computations and statistical analyses. Overall the paper presents a noteworthy contribution to the field of Wasserstein distances offering a unified framework for statistical analysis in highdimensional contexts. Addressing the identified weaknesses and questions could further enhance the impact and applicability of the research.", "lblv6NGI7un": " Peer Review of \"Efficient Graph Similarity Computation without NodetoNode Matching\"  1) Summary: The paper introduces ERIC a novel framework for efficient graph similarity computation without the need for expensive nodetonode matching. The framework incorporates the Alignment Regularization (AReg) technique which imposes a nodegraph correspondence constraint during training and a multiscale Graph Edit Distance (GED) discriminator to enhance the quality of learned representations. Extensive experiments on realworld datasets demonstrate that ERIC outperforms stateoftheart methods in terms of effectiveness efficiency and transferability.  2) Strengths:  The paper addresses an important problem in graph similarity computation by proposing a novel framework that avoids the computational costs of nodetonode matching.  The introduction of AReg as a regularization technique and the multiscale GED discriminator contribute to the effectiveness and efficiency of the proposed model.  Extensive experiments on realworld datasets demonstrate the superior performance of ERIC compared to existing methods and showcase its transferability to other models.  The ablation studies and sensitivity analysis provide deeper insights into the components of the framework and the impact of various parameters enhancing the clarity of the proposed methodology.  3) Weaknesses:  While the paper presents a thorough methodology and comprehensive experimental evaluation additional insights on the interpretability of the learned representations or on the impact of hyperparameters during training could further enhance the understanding of ERIC.  The specific implementation details and potential limitations of the proposed model such as scalability to larger datasets or sensitivity to specific graph structures could be addressed to provide a more comprehensive evaluation.  4) Questions:  How does the proposed ERIC framework handle scalability issues when applied to larger datasets with a higher number of nodes  Can the authors provide more insights into the interpretability of the learned representations and how the model captures finegrained interactions between graphs  What are the implications of the findings in realworld applications such as drug discovery or social group identification and how does ERIC compare in these contexts  5) Limitations:  The paper does not delve deeply into the interpretability of the learned representations which could be crucial in understanding the decisionmaking process of the model.  There could be limitations in the transferability of AReg to all GNNbased models and further analysis on various architectures may provide more insights into its efficacy. Overall the paper presents a novel and efficient approach to graph similarity computation with promising results. Further studies on the scalability interpretability and generalizability of ERIC could provide a more comprehensive assessment of its applicability in realworld scenarios.", "wUUutywJY6": " Peer Review of the Paper \"Sinkhorn Label Refinery for LongTailed PartialLabel Learning\"  Summary: The paper introduces a novel framework called SoLar for addressing the challenging problem of longtailed partiallabel learning. In this research the authors identify the limitations of existing methods in handling label disambiguation in the presence of imbalanced class distributions and propose an optimal transportbased approach to refine pseudolabels while aligning with the marginal class prior distribution. SoLar incorporates a technique for estimating the class prior distribution and demonstrates superior performance compared to stateoftheart methods on standardized benchmarks.  Strengths: 1. Novel Framework: The introduction of SoLar presents a unique approach that considers the longtailed nature of data along with partiallabeling addressing a critical challenge in weaklysupervised learning. 2. Theoretical Foundations: The paper provides detailed theoretical underpinnings of the proposed method demonstrating statistical consistency and ensuring the optimality of the learned classifier. 3. Empirical Evaluation: The extensive experimental evaluation on various benchmark datasets showcases the superior performance of SoLar highlighting its effectiveness in handling longtailed and partiallylabeled data. 4. Code Availability: The availability of code and data on a public repository enhances the reproducibility of the results and promotes further research in the field.  Weaknesses: 1. Complexity: The paper introduces a sophisticated framework that involves several components such as optimal transport class prior estimation and sample selection. The complexity of the method may present challenges in implementation and practical application. 2. Generalization: While the proposed method demonstrates significant improvements on benchmark datasets the paper could provide further insights into the generalizability of SoLar across diverse domains and realworld applications. 3. Comparative Analysis: While the paper compares SoLar with stateoftheart methods a more indepth analysis of the strengths and weaknesses of these baselines could provide a clearer perspective on the performance gains achieved by SoLar.  Questions: 1. Can you elaborate on the computational overhead of the proposed SinkhornKnopp algorithm in practical implementation How does it scale with larger datasets 2. How sensitive is the performance of SoLar to the hyperparameters such as the smoothing regularization coefficient and the movingaverage parameter for class prior estimation 3. Have you considered conducting user studies to assess the usability and interpretability of the refined labels generated by SoLar in realworld applications  Limitations: 1. RealWorld Applications: The paper focuses primarily on benchmark datasets and it would be beneficial to explore the applicability and performance of SoLar in realworld scenarios with diverse data distributions and label ambiguities. 2. Scalability: While the experimental results are promising the scalability of the proposed approach to largescale datasets and computationally intensive tasks remains a potential limitation that could be further investigated. Overall the paper presents a valuable contribution to the field of weaklysupervised learning by addressing the challenges of longtailed partiallabel learning. Further exploration into the practical implications and scalability of SoLar could enrich its impact in realworld applications.", "y8FN4dHdxOE": " Peer Review: 1) Summary (99 words): The paper introduces and comprehensively analyzes a novel multimode tensor clustering method FusedOrthALS designed to simultaneously factorize and cluster tensors. The algorithm is theoretically proven to achieve recovery and clustering consistency for noisecontaminated tensors with latent low rank CP decomposition structure. The paper further discusses the computational efficiency accuracy and robustness of the proposed method through simulations and real datasets. The methodology fills a significant gap in the field of tensor clustering and demonstrates promising results for highdimensional data analysis. 2) Strengths:  The paper addresses an important problem in tensor clustering using a novel and theoretically grounded method.  The algorithms statistical guarantees for recovery and clustering consistency are established enhancing the credibility of the approach.  The method is illustrated to be accurate computationally efficient and robust through both synthetic and real dataset experiments. Weaknesses:  The paper could benefit from more extensive comparisons with existing methods to provide a comprehensive evaluation of the proposed approach.  The theoretical derivations are complex which might pose challenges for readers not wellversed in the underlying mathematical concepts.  The paper lacks a detailed discussion on the scalability of the proposed algorithm to larger datasets or higherorder tensors. 3) Questions:  Could the authors provide more insights into the scalability of the FusedOrthALS algorithm for handling larger datasets or higher order tensors  How does the performance of the method vary with different types and levels of noise in the data especially in realworld applications  Is there any discussion on parameter sensitivity and are there any guidelines provided for selecting regularization weights and tuning parameters 4) Limitations:  The paper could benefit from providing more detailed explanations or illustrations to aid in the comprehension of the complex theoretical derivations for a wider audience.  The generalizability of the proposed method to diverse realworld applications and datasets needs more exploration and validation.  The absence of a detailed analysis of the methods performance under varying levels of noise or missing data could limit its applicability in practical scenarios. Overall Evaluation: The paper presents a novel and theoretically grounded approach to address tensor clustering demonstrating robustness efficiency and accuracy through experiments. However some areas such as scalability practical implications and comprehensive comparisons could be further explored to enhance the impact and applicability of the proposed method in diverse realworld scenarios.", "lHy09zPewmD": " Summary The paper introduces a natural generalization of the Bandits with knapsacks (BwK) model to allow nonmonotonic resource utilization. The authors propose an MDP policy with constant regret against an LP relaxation in the case where the decisionmaker knows the true outcome distributions. In the case where the decisionmaker does not know the true outcome distributions they introduce a learning algorithm with logarithmic regret against the same LP relaxation. The paper also presents a reduction from BwK to their model and shows a matching regret bound with existing results.  Strengths and Weaknesses Strengths: 1. The paper addresses an important extension of the BwK model to allow nonmonotonic resource utilization. 2. The introduction of the MDP policy and learning algorithm with provable regret bounds is a significant contribution. 3. The paper provides a comprehensive theoretical analysis including lemmas theorems and algorithms making the methodology clear and replicable. 4. The discussion on future research directions and limitations adds depth to the paper. Weaknesses: 1. The paper is heavy on theoretical content and may benefit from more practical implications or applications of the proposed models. 2. The paper lacks results from experimental validation or realworld case studies to demonstrate the effectiveness of the proposed models. 3. The paper could have included more intuitive explanations to aid readers in understanding the complex mathematical formulations.  Questions 1. Can you provide more insights into how the proposed models could be practically applied in realworld scenarios 2. How sensitive is the proposed learning algorithm to variations in problem parameters such as the number of resources or arms 3. Have you considered including numerical simulations to demonstrate the algorithms performance in different settings  Limitations 1. The paper\u2019s heavy reliance on theoretical analysis may limit its accessibility to a broader audience including those not wellversed in mathematical optimization. 2. Lack of empirical validation might raise concerns regarding the practical utility of the proposed models in realworld decisionmaking scenarios. 3. The paper does not address the computational complexity or scalability of the proposed algorithms which could be a limitation in largescale applications.", "lmmKGi7zXn": " Peer Review:  Summary: The paper proposes two novel approaches: \\xe2\\x88\\x9eAE an infinitelywide autoencoder for recommendation modeling and DISTILLCF a data distillation framework for collaborative filtering datasets. \\xe2\\x88\\x9eAE demonstrates superiority over sophisticated existing models by outperforming them with a single fullyconnected layer and a simple closedform optimization method. On the other hand DISTILLCF is adept at synthesizing concise yet accurate data summaries showcasing denoising capabilities and scalability for large datasets. The paper provides thorough experiments across multiple benchmarks and offers insightful insights into the crucial role of data quality in recommendation systems.  Strengths: 1. Innovative Approaches: Both \\xe2\\x88\\x9eAE and DISTILLCF introduce novel techniques for recommendation modeling and data summarization respectively. The use of infinitewidth networks is a significant contribution to the field. 2. Experimental Rigor: The paper conducts comprehensive experiments on various datasets and provides detailed comparisons with stateoftheart models showcasing the efficacy of the proposed methods. 3. Implications: The findings suggest practical implications such as reducing computational costs carbon footprint and privacy risks associated with large datasets making the work relevant and impactful.  Weaknesses: 1. Complexity: The paper delves into intricate technical details and mathematical notations potentially making it challenging for readers not wellversed in the domain to grasp the concepts. 2. Evaluation Metrics: While the paper evaluates model performance on several metrics it could benefit from a more extensive analysis of the limitations and implications of each metric choice to provide a more holistic view. 3. Practical Applicability: A further discussion on the practical implementation aspects and realworld usability of the proposed methods could enhance the paper.  Questions: 1. Scalability Concerns: Given the high computational cost for training \\xe2\\x88\\x9eAE are there inherent limitations when scaling up to even larger datasets How would the methods adapt to ultralarge datasets beyond the scope of current experiments 2. Robustness Analysis: How robust are \\xe2\\x88\\x9eAE and DISTILLCF to other forms of noise or biases present in realworld recommendation datasets apart from the uniform noise injection studied in the experiments 3. Interpretability: Could the paper elaborate on the interpretability of the feature representations learned by \\xe2\\x88\\x9eAE and the insights derived from the compressed data summaries produced by DISTILLCF  Limitations: 1. Dataset Dependency: The evaluation is largely focused on synthetic and benchmark datasets. The paper could discuss potential biases or limitations introduced by these datasets and the generalizability of the proposed methods to a wider range of recommendation scenarios. 2. Realworld Adoption: The paper could benefit from a discussion on the practical implementation challenges and adoption barriers of \\xe2\\x88\\x9eAE and DISTILLCF in realworld recommendation systems particularly in diverse industrial settings. Overall the paper presents innovative ideas with promising results in recommendation modeling and data summarization opening up avenues for further research in datacentric recommendation systems.  Recommendation: The paper provides valuable contributions to the field of recommendation systems with its innovative methods and thorough experiments. With few minor revisions enhancements in practical implications and addressing potential scalability concerns the work would be wellpositioned to make a significant impact on the research community and potentially in practical applications.", "lSqaDG4dvdt": " Peer Review  Summary: The paper presents a genetic programming framework EZNAS for automating the discovery of zerocost proxies for neural architecture scoring. The methodology efficiently discovers interpretable and generalizable zerocost proxies for neural networks accuracy evaluation. The proposed approach achieves stateoftheart scoreaccuracy correlation on various datasets and search spaces. The findings suggest a promising direction towards automatically discovering zerocost proxies that can work across network architecture design spaces datasets and tasks.  Strengths: 1. Innovative Approach: The use of genetic programming for discovering zerocost proxies is a novel and promising approach. 2. Interpretability: EZNAS generates interpretable and generalizable zerocost proxies which enhances understanding and model explainability. 3. Efficiency: EZNAS demonstrates efficiency in discovering the proxies with minimal CO2e emission which is a significant contribution towards sustainable research practices. 4. Generalizability: The method showcases high scoreaccuracy correlation across various datasets and design spaces indicating its generalizability. 5. Comprehensive Evaluation: The paper provides detailed analyses including fitness objective program representation mathematical operations and performance evaluation enhancing the credibility of the proposed framework.  Weaknesses: 1. Limited Exploration: The paper could benefit from a more extensive analysis of the evolutionary search process to understand the limitations and potential improvements better. 2. Evaluation Metrics: While the paper discusses Kendall Tau and Spearman rank correlation coefficients additional benchmarking against more diverse metrics could strengthen the evaluation. 3. Complexity: The methodologys complexity particularly in program design and mathematical operations might hinder replication and implementation for researchers with limited resources.  Questions: 1. How does the proposed genetic programming framework compare to existing methods in terms of computational efficiency and scalability 2. Can the interpretability of the zerocost proxies facilitate insights into the underlying neural architecture features that contribute to accuracy prediction 3. How robust is the discovered ZCNASM in handling diverse neural network architectures with varying complexities 4. What are the potential limitations or challenges faced during the evolutionary search process in EZNAS 5. Are there any plans to extend the evaluation to more diverse datasets and architectural design spaces to enhance the methods robustness and applicability  Limitations: 1. Limited Application Scope: The current evaluation focuses on specific datasets and design spaces potentially limiting the generalizability of the discovered proxies. 2. Complexity and Replication: The intricacies of the genetic programming approach may pose challenges for researchers aiming to replicate the methodology. 3. Scope for Bias: Given the manual intervention required in initial program design and mathematical operation selection potential biases or errors might affect the results. Overall the paper presents a significant advancement in automating the discovery of zerocost proxies for neural architecture scoring. Addressing the noted weaknesses and limitations could further enhance the credibility and applicability of the proposed EZNAS framework.", "rA2tItoRUth": " Peer Review 1) Summary: The paper introduces a LanguageGuided Denoising Network (LGDN) for videolanguage modeling which aims to filter out irrelevant frames in videos to improve crossmodal alignment at the token level. LGDN dynamically selects 24 salient frames per video based on language supervision demonstrating significant performance improvements over existing methods on five public datasets. The proposed SFP mechanism effectively addresses the noise issue in videolanguage modeling leading to enhanced videotext retrieval results across various datasets. 2) Strengths and weaknesses:  Strengths:  Novel approach: LGDN introduces a unique method to filter irrelevant video frames based on language supervision addressing the noise issue in videolanguage modeling effectively.  Experimental results: The paper provides extensive experiments on multiple datasets demonstrating superior performance of LGDN over stateoftheart methods by large margins.  Ablation study: The ablation study thoroughly investigates the individual contributions of components in the LGDN model providing insights into the effectiveness of different modules.  Weaknesses:  Lack of qualitative analysis: While the paper presents quantitative results and visualization of relevance scores a deeper qualitative analysis of model predictions or failure cases could have added more insights.  Complexity: LGDN involves multiple modules and mechanisms making it challenging for nonexperts to understand the intricacies of the proposed approach. Simplifying explanations or visual aids could enhance accessibility. 3) Questions:  How does LGDN perform in scenarios where the noise level in videos is exceptionally high or the semantic gap between video frames and text descriptions is significant  Are there scalability issues when applying LGDN to larger video datasets or more complex video tasks  Can LGDN be adapted to handle realtime video processing applications considering the computational resources required for dynamic frame filtering 4) Limitations:  Dataset bias: The evaluation relies on public datasets which may have inherent biases or limitations that could affect the generalizability of LGDN across diverse videolanguage applications.  Overfitting risk: While LGDN outperforms existing methods the risk of overfitting to specific datasets or scenarios should be discussed especially considering the complex architecture of the model. Overall the paper introduces a promising approach to videolanguage modeling by addressing the noise issue in video data. With further exploration of scalability generalizability and robustness aspects LGDN has the potential to make significant contributions to the field of multimodal understanding.", "lYZQRpqLesi": " Peer Review  1) Summary: The paper introduces Alternating Sparse Training (AST) scheme a novel method for training multiple sparse subnetworks simultaneously. AST aims to address the limitations of traditional dynamic inference methods by offering dynamic morphing capabilities without extra training costs. The proposed ASTGC with gradient correction shows improved accuracy while reducing training overhead. Experimental results on multiple datasets demonstrate the effectiveness of the proposed method in achieving high accuracy and training efficiency.  2) Strengths and weaknesses: Strengths:  The paper addresses an important problem in the field of dynamic inference and offers a novel solution with AST.  The AST method is wellmotivated and supported by both theoretical analysis and experimental results.  The paper provides a clear explanation of the proposed training scheme and the rationale behind the gradient correction technique.  Comprehensive experiments on CIFAR10 CIFAR100 and ImageNet datasets validate the effectiveness of AST compared to stateoftheart methods. Weaknesses:  The paper could benefit from more detailed comparisons with existing methods and further analysis on the limitations of AST.  The discussions on the impact and limitations could be expanded to provide a deeper understanding of the potential challenges and future directions of the proposed method.  Some parts of the paper such as the pseudocode of the proposed algorithm and additional experimental details are relegated to the appendix which may hinder the clarity of the main text.  3) Questions:  How would the proposed AST method perform with a larger number of subnets and are there any scalability limitations  Can the authors provide more insights into the practical implementation and deployment of AST on different hardware platforms beyond GPU  In what scenarios or applications do you foresee AST outperforming existing dynamic inference methods with multiple sparse subnets  4) Limitations:  The paper could provide more detailed discussions on the potential limitations of AST such as the impact of hyperparameter tuning model architecture choices and the constraints of the proposed approach on specific network structures.  The generalization of the results may be limited to the datasets and networks used in the experiments and more extensive evaluations on diverse datasets and architectures are recommended to validate the robustness of AST. Overall the paper presents a valuable contribution to the field of dynamic inference training with AST offering an innovative approach that shows promising results. The proposed method has the potential to advance research in sparse training and dynamic inference but further investigation and validation are needed to fully understand its capabilities and limitations.", "oNnv9XjClGK": " Peer Review  1) Summary The paper proposes a novel method called Direct Advantage Estimation (DAE) to model and estimate the advantage function directly from onpolicy data in reinforcement learning. The key idea is to interpret the advantage function as causal effects leveraging insights from the causality literature. DAE is shown empirically to outperform Generalized Advantage Estimation (GAE) in policy optimization across various discrete control domains. The paper highlights the importance of estimating causal effects in reinforcement learning for improved generalization and transfer learning.  2) Strengths and Weaknesses Strengths:  The paper presents a novel method DAE for estimating the advantage function directly from onpolicy data thereby minimizing variance in value estimation.  The connection between the advantage function and causal effects is well established contributing to a better understanding of credit assignment in reinforcement learning.  Empirical evaluation on multiple discrete control domains demonstrates the superiority of DAE over a strong baseline GAE.  The method is easy to implement and adaptable to modern actorcritic methods. Weaknesses:  The paper heavily focuses on theoretical derivations and empirical results potentially making it challenging for readers unfamiliar with causality literature to grasp the concepts.  The discussion on limitations and future directions for DAE could be further expanded to explore potential challenges in realworld applications and extensions to offpolicy settings.  It would be beneficial to provide a more detailed comparison with existing methods in terms of computational efficiency and scalability.  3) Questions  How does the Direct Advantage Estimation (DAE) method perform in environments with nonstationary dynamics or complex reward structures  Can DAE be extended to handle continuous action spaces given that centering constraints are challenging in such domains  Are there specific computational or memory requirements associated with implementing the DAE method compared to existing advantage estimation techniques  4) Limitations  The paper primarily focuses on empirical evaluations on discrete control domains limiting the generalizability of DAE to more complex environments.  Cautions need to be taken while comparing DAEs performance to GAE as GAE may have particular strengths or weaknesses in different contexts not fully addressed in this evaluation.  The theoretical foundations of using causal effects to estimate the advantage function might need further validation in realworld and more diverse settings beyond the controlled experiments conducted in this study. Overall the paper makes a significant contribution by introducing DAE as an intuitive and effective method for estimating the advantage function in reinforcement learning. Further exploration and validation of the method in various environments and the extension to handle continuous action spaces would enhance its applicability and robustness in practical scenarios.", "pNHT6oBaPr8": " Paper Review 1. Summary: The paper introduces Partial Group Convolutional Neural Networks (Partial GCNNs) that can learn layerwise levels of partial and full equivariance to discrete continuous groups and combinations thereof. The key idea is to adjust equivariance levels based on data characteristics. The experiments conducted on toy tasks and benchmark datasets show that Partial GCNNs outperform conventional GCNNs when full equivariance is disadvantageous showcasing the effectiveness of the proposed approach. 2. Strengths:  The paper addresses an important aspect of equivariant neural networks by introducing a novel design for learning layerwise partial or full equivariance directly from data.  The experiments are welldesigned with thorough evaluation on illustrative toy tasks and popular vision benchmark datasets demonstrating the advantages of Partial GCNNs over conventional GCNNs.  The approach of adjusting equivariance levels based on the needs of the data provides a flexible and adaptive framework for constructing equivariant models.  The discussion on memory consumption and the benefits of partial equivariance in large group representations point towards practical implications and future research directions. 3. Weaknesses:  The complexity of group theory and the mathematical formalism used in the paper might be challenging for readers without a strong background in the subject. Providing more intuitive explanations or examples could improve the accessibility of the content.  The paper could benefit from further analysis or discussion on the scalability of Partial GCNNs to even larger groups and addressing potential limitations in stability during training on discrete groups.  While the experimental results are comprehensive additional insights into the interpretability of the learned equivariance levels or visualizations could enhance the understanding of the model behavior. 4. Questions:  How does the proposed approach of learning partial equivariance affect model interpretability and generalization to unseen data distributions  Are there specific realworld applications or domains where Partial GCNNs could have significant advantages over traditional GCNNs and if so what are those areas  What are the computational implications of implementing Partial GCNNs in terms of training time and complexity compared to standard GCNNs 5. Limitations:  The paper acknowledges the limitations in training stability on discrete groups and highlights the need for more robust methods for learning discrete distributions. Addressing this limitation could enhance the practical applicability of Partial GCNNs.  The scalability of Partial GCNNs to larger groups is noted as a future direction and it would be relevant to explore how the approach could be extended to even more complex group representations. Overall the paper provides a valuable contribution to the field of equivariant neural networks with the introduction of Partial GCNNs showcasing the benefits of adaptively learning equivariance levels from data. The results presented in the paper support the effectiveness of the proposed approach and open up interesting avenues for future research in this area.", "wYgRIJ-oK6M": " Peer Review:  1) Summary: The paper \"Fully Binarized Transformer Networks with Elastic Activation Knowledge Distillation\" introduces a method to binarize transformer models with high accuracy using a series of innovative techniques. The approach includes a twoset binarization scheme an elastic binary activation function and a multidistillation method to progressively distill higher precision models into lower precision students. The proposed method achieves near fullprecision BERT baseline accuracy on the GLUE language understanding benchmark. The approach is detailed and results in significant advancements in binarizing transformer models.  2) Strengths and Weaknesses:  Strengths:  Introduces innovative techniques to achieve high accuracy in fully binarized transformer models.  Detailed explanation of the improvements in binarization approaches.  Demonstrates advancements in achieving near fullprecision BERT baseline accuracy.  Includes a thorough evaluation of the proposed method on the GLUE benchmark and SQuAD dataset.  Provides code and models for reproducibility.  Weaknesses:  The complexity of the proposed method might pose challenges for wider adoption and implementation.  Limited exploration of the performance of the approach on different types of transformer models and tasks beyond NLP.  Further analysis on the scalability and computational cost implications of the proposed techniques could enhance the paper.  3) Questions:  How does the proposed method compare to other stateoftheart binarization techniques for transformer models both in terms of accuracy and computational efficiency  Has the robustness of the proposed techniques been evaluated against various hyperparameters and different datasets to ensure generalizability  Are there specific limitations or challenges encountered during the implementation and deployment of fully binarized transformer models that need further investigation  How transferable are the findings and approaches presented in this work to other AI domains beyond NLP applications  4) Limitations:  The limited exploration of deploying binarized transformer models in realworld scenarios with diverse data sets and application domains.  The focus on the GLUE benchmark and SQuAD dataset may not fully capture the performance and limitations of the proposed method across a broader range of tasks.  The paper could benefit from more comprehensive discussions on the potential drawbacks and tradeoffs associated with the proposed binarization techniques. In conclusion the paper presents a significant advancement in fully binarizing transformer models with high accuracy showcasing innovative techniques such as elastic activation functions and multidistillation approaches. Further exploration of the practical implications and generalizability of the proposed methods could enhance the impact of the research.  Overall Rating: 45", "rY2wXCSruO": " Peer Review of the Scientific Paper  1) Summary: The paper introduces a novel modality interaction strategy for multimodal 3D object detection termed as DeepInteraction. This strategy maintains individual permodality representations to facilitate intermodality interaction thereby addressing the limitations of previous fusion approaches. Using a multimodal representational interaction encoder and a multimodal predictive interaction decoder DeepInteraction surpasses prior stateoftheart methods on the nuScenes dataset securing the top position on the leaderboard. The experimental results confirm the effectiveness of the proposed strategy in enhancing object detection performance.  2) Strengths and Weaknesses: Strengths:  The paper addresses a critical limitation of existing multimodal fusion approaches by introducing a novel modality interaction strategy.  Experimental results demonstrate the superiority of DeepInteraction over prior methods achieving stateoftheart performance.  Comprehensive experiments on the nuScenes dataset validate the effectiveness of the proposed approach.  The detailed architecture and design of DeepInteraction including the encoder and decoder components are well elaborated. Weaknesses:  While the experimental results demonstrate the effectiveness of DeepInteraction further analysis of the computational complexity and resource requirements of the proposed method would enhance the paper.  The paper lacks a detailed analysis of potential failure cases or scenarios in which the proposed method may underperform.  The paper could benefit from a more indepth discussion on the limitations of the proposed approach and suggestions for future research directions.  3) Questions:  Can you provide more insights into the computational efficiency and training time of DeepInteraction compared to existing fusion methods  How sensitive is the performance of DeepInteraction to hyperparameter choices such as the number of encoder and decoder layers or the query numbers  Could you discuss the potential challenges or failure modes faced by the proposed DeepInteraction strategy in scenarios with complex or cluttered environments  4) Limitations:  The lack of a detailed analysis of the computational overhead and resource requirements of DeepInteraction limits the understanding of its scalability and practical implementation.  The paper could benefit from additional discussion on the generalizability of DeepInteraction beyond the nuScenes dataset and its performance in diverse environments.  While the experimental results demonstrate the performance gains of DeepInteraction a more indepth discussion on the interpretability and robustness of the model would enhance the papers impact. Overall the paper presents a novel and promising modality interaction strategy for multimodal 3D object detection showcasing significant performance improvements over existing fusion methods. Further insights into the computational efficiency robustness and realworld applicability of DeepInteraction would contribute to its broader adoption and impact in the field.", "w5DacXWzQ-Q": " Peer Review  1) Summary: The paper introduces a novel approach Collaborative Pruning to efficiently prune Vision Transformers (ViTs) by considering interactions between heterogeneous components. They propose a Taylorbased optimization function to evaluate joint importance and achieve a balanced reduction across all components. The method is applied to various tasks including object detection and extensive experiments demonstrate superior performance compared to existing approaches in terms of accuracy and FLOPs reduction.  2) Strengths:  The paper introduces an innovative collaborative pruning method for ViTs considering interactions between components leading to improved performance.  The theoretical analysis and optimization function are welldetailed and provide a systematic framework for pruning all components in ViTs.  The proposed algorithm shows superior results in terms of accuracy and FLOPs reduction across different model sizes and tasks showcasing the effectiveness of the approach.  The paper provides a clear experimental setup detailed results and comparisons with stateoftheart methods demonstrating the competitiveness of the proposed approach.  3) Weaknesses:  While the paper extensively covers the theoretical foundations of the collaborative pruning approach more insights into the practical implementation challenges and computational complexity would enhance the understanding.  The paper could benefit from a more indepth discussion on the generalizability of the proposed method beyond the specific examples provided in the experiments.  The paper may consider discussing potential limitations in terms of realworld deployment scenarios or tradeoffs between computational complexity and pruning effectiveness.  4) Questions:  Can the proposed collaborative pruning approach be easily extended to other transformerbased architectures apart from ViTs  How sensitive is the method to changes in the optimization hyperparameters and what strategies were employed to ensure robustness in the experimental evaluations  Are there any specific scenarios or types of tasks where the proposed method may not be as effective and how could these limitations be addressed in future research  5) Limitations:  The paper could provide more insights into the limitations or constraints that may affect the scalability or realworld applicability of the collaborative pruning approach.  While the experimental results are promising a more detailed analysis of the potential failure cases or scenarios where the method might not perform well would provide a more comprehensive understanding of its limitations. Overall the paper presents a significant contribution to the field of neural network pruning particularly for Vision Transformers and offers a novel perspective on leveraging component interactions for efficient model optimization. Further exploration of practical implications and more robust evaluation in diverse scenarios would enhance the impact of the proposed technique.", "q6bZruC3dWJ": " Summary Knowledge distillation (KD) is an effective technique to compress neural networks by training a smaller network to mimic a larger one. This paper delves into the phenomenon where larger teacher models do not necessarily result in better student models attributing this to undistillable classes. Undistillable classes are those whose knowledge is incomprehensible to the student leading to inaccuracies in those classes postdistillation. The study establishes that undistillable classes are prevalent across datasets and distillation methods and proposes a \"Teach Less Learn More\" framework to identify and discard these classes. Experiments show that the proposed method outperforms existing techniques on various datasets and network architectures.  Strengths 1. The paper addresses an important and counterintuitive observation in knowledge distillation providing a novel perspective on the inefficacy of large teacher models. 2. The proposed framework TLLM is simple yet effective in identifying and discarding undistillable classes leading to improved performance. 3. The comprehensive experimental evaluation on multiple datasets and architectures validates the efficacy of the proposed method. 4. The paper is wellstructured with clear explanations and detailed empirical results to support the findings.  Weaknesses 1. The paper could benefit from more indepth analyses on the impact of undistillable classes on different types of models and datasets. 2. While the TLLM framework shows promising results further comparisons with other stateoftheart techniques could strengthen the argument for its effectiveness. 3. The study would be enhanced by providing insights into the computational complexity or overhead introduced by the TLLM framework.  Questions 1. How do undistillable classes impact the interpretability of the student model especially in realworld applications 2. Are there potential limitations of the TLLM framework when applied to more complex networks or diverse datasets 3. Can undistillable classes be dynamically identified and addressed during the distillation process rather than preemptively removing them  Limitations 1. The study focuses primarily on the task of improving overall accuracy without delving into potential tradeoffs in other model evaluation metrics. 2. The generalizability of the findings across a broader spectrum of distillation methods and architectures could be further explored. 3. The concrete impact of undistillable classes on resourceconstrained environments or edge devices is not thoroughly discussed. Overall the paper offers a fresh perspective on addressing the \"larger teacher worse student\" dilemma in knowledge distillation and the proposed TLLM framework shows promise in mitigating the effects of undistillable classes. Further investigations on the practical implications and scalability of this approach would be valuable.", "qTCiw1frE_l": "Peer Review Summary: The paper investigates the phenomenon of policy churn in valuebased reinforcement learning. It highlights the rapid and extensive policy changes observed in deep reinforcement learning settings contrary to common expectations. The study empirically characterizes policy churn explores its implications on exploration and proposes potential benefits and mechanisms. The implications of policy churn on learning dynamics exploration and performance are thoroughly discussed. Strengths: 1. Novelty and Relevance: The paper introduces a novel phenomenon policy churn which challenges conventional understanding in reinforcement learning. The research sheds light on a crucial aspect that has implications for exploration strategies and learning dynamics. 2. Empirical Validation: The study is wellsupported with empirical evidence and experimental results enhancing the credibility of the findings. The detailed investigations and ablations help in understanding the phenomenon thoroughly. 3. Detailed Analysis: The paper provides an indepth analysis of policy churn exploring its impact on exploration and learning processes. The discussion on related works and comparisons with traditional methods enrich the understanding of the phenomenon. Weaknesses: 1. Complexity: The technical depth and complexity of the paper might hinder accessibility for a broader audience. Simplifying certain sections or adding more explanatory details could increase the clarity and comprehensibility of the findings. 2. Generalization: The study primarily focuses on valuebased reinforcement learning potentially limiting the generalizability of the findings to other RL approaches. Addressing the transferability of policy churn across different algorithms could strengthen the relevance of the research. Questions: 1. Causative Factors: Can the study further elaborate on the specific mechanisms driving policy churn such as the interaction between function approximation noise in learning and convergence behavior 2. Exploration vs. Exploitation Balance: How does policy churn impact the balance between exploration and exploitation in reinforcement learning agents particularly in longterm learning scenarios 3. Practical Implications: What are the potential practical implications of policy churn for designing more efficient and stable reinforcement learning algorithms in realworld applications Limitations: 1. Scope: The study primarily focuses on the phenomenon of policy churn in valuebased RL which may limit the generalizability of findings to different RL paradigms. 2. Theoretical Emphasis: Despite the comprehensive theoretical analysis the study could benefit from more practical applications or experiments to validate the proposed hypotheses in realworld scenarios. Overall the paper presents a valuable contribution to the field of reinforcement learning by introducing and examining the policy churn phenomenon. It offers insights that could lead to significant advancements in exploration strategies and learning dynamics in RL systems. Addressing the listed concerns and extending the research to practical applications could further enhance the impact of the study.", "rTTh1RIn6E": "Peer Review: 1) Summary: The paper \"Conditionali: A Novel Independent Component Analysis Approach for OutofDistribution Detection\" introduces a new method for OutofDistribution (OOD) detection called Conditionali based on independent component analysis techniques. The model proposes a probabilistic framework that encodes class conditions and utilizes the HilbertSchmidt Independence Criterion to optimize variable dependencies. The Conditionali model demonstrates significant performance improvements over existing stateoftheart (SOTA) methods in both computer vision and natural language processing tasks. 2) Strengths and Weaknesses: Strengths:  Novel approach: The paper introduces a unique perspective on OOD detection using independent component analysis.  Theoretical justification: The Conditionali model is supported by valuable theoretical justifications providing transparency in the training strategy.  Performance: Empirical results show significant performance improvements over existing SOTA methods on computer vision and NLP benchmarks.  Efficient memory bank architecture: The use of memory banks facilitates endtoend training within a tractable budget. Weaknesses:  The paper lacks a comparative analysis with a larger variety of baseline methods to provide a more robust evaluation of Conditionali.  The paper may benefit from a more detailed explanation of the impact of the proposed method on interpretability and explainability.  The explanation of the ablation studies and their implications could be more elaborate for clearer understanding. 3) Questions:  What are the computational requirements for implementing the Conditionali model especially in terms of memory and processing power  How does Conditionali perform under scenarios with high data complexity or noisy datasets  Can Conditionali be easily applied to other domains outside computer vision and NLP 4) Limitations:  The evaluation of Conditionali against baseline methods is limited to a select few potentially restricting the generalizability of the findings.  The practical impact of Conditionali in realworld applications and its scalability to larger datasets should be explored further.  The effect of hyperparameter tuning on the performance of Conditionali and its generalizability to diverse datasets needs to be investigated. In conclusion the paper presents a compelling approach for OOD detection with theoretical foundations and empirical evidence to support its efficacy. The Conditionali model offers a new perspective on OOD detection and demonstrates promising results in both computer vision and NLP tasks. Further exploration into the scalability robustness and applicability of Conditionali in various settings would enhance the impact and significance of this research.", "vphSm8QmLFm": " Summary The paper introduces Global Batch gradients Aggregation (GBA) for training recommendation models aiming to enable tuningfree switching between highconcurrency asynchronous training and highperformance synchronous training. GBA aggregates gradients based on a global batch size addressing gradient staleness and achieving comparable convergence properties with synchronous training. Experimental results demonstrate that GBA improves AUC by 0.2 on average and speeds up training by at least 2.4x compared to synchronous training under strained hardware resources.  Strengths 1. Innovative Approach: Introducing GBA as a tuningfree method for switching between training modes is an innovative approach in distributed training. 2. Comprehensive Evaluation: Conducting experiments on three industrialscale recommendation tasks strengthens the validity of the proposed method. 3. Convergence Analysis: Providing a convergence analysis comparing GBA with synchronous training adds depth to the technical foundation of the paper. 4. Performance Improvements: Achieving improved AUC and training speed compared to stateoftheart training modes demonstrates the effectiveness of GBA.  Weaknesses 1. Limited Comparison: The paper could benefit from comparing GBA with a broader range of existing methods to provide a more comprehensive evaluation. 2. Hardware Specific: The evaluation focuses on a specific hardware setup limiting the generalizability of the results to other hardware configurations. 3. Complexity: The detailed technical description may make it challenging for nonexperts to grasp the methodology and its implications.  Questions 1. How does GBA handle scenarios with varying network latencies or worker speeds in a shared cluster 2. Have you considered the potential overhead introduced by the tokencontrol mechanism and how it might affect training efficiency 3. Could the staleness decay strategy impact training performance if the threshold \\xce\\xb9 is not properly tuned 4. Are there any considerations for adapting GBA to different recommendation model architectures beyond the ones evaluated in the paper  Limitations 1. Generalizability: The study is limited by its focus on recommendation models limiting the applicability of GBA to a broader range of deep learning tasks. 2. Scalability: The scalability of GBA to larger and more complex recommendation models or different hardware configurations needs further investigation. 3. Realworld Implementation: While promising the realworld implementation and usability of GBA outside of controlled experimental settings remain to be validated.", "zdmYnIRXvKS": "Peer Review: 1) Summary: In this paper the authors revisit the theory of efficient coding with spikes to develop spiking neural networks that are closer to biological circuits. They propose a biologically plausible spiking model using a generalized leaky integrateandfire network with excitatory and inhibitory units and various synaptic currents. The study focuses on understanding how the complexity of EE connectivity matrix shapes network responses and presents results from simulations showing network behavior under different conditions. 2) Strengths:  The study provides a comprehensive analysis of developing a biologically plausible spiking neural network model for efficient coding with spikes.  The modeling approach involving loss functions and optimization principles offers insight into the functional interpretation of network parameters.  The inclusion of structured connectivity adaptation currents and rebound currents enhances the biological realism of the proposed spiking network model.  The simulation results demonstrate how the complexity of EE connections influences network responses providing valuable insights into information processing dynamics. 3) Weaknesses:  The paper lacks a direct comparison with existing models or experimental data to validate the proposed spiking network model.  The complexity of the mathematical derivations and technical language used may make it challenging for nonspecialist readers to fully understand the study.  While the study highlights the role of network parameters on dynamics a more detailed discussion on the implications for information processing efficiency could further enrich the findings. 4) Questions:  Could the authors elaborate on the scalability and applicability of the proposed spiking neural network model to different scenarios or tasks beyond the scope of efficient coding  How do the simulated results align with existing empirical data on neural responses to complex stimuli and what future experimental validations or extensions do the authors envision  Are there specific advantages or limitations of the proposed spiking network model compared to other existing models in terms of biological realism and computational efficiency 5) Limitations:  The paper focuses primarily on the theoretical development of the spiking network model with limited discussion on practical applications or experimental validation.  The absence of a thorough comparison with existing models may limit the overall assessment of the novelty and utility of the proposed approach.  The studys emphasis on complex mathematical derivations and network dynamics may hinder its accessibility to a broader audience interested in neuroscience or computational modeling. In conclusion the paper presents a rigorous theoretical framework for developing biologically plausible spiking neural networks for efficient coding. Addressing the raised questions and limitations could enhance the clarity and impact of the study providing valuable contributions to the field of computational neuroscience.", "rlN6fO3OrP": "Peer Review: 1) Summary: The paper investigates the vulnerability of the continuous prompt learning paradigm to backdoor attacks in NLP tasks. It introduces BadPrompt a lightweight and taskadaptive algorithm for backdooring continuous prompts comprising trigger candidate generation and adaptive trigger optimization modules. The experimental results demonstrate BadPrompts effectiveness in attacking continuous prompts while maintaining high performance on clean test sets. 2) Strengths:  Novel Contribution: The study addresses an important gap by exploring backdoor attacks on continuous prompts introducing the BadPrompt algorithm.  Rigorous Methodology: The paper presents a detailed explanation of the proposed method including trigger candidate generation and adaptive trigger optimization.  Experimental Evaluation: The extensive experiments on different datasets and models demonstrate BadPrompts superiority compared to baseline methods.  Potential Impact: The research sheds light on security issues in promptbased learning raising awareness of the vulnerability of continuous prompts to backdoor attacks. 3) Weaknesses:  Explanation Depth: The paper could provide more comprehensive explanations of some technical aspects such as the Gumbel Softmax approximation process and the specific hyperparameters selection rationale.  Generalizability: The paper primarily focuses on specific promptbased models and tasks limiting the generalizability of the findings to a broader range of NLP scenarios.  Presentation of Results: The figures and tables are wellorganized but the visualization of results could be improved for better clarity and understanding. 4) Questions:  What are the implications of the proposed BadPrompt algorithm for realworld NLP applications considering its potential security risks and countermeasures  How does the performance of BadPrompt compare to existing stateoftheart defenses against backdoor attacks in NLP particularly in terms of scalability and efficiency  Can the trigger candidate generation and adaptive trigger optimization modules of BadPrompt be further enhanced or extended to address more diverse NLP tasks and scenarios 5) Limitations:  The study primarily focuses on a specific set of tasks and models limiting the applicability across a broader spectrum of NLP applications.  The generalizability of the findings might be restricted due to the focus on continuous prompt models and the limited consideration of defense mechanisms against backdoor attacks.  The study could benefit from exploring additional PLM models and tasks to enhance the robustness of the proposed backdoor attack method. Overall the paper presents a valuable contribution to the field of NLP security by addressing backdoor attacks on continuous prompts. Further enhancements in explanation depth generalizability and result presentation could strengthen the impact and applicability of the research findings. Note: This review focuses on the technical aspects implications and potential areas for improvement in the reviewed paper.", "mzze3bubjk": " Review of Paper: \"Connections Between FreeEnergy and LowDegree Approaches in Statistical Inference\"  1) Summary: The paper explores the relationship between freeenergybased computational hardness criterion and lowdegree hardness for statistical problems. It introduces the Franz\u2013Parisi criterion connecting it to Gaussian additive models and sparse planted models. The study establishes connections between various notions of computational hardness and provides new lowdegree lower bounds for sparse linear regression. The work sheds light on the complexities of highdimensional statistical inference problems and the challenges in bridging different computational lower bound approaches.  2) Strengths and Weaknesses:  Strengths:  The paper addresses an important gap in understanding computational hardness in statistical inference by connecting freeenergy and lowdegree approaches.  The formal definitions and theorems presented are rigorous and wellsupported with mathematical reasoning.  The study successfully reveals deep insights into the theoretical foundations of statistical inference problems and elucidates the connection between different notions of hardness.  Weaknesses:  The paper is dense with technical information which might make it challenging for readers unfamiliar with the subject matter to grasp the content.  While the results are comprehensive more illustrative examples or visual aids could enhance the understanding of complex concepts.  3) Questions:  Could the authors provide more practical implications of the theoretical findings How could these results impact realworld applications of statistical inference  How generalizable are the connections established between freeenergy and lowdegree approaches to other statistical problems beyond the models discussed in the paper  Are there limitations to the applicability of the Franz\u2013Parisi criterion in specific scenarios and how could those affect the broader understanding of computational hardness in statistical inference  4) Limitations:  The paper focuses heavily on theoretical results and may benefit from more discussions on the practical implications and applications of the findings.  The discussion around the limitations of the Franz\u2013Parisi criterion and its domain of applicability could be further expanded to provide a more comprehensive understanding. Overall the paper presents a substantial contribution to the field of statistical inference by linking different computational hardness criteria. Further clarification on the practical implications and potential extensions of the study could enhance the impact of the research findings.", "q41xK9Bunq1": " Summary: The paper introduces Neural Attentive Circuits (NACs) a generalpurpose neural architecture that combines the benefits of generalpurpose models and modular neural architectures. NACs learn both module parameterization and sparse connectivity endtoend without domain knowledge. They are evaluated on various data modalities demonstrating improvements in lowshot adaptation outofdistribution robustness and computational efficiency.  Strengths: 1. Novel Architecture: NACs propose a unique combination of generalpurpose and modular approaches allowing for joint learning of connectivity and parameterization. 2. Empirical Evaluation: The paper provides detailed experiments across different modalities showcasing the effectiveness of NACs in various tasks and the improvement in performance metrics. 3. Conditional Circuit Design: The analysis of sampleconditional circuit generation demonstrates the models adaptability and diversity in forming module connectivity based on input data.  Weaknesses: 1. Complexity: The NAC model is quite complex which may pose challenges in understanding and implementation for researchers not wellversed in neural architecture design. 2. Limited Comparison: While comparisons with Perceiver IO are made additional comparisons with more diverse architectures could provide a broader perspective on the performance of NACs. 3. Further Exploration: The paper hints at the potential impact of different graph priors on NAC performance but more indepth analysis and explanation on this aspect would enhance the paper.  Questions: 1. Scalability: How does the performance of NACs scale with the number of modules especially in scenarios where thousands of modules are used on a single GPU 2. Deployment: Are there considerations or challenges in deploying NACs in realworld applications given their complexity and the need for joint learning of connectivity and parameterization 3. Generalizability: Do the benefits observed in lowshot adaptation and OOD robustness hold across a wide range of datasets beyond the ones evaluated in the paper  Limitations: 1. Data Modalities: The experiments cover diverse data modalities but broader datasets and tasks could provide a more comprehensive evaluation of NACs generalpurpose nature. 2. Interpretability: While the analysis of generated connectivity graphs is insightful more clarity on the interpretability of these structures and their impact on overall performance could be beneficial. 3. Complexity vs. Performance: The tradeoff between the complexity of NACs and the observed performance improvements should be further explored to understand the practical utility of the model.", "xOK40an4ag1": " Peer Review:  1) Summary: The paper investigates how network dynamics are related to connectivity in Recurrent Neural Networks (RNNs) trained without specific constraints on tasks used in neuroscience. Despite the seemingly highdimensional connectivity of these RNNs the study identifies a lowdimensional subspace of the weight matrix that is functionally relevant enough for the RNNs to perform the tasks efficiently. This finding has implications for understanding the interplay between connectivity and network dynamics making RNNs more interpretable models for neuroscience and machine learning applications.  2) Strengths and Weaknesses:  Strengths:  The study addresses an important gap in understanding how network dynamics emerge from RNN connectivity.  The methodology of identifying operative dimensions offers new insights into the relation between connectivity and function enhancing interpretability.  The paper is wellstructured clearly presenting the research methodology results and implications.  The incorporation of diverse tasks and variations of RNNs strengthens the generalizability of the findings.  Weaknesses:  The study heavily relies on arbitrary selection of sampling locations for identifying operative dimensions which could introduce biases.  There could be limitations in extending the approach to more complex tasks or RNN architectures which could be addressed with further exploration and validation.  The paper could benefit from further discussion on the practical implications of operationalizing the identified functionally relevant subspaces in realworld applications.  3) Questions:  How sensitive is the identification of operative dimensions to the choice of sampling locations Are there strategies or refinements that could enhance the effectiveness of this process  Can the findings on lowdimensional subspace identification be extrapolated to broader AI problems beyond the specific tasks investigated in this study  How might the concept of operative dimensions be practically applied in realworld applications of RNNs especially in terms of continual learning or weight compression  4) Limitations:  The study\u2019s reliance on arbitrary sampling locations for the identification of operative dimensions may introduce uncertainties in the generalizability of the findings.  The applicability of the identified lowdimensional subspace in complex tasks or RNN architectures beyond those investigated in this study needs further validation.  The paper lacks an indepth exploration of the practical implications and potential uses of the identified functionally relevant subspaces in realworld RNN applications.  5) Overall the paper presents valuable insights into the interpretability of RNNs by identifying functionally relevant subspaces within highdimensional connectivity. Further addressing the sensitivity of operative dimension identification and exploring the practical implications of these findings would enhance the impact and applicability of the research.", "wKhUPzqVap6": " Peer Review:  1) Summary: The paper evaluates the quality of feature representations learned in the presence of spurious correlations using deep classifiers. It compares standard Empirical Risk Minimization (ERM) with specialized group robustness training using the Deep Feature Reweighting (DFR) procedure. The study demonstrates that while group robustness methods can improve worst group accuracy the quality of features learned by ERM is competitive. Key findings include the significance of model architecture pretraining strategy and the minimal impact of strong regularization on feature representations. The paper sheds light on the relationship between feature learning and model performance on various vision and NLP problems.  2) Strengths and Weaknesses: Strengths:  Extensive evaluation covering multiple vision and NLP problems with spurious correlations.  Provides a comparative analysis of ERM and specialized group robustness training methods.  Demonstrates the influence of design decisions like model architecture and pretraining strategy on feature learning.  Insights on feature decoding and practical implications for training robust classifiers. Weaknesses:  Lack of direct comparison with other stateoftheart methods beyond group robustness.  Limited discussion on the generalization of findings to other datasets or domains.  Potential biases in assessing models based on specific architectures and pretraining approaches.  Narrow exploration of methods for improving feature representations beyond ERM.  4) Questions:  How do the findings of the study generalize to datasets with different characteristics or domains outside of vision and NLP problems  Can the conclusions drawn from the study be extended to broader applications of deep learning beyond the specific benchmarks studied  What are the implications of the observed linear relationship between indistribution accuracy and worst group accuracy for realworld applications  How do the proposed insights on feature learning impact the development of more robust deep learning models in practical scenarios with spurious correlations  5) Limitations:  The paper primarily focuses on analyzing the impact of ERM and group robustness training methods on feature representations potentially overlooking other approaches for improving robustness.  The evaluation is limited to a specific set of vision and NLP problems raising questions about the generalizability of the findings to diverse datasets.  The absence of direct comparisons with existing stateoftheart methods limits the assessment of the proposed approach against alternative solutions.  The study lacks a detailed discussion on the interpretability of the learned features and their relevance to realworld applications beyond the experimental benchmarks.", "vK53GLZJes8": " Peer Review of the Scientific Paper: Role of Regularization in Temporal Difference Learning 1) Summary (111 words) The paper challenges the common notion that regularization effectively mitigates instability and unbounded errors in Temporal Difference (TD) learning in reinforcement learning. The authors introduce new counterexamples showing that regularization is insufficient to prevent divergence in offpolicy TD methods with function approximation. They demonstrate failure even under linear function approximation and neural networks and highlight potential drawbacks of regularization such as vacuous solutions training instability and divergence. The study emphasizes the need for a reevaluation of the role of regularization in TD methods to prevent potential pitfalls and underscores the complexity of applying regularization in reinforcement learning. 2) Strengths and Weaknesses Strengths:  The paper addresses a critical issue in reinforcement learning regarding the role of regularization in TD methods.  The introduction of new counterexamples enhances the understanding of the limitations of regularization in preventing divergence in offpolicy TD learning.  The study covers a wide range of scenarios from linear function approximation to neural networks providing a comprehensive analysis. Weaknesses:  The paper would benefit from a clearer outline of the proposed counterexamples at the beginning to guide readers through the study.  More detailed discussions on the practical implications of the findings such as recommendations for regularization strategies in TD algorithms could enhance the impact of the research. 3) Questions  How do the newly introduced counterexamples compare with existing literature that utilizes regularization in TD learning  Could the study provide insights into potential alternative methods to regularization for stabilizing offpolicy TD learning  Are there specific characteristics of neural network architectures that exacerbate or mitigate the challenges highlighted in the paper 4) Limitations  The paper focuses heavily on illustrating the pitfalls of regularization in TD learning but provides limited guidance on potential solutions or alternative strategies.  The complexity of the presented counterexamples may make it challenging for readers to fully grasp the implications without a more stepbystep explanation.  The study primarily concentrates on negative outcomes of using regularization potentially missing opportunities to explore critical positive aspects of regularization in specific scenarios.  Overall Evaluation: The paper offers valuable insights into the limitations of regularization in offpolicy TD learning in reinforcement learning. The introduction of new counterexamples fills a gap in the existing literature shedding light on the potential pitfalls of relying solely on regularization to mitigate instability. Further clarification on the practical implications of the findings and possible alternative approaches would enhance the impact and applicability of the research. With some revisions to provide clearer guidance and expand discussions on potential solutions the study can significantly contribute to the field of reinforcement learning and optimization.", "y-E1htoQl-n": "Peer Review 1) Summary: The paper introduces a robust training framework for Deep Reinforcement Learning (DRL) named Worstcaseaware Robust RL (WocaRRL) that ensures agents are robust against adversarial attacks with a bounded budget. The proposed framework evaluates and optimizes the longterm robustness of an RL agent without requiring additional samples for learning an attacker. The method outperforms existing robust training methods in terms of efficiency and robustness as demonstrated in experiments on various environments including MuJoCo and Atari games. 2) Strengths and weaknesses: Strengths:  The paper addresses the critical issue of adversarial robustness in DRL with a novel robust training framework (WocaRRL).  The proposed framework demonstrates superior performance and efficiency compared to existing stateoftheart methods as shown through experiments on MuJoCo and Atari environments.  The paper provides clear explanations and interpretations of the key mechanisms of WocaRRL enhancing the understanding of the proposed approach.  The detailed ablation studies verify the effectiveness of the individual components of WocaRRL. Weaknesses:  The paper focuses mainly on theoretical explanations and experimental results with limited discussion on realworld applications or deployment scenarios.  The discussion on the limitations of the proposed framework could be further elaborated to provide insights into potential challenges and future research directions.  While the experiments demonstrate the superiority of WocaRRL a comparison with more recent or advanced methods beyond the selected baselines could strengthen the evaluation. 3) Questions: 1. How does the worstattack value estimation in WocaRRL compare to other methods that estimate worstcase rewards under adversarial attacks 2. What are the implications of the proposed state importance weight (w(s)) in the context of training robust RL agents and how does it contribute to the overall performance of WocaRRL 3. Could the proposed framework be extended or adapted to handle different types of attacks beyond `p norm ball attacks such as patch attacks or other types of adversarial perturbations 4. Have there been considerations for further improving the computational efficiency of the proposed approach especially in largescale environments where scalability might be a concern 4) Limitations:  The paper focuses primarily on theoretical explanations and empirical evaluations lacking a detailed discussion on practical challenges scalability or realworld applicability.  The proposed framework is evaluated on selected environments and compared against specific baselines limiting the generalizability of the results to a broader spectrum of tasks or benchmarks.  The study focuses on robustness against `p norm attacks potentially overlooking other forms of adversarial perturbations that are prevalent in practical settings. Overall the paper presents a significant contribution to the field of DRL by introducing a novel robust training framework WocaRRL that enhances the adversarial robustness and efficiency of RL agents. Further exploration into the practical implications and extensions of the proposed framework could strengthen the impact of the research in realworld applications.", "vF3WefcoePW": "Peer Review Summary: The paper introduces a novel approach to training logic gate networks for efficient machine learning tasks. The authors propose differentiable logic gate networks that use a continuously parameterized relaxation of the network to enable training with gradient descent. This approach achieves fast execution speeds surpassing conventional neural networks in inference time on tasks such as MNIST classification. The experiments demonstrate the effectiveness of the proposed method on various datasets showcasing superior performance in terms of accuracy model complexity and inference speed. Strengths: 1. The paper addresses the challenge of training logic gate networks that are conventionally nondifferentiable offering a valuable solution for efficient machine learning. 2. The proposed differentiable logic gate networks show significant speed improvements in inference over traditional neural networks making them promising for realworld applications. 3. A comprehensive set of experiments across different datasets demonstrates the effectiveness and efficiency of logic gate networks compared to alternative methods. 4. The paper provides detailed explanations of the proposed methodology training considerations and the distribution of operators enhancing the understanding of the proposed approach. Weaknesses: 1. The paper lacks a detailed discussion of the interpretability of logic gate networks compared to traditional neural networks which could be an important aspect for practical applications. 2. While the experiments show promising results further analysis on larger and more diverse datasets would strengthen the generalizability of the proposed method. 3. The comparison with other stateoftheart methods could be further elaborated to highlight the unique advantages of differentiable logic gate networks. 4. The study could benefit from a more indepth exploration of potential limitations or failure cases of the proposed approach to provide a comprehensive analysis. Questions: 1. How does the performance of differentiable logic gate networks scale with the complexity of the dataset such as datasets with highdimensional input spaces or intricate patterns 2. Could the authors elaborate on the potential implications and challenges of deploying logic gate networks in realworld applications especially in scenarios requiring interpretability and model explainability 3. How does the proposed method handle noise or uncertainty in the input data and what impact does this have on the networks performance and reliability 4. Are there considerations or modifications needed when applying the proposed methodology to tasks beyond classification such as regression or unsupervised learning Limitations: 1. The study focuses on relatively small architectures compared to large deep learning models limiting the current applicability of logic gate networks to largerscale tasks. 2. The lack of discussions on the interpretability and explainability of logic gate networks may pose challenges in practical applications where model transparency is crucial. 3. While the experiments showcase impressive speeds and efficiencies further validations on more diverse and challenging datasets are required to establish the broader utility and robustness of the proposed approach. 4. The paper could benefit from a more detailed exploration of failure scenarios or limitations of the proposed method to provide a comprehensive understanding of its capabilities. Overall the paper presents a novel and promising approach in the realm of efficient neural network architectures demonstrating notable advancements in inference speed and computational efficiency. Further developments and investigations in scaling interpretability and broader applications would greatly enhance the impact and relevance of logic gate networks in machine learning research and practice.", "wUctlvhsNWg": " Peer Review  1) Summary: The paper introduces the Personalized Online Federated MultiKernel Learning (POFMKL) algorithm based on random feature (RF) approximation to address the challenges of online federated multikernel learning. POFMKL allows clients to update subsets of kernels reducing computational complexity and communication costs while providing personalized models. The proposed algorithm is theoretically analyzed for sublinear regret and experimentally validated for regression tasks showing improved accuracy compared to existing algorithms.  2) Strengths:  The paper introduces an innovative algorithm POFMKL addressing communication efficiency and heterogeneous data distribution challenges in online federated multikernel learning.  The theoretical analysis proves sublinear regret for clients and the server demonstrating the algorithms effectiveness.  Experimental results on real datasets showcase the advantages of POFMKL over existing algorithms in terms of MSE and runtime performance.  The proposed algorithm offers personalized model updates allowing each client to select a subset of kernels thus enhancing adaptability and privacy.  Comparative analysis with existing algorithms highlights the superiority of POFMKL in terms of accuracy and efficiency.  3) Weaknesses:  The paper could provide more detailed explanations of the experimental setup including hyperparameters selection dataset characteristics and training configurations to facilitate reproducibility.  While the theoretical analysis is comprehensive a clearer explanation of the algorithms practical implementation details could enhance the understanding of the proposed approach.  The paper could benefit from discussing potential limitations or challenges in the implementation of the POFMKL algorithm in practical scenarios.  4) Questions:  Could you elaborate on how the POFMKL algorithm handles sequential data streams and the adaptability of the algorithm to varying data distributions  How does the algorithm deal with concept drift or changes in the underlying data distribution over time in the federated learning setting  Are there any specific criteria or guidelines for selecting the number of random features (D) and the subset of kernels (M) in the POFMKL algorithm for optimal performance  How does the POFMKL algorithm ensure model privacy and data security in the federated learning setup  5) Limitations:  The paper lacks a comprehensive discussion on the scalability and performance of the POFMKL algorithm with increasing dataset sizes or network complexities.  While the experimental results are promising the evaluation on a broader range of datasets and scenarios could provide more insights into the algorithms robustness.  The study could benefit from assessing the sensitivity of the algorithm to hyperparameters and variations in the number of clients or kernels.  Further exploration of practical constraints such as communication latency resource limitations or network disruptions in a federated learning environment may enhance the realworld applicability of the proposed algorithm. Overall the paper presents a novel algorithm POFMKL with promising theoretical guarantees and empirical results for online federated multikernel learning. Addressing the raised questions and filling the identified gaps could strengthen the impact and applicability of the proposed algorithm in realworld scenarios.", "monPF76G5Uv": " Peer Review Summary: The paper introduces the Globally Normalized Autoregressive Transducer (GNAT) model to address the label bias problem in streaming speech recognition. Through theoretical analysis and empirical results on the Librispeech dataset the GNAT model achieves a significant reduction in the word error rate (WER) gap between streaming and nonstreaming speech recognition models. The model is developed in a modular framework that allows controlled comparison of modeling choices and the creation of new models. Strengths: 1. The paper addresses a relevant problem in streaming speech recognition and offers a novel solution with promising results. 2. The theoretical underpinning of the model and its empirical evaluation on the Librispeech dataset provide a comprehensive understanding of the GNAT model performance. 3. The modular framework presented allows for systematic comparison of different components and flexibility in creating new models. 4. The use of a JAX implementation and opensourcing of codes enhances reproducibility and implementation by the research community. Weaknesses: 1. The paper could benefit from more direct comparison with existing globally normalized models in streaming speech recognition to highlight the distinct advantages of GNAT. 2. The scalability of the GNAT model with larger numbers of label contexts should be explored further to assess its practical limitations. Questions: 1. Can you elaborate on how the GNAT model differs fundamentally from other globally normalized models in speech recognition such as MMI and in what scenarios does GNAT offer clear advantages 2. How sensitive is the GNAT model to variations in the context dependency and alignment lattice choices especially in complex realworld datasets beyond Librispeech 3. What are the implications of the observed performance differences between locally and globally normalized models on computational complexity and training time especially in largescale applications Limitations: 1. The paper focuses primarily on empirical evaluation on the Librispeech dataset which may limit the generalizability of the results to diverse speech recognition tasks and datasets. 2. The discussion on the practical implications of deploying the GNAT model in realworld applications such as computational overhead and deployment challenges could be further elaborated. Overall the paper presents a novel approach to addressing label bias in streaming speech recognition and offers valuable insights into the impact of global normalization on reducing the WER gap between streaming and nonstreaming models. The modular framework and theoretical foundations strengthen the contributions of this work but further investigation into scalability and applicability in diverse contexts would enhance the impact of the GNAT model in the field of speech recognition.", "uOii2cEN2w_": " Summary The paper introduces an extension of Bayesian Optimization via DensityRatio Estimation (BORE) to the batch setting providing theoretical analysis and an improved version called BORE. The proposed algorithms leverage probabilistic classification instead of regression models demonstrating performance guarantees and scalability benefits in global optimization tasks. Theoretical analysis includes regret bounds and assessments against traditional Bayesian optimization (BO) methods.  Strengths and Weaknesses Strengths: 1. Novel Approach: The extension of BORE to batch optimization using densityratio estimation is innovative and addresses the need for efficient optimization methods in highdimensional domains. 2. Theoretical Rigor: Theoretical analysis of regret bounds and algorithm performance provides a solid foundation for the proposed methods. 3. Empirical Validation: The experiments on global optimization benchmarks and realdata datasets demonstrate the practical effectiveness of the proposed algorithms. Weaknesses: 1. Complexity: The paper is highly technical and might be challenging for readers without a deep understanding of Bayesian optimization and densityratio estimation. 2. Presentation Clarity: The paper could benefit from clearer structuring of sections for easier readability and comprehension.  Questions 1. How do the proposed BORE and BORE approaches handle noise in the objective function evaluations particularly in sensitive applications 2. Are there any comparative studies or analyses to evaluate the scalability of batch BORE compared to other batch optimization methods 3. How do the theoretical results especially regret bounds align with the observed performance in the conducted experiments across different benchmarks  Limitations 1. Model Performance on Real Data: The paper demonstrates strong performance on synthetic optimization benchmarks but realworld application results are limited to specific domains. Further validation across a broader range of applications is needed to assess generalizability. 2. Assumptions and Conditions: The mathematical derivations and results heavily rely on specific assumptions related to noise characteristics model priors and kernel functions. The applicability of these results to diverse realworld scenarios should be further investigated. Overall the paper presents a valuable contribution to the Bayesian optimization literature introducing innovative batch optimization methods and providing comprehensive theoretical analysis and empirical validation. The findings have the potential to advance optimization algorithms for complex and highdimensional problems.", "mTXQIpXPDbh": " Summary The paper introduces a novel memoryefficient transfer framework called Back Razor that addresses the memory footprint challenge in transfer learning. Back Razor employs asymmetric sparsifying by pruning the activations used for backpropagation while keeping forward activations dense. The proposed method achieves up to 97 sparsity saving 9.2x memory usage without loss of accuracy. The experimental results on CNNs and Vision Transformers demonstrate the effectiveness of Back Razor in reducing memory usage while maintaining performance across various tasks.  Strengths 1. Novel Approach: Back Razor introduces a novel asymmetric sparsifying method for memoryefficient transfer learning demonstrating significant memory savings without compromising accuracy. 2. Theoretical Analysis: The paper provides a theoretical analysis of Back Razor showing a similar convergence rate to vanilla SGD under mild conditions. 3. Extensive Experimentation: The method is validated through thorough experiments on CNNs and ViTs for multiple tasks showcasing its effectiveness in reducing memory consumption. 4. Comparison with StateoftheArt: Comparative results with previous methods demonstrate the superiority of Back Razor in terms of memory efficiency and performance.  Weaknesses 1. Limitation to Specific Tasks: The paper mainly focuses on transfer learning tasks and may not cover a wide range of applications. 2. Theoretical Limitation: The theoretical analysis is limited to CNNs with a potential future direction for extending it to Transformers. 3. Applicability to New Architectures: The applicability of Back Razor to new neural network architectures beyond the tested ones remains to be explored.  Questions 1. How does the choice of pruning method for activation sparsification affect the overall performance of Back Razor 2. Can Back Razor be further optimized for specific downstream tasks such as object detection or reinforcement learning 3. What computational overhead if any does the asymmetric sparsifying approach introduce during training 4. Are there any potential tradeoffs or limitations to consider when applying Back Razor to extremely large datasets or complex models  Limitations 1. Limited Theoretical Framework: The theoretical analysis of convergence is restricted to linear networks and needs to be expanded to nonlinear networks like Transformers. 2. Architectural Dependency: The applicability of Back Razor to various neural network architectures beyond CNNs and ViTs needs further exploration. 3. TaskSpecific Validation: Although experiments cover a range of tasks further validation on diverse datasets and tasks would enhance the generalizability of Back Razor.", "wGF5mreJVN": "Peer Review 1) Summary The paper presents a novel approach to web navigation using behavioral cloning to learn effective link selection policies demonstrated on a graph version of Wikipedia. The model achieves high success rates in efficiently navigating between nodes and is applied to downstream fact verification and question answering tasks with competitive results. The approach is proven effective and scalable offering a viable solution to the problem of web navigation. 2) Strengths and weaknesses Strengths:  The paper addresses a significant challenge in web navigation with a wellstructured approach.  The use of behavioral cloning for learning effective link selection policies is novel and shows promising results.  The experiments conducted on the Wikipedia graph demonstrate the scalability and effectiveness of the approach.  Competitive results in downstream fact verification and question answering tasks highlight the practical utility of the proposed method.  Detailed explanations of the trajectory distribution model parameterization and application to different tasks provide a comprehensive understanding of the approach. Weaknesses:  The paper lacks a detailed discussion on potential limitations of the approach such as the generalizability to different web environments or the need for a good target encoder.  The comparison with existing approaches could be more extensive to provide a clearer evaluation of the proposed method against the stateoftheart.  The paper could benefit from a more thorough discussion on the computational complexity and scalability of the approach when applied to even larger web graphs beyond Wikipedia. 3) Questions  How does the proposed approach compare to reinforcement learning methods for web navigation  Can the model adapt to dynamic changes in the web environment or is it limited to static graph structures like Wikipedia  What are the implications of requiring a good target encoder for the success of the navigation tasks  How does the pretrain  finetune paradigm impact the performance and generalizability of the model across different downstream tasks 4) Limitations  The paper focuses primarily on navigation within graphstructured web data derived from Wikipedia limiting the generalizability of the approach to broader web environments.  The need for a good target encoder suggests potential challenges in scenarios where groundtruth target nodes are not readily available.  The work would benefit from an indepth analysis of the computational complexity and resource requirements when applied to larger web graphs beyond Wikipedia. Overall the paper presents a valuable contribution to the field of web navigation through behavioral cloning and effective link selection policies. Strengthening the discussion around limitations comparisons with existing methods and scalability considerations would enhance the overall impact and applicability of the proposed approach.", "z9poo2GhOh6": " Peer Review: 1) Summary: The paper investigates the dynamics of large batch stochastic gradient descent with momentum (SGDM) on the least squares problem in the scenario where both sample size and dimensions are large. Through theoretical analysis the study shows that the behavior of SGDM converges to a deterministic discrete Volterra equation with the increase in dimension. The authors introduce a stability measurement the implicit conditioning ratio which influences the acceleration capability of SGDM. The research provides insights into the performance of SGDM in different batch size regimes and offers optimal choices for learning rate and momentum parameters. Moreover the study highlights the importance of the problem and algorithmic constrained regimes and presents lower bounds to demonstrate convergence rates. The paper concludes by discussing the implications of their findings and suggests possibilities for future research. 2) Strengths and Weaknesses: Strengths:  Comprehensive analysis of the behavior of SGDM in large batch scenarios with detailed theoretical derivations and interpretations.  Introduction of a novel stability measurement the implicit conditioning ratio to explain the acceleration dynamics of SGDM.  Proposing explicit choices for optimal learning rate and momentum parameters to achieve improved performance.  Clear presentation of results through mathematical formulations propositions and corollaries.  Discussion of convergence rates and conditions in problem and algorithmically constrained regimes enhances the understanding of SGDM performance.  Theoretical analysis supported by numerical experiments on isotropic features model and MNIST dataset. Weaknesses:  The study heavily focuses on theoretical analysis which might make it challenging for readers not wellversed in the mathematical formalism to grasp the main contributions.  The paper could benefit from a more detailed explanation of the practical implications and applications of the findings to bridge the gap between theory and realworld machine learning applications.  The limitations of the theoretical assumptions and potential generalizability of the derived results should be discussed in more depth.  While the results are supported by numerical experiments a more detailed discussion on the experimental setup methodology and results is needed to provide a holistic evaluation of the research. 3) Questions:  Could you elaborate on how the implicit conditioning ratio (ICR) impacts the convergence behavior of SGDM in different batch size regimes  How do the proposed optimal learning rate and momentum parameters in the study compare with existing methods or practices in the field of stochastic optimization  Can you discuss the practical implications of the derived convergence rates and conditions in the problem and algorithmically constrained regimes for realworld machine learning tasks  What are the considerations for applying the findings of the study to datasets beyond the orthogonal invariant data matrix scenario considered in the analysis  Could you provide additional insights into the potential applications of the theoretical developments in practical machine learning models and algorithms 4) Limitations:  The study primarily focuses on the behavior of SGDM on the least squares problem with specific theoretical assumptions potentially limiting the generalizability of the findings to diverse machine learning tasks and datasets.  The heavy emphasis on theoretical analysis might present challenges in understanding the practical implications and applications of the research results for machine learning practitioners.  The applicability of the derived optimal parameters and convergence rates to realworld datasets and scenarios could be constrained by the assumptions made in the study. Overall the paper presents a detailed analysis of the dynamics of large batch stochastic gradient descent with momentum offering valuable insights into the behavior and optimization performance of SGDM in highdimensional settings. Addressing the identified weaknesses and providing further clarification on the practical implications and limitations of the research would enhance the impact and relevance of the study.  End of Peer Review ", "pqCT3L-BU9T": " Peer Review  Summary The paper introduces Matformer a transformer architecture designed for periodic graph representation learning of crystal materials. The key focus is on addressing the challenges of encoding periodic structures present in crystals including the need for periodic invariance and explicit capturing of repeating patterns. The proposed Matformer outperforms baseline methods on benchmark datasets showcasing the importance of periodic invariance and pattern encoding for crystal representation learning.  Strengths 1. Novel Contribution: The paper makes a significant contribution by introducing the concept of periodic invariance and explicit pattern encoding for crystal representation learning which is a unique and important aspect often neglected in existing studies. 2. Experimental Results: The experimental results demonstrate the superior performance of Matformer over baseline methods. The comparison across different tasks and datasets shows consistent improvement highlighting the effectiveness of the proposed approach. 3. Thorough Explanation: The paper provides detailed explanations of the proposed methods graph construction strategies and the message passing scheme of Matformer enhancing the understanding of the model architecture and its components.  Weaknesses 1. Complexity: The paper introduces new concepts like periodic invariance and pattern encoding which may be difficult for readers unfamiliar with crystal representation learning to grasp. Simplifying the descriptions or providing more intuitive examples could help improve clarity. 2. Code Availability: While the authors mention that the code is publicly available a direct link in the paper or more information on the implementation details could enhance reproducibility and facilitate further exploration by the research community. 3. Limited Discussion: The discussion on the limitations of the work is brief. Providing more insight into potential challenges or future directions could enrich the papers conclusions and recommendations.  Questions 1. Model Scalability: How does the performance of Matformer scale with increasingly larger crystal structures or datasets Have the authors considered the computational complexity and scalability of the model 2. Realworld Applicability: Could the proposed Matformer architecture be practically implemented for crystal material property prediction in industrial or research settings Are there any specific challenges or considerations for realworld deployment 3. Generalizability: How generalizable is Matformer beyond crystal structure representation learning Are there potential applications in other areas where periodic graph representations are relevant  Limitations 1. Angular Information Complexity: The paper briefly mentions the complexity of introducing angular information and its impact on running efficiency. Further exploration into the tradeoffs between performance gains and computational resources could provide more clarity on the practicality of incorporating angular information. 2. Societal Impacts Discussion: While the paper acknowledges potential negative societal impacts of material discovery further elaboration on how these impacts relate to the work or implications for ethical considerations could enhance the papers depth. Overall the paper presents a novel and impactful contribution to the field of crystal material representation learning. Addressing the identified weaknesses and questions could further enrich the discussion and strengthen the implications of the research findings.", "u3vEuRr08MT": "Peer Review: 1) Summary: The paper investigates the memorization dynamics of large language models through empirical studies focusing on causal and masked language modeling across different model sizes and training phases. The study reveals that larger models memorize faster with a larger portion of the data before overfitting and tend to forget less throughout training. The impact of model size learning rate and dataset size on memorization dynamics are explored along with the memorization rates of different parts of speech. The findings contribute to understanding language model training dynamics as they scale up. 2) Strengths and weaknesses: Strengths:  The paper conducts comprehensive empirical studies on the memorization dynamics of language models providing insights into how model size influences memorization.  The investigation into forgetting curves and the impact of model size on forgetting adds valuable contributions to the field.  The study design is wellstructured allowing for controlled experiments and clear data analysis. Weaknesses:  The paper lacks a clear discussion on the broader implications of the findings in realworld applications or practical use cases.  The explanation of certain technical concepts such as the memorization metric and experimental setups could be more accessible to a broader audience. 3) Questions:  How do the findings related to memorization dynamics translate to practical applications or model performance in realworld scenarios  Are there specific language tasks or applications where the memorization dynamics identified in the study would have a significant impact on model performance  Have the authors considered exploring the relationship between memorization dynamics and model interpretability in language models 4) Limitations:  The study primarily focuses on theoretical aspects of memorization dynamics in language models and may benefit from further realworld validation or case studies.  The discussion on generalization and transferability of memorization dynamics could be expanded to provide a more comprehensive understanding of model performance.  The paper lacks a detailed analysis of the implications of the findings on model robustness or adversarial attacks in language understanding tasks. Overall the paper presents valuable insights into the memorization dynamics of large language models and contributes to understanding the behavior of models as they scale up in size. Further exploration of the practical implications and broader applications of the findings could enhance the impact of the research in the field of natural language processing.", "x26Mpsf45P3": " Peer Review: 1. Summary: The paper presents a reinforcement learning principle for approximating the Bellman equations by enforcing their validity along userdefined test functions in the context of modelfree offline reinforcement learning (RL) with function approximation. The proposed method is used to derive confidence intervals for offpolicy evaluation and optimize policies within a given policy class. The work provides theoretical guarantees particularly focusing on the linear setting. 2. Strengths:  The paper introduces a novel approach to reinforcement learning addressing fundamental problems in offline RL with function approximation. The emphasis on confidence intervals for policy evaluation and optimization is a significant contribution.  The use of test function spaces to enforce Bellman residual constraints and derive theoretical guarantees is innovative and adds value to the RL literature.  The analysis in the linear setting and the practical implementation details provided offer insights into computationally tractable policy optimization procedures without assuming Bellman completeness.  The connection to concentrability coefficients and the examination of different test function classes for error control provide a comprehensive exploration of the theoretical foundations. 3. Weaknesses:  The paper is dense and technical making it challenging for readers unfamiliar with the specific concepts and mathematical formulations in reinforcement learning. Improved clarity in presenting the key ideas and results would enhance the accessibility of the paper.  The practical implications and potential realworld applications of the proposed methodology could be elaborated further to contextualize the theoretical contributions in a more applied setting.  The analysis assumes certain technical conditions and assumptions which may limit the generalizability of the proposed method to more complex or practical RL scenarios. Providing insights into the robustness of the approach in diverse RL environments would strengthen the paper. 4. Questions:  How does the proposed reinforcement learning principle compare to existing methods in terms of practical applicability computational efficiency and scalability to complex RL settings  Can the paper elaborate on the implications of the findings for practical offline RL applications and potential extensions to realworld problems  How sensitive is the approach to the choice of test function spaces and are there guidelines for selecting optimal test functions in different RL scenarios 5. Limitations:  The technical nature of the paper may hinder its accessibility to a wider audience potentially limiting the dissemination of the novel findings.  The assumptions made in the analysis such as weak realizability and Bellman closure might not hold in all RL scenarios raising concerns about the generalizability of the proposed approach.  The focus on theoretical guarantees and algorithmic development may overshadow practical considerations and deployment challenges in realworld RL applications. In conclusion the paper presents a novel reinforcement learning principle for offline RL with function approximation offering theoretical insights into policy evaluation and optimization. Enhancements in clarity practical implications and generalizability could further strengthen the impact and relevance of the research in the field of reinforcement learning.", "r-CsquKaHvk": "Peer Review: 1) Summary (avg word length 100): The paper introduces an algorithm that estimates a survival model in a dynamicdata setting by exploiting a temporalconsistency condition. Inspired by temporaldifference learning the algorithm provides better predictive performance with fewer samples compared to existing methods. The study demonstrates the benefits of leveraging sequence dynamics in survival analysis through a discretetime model offering insights into bridging reinforcement learning and survival analysis. Experimental evaluations on synthetic and realworld datasets showcase the algorithms effectiveness and sample efficiency. 2) Strengths and Weaknesses: Strengths:  The paper addresses a novel and relevant problem by integrating insights from reinforcement learning into survival analysis in dynamic settings contributing to the convergence of two separate academic communities.  The proposed algorithm demonstrates better sampleefficiency and predictive performance compared to traditional methods by exploiting temporal consistency.  The authors provide a clear and logical explanation of the algorithm supported by theoretical foundations and illustrative examples.  Empirical evaluation on synthetic and realworld datasets substantiates the algorithms statistical benefits and practical value in various applications. Weaknesses:  The paper could benefit from a more extensive discussion on the practical implications of the algorithm in realworld scenarios and potential limitations that might arise in varied application domains.  The generalization of the algorithm to complex scenarios like competing risks or irregularlysampled sequences requires further exploration and validation.  More indepth analysis of the running time of the proposed algorithm compared to traditional methods and the factors influencing convergence could enhance the practical understanding of the approach. 3) Questions:  Can the algorithm proposed in the paper handle scenarios with highly complex and interconnected sequence dynamics such as multistage processes or feedback loops  The paper mentions extensions to accommodate irregularlysampled continuoustime sequences. Could you elaborate on the challenges and strategies for implementing such extensions practically  In the experimental evaluation did you encounter any specific scenarios or datasets where the proposed algorithm exhibited limitations or suboptimal performance highlighting potential areas for improvement 4) Limitations:  One potential limitation of the study is the focus on parametric survival models which may not always capture the full complexity of realworld dynamic survival scenarios. Consideration of nonparametric or semiparametric models could provide a more flexible and comprehensive analysis in certain cases.  The assumption of a Markov chain for sequence dynamics might not hold in all realworld scenarios raising concerns about the algorithms applicability in highly nonMarkovian environments.  The proof of convergence and the stability of the algorithm in complex and diverse datasets warrant further investigation to ensure its scalability and robustness across different applications and data types. This paper contributes significantly to the field of survival analysis by introducing a novel algorithm that enhances predictive performance in dynamic scenarios showcasing the potential benefits of integrating insights from reinforcement learning. Further exploration and validation of the algorithms capabilities in diverse settings and extensive discussions on practical implications and extensions would strengthen the studys impact and relevance in the scientific community.", "o8vYKDWMnq1": " Peer Review: 1) Summary: This paper presents a comprehensive theoretical study on deep neural function approximation in reinforcement learning with \u03b5greedy exploration under the online setting. The study focuses on understanding deep RL from the perspective of function class and neural network architectures beyond the linear regime. The analysis explores valuebased algorithms with \u03b5greedy exploration using deep neural networks endowed by Besov and Barron function spaces to approximate an \u03b1smooth Qfunction in a ddimensional feature space. The main results show that scaling the width and depth of the neural network is sufficient for achieving sublinear regret in Besov spaces. The paper also explores the relationship between the smoothness of the Qfunction and regret bounds. The analysis framework reformulates the temporal difference error and transforms it into a generalization problem under the noniid setting. 2) Strengths:  The paper provides a rigorous theoretical analysis of deep RL using neural networks beyond the linear regime which contributes to the understanding of function approximation in RL.  The use of Besov and Barron function spaces in approximation through deep neural networks adds richness to the theoretical analysis.  The results demonstrate the efficiency of deep RL in terms of sublinear regret under the \u03b5greedy policy providing valuable insights for practitioners.  The theoretical framework and analysis presented in the paper are systematic and wellstructured. 3) Weaknesses:  The paper is highly theoretical and may be challenging for readers without a strong background in reinforcement learning theory and neural networks.  The practical implications of the theoretical findings could be further elaborated to facilitate the translation of these results into realworld applications.  The emphasis on the theoretical analysis may overshadow discussions on potential practical limitations or implementation challenges of the proposed approaches. 4) Questions:  How do the theoretical findings of the paper relate to practical implementations of deep reinforcement learning algorithms such as DQN in realworld scenarios  Are there any specific scenarios or tasks in deep RL where the \u03b5greedy exploration might be less effective and how could this be addressed theoretically or practically  How do the results of this study compare or complement existing theoretical frameworks or empirical studies in the field of deep RL 5) Limitations:  The paper primarily focuses on the theoretical analysis of deep RL using specific function spaces and exploration strategies which may limit the generalizability of the findings to diverse realworld applications.  The complexity of the theoretical framework and analyses might make it challenging for practitioners to directly apply the results in practical deep RL implementations.  The study relies heavily on assumptions and theoretical constructs which may not fully capture the complexity of realworld RL environments or neural network behaviors. Overall the paper presents a rigorous theoretical analysis of deep neural function approximation in reinforcement learning that contributes valuable insights to the field. Further discussions on the practical implications and potential limitations of the theoretical findings could enhance the relevance of the study for both research and application in deep RL.", "kuJQ_NwJO8_": " Peer Review  1) Summary (96 words) The paper introduces the SURGE framework for knowledgeconsistent dialogue generation in the context of Knowledge Graphs. Existing methods face challenges in leveraging structured knowledge. SURGE addresses this by first retrieving relevant subgraphs and then generating responses that reflect this knowledge. The empirical results on the OpendialKG dataset demonstrate the superiority of SURGE over baseline methods in terms of response quality and consistency with the provided knowledge.  2) Strengths and Weaknesses Strengths:  Introduction of the SURGE framework for generating knowledgeconsistent dialogues.  Comprehensive evaluation on the OpendialKG dataset showcasing the effectiveness of the proposed approach.  Novel use of Knowledgeverifying Question Answering (KQA) metric to evaluate factual correctness.  Detailed description of the model components and training objectives. Weaknesses:  The paper could benefit from more detailed literature review especially regarding existing methods in knowledgegrounded conversation generation.  Limited explanation of the datasets used and the reason for choosing the OpendialKG dataset.  Some complex technical terms could be explained more clearly for better understanding by a broader audience.  4) Questions  Can the SURGE framework be easily extended to handle multiturn dialogues and complex scenarios  How does the computational efficiency of SURGE compare to existing methods especially as the scale of knowledge graphs increases  5) Limitations  The evaluation is focused on a single dataset (OpendialKG) potentially limiting the generalizability of the findings.  The training details provided are minimal more specifics on hyperparameters selection and training procedure would enhance reproducibility.  Discussion of the scalability and resource requirements for deploying SURGE in realworld applications is lacking. Overall the paper presents a promising framework for knowledgegrounded dialogue generation with significant improvements over existing methods. Further clarification on certain aspects and addressing scalability concerns could enhance the impact of the research.", "t3X5yMI_4G2": " Peer Review: Summary: The paper introduces the concept of reincarnating reinforcement learning (RL) as an alternative to traditional tabula rasa RL aiming to leverage prior computational work for efficient training of RL agents. The researchers focus on transferring suboptimal policies to valuebased RL agents proposing a new algorithm QDagger to address limitations in existing approaches. The effectiveness of reincarnating RL is demonstrated on Atari games locomotion tasks and realworld problems like stratospheric balloon navigation. The study highlights the benefits of reincarnating RL in improving computational efficiency and democratizing RL research. Strengths: 1. The paper addresses a critical issue in RL research proposing a novel approach reincarnating RL that leverages existing computational work for more efficient training. 2. The development of the QDagger algorithm to address limitations in transferring suboptimal policies to RL agents is a significant contribution. 3. The empirical evaluation on various tasks including Atari games and realworld problems like balloon navigation provides concrete evidence of the benefits of reincarnating RL. 4. The study demonstrates the potential of reincarnating RL in democratizing RL research and making it more accessible to a broader community. Weaknesses: 1. The paper assumes a high level of familiarity with RL concepts and terminology which may limit accessibility to readers unfamiliar with the domain. 2. Some sections particularly the experimental setups and results could be more concise and structured for clarity. 3. While the paper addresses various aspects of reincarnating RL further discussion on scalability robustness and potential limitations of the proposed approach could enhance the completeness of the study. 4. More comparative analyses or discussions on the performance of the QDagger algorithm against existing methods could provide a deeper understanding of its effectiveness. Questions: 1. Can you elaborate on the scalability of the proposed reincarnating RL approach How well does it generalize to different types of RL tasks and agent architectures 2. In the comparison of different RL algorithms and methods what criteria were used to evaluate their performance and how do these metrics reflect the effectiveness of reincarnating RL 3. How does the QDagger algorithm compare to stateoftheart RL techniques in terms of both performance and computational efficiency 4. What are the potential challenges or limitations in implementing reincarnating RL in practical applications and how can these be addressed in future research Limitations: 1. The study focuses on specific applications and scenarios such as Atari games and locomotion tasks which may limit the generalizability of the findings to other RL domains. 2. While the QDagger algorithm shows promising results its performance across a broader range of RL tasks and benchmarks could provide a more comprehensive assessment of its capabilities. 3. The paper may benefit from additional discussions on the ethical implications of reincarnating RL particularly in terms of bias propagation and societal impacts. Overall the paper presents a novel concept of reincarnating RL and provides compelling evidence to support its effectiveness. Further exploration of the scalability generalizability and ethical considerations of reincarnating RL could enhance the impact of the research in the field of reinforcement learning.", "wwyiEyK-G5D": " Peer Review  Summary: The paper proposes the REVIVE method for knowledgebased Visual Question Answering (VQA) focusing on improving objectcentric visual representation. The REVIVE method utilizes regional visual information to retrieve knowledge and integrates visual features with retrieved knowledge in the answering model. Extensive experiments conducted on the OKVQA dataset show that REVIVE achieves stateoftheart performance with a 3.6 improvement in accuracy over previous methods. The paper systematically explores the role of visual features in knowledge retrieval and demonstrates the effectiveness of the proposed approach.  Strengths: 1. Innovative Approach: The paper introduces a novel method REVIVE which leverages regional visual representation for knowledgebased VQA filling a gap in existing research. 2. Experimental Rigor: The paper provides a detailed experimental analysis including ablation studies comparisons with stateoftheart methods and visualization of results demonstrating the effectiveness of the proposed approach. 3. StateoftheArt Performance: REVIVE achieves a significant performance improvement over existing methods on the OKVQA dataset showcasing its superiority in leveraging visual features for knowledgebased VQA tasks. 4. Public Availability: The authors have made the code publicly available enhancing reproducibility and facilitating further research in the domain.  Weaknesses: 1. Theoretical Framework: While the empirical results are comprehensive the paper lacks a detailed theoretical framework explaining the underlying principles behind the REVIVE method which could enhance the understanding of the proposed approach. 2. Evaluation Metrics: While the paper uses soft accuracy for evaluation discussing the limitations of this metric and exploring additional evaluation metrics could provide a more holistic assessment of the proposed methods performance.  Questions: 1. The paper mentions that the models of CLIP GLIP Vinvl and GPT3 are frozen during usage. Could the authors elaborate on the implications of using frozen models in the context of model performance and generalizability 2. How does the proposed method handle cases where object boundaries are ambiguous or when objects are occluded in the image Does the method exhibit robustness in such scenarios  Limitations: 1. Selection Bias: The use of pretrained language models and external knowledge resources may introduce selection bias affecting the generalizability of the approach to diverse datasets. 2. Generalization: While the method shows promising results on the OKVQA dataset its generalization to other datasets and reallife scenarios needs to be further explored to validate its broader impact and applicability. 3. Ethical Considerations: The paper briefly mentions the potential for societal impact but a more thorough analysis of ethical considerations biases and fairness in knowledgebased VQA systems could enhance the discussion. Overall the paper presents a novel approach with promising results in knowledgebased VQA. Addressing theoretical foundations exploring generalization to diverse datasets and delving deeper into ethical implications would further strengthen the contributions of the research.", "qSs7C7c4G8D": " Peer Review:  Summary: The paper presents a detailed analysis of federated learning (FL) with varying client participation patterns. The authors introduce a generalized FedAvg algorithm that amplifies parameter updates at intervals of multiple FL rounds. They provide a unified convergence analysis for FL with arbitrary client participation considering nonstochastic and stochastic cases. The novel methodology and results are discussed along with insights and experimental outcomes.  Strengths: 1. Theoretical Rigor: The paper demonstrates a strong theoretical foundation in analyzing FL convergence considering arbitrary client participation patterns. The formulation of the generalized FedAvg algorithm and the convergence analysis using unified methodology is commendable. 2. Contribution to the Field: The insights provided regarding regularized and stochastic participation patterns in FL contribute significantly to existing literature. The analysis on achieving zero error convergence and matching stateoftheart results is a valuable addition to the field. 3. Experimental Validation: The empirical results align well with the theoretical findings showcasing the practical benefits of the proposed generalized FedAvg algorithm with amplification compared to standard FedAvg and alternative waiting strategies.  Weaknesses: 1. Complexity: The detailed theoretical analysis while robust may be challenging for some readers to follow due to its complexity. Simplifying the explanation without compromising the core findings could enhance readability. 2. Practical Implementation: While the theoretical framework and experiments are welldocumented more insights on the practical implementation challenges and considerations in deploying the proposed algorithm in realworld scenarios could have added depth to the study.  Questions: 1. Generalizability: How applicable are the findings of the study to realworld FL scenarios with diverse datasets and client environments Are there specific use cases where the proposed algorithm may not perform optimally 2. Scalability: How does the performance of the generalized FedAvg algorithm scale with an increasing number of clients or data heterogeneity in practical applications Have scalability issues been addressed in discussion or future research directions 3. Comparison with Existing Works: How does the proposed algorithm compare and complement existing FL algorithms beyond standard FedAvg Are there specific limitations of the proposed algorithm highlighted in comparison with stateoftheart methods 4. Robustness Analysis: Are there sensitivity analyses or robustness tests conducted to assess the impact of varying parameters and assumptions on the convergence results How do different choices of hyperparameters affect the overall performance of the algorithm  Limitations: 1. Complexity vs. Accessibility: The intricate mathematical formulations and terminology may limit the accessibility of the study to a broader audience without a strong background in FL or optimization theory. 2. Realworld Application: Although the theoretical and experimental results are promising the translation of the findings into scalable and practical FL implementations may require additional investigation and validation in diverse use cases. In conclusion the paper presents a robust analysis of FL convergence in the presence of varying client participation patterns offering valuable insights and contributions to the field. Addressing the highlighted strengths weaknesses and questions could further enhance the impact and applicability of the study in advancing the understanding and implementation of FL algorithms.", "n6QYLjlYhkG": " Peer Review:  1) Summary: The paper introduces HYPRO a novel hybridly normalized probabilistic model for making longhorizon predictions of event sequences. The model combines an autoregressive base model with an energy function to improve the predictions by reweighting the proposals. The authors provide a detailed technical framework training objectives and inference algorithms for the HYPRO model. Experimental results on realworld datasets showcase the superiority of HYPRO over existing autoregressive models. Analysis of the models performance including comparisons with baselines and investigation of various components further support the effectiveness of HYPRO in handling longhorizon prediction tasks.  2) Strengths and Weaknesses: Strengths:  Introduces an innovative model HYPRO addressing the underinvestigated problem of longhorizon prediction of event sequences.  Comprehensive coverage of technical details including model architecture training objectives and inference algorithms.  Conclusive experimental results demonstrating the superior performance of HYPRO over existing models on realworld datasets.  Indepth analysis of various aspects of the models performance including discussions on the necessity of the energy function and the effectiveness of different training objectives. Weaknesses:  The paper could benefit from a more detailed discussion on the scalability of the proposed model for larger datasets or more complex sequences.  The authors could provide further comparisons with a wider range of stateoftheart models to strengthen the evaluation of HYPROs performance.  The ablation studies could be expanded to explore additional variations in the model or training methods to enhance the understanding of key components.  3) Questions:  How does the proposed HYPRO model scale in terms of computational efficiency with increasing dataset size or sequence complexity  Have you considered comparing HYPRO with other cuttingedge models or techniques in related areas to assess its competitiveness beyond the existing baselines  Could you elaborate on potential realworld applications where HYPRO could be particularly beneficial based on the strengths and unique features of the model  4) Limitations:  The investigation of event sequence predictions is limited to a few specific datasets possibly limiting the generalizability of the models performance.  The focus on comparing with autoregressive models may overlook the potential advantages of nonautoregressive or hybrid approaches not considered in the study.  The discussion on the interpretability of the models predictions or insights generated by HYPRO could be enhanced to offer more value to practitioners in various domains. Overall the paper presents a strong contribution to the field of event sequence prediction with the introduction of the HYPRO model. Further refinements in addressing scalability model comparisons and practical applications could elevate the impact and relevance of the research.", "nE6vnoHz9--": " Peer Review  1) Summary: The paper presents a novel approach CrossDomain Predictor (CDP) to address the challenges in Neural Architecture Search (NAS) by leveraging existing NAS benchmark datasets. CDP aims to enable efficient search for highperformance neural architectures in large search spaces by training a predictor using the benchmark data. The proposed method achieves promising results on ImageNet and CIFAR10 datasets with significantly reduced computational costs.  2) Strengths:  Innovative Approach: The use of existing NAS benchmark datasets to train a predictor for efficient architecture search is a novel and effective approach.  Efficiency: The reduced search cost of 0.1 GPU Days to achieve high classification accuracies on ImageNet and CIFAR10 demonstrates the efficiency of the proposed CDP.  Progressive Subspace Partition: The strategy to adapt subspaces progressively and address domain differences enhances the performance of the predictor.  Assistant Architecture Space: The introduction of an assistant space aids in smoothing the transfer between different architecture spaces.  3) Weaknesses:  Limited Evaluation: The evaluations are mostly focused on classification accuracy on ImageNet and CIFAR10. Including more diverse datasets or tasks could further demonstrate the generalizability of the proposed method.  Theoretical Background: The paper could benefit from a more detailed explanation of the theoretical concepts and their practical implications for a broader audience.  4) Questions:  Generalization: How does the proposed method generalize to other domains or tasks beyond image classification  Scalability: How scalable is the CDP approach to larger and more complex search spaces  Comparison: How does CDP compare with traditional NAS methods in terms of architecture diversity and performance stability  5) Limitations:  Small Search Spaces: The primary focus on NASBench101 and NASBench201 which are comparatively small search spaces raises concerns about scalability to larger and more diverse architecture spaces.  Limited Benchmarking: The paper lacks extensive benchmarking against a wider range of stateoftheart NAS methods which could provide a more comprehensive comparison. Overall the paper presents an innovative approach to address the challenges of NAS by leveraging existing benchmark datasets. The proposed CDP shows promising results in reducing search costs and achieving high accuracies but further evaluations and comparisons with a broader range of methods are recommended to validate its effectiveness across diverse tasks and domains. Additional clarity in theoretical explanations could also enhance the understanding and impact of the proposed method.", "ksVGCOlOEba": " Summary: The paper introduces a new compression framework for deep neural networks focusing on model compression in a posttraining setting without retraining. The framework covers both weight pruning and quantization in a unified manner resulting in improved compressionaccuracy tradeoffs compared to existing methods. The approach is based on the Optimal Brain Surgeon (OBS) framework extended to cover weight quantization for modern DNNs. Experimental results demonstrate significant improvements in compression performance enabling accurate compound applications of pruning and quantization.  Strengths: 1. Innovative Framework: The paper introduces a new compression framework that unifies weight pruning and quantization improving practical performance. 2. Technical Contribution: The ExactOBS algorithm reduces computational costs for solving the layerwise compression problem efficiently. 3. Experimental Results: The experiments show significant improvements in compressionaccuracy tradeoffs across different tasks and models demonstrating the effectiveness of the proposed framework. 4. Competitive Results: The results indicate that the approach performs competitively with stateoftheart methods achieving better accuracies in some cases. 5. Extensibility: The framework can be extended to structured pruning indicating potential for further optimizations and compatibility with diverse compression techniques.  Weaknesses: 1. Complexity: The detailed technical explanations may be challenging for readers without a strong background in neural network compression. 2. Experimental setup: The calibration datasets and tasks used in the experiments may not fully represent realworld scenarios potentially affecting the generalizability of the results. 3. Comparison Metrics: While the results are promising additional comparison metrics or benchmarks could provide a more comprehensive evaluation of the proposed framework.  Questions: 1. How does the proposed framework handle scalability to larger models and datasets beyond the tested models 2. Are there any specific limitations or constraints the framework faces in terms of realworld implementation especially in dynamic or changing environments 3. Can the framework adapt to scenarios involving different hardware configurations or platforms where optimization methods may vary 4. How does the framework address potential challenges related to model interpretability or explainability postcompression  Limitations: 1. Generalizability: The study mainly focuses on a specific compression setting and may require further validation across a broader range of tasks models and datasets. 2. Realworld applicability: The results may need validation in realworld deployment scenarios to assess the feasibility of the proposed framework in practical applications. 3. Latency Considerations: The paper does not extensively discuss latency implications of the compression techniques which are crucial for realtime inference applications. 4. Sensitivity Analysis: Sensitivity to hyperparameters or different calibration data sizes could be further explored to understand the robustness of the framework.", "pBpwRkEIjR3": " Summary The paper proposes a set of enhanced bilevel optimization methods utilizing the Bregman distance to tackle nonconvexstronglyconvex bilevel optimization problems with possibly nonsmooth regularization. The methods include BiOBreD for deterministic bilevel problems SBiOBreD for stochastic bilevel problems and accelerated ASBiOBreD method. The algorithms are evaluated through data hypercleaning and hyperrepresentation learning tasks demonstrating superior performance over existing approaches.  Strengths and Weaknesses Strengths: 1. Comprehensive and systematic approach proposed for enhanced bilevel optimization. 2. Implementation of novel methods based on Bregman distance accelerated techniques and stochastic gradient estimators. 3. Extensive theoretical analysis providing convergence guarantees and computational complexity comparisons. 4. Empirical evaluation showcasing the superiority of the proposed algorithms over existing techniques in realworld tasks. Weaknesses: 1. Theoretical concepts and notations might be complex for readers unfamiliar with the subject. 2. Limited realworld application scenarios evaluated further experiments with diverse datasets could enhance the generalizability of the methods. 3. The computational complexity of the proposed methods should be thoroughly evaluated for largescale datasets and highdimensional optimization problems.  Questions 1. How do the proposed algorithms handle noise and outliers common in practical datasets during the optimization process 2. Can the methods effectively scale to large datasets and highdimensional parameter spaces 3. Are there specific limitations of the proposed algorithms in terms of convergence speed or stability under different noise levels or smoothness of functions 4. How sensitive are the algorithms to hyperparameter settings and are there strategies proposed for automated hyperparameter tuning 5. Have the proposed methods been tested against alternative optimization techniques specifically designed for nonsmooth optimization problems or noiseresilient optimization settings  Limitations 1. The evaluation is limited to specific machine learning tasks and datasets potentially limiting the generalizability of the results. 2. The study lacks a thorough discussion on the limitations of the proposed methods in handling highly complex optimization landscapes or noisy datasets. 3. The theoretical analysis focuses primarily on convergence guarantees and computational complexity while practical usability aspects may need further exploration. 4. The comparison with more recent stateoftheart techniques in bilevel optimization could enhance the credibility and completeness of the evaluation. Overall the paper presents a comprehensive set of enhanced bilevel optimization methods with promising results but further investigation is needed to address potential limitations and enhance the practical applicability of the proposed algorithms.", "xvZtgp5wyYT": " Peer Review of the Paper \"Accelerating Simulation and Inverse Optimization of Partial Differential Equations using Latent Evolution (LEPDE)\"  Summary: The paper introduces a novel method named Latent Evolution of PDEs (LEPDE) to accelerate the simulation and inverse optimization of Partial Differential Equations (PDEs). LEPDE utilizes a global latent space representation to efficiently evolve the dynamics of PDEs leading to significant reduction in computational complexity and increased speed while maintaining competitive accuracy. The method is demonstrated through experiments on 1D and 2D benchmark problems showcasing its ability to generalize to new conditions and parameter settings.  Strengths: 1. Innovative method: The introduction of LEPDE utilizing latent space for evolution provides a unique approach to accelerate PDE simulations and inverse optimizations. 2. Significant results: The paper demonstrates substantial reduction in computational complexity speed improvement and competitive accuracy compared to stateoftheart models in both 1D and 2D benchmarks. 3. Thorough experimentation: The experiments cover diverse scenarios including novel PDE parameters and turbulent flows providing comprehensive validation of the methods performance. 4. Detailed methodology: The model architecture learning objectives and techniques for inverse optimization are described in detail enhancing reproducibility.  Weaknesses: 1. Complexity: The methods complexity might be a barrier to adoption for researchers without a strong background in deep learning or PDEs. A simpler explanation of the core concepts could improve accessibility. 2. Evaluation Metrics: While the experiments provide insightful results additional evaluation metrics such as convergence rates or robustness to noise could further enrich the analysis. 3. Scalability: The scalability of the method to higherdimensional problems or realworld applications with massive datasets could be an area for further investigation.  Questions: 1. Generalization: How does LEPDE perform when applied to more complex realworld problems with nonideal conditions or noisy input data 2. Interpretability: Can the latent space representations learned by LEPDE provide insights into the underlying physics of the systems being modeled 3. Comparison: How does the performance of LEPDE vary with different latent space dimensions and how sensitive is the method to hyperparameters in practice  Limitations: 1. Benchmark Selection: While the chosen benchmarks provide valuable insights extending the evaluation to additional realworld applications or more diverse PDE systems could strengthen the generalizability of the method. 2. Implementation Requirements: The computational resources and training time needed for the method are not fully discussed. Addressing these practical considerations could facilitate wider adoption by the research community. Overall the paper presents a novel and promising method for accelerating PDE simulations and inverse optimizations. Further exploration of its scalability interpretability and applicability to realworld problems could enhance its impact in scientific and engineering domains. This review provides constructive feedback for potential improvements and highlights the strengths and importance of the presented research.", "pF8btdPVTL_": " Peer Review 1. Summary: The paper investigates the phenomenon of benign overfitting in training twolayer convolutional neural networks (CNNs). It presents conditions under which CNNs trained by gradient descent can achieve small training and test losses demonstrating a phase transition between benign overfitting and harmful overfitting based on the signaltonoise ratio. The research provides new insights into the theoretical understanding of neural network training and generalization. 2. Strengths:  The paper addresses an important and timely issue in deep learning namely benign overfitting and presents novel insights into when and how this phenomenon occurs in neural networks.  The signalnoise decomposition technique used in the analysis is a strong point helping to elucidate the learning and memorization processes in CNNs.  The establishment of population loss bounds and negative results adds depth to the discussion providing a comprehensive analysis of the phase transition between benign and harmful overfitting.  The twostage analysis approach is wellstructured and helps in simplifying the complex relations among the coefficients enabling a clear understanding of the training process. 3. Weaknesses:  The papers clarity could be improved by providing more detailed explanations of the mathematical formulations and proofs. While the general idea is wellpresented some technical details might be difficult for readers without a strong background in the subject to follow.  The paper mentions extensions to deep convolutional neural networks as a future work direction. Including some brief discussions or insights on how the findings can be extended to more complex architectures would have added value.  It would be beneficial to discuss practical implications or applications of the research findings to make it more relatable to a broader audience of deep learning practitioners. 4. Questions:  Could the proposed signalnoise decomposition technique be applied to study the generalization behavior of other types of neural networks beyond CNNs  How do the conditions for benign overfitting in CNNs compare to those observed in linear models or other types of neural networks in prior research  Are there specific practical scenarios or applications in deep learning where understanding the phase transition between benign and harmful overfitting in neural networks can lead to significant improvements in performance or efficiency 5. Limitations:  The paper provides theoretical insights into benign overfitting in CNNs but the practical implications or applicability of these findings to realworld deep learning tasks are not extensively discussed.  The study focuses primarily on a specific twolayer CNN architecture and may not fully capture the complexities of deep neural networks with more layers or complex structures.  The research assumes certain conditions on parameters such as the signaltonoise ratio learning rate and initialization scale which may limit the generalizability of the findings to a wider range of scenarios. Overall the paper presents a valuable contribution to the theoretical understanding of benign overfitting in neural networks offering a new perspective on the training dynamics and generalization behavior of CNNs. Improved clarity in presentation and discussions on practical implications would enhance the impact and accessibility of the research.", "lhl_rYNdiH6": "Peer Review: 1) Summary: The paper introduces Contrastive Graph Structure Learning via Information Bottleneck (CGI) for recommendations. CGI optimizes graph structures by adaptively dropping edges or nodes to create optimized views of the original graph. It innovatively integrates information bottleneck into the contrastive learning process to enhance recommendation representations. The paper presents extensive experiments showcasing CGIs superiority over strong baseline methods across different datasets. 2) Strengths:  Novel Approach: CGI introduces a novel method for learning graph structures and enhancing recommendation systems via information bottleneck.  Extensive Experiments: The paper provides comprehensive experiments across various datasets to validate CGIs effectiveness.  Theoretical Justification: The theoretical proof regarding the application of the information bottleneck principle enhances the credibility of the proposed method.  Ablation Studies: Ablation studies demonstrate the efficacy of the learnable augmentation and information bottleneck aspects of CGI. Weaknesses:  Complexity: The paper can be challenging to digest due to the intricate technical details and concepts making it less accessible to a broader scientific audience.  Assumptions: The paper makes certain assumptions that may not be easily understandable without a deeper background in the field.  Clarity: Some parts of the paper lack clarity in explaining the technical details and the rationale behind certain decisions. 4) Questions:  How does CGIs performance scale with larger and more diverse datasets beyond the ones used in the experiments  Can you elaborate on the computational overhead of implementing CGI versus the baseline methods especially in terms of training time and resource requirements  What considerations were made while setting hyperparameters and are there any specific scenarios where parameter tuning significantly affects the results 5) Limitations:  The generalizability of CGI to other recommendation systems or scenarios beyond the scope of the study is not explicitly discussed.  The practical implementation challenges or potential limitations of deploying CGI in realworld applications are not deeply addressed.  The impact of varying noise levels or parameter settings on CGIs performance could provide insights into its robustness and adaptability. Overall Assessment: The paper introduces a promising method CGI for boosting recommendation systems. While the technical depth and experimental results showcase CGIs advantages further efforts could be made to improve clarity address generalizability concerns and explore practical implications for realworld deployment. The thorough analysis and empirical validation of CGI make it a strong contribution to the field of graphbased recommendation systems.", "rAVqc7KSGDa": "Peer Review Title: A Stochastic Active Sets Approach for Scaling Gaussian Process Decoders Summary: The paper proposes a novel Stochastic Active Sets (SAS) approach for scaling Gaussian Process (GP) decoders by approximating the logmarginal likelihood efficiently. The SAS approach is demonstrated to improve robustness and reduce computational cost in training GP decoders. The experimental results show that the SAS approach outperforms models trained using inducing points providing better structure in the latent space and achieving improved representations. Strengths: 1. The paper introduces a new and innovative Stochastic Active Sets approach for efficiently training GP decoders showcasing improved robustness and computational efficiency. 2. The theoretical foundation of the SAS approach linking active sets with crossvalidation and logmarginal likelihood is wellexplained providing a clear understanding of the methodology. 3. Empirical experiments on various datasets demonstrate the effectiveness of the SAS approach in learning better representations and scaling to a large number of observations. 4. The comparison with stateoftheart methods and the detailed evaluation metrics provide comprehensive insights into the performance of the proposed SASGP decoders. 5. The incorporation of amortization networks in the SAS approach and the discussion on practical benefits such as numerical precision and implementation simplicity contribute to the significance of the findings. Weaknesses: 1. The paper lacks a detailed comparison with a wider range of baselines beyond the mentioned models which could provide a more comprehensive evaluation of the SAS approach. 2. While the theoretical background is wellexplained the complexity of the proposed method and algorithm may be challenging for readers unfamiliar with Gaussian processes and variational inference. Questions: 1. How does the SAS approach handle datasets with highdimensional discrete data given the reliance on Gaussian likelihood in its current implementation 2. Can the SAS approach be extended to applications in supervised learning settings considering its success in scaling GP decoders for unsupervised learning tasks 3. What are the key factors influencing the selection of parameters in the SAS approach such as active set sizes and how do these choices impact the overall performance of the model 4. How does the SAS method perform in scenarios with noisy or incomplete data and how does it compare to other approaches in such cases Limitations: 1. The SAS approach is limited by its reliance on a Gaussian likelihood restricting its application to continuous data and warranting exploration of alternative likelihood models for discrete data. 2. The paper lacks detailed exploration of potential limitations of the SAS approach in handling complex data structures or noisy data which could present challenges in realworld applications. Overall the paper presents a novel and promising approach for training Gaussian Process decoders efficiently with clear theoretical foundations and empirical results. Addressing the mentioned weaknesses and questions could further enhance the impact and applicability of the SAS approach in various machine learning tasks.", "wOI0AUAq9BR": " Peer Review: Summary: The paper addresses the issue of poor performance of Graph Neural Networks (GNNs) in generalizing to graph sizes different from those in the training data. It introduces a regularization strategy that simulates a shift in the size of training graphs and enforces the model to be robust to such shifts improving sizegeneralization capabilities. The proposed method achieves performance improvements of up to 30 on standard datasets. Strengths: 1. The paper provides a detailed explanation of the problem and the proposed solution. 2. The regularization strategy is general and can be applied to any GNN simplifying its adoption across different models. 3. Experimental results show significant performance improvements across datasets demonstrating the effectiveness of the proposed method. 4. The ablation study and comparative analysis with other models add depth to the evaluation of the proposed strategy. Weaknesses: 1. The paper lacks a clear comparison with the current stateoftheart methods in the field. Addressing this would provide a more comprehensive evaluation of the proposed approach. 2. The limitations section could be expanded to discuss potential scenarios where the regularization strategy may not be effective. 3. The experimental methodology could benefit from discussing potential biases or limitations in the dataset split design. Questions: 1. How does the proposed regularization method compare to other methods that aim to improve sizegeneralization capabilities such as data augmentation techniques or transfer learning approaches 2. The paper mentions a 50 increase in training time with the introduced regularization. How does this overhead affect the scalability of the method for larger datasets or realtime applications 3. Is there an analysis of the computational complexity or resource requirements of the proposed regularization strategy compared to other methods Limitations: 1. The study primarily focuses on synthetic graphs and does not extensively explore realworld graph datasets limiting the generalizability of the results. 2. The evaluation is based on a specific traintest split design and the performance in different split scenarios or realworld applications may vary. 3. The conclusions could be further strengthened by additional experiments on a wider range of datasets to validate the robustness and effectiveness of the proposed approach. Overall the paper presents a novel regularization strategy to improve GNN performance in graph sizegeneralization with promising results. Addressing the mentioned weaknesses and limitations would enhance the impact and robustness of the proposed method.", "nC8VC8gVGPo": " Peer Review: 1. Summary: Spiking neural networks (SNNs) have garnered significant attention due to their biological plausibility and energy efficiency. However the lack of an efficient and generalized training method for deep SNNs especially for deployment on analog computing substrates remains a challenge. This paper introduces the Local Tandem Learning (LTL) rule a learning method for SNNs that combines features of ANNtoSNN conversion and gradientbased training approaches. By leveraging intermediate feature representations of pretrained ANNs rapid network convergence is achieved on the CIFAR10 dataset within five training epochs with low computational complexity. The LTL rule also demonstrates compatibility with different neuron models hardwarefriendliness and robustness against device nonideality issues. 2. Strengths:  The paper addresses an important research gap by proposing a novel learning rule for deep SNNs blending the advantages of different training approaches.  The LTL rule showcases rapid network convergence high accuracy and hardwarefriendliness which are crucial for deployment on mixedsignal neuromorphic computing chips.  Experimental results across CIFAR10 CIFAR100 and Tiny ImageNet datasets demonstrate effectiveness in achieving comparable accuracies to teacher ANNs.  The study systematically evaluates the proposed LTL rule in terms of pattern recognition learning convergence memory and time complexity and robustness to hardwarerelated noises.  The onchip implementation and comparison with existing approaches provide valuable insights into the efficiency and scalability of the proposed method. 3. Weaknesses:  The paper could benefit from a clearer delineation of the novelty of the LTL rule compared to existing methods. Providing more context on how the LTL rule improves upon current stateoftheart techniques would strengthen the contribution.  While the experimental results are extensive a more comprehensive comparison with a wider range of existing approaches including detailed performance metrics could enhance the evaluation.  Some technical details such as the rationale behind specific hyperparameters or design choices could be elaborated further to aid in understanding the proposed approach. 4. Questions:  Can the LTL rule be easily adapted to different network architectures beyond VGG models and are there specific considerations for scaling the approach to larger or more complex networks  How does the LTL rule compare in terms of training time and resource requirements to existing methods particularly when considering practical deployment on hardware platforms  Are there any limitations or tradeoffs associated with implementing the LTL rule in terms of accuracy convergence speed or hardware constraints that need to be considered in realworld applications 5. Limitations:  The experimental evaluation focuses primarily on image classification tasks expanding the study to include diverse applications or datasets could provide a more comprehensive assessment of the LTL rules generalizability.  The paper does not extensively address potential challenges or limitations of the proposed method especially in scenarios with noisy or dynamic environments which could affect realworld applicability.  While the onchip implementation is highlighted as an advantage more insights into the practical considerations scalability and any constraints associated with deploying the LTL rule on hardware would enhance the discussion. Overall the paper presents a promising contribution in the field of SNN training methods showcasing significant advancements in terms of efficiency accuracy and hardware compatibility. Clarifications on the novelty wider applicability and practical implications of the LTL rule along with a more detailed discussion on limitations and future directions would further strengthen the research findings.  End of Review.", "kRgOlgFW9aP": " Peer Review: 1) Summary: The paper introduces Thompson sampling for datadriven control of diffusion processes with uncertain drift matrices a problem where conventional discretetime analysis techniques are not applicable. The paper provides theoretical results and empirical simulations to validate the effectiveness of Thompson sampling in stabilizing the system and minimizing the cost. Various technical challenges are addressed and the algorithm is shown to balance exploration and exploitation efficiently. The paper concludes with numerical analysis confirming the theoretical findings through simulations on flight control scenarios. 2) Strengths and Weaknesses: Strengths:  The paper addresses a novel and challenging problem of datadriven control of diffusion processes with uncertain drift matrices.  Theoretical results are well substantiated with rigorous mathematical analysis providing clear insights into the properties of Thompson sampling in this context.  The empirical validations support the theoretical claims enhancing the credibility of the proposed approach.  The paper offers a comprehensive analysis of the algorithms stability explorationexploitation balance and performance with respect to regret and estimation error rates. Weaknesses:  The paper is highly technical and may be challenging for nonexperts to follow especially in the detailed mathematical derivations.  While the numerical results are discussed a more detailed analysis and comparison with alternative methods could further strengthen the empirical validation. 3) Questions:  How does the proposed algorithm scale with the dimensionality of the diffusion process and the complexity of the control problem  Are there specific assumptions or constraints on the diffusion process or drift matrices for the theoretical guarantees to hold  Can the algorithm handle nonlinearities or more complex dynamics in the diffusion process beyond the linear stochastic differential equations considered in the paper 4) Limitations:  The focus on linear stochastic differential equations limits the generalizability of the findings to other types of diffusion processes or control problems.  The complexity of the theoretical analysis and the technical requirements for the algorithm implementation may hinder practical adoption by researchers or practitioners with limited mathematical background. Overall the paper presents a valuable contribution to the field of datadriven control of dynamical systems particularly in the context of diffusion processes. Further clarification on the applicability and scalability of the proposed algorithm along with potential extensions to nonlinear models would enhance the impact and relevance of the work.", "siG_S8mUWxf": " Peer Review: Summary The paper proposes a novel backbone Subequivariant Graph Neural Network (SGNN) to address challenges faced by Graph Neural Networks (GNNs) in learning physical dynamics. Key contributions include relaxing equivariance to subequivariance introducing a novel subequivariant objectaware message passing and operating in a hierarchical fashion. Experimental results demonstrate the effectiveness of SGNN in contact prediction accuracy and data efficiency compared to stateoftheart GNN simulators on Physion and RigidFall datasets. Strengths: 1. The paper addresses important challenges in physical dynamics simulation using GNNs and proposes innovative solutions with clear theoretical justifications. 2. The methodology is wellstructured with indepth explanations on the formulation of subequivariance and objectaware message passing. 3. Experimental results showcase substantial improvements in contact prediction accuracy and data efficiency compared to existing simulators. 4. The ablation studies provide insightful analysis on the contribution of different components to the overall performance of SGNN. Weaknesses: 1. The paper could benefit from a more detailed comparison with other recent approaches beyond the mentioned baselines to highlight the uniqueness of SGNN. 2. Some sections might be overly technical and could be simplified for better understanding by a broader audience. 3. The generalization to scenarios involving rotations around nongravity axes could be further explored to analyze the models adaptability to complex environments. Questions: 1. How does the proposed SGNN handle scenarios with varying degrees of noise in the particlebased representation of physical systems 2. Could the models performance be influenced by the complexity of the physical interactions in scenarios with more deformable or interacting objects Limitations: 1. The paper primarily focuses on particlebased representations limiting the generalizability to scenarios with different types of physical systems. 2. The applicability of SGNN in scenarios involving selfcontact is not explicitly addressed presenting a potential gap in the models coverage. Overall the paper presents a novel approach SGNN for learning physical dynamics with GNNs showcasing significant improvements in accuracy and efficiency. Further investigations on noise resilience complex interactions and broader scenarios could enhance the robustness and applicability of the proposed model. Verdict: The paper offers valuable contributions to the field of physical dynamics simulation using GNNs. With improvements in clarity and broader scenario analysis SGNN has the potential to advance research in learningbased physical reasoning and simulations.", "wtuYr8_KhyM": " Peer Review  1) Summary The paper proposes a novel activation function named Adaptive SwisH (ASH) that facilitates different thresholds and adaptive activations based on the positions of units and the context of inputs in deep neural networks. ASH is designed to rectify inputs considering their contexts adaptively and demonstrates superior performance in various deep learning tasks. The theoretical foundations and experimental results presented in the paper validate the effectiveness of ASH in terms of accuracy convergence and feature extraction.  2) Strengths and Weaknesses Strengths:  The paper addresses the limitations of existing activation functions by introducing a novel adaptive activation function ASH that considers the context of inputs.  The proposed ASH activation function is mathematically formulated effectively and demonstrates superior performance in various tasks compared to traditional activation functions.  Experimental results show that ASH enhances accuracy localization and training time in classification detection and segmentation tasks.  The paper provides theoretical explanations for the superiority of the Swish activation function establishing ASH as a generalized form of it. Weaknesses:  The paper lacks a comparison with a wider range of stateoftheart activation functions to showcase ASH in a broader context.  The paper could benefit from more detailed explanations or visualizations of the mathematical derivations and properties of ASH to aid understanding.  While the experiments demonstrate the superior performance of ASH a more indepth analysis of the results and comparisons with different datasets could strengthen the conclusions.  3) Questions  How does the computational complexity of ASH compare to other activation functions especially in scenarios with larger datasets or models  Can you provide insights into the robustness of ASH in handling noisy or adversarial inputs compared to traditional activation functions  Have you considered investigating the interpretability of ASH activation function in comparison to other activation functions for understanding model decisions  4) Limitations  The generalizability of ASH across different domains or datasets beyond the ones tested remains unclear. Further testing on diverse datasets could strengthen the claims made.  The paper focuses on the theoretical and empirical performance of ASH in deep learning tasks. Further research could explore the interpretability and transferability of ASH in different applications or domains.  The limitations of ASH in scenarios with limited data availability or specific network architectures should be discussed to provide a comprehensive understanding of its applicability. Overall the paper presents a novel and promising activation function ASH that shows significant improvements in deep learning tasks. Further exploration and validation in diverse scenarios could enhance the understanding and applicability of ASH in realworld applications.", "rF6zwkyMABn": " Summary: The paper discusses online learning with general directed feedback graphs presenting bestofbothworlds algorithms that achieve tight regret bounds for adversarial environments and polylogarithmic regret bounds for stochastic environments. The study proposes algorithms for strongly observable and weakly observable graphs resolving open questions and improving on existing results. The algorithms are based on the followtheregularizedleader approach with novel update rules for learning rates. The paper provides regret bounds for both types of graphs and includes thorough theoretical analyses to support the proposed algorithms.  Strengths: 1. The paper addresses a significant problem in online learning with feedback graphs and proposes innovative algorithms to achieve nearly tight regret bounds. 2. The study provides a detailed theoretical analysis supporting the proposed algorithms and validates them with rigorous proofs. 3. The work resolves open questions and improves upon existing results in the field offering valuable contributions to the research community. 4. The algorithms are based on wellestablished frameworks like followtheregularizedleader enhancing the credibility of the approach taken in the study.  Weaknesses: 1. The paper is heavy on technical details and may be challenging for readers unfamiliar with the specific domain of online learning and regret minimization. 2. The complexity of the proposed algorithms and theoretical analysis may limit the practical applicability of the research findings. 3. Certain parts of the paper lack clarity in the explanation of concepts and methodologies which could make it difficult for readers to follow the mathematical derivations.  Questions: 1. Could the proposed algorithms be further evaluated through empirical studies or simulations to assess their performance in realworld scenarios or compared against existing algorithms 2. How do the proposed algorithms scale with increasing graph sizes and complexities Are there any limitations in terms of computational efficiency or scalability 3. Are there any considerations for implementing the algorithms in practical applications and what are the potential challenges in realworld deployment  Limitations: 1. The study is heavily theoretical and lacks empirical validation through experimentation which could limit the practical utility of the proposed algorithms. 2. The complexity of the algorithms and the theoretical analysis may hinder the adoption of the research findings by practitioners in the field. 3. The paper could benefit from clearer explanations and more intuitive examples to aid understanding for readers not deeply versed in the subject matter.", "sBrS3M5lT2w": " Peer Review:  1. Summary: The paper investigates the limitations of existing assumptions in stochastic gradient descent (SGD) theory in machine learning by analyzing three canonical models. It proposes new theory to address these limitations relaxing assumptions related to global Lipschitz continuity and bounded variance. The paper establishes that SGDs iterates will either globally converge to a stationary point or diverge presenting significant implications for the behavior of SGD on a broader range of stochastic optimization problems.  2. Strengths and Weaknesses:  Strengths:  Novelty: The paper addresses an important gap in the current literature by challenging and relaxing existing assumptions in SGD theory.  Methodological Approach: The study proposes new theory and analysis techniques providing a comprehensive examination of SGD behavior under nonconvexity and complex noise models.  Rigorous Analysis: The paper offers detailed proofs and demonstrates the implications of the proposed theory through concrete examples.  Weaknesses:  Complexity: The theoretical framework presented while robust may be challenging for readers less familiar with the intricacies of machine learning theory.  Interpretability: Some assumptions introduced are not easily interpretable and the implications of these assumptions could be explored further to enhance understanding.  3. Questions:  How do the proposed relaxation of assumptions impact the practical application of SGD in realworld machine learning problems  Are there any computational challenges or complexities associated with implementing the proposed theory in practical scenarios  Does the paper explore any potential extensions or future research directions based on the results presented  4. Limitations:  The complexity of the presented theory and assumptions could limit the practical accessibility of the findings for a broader audience.  While the paper addresses the limitations in the existing framework the lack of specific application examples or empirical validation might limit the direct implications of the proposed theory. Overall the paper makes a significant contribution to SGD theory by challenging existing assumptions and providing a more comprehensive understanding of its behavior. Further clarity on the practical implications and potential extensions could enhance the impact of this work in the field of machine learning optimization.", "nN3aVRQsxGd": "Peer Review 1) Summary: The paper presents a theoretical analysis and practical application of Khop message passing Graph Neural Networks (GNNs) and introduces the KPGNN framework to enhance the expressive power of traditional Khop message passing. The theoretical analysis characterizes the expressive power of Khop message passing extending beyond the bounds of 1WL and introducing the concept of distinguishing regular graphs. The KPGNN framework leverages peripheral subgraph information to achieve superior performance in distinguishing distance regular graphs and various graphrelated tasks across simulation and realworld datasets. 2) Strengths:  The paper fills a gap by providing a thorough theoretical characterization of Khop message passing in GNNs demonstrating its expressive power and limitations.  The introduction of the KPGNN framework is innovative and addresses the limitations of traditional Khop message passing by incorporating peripheral subgraph information.  The experimental results showcase the effectiveness of KPGNN across various datasets and tasks demonstrating its competitive performance.  The comprehensive analysis of expressive power comparative evaluations and running time comparisons add credibility to the proposed framework. 3) Weaknesses:  The paper could benefit from clearer delineation and explanation of the theoretical concepts introduced especially for readers not wellversed in graph theory and neural networks.  The connection between the theoretical analysis and practical implications could be more explicitly linked enhancing the papers overall coherence.  The limitations of the proposed KPGNN framework as mentioned could be expounded upon to provide a broader understanding of its potential drawbacks. 4) Questions:  Could the paper elaborate further on the practical implications and applications of the KPGNN framework in realworld scenarios beyond the datasets used in the experiments  How does the expressive power of KPGNN compare to other advanced GNN models particularly those mentioned in the related work section  Are there specific scenarios in which KPGNN outperforms traditional GNN models and if so what are the distinguishing factors  Given the linear relationship between computational overhead and K in the running time comparison are there strategies to optimize this relationship for more efficient computational performance 5) Limitations:  The evaluation of KPGNN seems limited to specific types of graphs and datasets raising questions about its generalizability to diverse graph structures.  The practical scalability of KPGNN to largescale graphs and datasets is an area of concern that needs to be addressed for wider adoption.  The applicability of KPGNN to realworld graphbased tasks beyond the ones evaluated in the paper could be explored to understand its utility in broader contexts. Overall the paper presents a significant contribution to the field of Graph Neural Networks by introducing the KPGNN framework and providing a comprehensive analysis of Khop message passing. Further improvements in clarifying the theoretical concepts addressing the limitations and exploring broader applications would enhance the impact and relevance of the research.", "vsNQkquutZk": " Peer Review  1) Summary Time series forecasting is a crucial task with widespread practical applications. This paper introduces a new regularization method WaveBound to address the issues of unstable training and overfitting in deep learningbased time series forecasting models. WaveBound dynamically estimates error bounds for each time step and feature to stabilize the training process and improve generalization. Extensive experiments demonstrate that WaveBound outperforms existing models including stateoftheart ones on various realworld benchmarks.  2) Strengths and weaknesses Strengths:  Innovative approach: The introduction of WaveBound which dynamically adjusts error bounds offers a unique solution to the overfitting problem in time series forecasting.  Extensive experiments: The paper provides thorough experiments on realworld datasets to validate the effectiveness of WaveBound showcasing consistent improvements over existing models.  Theoretical foundation: The paper includes detailed explanations of the proposed method theoretical analysis and comparisons with existing regularization techniques. Weaknesses:  Clarity: Some parts of the paper may be challenging to follow due to the complexity of the proposed method and technical details.  Comparative analysis: While the experiments demonstrate the superiority of WaveBound a more indepth comparison with a broader range of regularization methods could further strengthen the study.  Generalization: The study focuses on specific types of datasets and models and generalizability to other scenarios may need to be explored further.  3) Questions  Methodology: Could you provide more details on the implementation of WaveBound in comparison to existing regularization methods in terms of computational complexity and training time  Scalability: How does WaveBound perform with increasing dataset sizes and complexities Have scalability aspects been considered in your experiments  Realworld implications: Have you explored the practical implications of WaveBound in operational forecasting scenarios beyond the benchmark datasets  4) Limitations The study primarily focuses on the proposed regularization method WaveBound without delving deeply into the broader context of time series forecasting methodologies. Additionally the paper lacks a discussion on potential limitations or challenges in applying WaveBound to diverse forecasting scenarios. Further exploration into the robustness and adaptability of WaveBound across various domains and model architectures could enhance the studys impact. Overall the paper presents an innovative approach to address overfitting in time series forecasting models. With additional clarity comparative analysis and consideration of broader implications and limitations this work can significantly contribute to the field of time series forecasting and deep learning methodologies.", "tvwkeAIcRP8": "Peer Review Summary: The paper addresses the challenging problem of multiview scene reconstruction using singleview images captured under different point lights. The proposed method leverages neural reflectance field representation to recover 3D geometry and BRDFs of a scene utilizing shading and shadow cues to infer scene geometry. The experiments demonstrate the methods ability to reconstruct complete scene geometry including invisible parts from singleview images showcasing its robustness to depth discontinuities. Strengths: 1. The paper presents a novel approach to scene reconstruction by optimizing a neural reflectance field from singleview images under point lights addressing limitations of existing methods. 2. By jointly recovering geometry and BRDFs using shading and shadow cues the method achieves impressive results in reconstructing complete scene geometry from singleview images as demonstrated in the experiments. 3. The thorough analysis and ablation studies validate the effectiveness of the proposed method showing its capability in handling various scenarios and scene complexities. 4. The manuscript is wellorganized providing detailed explanations figures tables and supplementary materials to support the presented work. Weaknesses: 1. The method relies on known light positions which may pose challenges in realworld applications where lighting calibration is not feasible. Including techniques to handle unknown light positions could enhance the methods practicality. 2. The paper mentions limitations related to complex background geometry affecting shadow appearance and reflectance constraints in invisible regions. Recommendations for addressing these limitations could strengthen the methods adaptability in diverse scenes. 3. While the experiments showcase promising results providing a quantitative comparison with stateoftheart methods on standard datasets would further validate the methods competitiveness and generalizability. Questions: 1. How does the method handle occlusions and discontinuities in scene geometry when reconstructing complete scene models 2. Could the method be extended to incorporate interreflection effects for more accurate scene representation 3. What considerations were made in optimizing the neural field to ensure efficiency and scalability especially in handling largescale scenes Limitations: 1. The method requires manual calibration of light positions which may limit its applicability in practical scenarios. 2. Complex background geometries and lack of reflectance constraints in invisible regions could impact reconstruction accuracy. 3. The method does not account for interreflection effects potentially affecting the realism of rendered scenes. In conclusion the paper presents an innovative method for singleview scene reconstruction with impressive results. Enhancing the methods robustness to unknown light positions and complex scene backgrounds along with addressing interreflection effects could further elevate its practical utility. Conducting additional comparative evaluations and addressing the limitations mentioned would strengthen the methods contributions to the field of scene reconstruction. (Word count: 697)", "z2cG3k8xa3C": " Peer Review  1) Summary: The paper investigates the behavior of the Wasserstein2 distance between discrete measures when smoothed by Gaussian noise known as Gaussiansmoothed optimal transport (GOT). The study establishes sharp bounds on the approximation properties of GOT in the small noise regime revealing a phase transition phenomenon for unique perfect matchings. The results show exponential approximation for small noise and linear scaling for larger noise levels highlighting the impact of optimal transport plan structure on the approximation quality.  2) Strengths and Weaknesses: Strengths:  The paper addresses a relevant and novel topic in the context of optimal transport focusing on the statistical properties of GOT.  Theoretical analysis is rigorous supported by precise bounds and the introduction of strong cyclical monotonicity as a measure of robustness of optimality.  The paper offers clear definitions and theorems providing a comprehensive framework for understanding the behavior of Gaussiansmoothed optimal transport.  The results contribute to the understanding of approximation properties for discrete measures with different structures shedding light on noise sensitivity. Weaknesses:  The technical details and mathematical formalism throughout the paper might be challenging for readers not deeply familiar with optimal transport theory and related concepts.  The paper could benefit from more practical examples or scenarios to illustrate the relevance of the theoretical findings in realworld applications.  The papers contribution in terms of practical implementations or experimental validations is limited focusing primarily on theoretical developments.  3) Questions:  Could you elaborate on how the introduced notion of strong cyclical monotonicity connects to practical applications or potential extensions in other domains beyond theoretical optimal transport  Have you considered exploring the proposed model in experimental setups or realworld data scenarios to validate the theoretical findings and assess the practical implications of Gaussiansmoothed optimal transport  4) Limitations:  The reliance on theoretical analysis and mathematical proofs may limit the accessibility and applicability of the results to practitioners or researchers in fields outside of pure mathematics or theoretical statistics.  The scope of the paper primarily focuses on finite discrete measures potentially overlooking the challenges and complexities that arise with continuous or complex distribution scenarios.  The study does not extensively discuss computational feasibility or algorithmic implications of the proposed Gaussiansmoothed optimal transport framework leaving room for further exploration in this direction. Overall the paper provides valuable insights into the behavior of Gaussiansmoothed optimal transport and its approximation properties showcasing a rigorous theoretical foundation in the field of optimal transport theory. Addressing some practical applications or experimental validations could enhance the papers impact and relevance to a broader audience.", "vsShetzoRG9": " Summary The paper proposes a novel insertionbased text generator called INSNET that addresses the efficiency issues faced by existing insertionbased models during training. INSNET introduces insertionoriented relative positional encoding and aggregation strategies along with an algorithm for adaptive parallelization of insertion operations to enable efficient training and flexible decoding. Experimental results on various datasets demonstrate INSNETs advantages in terms of training speed inference efficiency and generation quality.  Strengths 1. Innovative Approach: INSNET introduces novel techniques such as insertionoriented relative position encoding slot representation aggregation and adaptive parallelization addressing challenges in insertionbased text generation. 2. Efficient Training: By enabling computation sharing and reducing the number of reencoding steps INSNET offers significant training speed advantages over existing models. 3. Flexible Decoding: INSNETs ability to switch between parallel and sequential decoding allows for handling diverse tasks efficiently from machine translation to opendomain text generation. 4. Experimental Validation: The paper includes comprehensive experiments on multiple datasets to showcase the effectiveness and performance of INSNET.  Weaknesses 1. Complexity: The proposed model and techniques might be challenging to implement and understand potentially limiting its practical adoption. 2. Limited Comparison: The paper lacks a detailed comparison with a broader range of stateoftheart models in the field particularly for nonautoregressive machine translation tasks. 3. Scalability: The scalability of INSNET to larger datasets and more complex tasks remains unclear from the provided experiments. 4. Generalization: The paper does not extensively discuss the generalizability of the proposed techniques to different languages or domains.  Questions 1. How does the performance of INSNET compare to other stateoftheart models for more diverse text generation tasks beyond the ones evaluated in the paper 2. Can the adaptive parallelization algorithm in INSNET be finetuned or optimized further to achieve even better tradeoffs between decoding speed and quality 3. Are there any insights on how the proposed techniques in INSNET can be integrated with pretrained language models for improved performance on downstream tasks 4. Is there any exploration on the interpretability of INSNET particularly in understanding how the insertionoriented relative positional encoding influences the generation process  Limitations 1. The paper primarily focuses on the efficiency and flexibility of text generation with INSNET with limited discussions on model interpretability and generalization to diverse tasks or languages. 2. The complexity of the proposed model and techniques may pose challenges for practical implementation and adoption requiring further simplification or optimization. 3. The scope of experiments is limited to specific text generation and machine translation tasks and the scalability of INSNET to more complex or largerscale problems remains to be explored.", "kb33f8J83c": " Peer Review: 1) Summary: This paper introduces a novel method named FreeForm CLIP (FFCLIP) that aims to enable semantic image manipulation based on freeform text prompts through an automatic latent mapping between the visual latent space of StyleGAN and the text embedding space of CLIP. The method consists of semantic modulation blocks that align and inject semantics into the latent code to generate visually realistic images across different types of images such as human portraits cars and churches. Experimental results demonstrate the effectiveness of FFCLIP in producing semantically accurate and visually pleasing images. 2) Strengths and weaknesses: Strengths:  The paper addresses a relevant and challenging problem in image manipulation by enabling effective semantic editing based on freeform text prompts.  The proposed FFCLIP method showcases promising results in producing visually realistic images across different image types.  The semantic modulation blocks for aligning and injecting semantics into the latent code are innovative and contribute to the robustness of the method.  The thorough experimental evaluation on various datasets and the comparison with stateoftheart methods provide strong evidence of the methods effectiveness. Weaknesses:  The paper lacks indepth discussion on the computational complexity of the proposed FFCLIP method especially regarding the training and inference times required.  The limitations section could be enhanced by discussing additional potential challenges or constraints related to the proposed method such as ethical considerations or generalizability.  The paper could benefit from more detailed explanations of the implementation details especially in areas like hyperparameter selection and model architecture specifications. 3) Questions:  Can you elaborate on the training strategy for the FFCLIP method including details like data preprocessing data augmentation techniques and convergence criteria  How does the FFCLIP method handle cases where the text prompt does not precisely match the semantics of the input image and what are the potential implications of such mismatches  Could you provide more insights into the generalizability of the FFCLIP method beyond the datasets mentioned in the experiments section 4) Limitations:  One limitation is related to the potential biases introduced by the CLIP model used for text embeddings which may affect the manipulated results. Future research could explore strategies to mitigate these biases.  Another limitation is the reliance on textual prompts that exhibit high semantic relevance to the image attributes being manipulated which could restrict the versatility of the FFCLIP method under certain conditions. Overall the paper presents a novel approach for textdriven image manipulation with promising results. Addressing the weaknesses and limitations highlighted above can further strengthen the contributions and impact of the proposed FFCLIP method in the field of image synthesis and editing.", "qcRgqCXv1o2": "Peer Review 1) Summary The paper introduces a novel approach to enhancing the robustness of graph classifiers against topological attacks by leveraging the GromovWasserstein discrepancy and resistance distance. The proposed method involves convex optimization techniques to develop certificates of robustness and efficient attack algorithms. Experimental results demonstrate the effectiveness of the approach in graph classification tasks using graph convolutional networks validating the tightness of the certificates and attacks. 2) Strengths and Weaknesses Strengths:  The paper addresses an important issue of enhancing the robustness of graph classifiers against topological attacks.  Novel approach introducing the GromovWasserstein discrepancy and resistance distance for measuring perturbations.  The use of convex optimization to provide robustness certificates and attack algorithms is a significant contribution.  Empirical evaluation on multiple datasets demonstrates the effectiveness of the proposed method. Weaknesses:  The paper may benefit from clarifying the practical implications and realworld applications of the proposed method to improve its readability.  Providing a more detailed discussion on the limitations of the proposed approach and areas for future research would enhance the quality of the paper.  Indepth analysis and comparison with existing methods in terms of computational complexity and scalability would strengthen the paper. 3) Questions  How does the proposed method compare to existing certification techniques for graph robustness in terms of performance and efficiency  Can the method handle larger and more complex graph structures and what are the scalability considerations  Has the proposed approach been tested on a wider range of datasets to validate its generalizability and applicability  Are there specific scenarios where the proposed method may not be suitable and how does it perform in adversarial settings with varied attack strategies 4) Limitations  The paper mainly focuses on the technical aspects of the proposed method with limited discussion on practical implementation challenges and deployment considerations.  The experimental evaluation while comprehensive may benefit from further analysis on the interpretability of the results and the significance of the observed performance improvements.  The generalizability of the approach to diverse graph classifier architectures and learning tasks could be explored further to assess its adaptability in various scenarios. In conclusion the paper provides a novel methodological approach to enhancing the robustness of graph classifiers against topological attacks using convex optimization techniques. Further exploration of the practical implications comparative analyses with existing methods and scalability considerations would enrich the contributions of the work. Overall Rating: The paper presents a valuable contribution to the field of graph classification and robustness with scope for further elaboration on practical implications and scalability considerations.", "zkk_7sV6gm8": " Peer Review:  1) Summary: The paper proposed a reinforcement learning framework named Learnable Impulse Control Reinforcement Algorithm (LICRA) to efficiently learn optimal actions in scenarios where actions incur costs. The framework sequentially decides when to act and what actions to take showcasing superior performance compared to baseline RL methods in various environments. The authors provided theoretical proofs algorithms and empirical results to validate the efficacy of LICRA.  2) Strengths and Weaknesses: Strengths:  The paper addresses a significant issue in RL with a welldefined framework for minimizing costs in action selection.  The theoretical foundation and proofs provided enhance the credibility and rigor of the proposed approach.  The empirical results demonstrate the superiority of LICRA over traditional RL methods in various environments showcasing its practical applicability. Weaknesses:  The paper could benefit from clearer structuring especially in the introduction section to help readers grasp the significance of the problem and the proposed solution.  Additional comparisons with a broader range of RL algorithms could strengthen the analysis of LICRAs performance.  Providing insights into the runtime complexity of the proposed algorithms could enhance the understanding of their scalability.  3) Questions:  How does LICRA perform compared to stateoftheart RL algorithms used specifically for handling costs in action selection scenarios  In what scenarios or environments would LICRA face challenges or limitations  Could the authors discuss the interpretability of the learned policies within the LICRA framework  4) Limitations:  The lack of a detailed complexity analysis may hinder the understanding of the practical feasibility and scalability of the proposed algorithm especially in largescale applications.  While the empirical results are promising a deeper analysis of the generalizability of LICRA across different environments and problem domains could provide better insights into its applicability. Overall the paper presents a novel and promising framework for learning optimal actions in RL scenarios with costincurred actions. Clearer organization and additional insights into scalability and generalizability aspects could further strengthen the findings and impact of the proposed LICRA framework.", "noyKGZYvHH": " Peer Review  1) Summary The paper introduces coVariance neural networks (VNN) that operate on covariance matrices as graphs drawing connections between Principal Component Analysis (PCA) and graph neural networks (GNNs). The study establishes the stability of VNNs to perturbations in the sample covariance matrix demonstrating advantages over PCAbased approaches. Experiments on realworld datasets validate the theoretical results showcasing VNN stability and transferability of performance over datasets with different dimensions.  2) Strengths and Weaknesses Strengths:  The paper presents a novel concept of VNNs operating on sample covariance matrices bridging the gap between PCA and GNNs.  The theoretical analysis establishing the stability of VNNs to perturbations in the sample covariance matrix is thorough and wellsupported.  Empirical validations on realworld and multiresolution datasets provide significant evidence of VNN stability and transferability of performance.  The experimental design and methodology are clearly described and the results are wellpresented. Weaknesses:  The paper lacks a detailed comparison with existing stateoftheart methods beyond PCAbased approaches to showcase the unique contributions of VNNs comprehensively.  The scalability of VNNs in terms of computational complexity for highdimensional datasets especially considering the practical implications could be further discussed.  3) Questions  Can the authors provide more insights into the computational complexity of VNNs particularly addressing the scalability concerns for large datasets  Are there any limitations or potential challenges in the practical implementation of VNNs that the authors have identified during the research  How do the results of the experiments on synthetic data corroborate the findings on realworld datasets and contribute to the understanding of VNN stability and transferability  4) Limitations  While the paper thoroughly discusses the stability of VNNs to perturbations in sample covariance matrices a more indepth analysis of the generalizability of VNNs to diverse realworld applications beyond neuroimaging datasets could enhance the impact of the research.  The study focuses on the theoretical and empirical aspects of VNNs however a more detailed discussion on the interpretability and robustness of VNNs in noisy or outlierprone datasets could further strengthen the conclusions. Overall the paper presents a novel concept of VNNs with a strong theoretical foundation and experimental validation. Further exploration of the practical implications limitations and potential extensions of VNNs could enrich the research findings.", "u8FDFtoMKp2": " Peer Review 1) Summary: The paper introduces a novel knowledge transfer network called Knowledge Transfer Network (KTN) for heterogeneous graph neural networks (HGNNs) to address the issue of label imbalance among different node types in industrial applications. They propose a zeroshot crosstype transfer learning approach within a single HG dataset demonstrating significant performance improvements across various transfer learning tasks in HGs. The technical aspects experiments and results are detailed with clear illustrations and theoretical derivations provided. 2) Strengths:  Novel Contribution: The paper introduces a novel problem definition and addresses a challenging issue in HGNNs demonstrating the effectiveness of KTN for zeroshot transfer learning.  Theoretical Foundation: The theoretical relationship between feature extractors in HMPNNs is wellexplained providing a solid foundation for the proposed KTN method.  Comprehensive Experiments: The experiments are extensive and welldesigned covering multiple datasets and task scenarios showcasing the generality and effectiveness of KTN across various HGNN models.  Clear Presentation: The paper is wellorganized with clear illustrations detailed explanations and thorough evaluations making it easy to follow the proposed methodology and results. Weaknesses:  Complexity: The paper delves into highly technical details which may make it challenging for some readers to grasp the concepts without a strong background in graph neural networks.  Sensitivity Analysis: While the sensitivity analysis provides insightful results the paper could benefit from a more indepth discussion on the implications and practical applications of these findings.  Limitation Statement: The limitation section focuses on the methods applicability to node types sharing the same task. A more thorough discussion on potential extensions and future directions could enhance the papers impact. 3) Questions:  Can the proposed KTN method be extended to facilitate transfer learning between node types with different tasks on a heterogeneous graph  How does the edge heterophily influence the performance of KTN compared to other baselines in the presence of noisy edges  What are the computational complexities and scalability of the KTN model when applied to largescale heterogeneous graphs in realworld settings 4) Limitations:  The paper focuses on a specific problem setting limiting the scope of applicability to scenarios where node types share the same task. Discussing potential extensions to address differenttask scenarios would enhance the practical relevance of the proposed method.  The analysis primarily focuses on synthetic datasets and specific experimental setups. Inclusion of realworld datasets and more diverse experimental scenarios could strengthen the generalizability of the findings. Overall the paper presents a significant contribution to the field of graph neural networks by introducing a novel knowledge transfer approach for heterogeneous graphs. The thorough theoretical derivations extensive experiments and clear presentation make the paper valuable to researchers working in graph learning and transfer learning domains. Addressing the raised questions and limitations would further enhance the clarity and impact of the research.", "lxsL16YeE2w": " Peer Review  1) Summary: The paper introduces UViM a unified approach for computer vision tasks using a twocomponent model: a base model for predicting raw vision outputs and a language model for generating guiding codes. UViM demonstrates competitive results in panoptic segmentation depth prediction and image colorization without taskspecific modifications. The study addresses the limitations of existing models and shows potential for a unified approach in computer vision.  2) Strengths and Weaknesses: Strengths:  UViM offers a promising unified approach for modeling various computer vision tasks.  The use of a twocomponent model allows for efficient handling of highdimensional outputs and structured data.  The study demonstrates competitive results in diverse tasks achieving near stateoftheart performance.  The methodology is welldefined and experimental setups are detailed and reproducible. Weaknesses:  The paper focuses more on the technical aspects of UViM and the experimental results rather than deep analysis of its theoretical underpinnings.  While the concept of UViM is innovative the paper lacks a comprehensive discussion on the limitations of the approach and potential areas for improvement.  The study could benefit from additional comparative analyses with existing models to highlight the unique strengths of UViM.  The discussion on computational efficiency and scalability could be further elaborated to address potential challenges in realworld applications.  3) Questions:  Can you provide more insights into the training procedure for UViM particularly regarding the convergence criteria and robustness to hyperparameter selection  How does UViM adapt to noisy or incomplete input data and are there mechanisms in place to handle such scenarios effectively  Are there plans to extend UViM to more complex vision tasks or integrate it with existing models to enhance its capabilities further  Could the authors elaborate on how UViM addresses interpretability and explainability concerns in computer vision applications  4) Limitations:  The paper could benefit from a more detailed exploration of the limitations of UViM especially in terms of handling diverse datasets and generalizability across different domains.  Additional evaluation on larger datasets and realworld scenarios would provide more insight into the scalability and practical feasibility of UViM.  The study focuses primarily on specific tasks like panoptic segmentation depth prediction and image colorization expanding the scope to cover a wider range of vision tasks would strengthen the generalizability of UViM. Overall the paper presents an innovative approach in the field of computer vision with UViM. However further research and analysis are needed to fully understand the capabilities limitations and realworld applicability of this unified modeling approach.", "p9lC_i9WeFE": " Peer Review 1) Summary: The paper investigates the generalization error of Message Passing Neural Networks (MPNNs) in the context of graph classification and regression tasks. It explores how the complexity of the MPNN affects the generalization gap based on the size and number of nodes in the graphs. Through a detailed analysis the authors derive a generalization bound that decays as the average number of nodes in the graphs increases. The work provides valuable insights into the theoretical properties and generalization capabilities of MPNNs in graphfocused problems. 2) Strengths and weaknesses:  Strengths:  The paper addresses an important topic by studying the generalization properties of MPNNs in graph tasks.  The theoretical analysis provides a comprehensive understanding of the influence of graph characteristics on generalization.  The derived generalization bounds offer insights into the conditions under which MPNNs can effectively generalize from limited data.  The experiments demonstrate the practical implications of the theoretical findings in a synthetic dataset scenario.  Weaknesses:  The paper focuses heavily on theoretical analysis potentially leading to a lack of practical implementation details.  The generalization bounds are based on assumptions about the data distribution which might not always hold in realworld scenarios.  The experiments are limited to synthetic datasets and the applicability of the findings to realworld graph data needs validation. 3) Questions:  How would the findings of this study generalize to realworld graph datasets with diverse characteristics and complexities  Are there any specific scenarios or types of graph tasks for which the derived generalization bounds may not hold true  How sensitive are the results to variations in the assumptions made regarding data distribution and model complexity 4) Limitations:  The theoretical nature of the study might limit the direct applicability of the findings to practical scenarios.  The analysis is primarily based on synthetic datasets potentially lacking realworld validation.  Assumptions about data distribution and model characteristics might not always align with the complexities seen in real data. Conclusion: Overall the paper provides valuable insights into the generalization capabilities of MPNNs in graph tasks. The theoretical analysis combined with experimental validation on synthetic datasets contributes to the understanding of how MPNNs generalize in the presence of varying graph complexities. However further validation on realworld datasets and consideration of more diverse scenarios could enhance the applicability and robustness of the proposed generalization bounds.", "zXE8iFOZKw": " Peer Review:  1) Summary: The paper introduces the DynamicsAware Hybrid OfflineandOnline Reinforcement Learning (H2O) framework to address the challenges of simtoreal gaps in reinforcement learning (RL) policy learning. H2O combines offline realworld datasets with online simulation rollouts by introducing a dynamicsaware policy evaluation scheme. The framework demonstrates superior performance compared to existing crossdomain RL algorithms through simulation and realworld tasks. Extensive experiments and analyses validate the effectiveness of H2O in bridging simtoreal gaps and providing a new hybrid offlineandonline RL paradigm.  2) Strengths and weaknesses: Strengths:  The paper addresses a timely and significant research problem related to simtoreal gaps in RL offering a novel approach to combine offline and online learning.  The proposed H2O framework is wellmotivated and theoretically supported providing adaptive value regularization and dynamics gap handling.  Extensive experiments in both simulation environments and realworld robotic control tasks demonstrate the superior performance of H2O over existing RL approaches.  The ablation study further validates the importance of key design components in the H2O framework. Weaknesses:  The paper could benefit from more detailed discussions on the generalizability and scalability of the H2O framework to a broader range of tasks and environments.  The theoretical analysis could be further expanded to provide deeper insights into the functioning of the dynamicsaware policy evaluation and its implications.  3) Questions:  Can you elaborate on the scalability of the proposed H2O framework to more complex and diverse realworld tasks beyond the specific scenarios considered in the experiments  How sensitive is H2O to variations in the quality and size of realworld offline datasets and simulation environments and how might these factors affect its performance  Have you considered the potential computational overhead or training complexity introduced by the dynamicsaware policy evaluation scheme in the H2O framework especially in largescale applications  4) Limitations:  The paper focuses on specific tasks and scenarios in simulation and realworld robotics which may limit the generalizability of the proposed H2O framework to a wider range of RL applications.  While the experiments showcase promising results more comprehensive comparisons with additional stateoftheart RL approaches could provide a broader perspective on the performance of H2O.  Overall Assessment: The paper presents a wellstructured and innovative approach to address simtoreal gaps in RL through the H2O framework demonstrating superior performance over existing methods in both simulation and realworld tasks. The theoretical foundation empirical evaluations and ablation study provide strong support for the efficacy of H2O. Further discussions on generalizability scalability and computational considerations could enhance the papers impact and relevance to the broader RL research community.  End of Peer Review.", "sPNtVVUq7wi": " Summary The paper introduces CONDOT a novel multitask approach that uses conditional optimal transport to estimate optimal transport maps conditioned on context variables. It leverages partially input convex neural networks (PICNN) and an efficient initialization strategy inspired by Gaussian approximations. CONDOT demonstrates promising results in inferring the effects of genetic or therapeutic perturbations on single cells even when faced with unseen contexts.  Strengths  Innovative Approach: Introduces a novel multitask framework for contextaware optimal transport estimation.  Theoretical Foundation: Builds on wellestablished optimal transport theory and neural network architectures.  Generalizability: Demonstrates the ability to generalize to unseen contexts and predict outcomes for new perturbations.  Experimental Validation: Provides detailed experiments showcasing the effectiveness of CONDOT in various scenarios.  Indepth Discussion: Thorough analysis of methodologies initialization strategies and results.  Weaknesses  Complexity: The methodology and the mathematical formulations can be challenging to grasp for readers less familiar with optimal transport theory.  Limited Comparison: Comparative analysis with more existing methods in the field could provide a broader perspective on the strengths and limitations of CONDOT.  Technical Details: The paper contains a wealth of technical details that may require significant effort for a nonexpert to understand fully.  Questions 1. How does CONDOT compare to stateoftheart approaches in terms of computational efficiency and scalability 2. What are the implications of the closedform solutions for Gaussian approximations on the practical applicability of CONDOT in realworld scenarios 3. How sensitive is CONDOT to variations in the training dataset especially with regard to the context variables and their distributions  Limitations  Complexity of Implementation: Implementing CONDOT may require sophisticated neural network expertise and computational resources.  Dataset Dependence: The performance of CONDOT heavily relies on the quality and diversity of the training dataset raising concerns about generalizability to realworld datasets.  Interpretability: The blackbox nature of neural networks used in CONDOT might hinder interpretability particularly in critical applications like healthcare. In conclusion the paper presents a compelling framework in the field of conditional optimal transport offering exciting potential for modeling complex contextual dependencies in various applications. However further validation and exploration of its practical implications and limitations are necessary for a comprehensive evaluation.", "sc7bBHAmcN": " Summary The paper explores Subgraph Graph Neural Networks (GNNs) and their potential for expressive power and performance. The authors propose a novel symmetry analysis to understand the symmetry group of nodebased subgraph collections. They introduce a new class of Subgraph GNNs called SUN which unifies existing architectures and demonstrates improved empirical performance on benchmarks. The study aims to deepen the understanding of Subgraph GNNs and their theoretical properties addressing questions on their expressive power and equivariant message passing layers.  Strengths 1. The paper provides a comprehensive analysis of Subgraph GNNs exploring symmetry groups and proposing a new architecture called SUN. 2. The authors introduce a novel symmetry analysis that offers valuable insights into the design space of Subgraph GNNs and their expressive power. 3. Evaluation on synthetic and realworld benchmarks demonstrates improved performance of the proposed SUN model over existing Subgraph GNN architectures. 4. The research fills gaps in understanding the theoretical properties of Subgraph GNNs and presents a unified framework for designing expressive and efficient architectures.  Weaknesses 1. The paper is dense with technical details which may make it challenging for readers unfamiliar with Graph Neural Networks to follow. 2. While the novel architecture SUN shows promising results a more detailed comparison with other stateoftheart GNN models could provide a deeper understanding of its superiority. 3. The paper could benefit from more detailed guidelines on the implementation and practical application of the proposed architecture for researchers and practitioners.  Questions 1. How does the proposed SUN architecture compare to other stateoftheart Graph Neural Networks in terms of performance and computational efficiency 2. Could the authors provide more insights into the limitations of the proposed methodology and potential directions for further research to address these limitations 3. How scalable is the SUN architecture for largescale graph datasets and what considerations were taken in designing it for realworld applications  Limitations 1. The paper focuses primarily on theoretical and experimental analysis lacking an indepth discussion on the practical implementation and scalability aspects of the proposed architecture. 2. The generalization ability of the SUN model while promising in lowdata regimes could benefit from further exploration on diverse datasets and domainspecific tasks to establish its robustness. Overall the paper represents a significant contribution to the field of Graph Neural Networks by advancing the understanding and design of Subgraph GNNs. Further investigations and practical applications of the SUN architecture could enhance its impact on diverse realworld applications.", "wzJcEb5Mm4": "Peer Review: Summary: The paper introduces a novel logpolar space convolution (LPSC) layer in convolutional neural networks where the convolution kernel is elliptical and adaptively segments its local receptive field into regions based on relative directions and logarithmic distances. LPSC exponentially grows the local receptive field without increasing parameters improving the encoding of local spatial information. Experimental results showcase the effectiveness of LPSC in various network architectures across different tasks and datasets. Strengths: 1. Innovative Approach: The introduction of LPSC with elliptical kernels and parameter sharing based on logpolar space is a novel and unique contribution. 2. Performance Demonstrations: Extensive experiments demonstrate the effectiveness of LPSC across multiple tasks and datasets showcasing superior performance compared to conventional convolution and dilated convolution. 3. Structured Presentation: The paper is wellstructured providing a clear theoretical foundation detailed explanation of LPSC implementation and thorough experimental validations. Weaknesses: 1. Limited Generalization: While LPSC shows promising results in imagerelated tasks the paper lacks exploration of its applicability to nonimage datasets or tasks with irregular data structures. 2. Complexity: The LPSC introduces additional hyperparameters potentially complicating model tuning and implementation. 3. Implementation Challenge: The paper acknowledges challenges in implementing LPSC via logpolar space pooling indicating potential efficiency issues in practical applications. Questions: 1. How does LPSC higherlevel receptive field (LRF) growth impact the interpretability of its convolutional outputs compared to traditional methods 2. Are there plans to explore LPSC integration in tasks beyond computer vision such as natural language processing or audio analysis 3. Could the LPSC method be adapted to handle largerscale datasets or realtime processing scenarios efficiently Limitations: 1. Parameter Sharing Tradeoff: While LPSC reduces the number of parameters by sharing weights it may lead to some loss of finegrained details in the data representations. 2. Computational Efficiency: LPSCs implementation through logpolar space pooling may result in increased memory usage and computational complexity potentially limiting its scalability. 3. Application Scope: LPSC may be more suitable for sparsely structured visual data potentially limiting its utility in densely populated data domains or tasks with varied data distributions. Overall the introduction of LPSC provides a valuable contribution to the field of convolutional neural networks with promising results in improving receptive field sizes while maintaining parameter efficiency. Further investigation into its generalizability interpretability and optimization for broader applications would enhance the impact and practicality of LPSC.", "mowt1WNhTC7": " Peer Review Summary: This paper focuses on a thorough manual review of the remaining mistakes made by top ImageNet models particularly in the multilabel evaluation subset. The study reveals that many mistakes considered errors by models are actually valid emphasizing the importance of expert review to accurately assess model performance. The authors propose a subset of 68 \"major errors\" called ImageNetM for future benchmarking providing valuable insights into model evaluation. The methodology is rigorous involving expert panels and detailed categorization of mistakes by severity and type. Strengths: 1. Rigorous methodology involving expert panels for manual review to assess remaining model mistakes. 2. Insightful analysis on the nature and severity of mistakes made by top ImageNet models. 3. Proposal of ImageNetM subset as a benchmark focusing on unambiguous major errors offering a new perspective on model evaluation. 4. Detailed categorization of errors into finegrained outofvocabulary spurious correlations and nonprototypical labels providing a comprehensive understanding of types of mistakes. 5. Indepth comparison of model performance with human annotators offering valuable insights into model capabilities. Weaknesses: 1. The study heavily relies on manual expert review making it resourceintensive and potentially subject to bias. 2. The paper lacks a detailed discussion on potential biases introduced by the expert panels worldview and the inherent subjectivity in determining major errors. 3. Limited discussion on how the proposed ImageNetM subset can be used in realworld applications beyond benchmarking models. 4. While the methodology is robust the paper could benefit from a more detailed discussion on potential ethical implications of manual mistake review in datasets. Questions: 1. How do the findings of the manual review in this paper add to the existing body of literature on ImageNet dataset evaluation and model benchmarking 2. What are the implications of the studys results on the future development of computer vision models and benchmarking standards 3. How would the proposed ImageNetM subset impact the development and evaluation of future computer vision models beyond the context of ImageNet Limitations: 1. The study heavily relies on manual expert review potentially introducing biases and making scalability a concern. 2. The paper lacks detailed discussions of the broader implications of the study beyond model benchmarking on ImageNet. 3. There could be limitations in the generalizability of the studys findings to other datasets and tasks. The paper provides valuable insights into the evaluation of top ImageNet models offering a detailed analysis of remaining mistakes and proposing a novel evaluation subset. However further exploration of the limitations biases and realworld implications of the study would enhance its impact and applicability in the field of computer vision research. Overall the paper contributes significantly to the ongoing discussion on model evaluation in computer vision highlighting the importance of expert review and the need for nuanced approaches in benchmarking and assessing model performance.", "krV1UM7Uw1": " Peer Review  1. Summary The paper proposes two novel robust regression algorithms TRIP and BRHT that integrate prior information to enhance robustness against adaptive adversarial attacks (AAAs). TRIP uses a simple prior while BRHT incorporates Bayesian reweighting. The algorithms are supported by theoretical convergence proofs and extensive experiments showing their superiority over benchmark methods in terms of robustness and efficiency.  2. Strengths and Weaknesses Strengths:  The paper addresses the challenging issue of robust regression under adaptive adversarial attacks providing novel algorithms with theoretical guarantees.  The integration of prior information significantly improves the recovery of regression coefficients.  Rigorous theoretical analyses and extensive experiments demonstrate the effectiveness of the proposed algorithms.  Comparison with existing methods highlights the superior performance of TRIP and BRHT in various attack scenarios. Weaknesses:  The presentation could be improved by providing clearer transitions between sections and subtopics to enhance readability.  The complexity of the algorithms and theoretical analyses might hinder the understanding for readers not wellversed in the field.  While the experiments showcase superior performance providing more detailed discussions on specific cases or scenarios could enhance the practical insight gained from the results.  3. Questions  Can you elaborate on the specific criteria considered for selecting the parameters of the prior distributions in TRIP and BRHT  How do the algorithms TRIP and BRHT compare in terms of computational efficiency especially when scaling to larger datasets  Have you considered applying the proposed algorithms to realworld datasets to evaluate their performance in practical applications  4. Limitations  The paper focuses on synthetic datasets it would be beneficial to explore the applicability of the algorithms to realworld datasets with varying characteristics and complexities.  The experiments primarily consider the recovery of coefficients additional analyses on other performance metrics such as convergence speed and sensitivity to different types of corruption could provide further insights. Overall the paper presents significant advancements in robust regression methodologies demonstrating promising results in enhancing resistance to adversarial attacks through the integration of prior information. Further exploration into practical applications and realworld datasets could strengthen the impact of the proposed algorithms in the field of robust regression.", "tWBMPooTayE": " Peer Review of the Paper \"FreGAN: Enhancing GANs Training under Limited Data by Raising Frequency Awareness\"  1) Summary: The paper proposes FreGAN a novel approach aimed at improving Generative Adversarial Networks (GANs) training under limited data conditions by enhancing frequency awareness. It addresses the issue of discriminator overfitting and poorquality generation by emphasizing highfrequency signal synthesis. FreGAN decomposes images into different frequency components using Haar wavelet transformation employs a highfrequency discriminator (HFD) and frequency skip connection (FSC) and introduces highfrequency alignment (HFA) to promote GAN equilibrium. Extensive experiments demonstrate the effectiveness of FreGAN in enhancing generation quality especially with less than 100 training data instances.  2) Strengths and weaknesses:  Strengths:  Novel Approach: The paper introduces an innovative method FreGAN to address the challenges of GAN training with limited data by emphasizing frequency awareness.  Methodology Clarity: The proposed approach is welldescribed with clear explanations of the components such as HFD HFA and FSC making it easy to understand.  Experimental Results: Extensive experiments are conducted across various datasets with limited data showcasing the superiority of FreGAN in improving generation quality especially under lowdata regimes.  Compatibility and GAN Equilibrium: The paper demonstrates compatibility with existing methods and effectively addresses the issue of GAN equilibrium through frequency alignment.  Weaknesses:  Unbalanced Dataset Content: The paper acknowledges limitations in generating photorealistic images with datasets having limited data but diverse contents indicating potential challenges in generalization.  Computational Cost: The impact of additional computational costs from utilizing higher scales of features could have been further discussed.  Lack of Abstraction: The paper focuses heavily on the technical details of the proposed method potentially making it challenging for readers less familiar with frequency signals and GAN structures to grasp the core concepts.  3) Questions:  How does FreGAN address the challenge of overfitting in the discriminator without compromising the generators training performance  Could the paper provide insight into potential applications of FreGAN beyond image generation tasks  What are the implications of the proposed approach for realworld applications with limited data scenarios such as medical imaging or satellite imagery analysis  4) Limitations: The limitations of the paper include constraints in generalizing FreGAN to datasets with wide content variations potential computational overhead with higher scales of features and a focus on technical details that may hinder accessibility for a broader audience.  5) Overall Assessment: The paper presents a valuable contribution to the GAN training under limited data regime by introducing FreGAN emphasizing frequency awareness to enhance generation quality. The experiments showcase the efficacy of the proposed method and the compatibility with existing techniques adds to its strength. Addressing the noted limitations and providing further insights could strengthen the papers impact and applicability in various domains. Overall the paper offers a significant advancement in GAN training with limited data showcasing the potential benefits of frequencyaware approaches in enhancing generation quality. [Note: This peer review provides constructive feedback to improve the papers clarity impact and applicability.]", "xubxAVbOsw": " Peer Review  1) Summary: The paper proposes a novel method named DiversityPromoting Collaborative Metric Learning (DPCML) to address the challenge of accommodating diverse user preferences in recommendation systems. The key innovation lies in introducing multiple user representations and a Diversity Control Regularization Scheme (DCRS) to enhance model performance. The theoretical analysis shows the generalization bounds of DPCML and experimental results demonstrate its superiority over existing methods through various metrics on benchmark datasets.  2) Strengths:  Innovation: The introduction of multiple user representations and the DCRS is a novel approach to handle diverse user preferences effectively.  Theoretical Analysis: The paper provides a detailed theoretical analysis of the generalization ability of DPCML which strengthens the theoretical support for the proposed method.  Empirical Evaluation: Extensive experiments on benchmark datasets demonstrate the superiority of DPCML over existing methods showcasing its effectiveness through various metrics.  Visualization: The paper includes clear visualizations to illustrate motivation preference diversity and experimental results enhancing the understanding of the proposed method.  3) Weaknesses:  Theoretical Complexity: The theoretical analysis including covering numbers and generalization bounds might be challenging for readers not wellversed in learning theory. Simplifying these sections or providing more intuitive explanations could improve clarity.  Applicability: The focus on implicit feedback might limit the applicability of DPCML in scenarios with explicit feedback. Discussing potential ways to extend the method to explicit feedback could enhance the practical relevance of the work.  4) Questions:  Scalability: How does the proposed DPCML method scale with increasing dataset sizes particularly in scenarios with millions of usersitems  Computational Efficiency: What are the computational costs associated with training and inference for DPCML especially concerning the introduction of multiple user representations  Robustness: Have there been tests conducted to evaluate the robustness of DPCML to noise in the data or potential biases in user preferences  Comparison: Could the paper explore comparisons of DPCML with stateoftheart methods beyond the ones mentioned in the results section How does it perform against more recent advancements in recommendation systems  5) Limitations:  Practical Applicability: The focus on implicit feedback limits the immediate applicability of DPCML in scenarios with explicit feedback which constitutes a significant limitation.  Complexity: The theoretical complexity and technical nature of the analysis may pose challenges for readers with a limited background in learning theory or deep learning. Overall the paper presents an innovative approach to address the challenge of diverse user preferences in recommendation systems. Addressing theoretical complexity and expanding the evaluation to different scenarios could further enhance the impact and relevance of the proposed method.", "zYc5FSxL6ar": "Summary: The paper introduces a SynergyOriented LeARning (SOLAR) framework that integrates the concept of muscle synergies into modular reinforcement learning to simplify the control of multijoint robots. SOLAR utilizes an unsupervised learning method to discover synergy structures based on functional similarities and morphological context of actuators. It then employs a synergyaware policy architecture to generate synergy actions reducing the learning complexity by operating at a lowrank control level. Evaluation results demonstrate superior efficiency and generalizability of SOLAR especially on robots with a large Degree of Freedom (DoF) like Humanoid and UNIMALs. Strengths: 1. The paper addresses a significant challenge in modular reinforcement learning by proposing a novel approach that leverages muscle synergies to simplify control policies for multijoint robots. 2. The experimental evaluation is comprehensive demonstrating the effectiveness of SOLAR in various tasks and settings and comparing it with stateoftheart methods. 3. The paper provides detailed explanations of the unsupervised learning method for synergy discovery and the architecture for synergyaware policy learning enhancing the clarity of the proposed framework. Weaknesses: 1. The paper could benefit from further discussing the computational complexity and scalability of the SOLAR framework particularly as the number of synergies and actuators increases. 2. While the results are promising a more indepth analysis and discussion on the limitations of the proposed approach such as failure cases or scenarios where SOLAR might not perform as well would enhance the papers robustness. Questions: 1. How does the affinity propagation algorithm handle the potential noise or inaccuracies in the value information used for synergy discovery 2. Can the synergy structure discovered by SOLAR be easily interpreted and modified by human operators to finetune control policies for specific robot morphologies 3. How does the computational efficiency of SOLAR compare to existing modular RL methods particularly when scaling up to more complex robots with higher DoF Limitations: 1. The paper mainly focuses on showcasing the effectiveness of the SOLAR framework through experimental results but it lacks a detailed discussion on potential drawbacks or challenges that may arise when applying this approach in realworld scenarios. 2. The generalizability and robustness of SOLAR across a wider range of robot morphologies and tasks could be further evaluated to provide a more comprehensive understanding of its capabilities in diverse environments.", "w6fj2r62r_H": " Peer Review  1) Summary: The paper introduces torsional diffusion a novel framework for molecular conformer generation by leveraging a diffusion process specifically on torsion angles. The proposed method outperforms existing machine learning and cheminformatics approaches in terms of conformer ensembles quality and speed providing exact likelihoods and enabling the development of the first generalizable Boltzmann generator.  2) Strengths and Weaknesses: Strengths:  The paper introduces a novel and effective torsional diffusion framework for molecular conformer generation addressing the challenge of balancing accuracy and speed.  The model outperforms stateoftheart methods in terms of conformer ensemble quality and speed with significant runtime improvements.  The incorporation of exact likelihoods and the development of a generalizable Boltzmann generator are significant contributions.  The experimental evaluation is comprehensive utilizing relevant datasets and metrics to showcase the superiority of the proposed method. Weaknesses:  The paper could benefit from a clearer explanation of the method in certain sections for better understanding by readers unfamiliar with the specific domain terminologies and concepts.  Some parts of the manuscript could be more concise focusing on essential details to enhance readability and accessibility.  The limitations and potential areas for improvement of the proposed method could be explored further for a more comprehensive discussion.  3) Questions:  Could you elaborate on how the proposed torsional diffusion framework handles molecules with varying degrees of flexibility and complexity especially those with a higher number of rotatable bonds  How does the proposed model address any potential challenges related to the generalizability of the Boltzmann generator across different molecular systems or classes  In practical applications how would the proposed torsional diffusion method scale when dealing with larger datasets or complex molecular structures  4) Limitations:  The absence of a detailed discussion on potential limitations or challenges of the torsional diffusion framework in handling certain types of molecular conformers or dataset characteristics limits the scope of the paper.  The focus on druglike molecules in the evaluation datasets may constrain the generalizability of the proposed method to other types of molecular systems or applications.  The potential impact of noise or variations in input data on the performance of the torsional diffusion model is an aspect that could be further explored in terms of robustness and stability. Overall the paper presents a novel and promising approach to molecular conformer generation showcasing significant advancements over existing methodologies. Further exploration and validation of the methods robustness scalability and applicability across diverse molecular systems would enhance the papers impact and relevance in the field of computational chemistry and machine learning.", "yQDC5ZcqX6l": "Peer Review: Summary: The paper presents a novel approach for biclustering bipartite graphs using optimal transport (OT). Instead of deriving biclustering as a consequence the proposed approach integrates biclustering from the beginning. The authors introduce two techniques: one for hard biclustering and another for fuzzy biclustering. The methodology is thorough and includes an indepth formulation of the OT problem and its application to biclustering. Extensive experiments on documentterm matrices and other datasets are conducted to validate the proposed approach. The results indicate that the proposed method outperforms existing OT biclustering algorithms in terms of efficiency and effectiveness. Strengths: 1. Novelty: The paper introduces a novel framework for biclustering that incorporates OT from the beginning. 2. Thorough Methodology: The paper provides a detailed description of the OT formulation the application to biclustering and the proposed algorithms for hard and fuzzy biclustering. 3. Experimental Validation: Extensive experiments on diverse datasets validate the efficacy of the proposed approach and demonstrate its superiority over existing OT biclustering methods. 4. Efficiency: The proposed approach is shown to be more computationally and memory efficient compared to previous OT biclustering algorithms. Weaknesses: 1. Limited Comparison: The comparison of the proposed approach with existing methods seems focused primarily on OT biclustering algorithms. A broader comparison with traditional biclustering algorithms could enhance the evaluation. 2. Theoretical Depth: Although the paper is methodologically comprehensive some sections especially related to mathematical formulations may require additional clarification for readers less familiar with optimal transport and biclustering concepts. Questions: 1. What criteria were used to select hyperparameters particularly in the context of regularization parameters lambda for entropic regularization 2. How generalizable is the proposed framework to different types of dyadic datasets beyond the ones discussed in the paper 3. Can the proposed biclustering approach handle noisy or incomplete data and is there any discussion on robustness to such scenarios Limitations: 1. Applicability: The paper focuses on dyadic data limiting the generalizability of the proposed approach to other types of datasets. 2. Complexity: Although the proposed method is more efficient than existing OT biclustering algorithms it may still face challenges with very large datasets or datasets with high sparsity. 3. Scalability: While the approach is shown to be scalable the scalability to extremely large datasets or realtime applications may need further investigation. In conclusion this paper presents an innovative biclustering approach using optimal transport that addresses some of the limitations of existing methods. The indepth methodology and extensive experiments provide strong support for the approachs efficacy. Addressing the questions and limitations mentioned above could enhance the clarity and robustness of the proposed framework.", "oQIJsMlyaW_": " Summary The paper introduces a novel integrated gradient pruning criterion called SInGE which combines ideas from neural network pruning and attribution methods. The authors address limitations of existing pruning heuristics proposing a method that determines the importance of neurons based on integral of gradient variations. SInGE is applied in a finetuning flowchart to preserve DNN accuracy while removing parameters. Extensive validation on various datasets and architectures showed that SInGE outperforms existing stateoftheart DNN pruning methods.  Strengths and Weaknesses Strengths:  The paper tackles an important issue of reducing computational cost in deep neural networks through efficient pruning methods.  The proposed SInGE criterion demonstrates superior performance in maintaining DNN accuracy while achieving significant model compression.  The experimental validation on multiple datasets and architectures provides comprehensive evidence of the effectiveness of the proposed method.  The integration of attribution methods into pruning introduces a novel perspective and contributes to the explainability of predictive models. Weaknesses:  The paper lacks a detailed discussion on the computational complexity and scalability of the proposed method especially in largescale applications.  The paper could benefit from more detailed comparisons with a wider range of existing pruning methods discussing potential tradeoffs and scenarios where different techniques may be more suitable.  The detailed methodology and algorithm descriptions are comprehensive but may be challenging for readers unfamiliar with the field of neural network pruning.  Questions 1. How does the proposed SInGE criterion compare with other stateoftheart neural network pruning methods in terms of computational efficiency and scalability 2. Can the authors provide insights into the interpretability of the SInGE criterion and its implications for model explainability 3. How does the finetuning process impact the overall training time and convergence of the pruned models compared to traditional pruning methods 4. Are there any specific architectural constraints or limitations where the SInGE method may not perform optimally  Limitations  The study mainly focuses on the superior performance of the proposed method in terms of accuracy versus pruning ratio. Further analysis on other metrics such as inference time and energy efficiency would enhance the comprehensiveness of the evaluation.  The generalizability of the SInGE method across different types of neural networks and application domains could be explored further to validate its versatility and robustness.  The proposed SInGE method may require extensive finetuning steps which could potentially impact the overall efficiency of the pruning process in realworld applications with limited computational resources.", "uCBx_6Hc7cu": " Peer Review: 1. Summary: The paper proposes a variational inference formulation of autoassociative memories that combines perceptual inference and memory retrieval in a unified framework. The methodology involves memorydependent prior probability distribution comparing neural network approaches such as Variational Auto Encoders and Predictive Coding. The study evaluates the proposed algorithms on CIFAR10 and CLEVR image datasets comparing them with other associative memory models. The contributions include designing AM models based on variational inference and drawing a connection between PC and modern continuous Hopfield networks. 2. Strengths:  The paper provides a detailed exploration and comparison of different neural network approaches for memory retrieval highlighting the strengths and limitations of each.  The proposed framework for autoassociative memories based on variational inference is novel and welldescribed offering a fresh perspective in the field of associative memories.  The experiments conducted on CIFAR10 and CLEVR datasets are comprehensive allowing for a thorough evaluation of the proposed models against benchmark approaches.  The inclusion of theoretical derivations and mathematical formulations enhances the credibility and depth of the study. 3. Weaknesses:  While the study presents a rich set of experiments a more detailed comparison and analysis of the results would strengthen the papers impact. Providing insights into specific scenarios where each model excels or underperforms could enhance the clarity for readers.  The paper could benefit from a more extensive discussion on the implications of the study\u2019s findings potential realworld applications and avenues for future research. 4. Questions:  How do the proposed models scale in terms of memory capacity and computational complexity as the size of stored patterns increases  Could the authors elaborate on the experimental setup particularly regarding hyperparameters chosen and how they impact model performance  How do the proposed models generalize to other types of datasets or memory retrieval tasks beyond the ones explored in the study 5. Limitations:  The study focuses primarily on contentbased addressing and may overlook the potential benefits of more complex addressing mechanisms available in other associative memory models like NTMs.  The lack of indepth analysis on memory capacity and convergence properties could limit the understanding of the proposed models practical utility and scalability.  The limited investigation into the role of the representation structure of stored patterns in memory retrieval may overlook important aspects that could influence model performance. Overall the paper provides a valuable contribution to the field of autoassociative memories and variational inference. Addressing the weaknesses and questions raised could further enhance the clarity impact and applicability of the study.", "peFP9Pl-6-_": " Peer Review:  1) Summary: The paper proposes a new variational framework OGradient for sampling on a manifold with equality constraints without requiring prior knowledge of the manifold. The method is applied to Langevin dynamics and SVGD and demonstrated across various constrained ML problems including fairness and logic constraints. Theoretical convergence analysis and empirical experiments show the effectiveness of the proposed approach.  2) Strengths and Weaknesses: Strengths:  Novel framework: Introduces a new variational framework OGradient for constrained sampling problems on implicitly defined manifolds.  Theoretical analysis: Provides convergence proofs and introduces orthogonalspace Stein equations adding clarity to the methods operation.  Empirical results: Demonstrates the effectiveness of OGradient on various constrained ML tasks showcasing its superiority over previous methods.  Applicability and versatility: Successfully applied to Bayesian deep neural networks fairness constraints logic rules and synthetic distributions. Weaknesses:  Limited comparison: While the method outperforms existing approaches a more comprehensive comparison with a wider range of conventional and stateoftheart methods could further strengthen the paper.  Simplifying assumptions: Dependency on certain regularity assumptions for theoretical results may limit the methods applicability in more complex scenarios.  Practical implementations: The method may face challenges in realworld largescale applications due to computational requirements or stringent convergence conditions not explicitly addressed.  3) Questions: 1. How robust is the proposed OGradient method to noisy or uncertain constraints in realworld scenarios 2. Can the approach be extended to handle a combination of equality and inequality constraints simultaneously in a unified framework 3. How well does the method scale with higherdimensional data and complex manifold structures in comparison to existing methods 4. Are there specific criteria or guidelines provided for selecting appropriate hyperparameters in practical applications of the OGradient method  4) Limitations:  Sampling Precision: The paper notes the samples might not be exactly on the manifold potentially affecting the accuracy of the constrained solutions.  Constraint Type: The focus on equality constraints limits the applicability to scenarios requiring inequality or mixed constraints.  Scalability: The limitations in scalability for largescale or highdimensional datasets could impact realworld usability. Overall the paper presents a novel and promising method for constrained sampling supported by rigorous theoretical analysis and practical demonstrations. Addressing the identified limitations and further expanding the methods applicability would enhance its significance in the field of machine learning and constrained optimization.", "mMuVRbsvPyw": " Summary: The paper introduces GMMSeg a novel family of segmentation models that leverage a dense generative classifier for joint distribution modeling. By combining generative and discriminative approaches GMMSeg outperforms discriminative counterparts on closedset datasets and also performs well on openworld datasets without modification. The proposed method addresses shortcomings of existing discriminative solutions by explicitly modeling data distribution. Experimental results show improvements in mIoU and anomaly segmentation tasks across multiple datasets and architectures.  Strengths: 1. Innovative Approach: GMMSeg introduces a novel method by combining the generative classifier with segmentation models providing advantages of both generative and discriminative classifiers. 2. Performance Improvement: The experimental results show consistent performance improvements over discriminative counterparts on both closedset and openworld datasets. 3. Comprehensive Evaluation: The paper covers a thorough evaluation of GMMSeg on various segmentation architectures backbones and datasets demonstrating its robustness and effectiveness.  Weaknesses: 1. Lack of Comparison to More Baselines: While the paper compares GMMSeg with a few discriminative counterparts and other generative algorithms the comparison could be strengthened by including more relevant baselines. 2. Theoretical Depth: The paper lacks indepth theoretical discussions on certain aspects such as the impact of architecture changes and computational complexity. 3. Detailed Analysis of Failure Cases: Providing indepth analysis on failure cases or discussing scenarios where GMMSeg may not perform optimally could enhance the discussion.  Questions: 1. In the hybrid training strategy how is the balance maintained between the generative and discriminative components during training 2. What are the practical implications of incorporating EM with stochastic gradient descent in the online version of EM 3. How does GMMSeg handle situations where classes have significant overlap or when there are inherent ambiguities in data distribution  Limitations: 1. Generalization to More Datasets: While the paper demonstrates effectiveness on several datasets further validation on a broader range of datasets could strengthen the generalizability of GMMSeg. 2. Scalability: The scalability of GMMSeg to largescale datasets and realworld applications could be a limitation that needs consideration. 3. Computational Efficiency: The computational efficiency of GMMSeg especially during inference time in realworld applications could be a concern. Overall the paper presents an innovative and promising approach to semantic segmentation showing significant performance improvements and robustness. Further enhancements in theoretical discussions comparisons and addressing limitations could improve the impact of GMMSeg in the field of computer vision and deep learning.", "px87A_nzK-T": "Peer Review Summary: The paper introduces kernel memory networks as optimal models for training neural networks to store patterns with maximal noise robustness. By applying the kernelbased approach to both feedforward and recurrent networks the authors derive these optimal models termed as kernel memory networks. The study shows how these models encompass traditional hetero and autoassociative memory models and provide a way to interpret memory capacity in neural networks with biological analogies. The paper explores various applications of these kernel memory networks such as in designing memory networks for continuousvalued patterns and demonstrating storage capacity scaling. Additionally the study discusses the biological interpretations and implications of these models. Strengths and Weaknesses: Strengths:  The paper addresses a significant problem in neural network training by providing a novel approach for optimal models.  The derivation of normative kernelbased models and their application to various memory network types is a strong contribution to the field.  The exploration of biological interpretations and connections enhances the papers value in the context of neural models.  The study provides indepth explanations properties and mathematical formulations to support the proposed concepts. Weaknesses:  The paper could benefit from clearer explanations in certain sections particularly for readers not deeply familiar with the mentioned theoretical frameworks.  The transition between theoretical concepts and practical applications could be improved to enhance the readability and understanding for a broader audience.  The paper might have room for more detailed discussions on potential practical implementations or experimental validations of the proposed models. Questions: 1. Could further experimental validation be conducted to assess the realworld performance and generalizability of these kernel memory networks in comparison to traditional memory models 2. How do the proposed models handle scenarios with noisy inputs or overlapping patterns and are there strategies to improve robustness in such cases 3. Considering the application of these models in machine learning how adaptable are the kernel memory networks to various types of data or learning tasks beyond the discussed biological interpretations Limitations:  The application of the proposed models to realworld datasets or scenarios is not explicitly demonstrated limiting the practical validity of the findings.  The complexity and mathematical nature of the paper may pose challenges for readers less familiar with advanced neural network concepts.  The study lacks empirical results or case studies to illustrate the effectiveness and advantages of kernel memory networks over existing memory models. In conclusion the paper introduces an innovative concept of kernel memory networks for noiserobust pattern storage in neural networks. While the theoretical developments are commendable further exploration through practical implementations and comparative analyses could enhance the papers impact and relevance in the field of neural networks and memory modeling. End of Review.", "szt95rn-ql": " Summary The paper introduces a new model Bursting CorticoCortical Networks (BurstCCN) that addresses the credit assignment problem in artificial neural networks by integrating biological features of cortical networks. BurstCCN effectively backpropagates error signals through multiple layers using a singlephase learning process approximates backpropderived gradients and achieves good performance on complex image classification tasks like MNIST and CIFAR10. The model leverages bursting activity shortterm plasticity (STP) and dendritetargeting interneurons for backproplike synaptic updates.  Strengths 1. Innovative Model: BurstCCN proposes a novel approach by integrating cortical network properties to address the credit assignment problem effectively. 2. Biological Plausibility: The models reliance on biological mechanisms like bursting STP and dendritetargeting inhibition enhances its biological realism. 3. Performance on Image Classification: BurstCCN demonstrates strong performance on image classification tasks like MNIST and CIFAR10 showcasing its practical applicability. 4. SinglePhase Learning: The ability to learn without multiple phases is a significant advancement aligning more closely with biological learning processes.  Weaknesses 1. Biological Constraints: While BurstCCN aims for biological plausibility it still relies on feedback alignment for weight transport impacting performance in challenging tasks. 2. Learning Rule Complexity: The models learning mechanisms involving feedback weights may be complex and might not fully address the weight transport problem. 3. Performance Variability: The performance differences between random and symmetric feedback weight regimes suggest potential limitations in learning dynamics.  Questions 1. How does BurstCCN handle noise and variability typical in realworld datasets that might impact learning and generalization 2. Can the model scale effectively to even deeper networks beyond those tested in the experiments 3. How does the BurstCCN model adapt to changes in task complexity or domain shifts during learning  Limitations 1. The reliance on feedback alignment for weight transport poses a limitation on overall performance and may not fully resolve the weight transport problem. 2. The models biological plausibility is subject to simplifications and assumptions affecting its accuracy in emulating brainlike learning processes. 3. The performance variance between different feedback weight regimes suggests potential challenges in model stability and convergence. Overall Bursting CorticoCortical Networks (BurstCCN) presents a promising model for addressing the credit assignment problem in neural networks with notable strengths in biological realism and singlephase learning. However further research is needed to address limitations related to learning dynamics and performance variability.", "znNmsN_O7Sh": "Peer Review: 1) Summary: The paper introduces Object Scene Representation Transformer (OSRT) a model designed for unsupervised learning of 3Dconsistent decompositions of complex scenes into individual objects. OSRT utilizes novel view synthesis to achieve efficient and scalable object discovery in 3D scenes. The proposed Slot Mixer decoder allows for rendering arbitrary views with 3Dconsistent object instance decompositions in a single forward pass. The experiments demonstrate that OSRT outperforms existing methods in terms of object decomposition and efficiency across various datasets including the challenging MSNHard dataset. 2) Strengths:  The paper addresses the important problem of 3Dconsistent decompositions of complex scenes into individual objects in an unsupervised manner.  The integration of Slot Mixer decoder enables efficient objectaware rendering with minimal computational overhead.  Novel view synthesis approach facilitates scalable learning of objectcentric 3D scene representations without supervision.  The paper provides a comprehensive experimental evaluation on multiple datasets showcasing the superior performance of OSRT compared to existing methods. Weaknesses:  The paper focuses more on the technical details and experimental results potentially leaving the theoretical underpinnings and implications underexplored.  Limited discussion on the potential applications of OSRT beyond scene decomposition and rendering.  The explanation of the methodology and technical aspects could be further simplified to enhance readability for a broader audience. 4) Questions:  Are there any plans to extend the application of OSRT beyond scene decomposition to other tasks in computer vision or even to other domains  Can the Slot Mixer decoder handle highly dynamic scenes with moving objects and if so how does it adapt to such scenarios  How does the proposed model perform in noisy or cluttered scenes where object boundaries might not be welldefined 5) Limitations: The abovementioned limitations of the paper were noted during the review. Specifically more focus on theoretical implications and broader potential applications of OSRT could enhance the impact of the work. Additionally addressing potential challenges such as handling noisy or cluttered scenes could further strengthen the contributions of this research. Overall the paper presents a novel and promising approach for objectcentric 3D scene representation learning backed by thorough experimental evaluation and performance comparisons with existing methodologies. With some clarity improvements and extensions to explore the applicability of OSRT in more diverse scenarios this work has potential to significantly impact the field of neural scene representation learning.", "zD65Zdh6ZhI": "Summary: This paper delves into the realm of explainable artificial intelligence (XAI) focusing on formal explainability within machine learning models particularly decision trees. The authors investigate the computation of \u03b4sufficientreasons a probabilistic explanation approach and tackle the computational complexity of finding minimal and minimal inclusionwise \u03b4sufficient reasons for decision trees. Several structural restrictions of decision trees that enable tractable computation are identified and SAT solvers are proposed as a practical solution backed by experimental validations. Strengths: 1. Theoretical Depth: The paper delves into the theoretical complexity of explaining decision trees offering insights into formal notions of explainability and probabilistic explanations. 2. Structural Restrictions: By identifying tractable restrictions of decision trees the study offers practical insights for efficient explanation computation. 3. Practical Validation: The experimental demonstration of using SAT solvers for solving explanation problems on practical instances adds empirical validity to the theoretical findings. 4. Open Problems Addressed: By settling open problems and extending previous research the paper contributes significantly to the stateoftheart in XAI. Weaknesses: 1. Complexity Overload: The paper might overwhelm readers with its detailed technical content potentially hindering accessibility for a broader audience. 2. Lack of Comparative Analysis: The paper lacks a comparative analysis with existing methodologies or solutions in the XAI field which could provide a more comprehensive understanding of the contributions. 3. Empirical Validation Limitation: While the experimental results demonstrate the feasibility of SAT solvers a more indepth empirical analysis on diverse datasets could enhance the practical applicability of the proposed approach. Questions: 1. What specific realworld applications or scenarios can benefit most from the theoretical advancements presented in this paper 2. How do the findings of this study align with or differ from existing research on XAI and explainable decisionmaking processes 3. How generalizable are the findings to different machine learning models beyond decision trees 4. Could the proposed methodology be adapted to address interpretability challenges in nonbinary classification tasks or regression models Limitations: 1. The theoretical nature of the study may limit direct applicability to practical XAI scenarios without extensive adaptation. 2. The complexity of the proposed solutions and encoding methodologies could pose implementation challenges outside controlled experimental settings. 3. The focus primarily on decision trees may limit the generalizability of the findings to other ML models with more complex structures or features.", "xwBdjfKt7_W": " Peer Review:  1) Summary: The paper addresses the vulnerability of Spiking Neural Networks (SNNs) to adversarial attacks by proposing a regularized adversarial training scheme termed SNNRAT. The study theoretically analyzes the perturbation robustness of SNNs and introduces a Lipschitz constant specifically for spike representation. The proposed approach is validated through experiments on image recognition benchmarks demonstrating improved robustness against various adversarial attacks.  2) Strengths and weaknesses: Strengths:  The paper introduces a novel regularization method for training robust SNNs against adversarial attacks.  The theoretical perturbation analysis of spike representation adds value to the understanding of SNN vulnerability.  Comprehensive experiments on CIFAR datasets validate the effectiveness of the proposed SNNRAT method.  Results show significant improvement in model robustness especially against powerful adversarial attacks. Weaknesses:  The paper lacks a detailed comparison with existing methods and benchmarks limiting the understanding of the proposed approachs superiority.  The emphasis on theoretical analysis could be further complemented with practical implications and realworld applications.  The computational overhead of the proposed method should be addressed especially in terms of training time and resource consumption.  3) Questions:  How does the proposed SNNRAT method compare to stateoftheart adversarial training techniques in terms of performance and computational efficiency  Can the regularization approach adopted for SNNs be extended to other types of neural networks for improved robustness against adversarial attacks  How generalizable is the proposed approach beyond image recognition tasks to other domains requiring SNNs in safetycritical applications  4) Limitations:  The generalizability of the proposed SNNRAT method to different datasets and neural network architectures should be further explored to ensure its applicability across various scenarios.  The evaluation focused on image recognition tasks and extending the study to other domains could provide a more comprehensive understanding of the methods effectiveness.  The computational overhead associated with the proposed regularization and adversarial training scheme needs to be addressed for practical scalability and adoption. Overall the paper presents a novel approach to enhance the robustness of SNNs against adversarial attacks through regularization and adversarial training. Further investigations on scalability broader applicability and efficiency considerations would strengthen the impact of the proposed method in realworld scenarios.", "qtQ9thon9fV": " Peer Review:  1) Summary: The paper introduces Fourier Occupancy Field (FOF) as a novel 3D geometry representation for monocular realtime and accurate human reconstruction. FOF efficiently represents 3D objects with a 2D field using Fourier series bridging the gap between 2D images and 3D geometries. The proposed method achieves highquality results at 30FPS outperforming existing representations in terms of generality alignment with input and computational efficiency. Experimental results demonstrate the effectiveness of FOF on public datasets and real captured data.  2) Strengths and weaknesses: Strengths:  Innovation: FOF is a novel and efficient representation for 3D human reconstruction enabling realtime highfidelity results.  Performance: FOF outperforms existing representations in terms of computational efficiency generality and highfidelity reconstruction.  Implementation: The proposed method is simple efficient and endtoend suitable for realtime applications.  Experimental Results: Extensive experiments on public datasets and real captured data validate the effectiveness of the proposed approach. Weaknesses:  Representation Limitation: FOF may have limitations in representing very thin objects due to the nature of the Fourier series expansion.  Privacy Concerns: The efficiency in 3D reconstruction may raise privacy and ethical issues that need to be addressed.  3) Questions:  Scalability: How scalable is the FOF representation to different sampling rates and resolutions in practical applications  Realtime Requirements: Are there specific hardware or computational requirements to achieve the claimed realtime performance  Adaptability: How adaptable is FOF to different types of objects beyond human reconstruction  4) Limitations:  Representation Limitation: FOF may not effectively represent very thin objects due to the necessity for additional terms in the Fourier series.  Privacy Concerns: The efficiency in 3D reconstruction may raise privacy and ethical concerns that require careful consideration.  5) Recommendations:  Further Investigations: Explore the use of wavelet transformation to enhance the representation capability of FOF.  Algorithm Optimization: Investigate potential algorithmic optimizations to address representation limitations for thin objects.  Ethical Consideration: Emphasize the importance of privacy and ethical considerations in the development and deployment of the proposed method. Overall the paper presents a novel and efficient method for 3D human reconstruction. The experimental results and comparison with existing methods demonstrate the effectiveness of FOF in achieving realtime highfidelity results. Further exploration of scalability adaptability to diverse objects and addressing representation limitations would enhance the impact and potential applications of FOF in the field of 3D reconstruction.", "p_g2nHlMus": " Peer Review of the Paper  1. Summary: The paper introduces FewTURE a novel method for fewshot learning that addresses the challenge of supervision collapse caused by onehot imagelevel labels. The approach decomposes images into patches uses selfsupervised pretraining with Masked Image Modeling and utilizes a Vision Transformer architecture for representation. It employs a classifier based on patch embedding similarities and a token importance reweighting mechanism to enhance classification. Experimental results show improved generalization and stateoftheart performance on popular fewshot benchmarks.  2. Strengths and Weaknesses: Strengths:  Novel approach addressing supervision collapse in fewshot learning.  Introduction of patchbased embedding similarity classifier.  Effective use of selfsupervised training with Vision Transformers.  Stateoftheart results on multiple fewshot classification benchmarks. Weaknesses:  The paper is quite technical and may be challenging for readers without a strong background in machine learning and computer vision.  Limited discussion on computational efficiency and scalability of the proposed method.  Further explanation on the inner workings of token importance reweighting may be beneficial for better understanding.  3. Questions:  How does the proposed method perform on larger datasets with more diverse content  Can FewTURE be applied to other computer vision tasks beyond fewshot learning  What additional experiments or scenarios could provide more insights into the methods capabilities  How sensitive is the performance of the approach to hyperparameters selection  4. Limitations:  The methods efficiency and scalability on larger datasets need further exploration.  The complexity of the approach may hinder its practical deployment in realworld applications.  The discussion on limitations and potential failure modes of the method could be more elaborated.  The reproducibility and generalizability of the results across different datasets and scenarios require more comprehensive evaluation. Overall the paper introduces an innovative approach to addressing supervision collapse in fewshot learning through patchbased representation and token reweighting. The thorough experimental evaluation demonstrates the effectiveness of the proposed method on existing benchmarks. Further exploration into scalability realworld applicability and potential extensions of the method could enhance the impact and significance of the research.", "pDUYkwrx__w": " Peer Review  1) Summary The paper addresses foundational theoretical questions regarding the privacy loss of the widely used NOISYSGD algorithm in machine learning. It resolves questions about the privacy loss and characterizes differential privacy up to a constant factor for NOISYSGD in various settings. The main result shows that the privacy loss of NOISYSGD does not increase indefinitely in the number of iterations contrary to previous analyses. The analysis uses innovative techniques based on optimal transport and the Sampled Gaussian Mechanism.  2) Strengths and weaknesses Strengths:  The paper addresses a crucial issue in machine learning by providing a thorough analysis of the privacy loss in the NOISYSGD algorithm.  The results provide new insights into the behavior of privacy loss in iterative algorithms.  Innovative techniques based on optimal transport and the Sampled Gaussian Mechanism are employed for the analysis.  The results are supported by detailed proofs and discussions in the Supplementary Materials. Weaknesses:  The paper focuses primarily on theoretical aspects and may benefit from empirical validation or practical examples to illustrate the implications of the findings.  The complex mathematical analysis and technical language might make it challenging for readers without a strong background in differential privacy and optimization.  3) Questions  Can the analysis techniques used in this paper be extended to other machine learning algorithms beyond NOISYSGD  How do the results and conclusions of this study impact the practical implementation of privacypreserving machine learning algorithms in industry settings  Do the findings of the paper suggest any future research directions in the field of privacypreserving machine learning or differential privacy  4) Limitations  The study primarily focuses on convex optimization and may not generalize well to more complex nonconvex optimization problems commonly encountered in deep learning applications.  The paper lacks practical validation or realworld case studies to demonstrate the applicability and implications of the theoretical results in practical machine learning scenarios.  The technical complexity of the analysis techniques and the mathematical formalism used in the paper may limit the accessibility of the findings to a broader audience of researchers and practitioners in machine learning. Overall the paper provides valuable insights into the privacy properties of the NOISYSGD algorithm but additional empirical validation and a more accessible presentation of the results could enhance the impact and reach of the study within the machine learning research community.", "ldxUm0mmhl8": "Peer Review: 1) Summary: The paper introduces a novel approach to address the limitations of machine learning models based on temporal point processes. It focuses on developing a causal model of thinning and a sampling algorithm to generate counterfactual realizations of temporal point processes under alternative intensity functions. The proposed methodology is evaluated on synthetic and real epidemiological data to showcase its potential to enhance targeted interventions. 2) Strengths:  The paper addresses an important gap in existing machine learning models by enabling counterfactual reasoning in temporal point processes.  The use of causal modeling and sampling algorithms is welljustified and contributes to the advancement of research in the field.  The experimental evaluation on synthetic and real epidemiological data demonstrates the effectiveness of the proposed methodology.  The detailed explanation of algorithms theoretical foundations and experimental results provides a comprehensive understanding of the work. 3) Weaknesses:  The paper could benefit from clearer illustrations or figures to enhance the understanding of the proposed algorithms and their applications.  The limitations and assumptions of the proposed methodology could be further discussed to provide a more nuanced perspective on the applicability of the approach.  The practical implications of the findings in realworld scenarios beyond epidemiology could be elaborated for a broader impact on diverse application domains. 4) Questions:  How does the proposed causal model of thinning compare to existing approaches in terms of computational efficiency and accuracy  Can the methodology be easily generalized to handle more complex temporal point processes beyond inhomogeneous Poisson processes and linear Hawkes processes  Are there any specific challenges or considerations in deploying the proposed algorithm in realworld settings especially in dynamic and evolving environments 5) Limitations:  The paper acknowledges the limitation of not being able to validate counterfactual predictions using observational or interventional experiments. Exploring potential validation strategies could strengthen the credibility of the findings.  The sensitivity of the methodology to the choice of the structural causal model (SCM) is highlighted as an important aspect to investigate. Further analysis on this aspect could enhance the robustness of the approach. Overall the paper makes a valuable contribution to the field by introducing a method for counterfactual reasoning in temporal point processes. With additional clarity on the limitations and implications of the proposed methodology as well as further investigation into validation strategies and SCM sensitivity the research could have a broader impact on diverse applications involving discrete event data in continuous time.", "rDT-n9xysO": " Peer Review:  1) Summary: The paper introduces a novel twostage solution for TCP congestion control proposing a symbolic distillation framework that converts deep reinforcement learning policies into interpretable symbolic rules. The approach aims to combine the efficiency of traditional handcrafted algorithms with the learning capabilities of neural networks. The research introduces a branching technique for training multiple contextdependent symbolic policies and validates their performance in simulation and emulation environments. The results demonstrate competitive performance with improved speed and efficiency compared to existing approaches.  2) Strengths:  Innovative Approach: The paper presents a novel solution to enhance TCP congestion control by distilling NNbased RL policies into symbolic rules improving interpretability and efficiency.  Branching Technique: The proposed branching technique for training contextdependent symbolic agents is a valuable contribution that enhances the adaptability of the system to diverse network conditions.  Performance Results: The experimental results show that SymbolicPCC achieves competitive performance and faster execution times compared to existing methods highlighting the effectiveness of the approach.  Interpretability and Efficiency: The emphasis on creating interpretable rules and maintaining computational efficiency is a significant strength of the research making the solution practical for deployment.  Weaknesses:  Lack of Comparative Analysis: While the paper presents results showing the performance of SymbolicPCC a more detailed comparative analysis against existing methods would strengthen the research by providing a clearer benchmark for evaluation.  Generalization of Results: The paper could benefit from discussing the potential generalizability of the proposed approach to other RL domains or networking problems beyond TCP congestion control.  Evaluation on Real Networks: While emulation results are presented realworld deployment scenarios and their challenges could add depth to the evaluation of SymbolicPCCs practical applicability.  4) Questions:  Scalability: How does SymbolicPCC scale with more complex network topologies or varied network conditions How does the proposed branching technique adapt to new conditions  Deployment Challenges: What are the potential challenges or limitations in deploying SymbolicPCC in real networking environments especially considering dynamic and unpredictable conditions  Interpretation Complexity: While the distilled symbolic rules are simpler how does the complexity of interpreting these rules scale as the network conditions become more diverse or complicated  5) Limitations:  Limited Generalization: The paper focuses on TCP congestion control limiting the generalizability of the symbolic distillation approach to other domains or network applications.  Evaluation Environment: While emulation results are promising realworld deployment challenges and the transition from simulation to actual implementation remain unaddressed.  Complexity Management: Managing the complexity of SymbolicPCC rules as they scale to diverse network conditions may pose challenges in rule interpretation and analysis.  Overall the research contributes significantly to the field of TCP congestion control by introducing a novel symbolic distillation framework. Further exploration of the scalability interpretability and practical deployment of SymbolicPCC could enhance the impact and applicability of the proposed solution in realworld networking scenarios. The strengths in efficiency and performance along with the interpretative benefits make the research promising for future developments in network optimization and congestion control.", "r4RRwBCPDv5": " Summary: The paper presents a VCtheoretical analysis of the double descent phenomenon in multilayer neural networks. It examines the application of VC bounds to explain generalization performance during both first and second descent modes. The paper discusses the relationship between VCdimension training error and test error providing insights into the factors influencing the generalization performance of deep learning models.  Strengths: 1. The paper addresses an important topic in deep learning and provides a theoretical framework for understanding the generalization performance of neural networks. 2. It explores the application of VC theory to explain the double descent phenomenon offering a fresh perspective on the biasvariance tradeoff. 3. The paper includes empirical results for multiple learning methods such as SVM and Least Squares classifiers illustrating the effectiveness of the VCtheoretical explanation.  Weaknesses: 1. The paper could benefit from more detailed explanations or examples to enhance the understanding of the theoretical concepts presented. 2. The lack of information on reproducibility elements such as code data and instructions limits the ability to validate the empirical results and findings. 3. The paper focuses primarily on theoretical analysis and may benefit from further discussion on practical implications or realworld applications.  Questions: 1. How does the proposed VCtheoretical explanation of double descent compare to existing explanations or theories in the deep learning literature 2. Can the findings of this study be extrapolated to more complex neural network architectures beyond the simplified models considered in the paper 3. How do the suggested values of the positive constants a1 and a2 impact the accuracy of VCbounds for modeling double descent in practical scenarios  Limitations: 1. Limited discussion on the societal impacts or ethical considerations of the research. 2. Lack of reproducibility elements like code and data availability may hinder the verification and extension of the experimental results. 3. The generalizability of the findings to more intricate deep learning models and diverse datasets is not extensively explored in the paper.", "yJV9zp5OKAY": "Peer Review of the Paper: Autonomous LabelPreserving Adversarial AutoAugment for Representation Learning 1) Summary: The paper introduces an autonomous data augmentation method LabelPreserving Adversarial AutoAugment (LPA3) which focuses on creating taskinformed augmentations without the reliance on domain knowledge. LPA3 is derived from a representation learning principle aiming to preserve label information while discarding nuisance information. Using a surrogate objective based on perceptual distance LPA3 generates hard positive examples for data augmentation without the need for an extra generative model. The method is demonstrated to improve performance and convergence in various machine learning tasks including supervised semisupervised and noisylabel learning particularly in scenarios where existing augmentation techniques are ineffective or domain knowledge is unavailable. 2) Strengths and weaknesses: The paper presents a novel and practical approach to autonomous data augmentation with LPA3 which is a significant strength. The method does not require additional generative models making it efficient and easy to integrate with existing algorithms. The experimental results demonstrate consistent improvements across multiple learning tasks showcasing the methods effectiveness. The thorough explanation of theoretical principles and the implementation of LPA3 are commendable. However some weaknesses include the complexity of the method which may limit its accessibility to a broader audience. Additionally the paper would benefit from clearer explanations of certain technical aspects and may require more detailed discussions on potential limitations and future directions. 3) Questions:  How does LPA3 compare to other stateoftheart autonomous data augmentation methods in terms of performance and computational efficiency  Can the proposed surrogate objective in LPA3 handle diverse and complex datasets effectively considering the potential tradeoffs between label preservation and noise amplification  Has LPA3 been tested on realworld datasets outside the experimental environment to validate its applicability to various domains with diverse data characteristics  How sensitive is LPA3 to hyperparameters and are there specific recommendations for parameter settings based on different learning tasks 4) Limitations: The paper could provide more detailed explanations regarding the potential limitations and assumptions of LPA3 especially in realworld scenarios where datasets may exhibit high variability. The generalizability of LPA3 to different domains data types and task complexities might need further exploration. Additionally the paper could explore the interpretability of the generated augmentations and how they affect the models decisionmaking process. Overall the paper makes a valuable contribution to the field of autonomous data augmentation offering a principled approach with practical implications for representation learning tasks. Further studies addressing the questions and limitations raised above could enhance the robustness and applicability of LPA3 in diverse machine learning scenarios.", "ucNDIDRNjjv": "Peer Review 1. Summary The paper \"Nonstationary Transformers: Bridging the Gap Between Series Predictability and Model Capability in Time Series Forecasting\" introduces a generic framework to enhance the predictive capability of Transformers in time series forecasting by addressing the overstationarization problem. The proposed Nonstationary Transformers framework consists of Series Stationarization and Destationary Attention modules to unify statistics and recover nonstationarity in deep models. Experimental results show significant improvements in forecasting performance on realworld datasets compared to traditional stationarization methods making it stateoftheart across various mainstream Transformer models. 2. Strengths and weaknesses Strengths:  The paper addresses an important issue in time series forecasting and proposes a novel framework to handle data nonstationarity.  The study extensively evaluates the proposed framework on realworld datasets and compares it with stateoftheart models showcasing consistent performance improvements.  The ablation study and model analysis provide indepth insights into the effectiveness of each module enhancing the understanding of the proposed approach. Weaknesses:  The paper lacks a thorough explanation of the baseline algorithms used for comparison which may hinder readers understanding of the performance comparisons.  While the paper highlights the performance improvements achieved more detailed analysis on why the proposed framework outperforms traditional methods could provide stronger validation.  The paper would benefit from a more concise presentation of the method and results to improve readability and clarity. 3. Questions  Can the authors elaborate more on the reasoning behind the selection of specific datasets for evaluation and how they represent realworld time series forecasting challenges  How does the proposed framework generalize to different types of time series data beyond the ones evaluated in this study  Is there any insight on the computational overhead introduced by the Nonstationary Transformers framework compared to traditional methods 4. Limitations  The paper mainly focuses on the performance improvements achieved by the proposed framework but lacks a deeper exploration of potential limitations or failure cases.  The evaluation metrics primarily focus on forecasting accuracy and additional analysis on the interpretability and robustness of the model could provide a more holistic assessment.  While the proposed framework shows promising results the study could be further strengthened by discussing possible scenarios where the framework might not be as effective. In conclusion the paper makes a valuable contribution to the field of time series forecasting by introducing a novel framework to address nonstationarity issues. Further exploration and validation in diverse scenarios and datasets could enhance the robustness and applicability of the proposed Nonstationary Transformers framework.", "wo-a8Ji6s3A": " Peer Review  1. Summary The paper introduces a novel algorithm Loopless Projection Stochastic Approximation (LPSA) to address linearly constrained stochastic approximation problems with federated learning as a special case. LPSA utilizes a probabilistic projection approach and provides insights into the tradeoff between bias and variance. The authors analyze the algorithm from both a nonasymptotic and asymptotic perspective generating significant theoretical results and conducting experiments to validate their findings.  2. Strengths and Weaknesses Strengths:  The paper addresses an important and timely topic in federated learning and optimization for distributed systems.  The proposed LPSA algorithm is innovative and shows promise in achieving efficient convergence with a tradeoff analysis.  The theoretical analysis is comprehensive covering nonasymptotic and asymptotic behaviors with detailed proofs.  The experiments provide empirical validation of the theoretical results on synthetic datasets enhancing the credibility of the findings. Weaknesses:  The paper lacks a clear motivation section to emphasize the significance of the research problem addressed.  The background information on federated learning and related concepts could be expanded to better situate the study within the existing literature.  Some parts of the paper may be challenging to follow for readers not wellversed in stochastic optimization and mathematical analysis.  Limited practical applications or realworld use cases are discussed potentially limiting the broader relevance of the proposed algorithm.  3. Questions  How does the performance of LPSA compare to existing stochastic approximation algorithms in terms of convergence speed and accuracy  Could you elaborate on the practical implications and potential applications of LPSA in realworld scenarios beyond synthetic datasets  Have you considered additional experiments or benchmarks on more realistic datasets to further validate the generalizability and effectiveness of LPSA  4. Limitations  The paper focuses heavily on theoretical analysis and may benefit from clearer explanations and examples to make the concepts more accessible to a wider audience.  The complexity of the algorithm and the theoretical derivations might limit the adoption and understanding of LPSA by practitioners in the field.  The generalizability of the findings to various scenarios and datasets needs to be further explored through additional experiments and evaluations. Overall the paper presents a valuable contribution to the field of stochastic optimization and federated learning offering a novel algorithmic approach and insightful theoretical analyses. Addressing the mentioned weaknesses and limitations could enhance the clarity and applicability of the research findings.", "k_XHLBD4qPO": "Peer Review of \"ClassIncremental Semantic Segmentation\" Summary: The paper presents a novel approach termed Dropout Continual Semantic Segmentation (DCSS) to address the ClassIncremental Semantic Segmentation (CISS) problem through a focus on offline training procedures and the representation quality. By incorporating wider classifiers and introducing dropout layers the study demonstrates advancements in IoU metrics across different scenarios while maintaining efficient model size and improved scalability. Strengths: 1. The paper addresses an important problem in computer vision  ClassIncremental Semantic Segmentation (CISS)  with proposed innovative solutions. 2. The inclusion of experimental results and comparison with existing methods provides solid evidence of the effectiveness of the DCSS model. 3. The authors provide detailed explanations of the methods used the rationale behind the design choices and theoretical underpinnings enhancing the papers scientific rigor. Weaknesses: 1. Lack of reporting of error bars for experimental results could limit the robustness of the findings. 2. The discussion on potential negative societal impacts could be further elaborated upon to consider broader ethical implications. 3. While the paper describes the limitations of the work exploring or addressing these limitations more explicitly could enhance the papers depth. Questions: 1. What considerations were made in choosing the specific hyperparameters such as the probability values for Dropout the width of the classifiers and the choice of the segmentation model 2. How generalizable do the proposed improvements in DCSS appear to be across different architectures beyond the DeepLabV3 model 3. How does the DCSS model adapt to highly dynamic or evolving datasets where the nature of semantic segmentation classes changes frequently over time Limitations: 1. The absence of error bars and discussing potential participant risks or consent considerations pose limitations in the papers robustness. 2. The reliance on specific datasets and scenarios may limit the generalizability of the proposed DCSS model to diverse realworld applications. 3. The paper might benefit from a more extensive discussion on the implications of the Information Bottleneck principle and its broader implications for continual learning models. Overall the paper presents a wellstructured study with innovative contributions to the field of continual learning in semantic segmentation. Addressing the identified weaknesses and limitations can further enhance the impact and applicability of the presented research findings.", "suHUJr7dV5n": " Peer Review:  Summary: The paper introduces a novel approach named PZOBO for Hessianfree bilevel optimization in machine learning. The authors address the challenges of nested structures in bilevel optimization particularly focusing on approximating the response Jacobian matrix effectively. PZOBO utilizes a zerothorderlike method to estimate the Jacobian matrix by computing the difference between two optimization paths. The paper provides theoretical convergence rate analysis and demonstrates the superiority of PZOBO over existing bilevel optimizers through experiments on various bilevel problems including fewshot metalearning and hyperparameter optimization.  Strengths: 1. Innovative Approach: The proposed PZOBO method introduces a unique strategy for approximating the response Jacobian matrix in bilevel optimization offering a novel perspective on Hessianfree algorithms. 2. Theoretical Analysis: The paper provides a rigorous theoretical foundation for the proposed algorithm including convergence rate analysis making a valuable contribution to the understanding of Hessianfree bilevel algorithms. 3. Empirical Performance: The experimental results demonstrate the effectiveness of PZOBO in outperforming baseline methods across a range of bilevel problems showcasing its practical utility and superior performance in specific applications.  Weaknesses: 1. Complexity: The introduction of a new concept and method while innovative might make the paper less accessible to readers unfamiliar with the topic. Providing more context or simplifying the presentation could enhance the papers readability. 2. Experimental Details: While the experimental results are promising additional details on hyperparameters datasets and implementation specifics could strengthen the reproducibility and comprehensibility of the experiments. 3. Comparative Analysis: While the experiments demonstrate the superiority of PZOBO a more comprehensive comparison with a wider range of stateoftheart methods could further validate the proposed algorithms effectiveness.  Questions: 1. What are the key limitations of the proposed PZOBO method and are there specific scenarios or types of problems where it may not perform as well as existing approaches 2. How does the computational complexity of PZOBO compare to other bilevel optimization methods particularly in terms of scalability to larger datasets or models 3. Could the authors elaborate on the practical implications of the proposed algorithm especially in realworld machine learning applications where computational efficiency and performance are crucial  Limitations: 1. Scalability: The scalability of PZOBO to larger datasets or complex models may be a potential limitation that needs to be further explored. 2. Generalization: The generalizability of PZOBO to a wider range of machine learning problems beyond the specific bilevel scenarios investigated in the paper should be examined to assess its versatility. Overall the paper presents a valuable contribution to the field of bilevel optimization offering an innovative approach with promising results. Addressing the weaknesses and limitations identified as well as providing additional clarifications and context would further enhance the papers impact and appeal to a broader audience within the machine learning research community.", "mjUrg0uKpQ": " Summary The paper presents I2DFormer a transformerbased zeroshot learning (ZSL) framework that leverages online textual documents such as Wikipedia for learning unsupervised semantic embeddings to improve zeroshot image classification. The model aligns images and documents by jointly encoding them in a shared embedding space and uses a novel crossmodal attention module to learn interactions between image patches and document words. Through experiments on three public datasets the researchers demonstrate that I2DFormer outperforms existing unsupervised semantic embeddings and achieves highly interpretable results where document words are grounded in image regions.  Strengths  Innovative Approach: The use of online textual documents for unsupervised semantic embeddings in ZSL is a novel and promising approach.  Performance: The experimental results show that I2DFormer consistently outperforms existing methods in both zeroshot and generalized zeroshot learning settings on multiple datasets.  Interpretability: The model generates interpretable results where document words can be linked to image regions enhancing the explainability of the predictions.  Weaknesses  Data Constraints: The model is trained on a limited number of training images for each class which may pose challenges in generalizing to unseen classes.  Document Noise: Since online documents contain noise and irrelevant details the model may struggle to extract relevant visual information from the text.  Baseline Comparison: The comparison with existing baseline ZSL methods could be more comprehensive to provide a clearer understanding of the models performance.  Questions 1. Model Generalizability: How does the model perform when applied to datasets beyond AWA2 CUB and FLO Have there been any tests on datasets with different characteristics 2. Scalability: How scalable is the proposed approach to handle a larger number of classes and more diverse textual content from various sources 3. Document Filtering: Can you elaborate on the document filtering process to address noise and irrelevant information in the text and how does it affect model performance  Limitations  Model Interpretability: While the results are interpretable the paper could further discuss the limitations or uncertainties in the interpretability of the models decisions.  Document Quality: The impact of the quality accuracy and bias in the online textual data on the models performance and generalizability could be explored in more depth.  Computational Efficiency: The computational cost of training and inference for the proposed model could limit its practical deployment especially with larger datasets. Overall the paper introduces a promising approach with practical implications for ZSL highlighting the significance of leveraging online textual data for improved zeroshot learning performance. Further exploration and validation on diverse datasets and realworld applications can strengthen the significance of the proposed method.", "lme1MKnSMb": "Peer Review Summary: The paper introduces a novel transformerbased neural video compression method that outperforms traditional methods without relying on specific architectural priors like motion prediction. The approach simplifies video compression by leveraging transformers to model temporal redundancies and predict the distribution of video representations. Experimental results demonstrate superior performance on standard video compression datasets and synthetic data across various motion patterns. Strengths: 1. The paper presents an innovative approach to neural video compression showcasing the power of transformers in handling temporal redundancies without conventional handcrafted components like motion prediction. 2. The detailed explanation of the method illustrating the stepbystep process and architectural choices enhances the reproducibility of the research. 3. Extensive experiments on both standard and synthetic datasets provide strong evidence of the effectiveness of the proposed video compression transformer (VCT) method. 4. The comparison with existing methods including nonneural codecs and neural compression approaches adds value to understanding the performance and efficiency of VCT. 5. The release of code and models to facilitate future research and reproducibility of the results is commendable. Weaknesses: 1. While the methods performance is shown to be superior additional insights into the limitations or failure cases of VCT could further enrich the discussion. 2. The paper could benefit from a more comprehensive discussion on the scalability and potential challenges of extending the approach to handle longterm memory for videos with more complex temporal patterns. 3. The study primarily focuses on quantitative metrics like PSNR and MSSSIM but a qualitative evaluation through visual comparisons or perceptual metrics could provide a holistic understanding of compression quality. 4. Improved visualization aids or examples could enhance the readers understanding of the models predictions and the compression process. Questions: 1. How does the proposed VCT approach handle videos with high levels of complexity such as fastpaced action scenes or dynamic backgrounds 2. Can the method adapt to diverse video resolutions and frame rates effectively or are there specific constraints that may impact its performance 3. To what extent does the transformer model learn semantics or context within video frames and are there specific challenges in capturing such information 4. Would introducing more advanced transformer architectures such as GPTlike models bring additional benefits to video compression tasks or are the current transformer configurations optimal Limitations: 1. The study primarily focuses on standard and synthetic datasets potentially limiting the generalizability of the proposed method to a broader range of video content. 2. While the paper presents a novel approach that outperforms existing methods the limitations of the VCT model in realworld scenarios or specific video genres remain a topic for further investigation. 3. The research emphasizes ratedistortion performance and exploring other aspects like computational efficiency or energy consumption could provide a more comprehensive evaluation of the approach. Overall the paper presents an innovative method that significantly advances neural video compression techniques. Addressing the suggested points could further strengthen the study and its implications for the field of video compression and deep learning.", "mNtFhoNRr4i": " Summary The paper introduces a novel algorithm to evaluate hierarchical classifiers by producing operating characteristic curves to capture the tradeoff between specificity and correctness. The study compares various loss functions for training differentiable models and evaluates their performance on image classification datasets. Surprisingly the results show that the flat softmax classifier outperforms more complex topdown classifiers across all operating points. The paper proposes two new loss functions with the softmaxmargin loss showing significant improvements over existing methods.  Strengths 1. Novel Algorithm: The paper introduces an efficient algorithm to evaluate hierarchical classifiers using operating characteristic curves providing a comprehensive assessment of classifiers at multiple operating points. 2. Comparison of Loss Functions: The study compares various loss functions including novel ones and demonstrates the effectiveness of the softmaxmargin loss in outperforming existing methods. 3. Empirical Evaluation: The empirical study on image classification datasets provides practical insights into the performance of hierarchical classifiers shedding light on the effectiveness of different approaches. 4. Social Impact Consideration: The paper briefly discusses the social impact of automatic classification systems emphasizing the importance of exercising caution in consequential settings.  Weaknesses 1. Dominance of Flat Softmax: The unexpected finding that the flat softmax outperforms more complex topdown classifiers across all operating points raises questions about the efficacy of hierarchical classifiers in this context. 2. Ineffectiveness of Some Loss Functions: The softmaxdescendant loss is found to be ineffective suggesting limitations in the proposed novel loss functions. 3. Limited Generalization Discussion: The paper does not extensively discuss the generalization capabilities of the proposed algorithms beyond the evaluation on the provided datasets. 4. Limitations of the Study: The study mainly focuses on image classification datasets potentially limiting the generalizability of the findings to other domains.  Questions 1. Interpretation of Results: Can the unexpected dominance of the flat softmax over topdown classifiers be further clarified What could be the underlying reasons for this phenomenon 2. Generalization: How well do the proposed algorithms generalize to different datasets outside of image classification tasks 3. Effect of Hyperparameters: How sensitive are the algorithms to hyperparameters particularly in realworld scenarios where these might not be easily optimized 4. Scalability: Given the mention of limited scalability to millions of classes what are potential strategies to address scalability issues with fullyconnected prediction layers 5. Future Directions: What are some potential future research directions building upon the findings of this study  Limitations 1. Dataset Limitation: The findings are primarily based on image classification datasets potentially limiting the applicability of the results to other domains. 2. Algorithm Complexity: The study heavily focuses on loss functions and their effectiveness potentially overlooking other aspects that could influence classifier performance. 3. Hyperparameter Sensitivity: The discussion of hyperparameters and their impact is limited raising questions about the robustness of the proposed algorithms in varying settings.", "t6O08FxvtBY": " Peer Review:  1) Summary: The paper introduces Bilevel Optimizationoriented Pruning (BIP) as a method to improve model pruning efficiency and identify winning tickets in deep learning models. By formulating model pruning as a bilevel optimization problem BIP separates the tasks of pruning and retraining achieving higher accuracy and sparsity compared to existing methods. Through extensive experiments on various datasets and model architectures BIP demonstrates superior performance and computational efficiency over competing methods.  2) Strengths and weaknesses: Strengths:  The paper addresses an important problem in deep learning model pruning and introduces a novel approach BIP which combines theoretical grounding with practical efficiency.  The proposed BIP method achieves better accuracy and sparsity levels compared to existing pruning methods in most cases across various datasets and model architectures.  Extensive experiments are conducted to validate the effectiveness and efficiency of BIP showcasing its superiority over traditional pruning methods.  The paper provides detailed algorithmic explanations theoretical foundations and implementation strategies for BIP enhancing the reproducibility and understanding of the proposed method. Weaknesses:  While the theoretical foundations and efficacy of BIP are welldocumented a deeper analysis of the limitations or failure cases of the proposed method could provide additional insights into its applicability across different scenarios.  The paper could benefit from a comparison with more recent stateoftheart pruning methods beyond the selected baselines to further highlight the uniqueness of BIP.  The discussion on potential realworld applications or implications of BIP in practical deep learning tasks could enhance the relevance and impact of the proposed method.  3) Questions:  Can BIP be extended or adapted to address specific challenges or constraints in realworld deployment scenarios such as edge computing or resourceconstrained devices  How does BIP perform in scenarios where model architectures or datasets exhibit high complexity or variability and are there specific limitations when applied to such cases  Are there considerations or optimizations that can be explored to improve the scalability and generalizability of BIP beyond the experimental setups presented in the paper  4) Limitations:  The limited discussion on the realworld applications or practical implications of BIP may restrict the broader understanding of how the proposed method can be integrated into industry or research settings.  The focus on structured pruning in the results may lead to an incomplete assessment of BIPs performance across different pruning techniques or strategies potentially overlooking areas of improvement or optimization.  The evaluation metrics used solely based on accuracy and sparsity may not encompass the full spectrum of model performance indicators such as inference speed or robustness warranting a more comprehensive evaluation framework. Overall the paper presents a novel and promising approach to model pruning through Bilevel Optimization demonstrating significant improvements over existing methods in terms of accuracy and efficiency. Further exploration of realworld applications scalability and adaptability of BIP could enhance its impact and relevance in the deep learning community.", "wA7vZS-mSxv": " Peer Review of the Paper: \"Estimating Intrinsic Dimensionality using Normalizing Flows\"  Summary: The paper proposes a novel method for estimating the intrinsic dimensionality of datasets using Normalizing Flows (NFs). The method leverages simple calculations predicting the change in singular values of the flows Jacobian when injecting noise into the data. The study demonstrates the effectiveness of the proposed method on various datasets including highresolution RGB images achieving stateoftheart results. The paper addresses the challenges of estimating intrinsic dimensionality particularly in high dimensions and provides a thorough theoretical framework for the proposed approach.  Strengths and Weaknesses:  Strengths: 1. Theoretical Foundation: The paper is theoretically wellgrounded providing a comprehensive overview of intrinsic dimensionality estimation challenges and proposing a method based on sound theoretical principles. 2. Novel Approach: The utilization of Normalizing Flows for intrinsic dimensionality estimation offers a unique and innovative solution to the scalability issues faced by traditional methods. 3. Experimental Validation: The method is tested on various datasets including realworld examples showcasing its efficacy and potential for practical applications. 4. Comparative Analysis: The comparison with existing methods like twoNN and LIDL provides valuable insights into the advantages of the proposed approach. 5. Detail and Clarity: The paper is wellstructured with detailed explanations of the method algorithms and experimental results enhancing the readers understanding.  Weaknesses: 1. Computational Complexity: The methods computational requirements especially in training NFs on highresolution image datasets could be a potential limitation for practical implementation. 2. Sensitivity: The sensitivity to parameters such as the number of samples used for estimation and noise magnitude may impact the methods robustness across different datasets and settings. 3. Scalability: While the method is shown to scale to high dimensions further investigation into its scalability on extremely large datasets may be necessary.  Questions: 1. Robustness: Could you elaborate on the robustness of the proposed method to variations in noise levels and dataset characteristics How sensitive is the estimation to noise perturbations 2. Generalizability: How well does the method generalize to diverse types of datasets beyond the examples presented in the paper Have you tested the approach on datasets with varying intrinsic complexities 3. Comparison: Can you provide more insights into the comparative performance of the proposed method against existing approaches in scenarios with limited training data or noisy samples 4. Interpretability: How intuitive is the method for practitioners without a strong background in theoretical concepts Are there practical guidelines for selecting parameters in realworld applications  Limitations: 1. Computational Resources: The computational demands of training NFs and calculating the eigenvalues for highresolution images may restrict the methods application on largescale datasets or resourceconstrained environments. 2. Applicability: The methods applicability across different domains and datasets with varying characteristics needs further exploration to understand its limitations and generalizability. 3. Parameter Sensitivity: The methods sensitivity to parameters like noise magnitude and number of samples could limit its robustness in realworld implementations without careful parameter tuning. Overall the paper presents a promising approach for intrinsic dimensionality estimation using Normalizing Flows and contributes significantly to the field of representation learning. Addressing the outlined questions and limitations could further enhance the practical utility and robustness of the proposed method.", "v9pljSdlUNP": " Summary The paper presents an indepth study on Stochastic Backpropagation (SBP) for memoryefficient training of deep neural networks in image classification and object detection tasks. SBP utilizes partial execution of gradients during backpropagation by caching only a subset of feature maps resulting in significant GPU memory savings and reduced training runtime. The authors explore the effectiveness and generalizability of SBP on various tasks and networks showing that SBP can achieve up to 40 memory savings with less than 1 accuracy degradation. They provide insights on key design choices and strategies for implementing SBP in practice.  Strengths and Weaknesses Strengths: 1. Comprehensive study on SBP for memoryefficient training of deep learning models. 2. Provides insights on implementing SBP for image classification and object detection tasks. 3. Demonstrates significant memory savings while maintaining high model accuracy. 4. Offers practical recommendations for applying SBP in realworld scenarios. Weaknesses: 1. Limited investigation into the speedup of the training process using SBP. 2. Lack of extensive evaluation on a wider range of architectures beyond the specific models tested. 3. The paper could provide more robust comparisons with existing memoryefficient training methods. 4. The experiments are primarily focused on memory efficiency with less emphasis on training speed enhancement.  Questions 1. Can SBP be extended to more complex deep learning architectures beyond the models tested in the paper 2. How does the memory savings of SBP compare to other memoryefficient training techniques like gradient checkpointing or sparse networks 3. What are the implications of using SBP on model interpretability and generalization performance 4. Is there a tradeoff between the level of memory savings and the accuracy drop when applying SBP to different network architectures  Limitations 1. The study focuses primarily on memory savings and accuracy tradeoffs with limited exploration of training speed improvements. 2. The generalizability of SBP to a broader range of tasks and architectures needs further validation. 3. The paper lacks detailed comparisons with existing memoryefficient training methods and their impact on model performance. 4. The findings may benefit from more extensive empirical evaluation on larger datasets and diverse model architectures. Overall the paper provides valuable insights into the application of SBP for memoryefficient training of deep neural networks but further investigation is warranted to address the identified limitations and questions.", "vKBdabh_WV": " Summary The paper introduces Meta Optimal Transport (Meta OT) as a method to improve the computational efficiency of solving optimal transport (OT) problems repeatedly between different measures. By leveraging amortized optimization and machine learning techniques the Meta OT models predict optimal solutions between discrete and continuous measures. The study demonstrates its effectiveness in various settings such as MNIST images spherical data and color palettes.  Strengths and Weaknesses Strengths: 1. Innovative Approach: The utilization of amortized optimization techniques for rapid prediction of optimal transport solutions is innovative and promising. 2. Experimental Validation: The paper provides a thorough experimental validation showcasing the effectiveness of Meta OT in improving computational efficiency. 3. Detailed Explanations: The paper goes into depth explaining the theoretical background of optimal transport and the implementation of Meta OT models. Weaknesses: 1. Limited RealWorld Applications: The paper focuses on theoretical concepts and limited experimental validation. Realworld applications and practical implications could be further explored. 2. Lack of Comparative Analysis: While the results are presented effectively a more detailed comparative analysis with existing approaches could enhance the papers impact. 3. Limited Discussion on Limitations: The section on limitations is brief and could benefit from a more indepth discussion to provide a comprehensive view of the constraints and challenges of the proposed approach.  Questions 1. Could the Meta OT approach be extended to handle more complex OT problems beyond the ones considered in the paper 2. How robust is the Meta OT model to variations in the input measures and costs 3. Have you explored scenarios where Meta OT is applied in more highdimensional settings and if so what were the outcomes and challenges 4. How sensitive is the Meta OT model to changes in hyperparameters and what strategies were employed to handle hyperparameter tuning  Limitations 1. Limited Generalization: The papers results may lack generalizability to other OT tasks outside the considered domains. 2. Realworld Implementation: The practical implementation and scaling of the Meta OT model to large datasets and diverse application scenarios remain unexplored. 3. Computational Complexity: The computational requirements and scalability of the Meta OT approach in handling larger and more complex OT problems might pose challenges. Overall the paper presents a novel approach with promising results in improving the computational efficiency of solving optimal transport problems. Further exploration and experimentation in diverse scenarios could enhance the applicability and robustness of the Meta OT framework.", "pGcTocvaZkJ": " Peer Review:  1) Summary: The paper introduces a novel algorithm Censored Quantile Regression Neural Network (CQRNN) which efficiently combines the Portnoys censored quantile regression estimator with neural networks for quantile regression on censored data in survival analysis. The algorithm is shown to outperform existing methods in terms of quantile estimation accuracy and calibration on both synthetic and realworld datasets. The work offers theoretical insights by interpreting CQRNN as a form of expectationmaximisation (EM) and demonstrating a selfcorrecting property. Experimental results and comparisons with baselines demonstrate the efficacy of the proposed algorithm.  2) Strengths and weaknesses: Strengths:  The paper introduces a novel algorithm CQRNN that efficiently combines Portnoys censored quantile regression estimator with neural networks for quantile regression on censored data.  The theoretical insights provided by interpreting CQRNN as a form of EM and showcasing its selfcorrecting property add depth to the analysis.  The empirical results demonstrate the superior performance of CQRNN compared to existing methods on a variety of datasets. Weaknesses:  The paper could benefit from a more detailed discussion of the limitations of the proposed algorithm especially in handling datasets where higher quantiles are undefined.  The experimental section could be expanded to include a wider range of realworld datasets to further validate the performance of CQRNN in diverse scenarios.  While the theoretical insights are interesting more rigorous analysis of the algorithms convergence properties and optimality guarantees would enhance the papers scientific robustness.  3) Questions:  Could you elaborate further on the implications of the selfcorrection property of CQRNN and how it contributes to the algorithms performance  How do you envision the scalability of CQRNN when applied to larger and more complex datasets in practical applications such as in healthcare or reliability engineering domains  Are there any specific scenarios or types of datasets where CQRNN may not perform well and what measures could be taken to address these challenges  4) Limitations: While the paper presents a novel and promising algorithm for quantile regression on censored data using neural networks there are certain limitations that could be addressed in future work:  Limited validation on a modest number of realworld datasets with a focus on biomedical domains may restrict the generalizability of the algorithm across diverse applications.  The discussion on the convergence properties and optimization guarantees of CQRNN could be further explored to ensure the reliability of the algorithm in various scenarios.  Handling datasets with undefined higher quantiles or challenging conditions could pose limitations that require careful consideration in practical applications. Incorporating robustness tests and sensitivity analysis would strengthen the algorithms applicability.  Conclusion: Overall the paper presents a significant contribution to the field of survival analysis by introducing an efficient algorithm Censored Quantile Regression Neural Network (CQRNN) that combines Portnoys censored quantile regression estimator with neural networks. The theoretical insights empirical evaluations and comparisons with existing methods demonstrate the effectiveness of CQRNN in quantile estimation on censored data. Future work could focus on addressing the identified limitations and expanding the experimental validation to encompass a wider range of datasets for comprehensive assessment.", "xuw7R0hP7G": " Peer Review  Summary: The paper presents a novel framework for establishing convergence guarantees for stochastic nonconvex optimization focusing on Gradient Flow (GF) and its implications for Stochastic Gradient Descent (SGD) and Gradient Descent (GD). It introduces the concept of admissible potentials and admissible rate functions and presents results on how these can be used to provide convergence guarantees for GDSGD algorithms. The framework is applied to classic settings like convex functions as well as more complex problems such as Phase Retrieval and Matrix squareroot. The paper provides theoretical insights into the convergence behavior of these optimization algorithms shedding light on conditions under which SGDGD can converge for complex nonconvex models.  Strengths: 1. Theoretical Framework: The paper introduces a novel theoretical framework based on admissible potentials and rate functions providing a clear and structured approach for analyzing the convergence of optimization algorithms. 2. Applicability: The application of the framework to various nonconvex problems such as Phase Retrieval and Matrix squareroot demonstrates the versatility and applicability of the proposed methodology. 3. Deep Analysis: The paper delves into the intricacies of convergence analysis providing a detailed discussion on the relationships between GF GD and SGD showcasing an indepth understanding of the optimization landscape.  Weaknesses: 1. Complexity: The papers technical depth and intricate theoretical framework may pose a challenge for readers not wellversed in advanced optimization concepts limiting the accessibility of the content. 2. Generalizability: While the framework is theoretically sound more practical examples or demonstrations of its application to realworld optimization problems could enhance the papers relevance and impact.  Questions: 1. How do the results and insights presented in the paper compare with existing methods for analyzing the convergence of optimization algorithms in nonconvex settings 2. Are there specific realworld applications or scenarios where the proposed framework has demonstrated significant practical advantages over traditional optimization analysis techniques  Limitations: 1. Accessibility: The complex mathematical formalism and theoretical nature of the paper may limit its accessibility to a broad audience potentially hindering the adoption of the proposed framework in practical applications. 2. Empirical Validation: The lack of empirical validation or case studies showcasing the utility and effectiveness of the framework in realworld optimization scenarios could limit the practical impact of the proposed methodology. Overall the paper presents a novel and sophisticated framework for analyzing the convergence of optimization algorithms in nonconvex settings. While the theoretical contributions are substantial further work on practical applicability and empirical validation could enhance the impact of the proposed methodology in realworld optimization tasks.", "sMezXGG5So": " Peer review of the paper titled \"NODEFORMER: Scalable Transformers for Node Classification\"  Summary: The paper introduces a novel NODEFORMER model that leverages a kernelized GumbelSoftmax operator for efficient message passing between arbitrary node pairs in large graphs. The model addresses deficiencies in existing Graph Neural Networks (GNNs) related to oversquashing handling longrange dependencies and the absence of input graphs. Extensive experiments demonstrate the efficacy of the proposed method in node classification on graphs with up to 2M nodes as well as in graphenhanced applications like image classification. The paper provides theoretical justifications for the design choices and showcases promising results in various tasks.  Strengths: 1. Novel Methodology: The introduction of the allpair message passing scheme using a kernelized GumbelSoftmax operator is innovative and addresses key deficiencies in existing GNNs. 2. Efficiency and Scalability: The reduction of algorithmic complexity to linear w.r.t. node numbers without compromising precision is a significant contribution. 3. Performance: The promising results in diverse experiments across different datasets and tasks validate the efficacy of the proposed approach compared to strong GNN models and stateoftheart methods. 4. Theoretical Justification: The theoretical analyses provided for the novel operator demonstrate a sound understanding of the models behavior and efficacy.  Weaknesses: 1. Complexity: The paper presents a sophisticated model and methodology which may be challenging for readers with limited background in deep learning and graph neural networks. 2. Evaluation: While the experimental results are promising a more detailed analysis of the models performance on various aspects such as interpretability robustness to hyperparameters and generalizability could enhance the papers value. 3. Comparative Analysis: The comparison with existing models and methods could be further elaborated to provide a clearer understanding of the strengths and limitations of NODEFORMER relative to other approaches. 4. Reproducibility: While the code availability is mentioned more emphasis on reproducibility transparency and ease of implementation could enhance the practical utility of the proposed model.  Questions: 1. How does the proposed NODEFORMER model compare to Transformerbased models that do not focus on graph structures 2. Could the paper provide more insights into the interpretability and explainability of the learned latent graphs in NODEFORMER 3. How does the model handle noisy or incomplete graph structures and what are the implications for realworld applications 4. Could you elaborate on the impact of different hyperparameters such as temperature and feature map dimension on the models performance and convergence 5. Are there any computational bottlenecks or potential limitations in scaling the proposed method to even larger datasets or more complex tasks  Limitations: 1. The complexity of the proposed model and the underlying theoretical analysis may pose challenges for readers unfamiliar with advanced deep learning concepts. 2. The paper could provide more insights into the models interpretability robustness and adaptability to handling noisy or incomplete input graph structures. 3. While the experimental results are promising a deeper analysis of the generalization and scalability of NODEFORMER across various datasets and tasks would add value. 4. Emphasizing reproducibility and providing a comprehensive explanation of the codebase could facilitate wider adoption and understanding of the proposed model. 5. Comparisons with a broader range of stateoftheart methods and detailed discussions on the practical implications of the model would enhance the papers contributions. Overall the paper presents an innovative approach to addressing challenges in graph neural networks and demonstrates promising results in node classification tasks. Further enhancements in reproducibility interpretability and scalability would strengthen the impact of the NODEFORMER model in practical applications.", "uOJZ_zU9qZm": "Peer Review: 1) Summary: The paper introduces Automatic Propagation (AP) software as a generalization of existing automatic differentiation frameworks to enable custom and composable construction of complex learning algorithms. This aims to overcome limitations in existing tools by simplifying the intermediate stage of implementing reusable code for new algorithms. The paper demonstrates the utility of AP through the implementation of Monte Carlo gradient estimation techniques highlighting significant improvements in gradient accuracy particularly at the edge of chaos. The experiments showcase the scalability and practicality of the AP software in replicating results from modelbased reinforcement learning. Overall the paper provides a comprehensive introduction to AP software and its potential impact on improving computational results in machine learning systems especially in complex scenarios. 2) Strengths and weaknesses: Strengths:  The paper proposes a novel concept of AP software to address limitations in current automatic differentiation frameworks allowing for more flexible and composable construction of learning algorithms.  The experiments demonstrating the utility of AP in increasing gradient estimation accuracy by orders of magnitude showcase the practical impact of the proposed framework.  The thorough explanation of the components and functioning of the Proppo software including detailed examples of propagator implementation and usage adds clarity to the concept of automatic propagation.  The papers comparison with existing related work and discussion on the advantages of AP over standard backpropagation techniques provide valuable insights for the research community. Weaknesses:  The paper could benefit from providing more concrete details on the performance metrics used in the experiments the specific settings explored and any potential limitations encountered during the implementation.  While the comparison with existing related work is informative a more indepth discussion on the limitations or drawbacks of AP software could provide a more balanced perspective on the proposed framework.  The paper could be improved by including a more extensive evaluation of the scalability of the Proppo software to handle larger and more complex machine learning tasks. 3) Questions:  How does the proposed AP software handle complex computational graphs and interdependencies between different components of a machine learning model  Can the Proppo software be seamlessly integrated with existing machine learning frameworks and libraries and are there any compatibility issues that researchers should be aware of  What are the computational overheads associated with using the AP software especially when scaling up to larger datasets and models 4) Limitations:  The experiments conducted in the paper are described as minimalistic which may not fully capture the performance and scalability of the AP software in realworld scenarios with more complex models and datasets.  The paper focuses primarily on showcasing the benefits of AP in gradient estimation techniques and further exploration of its applicability to other areas of machine learning could enhance the breadth of the research. In conclusion the paper presents a novel and promising concept with AP software providing a detailed explanation of its implementation and showcasing its utility in improving gradient estimation accuracy. With some enhancements in experimental details and scalability analysis the paper could further strengthen its contribution to the machine learning research community.", "uxWr9vEdsBh": " Peer Review  1) Summary: The paper introduces a novel approach named Active Learning via Lagrangian dualitY (ALLY) for poolbased active learning tasks. It employs a constrained learning formulation using Lagrangian duality to select a batch of diverse and informative samples for model improvement. The study demonstrates the benefits of ALLY in classification and regression tasks comparing its performance with existing methods across different datasets.  2) Strengths and weaknesses: Strengths:  The paper proposes a principled approach that integrates constrained learning and Lagrangian duality for batch active learning showcasing innovation in the field.  ALLY leverages dual variables as a measure of sample informativeness offering a unique perspective on selecting unlabeled samples.  The empirical evaluation across various classification and regression tasks demonstrates the effectiveness of ALLY showcasing competitive performance compared to stateoftheart methods.  The study explores the impact of dataset redundancy and constraint tightness on ALLYs performance adding depth to the analysis. Weaknesses:  The paper would benefit from a comparative analysis on the computational efficiency and scalability of ALLY against other methods to provide a comprehensive view for potential users.  While the generative mode to create informative samples is intriguing a more robust analysis and examples would further strengthen the findings.  Further elaboration on the limitations and challenges of implementing ALLY in realworld scenarios especially addressing practical aspects and potential roadblocks would enhance the applicability of the approach.  3) Questions:  How does the complexity of implementing ALLY compare to other active learning methods particularly in terms of computational resources and time efficiency  Could the paper delve deeper into the interpretability of the dual regression head and provide insights into how the informativeness of the samples is translated into model improvements  What are the implications of potential noise or uncertainty in the dual variables estimation on the overall performance of ALLY and how robust is the method to such variations  4) Limitations:  The paper could expand on the discussion around the practical deployment of ALLY addressing challenges such as model scalability dataset size requirements and potential issues in realworld active learning scenarios.  Extending the analysis to more diverse datasets and complex tasks could provide a broader understanding of ALLYs performance across different domains. Overall the paper presents a novel and promising method for batch active learning showcasing the benefits of incorporating Lagrangian duality and constrained learning principles. Further exploration into the practical aspects scalability and realworld implications of ALLY could enhance its utility and adoption in the active learning community.", "xUK4E1jpV7z": " Peer Review  Summary: The paper addresses the sample compressibility of concept classes in learning theory focusing on the conjecture regarding the existence of compression schemes of size O(d) for classes of VC dimension d. It simplifies and extends the proof technique introduced by Chalopin et al. (2018) to deal with extremal classes containing maximum classes of VC dimension d\u2212 1. The results show that intersectionclosed classes can be embedded into extremal intersectionclosed classes leading to unlabelled compression schemes of size linear in their VC dimension.  Strengths: 1. The paper provides a clear and thorough exploration of compression schemes for concept classes with implications for PAC learnability and generalization in adaptive data analysis. 2. The discussion and proofs are logically structured making the complex concepts more accessible to the reader. 3. The use of rigorous mathematical reasoning and Lemmas supports the validity of the results presented.  Weaknesses: 1. The paper delves into highly technical and specialized concepts in learning theory which may limit its accessibility to a general scientific audience. 2. While the proofs are detailed the extensive use of mathematical notation and terminology might be challenging for readers not wellversed in the subject area. 3. The connection between theoretical results and practical applications in machine learning or data analysis could be further elaborated to enhance the broader relevance of the findings.  Questions: 1. Could the discussed compression schemes have implications for practical machine learning algorithms or applications in data analysis 2. Are there limitations to the proposed approach when dealing with realworld datasets or complex learning scenarios 3. How do the results contribute to addressing the longstanding open problem regarding compression schemes for classes of VC dimension d  Limitations: 1. The complexity and technical nature of the concepts discussed might hinder the papers accessibility and applicability to a wider audience. 2. The focus on theoretical proofs and mathematical reasoning may limit the direct practical implications of the findings for practitioners in machine learning or related fields. 3. The generalization of the results beyond the theoretical framework presented in the paper could benefit from further contextualization or discussion of practical implications.", "uAIQymz0Qp": " Summary: The paper investigates the challenge of learning locomotion in the presence of morphological perturbations in four continuous control environments. The authors propose a Distributed Morphological Attention Policy (DMAP) architecture based on principles of sensorimotor control to address this problem. They compare DMAP with alternative agents and demonstrate that DMAP can adapt to varying body configurations without access to the perturbation information achieving performance comparable to an oracle agent. The experiments show that DMAP is effective in challenging locomotion tasks highlighting the importance of biological principles in sensorimotor learning.  Strengths: 1. Innovative Architecture: The introduction of DMAP architecture incorporating principles of sensorimotor control is a novel approach to dealing with morphological perturbations in locomotion tasks. 2. Comparison with Alternative Agents: The comparative analysis with Simple Oracle and rapid motor adaptation (RMA) agents provides a comprehensive evaluation of DMAPs performance in different environments demonstrating its effectiveness. 3. Experimental Rigor: The experiments involve testing different policy networks using a test set of unseen body configurations ensuring robust evaluation under zeroshot adaptation scenarios.  Weaknesses: 1. Complexity: The paper involves a high level of technical detail which may make it challenging for readers unfamiliar with the specific domain to fully grasp the content. 2. Lack of Realworld Application: The study is primarily focused on simulation environments limiting the direct extrapolation of findings to realworld scenarios.  Questions: 1. Generalization to Realworld Settings: How transferable are the findings and the DMAP architecture to realworld robotic systems with dynamic and unstructured environments 2. Scalability: Could the DMAP architecture be scaled for more complex locomotion tasks or diverse morphological perturbations beyond the studied environments  Limitations: 1. Simulationbased Study: The reliance on simulated environments may not fully capture the complexities and uncertainties present in realworld scenarios limiting the generalizability of the results. 2. Oracle Critic: The use of an oracle critic in training the policy introduces an external bias that might not be practical in realworld applications where complete knowledge might not be available. In conclusion the paper presents a valuable contribution in addressing the challenge of adaptive locomotion in the face of morphological perturbations. However further studies are needed to explore the scalability and applicability of the proposed DMAP architecture in realworld robotic systems.", "x-i37an3uym": "Peer Review Summary: The paper proposes a ModuleAware Optimization approach for Auxiliary Learning (MAOAL) to address the modulelevel influence in deep learning. It introduces learnable modulelevel auxiliary importance to optimize model parameters efficiently. The method is evaluated across various tasks and datasets demonstrating consistent outperformance of stateoftheart methods. From the experiments insights into the benefits of the proposed MAOAL method and the importance of considering modulelevel influence are provided. Strengths: 1. Novel Approach: The paper addresses a critical aspect of auxiliary learning by considering modulelevel influence which distinguishes it from existing methods. 2. Detailed Explanation: The paper provides a thorough explanation of the proposed approach including the lower and upper optimizations and the complete algorithm enhancing understanding. 3. Extensive Experiments: The paper conducts experiments across multiple tasks and datasets demonstrating the effectiveness of the MAOAL method and providing comprehensive results for validation. 4. Insightful Analysis: The paper offers insights into the modulelevel importance and its impact on auxiliary learning shedding light on how the method performs under different settings. Weaknesses: 1. Complexity: The method seems computationally intensive especially in scenarios with a larger number of auxiliary losses which could limit its practical application in some cases. 2. Evaluation: While the experiments are extensive additional comparative analysis with more diverse baselines and scenarios could further strengthen the evaluation of the proposed approach. 3. Generalizability: The general applicability of the MAOAL method across different architectures or tasks needs further exploration to ensure its versatility and scalability. Questions: 1. How does the MAOAL method handle scenarios where the underlying model architectures differ significantly such as in tasks requiring distinct network architectures like CNNs versus Transformers 2. Can the method adapt dynamically to changing data distributions or task priorities without the need for extensive retraining 3. How sensitive is the MAOAL method to hyperparameters and are there strategies proposed in the paper to mitigate this sensitivity effectively Limitations: 1. Computational Cost: The computational overhead associated with the proposed method could limit its practicality for largescale or realtime applications. 2. Applicability: The methods performance may vary based on the specific architecture and dataset characteristics requiring careful consideration of the usecase before application. Overall the paper presents a valuable contribution to the field of auxiliary learning by introducing a novel approach to incorporate modulelevel influence. Further research could focus on optimizing computational efficiency and expanding the methods applicability to a wider range of scenarios and architectures. Additionally addressing the questions raised above could provide a more comprehensive understanding of the MAOAL method.", "q16HXpXtjJn": " Peer Review of the Paper  1) Summary: The paper introduces a novel approach to address the distribution functional estimation problem in both online and offline settings within the context of the infinitearmed bandit problem. The paper formulates unified meta algorithms to estimate different functionals beyond the maximum like mean median trimmed mean and maximum. Utilizing Wasserstein distances the paper establishes information theoretic lower bounds and efficient algorithms for estimating the mentioned functionals. Furthermore the paper identifies a thresholding phenomenon for median estimation providing insights and contributions to the field of distributed learning.  2) Strengths and Weaknesses: Strengths:  The paper tackles a relevant and complex problem of distribution functional estimation in both online and offline settings.  The unified meta algorithms proposed for estimating different functionals provide a comprehensive and efficient approach to address the problem.  Utilizing Wasserstein distances to establish lower bounds contributes significantly to the statistical limits of the estimation problem.  The identification of the thresholding phenomenon for median estimation offers unique insights and adds novelty to the research. Weaknesses:  The clarity of the paper could be improved by providing more intuitive examples or visual aids to aid in understanding the complex concepts discussed.  The paper may benefit from a more detailed explanation of the algorithms and their implementations for readers unfamiliar with the field.  The paper could expand on the practical implications and realworld applications of the proposed algorithms to enhance the papers relevance and impact.  3) Questions:  How does the proposed unified meta algorithm handle scenarios with varying levels of noise or uncertainty in the underlying distributions  Have the proposed algorithms been tested on realworld datasets or simulations to validate their performance and efficiency  Are there any specific constraints on the distributions F considered in the lower bounds analysis and how do these assumptions impact the generalizability of the results  Do the proposed algorithms exhibit robustness to scenarios with highdimensional or complex underlying distribution structures  4) Limitations:  The paper primarily focuses on theoretical analysis and may benefit from more empirical validation to demonstrate the practical effectiveness of the proposed algorithms.  The thresholding phenomenon identified for median estimation calls for further exploration and explanation to understand the underlying mechanisms driving this behavior.  The applicability and scalability of the proposed algorithms to largescale or realtime scenarios may pose challenges not addressed in the current paper. Overall the paper presents a valuable contribution to the field of distribution functional estimation offering innovative algorithms and insights into the complexities of the infinitearmed bandit problem. Further empirical validation and exploration of the thresholding phenomenon could enhance the papers impact and practical relevance.", "qm5LpHyyOUO": "Peer Review 1) Summary: The paper introduces MCMAE a selfsupervised learning framework that enhances Vision Transformers through a combination of hybrid convolutiontransformer architectures and masked autoencoding. MCMAE shows improved performances on various vision tasks such as image classification object detection semantic segmentation and video understanding. The proposed method addresses issues related to computational efficiency and pretrainingfinetuning discrepancy commonly encountered in masked autoencoding frameworks. 2) Strengths: The paper successfully demonstrates the effectiveness of the MCMAE framework in improving the performance of Vision Transformers on a range of vision tasks. The approach of introducing hybrid convolutiontransformer architectures for masked autoencoding is novel and shows promising results. The ablation studies and extensive experiments conducted provide a thorough analysis of the proposed methods performance across different tasks such as ImageNet1K classification COCO object detection ADE20K semantic segmentation and video understanding on Kinetics400 and Somethingsomething V2 datasets. Weaknesses: One potential weakness of the paper is the lack of comparison with a wider range of stateoftheart methods beyond the selected few. Furthermore while the ablation studies provide valuable insights further discussion on the limitations of the proposed method in specific scenarios could enhance the papers comprehensiveness. Additionally the robustness of the MCMAE framework to hyperparameter variations or different dataset characteristics could be explored further to strengthen the papers conclusions. 3) Questions:  How does MCMAE perform on extremely largescale datasets compared to other methods  Can the proposed framework be easily extended to incorporate other types of selfsupervised learning objectives  What are the tradeoffs in terms of computational efficiency when scaling up the MCMAE framework to larger models or more complex architectures 4) Limitations: One limitation of the paper is the focus on a limited set of vision tasks for evaluation. Including a wider variety of tasks or datasets could provide a more comprehensive evaluation of MCMAEs capabilities. Additionally further exploration of the generalization and transferability of representations learned by MCMAE across different domains or datasets could strengthen the papers contributions. Overall the paper presents a compelling approach in enhancing Vision Transformers through the MCMAE framework showcasing significant improvements in various vision tasks. Addressing the highlighted strengths weaknesses questions and limitations can enhance the papers impact and clarity for the research community.", "xxgp42Qz6dL": " Peer Review:  1) Summary: The paper introduces Energyguided Stochastic Differential Equations (EGSDE) for unpaired imagetoimage translation (I2I) tasks aiming to improve realism and faithfulness by leveraging an energy function pretrained on both the source and target domains. EGSDE is compared against existing Scorebased Diffusion Models (SBDMs) and GANsbased methods on popular datasets showcasing superior performance in terms of FID L2 distance and human evaluation metrics. The proposed method provides a flexible tradeoff between realism and faithfulness achieving promising results on various tasks.  2) Strengths and Weaknesses: Strengths:  The paper addresses a relevant and challenging problem in computer vision proposing a novel approach that bridges the realism and faithfulness aspects in unpaired I2I tasks.  The use of an energy function pretrained on both source and target domains differentiates EGSDE from existing methods showcasing improved performance across various metrics.  Extensive experiments on popular datasets and comparison against stateoftheart methods add credibility and demonstrate the effectiveness of the proposed technique. Weaknesses:  The paper lacks a detailed analysis of the computational complexity and runtime requirements of EGSDE compared to existing approaches which can be crucial for practical deployment.  While the empirical results are promising additional insights into the interpretability of the energy function and feature extractors could enhance the understanding of the method.  The paper could benefit from a more indepth discussion on potential ethical considerations related to image generation given the rise of fake images and misuse of AIgenerated content.  3) Questions:  How sensitive is EGSDE to hyperparameter tuning and are there guidelines provided for selecting optimal values  Could the authors elaborate on the scalability and generalizability of EGSDE to datasets beyond the ones mentioned in the paper  Is there any analysis or discussion on the robustness of EGSDE to noisy or imperfect input images considering realworld scenarios  4) Limitations:  The paper lacks a thorough investigation into the interpretability of the energy function and feature extractors which could provide deeper insights into the models decisionmaking process.  The presented results are limited to specific tasks and datasets potentially hindering the generalizability of EGSDE to a wider range of image translation problems. Overall the paper presents a novel approach with promising results in the field of unpaired imagetoimage translation. Addressing the mentioned weaknesses and exploring the questions raised could further enhance the quality and impact of the research.  As a reviewer the paper shows promising advancements in unpaired image translation tasks with the introduction of EGSDE. The thorough experimental evaluation and comparison against stateoftheart methods strengthen the credibility of the proposed approach. However addressing computational complexity interpretability and generalizability aspects can further bolster the impact and relevance of the research.", "qC2BwvfaNdd": " Peer Review: 1) Summary: The paper introduces DataIQ a framework for systematically stratifying data examples into subgroups based on outcome heterogeneity in the tabular setting. The framework utilizes aleatoric uncertainty to categorize examples into Easy Ambiguous and Hard subgroups. The paper demonstrates the robustness of DataIQ in characterizing data examples across different models and its practical applications in feature acquisition dataset selection and model deployment reliability using medical datasets. 2) Strengths:  The paper addresses a significant issue of model underperformance on specific subgroups of data offering a systematic framework to identify and stratify such subgroups.  DataIQs use of aleatoric uncertainty for subgroup stratification is innovative and demonstrates robustness across different models providing consistent insights into data characteristics.  The experimental validation on realworld medical datasets highlights the utility of DataIQ in various applications including principled feature acquisition and dataset comparison.  The comparison with existing methods and the detailed experiments provide strong empirical evidence supporting the effectiveness and reliability of DataIQ. 3) Weaknesses:  The paper could benefit from a more detailed discussion on the computational complexity of the DataIQ framework especially when applied to largescale datasets.  While the experiments demonstrate the effectiveness of DataIQ a more comprehensive comparison with additional stateoftheart methods in subgroup identification could strengthen the papers contributions. 4) Questions:  How does the DataIQ framework handle imbalanced datasets and potential biases in subgroup identification  Could the paper provide further insights into the computational efficiency of DataIQ especially when scaling to larger datasets or when deployed in realtime applications  What are the potential implications of inaccurate subgroup identification by DataIQ on downstream tasks and model performance 5) Limitations:  The paper focuses mainly on the tabular setting limiting the generalizability of DataIQ to other data modalities.  The inability of DataIQ to explain the attributes responsible for subgroup characterization poses a limitation in understanding the underlying reasons for subgroup classifications.  While the experiments cover various aspects of model performance and data insights additional realworld case studies or diverse datasets could provide further validation of DataIQs utility. Overall the paper presents a wellstructured framework DataIQ addressing an important aspect of model performance under specific subgroup conditions. The rigorous experimental evaluation and comparisons with existing methods strengthen the papers contributions providing valuable insights for the machine learning research community. Further discussions on computational efficiency and generalization to other domains could enhance the papers impact and applicability.", "oDWyVsHBzNT": " Peer Review of the Scientific Paper: \"Modelbased Offline Reinforcement Learning with PessimismModulated Dynamics Belief\"  1) Summary: The paper introduces a novel approach PessimismModulated Dynamics Belief (PMDB) for modelbased offline reinforcement learning. The method maintains a belief distribution over system dynamics and optimizes policy through biased sampling from this belief. The proposed approach outperforms previous stateoftheart methods on a wide range of benchmark tasks. The work combines theoretical formulations with empirical results to demonstrate the effectiveness of PMDB in learning policies from offline datasets.  2) Strengths and Weaknesses:  Strengths:  The paper addresses the limitations of existing modelbased RL methods and proposes a novel PMDB approach to improve policy learning.  The method is wellmotivated and supported by theoretical proofs which enhance the credibility of the proposed framework.  Empirical results demonstrate superior performance compared to previous stateoftheart methods across various benchmark tasks.  Weaknesses:  The paper could provide more detailed insights into the computational complexity of the proposed algorithm especially when dealing with largescale datasets or complex environments.  The limitations of the PMDB approach such as sensitivity to hyperparameters or potential challenges in realworld applicability could be further explored and discussed.  3) Questions:  Can the PMDB approach handle highdimensional state and action spaces efficiently  How does the bias towards pessimism in sampling affect the robustness and stability of the learned policies in uncertain or noisy environments  What are the potential implications of the monotonicity properties observed in the theoretical analyses on the practical performance of the PMDB algorithm  4) Limitations:  The study could benefit from a more comprehensive analysis of the impact of hyperparameters on the performance of the PMDB approach particularly in terms of generalizability to diverse environments.  The paper lacks a discussion on the scalability and generalization capabilities of PMDB to realworld applications where dynamics prior knowledge may be limited or uncertain. In conclusion the paper presents an innovative and promising method for modelbased offline RL with the PMDB approach. While the theoretical foundations and empirical results are strong further investigations into the scalability robustness and adaptability of PMDB in practical scenarios could enhance the impact and applicability of this work in the field of reinforcement learning.", "xvLWypz8p8": " Peer Review:  1) Summary: The paper investigates the generalization properties of majority voting on finite ensembles of classifiers by providing marginbased generalization bounds through PACBayes theory. The study contrasts with previous works by applying bounds to nonrandomized votes using margins. The research adds perspective to the margins theory proposed by Schapire et al. (1998) for ensemble classifier generalization. The contributions leverage recent Dirichlet posteriors in training voting classifiers. The paper presents theoretical results and empirical evaluations showcasing favorable performance on various classification tasks.  2) Strengths and weaknesses: Strengths:  The paper presents a novel approach combining margin bounds and the use of Dirichlet majority votes to derive a new margin bound for nonrandomized majority votes.  The study provides theoretical results supported by empirical evaluations showcasing the effectiveness of the proposed method.  The comparison with existing margin and PACBayes bounds demonstrates the superior performance of the new approach in terms of tightness and applicability.  The discussion of strong and weak voters optimization strategies and datasets used in the evaluation adds depth to the research. Weaknesses:  The complexity of the mathematical formalism and technical details might make it challenging for readers not wellversed in the field to grasp the core contributions easily.  The paper could benefit from more visual aids to help visualize the results and make it more accessible to a wider audience.  The limitations associated with the practical implications of the proposed method in realworld applications are not extensively discussed.  While the empirical evaluations are robust a comparison with more advanced machine learning models could further strengthen the validity of the results.  3) Questions:  How do the proposed margin bounds compare to other stateoftheart machine learning algorithms beyond ensemble classifiers  What implications do the results have for practical applications of majority voting in realworld scenarios especially in industries such as finance healthcare or security  Can the method be extended to handle more complex voting schemes or diverse types of base classifiers while maintaining the same level of performance and applicability  How sensitive is the performance of the proposed method to variations in hyperparameters especially in complex datasets with high dimensionality and diverse class distributions  Are there any plans to validate the proposed approach with larger more diverse datasets to assess its scalability and robustness across a wider range of applications  4) Limitations:  The study primarily focuses on the theoretical aspects of majority voting ensemble classifiers which could limit the generalizability of the findings to other domains or machine learning paradigms.  The practical utility of the proposed margin bounds and their impact on realworld machine learning tasks may require further validation and experimentation in industryspecific applications.  The generalization of the results to handle highly imbalanced datasets noisy data or nonstationary environments is not explicitly discussed in the paper indicating potential limitations in scenarios with such complexities.", "prQkA_NjuuB": "Peer Review Summary: The paper investigates the parameterization of deep neural networks to satisfy the continuity equation by building divergencefree neural networks through the concept of differential forms. The authors propose two practical constructions to realize this proving the models universality and demonstrating their application in computational fluid dynamics and dynamical optimal transport maps. The paper is wellstructured providing a detailed explanation of the theoretical framework and experimental validation. Strengths: 1. Originality: The paper introduces a novel approach to directly bake in constraints into deep neural networks focusing on divergencefree vector fields and the continuity equation. 2. Theoretical Foundation: The paper is wellsupported by a robust theoretical framework utilizing concepts from differential forms to develop practical constructions for parameterizing divergencefree vector fields. 3. Practical Applications: The experimental validation of the proposed methods in fluid dynamics and dynamical optimal transport maps showcases the practical utility of the developed models. 4. Clarity: The paper is wellwritten and structured making it accessible to readers with a background in machine learning physics and mathematics. Weaknesses: 1. Complexity: The theoretical derivations and mathematical formulations might be challenging for readers without a strong background in differential forms and computational fluid dynamics. 2. Scalability Concerns: The authors mention scalability issues due to the extensive use of automatic differentiation for computing divergence of neural networks suggesting future work to address these limitations. Questions: 1. How does the proposed approach compare to existing methods that enforce constraints in deep neural networks in terms of efficiency and accuracy 2. Have the authors considered the potential impact of noise or uncertainties in the input data on the performance of the divergencefree neural networks Limitations: 1. Generalizability: The applicability of the proposed methodology beyond the specific examples presented in fluid dynamics and optimal transport maps needs further exploration. 2. Computational Complexity: The potential computational overhead of the proposed modeling tools especially in scenarios with highdimensional data could be a limiting factor for practical implementation. In conclusion the paper presents an innovative approach to enforcing constraints in deep neural networks through the parameterization of divergencefree vector fields. Despite some complexity and scalability concerns the paper provides a significant contribution to the field of machine learning and computational physics. Further research to address scalability issues and explore broader applications of the proposed methodology would enhance the impact of this work.", "uV_VYGB3FCi": " Peer Review  1) Summary The paper proposes a novel approach called Code Editing for neural image compression (NIC) based on semiamortized inference and adaptive quantization. The method aims to achieve flexible variable bitrate control ROI coding and multidistortion tradeoff with a single decoder surpassing existing methods in RD performance. Experimental results demonstrate the effectiveness of Code Editing in achieving continuous bitrate control and improved performance compared to other variablerate NIC methods.  2) Strengths and Weaknesses Strengths:  The paper addresses a significant challenge in NIC by proposing Code Editing for flexible bitrate control.  The experimental results show improved performance compared to existing variablerate methods.  Code Editing can achieve ROI coding and multidistortion tradeoff with a single decoder offering versatility in image compression. Weaknesses:  The paper may benefit from clearer explanations in some sections especially regarding the technical implementations of Code Editing and its comparison to existing methods.  The discussion on limitations is somewhat brief and could be expanded to provide a more comprehensive understanding of potential drawbacks and future directions.  3) Questions  Can you elaborate on the specific advantages of Code Editing over traditional codecs in terms of computational complexity and efficiency  How does Code Editing address the issue of bitrate control for images with diverse levels of complexity or visual content  Is the proposed method robust to variations in image characteristics such as texture complexity color distribution or object shapes  4) Limitations  The study mainly focuses on scalar quantization step sizes \\xe2\\x88\\x86. How might the performance of Code Editing be affected by the use of vector \\xe2\\x88\\x86 for different regions of an image  The discussion on the efficiency of Code Editing for realtime applications is valuable. Could you provide insights into potential strategies to enhance the efficiency of the method for realtime deployment In conclusion the paper introduces an innovative approach to address the challenges of flexible bitrate control in neural image compression. The experimental results showcase the potential of Code Editing but further clarification on technical details and thorough exploration of limitations and future research directions would enhance the papers impact and relevance to the field of image compression.", "yjybfsIUdNu": "Peer Review: Summary: The paper explores reinforcement learning (RL) in the presence of low and highfidelity environments focusing on improving agent performance using multifidelity data. A multifidelity estimator based on control variates is proposed to reduce variance in estimating stateaction value functions. The proposed MFMCRL algorithm is shown to outperform standard RL algorithms in various scenarios including synthetic MDPs and NAS. The theoretical analysis numerical experiments and discussions on broader impacts are comprehensive. Strengths: 1. The paper addresses an important area of study by integrating multifidelity data into reinforcement learning which has practical application implications in computational science and engineering. 2. The theoretical analysis provides a strong foundation for understanding the proposed multifidelity estimator and its impact on RL performance. 3. The experimental results on both synthetic MDPs and NAS cases showcase the effectiveness of the proposed MFMCRL algorithm. 4. The use of code and data sharing along with comprehensive training details and experimental setups allows for reproducibility of results. 5. Discussion on broader impacts ethical considerations and societal implications is included enhancing the papers accountability. Weaknesses: 1. While the experimental results are promising further analysis on the scalability and robustness of the proposed algorithm across a wider range of RL tasks could provide broader insights. 2. Including additional realworld applications or benchmarks where multifidelity RL can be beneficial would further validate the practical utility of the proposed approach. 3. The paper could further elaborate on potential extensions or future works to address current limitations and open research questions. Questions: 1. How scalable is the proposed MFMCRL algorithm when applied to more complex RL environments or tasks compared to traditional RL algorithms 2. Can the proposed multifidelity RL framework be extended to address continuous stateaction space RL problems 3. How does the proposed approach handle cases where the low and highfidelity environments exhibit significant divergence in correlations Limitations: 1. The paper primarily focuses on theoretical analysis limited to episodic RL problems and may not fully generalize to other RL settings. 2. The experiments are confined to synthetic MDPs and NAS potentially limiting the realworld applicability of the proposed MFMCRL algorithm. 3. The studys impact might be constrained by the specific scenarios chosen for investigation emphasizing the importance of further validation in diverse RL environments.", "nJWcpq2fco3": " Peer Review:  1) Summary: The paper introduces a representation learning framework for spatial trajectories focusing on handling challenges like partial observations continuous nature and uncertainty. The framework uses a learned latent space to represent trajectories as probability distributions enabling tasks like prediction and interpolation. Experimental results demonstrate superiority over baselines showcasing the methods flexibility and accuracy.  2) Strengths and weaknesses: Strengths:  The paper addresses a significant problem in representation learning for spatial trajectories.  The frameworks flexibility in handling various input types and tasks is commendable.  Experimental results demonstrate the methods advantage over existing baselines.  The use of probability distributions in the learned latent space is a novel and effective approach. Weaknesses:  The paper could benefit from a more detailed comparison with existing approaches especially in terms of computational efficiency and scalability.  Clarifications on the practical implications of the proposed method in realworld scenarios could enhance the papers impact.  The importance of the experimental datasets and their relevance to realworld applications should be discussed more explicitly.  3) Questions:  How does the proposed framework perform when dealing with noisy or incomplete input trajectories  Can the framework be adapted to handle dynamic environments where trajectory patterns evolve over time  How does the method scale with larger and more complex trajectory datasets  4) Limitations:  The paper introduces a strong representation learning framework but more indepth analysis of the models interpretability and generalizability would strengthen the findings.  The applicability of the method to domains beyond human movement datasets should be explored to demonstrate its broader utility.  Further discussion on potential limitations or failure modes of the framework could provide insights into areas for future research. In conclusion the paper presents a promising representation learning framework for spatial trajectories demonstrating its effectiveness through experiments. Addressing the raised questions and limitations could enhance the papers impact and pave the way for future advancements in trajectory modeling.", "s0AgNH86p8": " Peer Review:  1) Summary: The paper introduces TransBoost a novel transductive learning procedure for finetuning deep neural models to optimize their performance on a specific (unlabeled) test set provided at training time. The method is based on a large margin principle and is shown to significantly improve ImageNet classification across various architectures leading to stateoftheart transductive performance.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem of deep transductive learning and proposes a novel and effective procedure TransBoost.  The method is wellexplained and the algorithmic details are clearly presented enhancing reproducibility.  Extensive experiments are conducted across various datasets and architectures demonstrating consistently significant performance improvements.  The study provides insightful comparisons with existing methods such as SSL and tFSL showing superiority in transductive performance. Weaknesses:  The time complexity of the proposed TransBoost loss component for large test sets could be a limitation and future work could focus on optimizing this aspect.  The study lacks detailed analysis on the impact of hyperparameters on the performance of TransBoost which could provide additional insights for optimizing the method.  While the proposed implementation of S \u03b4 and \u03ba is straightforward exploring more advanced implementations of these functions could be beneficial for further improving performance.  3) Questions:  What considerations were taken into account while selecting the hyperparameters for the TransBoost procedure and how sensitive is the methods performance to variations in these hyperparameters  The paper proposes simple instantiations of the functions S \u03b4 and \u03ba for implementing the TransBoost loss component. Have there been any explorations into more sophisticated methods for these functions and how might they impact the performance of TransBoost  The study demonstrates impressive performance gains with TransBoost across various architectures. Are there particular characteristics or design aspects of certain architectures that make them more suitable for transductive learning with TransBoost  4) Limitations:  The computational complexity of the TransBoost loss component for large test sets might restrict its scalability to realworld applications with extensive datasets.  The lack of indepth analysis on hyperparameter sensitivity and exploration of more advanced instantiations of functions could limit the understanding of potential improvements in the methods performance.  The paper mainly focuses on image classification tasks exploring the applicability and performance of TransBoost in other domains such as natural language processing (NLP) could broaden the scope of the study. Overall the paper presents an innovative approach to deep transductive learning with significant empirical results. Addressing the questions and limitations raised could further enhance the clarity and impact of the research findings.", "wgRQ1IM4g_w": " Summary The paper presents a study on stochastic bandit optimization with general unknown reward and constraint functions allowing violations of soft constraints. The authors propose a framework leveraging primaldual optimization for algorithm design and analysis demonstrating sublinear regret and constraint violation under various exploration strategies. Experimental results on synthetic and realworld datasets compare the proposed algorithms with existing methods highlighting the tradeoff between regret complexity and constraint violation in constrained kernelized bandits.  Strengths and Weaknesses Strengths: 1. The paper addresses an important problem of constrained bandit optimization with soft constraints providing theoretical analysis and practical considerations. 2. The proposed framework offers a unified approach for algorithm design and performance analysis under various exploration strategies. 3. The comparison with existing methods and experimental results on datasets demonstrate the effectiveness of the proposed algorithms. 4. The theoretical results sufficient condition and algorithm modifications are wellstructured and provide insights into achieving sublinear regret and constraint violation. Weaknesses: 1. The paper can be improved by providing clearer explanations of the key concepts and methods used especially for readers less familiar with bandit optimization and optimization techniques. 2. The practical considerations and implications of the proposed algorithms could be further detailed to enhance understanding of their realworld applicability. 3. The limitations and implications of the theoretical assumptions made (such as the noisy reward feedback model) should be discussed to provide a broader perspective on the applicability of the proposed framework.  Questions 1. How do the proposed algorithms under the framework perform in scenarios with varying degrees of complexity in reward and constraint functions such as nonsmooth functions or functions with unknown structure 2. Can the framework be extended to handle dynamic constraints that evolve over time or change based on the learning agents actions in a sequential decisionmaking setting 3. How sensitive are the proposed algorithms to hyperparameters such as the regularization parameter \\xe2\\x87\\xa2 and the choice of exploration strategies and how do these impact the tradeoff between regret and constraint violation 4. Are there insights from the experimental results that provide guidance on the generalizability of the proposed algorithms to diverse realworld applications beyond the specific datasets considered in the study  Limitations 1. The study relies on assumptions of subGaussian noise fixed functions in RKHS and Bayesiantype frequentist settings limiting the generalizability to broader contexts with different noise distributions or nonfixed functions. 2. The solutions for practical implementation on continuous domains may need further exploration especially in scenarios with highdimensional or complex action spaces. 3. The paper could benefit from a more detailed discussion on the computational efficiency scalability and convergence properties of the proposed algorithms especially when applied to largescale or dynamic optimization problems.", "tzNWhvOomsK": " Peer Review 1) Summary: The paper presents a novel theoretical analysis of zeroshot learning with attributes focusing on the lower bound of the best achievable error given the classattribute matrix. The authors propose a rigorous mathematical framework to quantify the intrinsic difficulty of zeroshot learning based on available information. They demonstrate the lower bounds tightness and show how it can be predictive of ZSL methods performance. Empirical experiments validate the theory on popular attributebased ZSL models showing the lower bounds efficacy in predicting misclassification errors. Overall the work aims to provide a foundational understanding of ZSL by quantifying information obtained from the classattribute matrix. 2) Strengths and weaknesses: Strengths:  The paper addresses an important theoretical aspect of zeroshot learning offering insights into the intrinsic difficulty of the problem.  The development of a lower bound based on the classattribute matrix provides a novel contribution that can guide model development and evaluation.  The theoretical framework is supported by empirical experiments enhancing the papers practical relevance and validity.  The proposed adversarial optimization approach and linear program formulation are mathematically sound and offer efficient methods for computing the lower bound. Weaknesses:  The computational complexity of the linear program in high dimensions may limit practical implementation for datasets with a large number of attributes.  The paper could benefit from more detailed explanations of the experimental setup results and datasets used to enhance reproducibility and clarity.  The discussion on limitations and future work could be expanded to provide insights into potential extensions of the proposed framework and its applicability to other ZSL variants. 3) Questions:  How does the lower bound analysis account for noise or inaccuracies in the classattribute matrix given that realworld data may not align perfectly with ideal assumptions  Could the framework be extended to incorporate additional sources of information beyond the classattribute matrix such as textual descriptions or semantic embeddings to provide a more comprehensive analysis of ZSL methods  Which aspects of the theoretical framework would benefit from further validation or refinement through more extensive empirical studies on diverse datasets 4) Limitations:  The computational complexity of the linear program formulation for large numbers of attributes may restrict its practical utility in realworld ZSL applications.  The empirical validation is limited to a specific set of ZSL models and datasets potentially limiting the generalizability of the findings to diverse ZSL scenarios.  The paper focuses predominantly on the lower bound analysis while additional insights into potential upper bounds or optimization strategies for improving ZSL performance could further enhance the theoretical contributions. In conclusion the paper presents a significant contribution to the theoretical understanding of zeroshot learning with attributes offering a novel perspective on the intrinsic difficulty of the problem. The proposed lower bound analysis provides valuable insights for guiding future research in ZSL and highlights the importance of quantifying information extracted from auxiliary data sources. Further work on extending the framework addressing practical limitations and validating findings on diverse datasets would enhance the impact and applicability of the proposed theoretical analysis.", "wOUH1VQ9Rcj": " Peer Review of the Scientific Paper titled \"Transformed Independent Noise for Causal Discovery under Measurement Error\"  1) Summary: The paper introduces the Transformed Independent Noise (TIN) condition for causal discovery under measurement error in the context of a linear nonGaussian acyclic model. By leveraging the nonGaussianity and higherorder statistics of data the TIN condition aims to identify the causal structure among unobserved target variables. The proposed method utilizes independence tests to achieve what previously required computationally intensive methods like overcomplete independent component analysis (OICA). Experimental results on synthetic and realworld datasets demonstrate the effectiveness and reliability of the TIN method.  2) Strengths and Weaknesses: Strengths:  Introduces a novel Transformed Independent Noise (TIN) condition for causal discovery under measurement error.  Utilizes the nonGaussianity and higherorder statistics of data to identify causal structures among unobserved variables.  Achieves identifiability of ordered group decomposition without relying on computationally heavy methods like overcomplete independent component analysis.  Demonstrates effectiveness and reliability of the method through experiments on synthetic and realworld datasets. Weaknesses:  While the method shows promising results in identifying causal structures under measurement error its scalability to larger datasets or more complex scenarios needs to be further investigated.  The paper focuses on explaining theoretical concepts and the proposed method more emphasis on practical implementation details and application scenarios would enhance the paper.  The limitation of using simulated datasets calls for validation on more diverse and realworld datasets to generalize the methods applicability.  3) Questions:  How does the proposed Transformed Independent Noise (TIN) condition address the challenges posed by measurement errors in causal discovery compared to existing methods  Can the TIN method handle scenarios with a larger number of variables and more complex causal structures effectively  Are there specific assumptions or limitations that need to be considered when applying the TIN method to realworld datasets with varying characteristics  4) Limitations:  The paper focuses primarily on theoretical explanations and limited realworld application scenarios which may limit the practical understanding and usability of the proposed method.  The reliance on simulated datasets for evaluation might not fully capture the complexity and variability of realworld scenarios warranting further validation on diverse datasets. Overall the paper makes a significant contribution to the field of causal discovery under measurement error particularly with the introduction of the Transformed Independent Noise (TIN) condition. However further exploration into practical implementation scalability and validation on diverse datasets would enhance the impact and applicability of the proposed method.", "sRKNkpUMQNr": " Peer Review:  Summary: The paper presents an informationtheoretic knowledge distillation approach for compressing generative adversarial networks (GANs). The proposed method aims to maximize mutual information between teacher and student networks by optimizing a variational lower bound of the mutual information using an energybased model. The energybased model is utilized for a flexible variational distribution representation particularly effective for handling highdimensional data like images. The approach demonstrates promising results in compressing GAN models while maintaining performance across various datasets and architectures.  Strengths and Weaknesses:  Strengths: 1. The proposed method addresses the challenge of compressing GANs by maximizing mutual information between teacher and student networks. 2. The incorporation of an energybased model for variational distribution provides flexibility and improved performance in model compression. 3. The method achieves consistent performance improvements across different GAN compression tasks and datasets. 4. The paper provides detailed experimentation and evaluation metrics to validate the effectiveness of the proposed approach.  Weaknesses: 1. The paper lacks a detailed comparison with other stateoftheart compression methods outside the specific GAN compression realm. 2. The complexity of the proposed method particularly the optimization of the energybased model and the iterative procedures could make implementation and understanding challenging for some researchers.  Questions: 1. Could the authors provide more insights into the computational overhead introduced by the adoption of the proposed energybased model compared to existing compression methods 2. How generalizable is the proposed approach beyond GAN compression tasks Are there potential applications in other domains or network architectures 3. It would be beneficial to understand how the proposed method scales with larger datasets and more complex GAN models. Have the authors tested the approach on extremely largescale datasets  Limitations: 1. The paper lacks a deep dive into the interpretability of the energybased model and how it impacts the knowledge distillation process. 2. The necessity for Langevin dynamics and MCMC sampling for training the energybased model might introduce additional computational burdens and clarification on this aspect would be useful. 3. The evaluation metrics are focused on quantitative assessments like FID and mIoU more qualitative analysis or user studies could provide a richer perspective on the methods efficacy. Overall the paper presents a novel approach to GAN compression using an energybased model for variational distribution showcasing promising results across multiple datasets and GAN architectures. However further clarification on practical implications scalability and comparisons with diverse compression techniques would enhance the paper.", "uRSvcqwOm0": "Peer Review 1. Summary: The paper introduces a novel causal model for categorical data titled Classification with Optimal Label Permutation (COLP). The proposed method aims to identify causal relationships between categorical variables efficiently by incorporating an automatic category ordering mechanism. The paper demonstrates the effectiveness of the COLPbased causal model through experiments with synthetic and real data showcasing its superiority over existing methods. The paper also provides theoretical justifications and a learning algorithm for causal discovery using COLP. 2. Strengths and weaknesses: Strengths:  The paper addresses an important gap in causal discovery by focusing on categorical data which is less studied compared to quantitative data.  The proposed COLP model is innovative and introduces a parsimonious approach to identifying causal relationships in categorical data.  The theoretical foundation provided for the identifiability of the proposed model is a significant contribution.  The experimental results demonstrate the superior performance of COLP over existing methods in both synthetic and real data settings. Weaknesses:  The paper could benefit from additional discussion on the limitations of the proposed method and potential areas for future research.  The realworld applications of the COLP model could be further explored to showcase its practical utility beyond experimental settings.  More detailed comparisons with other stateoftheart methods and discussions on the scalability of the proposed algorithm could enhance the papers impact. 3. Questions:  How does the performance of the COLP model vary with the complexity of the categorical variables (e.g. number of categories)  Can the proposed approach handle cases where there are multiple causal factors influencing the outcome in complex causal networks  How robust is the COLP model to noisy or incomplete categorical data in realworld scenarios  In what ways can the proposed method be extended to handle multivariate causal discovery problems involving categorical variables 4. Limitations:  The paper focuses mainly on bivariate causal discovery limiting the exploration of multivariate causal relationships that may exist in realworld scenarios.  The identifiability theory assumes no unmeasured confounders which may not hold true in many practical situations.  The scalability and computational efficiency of the proposed COLP model when dealing with largescale categorical datasets need further investigation. Overall the paper presents a novel and promising approach to causal discovery in categorical data. The theoretical foundations experimental validations and practical implications of the proposed COLP model make it a significant contribution to the field of causal inference. Addressing the identified weaknesses and limitations could further strengthen the impact and applicability of the proposed method.", "yHFATHaIDN": " Peer Review:  Summary: The paper introduces a novel algorithm MABSplit designed to accelerate the training process of random forests and other treebased learning methods by improving the nodesplitting subroutine. This algorithm is based on the principles of multiarmed bandits and aims to reduce the computational complexity of split point identification. The study provides theoretical guarantees and experimental results demonstrating significant improvement in training speed without compromising generalization performance across various forestbased variants. MABSplit outperforms existing methods in terms of generalization performance and feature importance calculations under fixed computational budgets.  Strengths: 1. Innovative Algorithm: The introduction of MABSplit as a novel algorithm for improving the nodesplitting process in treebased learning models is a significant contribution to the field of machine learning. 2. Theoretical Guarantees: The paper provides theoretical guarantees about the improvement brought by MABSplit in terms of sample complexity leading to a reduction in training time. 3. Experimental Validation: The experimental results showcase the effectiveness of MABSplit by demonstrating substantial improvements in training speed with comparable generalization performance. 4. Reproducible Methodology: The availability of code on GitHub for reproducibility enhances the transparency and credibility of the study.  Weaknesses: 1. Lack of Comparison with StateoftheArt: While the paper discusses improvements over existing methods a direct comparison with stateoftheart algorithms or implementations like scikitlearn could have provided more context and a deeper understanding. 2. Theoretical Assumptions: The paper relies on certain assumptions for the theoretical guarantees and a detailed discussion on the sensitivity of these assumptions or their applicability to realworld scenarios would have enhanced the completeness of the study. 3. Discussion on Scalability: While the algorithms efficiency in reducing the computational complexity is highlighted a deeper discussion on its scalability to larger datasets or scenarios with high dimensionality could provide valuable insights.  Questions: 1. What are the specific criteria used to benchmark the generalization performance of the forestbased variants with MABSplit against the baseline methods 2. How does the adaptive estimation procedure in MABSplit address the potential biasvariance tradeoff in feature selection or nodesplitting 3. Could the authors elaborate on the practical implications of incorporating MABSplit into realworld applications that involve training models on massive datasets or resourceconstrained environments  Limitations: 1. Limited Dataset Variability: The evaluation of MABSplit across various datasets is crucial and the paper should include a broader range of datasets to demonstrate the algorithms effectiveness under different scenarios. 2. Realworld Application Scenarios: While the study showcases improvements in speed and performance the practical implications and realworld applications of MABSplit could be further emphasized to illustrate its relevance beyond experimental settings. 3. Scalability Concerns: Addressing how MABSplit scales with respect to dataset size and dimensionality especially in scenarios with extremely large datasets could provide more insights into the algorithms practical utility. Overall the paper presents a novel algorithm that shows promising results in accelerating the training of treebased models. However addressing the limitations and providing deeper insights into practical applications and scalability could further enhance the studys impact and relevance in the machine learning community.", "yLilJ1vZgMe": " Peer Review:  1) Summary: The paper introduces methods to efficiently compute neural kernels for general activations addressing limitations in applying kernel methods to largescale learning settings. The authors propose a fast sketching method to approximate neural network Gaussian Process (NNGP) kernel and Neural Tangent Kernel (NTK) matrices for a wide range of activation functions going beyond the commonly analyzed ReLU activation. The method leverages dual kernel approximations through Hermite expansions and provides subspace embeddings for NNGP and NTK matrices. Empirical results demonstrate significant speedups in approximating CNTK for a 5layer neural network on the CIFAR10 dataset.  2) Strengths and weaknesses: Strengths:  Comprehensive exploration of neural kernel computation for general activations providing theoretical foundations and efficient computational methods.  Novel approach to approximate NNGP and NTK matrices extending beyond commonly used activation functions.  Theoretical derivations are detailed and wellsupported enhancing the understanding of the proposed methods.  Empirical results demonstrate substantial speedups in computation without sacrificing accuracy. Weaknesses:  The paper could benefit from more detailed comparisons with existing methods for computing neural kernels particularly in terms of accuracy and efficiency on various datasets.  The limitations of the proposed sketching method and potential failure cases or scenarios where it may not be applicable could be further explored.  3) Questions:  How sensitive is the proposed method to the choice of polynomial degree q in approximating the activation functions Have you performed sensitivity analyses to assess the impact of different polynomial degrees on the accuracy of the approximations  Can you provide more insights into the practical implications of the proposed subspace embedding method particularly in scenarios with highdimensional datasets or complex neural network architectures  How generalizable is the sketching algorithm proposed for approximating NNGP and NTK matrices to different datasets and network structures beyond the examples provided in the experiments  4) Limitations:  The paper focuses on demonstrating the effectiveness of the proposed methods through empirical evaluations on specific datasets and activations. The generalization to a broader range of datasets network architectures and practical applications could be further investigated.  The scalability of the sketching algorithm to extremely large datasets or more complex architectures might be challenging and its limitations under such scenarios should be elucidated. Overall the paper presents a novel and promising approach to efficiently compute neural kernels for a wide range of activation functions addressing key limitations in largescale learning settings. Further investigations into the practical implications and generalizability of the proposed methods will enhance the impact and applicability of the research findings.", "wlqb_RfSrKh": "Abstract The paper presents a novel framework Selfsupervised Amodal Video Object Segmentation (SaVos) for amodal object segmentation. The framework leverages temporal sequences to infer amodal masks of objects through selfsupervised learning. SaVos incorporates spatiotemporal information to achieve stateoftheart performance on synthetic and realworld benchmarks demonstrating effectiveness in amodal segmentation tasks and generalization to unseen types of objects. Strengths 1. Innovative Framework: SaVos introduces a unique approach that combines selfsupervised learning and spatiotemporal priors for amodal segmentation addressing challenges in inferring occluded object parts. 2. Performance: The framework achieves stateoftheart results on FISHBOWL and KINSVideoCar benchmarks demonstrating superior performance in amodal segmentation tasks. 3. Generalization: SaVos shows strong generalization performance especially with testtime adaptation indicating adaptability to distribution shifts and unseen object types. 4. Robustness: The bidirectional prediction approach helps in overcoming the cold start problem ensuring informative predictions even in initial frames. Weaknesses 1. Complexity: The frameworks architecture and methodology may be intricate potentially limiting its ease of implementation and understanding for users without advanced technical expertise. 2. Dependencies: SaVos heavily relies on selfsupervised learning and spatiotemporal priors which may introduce challenges in scenarios with limited or lowquality annotation data. 3. Evaluation on Realworld Scenarios: While performance on synthetic datasets is strong further validation on diverse realworld scenarios with varying complexities could provide a more comprehensive assessment. Questions 1. Scalability: How does the proposed framework scale when applied to larger datasets or scenarios with a higher number of object types 2. Model Interpretability: How easily can the models predictions be interpreted especially in scenarios where visual cues may not directly align with human perceptions 3. Efficiency: What computational resources are required for training and implementing the framework particularly in applications demanding realtime processing 4. Algorithm Robustness: How does SaVos handle noisy data or inaccuracies in annotation and what measures are in place to enhance robustness in practical applications 5. Comparison with Existing Models: How does SaVos compare with traditional methods and existing deep learning models in terms of computational efficiency and performance in amodal segmentation tasks Limitations 1. Training Data: The reliance on highquality annotated data for training might be a limitation especially in scenarios where amodal labels are scarce or difficult to obtain. 2. Generalization to Unseen Types: While the framework shows promising generalization abilities further validation on a broader range of unseen object types is necessary to assess its true robustness. 3. Realworld Applicability: Additional validation in complex realworld settings such as autonomous driving or robotics applications could provide more insights into the practical utility of SaVos. Overall the paper presents a novel and effective framework for selfsupervised amodal segmentation with notable strengths in performance and generalization. Further exploration of scalability interpretability efficiency and robustness would enhance the understanding and applicability of SaVos in diverse scenarios.", "rwyISFoSmXd": " Peer Review of \"Conformal Fair Quantile Prediction Framework for Algorithmic Fairness\"  Summary: The paper proposes a novel framework for quantile fairness in machine learning focusing on deriving reliable fair prediction intervals under the fairness requirement of Demographic Parity. The authors introduce the Conformal Fair Quantile Prediction (CFQP) approach which includes the use of Wasserstein2 barycenters and kernel smoothing techniques to ensure distributionfree coverage and exact fairness. The proposed method is evaluated on benchmark datasets and compared to existing fairnessoriented techniques showcasing superior empirical performance. Theoretical guarantees are provided for both prediction coverage and demographic parity.  Strengths and weaknesses: Strengths: 1. Novel Contribution: The paper introduces a unique framework for ensuring quantile fairness in machine learning which addresses a significant gap in the existing literature. 2. Theoretical Guarantees: The authors provide rigorous theoretical guarantees for both prediction coverage and demographic parity enhancing the credibility and reliability of the proposed method. 3. Empirical Evaluation: The empirical results demonstrate the effectiveness of the CFQP approach in reducing discriminatory bias and maintaining reasonable prediction interval lengths. 4. Detailed Methodological Explanation: The paper offers a comprehensive explanation of the proposed method including algorithms and mathematical formulations aiding in understanding and implementation. Weaknesses: 1. Experimental Validation: While the empirical results are promising additional experiments on diverse datasets with varying characteristics would strengthen the generalizability of the proposed method. 2. Comparison to existing methods: The paper lacks a detailed comparison with existing fairnessoriented techniques in terms of quantitative metrics making it challenging to assess the relative performance of the proposed method. 3. Scalability and Efficiency: The computational complexity and scalability of the proposed method particularly on large datasets should be addressed to ensure practical utility.  Questions: 1. Generalization: How does the proposed CFQP approach perform on highly imbalanced datasets or in scenarios where the sensitive attributes have low representation 2. Practical Implementation: Are there specific considerations or challenges in implementing the CFQP approach in realworld applications 3. Extension to Other Domains: Have you considered applying the CFQP method to domains beyond societal and medical applications such as finance or marketing 4. Interpretability: How does the CFQP approach ensure interpretability and transparency in the fairness adjustments made to the quantile predictions  Limitations: 1. Dataset Dependency: The effectiveness of the CFQP method might be influenced by the characteristics of the datasets used raising concerns about generalizability to diverse realworld scenarios. 2. Complexity: The complexity of the proposed method especially in terms of computational resources and time may limit its scalability to largescale applications. Overall the paper presents a welldeveloped framework for achieving quantile fairness in machine learning backed by strong theoretical foundations and empirical evidence. Addressing the noted weaknesses and limitations would further enhance the significance and practical utility of the proposed CFQP method.", "pl279jU4GOu": "Peer Review: 1) Summary: The paper presents a new strategy using Kurdyka\\xc5\\x81ojasiewicz inequalities to prove convergence of gradient flows to zero loss in various deep learning architectures. The analysis focuses on Rayleigh quotients to achieve convergence even in scenarios where the neural network is not overparameterized extending the proof to scenarios with finite parameters and infinite data. The paper provides insights into the convergence of models under different conditions showcasing the importance of functional properties in neural network convergence analysis. 2) Strengths and weaknesses: Strengths:  The paper introduces a novel strategy using Kurdyka\\xc5\\x81ojasiewicz inequalities for proving convergence in deep learning architectures.  The analysis incorporates Rayleigh quotients to extend convergence proofs to scenarios beyond overparameterized regimes.  The paper provides mathematical rigor in proving convergence guarantees for various examples of parametric learning. Weaknesses:  The paper may be challenging for readers without a strong mathematical background due to the complex mathematical formulations and proofs.  While the paper presents robust theoretical foundations practical implications and applications in realworld deep learning scenarios could have been explored further to enhance relevance.  The implications of the findings in the paper on practical deep learning models are not explicitly discussed. 3) Questions:  How does the new strategy proposed in the paper compare to existing methods for proving convergence in deep learning architectures  Can the findings in the paper be extended to more complex neural network architectures beyond the examples discussed  How sensitive are the convergence guarantees to variations in model complexity such as the number of layers or different activation functions 4) Limitations:  The paper heavily focuses on theoretical aspects and may lack practical insights or experiments to validate the proposed strategies in realworld scenarios.  The approach seems to be limited to simple neural network architectures and may not directly translate to more complex models with intricate structures.  The mathematical complexity of the proofs and formulations may pose a barrier for readers without advanced mathematical knowledge limiting the accessibility of the research findings. Overall the paper presents a rigorous theoretical framework for proving convergence in deep learning architectures using Kurdyka\\xc5\\x81ojasiewicz inequalities and Rayleigh quotients however further exploration of practical implications and experiments could enhance the impact and applicability of the research.", "qbSB_cnFSYn": " Summary The paper introduces a novel method called Differential Equation GAN (DEQGAN) for solving differential equations using generative adversarial networks. The method leverages GANs to learn the loss function aiming to optimize neural networks for solving differential equations. Through a suite of experiments on twelve ordinary and partial differential equations the paper shows that DEQGAN outperforms traditional methods in terms of mean squared errors and solution accuracies. It also presents techniques for improving the robustness of DEQGAN to hyperparameter settings.  Strengths 1. Novel Approach: DEQGAN introduces a novel methodology that leverages GANs to learn the loss function for solving differential equations. 2. Performance: The paper demonstrates superior mean squared errors and solution accuracies compared to traditional numerical methods. 3. Experimental Rigor: Conducts experiments on a diverse set of differential equations and provides detailed results and comparisons. 4. Robustness: Discusses techniques for improving the robustness of the proposed method to different hyperparameter settings.  Weaknesses 1. Theoretical Justification: While the method outperforms traditional approaches there is a lack of indepth theoretical justification for the performance improvements. 2. Interpretability: The paper could benefit from more discussion on the interpretability of the loss function learned by the GAN discriminator.  Questions 1. Theoretical Justification: Can the paper provide more insights into how the discriminator learns the loss function and its implications for optimizing neural networks 2. Generalizability: How well does DEQGAN generalize to a broader range of differential equations outside the tested suite 3. Computational Efficiency: What is the computational overhead of using DEQGAN compared to conventional methods  Limitations 1. Ethical Considerations: The paper mentions potential negative societal impacts and ethical implications but a more detailed discussion on broader ethical considerations would be beneficial. 2. Evaluation Metrics: While mean squared error is used as a performance metric discussing additional evaluation measures could provide a more comprehensive view of DEQGANs effectiveness. 3. Scalability: The scalability of DEQGAN to larger and more complex differential equations could be an area of further investigation. Overall the paper presents a promising method that addresses challenges in solving differential equations but further exploration and theoretical grounding would strengthen its contributions to the field.", "yRhbHp_Vh8e": " Summary: The paper introduces VideoWhisperer a threestage Transformer model designed for holistic video understanding by incorporating spatiotemporal grounding into the structured prediction task of Video Situation Recognition (VidSitu). The model jointly predicts verbs verbrole pairs their corresponding nouns and their grounding in a weakly supervised setting. It operates on multiple video events simultaneously and significantly improves entity captioning accuracy on the VidSitu dataset. Through extensive experiments and ablations the model demonstrates stateoftheart performance on the benchmark dataset.  Strengths and Weaknesses: Strengths: 1. The paper addresses the challenging task of holistic video understanding by combining video features with objectlevel information for structured predictions. 2. The proposed VideoWhisperer model shows substantial improvements in entity captioning accuracy and grounding performance. 3. The models threestage architecture allows for endtoend training and enables joint predictions of verbs roles entities and grounding. 4. Extensive ablation experiments provide insights into the impact of different design choices on model performance. Weaknesses: 1. The paper focuses on technical details and model architecture potentially making it challenging for nonspecialists to grasp the content easily. 2. Limited discussion on potential realworld applications or practical implications of the proposed model. 3. The paper lacks a detailed comparison with existing works beyond the VidSitu benchmark dataset which could provide a broader perspective on the models performance. 4. The clarity of the language and readability can be improved for better understanding by a wider audience.  Questions: 1. How does the proposed model handle scenarios where multiple entities share similar attributes potentially leading to biases in captioning predictions 2. Are there plans to extend the evaluation of the model to other diverse video datasets to assess its generalizability and effectiveness across different domains 3. In what ways does the weakly supervised grounding approach impact the overall model performance compared to scenarios where full supervision of grounding annotations is available 4. How does the aggregation of information across multiple video events influence the models ability to capture longterm dependencies and contextual information in complex video sequences  Limitations: 1. The paper primarily focuses on demonstrating the models performance improvements on the VidSitu dataset without analyzing its scalability or adaptability to other video understanding tasks. 2. The evaluation of the model is constrained to a specific dataset and may not fully capture the diverse complexities found in realworld video data. 3. The lack of consideration for realtime applications and computational efficiency could limit practical implementation in video analysis systems that require low latency. 4. The generalization of the proposed model beyond structured prediction tasks to other types of video understanding tasks remains unclear highlighting a limitation in versatility.", "rwdpFgfVpvN": " Peer Review: 1) Summary: The paper introduces the RECtified Online Optimization algorithm (RECOO) for online convex optimization with hard constraints. It addresses both fixed constraints and adversarial constraints settings. RECOO achieves significant improvements in regret and violation bounds compared to existing algorithms. The paper covers theoretical analysis introduces key lemmas experiments on synthetic and real datasets and application in online job scheduling in distributed data centers. 2) Strengths:  The paper addresses an important problem of online convex optimization with hard constraints.  The proposed RECOO algorithm demonstrates impressive improvement in regret and violation bounds.  The theoretical analysis is thorough incorporating key lemmas and assumptions and illustrates the effectiveness of the algorithm.  The experiments on synthetic and real datasets as well as the job scheduling application provide strong evidence of the algorithms performance.  The comparison with existing algorithms and the demonstration of superior performance reinforce the relevance and significance of RECOO. 3) Weaknesses:  The paper could benefit from a more detailed explanation of the algorithms complexity and scalability.  The novelty of the algorithm could be further emphasized by highlighting its unique approach or features.  The paper could explore more scenarios or datasets to showcase the generalizability of RECOO.  The presentation of experimental results could be more detailed and include additional analyses to support the conclusions. 4) Questions:  Can the authors provide more insight into how the RECOO algorithm performs in scenarios with complex constraint functions  How does the algorithm handle situations where constraints vary significantly over time or across different rounds  Are there any specific limitations or challenges faced during the implementation of the algorithm that could be elaborated on  Are there practical considerations or assumptions that might limit the algorithms applicability in realworld scenarios 5) Limitations:  The paper focuses on theoretical analysis and experiments with limited scenarios which might not fully capture the algorithms performance under diverse conditions.  The generalizability of the algorithm across different applications or domains could be further explored to understand its versatility.  The paper lacks indepth discussions on potential risks or limitations associated with the adoption of the RECOO algorithm in practical settings. Overall the paper presents a novel algorithm for online convex optimization with hard constraints and provides strong theoretical and empirical support for its effectiveness. Enhancing the discussion on limitations scalability and broader applicability could further strengthen the research. Further exploration into realworld implementations and additional scenarios could contribute to the algorithms impact in various fields.", "lHj-q9BSRjF": "Summary: The paper investigates the emergence of incontext learning in large transformerbased models without explicit training. The study explores how the distributional properties of the training data such as burstiness large numbers of rare classes and dynamic item interpretation drive incontext learning. Through experiments using the Omniglot dataset the authors demonstrate that transformers exhibit incontext learning when trained on data with specific characteristics. Additionally the paper compares transformers to recurrent models and highlights the importance of both data properties and architecture in achieving incontext learning. Strengths: 1. The paper addresses an intriguing question about the emergence of incontext learning in large transformer models based on the distributional properties of the training data. 2. The experimental design is thorough using the Omniglot dataset to manipulate data distributions and measure the effects on incontext fewshot learning. 3. The findings provide valuable insights on how transformers outperform recurrent models in incontext learning emphasizing the importance of both data properties and architecture. 4. The discussion on the implications for understanding language models broader applications in nonlanguage domains and potential cognitive and neuroscience insights adds depth to the research. Weaknesses: 1. The paper could benefit from clearer explanations of certain concepts and methodologies for readers with varying levels of expertise in the field. 2. The limitations of the study such as potential biases in the experimental setup or the generalizability of findings to other domains beyond language models could be more explicitly discussed. 3. The paper focuses heavily on the technical aspects of the research leaving little room for broader implications or practical applications for the findings. Questions: 1. How did the authors ensure that the observed incontext learning was not influenced by specific characteristics of the Omniglot dataset 2. Are there plans to explore the transferability of these findings to other types of models or datasets beyond the specific scenarios tested in the paper 3. Can the results of this study be applied to inform the development of more efficient learning algorithms or training strategies for neural networks in practical applications Limitations: 1. The study primarily focuses on the emergence of incontext learning in transformer models based on specific properties of training data potentially limiting the generalizability of findings. 2. The investigation is confined to artificial datasets like Omniglot and the applicability of the results to realworld scenarios or diverse datasets may require further validation. 3. The emphasis on the technical aspects of data distributions and model architectures may overshadow broader implications or realworld implications of the research. Overall the paper provides valuable insights into the relationship between data properties architecture and incontext learning in large transformer models but further research is needed to explore the broader implications and applicability of these findings.", "zyrBT58h_J": " Peer Review  1) Summary The paper introduces a novel Sustainable Online Reinforcement Learning (SORL) framework to address the Inconsistencies Between Online and Offline (IBOO) challenge in the autobidding problem in online advertising systems. They argue that the traditional method of training autobidding policies in a virtual advertising system (VAS) based on historical data suffers from IBOO leading to decreased performance in realworld advertising systems (RAS). The SORL framework directly interacts with the RAS to train the autobidding policy avoiding IBOO. The framework consists of a Safe and Efficient Online Exploration (SER) policy and a VarianceSuppressed Conservative Qlearning (VCQL) method for offline training. Extensive simulated and realworld experiments validate the superiority of their approach over stateoftheart methods.  2) Strengths and Weaknesses Strengths: The paper addresses a critical challenge in autobidding policies with an innovative and practical framework. The theoretical foundation algorithms and iterative structure are clearly explained. Extensive experiments are conducted to validate the proposed approach. The paper provides indepth analyses definitions and thorough discussions on the IBOO challenge. The results demonstrate the effectiveness of the SORL framework over existing methods. Weaknesses: The paper could benefit from further elaboration on the limitations of their approach and potential risks associated with directly interacting with the RAS. The discussion on scalability generalizability and potential ethical implications could be more detailed. Additionally the theoretical proofs and mathematical formulations could be made more accessible for a wider readership.  3) Questions  How does the SER policy balance safety with efficiency in data collections from the RAS  Can the SORL framework handle dynamic and evolving bidding environments in online advertising systems  What are the computational requirements and deployment considerations for implementing the SORL framework in realworld settings  4) Limitations The main limitation of the paper lies in the scalability and generalizability of the proposed SORL framework. While the experiments show promising results the applicability of the approach to largescale advertising systems and diverse industry settings needs further investigation. The reliance on Lipschitz smooth property and quadratic Q functions for computational efficiency may restrict the frameworks adaptability to complex and nonlinear systems. Moreover the realworld feasibility of directly interacting with the RAS for training policies may pose practical challenges that require thorough exploration.  Overall Evaluation The paper presents a novel and systematic approach to address the IBOO challenge in autobidding policies showcasing significant contributions to the field of online advertising systems. The theoretical foundations algorithms and experimental validations strengthen the credibility and impact of the proposed SORL framework. Further exploration of scalability generalizability and practical implications would enhance the significance and applicability of the research findings.", "kHrE2vi5Rvs": " Peer Review  1) Summary: The paper introduces a novel training scheme called SymNCO which leverages symmetricities in combinatorial optimization (CO) problems to improve the performance of deep reinforcement learningbased combinatorial optimization solvers (DRLNCO). The authors demonstrate that SymNCO significantly enhances the generalization capability of DRLNCO methods without relying on problemspecific expert domain knowledge. Experimental results on four CO tasks including the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) show that SymNCO outperforms existing DRLNCO methods and even conventional solvers like the iterative local search (ILS) at a 240\u00d7 faster speed.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem by proposing a novel training scheme SymNCO to improve the performance of existing deep reinforcement learningbased combinatorial optimization solvers.  The concept of leveraging symmetricities in CO problems to enhance generalization is novel and has the potential to advance the field of neural combinatorial optimization.  Experimental results demonstrate the effectiveness of SymNCO in improving the performance of DRLNCO methods across different CO tasks. Weaknesses:  While the paper provides a detailed description of the proposed SymNCO method some sections might be too technical for readers unfamiliar with deep reinforcement learning and combinatorial optimization.  The paper could benefit from a more thorough discussion on the limitations and potential drawbacks of the proposed SymNCO method.  3) Questions:  How does SymNCO compare to other regularizationbased methods in similar domains  Are there specific types of CO problems for which SymNCO may not be as effective How generalizable is SymNCO across different classes of CO problems  How scalable is the SymNCO approach to larger and more complex combinatorial optimization tasks  4) Limitations:  The paper focuses mainly on Euclidean COPs raising questions about the applicability of SymNCO to nonEuclidean spaces and graphbased COPs.  The potential limitations of SymNCO in terms of scalability to larger problem sizes and its performance on more complex CO tasks should be explored further.  5) Suggestions for Improvement:  Provide a clearer explanation of the SymNCO method and its implementation for readers outside the specific research domain.  Address potential future directions for research such as extension to more diverse types of CO problems and scalability for largerscale applications. Overall the paper presents an interesting approach with potential implications for the field of combinatorial optimization and deep reinforcement learning. Further experimentation and exploration of the limitations and scalability of SymNCO would strengthen the contribution of this work.", "nOw2HiKmvk1": " Peer Review: 1) Summary: The paper proposes a new method for training debiased classifiers without requiring spurious attribute labels. The method called Learning with Biased Committee (LWBC) utilizes a committee of biased classifiers to identify biasconflicting samples and assign weights based on the consensus within the committee. LWBC is trained with knowledge transfer from the main classifier and utilizes a selfsupervised representation for debiasing. Extensive experiments on realworld datasets demonstrate the superior performance of LWBC over existing methods even occasionally surpassing those relying on bias labels. 2) Strengths:  The paper addresses a crucial issue of bias in neural networks and proposes an innovative method LWBC to train debiased classifiers without spurious attribute labels.  The utilization of a committee of biased classifiers and the consensus mechanism for identifying biasconflicting samples is a novel and effective approach.  Extensive experiments on five realworld datasets demonstrate the superiority of LWBC over existing methods both with and without spurious attribute labels.  The paper is wellstructured providing detailed explanations of the proposed method experiments and ablation studies. 3) Weaknesses:  The paper lacks discussion on the generalizability of LWBC to other domains or datasets beyond the ones tested in the experiments.  The methods computational complexity and training time compared to existing approaches are not thoroughly discussed which could be crucial for practical implementation.  The paper could benefit from including a qualitative comparison with other stateoftheart debiasing methods in addition to quantitative results. 4) Questions:  How does the LWBC method handle scenarios where bias types are not predefined or vary significantly across datasets  Could the proposed method be extended to handle more complex scenarios involving multiple types of biases or conflicting biases within the same dataset  What are the implications of the randomized subsets in the bootstrapped ensemble on the overall performance and reliability of the LWBC method 5) Limitations:  The LWBC methods reliance on the selfsupervised representation and the bootstrapped ensemble approach introduces training complexity and potential performance fluctuations as noted in the limitations section.  The use of a committee of biased classifiers may raise concerns about the robustness of the method to noise in the dataset or the quality of individual classifiers.  The limitation section could be enhanced by discussing potential challenges in scaling the LWBC method to larger datasets or domains with diverse biases. Overall the paper presents a novel and effective method for training debiased classifiers without spurious attribute labels. The extensive experiments ablation studies and comparisons with existing methods provide strong evidence of the methods effectiveness. Addressing the weaknesses and limitations highlighted above could further strengthen the papers contributions and practical implications.", "tglniD_fn9": " Summary The paper investigates the disparity in performance between soft and hard Concept Bottleneck Models (CBMs) by addressing issues related to concept leakage and inflexible concept predictors. The authors propose modifications to hard CBMs including incorporating a sidechannel for latent concepts and utilizing an autoregressive configuration for the concept predictor. Their experiments on synthetic and real datasets demonstrate that these modifications improve predictive performance concept accuracy and intervention accuracy making hard CBMs competitive with soft CBMs while maintaining resilience to leakage.  Strengths  The paper tackles a crucial issue of leakage in CBMs and provides innovative solutions to improve performance without compromising interpretability.  The proposed modifications are wellexplained and theoretically grounded offering technical insights for researchers working in this area.  Empirical results from experiments on synthetic and real datasets showcase the effectiveness of the proposed modifications in enhancing concept accuracy and intervention accuracy.  Weaknesses  While the theoretical aspects of the modifications are wellelaborated a more detailed explanation or insights into the practical implementation challenges and considerations could be beneficial.  The comparison between soft and hard CBMs could benefit from a more comprehensive analysis regarding computational efficiency especially in largescale applications.  Questions  How do the proposed modifications in the autoregressive architecture impact the scalability of the CBMs in handling larger and more complex datasets  Are there any specific limitations or assumptions inherent in the sidechannel approach that might affect its applicability to diverse domains or datasets  Can the paper provide insights into the potential tradeoffs or implications of adopting the proposed modifications on model interpretability and transparency  Limitations  The paper focuses on improving the predictive performance and accuracy of CBMs but could provide further discussions on the ethical considerations related to using these models in realworld decisionmaking scenarios.  The empirical evaluation while informative could benefit from more diverse datasets and benchmarking against existing stateoftheart CBM models for a more comprehensive comparison. Overall the paper offers valuable contributions toward enhancing the performance of CBMs while addressing critical issues related to leakage concept accuracy and intervention accuracy. Further exploration of practical implications and realworld applications of the proposed modifications would enrich the impact of the research.", "ldRyJb_cjXa": "Peer Review 1) Summary The paper introduces Star Temporal Classification (STC) as an algorithm for learning from partially labeled and unsegmented sequential data addressing the issues that arise with missing labels in traditional loss functions. By incorporating a special star token STC enables alignments with missing tokens and achieves competitive results in automatic speech recognition and handwriting recognition tasks. The authors present key optimizations for efficiency and demonstrate the applicability of STC in various sequence transduction tasks. 2) Strengths and weaknesses Strengths:  The introduction of the STC algorithm for handling missing labels in sequential data is novel and addresses an important problem in weakly supervised learning.  The experiments show promising results in automatic speech recognition and handwriting recognition indicating the effectiveness of STC in closing the performance gap with supervised baselines.  The use of weighted finitestate transducers (WFSTs) and automatic differentiation simplifies the implementation of STC and enables efficient computation of gradients. Weaknesses:  The paper lacks a detailed comparison with existing approaches in weakly supervised learning particularly in terms of performance metrics and scalability.  The impact and robustness of the proposed optimizations for efficiency are not extensively evaluated warranting further analysis.  The experimental setup could benefit from discussing the robustness of STC under different noise levels or data distributions. 3) Questions  How does the token insertion penalty parameter \u03bb influence the training dynamics and convergence of the STC model  Can the STC algorithm be extended or modified to handle varying degrees of noise in the input data such as erroneous or mislabeled tokens  Are there any specific challenges or limitations in scaling the STC approach to larger datasets or more complex sequence transduction tasks 4) Limitations  The limitations of the paper include a lack of extensive comparison with existing weakly supervised learning methods limited exploration of hyperparameter sensitivity and the need for more indepth analysis of the impact of STC on diverse datasets and domains.  The proposed optimizations for efficiency need to be rigorously evaluated across different scenarios and dataset sizes to determine their generalizability and effectiveness. Overall the paper presents a novel algorithm STC for weakly supervised learning in sequential tasks with missing labels demonstrating promising results in automatic speech recognition and handwriting recognition. Further research could focus on addressing the identified limitations and exploring the scalability and robustness of the STC approach.", "x8DNliTBSYY": " Peer Review:  1) Summary: The paper explores the wellconditioning of Neural Tangent Kernels (NTK) in deep neural networks with sublinear layer widths. The authors establish a lower bound on the smallest NTK eigenvalue for deep networks with minimum overparameterization. The study finds implications for memorization capacity and optimization guarantees for gradient descent training.  2) Strengths and weaknesses: Strengths:  The research addresses an important and novel question regarding deep neural networks with sublinear layer widths.  The paper provides rigorous theoretical analysis and establishes lower bounds on NTK eigenvalues for such networks.  The findings have significant implications for both memorization capacity and optimization guarantees in deep networks.  The methodology of the study is welldefined and the proofs are detailed.  The paper presents a concrete application in the form of memorization and optimization results. Weaknesses:  The text can be technical and may be challenging for readers not deeply familiar with the topic.  Some key concepts and assumptions are highly specialized which may limit the understanding of the broader scientific community.  It is important to clearly link the theoretical findings to practical implications in realworld scenarios.  3) Questions:  How applicable are the findings regarding sublinear layer widths to practical deep learning scenarios  Could the proposed methodology for NTK analysis be extended to other types of neural networks or methodologies beyond gradient descent  Is there a plan for experimental validation of the theoretical results in real neural networks  4) Limitations:  The study primarily focuses on theoretical analysis and there could be limitations in translating the findings to practical implementations.  The specialized assumptions and technical nature of the research may limit broader adoption and understanding by the general scientific community.  The lack of validation through practical experiments in realworld deep learning applications could be seen as a limitation. Overall the paper presents a strong theoretical study on the wellconditioning of NTK in deep networks with sublinear layer widths. The findings have significant implications for understanding memorization capacity and optimization behavior in these networks. To enhance the impact of the research addressing some of the limitations such as experimental validation and broader accessibility of the findings could be beneficial.", "xaWO6bAY0xM": " Peer Review  1) Summary The paper investigates the problem of designing Lipschitz neural networks for achieving certified robustness against adversarial examples. It bridges the gap in research related to robustness in the \\xe2\\x84\\x93\\xe2\\x88\\x9e perturbation setting. The study presents novel insights into the expressive power of Lipschitz networks by focusing on representing Boolean functions. The paper introduces a unified Lipschitz network framework called SortNet which outperforms prior Lipschitz networks in terms of certified robustness across various datasets and perturbation radii.  2) Strengths and Weaknesses Strengths:  The paper provides significant theoretical contributions by identifying limitations of standard Lipschitz networks in representing symmetric Boolean functions and order statistics.  The proposed SortNet architecture is wellmotivated extending prior Lipschitz networks and showing improved certified robustness performance in experiments.  The empirical evaluation on benchmark datasets demonstrates the scalability and effectiveness of SortNet in achieving stateoftheart certified robustness. Weaknesses:  The paper primarily focuses on \\xe2\\x84\\x93\\xe2\\x88\\x9e perturbation setting limiting the generalizability of the findings to other norms.  The discussion on limitations could be further elaborated especially concerning the scalability and efficiency of training more complex Lipschitz networks like SortNet for largescale datasets.  The evaluation metrics could be more comprehensive covering aspects beyond robustness such as computational efficiency particularly with respect to training time and certification.  3) Questions  How does the proposed SortNet architecture compare with randomized smoothing methods especially concerning probabilistic guarantees and scalability to large datasets  What are the implications of the findings on expressive power for different Lipschitz activation functions other than those considered in the paper  Are there potential practical applications or extensions of SortNet beyond image classification tasks  4) Limitations The limitations of the paper include the focus on \\xe2\\x84\\x93\\xe2\\x88\\x9e perturbations the lack of discussions on training efficiency for large models and the need for further exploration of the implications of the theoretical results in practical settings.  5) Overall Evaluation The paper offers valuable insights into enhancing certified robustness through Lipschitz neural networks particularly with the development of the SortNet framework. The combination of theoretical analyses and practical demonstrations through experiments showcases the potential of SortNet in achieving superior certified robustness indicating promising directions for future research in robust neural network design. More detailed discussions on the scalability and broader applicability of SortNet could further enrich the impact and relevance of the paper.", "owDcdLGgEm": "Peer Review 1) Summary The paper introduces a method for estimating the poses of CryoEM images under the group synchronization framework using the relative poses to recover an estimation of the absolute poses. By leveraging the symmetries of CryoEM particularly the group O(2) the proposed method improves on existing multifrequency vector diffusion maps (MFVDM) by not only predicting the similarity between images\u2019 viewing directions but also recovering their poses. The study validates the method on synthetic datasets and demonstrates its potential in improving 3D reconstruction of biomolecules. 2) Strengths and weaknesses Strengths:  The paper addresses a crucial challenge in CryoEM imaging by proposing a method that leverages the symmetries of the images to estimate poses effectively.  The use of the group synchronization framework and the focus on the group O(2) poses provide a novel approach to pose estimation in CryoEM.  The method has been validated on synthetic datasets showcasing promising results in recovering image poses and potentially improving 3D reconstruction algorithms.  The paper is wellstructured providing detailed explanations of the methods theoretical foundations and algorithmic steps. Weaknesses:  The paper could benefit from more concrete experimental results especially on realworld CryoEM datasets to further validate the proposed methods effectiveness.  The limitations section could be expanded to include more discussion on potential challenges and how the method addresses them.  Some technical terms and mathematical concepts might be difficult for readers not wellversed in CryoEM imaging or linear algebra. 3) Questions  How does the proposed method compare to other stateoftheart techniques in terms of accuracy computational efficiency and robustness in handling realworld CryoEM data  Can the method be extended to handle more complex symmetries or diverse biological structures observed in CryoEM imaging  Have you considered incorporating additional validation methods or comparison studies with existing algorithms to further support the methods performance and reliability 4) Limitations  The biggest limitation of the study is the lack of realworld data validation. Although synthetic datasets offer controlled testing conditions the methods applicability to actual experimental data should be thoroughly investigated.  The assumptions made in the study such as uniformly distributed poses might impact the methods generalizability to a wider range of CryoEM scenarios.  The global ambiguity in chirality and poses while addressed to some extent still presents challenges in accurate pose estimation which could impact the reliability of the reconstruction results. Overall the paper presents an innovative method for pose estimation in CryoEM imaging offering a promising direction for further research in the field. Addressing the weaknesses and limitations highlighted above could strengthen the impact and practical utility of the proposed method.", "lIeuKiTZsLY": " Peer Review: 1) Summary: The paper introduces a novel method for causal structure identification in the presence of latent variables within a hierarchical graph structure. The proposed algorithm leverages rank deficiency constraints to efficiently locate latent variables determine their cardinalities and identify the latent hierarchical structure. The study shows that the algorithm can asymptotically identify the correct Markov equivalence class of the entire graph under certain restrictions. The paper is wellstructured and provides formal definitions conditions algorithms and theoretical results to support the proposed method. 2) Strengths:  The paper addresses an important and challenging problem in causal structure identification particularly in the context of latent hierarchical causal models.  The proposed method is innovative and leverages rank deficiency constraints effectively to identify latent variables and their relationships.  The thorough theoretical analysis and formal definitions provided make the paper comprehensive and wellgrounded.  The experimental results demonstrate the effectiveness of the proposed method compared to existing approaches on various types of latent graphs.  The organization of the paper is clear and logical facilitating understanding for the readers. 3) Weaknesses:  The paper could benefit from more detailed explanations of the experimental setup data generation process and comparison metrics used in the evaluation of the proposed algorithm.  The limitations of the proposed method and potential challenges in realworld applications could be discussed further to provide a clearer understanding of the practical implications. 4) Questions:  How does the proposed algorithm handle scenarios with nonlinear causal relationships and situations where measured variables can cause latent variables  Could you elaborate on the computational complexity of the algorithm and how it scales with the size of the graph and the number of latent variables  Have you considered applying the method to realworld datasets with latent confounders to validate its performance in practical scenarios 5) Limitations:  The paper primarily focuses on synthetic data experiments therefore the generalizability of the proposed method to realworld datasets needs to be further investigated.  The complexity of the method and the assumptions made may limit its applicability in highly complex systems with numerous latent variables and intricate causal relationships.  The study lacks empirical evaluation on realworld datasets to demonstrate the methods effectiveness in practical applications. Overall the paper presents a promising approach to address latent confounders in causal structure identification with a focus on hierarchical graph structures. Further exploration of the methods applicability to realworld datasets and consideration of more complex causal relationships will enhance the significance and practical utility of the proposed algorithm.", "wiHzQWwg3l": "Peer Review: 1) Summary: The paper proposes a novel approach to incorporating internal predictive modeling in unsupervised domain adaptation for handling nonstationary streaming data in realtime applications. The authors introduce a continuoustime Bayesian filtering problem within a stochastic dynamical system context to model the dynamics of unsupervised domain adaptation model parameters evolving with nonstationary streaming data. They develop extrapolative continuoustime Bayesian neural networks (ECBNN) to extrapolate the distribution of model parameters before new data arrives reducing latency. The empirical results demonstrate the effectiveness of ECBNN in achieving trainingfree testtime adaptation with low latency and gradually improving model performance over time during realtime testing. 2) Strengths and weaknesses: Strengths: a. The paper addresses a significant challenge of handling nonstationary streaming data in realtime applications. b. The introduction of internal predictive modeling and continuoustime Bayesian filtering enhances unsupervised domain adaptation. c. The proposed method ECBNN shows promising results in achieving lowlatency testtime adaptation and improved model alignment and performance over time. d. Extensive experiments on synthetic and realworld datasets support the effectiveness of ECBNN. Weaknesses: a. The paper could benefit from clearer explanations in some sections especially regarding the formulation of the mathematical framework and the technical details of the proposed method. b. The description of the ablation study and its results could be more detailed to provide a better understanding of the contributions of different components of the model. c. It would be valuable to include a more indepth analysis of the limitations of the proposed approach and potential avenues for future research. 3) Questions: a. Can you elaborate on the computational complexity of the proposed ECBNN method compared to existing approaches in unsupervised domain adaptation b. How does ECBNN handle situations where the streaming data exhibits abrupt changes or outliers c. Could you provide more insights into the overall training process of ECBNN including hyperparameter tuning and convergence criteria 4) Limitations: a. The paper predominantly focuses on the effectiveness of ECBNN in specific scenarios. It would be beneficial to discuss the generalizability of the proposed method to a broader range of realtime streaming applications. b. The empirical evaluation primarily covers synthetic and limited realworld datasets. A more extensive evaluation on diverse datasets could further enhance the robustness and applicability of ECBNN. c. The proposed approach assumes continuous dynamics of NN parameters leaving room for future work to explore discrete dynamics introduced by discrete events. Overall the paper presents a novel and promising approach to enhance unsupervised domain adaptation for realtime streaming applications. Further clarity in explanations detailed experimental analysis and addressing the raised questions and limitations could strengthen the impact and applicability of the proposed ECBNN method in practical settings. 5) Recommendation: Based on the innovative nature of the proposed method and the encouraging results demonstrated in the paper I recommend further refining the paper by addressing the raised questions overcoming the highlighted weaknesses and conducting additional experiments for a more comprehensive evaluation. The authors should consider incorporating the suggestions to enhance the clarity depth and generalizability of their approach.", "nVV6S2sb_UL": "Peer Review: 1. Summary: The paper presents a new framework called MultiRoundSecAgg for secure aggregation in federated learning aiming to extend privacy guarantees over multiple training rounds. It addresses the issue of significant privacy leakages due to partial user selection in conventional secure aggregation protocols. The framework employs a structured user selection strategy to ensure longterm privacy while considering fairness and the average number of participating users at each round. Experimental results on MNIST CIFAR10 and CIFAR100 datasets show improved privacy protection and test accuracy over baseline methods in both IID and nonIID settings. 2. Strengths:  The paper introduces a novel concept of longterm privacy guarantees in federated learning filling an important gap in existing secure aggregation protocols.  The MultiRoundSecAgg framework provides a structured user selection strategy that balances privacy protection and fairness.  Experiments are conducted on multiple datasets in different settings demonstrating the effectiveness of the proposed framework.  The theoretical guarantees and convergence analysis provide a solid foundation for the proposed approach. 3. Weaknesses:  The paper could benefit from more detailed explanations of the experiments and a deeper analysis of the results to enhance the understanding of the proposed frameworks performance.  While the theoretical guarantees are mentioned a more thorough discussion of their implications and applications could strengthen the paper. 4. Questions:  How is the proposed framework affected by the size of the dataset or the complexity of the machine learning model used in federated learning  How does the choice of the target multiround privacy guarantee (parameter T) impact the practical implementation and performance of MultiRoundSecAgg  Are there specific scenarios or applications where MultiRoundSecAgg may outperform other secure aggregation protocols significantly 5. Limitations:  The experiments could be expanded to include more diverse datasets or scenarios to evaluate the frameworks robustness across a wider range of applications.  The paper could further discuss potential realworld implications and deployment challenges of MultiRoundSecAgg for practical implementations of federated learning systems. Overall the paper introduces a valuable contribution to the field of federated learning by addressing longterm privacy guarantees and the MultiRoundSecAgg framework shows promise for enhancing privacy protection in this context. Further refinements in the presentation of experimental results and thorough discussions on practical implications could enhance the impact of the paper.", "tTWCQrgjuM": "Peer Review: 1) Summary: The paper proposes a novel MCMC framework for performing Bayesian inference on model parameters from privatized data. The framework addresses the challenge of intractable likelihood functions resulting from privacy mechanisms and is applicable to a wide range of statistical models. The authors demonstrate the efficacy of their methods on a na\u00efveBayes loglinear model and a linear regression model. The paper provides theoretical analysis computational complexity results and simulation experiments to validate the proposed approach. 2) Strengths and weaknesses: Strengths:  The paper addresses an important challenge in statistical inference from privatized data with a novel MCMC framework.  The approach is generalpurpose and applicable to a wide range of models and privacy mechanisms.  The theoretical analysis of computational complexity acceptance rate and mixing properties provides a strong foundation.  Simulation experiments on loglinear and linear regression models demonstrate the practical efficacy of the proposed methods. Weaknesses:  The paper heavily focuses on technical details which may be overwhelming for readers unfamiliar with MCMC methods and differential privacy.  The limitations of the proposed algorithm such as susceptibility to correlation between x and \u2713 are acknowledged but not fully addressed in the current work.  The discussion on the implications of the method in realworld applications is limited more practical examples or scenarios could enhance the relevance of the paper. 3) Questions:  Can you elaborate on the specific assumptions or conditions under which the proposed MCMC framework is most effective or efficient  How does the proposed algorithm perform in scenarios with highdimensional models or complex data structures  Could you provide more insights into the computational efficiency and scalability of the method when applied to largescale datasets or realworld applications 4) Limitations:  The paper focuses on a specific class of privacy mechanisms and the generalizability of the proposed algorithm to other privacy guarantees may need further investigation.  The discussion on the realworld implications and practical usability of the proposed method could be expanded to provide a clearer understanding of its application in diverse settings.  While the theoretical results are presented rigorously more empirical validations on a broader range of models and datasets would strengthen the practical relevance of the approach. 5) Overall: The paper presents a novel MCMC framework for Bayesian inference from privatized data showing promising results on loglinear and linear regression models. The theoretical foundation is strong but more emphasis on practical implications and realworld applications may enhance the impact of the proposed method. Addressing the limitations and potential scalability issues along with broader empirical validations would further strengthen the papers contribution to the field of statistical inference under differential privacy.", "rUb6iKYrgXQ": "Peer Review 1) Summary: The paper introduces a novel approach Conditional Robust Optimization (CRO) for datadriven decisionmaking under uncertainty leveraging contextual information. The proposed framework designs conditional uncertainty sets by learning from historical data and establishes connections with Contextual ValueatRisk Optimization (CVO). The study includes theoretical guarantees empirical demonstrations and comparisons against benchmark approaches using simulated and realworld data. 2) Strengths and weaknesses: Strengths:  Introduction of a novel approach CRO addressing decisionmaking under uncertainty with contextual information.  The paper provides theoretical guarantees and empirical demonstrations validating the proposed method.  Integration of deep clustering methods and oneclass classification in designing conditional uncertainty sets.  Extensive experimental evaluation using simulated and realworld data from the US stock market.  Comparative analysis with benchmark approaches to showcase the effectiveness of the proposed CRO model. Weaknesses:  The paper lacks indepth exploration of the limitations of the proposed approach.  A more detailed explanation of the integration of deep learning and uncertainty set design would enhance the clarity of the study.  The practical implications of using CRO in various application domains could have been discussed further to highlight its realworld utility. 3) Questions:  Could you provide more insights into the generalizability and scalability of the proposed CRO model across different domains and datasets  How do the computational costs of implementing the CRO model compare to traditional robust optimization methods  Have you considered the potential biases or ethical implications associated with using contextual information in decisionmaking processes  What are the key criteria for selecting hyperparameters in the deep learning setups for uncertainty set design 4) Limitations:  The paper could benefit from a more comprehensive discussion on the limitations and challenges faced during the implementation of the proposed CRO model.  The experimental evaluation focuses on a specific case study using US stock market data limiting the generalizability of the results to other domains.  The paper does not address the potential tradeoffs between model complexity and interpretability in the context of datadriven decisionmaking under uncertainty. In conclusion the paper presents a wellstructured study introducing a novel approach for contextual optimization under uncertainty. Addressing the limitations providing clearer explanations and exploring broader implications could further strengthen the impact and relevance of the research.", "lDohSFOHr0": " Review of \"NACH: A New SemiSupervised Learning Method for Classifying Seen and Unseen Classes\"  1) Summary The paper introduces a new semisupervised learning (SSL) method called NACH to address the challenge of classifying both seen and unseen classes when not all classes in the training data have labels. NACH consists of two key modules: unseen class classification and learning pace synchronization. Unseen classes are classified by exploiting pairwise similarity and the learning pace is synchronized between seen and unseen classes using an adaptive threshold with distribution alignment. Experimental results demonstrate significant performance improvements in both seen and unseen classes compared to previous SSL and novel class discovery (NCD) methods.  2) Strengths and Weaknesses Strengths:  The paper addresses an important and practical problem in SSL.  Introduces a novel SSL method that achieves significant performance improvements in both seen and unseen classes.  Experimental evaluation on multiple datasets demonstrates the effectiveness of the proposed approach. Weaknesses:  The paper lacks theoretical guarantees for the proposed approach.  The paper could further elaborate on the scalability and efficiency of the proposed method for largescale datasets.  Limited discussion on the interpretability and explainability of the model.  3) Questions  Can the proposed NACH method handle imbalanced class distributions effectively  How robust is the NACH method to noise and outliers in the data  How does the proposed method compare to stateoftheart SSL methods in terms of computational efficiency  4) Limitations  The lack of theoretical guarantees for the proposed method limits the confidence in its generalizability.  The explanation of how the adaptive threshold and distribution alignment work together could be more detailed.  Training details like initialization hyperparameter tuning and convergence analysis could be more explicitly discussed.  Overall Assessment The paper presents an innovative SSL method NACH that effectively classifies both seen and unseen classes. The experimental results show promising performance improvements over existing methods. However further investigations are needed to address theoretical foundations scalability and interpretability aspects of the proposed approach. The work opens up avenues for future research in SSL and has the potential for practical impact in realworld applications.", "nOdfIbo3A-F": " Peer Review of the Scientific Paper  1) Summary: The paper introduces a Lagrangian graph neural network (LGNN) for learning the dynamics of articulated rigid bodies. The LGNN framework is demonstrated to generalize to larger system sizes incorporating unseen complex topologies including tensegrity structures. The LGNN approach exhibits superior performance compared to traditional methods and demonstrates zeroshot generalizability. The model architecture and training setup are detailed and empirical evaluations showcase the effectiveness of LGNN in simulating complex realworld systems.  2) Strengths and Weaknesses: Strengths:  Introduction of a novel Lagrangian graph neural network (LGNN) for modeling articulated rigid bodies.  Demonstrated outperformance of LGNN over traditional methods like forcebased or energybased approaches.  Generalizability and zeroshot adaptability of LGNN to different system sizes and complex topologies.  Detailed experimental setup comparisons with baselines and evaluation metrics. Weaknesses:  Lack of discussion on the computational complexity and efficiency of the LGNN compared to traditional methods.  Limited discussion on the practical application and scalability of LGNN in realworld scenarios.  The paper could benefit from addressing potential biases in the dataset or model parameters that might affect the generalizability of LGNN.  Absence of discussion on error analysis or sensitivity analysis of the LGNN model.  3) Questions:  How does the computational efficiency of LGNN compare to traditional physicsbased methods in terms of training time and inference speed  Are there specific limitations or failure scenarios where LGNN might not perform adequately in simulating dynamic systems  What factors influence the generalizability and robustness of LGNN when applied to unseen complex topologies or structures  4) Limitations:  The study focuses on theoretical demonstrations and empirical evaluations without direct implementation or validation in practical applications.  The complexity of realworld systems involving contacts collisions and deformations is acknowledged as a future research direction posing a current limitation.  The paper lacks discussion on potential biases assumptions or uncertainties that could affect the models performance in complex physical scenarios. In conclusion the paper presents an innovative approach using LGNN for simulating articulated rigid bodies showcasing promising results in terms of generalizability and performance. Further work is suggested to address scalability computational efficiency and realworld applicability of the LGNN framework.", "pCrB8orUkSq": "Peer Review Summary: The paper focuses on dynamic view synthesis (DVS) from monocular videos aiming to address discrepancies in existing experimental protocols that arise from unrealistic multiview signals. The authors introduce effective multiview factors (EMFs) to quantify the multiview signals present in input sequences propose new evaluation metrics and provide a new iPhone dataset for comprehensive evaluation. Through extensive experiments they demonstrate performance drops in stateoftheart methods under more practical conditions. The paper contributes insights into evaluating DVS methods in a more realistic setup. Strengths: 1. Innovative Approach: The introduction of EMFs and new evaluation metrics fills a crucial gap in evaluating DVS methods. 2. Extensive Evaluation: The authors conduct thorough experiments on existing and proposed datasets providing insights into the limitations of current methods. 3. Practical Implications: The proposed changes to experimental protocols have practical implications for reallife applications of DVS methods. Weaknesses: 1. Complexity: The paper introduces several new concepts and metrics which may be challenging for readers without prior knowledge in the field. 2. Limited Comparison: The comparison with existing stateoftheart methods could be further expanded to include a broader range of approaches. 3. Dataset Limitations: While the iPhone dataset is a valuable addition the paper could benefit from more extensive datasets with diverse reallife scenarios. Questions: 1. How do the proposed metrics compare to existing evaluation protocols in terms of sensitivity and reliability 2. Have the authors considered the computational cost implications of implementing the proposed metrics and factors in practical applications 3. How do the findings of this study impact the development of future DVS methods and what key takeaways can be applied to improve existing approaches Limitations: 1. Generalizability: The study primarily focuses on a specific aspect of DVS evaluation which may not cover all possible challenges in dynamic scene synthesis. 2. Evaluation Scenarios: While the proposed metrics address existing limitations additional scenarios and benchmarks could further validate the effectiveness of the new evaluation framework. 3. RealWorld Validation: The extent to which the results from the proposed benchmarks align with realworld DVS applications remains to be explored. Overall the paper offers significant contributions to the field of DVS by addressing critical methodological issues in evaluation protocols. The proposed metrics and dataset provide valuable insights into the challenges of dynamic view synthesis from monocular videos. Further clarifications on the practical implications and generalizability of the findings are recommended for broader audience comprehension.", "o3HXEEBKnD": " Peer Review:  1) Summary: The paper introduces a novel method for Single Positive MultiLabel Learning (SPML) by proposing a LabelAware global Consistency (LAC) regularization. This approach leverages manifold structure information to enhance the recovery of potential positive labels. By combining pseudolabeling consistency regularization and labelaware global consistency the proposed method achieves stateoftheart performance on SPML tasks across multiple benchmark datasets.  2) Strengths and weaknesses: Strengths:  The paper addresses an important problem of Single Positive MultiLabel Learning and proposes a novel regularization method LAC to enhance model performance.  The proposed approach is wellmotivated and theoretically grounded leveraging manifold structure information to recover potential positive labels.  A comprehensive experimental evaluation is conducted on multiple benchmark datasets demonstrating the effectiveness of the proposed method. Weaknesses:  The paper lacks a comparison with more recent stateoftheart methods beyond those cited in the references. Including comparisons with current cuttingedge approaches would strengthen the evaluation.  The explanation of some technical aspects such as the construction of connection matrices for intra and interclass connections could be clearer for better understanding.  The paper could benefit from more detailed explanations on the implications of the proposed approach beyond performance improvements particularly in realworld applications.  3) Questions:  Could the proposed LAC regularization be extended or adapted to other types of multilabel learning beyond the SPML context  How does the proposed memory queue mechanism affect the overall efficiency and computational demands of the model during training and inference  Are there specific limitations or challenges in realworld applications that could impact the performance or generalizability of the proposed method that need to be considered  4) Limitations:  The paper primarily evaluates the proposed method on benchmark datasets. Additional experiments on more diverse or challenging datasets could provide a broader perspective on the generalizability of the approach.  While the proposed method shows improvements in performance the discussion lacks insights into the interpretability of the model and the implications of the learned representations for realworld applications. Overall the paper presents a valuable contribution to the field of Single Positive MultiLabel Learning introducing a novel LabelAware global Consistency regularization that achieves stateoftheart performance on benchmark datasets. Addressing the identified weaknesses and limitations would further enhance the impact and applicability of the proposed method.", "xdZs1kf-va": " Peer Review  1) Summary The paper proposes a new method I2Q for decentralized cooperative multiagent reinforcement learning to address nonstationarity issues in independent Qlearning. By modeling ideal transition probabilities based on optimal conditional joint policies of other agents and performing independent Qlearning on these transitions I2Q converges to the optimal joint policy in deterministic environments. The method is evaluated empirically on matrix games MPEbased differential games MultiAgent MuJoCo and SMAC tasks showing remarkable improvements in various cooperative multiagent scenarios.  2) Strengths and weaknesses Strengths:  Theoretical Foundation: The paper provides a strong theoretical foundation proving convergence to the optimal joint policy in deterministic environments.  Empirical Results: I2Q shows impressive performance improvements across a variety of cooperative multiagent tasks showcasing its effectiveness.  Value Proposition: I2Q addresses the nonstationarity issue in decentralized learning through decentralized modeling of ideal transition probabilities. Weaknesses:  Theoretical Assumptions: The method assumes deterministic environments limiting generalizability to stochastic settings.  Model Complexity: Modeling ideal transition functions might introduce additional complexity impacting scalability in largescale multiagent systems.  Limited Comparison: Comparing I2Q with a wider range of decentralized MARL methods could provide a more comprehensive evaluation of its performance.  3) Questions  How does the scalability of I2Q hold up in scenarios with a larger number of agents  Can I2Q efficiently handle scenarios with complex highdimensional state spaces beyond the evaluated tasks  Have you considered incorporating communication mechanisms for coordination among agents in future iterations of I2Q  4) Limitations  Stochastic Environments: The applicability of I2Q is limited to deterministic settings posing challenges in realworld stochastic environments.  Implementation Complexity: The methods reliance on modeling ideal transitions and training multiple individual Qlearning models might increase computation and memory requirements.  Generalizability: The paper primarily focuses on decentralized tasks limiting insights into the methods adaptability to centralized settings or hybrid approaches. Overall the paper provides a comprehensive exploration of I2Q for decentralized MARL backed by solid theoretical analysis and promising experimental results. Addressing limitations related to scalability stochastic environments and complexity could enhance the methods applicability and robustness in diverse multiagent scenarios.", "yhZLEvmyHYQ": " Peer Review  Summary: The paper explores the biasvariance tradeoff in active learning with Gaussian Processes (GPs) to improve querying efficiency and data labeling. It introduces two novel acquisition functions: Bayesian QuerybyCommittee (BQBC) and Query by Mixture of Gaussian Processes (QBMGP). Empirical results demonstrate the effectiveness of these functions in achieving better marginal likelihood and predictive performance compared to conventional methods across multiple simulators.  Strengths: 1. Novel Contribution: The paper introduces innovative acquisition functions that explicitly handle the biasvariance tradeoff in active learning with GPs. 2. Theoretical Framework: The detailed discussion on the biasvariance tradeoff hyperparameters optimization and multimodal posterior analysis enhances the theoretical foundation of the research. 3. Empirical Validation: The experimental results across multiple simulators demonstrate the efficacy of the proposed acquisition functions in reducing unnecessary data labeling and improving predictive performance. 4. Comprehensive Approach: The paper covers a range of related work thoroughly explains active learning schemes and provides detailed experimental settings and evaluation metrics enhancing the completeness of the study.  Weaknesses: 1. Complexity and Accessibility: The paper though thorough is highly technical and may be challenging for readers with limited background in Gaussian Processes and active learning. 2. Limited Scope: The experiments primarily focus on classic simulators and additional realworld or diverse datasets could increase the generalizability and applicability of the proposed methods.  Questions: 1. Are there any specific considerations or limitations in applying the proposed acquisition functions to realworld datasets or scenarios outside the simulator environment 2. How do the proposed functions perform when the scope of the data or the number of dimensions increases significantly especially in practical applications 3. Can the computational complexity of the fully Bayesian GP and MCMC sampling hinder scalability when dealing with larger datasets or applications requiring realtime decision making  Limitations: 1. The empirical results are based on classic simulators which may not fully represent the complexity and diversity of realworld datasets. 2. The papers technical nature and reliance on Gaussian Processes might limit its accessibility and applicability for researchers unfamiliar with the domain. Overall the paper presents a valuable contribution to active learning with Gaussian Processes offering novel acquisition functions to address the biasvariance tradeoff. Further exploration into practical applications and scalability considerations could enhance the relevance and impact of the proposed methods in realworld scenarios.", "xI5660uFUr": " Summary The paper introduces a selective compression method SCR for deep learningbased variablerate image compression. SCR selectively encodes representation elements via a novel 3D importance map generation process adjusting importance for different quality levels. The method achieves comparable compression efficiency to separately trained models and reduces decoding time significantly.  Strengths 1. Novel Methodology: The SCR method introduces a unique approach to selectively compress representation elements for variablerate image compression. 2. Efficiency: SCR achieves comparable coding efficiency to separately trained models while reducing decoding time. 3. Applicability: The method can be easily integrated into existing compression models with minimal overhead increase demonstrating high applicability.  Weaknesses 1. Complexity: The method involves multiple stages in the compression process potentially introducing complexity. 2. Training Overhead: Training the SCR model may require additional parameters and computational resources. 3. Model Interpretability: Clarification may be needed on how the importance adjustment curves are learned and how they impact the compression process.  Questions 1. How does the application of the SCR method impact the overall computational overhead during the compression process 2. Can the SCR method be generalized to other types of data beyond images such as videos or audio 3. How sensitive is the SCR method to variations in target quality levels and input content complexity  Limitations 1. Experiment Variation: The paper could benefit from more detailed experimental variations to showcase the robustness of the SCR method across different scenarios. 2. Generalization: The generalization of the SCR method to different models and datasets may need further investigation for broader applicability. 3. Comparative Analysis: A more indepth comparison with a wider range of existing methods could enhance the validation of SCRs efficiency and effectiveness. This paper presents an innovative approach to image compression with promising results. However addressing the identified weaknesses and limitations could further strengthen the methods practicality and impact in the field of deep learningbased compression techniques.", "tz1PRT6lfLe": " Peer Review  1) Summary: The paper presents a unified framework called SoteriaFL that enhances the communication efficiency of private federated learning by combining communication compression and privacy preservation. The framework accommodates various gradient estimators and utilizes advanced shifted compression for improved convergence behavior. The performance tradeoffs in terms of privacy utility and communication complexity are comprehensively characterized showing the superiority of SoteriaFL over existing private federated learning algorithms without communication compression.  2) Strengths and weaknesses: Strengths:  The paper addresses an important research gap by proposing a unified framework that combines communication compression and privacy preservation in private federated learning.  The inclusion of various algorithms within the SoteriaFL framework such as SoteriaFLGD SoteriaFLSGD SoteriaFLSVRG and SoteriaFLSAGA provides versatility and adaptability to different scenarios.  The theoretical analysis and formal guarantees provided for privacy utility and communication complexity enhance the credibility and applicability of the proposed framework.  The numerical experiments validate the effectiveness of the proposed algorithms and demonstrate their advantages over existing approaches particularly in scenarios with large local datasets. Weaknesses:  The paper could benefit from a more detailed discussion on the practical implementation challenges of the proposed framework such as computational overhead scalability issues and adaptability to realworld federated learning systems.  While the theoretical guarantees are thorough practical implications and performance under diverse realworld conditions should be tested further to reinforce the applicability of the framework.  The experimental section could be expanded to include more diverse datasets and scenarios to provide a comprehensive evaluation of the proposed algorithms performance in various settings.  4) Questions:  How does the proposed framework handle nonIID data distributions across clients and what impact does this have on the utility and convergence properties of the algorithms  Are there any specific constraints or limitations in the practical deployment of the SoteriaFL framework in realworld federated learning scenarios especially considering heterogeneous client devices and network conditions  How scalable is the SoteriaFL framework in terms of the number of clients and does it maintain its performance characteristics as the system scales up  5) Limitations:  The paper primarily focuses on the theoretical analysis and numerical experiments potentially lacking realworld deployment considerations and scalability assessments.  The evaluation on a limited number of datasets and scenarios may not fully capture the diverse challenges and performance variations that could be encountered in practical federated learning environments.  Further investigations are warranted to assess the robustness and generalizability of the proposed algorithms in scenarios with highly dynamic or adversarial data distributions. Overall the paper presents a promising contribution to the field of private federated learning by introducing a novel framework that combines communication efficiency and privacy preservation. However further research and experimentation are needed to assess its practical feasibility and performance across a wider range of practical settings and scenarios.", "kI_kL5vq6Oa": "Peer Review 1) Summary: The paper presents a riskdriven approach to designing perception systems for safetycritical autonomous systems. The methodology involves formulating a risk function to quantify the impact of perceptual errors on overall safety and incorporating it into the design of perception systems. The approach is demonstrated on a visionbased aircraft detect and avoid application showing a 37 reduction in collision risk compared to a baseline system. 2) Strengths and Weaknesses: Strengths:  The paper addresses the important issue of designing perception systems for safetycritical autonomous systems.  The methodology of incorporating risk awareness into the design process is novel and has potential implications for enhancing safety in autonomous systems.  The experimental validation on a visionbased aircraft detect and avoid system provides practical insights into the effectiveness of the proposed approach. Weaknesses:  The paper focuses heavily on methodology and technical details which may be challenging for readers not deeply familiar with the domain.  The simulationbased evaluation may lack realworld validation which limits the generalizability of the findings.  Some sections of the paper could benefit from clearer explanations and more intuitive examples to enhance understanding for a broader audience. 3) Questions:  How does the approach proposed in the paper compare to existing methods for designing perception systems in safetycritical applications  Can the riskdriven design methodology be easily adapted to other types of perception systems beyond visionbased applications  How robust is the risk function in capturing the variability and complexity of perception errors in different operating conditions 4) Limitations:  The papers findings are based on simulation results which may not fully capture the complexities and uncertainties of realworld scenarios.  The assumption of a Markov perception system limits the applicability of the proposed technique to certain types of perception systems.  The paper does not provide explicit safety guarantees highlighting the need for additional testing and validation before deployment in practical systems. Overall the paper presents a valuable contribution to the field of autonomous systems design particularly in addressing the critical issue of perception system safety. Further research to apply the riskdriven design methodology in practical settings and enhance its scalability and robustness would be beneficial.", "xL8sFkkAkw": " Peer Review:  1) Summary: This paper introduces a novel approach named GradCosine for evaluating the initial state of a neural network by maximizing the cosine similarity of samplewise gradients with the goal of improving both training and test performance. The proposed Neural Initialization Optimization (NIO) algorithm leverages GradCosine to automatically find better initializations for neural architectures. Experimental results on CIFAR10 CIFAR100 ImageNet and SwinTransformer demonstrate the effectiveness of NIO in enhancing classification performance. The paper is wellstructured providing indepth theoretical foundations detailed algorithm descriptions and comprehensive experimental evaluations across various datasets and architectures.  2) Strengths:  Originality: The paper introduces a novel gradientbased approach GradCosine for initializing neural networks which is differentiated from existing heuristic and learningbased initialization methods.  Theoretical Foundation: The paper provides strong theoretical foundations especially in linking the proposed quantity to improved training and generalization performance.  Algorithm Development: The NIO algorithm is welldeveloped adapting the theoretical insights into a practical optimization process for initializing neural networks.  Experimental Validation: The paper provides extensive experimental results showcasing the effectiveness of NIO across different datasets and architectures demonstrating improvements over baseline initialization methods.  Clear Presentation: The paper is wellstructured with clear sections outlining the problem theoretical foundations algorithm development and experimental evaluations.  Weaknesses:  Complexity: The theoretical foundations and algorithm derivation might be challenging for readers without a strong background in optimization and neural networks.  Limitation of Samplewise Analysis: The discussion on the computational requirements and scalability of NIO for large models is limited. It would be valuable to address the memory and time constraints in realworld applications.  4) Questions:  What computational complexity is associated with calculating GradCosine accurately for largescale models and how does it impact algorithm scalability  How sensitive is the proposed NIO algorithm to the hyperparameters such as the upper bound of the gradient norm and how were these values determined in the experiments  Are there considerations for extending the NIO algorithm beyond neural network initialization to other areas of deep learning research  Can the NIO algorithm benefit from adaptive learning rate strategies to further enhance optimization in the initialization phase  5) Limitations:  Scalability: The calculation of GradCosine for large models may pose scalability challenges in terms of memory and computation resources.  Hyperparameter Sensitivity: The dependence on hyperparameters such as the upper bound of gradient norm might introduce additional tuning requirements that need further investigation.  Generalization: The general applicability of NIO across diverse datasets and architectures beyond the ones presented in the experiments should be explored to assess its robustness. In conclusion the paper presents a novel approach to neural network initialization with strong theoretical foundations and promising experimental results. Addressing the questions and limitations mentioned could further strengthen the practical applicability and impact of the proposed NIO algorithm in the field of automated machine learning and neural network design.  This review provides an evaluation of the papers strengths weaknesses questions for further clarification and limitations. It offers constructive feedback on potential areas for improvement and highlights the significance of the research contributions.", "yfNSUQ3yRo": "Peer Review 1) Summary: The paper introduces a novel method Noise Attention Learning (NAL) to address the issue of label noise in deep neural networks. NAL uses an attention branch to distinguish mislabeled samples from clean samples and prevent memorization of mislabeled samples during training. The method is empirically evaluated on various datasets with simulated and realworld label noise showcasing superior performance compared to stateoftheart methods. The proposed approach demonstrates effectiveness in preventing overfitting to noisy labels and improving generalization. 2) Strengths and weaknesses: Strengths:  The paper addresses a significant problem in machine learning namely label noise and proposes a novel solution in the form of NAL.  The method is theoretically grounded with a clear explanation of how NAL scales gradients to prevent memorization of mislabeled samples.  Extensive empirical evaluations on multiple datasets demonstrate the effectiveness of NAL in comparison to existing methods.  The paper provides thorough explanations of the proposed method including attention branch design loss function formulation and target estimation strategy. Weaknesses:  The paper could benefit from a more detailed discussion on the computational complexity and runtime efficiency of the proposed method.  The theoretical explanations could be further elaborated to enhance clarity especially for readers less familiar with the mathematical aspects.  More comparative analysis with advanced stateoftheart methods could add depth to the evaluation results and highlight the uniqueness of NAL. 3) Questions:  How does NAL perform when the amount of label noise in the datasets is significantly high  Can the attention weights produced by the attention branch be interpreted or visualized to provide insights into the models decisionmaking process  Are there any specific limitations or scenarios where NAL might underperform compared to existing methods 4) Limitations:  The generalizability of NAL across various types of datasets beyond image classification tasks could be explored to understand its applicability in diverse domains.  The paper primarily focuses on empirical evaluations and further theoretical analyses could strengthen the methods theoretical foundations.  The discussion on the interpretability of the attention weights in realworld applications and their impact on model explainability can be expanded. Overall the paper presents a novel and promising approach to addressing label noise in deep learning with strong empirical results and theoretical justifications. Further exploration of the methods limitations and implications in practical applications would enhance the impact of the research.", "m2JJO3iEe_5": "Peer Review Paper Title: A Provably Robust Randomized Smoothing Approach to FewShot Learning Summary: The paper introduces a provably robust defense approach based on randomized smoothing for fewshot learning models. It extends the concept of randomized smoothing to fewshot learning scenarios by mapping inputs to normalized embeddings and provides theoretical robustness guarantees against `2bounded perturbations. Lipschitz continuity analysis is performed to derive a robustness certificate which is validated through experiments on various datasets. Strengths: 1. The paper addresses an important and timely issue of adversarial attacks in fewshot learning scenarios and provides provable defense mechanisms. 2. The theoretical analysis of Lipschitz continuity and robustness guarantees adds significant value to the field of fewshot learning and adversarial attacks. 3. The experimental validation on different datasets enhances the credibility of the proposed approach. 4. The extension of randomized smoothing to classification in the embedding space fills a critical gap in existing defenses against adversarial perturbations. Weaknesses: 1. The paper lacks a detailed discussion on the potential limitations of the proposed approach especially in realworld deployment scenarios. 2. The computational complexity of the certification procedure is acknowledged but not fully addressed in terms of practical implementation. 3. More comparative analysis with existing defense mechanisms for fewshot learning could strengthen the papers contributions. Questions: 1. How does the proposed approach handle adversarial attacks beyond `2bounded perturbations in the fewshot learning context 2. Are there any plans to optimize the computational complexity of the certification procedure for practical deployment in realworld fewshot learning applications 3. Can the Lipschitz continuity analysis be extended to different types of embeddings and distance metrics in the future Limitations: 1. The paper lacks an indepth analysis of potential failure scenarios or edge cases where the proposed defense mechanism might not be effective. 2. The applicability of the approach to diverse fewshot learning scenarios beyond the datasets tested remains unclear. 3. The theoretical robustness guarantees may not fully translate into realworld adversarial scenarios without further empirical validation and stress testing. Overall the paper presents a significant contribution to the field of fewshot learning defenses against adversarial attacks. However addressing the identified weaknesses expanding the discussion on limitations and conducting further empirical evaluations could enhance the impact and applicability of the proposed approach.", "wJwHTgIoE0P": " Peer Review of the Paper titled \"Learning Image Representations with Procedural Shaders\"  1) Summary: The paper introduces a novel approach to learning image representations by utilizing a large dataset of diverse procedural image programs namely shaders coded in the OpenGL shading language. These shaders enable training neural networks for supervised and unsupervised representation learning proving to be effective in bridging the gap between pretraining with real and procedurally generated images. The study highlights the scalability and performance benefits of using a large collection of generative processes showcasing stateoftheart results in representation learning without real data.  2) Strengths and Weaknesses: Strengths:  Novel approach using diverse procedural image programs for representation learning.  Utilization of a large dataset of shaders for both supervised and unsupervised learning.  Stateoftheart performance in representation learning without real data.  Detailed experiments showcasing scaling effects and performance comparisons.  Insightful analysis on the characteristics of good generative image programs. Weaknesses:  Lack of comparison with more traditional datasets or real image pretraining approaches.  Limited discussion on potential drawbacks or limitations of the proposed method.  The applicability of the proposed method to broader domains beyond neural network representation learning should be addressed.  3) Questions:  How does the computational efficiency of training with procedurally generated shaders compare with real data pretraining methods  Can the proposed methodology be extended to other domains beyond neural network representation learning such as image generation tasks  Are there specific limitations or challenges associated with using a large collection of procedural image programs that were not addressed in the paper  4) Limitations:  The generalizability of the findings to realworld applications beyond representation learning is unclear.  The need for expert knowledge to design or optimize new shaders may limit the scalability of the proposed approach.  Additional studies on the interpretability and robustness of learned features from shaderbased training are warranted.  Ethical considerations regarding the biases or limitations of training on synthetic data should be discussed further. In conclusion the paper presents a compelling approach to learning image representations using procedural shaders demonstrating promising results in representation learning tasks. Further exploration of the scalability interpretability and broader applicability of the proposed method would enhance the impact of the research. Overall the paper provides valuable insights into the potential of leveraging procedural image programs for neural network training and opens avenues for future research in this domain. Further refinements and validations could strengthen the applicability of the proposed methodology in diverse settings.", "mMdRZipvld2": "Peer Review: 1. Summary: The paper presents a novel deconfounding approach to enhance the consistency and interpretability of representation similarity metrics in neural networks such as CKA and RSA by addressing the confounding effects of input similarity structures. Through a series of experiments the authors demonstrate that their deconfounding method improves the detection of functionally similar networks consistency in transfer learning scenarios and correlation with outofdistribution accuracy. The proposed method is shown to be effective in various domains including both image and natural language datasets and provides valuable insights into understanding the intricacies of neural networks. 2. Strengths:  The paper addresses an important and practical issue related to confounding effects in similarity metrics for neural networks.  The proposed deconfounding method is intuitive and generalizable enhancing the interpretability and consistency of CKA and RSA metrics across multiple domains.  The experiments conducted are thorough and welldesigned showcasing the effectiveness of the deconfounding approach in detecting functional similarities transfer learning scenarios and outofdistribution accuracy correlations.  The invariance properties of the deconfounded similarity metrics are carefully studied and presented solidifying the reliability and applicability of the approach across different settings. 3. Weaknesses:  The paper could benefit from a clearer explanation of the underlying statistical principles and assumptions of the deconfounding method for the benefit of readers less familiar with statistical regression techniques.  While the experiments are comprehensive a more detailed analysis of the potential limitations or caveats of the deconfounding approach could enhance the discussion section.  Further explanations on the choice of specific datasets domains or models used in the experiments may help readers understand the generalizability and limits of the proposed method. 4. Questions:  How does the proposed deconfounding approach compare to other existing methods or techniques aimed at addressing confounding effects in similarity metrics for neural networks  Could you elaborate on the computational efficiency and scalability of the deconfounding method especially when applied to larger neural networks or complex datasets  Are there any considerations or implications that researchers or practitioners should keep in mind when applying the deconfounding method in practical machine learning scenarios or realworld applications 5. Limitations: The limitations of the study include potential datadriven confounding effects not fully accounted for the assumption of linear separability between input and representation similarities which may not hold in all cases and the need for further validation across a wider range of network architectures and datasets to assess the generalizability of the deconfounding approach. Overall the paper presents a significant contribution in the domain of understanding neural networks and similarity metrics and the proposed deconfounding method has the potential to advance research in this field and improve the robustness of neural network analysis techniques. Further insights into the limitations and implications of the deconfounding approach as well as additional comparisons with alternative methods could enhance the impact and applicability of the findings presented in the paper.", "uytgM9N0vlR": "Peer Review of the Paper: \"Extracting informative representations of molecules using Graph neural networks (GNNs)\" 1) Summary: The paper investigates the effectiveness of selfsupervised graph pretraining on small molecular data using graph neural networks (GNNs). The authors conduct thorough ablation studies on pretraining components data splitting methods input features pretraining dataset scales and GNN architectures. Their findings suggest that selfsupervised pretraining does not consistently provide significant advantages over nonpretrained methods and the benefits of supervised pretraining may diminish with richer features or balanced data splits. The paper emphasizes the importance of experimental hyperparameters in influencing downstream task accuracy and provides insights on the complexity of pretraining methods for small molecules. 2) Strengths:  Thorough ablation studies: The paper performs detailed investigations on various components of graph pretraining shedding light on the factors influencing downstream task accuracy.  Insightful findings: The identification of key factors affecting the effectiveness of selfsupervised pretraining and the impact of supervised pretraining on downstream tasks provides valuable insights for the research community.  Clear presentation: The paper presents a structured analysis of the experimental results making it easy for readers to follow the research methodology and conclusions. 3) Weaknesses:  Limited generalizability: The study focuses on small molecular graphs and may not fully represent the broader landscape of graph pretraining in diverse domains. Extending the analysis to larger graphs or different application domains could enhance the generalizability of the findings.  Lack of comparison with stateoftheart: While the paper discusses previous works on graph pretraining a direct comparison with stateoftheart methods in molecular representation learning would strengthen the conclusions. 4) Questions:  How do the findings of this study contribute to the existing literature on graph pretraining and molecular representation learning  Are there specific types of selfsupervised pretraining tasks that were more effective in certain scenarios and what implications do these findings have for future research in this area  How do the results of this study impact the practical application of GNNs in AIdriven drug discovery and biomedicine 5) Limitations:  Limited datasets and architectures: The study focuses on a specific set of datasets and GNN architectures limiting the generalizability of the findings to other datasets or architectures.  Simplified experimental setup: While the ablation studies provide valuable insights the study might benefit from exploring more complex interactions between pretraining components.  Lack of realworld validation: The conclusions are drawn based on experimental results and realworld validation of the proposed insights could further strengthen the conclusions of the paper. Overall the paper provides valuable insights into the effectiveness of selfsupervised pretraining for molecular representation using GNNs. Addressing the limitations and expanding the scope of the study could enhance the significance and applicability of the findings in the field of AIdriven drug discovery and beyond.", "qmy23tNBvbh": "Peer Review 1) Summary (103 words) The paper proposes the use of kernel exponential and deformed exponential family densities for continuous attention mechanisms. The authors introduce novel theoretical results including existence properties and approximation capabilities for the new density classes. Experimental results show that the proposed kernel continuous attention outperforms unimodal attention with the sparse variant highlighting time series peaks. The paper aims to bridge the gap between existing attention mechanisms and multimodal continuous attention demonstrating the superiority of kernelbased approaches in various practical scenarios. 2) Strengths and Weaknesses: Strengths:  The paper introduces novel kernel exponential and deformed exponential family densities for continuous attention mechanisms.  The theoretical analyses regarding the existence and approximation capabilities of the proposed density classes are significant contributions.  Empirical results demonstrate the superiority of kernel continuous attention over unimodal attention in practical scenarios.  Extensive experiments on synthetic and realworld datasets provide validation of the proposed approach. Weaknesses:  The paper heavily focuses on theoretical derivations and mathematical formulations which might make it challenging for readers not wellversed in the specific domain.  The description of the experiments and results could be more straightforward and detailed and additional discussion on practical implications could enhance the papers clarity.  Further comparisons with stateoftheart methods and a more indepth discussion on limitations and future directions would improve the papers comprehensiveness. 3) Questions:  Can the authors elaborate on the computational complexity and scalability of the proposed kernelbased continuous attention mechanisms especially when dealing with highdimensional data  How do the kernel deformed exponential families compare with other attention mechanisms in terms of interpretability and generalizability  Could the authors provide more context on the suitability of their proposed approach in realworld applications such as natural language processing or computer vision tasks 4) Limitations: The paper would benefit from addressing the following limitations:  The heavy emphasis on theoretical formulations may hinder accessibility and practical applicability for readers outside the specific domain of study.  The potential challenges or drawbacks of implementing the proposed kernel continuous attention mechanisms in largescale applications or resourceconstrained environments are not fully explored.  The generalizability and robustness of the proposed approach to diverse datasets and task domains should be further investigated to validate its applicability beyond the presented experiments. In conclusion the paper presents innovative contributions in the realm of continuous attention mechanisms using kernel exponential and deformed exponential families. Addressing the outlined weaknesses and limitations would enhance the papers overall impact and relevance to both the scientific community and practical applications.", "x7S1NsUdKZ": " Peer Review  1. Summary: This paper explores the Adaptive Sampling for Discovery (ASD) problem focusing on the explorationexploitation tradeoff and proposing the Informationdirected Sampling (IDS) algorithm. The study formulates the ASD problem with various structural assumptions evaluates its performance using theoretical analysis and conducts simulation experiments and realdata experiments in the context of reaction condition discovery in chemistry. The paper establishes the benefits of IDS over traditional methods like UCB and TS in linear graph and lowrank models.  2. Strengths and Weaknesses:  Strengths:  Clear Problem Formulation: The paper rigorously defines the ASD problem and connects it to existing literature on Multiarmed Bandit and Active Learning.  Novel Algorithm: Proposing the IDS algorithm and providing theoretical guarantees for its performance in different models is a significant contribution.  Extensive Experiments: The paper includes simulation studies and realworld experiments to validate the proposed algorithms effectiveness.  Comprehensive Coverage: The paper explores various structural assumptions across linear graph and lowrank models catering to a broad audience.  Weaknesses:  Complexity: Some parts of the paper especially the theoretical analyses may be challenging for readers not familiar with the field.  Lack of Comparative Metrics: While the experiments demonstrated IDS outperforming other algorithms additional metrics or comparisons could provide a more thorough evaluation.  Generalizability: The study focuses on specific domains like chemistry which may limit the generalizability of the proposed IDS algorithm to other fields.  3. Questions:  How does the IDS algorithm compare to stateoftheart methods in tasks beyond the specific domains studied here  Are there specific scenarios or datasets where IDS may not perform as well as expected and if so how can those limitations be addressed  Can the paper provide more insights into the interpretability of the IDS model and how decisions are made in realworld applications  4. Limitations:  Statistical Assumptions: The theoretical analyses heavily rely on assumptions such as linear models or specific structural characteristics which may not always hold in practical scenarios.  Computational Complexity: The paper mentions challenges with sampling from posterior distributions for certain models indicating potential limitations in scalability to largescale applications.  Generalizability: The effectiveness of IDS in diverse discovery tasks beyond the specific problems studied remains to be fully demonstrated. In conclusion this paper makes a valuable contribution to the field of sequential decisionmaking in discovery problems showcasing the potential of the IDS algorithm. Further clarity on the assumptions practical applicability and broader comparisons could enhance the impact of the research.", "wmwgLEPjL9": " Peer Review  Summary: The paper challenges the prevailing notion in the field of multitask learning by proposing that traditional unitary scalarization combined with standard regularization and stabilization techniques can match or surpass the performance of specialized multitask optimizers (SMTOs). Through extensive experiments across supervised and reinforcement learning domains the authors demonstrate that the added complexity and computation overhead of SMTOs may not always be justified. They also provide a technical analysis suggesting that these SMTOs can be interpreted as forms of regularization.  Strengths: 1. Comprehensive Experimental Evaluation: The paper provides a unified experimental pipeline to evaluate various SMTOs across multiple benchmarks including both supervised and reinforcement learning offering a holistic view of the performance of these algorithms. 2. Novel Hypothesis and Analysis: The proposal that SMTOs serve as regularizers is a novel insight that challenges the existing understanding in the field. The technical analysis and experimental evidence provide nuanced support for this hypothesis. 3. Clarity and Detail: The paper is wellstructured providing a detailed overview of the methods experiments and technical results. The substantial technical content enhances the robustness of the arguments presented.  Weaknesses: 1. Potential Bias: The paper while presenting intriguing results may have a bias towards advocating for unitary scalarization as the authors themselves champion this approach. To enhance credibility a more balanced treatment and consideration of potential drawbacks of unitary scalarization could be beneficial. 2. Incomplete Comparison: While the paper addresses a wide range of benchmarks there could be additional experiments or comparisons that could further strengthen the conclusions. Exploring scenarios where SMTOs might outperform unitary scalarization can provide more nuanced insights.  Questions: 1. Generalization to Other Scenarios: How generalizable are the findings of this study across different problem domains network architectures and datasets Can the proposed regularization hypothesis apply universally 2. Consideration of Hyperparameters: How sensitive are the results to hyperparameter choices for the algorithms tested Was there a systematic approach to hyperparameter selection or were they chosen adhoc  Limitations: 1. Resource Limitations: The limited computational resources and grid search for hyperparameters could potentially impact the generalizability of the findings. Replicating experiments at a larger scale could further validate the results. 2. Scope of Analysis: While the paper focuses on a specific set of SMTOs there may be newer more advanced algorithms or approaches in the field of multitask learning that were not considered. A broader analysis could provide a more comprehensive understanding of the landscape. Overall the paper presents valuable insights into the role of regularization in multitask learning and challenges the necessity of complex optimization algorithms. Addressing the highlighted weaknesses and exploring the posed questions could enhance the impact and credibility of the study.", "vbPsD-BhOZ": " Peer Review:  1) Summary: The paper presents a novel framework based on cellular sheaf theory for analyzing and improving the performance of Graph Neural Networks (GNNs) in heterophilic settings and concerns related to oversmoothing behavior. The study links the underlying geometry of graphs with the effectiveness of GNNs by introducing the concept of sheaf diffusion models. Through a rigorous theoretical analysis and realworld experiments the paper demonstrates the advantages of using sheaf structures in GNNs and the ability to learn sheaves from data. The proposed Neural Sheaf Diffusion models show impressive results across various datasets outperforming existing GNN models in handling heterophily and oversmoothing.  2) Strengths and weaknesses: Strengths:  The paper introduces a novel perspective based on cellular sheaf theory to address key challenges faced by GNNs providing a theoretical foundation and experimental validation.  The theoretical analysis is comprehensive covering the expressive power of sheaf diffusion the impact of different types of sheaves on node classification and the ability to learn sheaf structures from data.  Realworld experiments conducted on multiple datasets demonstrate the superiority of the proposed Neural Sheaf Diffusion models over existing GNNs in both heterophilic and homophilic settings.  The paper provides valuable insights into the interplay between graph geometry sheaf structures and GNN performance contributing to the fields of algebraic topology and machine learning. Weaknesses:  The paper may benefit from a clearer explanation or visualization of the sheaf diffusion process and how it captures the underlying graph geometry.  While the results are promising further analyses on the generalization capabilities of sheaf structures and Neural Sheaf Diffusion models could enhance the completeness of the study.  4) Questions:  Could you provide more insights into the interpretability of sheaf diffusion models and how the learned sheaves impact the decisionmaking process in GNNs  How sensitive are the results to the choice of hyperparameters or model architectures in the Neural Sheaf Diffusion models  Have you considered investigating the computational efficiency of the proposed models in comparison to traditional GNNs especially in largescale graph applications  5) Limitations:  The study lacks a thorough exploration of the generalization capabilities of the proposed models which could be crucial for understanding their effectiveness in diverse graph datasets.  While the realworld experiments yield promising results further investigations into the robustness and scalability of the Neural Sheaf Diffusion models are needed. Overall the paper presents a significant advancement in the integration of cellular sheaf theory with Graph Neural Networks offering novel insights into improving GNN performance in various graph settings. The theoretical foundation experimental validation and potential applications make this work a valuable contribution to the fields of machine learning and topology.", "mkEPog9HiV": " Peer Review:  1) Summary: The paper introduces the Neural Sewing Machine (NSM) for 3D garment modeling addressing the limitations of existing methods by proposing a learningbased framework capable of modeling generic garments with diverse shapes and topologies. The NSM utilizes sewing patterns as a strong prior to model the garment surface achieving stateoftheart results in 3D garment reconstruction and manipulation tasks. Extensive experiments demonstrate the effectiveness of NSM in representing 3D garments reconstructing garments from 2D images and allowing controllable garment manipulation.  2) Strengths and Weaknesses: Strengths:  The paper addresses a critical and challenging topic in computer vision and graphics proposing a novel approach for structurepreserving 3D garment modeling.  The use of sewing patterns as a unique prior for garment representation is a notable and innovative contribution.  The detailed explanation of the NSM framework including the sewing pattern encoding 3D garment decoding and structurepreserving learning is comprehensive and wellstructured.  Extensive experiments and comparison with stateoftheart methods showcase the superiority of NSM in various 3D garment modeling tasks. Weaknesses:  The paper lacks a detailed discussion of the computational complexity or efficiency of the proposed NSM framework compared to existing methods.  The limitations section focuses more on technical aspects of the model rather than broader impacts or potential challenges for realworld applications.  The paper could benefit from more visual aids or diagrams to facilitate understanding of the complex architecture and processes involved in NSM.  3) Questions:  Can you elaborate on the process of obtaining the sewing pattern embeddings and how the choice of basic panel groups impacts the overall garment representation  How sensitive is the NSM framework to variations in garment complexity or topology especially in scenarios where garments have irregular or unique panel shapes  What are the key factors influencing the scalability of the NSM model when applied to large datasets or realworld scenarios  How does NSM handle challenges related to garment deformation based on different human poses especially with loose garments or diverse shape variations  4) Limitations:  While the paper extensively discusses the technical aspects of the NSM framework it could benefit from a more indepth discussion on the broader impacts of 3D garment modeling in areas such as fashion industry virtual reality and metaverse applications.  The representation of garment deformation with different human poses is acknowledged as a challenge but additional insights or proposed solutions could enhance the completeness of the study.  The discussion on the limitations of the NSM framework mainly focuses on technical aspects a broader discussion on ethical considerations or societal impacts of realistic garment modeling could add depth to the paper. Overall the paper presents a novel approach to 3D garment modeling with significant potential in computer vision and graphics. Addressing the questions and limitations mentioned above could further enhance the comprehensiveness and practical relevance of the study.  Reference:  Reference formatting and citations need to be added as per the journals guidelines. This work demonstrates a strong contribution to the field of 3D garment modeling and provides a solid foundation for future research in this area. The innovative use of sewing patterns as a modeling prior shows promise for realistic garment representation and manipulation in various applications.", "m7CmxlpHTiu": " Peer Review  1) Summary The paper introduces a novel approach called Selfsupervised Aggregation of Diverse Experts (SADE) for testagnostic longtailed recognition. The research addresses the challenges posed by varying class distributions between training and test data in real applications. SADE consists of two strategies  skilldiverse expert learning and testtime selfsupervised aggregation. The proposed method is theoretically analyzed and empirically tested on both vanilla and testagnostic longtailed recognition tasks showing promising results.  2) Strengths and weaknesses  Strengths:  Innovative Approach: SADE proposes a novel method for handling the practical challenge of testagnostic longtailed recognition.  Theoretical Foundation: The paper provides a theoretical analysis of the proposed method enhancing the understanding of the approach.  Empirical Validation: The empirical results demonstrate the effectiveness of SADE on both vanilla and testagnostic longtailed recognition showcasing superior performance.  Comparative Evaluation: The comparison with existing methods and detailed experimental results provide a comprehensive understanding of the proposed approach.  Open Source: Availability of the source code on GitHub enhances reproducibility and further research.  Weaknesses:  Complexity: The method may seem complex due to the detailed technical description which could potentially hinder its adoption by practitioners.  Empirical Evaluation: The empirical evaluation could be further strengthened by including a broader range of baseline methods and datasets to provide a more comprehensive comparison.  Clarity: Certain sections are dense with technical information and could benefit from clearer explanations for readers less familiar with the topic.  3) Questions  Generalization: How well does the proposed SADE method generalize to datasets beyond those mentioned in the paper  Scalability: Are there scalability concerns or limitations when applying SADE to larger datasets or more complex scenarios  RealWorld Application: How practical is the proposed method for deployment in realworld scenarios given the complexity of the approach  4) Limitations  Dataset Dependency: The performance of SADE is evaluated on selected benchmark datasets there may be limitations on the generalizability of results to other datasets.  Complexity: The complexity of the proposed method may limit its adoption by practitioners who seek more straightforward solutions.  Computational Resources: The computational requirements to train and deploy SADE especially in realtime applications could be a limiting factor. Overall the paper presents a novel and promising approach to addressing the challenges of testagnostic longtailed recognition. The theoretical foundation experimental validation and comparison with existing methods strengthen the contribution of the research. Further improvements in clarity and broader empirical evaluations would enhance the impact of the proposed method.", "kUOm0Fdtvh": " Peer Review  1) Summary The paper introduces a novel calibrationaware adaptive focal loss AdaFocal aimed at improving calibration in neural networks during training. By adjusting the parameter \u03b3 for each training sample based on the models underoverconfidence and knowledge from the validation set AdaFocal significantly enhances calibration across various image and text classification tasks. The method outperforms existing techniques like FLSD53 and demonstrates enhanced outofdistribution detection capabilities.  2) Strengths and Weaknesses Strengths:  Introduces a novel method AdaFocal which addresses the overunderconfidence issue in neural network calibration.  Demonstrates superior calibration performance compared to existing methods across different datasetmodel pairs.  Provides a comprehensive evaluation on various image and text classification tasks highlighting the effectiveness of AdaFocal.  Improves outofdistribution detection showcasing the practical benefits of calibration. Weaknesses:  The paper lacks detailed explanations of the experimental setup such as hyperparameter tuning model architectures used and training specifics.  Some terms and concepts related to focal loss and calibration could be explained more clearly for readers unfamiliar with these topics.  The paper could benefit from a more detailed comparative analysis with other stateoftheart methods to further highlight the superiority of AdaFocal.  3) Questions  How does the performance of AdaFocal compare with other calibration methods that adjust parameters based on the models performance on the validation set  Can the authors provide more insights into the hyperparameter selection process for AdaFocal particularly the choice of \u03bb \u03b3minmax and Sth  Have you considered the computational efficiency and scalability of AdaFocal when applied to larger datasets or more complex models  4) Limitations  The paper lacks a thorough discussion of the potential limitations or challenges associated with implementing AdaFocal in realworld scenarios.  The generalizability of AdaFocal to various types of neural network architectures or different tasks could be further explored.  An indepth analysis of the robustness of AdaFocal to noise or data perturbations would enhance the credibility of the proposed method. Overall the paper introduces a promising approach for improving neural network calibration during training through AdaFocal. Further elaboration on experimental details comparative analyses and potential limitations would strengthen the papers impact in the field of calibrationaware training methods.", "thirVlDJ2IL": " Peer Review:  Summary: The paper revisits the problem of learning mixtures of spherical Gaussians with a focus on the lowdimensional regime where the dimension is logarithmic in the number of clusters. The authors propose an algorithm that efficiently learns the means of Gaussian mixtures in d  O(log k log log k) dimensions under a separation condition. They establish the critical separation threshold for efficient learning and extend their algorithm to learning mixtures of nonGaussian distributions. The analysis is based on estimating the Fourier transform of the mixture at carefully chosen frequencies.  Strengths and Weaknesses: Strengths: 1. The paper addresses an important problem in machine learning with practical implications. 2. The algorithm proposed is efficient and fixedparameter tractable in parameters d and \\xe2\\x9c\\x8f. 3. The analysis is simple and elementary enhancing the accessibility of the work. 4. The results provide insights into the critical separation threshold for efficient learning. Weaknesses: 1. The complexity and technical depth of the paper might make it less accessible to readers with limited background in this specific area. 2. The limitation to the lowdimensional regime might restrict the generalizability of the results. 3. The paper primarily focuses on theoretical aspects without practical demonstrations or realworld applications.  Questions: 1. Can the proposed algorithm handle noise in the data and if so how sensitive is it to different noise levels 2. Have the authors considered applying their algorithm to realworld datasets or simulating scenarios to demonstrate the practical utility of their approach 3. How robust is the algorithm to variations in the separation and dimension parameters and how does its performance degrade in extreme cases 4. Are there any specific assumptions made in the algorithm that could limit its applicability in realworld scenarios  Limitations: 1. The positive results are constrained to specific dimensional and separation regimes limiting the applicability to a broader range of scenarios. 2. The algorithms practical significance is not extensively explored through realworld experiments or applications. 3. The focus on theoretical analysis may limit the understanding of the algorithms behavior in realistic settings. In conclusion the paper presents valuable contributions to the field of learning mixtures of Gaussian distributions providing insights into critical separation thresholds and efficient parameter learning in low dimensional spaces. Further exploration into the practical implications and robustness of the algorithm could enhance the impact of the work.", "vriLTB2-O0G": "Peer Review Summary: The paper proposes a novel learningbased method called Pareto set learning (PSL) to approximate the whole Pareto set for expensive multiobjective optimization problems with limited evaluation budgets. The method aims to overcome the limitations of existing finiteset approximations by generalizing the decompositionbased multiobjective optimization algorithm. PSL utilizes surrogate models and a set model to map tradeoff preferences to corresponding Pareto solutions. The paper demonstrates the effectiveness of PSL through experiments on synthetic and realworld problems highlighting its efficiency usefulness and support for flexible decisionmaking. Strengths: 1. Innovation: The paper introduces a novel approach PSL which represents a significant innovation in the field of multiobjective optimization by aiming to approximate the entire Pareto set. 2. Evaluation: The experimental evaluation on synthetic benchmarks and realworld problems demonstrates the effectiveness and efficiency of PSL compared to existing methods. 3. Modeling Approach: The use of surrogate models and a set model to map tradeoff preferences to Pareto solutions provides a structured and efficient way to explore the whole approximate Pareto set. 4. Flexibility: PSL allows decisionmakers to interactively explore the Pareto front facilitating more informed and flexible decisionmaking. 5. Clear Presentation: The paper is wellstructured with a clear introduction of the problem statement detailed methodology and thorough explanation of experiments. Weaknesses: 1. Clarity: While the paper is wellorganized certain sections particularly the algorithmic details could be more clearly explained for readers not familiar with the specific methodologies employed. 2. Discussion: The paper could benefit from a more indepth discussion on the limitations and constraints of the proposed method including scenarios where PSL may not be as effective. Questions: 1. How does the proposed PSL method handle highdimensional optimization problems and were there any specific challenges encountered in these cases during the experiments 2. Can the authors provide insights into the computational overhead of the PSL method compared to other MOBO approaches especially during iterative optimization processes 3. How does the performance of PSL vary with different evaluation budget constraints and were there any considerations regarding the scalability of the method for largerscale applications Limitations: 1. Surrogate Models: The accuracy of the PSL method heavily relies on the quality of surrogate models employed. The paper would benefit from discussing potential vulnerabilities arising from inaccuracies in the surrogate models. 2. Evaluation Constraints: The experimental evaluation while comprehensive primarily focuses on synthetic and specific realworld problems. Generalizability to a wider range of applications may pose challenges. Overall the proposed PSL method presents a promising direction for advancing the field of expensive multiobjective optimization. The papers strengths lie in its innovation experimental evaluation and usercentric approach to decisionmaking. Addressing areas of clarity scalability and generality will further enhance the impact of the research.", "tVHh_vD84EK": " Peer Review  1) Summary The paper proposes a universal spatiotemporal modeling framework UniST to address the modeling gap in spatiotemporal forecasting tasks. It introduces three distinctive modeling units (S2T T2S STS) and an automatic search strategy AutoST to optimize the spatiotemporal modeling priority. Extensive experiments demonstrate that UniST outperforms baseline methods and achieves stateoftheart performance on various tasks.  2) Strengths and Weaknesses Strengths:  Novel Approach: The paper introduces a unique framework with distinct modeling units and an automatic search strategy addressing an important issue in spatiotemporal forecasting.  Experimental Results: Extensive experiments on realworld datasets demonstrate the superiority of UniST and AutoST over baseline methods across various forecasting tasks.  Visualization: The paper provides clear visualizations of forecasting results and learned architectures enhancing the understanding of the proposed methods. Weaknesses:  Lack of Comparison: The paper could benefit from a more indepth comparison with existing models or frameworks with similar objectives to highlight the specific advantages of UniST and AutoST.  Explanation of Results: While the results are presented effectively a deeper analysis or discussion on why certain methods or configurations perform better could enhance the papers impact.  Complexity: The proposed models and search strategies may be complex which could hinder practical implementation and scalability in certain scenarios.  3) Questions  How does the proposed framework adapt to different types of spatiotemporal datasets with varying complexities  Are there any computational or memory limitations when scaling the proposed models to larger datasets or more complex forecasting tasks  Can the findings from the experiments on realworld datasets be generalized to other domains beyond spatiotemporal forecasting  4) Limitations  Generalization: The effectiveness of UniST and AutoST on specific forecasting tasks needs to be validated on a broader set of datasets and applications to ensure generalizability.  Interpretability: The interpretability of the learned architectures and the decisionmaking process of AutoST could be further explored to enhance understanding and trust.  Scalability: The scalability of the proposed framework to handle massive and highdimensional spatiotemporal data needs to be investigated for practical deployment in realworld scenarios. The paper presents an innovative approach to address spatiotemporal modeling challenges supported by comprehensive experiments and visualizations. By addressing the identified weaknesses and limitations the frameworks utility and applicability can be further strengthened.", "ohk8bILFDkk": " Peer Review 1) Summary The paper introduces a novel framework SemiInfinitely Constrained Markov Decision Process (SICMDP) as a generalization of Constrained Markov Decision Processes (CMDP). The authors develop a reinforcement learning algorithm SICRL to solve SICMDP problems. The study includes theoretical analysis on sample complexity and iteration complexity of SICRL along with extensive numerical experiments to validate the proposed model and algorithm. 2) Strengths and weaknesses Strengths:  Introduction of a novel framework SICMDP addressing problems with a continuum of constraints.  Development of SICRL algorithm for solving SICMDP problems.  Theoretical analysis provides insights into sample complexity and iteration complexity of SICRL.  Extensive numerical experiments validate the proposed framework and algorithm efficacy. Weaknesses:  The applicability and efficiency of SICRL algorithm are evaluated only for tabular cases with an offline dataset.  The empirical validation could be further expanded to diverse and more complex scenarios to demonstrate the generalizability of the proposed approach. 3) Questions  How does the SICRL algorithm deal with highdimensional or continuous state and action spaces considering the focus on tabular cases in the current study  Are there plans to extend the algorithm to handle online dataset generation or continuous reinforcement learning scenarios 4) Limitations  The study primarily focuses on tabular cases and offline dataset settings limiting the practical applicability of the proposed SICRL algorithm.  The assumption of generative modelbased dataset may not fully represent realworld scenarios where data generation might be more complex. Overall the paper presents a novel framework and algorithm for addressing reinforcement learning problems with continuous constraints with strong theoretical foundations and promising empirical results. However future work should focus on extending the algorithm to handle more realistic and complex scenarios to enhance its practical utility and generalizability.", "wQ2QNNP8GtM": " Peer Review  1) Summary: The paper introduces a novel image restoration model called Cross Aggregation Transformer (CAT) that leverages the Transformer architecture for image restoration tasks. The core innovation of CAT is the RectangleWindow SelfAttention (RwinSA) mechanism which allows for feature aggregation across different windows to model longrange dependencies effectively. Additionally the paper proposes the AxialShift operation and Locality Complementary Module to enhance the performance of the model. Extensive experiments demonstrate that CAT outperforms existing stateoftheart methods in image restoration applications.  2) Strengths and weaknesses: Strengths:  Novel Contribution: CAT introduces innovative mechanisms like RwinSA AxialShift and Locality Complementary Module which enhances the Transformer models capabilities in image restoration.  Experimental Results: Extensive experiments showcase the superior performance of CAT compared to existing methods across different image restoration tasks.  Availability: The availability of code and models on GitHub enhances reproducibility and facilitates further research in the field. Weaknesses:  Complexity: The paper might benefit from a more detailed discussion on the computational complexity of the proposed model especially in comparison to existing methods.  Generalization: While the experiments demonstrate effectiveness across various image restoration tasks additional analyses on the generalizability of CAT to different datasets and scenarios would strengthen the paper.  Comparison: A more detailed comparison with existing works including a broader spectrum of methods and additional metrics could provide a more comprehensive evaluation.  3) Questions:  How does the performance of CAT vary in scenarios with different levels of noise or image degradation compared to existing methods  Are there specific limitations or drawbacks of the proposed mechanisms (RwinSA AxialShift) that need to be addressed in future research  How does the proposed model manage spatial information in comparison to existing CNNbased methods especially in regions with detailed textures or structures  4) Limitations:  Dataset Bias: The performance of CAT on a diverse set of datasets is crucial to evaluate its generalizability and potential biases towards specific types of images.  Scalability: While the paper presents results on specific tasks the scalability and applicability of CAT to larger and more complex image restoration scenarios need to be explored.  Interpretability: A discussion on the interpretability of the proposed mechanisms and how they contribute to the models performance could enhance the understanding of CATs inner workings. Overall the paper presents a novel approach to image restoration leveraging Transformer architecture and introduces innovative mechanisms to address longrange dependencies effectively. Further analysis clarity on computational complexity and broader comparisons with existing methods would enhance the impact and applicability of the proposed CAT model.", "rH-X09cB50f": " Peer Review:  Summary: The paper presents an empirical investigation into the effectiveness of supervised learning (SL) and selfsupervised learning (SSL) for crossdomain fewshot learning (CDFSL). It explores the impact of domain similarity and fewshot difficulty on CDFSL pretraining schemes proposing insights on when to prefer SL SSL or a mixedsupervised learning (MSL) approach. Twostage pretraining schemes are also evaluated. Extensive experiments on multiple datasets support the findings highlighting the performance gains of SSL over SL and the benefits of synergy between SL and SSL.  Strengths: 1. Novelty and Importance: The paper addresses a pertinent issue in realworld scenarios by investigating pretraining schemes in CDFSL a topic of increasing interest in machine learning. 2. Comprehensive Experiments: The extensive experiments on multiple datasets and detailed analyses provide robust evidence to support the proposed insights. 3. Clarity: The document is wellstructured with clear definitions of concepts metrics and experimental setups.  Weaknesses: 1. Theoretical Discussion: While the empirical analysis is thorough the paper lacks theoretical discussions to complement the experimental findings and provide deeper insights into the underlying mechanisms. 2. Generalization: The focus on empirical results limits the generalizability of the conclusions more theoretical insights could help in generalizing the findings to different scenarios. 3. Comparison with StateoftheArt: It would be beneficial to compare the proposed approaches with existing stateoftheart methods in CDFSL to ascertain their competitiveness.  Questions: 1. How do the findings of this research align with existing theories or models in fewshot learning and transfer learning 2. Could the authors elaborate on the reasons behind the varying performance gains of SSL over SL in different datasets based on domain similarity and fewshot difficulty 3. Is the proposed twostage pretraining approach flexible enough to be extended to more complex models or scenarios in CDFSL  Limitations: 1. Scope: The focus on empirical analysis limits the depth of theoretical discussions and may restrict the generalizability of findings. 2. Benchmark Datasets: While the paper uses multiple datasets the choice of benchmark datasets and their representation of realworld scenarios could be discussed further for better understanding and applicability. 3. Algorithm Complexity: The paper mainly focuses on the comparison of SL SSL and MSL without delving into more complex pretraining techniques which could be considered in future research to enhance performance. The paper presents valuable insights into CDFSL pretraining schemes but could benefit from a deeper theoretical discussion and comparisons with stateoftheart methods to fully assess its contributions to the field.", "s6ygs1UCOw1": " Peer Review  1) Summary The paper presents a novel algorithm for constructing PAC prediction sets in the context of meta learning focusing on quickly adapting predictors to new tasks with limited training examples. The proposed algorithm aims to provide uncertainty quantification with guarantees tailored to the metalearning setting demonstrating its efficacy on datasets from visual language and medical domains. The approach is shown to satisfy the PAC guarantee with smaller prediction set sizes compared to existing baselines.  2) Strengths and Weaknesses Strengths:  The paper addresses an important problem in machine learning related to uncertainty quantification in safetycritical systems.  The proposed algorithm is novel and tailored to the metalearning setting offering PAC guarantees over future tasks.  Experimental results on various datasets across different application domains demonstrate the effectiveness of the approach.  The paper provides a clear theoretical background and algorithm details to support the proposed method. Weaknesses:  The paper lacks a detailed comparison with more existing methods in the metalearning domain for a comprehensive evaluation.  The paper can benefit from more indepth discussions on the limitations and potential extensions of the proposed approach.  The practical implications and realworld applicability of the proposed method could be further elaborated.  3) Questions  Can the proposed algorithm generalize well to more diverse and complex datasets outside the ones tested in the experiments  How does the performance of the proposed algorithm scale with the size of the training data and the complexity of tasks  Are there any considerations for realtime deployment and computational efficiency of the algorithm in practical applications  4) Limitations  The paper focuses on demonstrating efficacy through empirical experiments but lacks thorough analysis of scalability and robustness under varying conditions.  The proposed algorithms reliance on sufficient calibration data points for PAC guarantees might limit its applicability in scenarios with limited annotated data. Overall the paper presents a valuable contribution to uncertainty quantification in meta learning providing a novel algorithm with promising results across multiple domains. Additional experiments and discussions on scalability robustness and practical considerations could enhance the completeness of the work.", "kxXvopt9pWK": " Peer Review 1) Summary (avg word length 100): The paper introduces Denoising Diffusion Restoration Models (DDRM) an efficient unsupervised method for solving general linear inverse problems in image restoration. DDRM leverages pretrained denoising diffusion generative models to handle tasks like superresolution deblurring inpainting and colorization. Empirical evaluations demonstrate DDRMs superiority over existing unsupervised methods in terms of quality versatility and runtime efficiency particularly on the ImageNet dataset. 2) Strengths:  The paper addresses an important issue in image restoration by introducing an efficient unsupervised method for solving general linear inverse problems.  DDRMs utilization of pretrained denoising diffusion generative models showcases a novel approach towards achieving highquality results.  The empirical evaluations on various image datasets demonstrate DDRMs versatility and performance superiority over existing unsupervised methods particularly on the ImageNet dataset.  By outperforming competitors in reconstruction and perceptual quality while being faster DDRM establishes itself as a promising solution for image restoration tasks. Weaknesses:  The paper might benefit from a more substantial discussion on the limitations and potential areas for improvement of DDRM.  While the methodology and results are welldetailed a more indepth comparison with other stateoftheart methods could provide further context for DDRMs performance. 3) Questions:  Can you clarify how DDRM handles noise in the measurements during the restoration process and what mechanisms are in place to mitigate noise artifacts in the results  How does DDRM generalize to different types of images and datasets beyond those used in the empirical evaluation and are there any specific challenges or considerations when working with diverse image content  Could you elaborate on the implementation details of DDRM particularly the computational efficiency aspects that enable it to outperform competitors in terms of runtime 4) Limitations:  The paper focuses primarily on the efficacy and efficiency of DDRM in solving linear inverse problems for image restoration tasks but lacks a deeper exploration of its performance in more complex or nonlinear inverse problems.  The evaluation of DDRM is centered on specific image datasets raising questions about its effectiveness on more diverse or specialized datasets and the generalizability of the approach across varied scenarios.  The discussion on how DDRM might handle scenarios with unknown degradation operators or selfsupervised training techniques could provide valuable insights into the methods adaptability to realworld applications beyond the scope of this study. Overall the paper presents a novel approach in image restoration with DDRM demonstrating promising results in terms of efficiency and quality. Further exploration and validation across a broader range of scenarios and datasets could enhance the comprehensiveness and applicability of DDRM in realworld image processing tasks.", "x5ysKCMXR5s": " Peer Review:  Summary: The paper introduces a novel algorithm for sparse principal component analysis (PCA) of incomplete and noisy data using a semidefinite program (SDP) relaxation. The algorithm aims to recover the true support of the sparse leading eigenvector despite observing incomplete and noisy data. Theoretical conditions for exact recovery are derived and validated experimentally on synthetic and gene expression datasets. The algorithm outperforms existing sparse PCA methods in terms of support recovery.  Strengths: 1. The paper addresses a significant problem in machine learning by proposing a computational friendly convex optimization solution for sparse PCA on incomplete data. 2. The theoretical basis of the algorithm is comprehensive and includes conditions involving matrix coherence spectral gap observation probability noise variance and sample complexity. 3. Experimental results on synthetic and real gene expression data validate the theoretical claims and show the practical applicability of the proposed algorithm. 4. The comparison with other sparse PCA methods demonstrates the superiority of the proposed SDP algorithm in recovering the true support. 5. The paper is wellstructured with a clear introduction theoretical background experimental validation and conclusions.  Weaknesses: 1. The paper lacks discussion on potential realworld applications and implications of the proposed algorithm beyond the gene expression dataset. 2. Limited details are provided on the specific data preprocessing steps and parameter selection for the experimental validation. 3. The paper could benefit from additional visual aids such as figures or tables to better illustrate the experimental results and comparisons. 4. The computational complexity and scalability of the proposed algorithm could be further discussed especially concerning largescale datasets.  Questions: 1. Can the proposed algorithm be extended to handle different types of missing data patterns beyond the uniformly missing at random setup 2. How sensitive is the algorithm to parameter tuning particularly in realworld scenarios with complex data distributions 3. Are there any constraints or limitations in terms of dimensionality that may affect the algorithms performance 4. How would the algorithm perform on datasets with extreme levels of missing data such as above 90  Limitations: 1. The analysis is primarily focused on uniformly missing at random setups limiting the generalizability to more complex missing data structures. 2. The paper lacks a detailed discussion on the computational efficiency and scalability of the proposed algorithm for largescale datasets. 3. The experimental validation could be further expanded to include diverse datasets and scenarios to assess the algorithms robustness and generalization capabilities.", "qOgSCLE5E8": " Summary The paper proposes a novel hierarchical Optimal Transport (HOT) framework for fewshot learning specifically focusing on distribution calibration between base and novel classes. By introducing a lowlevel OT to learn the cost function and a highlevel OT to measure the distance between base and novel classes the model achieves superior results in standard benchmarks and demonstrates better crossdomain generalization ability compared to existing methods. The HOT is shown to effectively transfer the statistics of base classes to novel samples providing a principled and adaptive approach for improving fewshot learning tasks.  Strengths 1. The paper addresses an important problem in fewshot classification and proposes a novel method HOT which leverages Optimal Transport. 2. The hierarchical approach of learning the transfer weights between base and novel classes is a key strength ensuring more effective distribution calibration. 3. Extensive experiments are conducted on standard benchmarks demonstrating the effectiveness and superiority of the proposed model over existing methods. 4. The thorough evaluation includes comparisons with stateoftheart approaches and demonstrates the models performance in crossdomain scenarios.  Weaknesses 1. The paper could provide more detailed explanations or visualizations of the model architecture and how the adaptive weights are learned between base and novel classes. 2. The limitations or failures of the proposed method should be discussed to provide a wellrounded review of the approach. 3. The text is dense and could benefit from clearer transitions between sections for better readability.  Questions 1. Can the proposed HOT framework be extended or adapted to tackle other types of classification tasks beyond fewshot learning 2. How sensitive is the models performance to hyperparameter choices such as the entropic regularization \\xcf\\xb5 or the weight \u03b1 in Equation (10) 3. How scalable is the proposed approach in terms of computational resources and time for larger datasets or more complex scenarios  Limitations 1. The paper lacks indepth discussion on the computational costs or scalability of the proposed method which could be crucial for practical applications. 2. The absence of an analysis on the interpretability of the learned adaptive weights and their impact on the models decisionmaking process is a limitation in understanding the models inner workings. 3. The paper could delve deeper into the robustness of the model to noisy or skewed data distributions which are common challenges in realworld scenarios. Overall the paper presents a promising contribution to the field of fewshot learning with its novel HOT framework demonstrating superior performance and crossdomain generalization abilities. More thorough explanations detailed discussions on limitations and further investigations into interpretability and scalability will enhance the impact of the proposed method.", "pcgMNVhRslj": "Peer Review 1) Summary: The paper introduces Alignmentguided Temporal Attention (ATA) a novel approach for temporal modeling in video learning tasks. It addresses the efficiency and sufficiency of interactions among frames by proposing framebyframe alignments to increase mutual information between neighboring frame representations. The authors provide theoretical proofs and practical implementations demonstrating the effectiveness of ATA in enhancing taskrelevant information without additional parameters. Extensive experiments on various benchmarks showcase the superiority and generality of ATA as a plugin module for improving action recognition tasks. 2) Strengths and weaknesses: Strengths:  The paper introduces a novel concept of framebyframe alignments for enhancing temporal modeling in video action recognition tasks.  The theoretical analysis and proofs provided to support the proposed ATA approach are rigorous and wellstructured.  Extensive experiments on multiple benchmarks demonstrate the superiority and generality of ATA compared to conventional temporal modeling methods.  The proposal of ATA as a plugin module for extending image backbones for video learning tasks adds practical relevance and applicability to the research. Weaknesses:  The paper lacks a direct comparison with stateoftheart alignment methods in video action recognition limiting the evaluation of ATAs performance against existing approaches.  The theoretical analysis may be challenging to grasp for readers not wellversed in information theory or deep learning concepts potentially hindering the accessibility of the proposed methodology.  While the extensive experiments show promising results additional ablation studies on specific aspects of the ATA approach could enhance the understanding of its impact on different video architectures. 3) Questions:  How does the proposed Alignmentguided Temporal Attention approach compare to other alignment methods in terms of computational efficiency and model performance  Could the authors provide more detailed insights into the interpretability of the alignment guidance and its impact on the learning process of video action recognition models  Are there any plans to explore the scalability of ATA to larger datasets or more complex video recognition tasks in future research 4) Limitations:  The proposed ATA approach being parameterfree and not differentiable might limit its performance potential. Addressing this limitation by exploring differentiable alignment techniques or soft alignment strategies could enhance the models capabilities.  The reliance on framebyframe alignments may pose challenges in capturing longterm temporal dependencies or complex interactions in videos with dynamic content. Exploring adaptive alignment mechanisms or hybrid approaches to address diverse temporal patterns could further improve ATAs effectiveness.  The study mainly focuses on action recognition tasks further investigations into the applicability of ATA in other video learning tasks such as video captioning anomaly detection or video generation could broaden the scope and impact of the proposed approach. Overall the paper presents a novel and theoretically wellfounded approach to temporal modeling in video tasks. The proposed ATA method shows promise in enhancing taskrelevant information extraction and model performance. Addressing the identified weaknesses and limitations could further solidify the contribution and impact of the research in the field of video learning and action recognition.", "uxc8hDSs_xh": " Summary The paper introduces a novel approach using a geometric scatteringbased graph neural network (GNN) for approximating solutions to the maximum clique (MC) problem a wellknown NPhard problem. The method incorporates scattering transform to address the oversmoothing issue faced by traditional GNNs. It achieves superior performance in terms of solution accuracy and inference speed compared to existing GNN baselines and conventional solvers with significantly fewer parameters.  Strengths 1. Innovation: The paper introduces a novel approach incorporating geometric scattering into GNNs to address the oversmoothing problem achieving significant performance improvement. 2. Performance: The proposed method outperforms existing GNN baselines and traditional solvers like Gurobi showcasing superior solution accuracy and inference speed. 3. Efficiency: With only 0.1 of the parameters compared to previous GNN baseline models the proposed scattering model offers a parameterefficient solution. 4. Empirical Results: The experimental results demonstrate the effectiveness of the proposed method on realworld datasets showcasing competitive performance.  Weaknesses 1. Evaluation: The paper lacks a detailed comparison with a wider range of baselines including more traditional heuristics and solvers to provide a comprehensive evaluation. 2. Explanation Complexity: Some sections especially those detailing the background and related work could be simplified for better readability by a broader audience.  Questions  How generalizable is the proposed method to other combinatorial optimization problems beyond the maximum clique problem  Can the same approach be applied to larger and more complex graphs and how would the performance scale  How sensitive is the proposed method to hyperparameters and are there specific considerations for tuning these parameters effectively  Limitations 1. Complexity of Implementation: The proposed method may be challenging to implement and require a considerable level of expertise in graph neural networks and related concepts. 2. Dataset Consideration: The study evaluates the method on a specific set of datasets further assessment on more diverse datasets could elucidate its generalizability. 3. Scalability: Although the method performs well on the proposed datasets the scalability of the approach to even larger graphs needs to be tested. Overall the paper presents a novel and promising approach to solving the maximum clique problem using geometric scatteringbased GNNs with notable strengths in terms of performance and efficiency. Further exploration and validation across diverse datasets and broader optimization problems would enhance the significance of the proposed method.", "pELM0QgWIjn": " Summary This paper introduces novel quasiNewton methods for solving stronglyconvexstronglyconcave saddle point problems. The proposed algorithms approximate the square of the Hessian matrix providing explicit local superlinear convergence rates. The study includes two specific Broyden family algorithms: one with BFGStype updates and another with SR1type updates. Theoretical analysis and numerical experiments demonstrate the effectiveness of these algorithms in outperforming classical firstorder methods.  Strengths and Weaknesses Strengths: 1. The paper introduces innovative quasiNewton methods for a specific class of optimization problems. 2. The theoretical analysis provides clear convergence rates and conditions for the proposed algorithms. 3. The numerical experiments validate the superiority of the proposed algorithms over classical firstorder methods. 4. Detailed explanations and algorithms are provided enhancing the reproducibility of the study. Weaknesses: 1. The paper lacks comparisons with other stateoftheart methods for similar problems. 2. The complexity and space requirements are mentioned as limitations which could impact scalability in practical applications. 3. Further theoretical discussions or practical implications could enhance the depth of the study.  Questions 1. How do the proposed Broyden family algorithms compare with other quasiNewton methods such as DFP or SR1 in terms of convergence rates and computational efficiency 2. Could the authors elaborate on how the proposed algorithms handle noise or uncertainties in the objective function particularly in nonconvex scenarios 3. Are there any insights on how these quasiNewton methods perform on largescale datasets or highdimensional problems compared to conventional optimization techniques  Limitations 1. The study focuses on specific types of saddle point problems and may not generalize well to a broader class of optimization problems. 2. The computational cost and space complexity increase with the higher dimensions of the problems limiting the scalability of the proposed algorithms. 3. The experimental validation is limited to specific machine learning applications which may not fully capture the performance of the algorithms in diverse scenarios. Overall this paper provides valuable insights into quasiNewton methods for saddle point problems but further exploration and evaluations could strengthen the robustness and applicability of the proposed algorithms.", "m8vzptcFKsT": " Peer Review  1. Summary The paper investigates the average robustness of deep neural networks under different settings including widenarrow deepshallow lazynonlazy training regimes. The findings demonstrate the impact of width and depth on robustness with varying effects based on initialization and training mode. The research provides new insights into the interplay between network parameters and robustness supported by empirical and theoretical evidence.  2. Strengths and Weaknesses Strengths:  The paper addresses an important problem related to the robustness of deep neural networks.  The theoretical analysis is thorough providing insights into the impact of width depth and initialization on average robustness.  The use of numerical experiments to validate theoretical findings enhances the credibility of the results.  The paper is wellstructured with clear explanations of complex concepts. Weaknesses:  The paper may benefit from more detailed explanations in certain sections to aid comprehension especially regarding the specific theoretical derivations.  The paper could elaborate more on the practical implications of the results for realworld applications and potential future research directions.  3. Questions  Could the authors provide more insights into how the findings on robustness could be practically applied to improve the performance of realworld deep neural networks  How do the results of this study contribute to the existing literature on the robustness of deep neural networks and what novel insights do they bring  Can the authors elaborate on the limitations of the study and potential areas for future research based on the findings  4. Limitations  The study primarily focuses on theoretical analysis and numerical experiments. More extensive empirical validation on a variety of datasets and network architectures could further strengthen the findings.  The assumptions made in the theoretical analysis such as data distribution and model initialization may not fully capture the complexities of realworld scenarios. Overall the paper presents valuable insights into the relationship between network parameters and robustness in deep neural networks. With further enhancements in clarity practical implications and empirical validation the research could have a significant impact on advancing the understanding of network robustness in machine learning applications.", "zFW48MVzCKC": " Summary: The paper introduces a novel value factorization framework called QSCAN in the realm of cooperative multiagent reinforcement learning. QSCAN aims to represent subteam coordination while maintaining the individualglobalmax (IGM) condition. The framework is positioned as an improvement over existing methods by flexibly capturing coordination within subteams ultimately leading to faster learning. Experimental results demonstrate the superior performance of QSCAN in a variety of tasks compared to stateoftheart methods.  Strengths: 1. Innovative Framework: QSCAN introduces a novel approach to value factorization that addresses the limitations of existing architectures. 2. Extensive Experimental Evaluation: The paper includes a thorough evaluation of QSCAN against stateoftheart methods across various scenarios demonstrating its effectiveness. 3. Theoretical Analysis: The paper provides a comprehensive theoretical foundation for the proposed QSCAN framework enhancing its credibility. 4. Clear Explanations: The detailed discussions on subteam coordination patterns architecture design and empirical results provide a clear understanding of the proposed framework.  Weaknesses: 1. Complexity: The paper dives deep into technical details which may be challenging for readers who are not experts in the field. 2. Assumption Clarity: Some assumptions made in the theoretical discussions could be articulated more explicitly to enhance clarity. 3. Comparison Metrics: It would be beneficial to include a more detailed comparison of QSCANs performance against the baselines in terms of specific metrics or statistical significance.  Questions: 1. What are the specific criteria for determining subteam size in the QSCAN framework and how does this affect the frameworks performance 2. Can you elaborate on the computational complexity of QSCAN compared to existing methods especially as subteam size increases 3. How does QSCAN adapt to dynamic environments where the optimal coordination patterns may change over time 4. Are there any additional considerations or extensions of QSCAN that could further enhance its applicability to a wider range of MARL scenarios  Limitations: 1. General Applicability: While QSCAN performs well in various tasks the generalizability of its performance across a wider range of complex MARL scenarios needs further investigation. 2. Scalability: The scalability of QSCAN to scenarios involving a large number of agents or intricate coordination requirements could be a potential limitation. 3. Realworld Implementation: Practical considerations and constraints involved in implementing QSCAN in realworld multiagent systems could pose challenges that need to be addressed. Overall the paper presents a novel and promising framework in the field of MARL but further research and validation on practical applications and scalability are warranted.", "qHs3qeaQjgl": " Peer Review  Summary: The paper proposes a novel algorithm Barbarik3 for testing constrained samplers over highdimensional distributions with guarantees of closeness farness and confidence. Compared to the existing stateoftheart algorithm Barbarik2 Barbarik3 demonstrates an exponential improvement in query complexity making it more scalable for practical use. The experimental results show that Barbarik3 outperforms Barbarik2 significantly in terms of sample efficiency with substantial speedups observed in both scalable and reallife benchmarks.  Strengths: The paper addresses an important problem in testing constrained samplers over highdimensional distributions catering to the increasing use of samplers in ML applications. The proposed algorithm Barbarik3 offers a substantial improvement over the existing stateoftheart Barbarik2 with a linear query complexity in n. Extensive empirical evaluations on both scalable and reallife benchmarks demonstrate the effectiveness and efficiency of Barbarik3 in comparison to Barbarik2 showcasing significant improvements in terms of sample requirements. The theoretical analysis is wellstructured and the algorithm is clearly explained with detailed pseudocode and explanations of the oracles used in the testing process. The experiments are conducted on a highperformance compute cluster and the results are presented in a comprehensive manner including visual representations for better understanding.  Weaknesses: The paper lacks a detailed discussion on the choice of specific parameter values (\u03b5 \\xce\\xb7 \u03b4) and their implications on the testing process. Providing insights into how these parameters were selected and their impact on the algorithms performance would strengthen the analysis. While the experimental results demonstrate the superior performance of Barbarik3 compared to Barbarik2 additional discussion on the practical implications of these findings in realworld applications would enhance the papers relevance. The limitations section could be more elaborate highlighting potential challenges or scenarios where Barbarik3 may not be as effective providing a more comprehensive view of the algorithms applicability.  Questions: How does the proposed algorithm handle cases where the distribution samplers deviate significantly from the desired distribution Are there scenarios where Barbarik3 may struggle in detecting such deviations Can you provide more insights into the computational efficiency of Barbarik3 compared to Barbarik2 in terms of runtime complexity and resource utilization especially for larger instances and datasets How sensitive is Barbarik3 to variations in the input parameters (\u03b5 \\xce\\xb7 \u03b4) and are there specific scenarios where parameter tuning is crucial for optimal performance  Limitations: While the experimental evaluations provide valuable insights the study is limited by the computational resources and the fixed sample limit of 108 samples potentially restricting the scalability analysis for larger datasets or more complex distributions. Overall the paper presents a novel algorithm for testing constrained samplers with compelling results in terms of sample efficiency and scalability. Addressing the weaknesses and providing additional insights into practical implications could further enhance the impact and relevance of the work in the field of ML applications.", "r0bjBULkyz": " Peer Review  1) Summary: The paper introduces Active Bayesian Causal Inference (ABCI) a novel framework for integrated causal discovery and reasoning using actively collected interventional data. The framework is based on a fully Bayesian active learning approach and is particularly suited for nonlinear additive noise models modeled using Gaussian processes. The paper demonstrates through simulations that the proposed approach is more dataefficient and provides accurate causal inference with wellcalibrated uncertainty estimates.  2) Strengths and weaknesses: Strengths:  The paper presents a novel and wellmotivated approach linking causal discovery and causal reasoning in a unified framework.  The integration of Bayesian active learning with experimental design is a valuable contribution for efficient causal inference.  The use of Gaussian processes and Bayesian optimization for posterior inference and experimental design is a sophisticated methodological approach.  The experiments and simulations provide evidence of the effectiveness and efficiency of the ABCI framework. Weaknesses:  The paper is highly technical and may be challenging for readers with limited familiarity with Bayesian modeling and causal inference.  The paper assumes several simplifying assumptions such as exclusion of heteroscedastic noise and unobserved confounding which might limit the generalizability of the framework.  The computational complexity of the proposed approach as acknowledged by the authors is a potential limitation for practical applications.  3) Questions:  How does the proposed ABCI framework handle the scenario of unknown intervention targets Are there strategies for identifying the most informative intervention targets in practice  Could the framework be extended to accommodate soft interventions or more complex causal queries beyond the modeled scenarios  The paper mentions a computational challenge with the intractability of some integrals. Can the authors provide more insights into strategies for mitigating this challenge and scalability considerations  4) Limitations:  The paper focuses mainly on synthetic data and may benefit from empirical validation on realworld datasets to demonstrate the generalizability of the ABCI framework.  The assumptions made such as simplifications in the noise model and causal relationships might not fully capture the complexity of practical scenarios. Overall the paper offers a sophisticated framework for integrated causal discovery and reasoning. However addressing scalability and computational challenges exploring the frameworks applicability in realworld settings and extending the model to handle more complex causal scenarios could further strengthen the contribution.  Note to Authors: The paper presents a comprehensive and technically advanced approach to integrated causal inference. Addressing the limitations identified providing insights into practical implementations and extending the frameworks capabilities could enhance the impact and relevance of the work. Great job on the detailed methodology and experimental evaluation. Good luck with further developments in this exciting area of research.", "yhlMZ3iR7Pu": " Peer Review of the Paper  1) Summary: The paper addresses the problem of inferring highdimensional data x in a model with a prior p(x) and an auxiliary differentiable constraint c(xy) based on additional information y. It introduces the use of denoising diffusion generative models as priors and explores various applications such as conditional image generation image segmentation and solving combinatorial optimization problems like the Traveling Salesman Problem. Key contributions include enabling plugandplay functionality proposing algorithms without additional training and demonstrating inference under differentiable constraints.  2) Strengths and Weaknesses: Strengths: 1. The paper introduces a novel approach to utilizing denoising diffusion models as plugandplay priors with auxiliary constraints demonstrating applications across various domains. 2. The experiments are diverse covering conditional image generation semantic image segmentation and combinatorial optimization showcasing the versatility of the proposed method. 3. The paper presents a detailed algorithmic framework and code availability for reproducibility on GitHub. 4. The integration of independently trained components without joint training or finetuning adds to the practical utility of the approach. Weaknesses: 1. While the paper explores a wide range of applications the focus is distributed potentially leading to a lack of indepth analysis in specific areas. 2. The effectiveness of the proposed method in realworld complex scenarios beyond the experimental setups is not extensively discussed. 3. The high cost of inference with denoising diffusion models is acknowledged but not extensively addressed in terms of scalability.  3) Questions: 1. Could the plugandplay functionality of denoising diffusion models be further enhanced by incorporating more complex constraints beyond differentiable functions 2. How does the proposed approach perform when applied to noisy or incomplete datasets where the auxiliary constraints may not be explicit or reliable 3. Are there plans to optimize the inference process to reduce the computational overhead associated with using denoising diffusion models repeatedly  4) Limitations: 1. The paper focuses on technical aspects and experimental results lacking indepth discussions on the theoretical foundations or implications of the proposed approach. 2. There could be limitations in the generalizability of the results especially when scaling to larger or more complex datasets not covered in the experiments. 3. The reliance on the availability of independently trained components might restrict the applicability of the method in scenarios where such components are not readily accessible or compatible. In conclusion the paper presents a novel perspective on leveraging denoising diffusion generative models as priors with differentiable constraints showcasing potential applications across multiple domains. While the experimental results are promising there is room for further investigation into the scalability robustness and efficiency of the proposed approach in realworld settings. Additionally addressing the computational challenges associated with inference using denoising diffusion models could enhance the practicality and wider adoption of the method.", "yTJze_xm-u6": " Peer Review  1) Summary The paper proposes a novel approach for sourcefree domain adaptation by introducing variational model perturbations which perturb the weights of a pretrained model to adapt to target domains. The perturbations are learned through variational Bayesian inference providing a probabilistic framework for adaptation while preserving the source models discriminative ability. The paper also demonstrates a theoretical connection to Bayesian neural networks and employs a parameter sharing strategy to reduce learnable parameters. Experimental results on five datasets under different evaluation settings validate the effectiveness of the proposed approach.  2) Strengths and Weaknesses  Strengths:  The paper addresses an important problem of sourcefree domain adaptation.  Introducing uncertainty into the model through perturbations is a novel and effective approach.  The theoretical connection to Bayesian neural networks adds depth to the proposed method.  Demonstrating effectiveness across multiple benchmarks and evaluation settings shows the robustness of the approach.  The comparison with existing methods and the extensive experimental analysis strengthen the credibility of the proposed model.  Weaknesses:  The paper could provide more insight into the implementation details especially focusing on the hyperparameters sensitivity.  While the experiments cover various scenarios additional insights into the scalability and computational efficiency of the proposed method would be beneficial.  The paper might benefit from a more detailed discussion on the limitations and future research directions.  3) Questions  Could the authors discuss the sensitivity of the hyperparameters in the variational model perturbation method  How scalable is the proposed approach to larger and more complex datasets  In what scenarios might the proposed approach struggle and how might those challenges be addressed  Considering the parameter sharing strategy could the authors elaborate on potential tradeoffs between reducing learnable parameters and maintaining flexibility in adaptation  4) Limitations  The paper could offer more extensive discussions on the potential limitations and constraints of the proposed model particularly in realworld applications.  While the experimental results are promising further analysis on the models robustness to noise and perturbations could enhance the papers contributions. Overall the paper presents a compelling approach to sourcefree domain adaptation through variational model perturbations. By addressing the questions and limitations mentioned above the paper could further enrich its findings and relevance in the field of domain adaptation research.", "yKYCwTvl8eU": "Peer Review: Summary: The paper proposes two novel approaches CCM RES and CCM EYE to learn accurate models while mitigating shortcuts by incorporating domain knowledge. The authors demonstrate the effectiveness of these approaches using realworld datasets. They show that both CCMs successfully reduce shortcut learning without sacrificing accuracy even when assumptions about the relation between features do not hold. Strengths: 1. The paper presents a novel approach to address shortcut learning a pressing issue in machine learning models providing valuable advancements in the field. 2. Both proposed approaches CCM RES and CCM EYE are theoretically grounded and empirically demonstrated to be successful in reducing shortcut learning. 3. The experiments conducted on realworld datasets CUB and MIMIC illustrate the applicability and effectiveness of the proposed methods in practical scenarios. 4. The theoretical analysis and empirical results provided a comprehensive understanding of the limitations and capabilities of the proposed models. Weaknesses: 1. The paper focuses on linear models for consistency proofs limiting the generalizability of the results to nonlinear models commonly used in practice. 2. The experiments are conducted on a limited number of datasets and the generalizability of the proposed methods to a broader range of datasets may need further verification. 3. The explanation of the methods and theoretical concepts could be made more accessible to readers unfamiliar with the subject matter as it is quite technical. Questions: 1. How sensitive are the proposed methods to the selection of hyperparameters such as the regularization parameter \u03bb in CCM EYE 2. Can the proposed approaches be extended to more complex models beyond linear models and how might their performance vary in such cases 3. Is there a plan to explore the interpretability of the implicitly learned features (U) in future work as this could provide valuable insights into the learned representations Limitations: 1. The assumption regarding knowledge of C and its sufficiency in representing all relevant factors may not always hold in practical applications potentially limiting the applicability of the proposed methods. 2. The reliance on artificial shortcuts introduced in the experiments such as correlation between sex and diagnoses may not fully capture the complexity of realworld data distributions impacting the generalizability of the findings. 3. The paper does not directly address the computational complexity of implementing the proposed methods which could be a critical aspect for adoption in realworld applications. Overall the paper provides valuable insights into mitigating shortcut learning using domain knowledge but further validation on a wider range of datasets and exploration of interpretability aspects could enhance the significance of the results. The proposed approaches show promise in addressing shortcut learning opening up avenues for practical implementation in machine learning systems.", "pm8Y8unXkkJ": " Peer Review:   1) Summary: The paper introduces a novel theoretical framework for binary classification in the presence of severe class imbalance. It derives a generalized Bayesoptimal classifier that extends beyond accuracy to various performance metrics particularly focusing on imbalanced datasets. The study provides finitesample statistical guarantees for nonparametric binary classification and examines the behavior of knearest neighbor (kNN) classification under uniform loss. Experimental results are presented to validate the theoretical findings. The work offers valuable insights into handling imbalanced datasets with different class imbalance characteristics.   2) Strengths and Weaknesses: Strengths:  Novel Theoretical Framework: The paper introduces a novel framework for binary classification in the context of severe class imbalance addressing the limitations of accuracybased metrics.  Generalized BayesOptimal Classifier: The derivation of a generalized Bayes classifier for various performance metrics is a significant contribution that can enhance the understanding of optimal classification under imbalanced data.  FiniteSample Guarantees: Providing finitesample statistical guarantees specific to imbalanced binary classification is a notable strength enabling practical applications of the proposed framework.  Insights on kNN Classification: The examination of kNN classification under uniform loss and the introduction of Uniform Class Imbalance add depth to the analysis of class imbalance effects.  Experimental Validation: The numerical experiments complement the theoretical findings and demonstrate the applicability of the proposed framework. Weaknesses:  Complexity of Results: The theoretical derivations and proofs presented in the paper are intricate which might limit the accessibility of the work to a broader audience.  Assumption Dependence: The effectiveness of the proposed framework relies on certain assumptions about the data distribution and properties which could impact the generalizability of the results.  Limited Practical Applications: While the theoretical contributions are substantial the practical implications and realworld applications of the findings could be further elaborated.   3) Questions:  Clarification on Assumptions: Could the authors elaborate on the implications of the assumptions made in the study particularly regarding the density lower bound and H\u00f6lder continuity requirements  Relation to Existing Classifiers: How does the proposed generalized Bayes classifier compare to existing classifiers such as SVM or decision trees especially in handling severe class imbalance  Scalability: How scalable is the proposed framework for largescale datasets especially in scenarios with highdimensional feature spaces  Extension to Multiclass Classification: Are there plans to extend the framework to multiclass classification problems considering the complexities of imbalanced datasets across multiple classes   4) Limitations:  Generalizability: The extent to which the findings can be generalized across various types of datasets and realworld applications remains a potential limitation.  Implementation Challenges: The practical implementation and computational demands of the proposed framework may pose challenges especially in scenarios with extensive data processing requirements.  Interpretability: The interpretability of stochastic classifiers and their implications for decisionmaking in practical settings could be a limitation in realworld deployment.  Overall the paper presents a thorough and insightful framework for addressing imbalanced binary classification with a focus on theoretical advancements and practical implications. Addressing the questions and limitations highlighted could further enhance the impact and applicability of the proposed framework.", "rrYWOpf_Vnf": " Peer Review  1) Summary: This paper investigates statistical limits in terms of Sobolev norms of gradient descent for solving inverse problems from randomly sampled noisy observations using a general class of objective functions. The theory presented in the paper covers various aspects such as gradient descent convergence rates acceleration effects with Sobolev norms and applications to solving elliptic PDEs with methods like Deep Ritz Methods and Physics Informed Neural Networks. The results show the statistical optimality of gradient descent and implications of using Sobolev norms as the loss function leading to accelerated training for higher frequency components.  2) Strengths and Weaknesses:  Strengths:  The paper introduces a novel approach to investigating statistical optimality in gradient descent for inverse problems using Sobolev norms.  The theory is welldeveloped and covers a broad range of applications in solving inverse problems particularly in the context of PDEs.  Provides theoretical results on convergence rates acceleration effects and comparisons between different methods like Deep Ritz Methods and PINNs.  The experiments conducted to illustrate the concepts and provide empirical validation show promising results.  The paper offers valuable insights into the impact of utilizing Sobolev norms for training neural networks and solving PDEs.  Weaknesses:  The paper could benefit from more detailed explanations of certain theoretical concepts and mathematical derivations to aid in understanding for readers less familiar with the topic.  While the experiments presented are insightful a more extensive evaluation with diverse datasets and scenarios could enhance the robustness of the findings.  The presentation of the results and discussion could be made more concise and structured to improve readability.  3) Questions:  How does the empirical validation align with the theoretical findings presented in the paper  Are there specific limitations in the methods proposed that could affect their practical applicability especially in realworld scenarios  Could further investigations be done to analyze the generalizability of the proposed approach to a wider range of inverse problems beyond the ones considered in this paper  4) Limitations:  The paper could provide more thorough discussions on potential practical limitations or challenges in implementing the proposed methods in realworld applications.  The experimental validation while providing valuable insights could be expanded to cover a broader range of scenarios to establish the effectiveness and generalizability of the approach. Overall the paper presents a novel perspective on using Sobolev norms with gradient descent for solving inverse problems offering valuable theoretical insights and implications for practical applications in machine learning and PDEs. Further refinement in explaining the theoretical concepts and conducting additional experiments could enhance the impact and clarity of the research findings.", "zSdz5scsnzU": "Peer Review 1) Summary (107 words): The paper introduces Expansive Latent Space Trees (ELAST) a latent planning algorithm for longhorizon goalreaching tasks from visual inputs. ELAST integrates concepts from classical samplingbased motion planning and selfsupervised contrastive learning to perform explorative tree search in a lowerdimensional latent space. The method optimizes a tree representation to efficiently navigate in the latent space showcasing significant performance improvements over existing baselines in challenging visual control tasks in simulation including deformable object navigation. 2) Strengths:  ELAST introduces an innovative approach to latent planning combining elements from classical methods and selfsupervised learning.  The method shows significant performance improvements over existing baselines in challenging visual control tasks.  The paper provides a thorough and detailed description of ELAST including the underlying concepts algorithms and experimental setup. Weaknesses:  The paper could benefit from a clearer explanation of the limitations and practical challenges of implementing ELAST in realworld scenarios.  More detailed comparisons with existing stateoftheart methods would enhance the papers impact in the field.  The evaluation section could be expanded to provide a more comprehensive analysis of ELASTs performance under different conditions and environments. 4) Questions:  How do the spatial and temporal complexities of the environments impact ELASTs performance compared to other latent planning algorithms  Can ELAST be easily adapted to handle partially observable environments and uncertainty in state observations  What are the computational requirements of ELAST and how scalable is the method to larger and more complex scenarios 5) Limitations:  The paper primarily focuses on simulationbased experiments limiting the generalizability of ELAST to realworld robotic control scenarios.  The evaluation does not extensively explore the robustness and adaptability of ELAST across a wide range of environments and tasks.  ELASTs reliance on selfsupervised contrastive learning may introduce challenges in terms of training efficiency and transferability to different domains. Overall the paper presents a novel approach to latent planning with potential applications in complex decisionmaking problems. Further enhancements addressing the identified weaknesses and questions would strengthen the papers contribution to the field of autonomous agents and robotic control.", "zp_Cp38qJE0": "Peer Review: 1) Summary: The paper addresses the important issue of transferring fairness in machine learning models under distribution shifts. It proposes a practical selftraining algorithm with fair consistency regularization as the key component. The study includes a thorough theoretical analysis algorithm development and experimental validation on synthetic and real datasets. The authors find that domain shifts present significant challenges in transferring fairness but their proposed method effectively addresses this issue by ensuring similar consistency across groups. Overall the research provides valuable insights and contributions to the field of algorithmic fairness under distribution shifts. 2) Strengths and Weaknesses: Strengths:  Comprehensive and detailed analysis of fairness under various distribution shifts.  Novel theoretical framework with practical algorithm development.  Experimental validation on both synthetic and real datasets.  The proposed fair consistency regularization shows promising results in transferring fairness.  Clear presentation of methodology results and contributions. Weaknesses:  The paper could benefit from more rigorous comparisons with existing methods in terms of performance metrics.  The proposed methods generalizability to a broader range of datasets and applications could be further explored.  The explanation of technical concepts and algorithms may be challenging for readers not familiar with the field. 4) Questions: 1. What are the computational complexities of the proposed method especially in terms of scalability to larger datasets 2. How does the fair consistency regularization method perform in scenarios where sensitive attributes are correlated with other factors 3. Could the proposed algorithm be extended to handle more complex and multidimensional shifts in distribution 4. Are there specific considerations for deploying the proposed method in realworld applications particularly in highstakes decisionmaking tasks 5) Limitations:  The reliance on synthetic datasets for the evaluation may not fully capture the complexity of realworld scenarios.  The proposed fair consistency regularization method may need further validation in diverse application domains and datasets.  The paper focuses on the theoretical framework and algorithmic development without delving deeply into the ethical implications and practical considerations of deploying such methods in realworld contexts. Overall the paper presents a significant contribution to the field of algorithmic fairness under distribution shifts. The proposed method shows promise in transferring fairness across different types of shifts and further exploration and validation on diverse datasets could enhance its applicability and effectiveness in practical settings.", "sde_7ZzGXOE": " Summary: The paper investigates the learnability of outofdistribution (OOD) detection using the probably approximately correct (PAC) learning theory. It explores necessary conditions for learnability proves impossibility theorems in certain scenarios and offers necessary and sufficient conditions for successful OOD detection in practical domain spaces. The study bridges the gap between theoretical analysis and practical applications of OOD detection.  Strengths: 1. Theoretical Contributions: The paper provides a comprehensive theoretical framework for understanding the learnability of OOD detection addressing an important gap in the field. 2. Rigorous Analysis: The study rigorously examines various domain spaces and hypothesis spaces proving both necessity and sufficiency conditions for the learnability of OOD detection. 3. Practical Relevance: By relating theoretical results to practical scenarios the paper offers valuable insights for designing effective OOD detection algorithms. 4. Clear Structure: The paper is wellorganized with a clear progression from problem definition to theoretical analysis and implications for realworld applications.  Weaknesses: 1. Complexity: The theoretical framework presented may be challenging for readers without an indepth understanding of learning theory. 2. Limited Empirical Validation: The paper focuses primarily on theoretical analysis lacking empirical validation or practical implementation of the proposed concepts. 3. Assumptions: The study is based on certain assumptions and conditions which may not always hold in realworld OOD detection scenarios.  Questions: 1. How do the proposed conditions for learnability of OOD detection align with existing practical OOD detection algorithms 2. Are there specific realworld applications or datasets where the theoretical framework presented in the paper might be directly applicable 3. How do the implications of the theory presented in the paper align with the challenges faced by practitioners in deploying OOD detection algorithms  Limitations: 1. Generalization: The study focuses on theoretical analysis and may not directly translate to realworld implementation challenges or empirical performance of OOD detection algorithms. 2. Empirical Validation: The lack of empirical validation limits the concrete applicability of the theoretical results to practical OOD detection scenarios. 3. Sensitivity to Assumptions: The theoretical framework relies on specific assumptions and conditions which may not fully capture the complexity and variability of realworld OOD data distributions.", "weoLjoYFvXY": "Peer Review: Summary: The paper presents a novel hierarchical and localized learning algorithm Root Cause Discovery (RCD) for efficiently detecting the root cause of failures in complex microservice architectures. The proposed algorithm leverages distributional invariances across normal and anomalous datasets to identify interventional targets without the need to learn the complete causal graph. The evaluation results demonstrate the effectiveness of RCD in terms of both accuracy and execution time compared to existing techniques. The algorithm is also applied to realworld cases with promising results showcasing its potential practical utility. Strengths: 1. Innovation: The paper introduces a novel approach that addresses the challenges of root cause analysis in complex microservice architectures offering a fresh perspective on causal discovery. 2. Scalability: RCD demonstrates high scalability allowing for efficient detection of root causes even in systems with numerous metrics as validated in both synthetic and realworld experiments. 3. Performance: The evaluation results show significant improvement in topk recall while reducing execution time showcasing the efficacy of the proposed algorithm compared to existing solutions. 4. Applicability: The algorithm is demonstrated on synthetic datasets the Sockshop application test bed and reallife failures from a large cloud service provider highlighting its broad applicability and effectiveness in diverse scenarios. Weaknesses: 1. Detailed Implementation: While the theoretical and algorithmic aspects of RCD are wellexplained more insights into the practical implementation details challenges and possible limitations in realworld deployment would enhance the papers completeness. 2. Comparison Metrics: While the evaluation focuses on recall at topk and execution time other important performance metrics such as precision false positive rate or potential tradeoffs between accuracy and efficiency could provide a more comprehensive evaluation. 3. Impact Analysis: A discussion on the potential impact of the proposed algorithm on realworld scenarios such as improved system reliability reduced downtime or financial implications would add depth to the implications of the research. Questions: 1. How does the hierarchical and localized learning approach of RCD handle scenarios with hidden confounders or latent variables that might impact the accuracy of root cause detection 2. Could the paper elaborate on the interpretability and explainability of the root cause identification process facilitated by RCD especially in scenarios where multiple potential root causes are identified 3. Considering the sensitivity of root cause analysis in production systems how does RCD deal with cases of false positives or misidentifications and what measures are in place to mitigate such risks Limitations: 1. Data Restrictions: The realworld application of RCD is limited by the availability and quality of metrics data and extending the research to more diverse datasets or different industry contexts could provide further insights into the algorithms adaptability. 2. Scalability Barrier: While RCD presents scalability advantages over existing approaches exploring the algorithms limitations in extremely largescale systems with dynamic configurations or varying fault patterns could be a potential area of investigation for future work.", "sNcn-E3uPHA": "Peer Review Summary: The paper introduces a text classification algorithm inspired by the concept of superposition in quantum physics treating text as a superposition of words and utilizing the Born rule to compute transition probabilities. The algorithm is implemented explicitly and embedded in a neural network architecture. Experimental results on benchmark datasets demonstrate the methods classification performance explainability and computational efficiency. The paper discusses the broader applicability of the method to nontextual data. Strengths: 1. Originality: The use of quantum principles in machine learning is novel and adds a unique perspective to text classification. 2. Explainability: The method provides interpretable results that can shed light on the classification process. 3. Computational Efficiency: The analysis of computational time and performance showcases the potential for practical implementation. 4. Advanced Methodology: The integration of quantuminspired concepts into a machine learning framework opens up new possibilities for algorithm development. Weaknesses: 1. Complexity: The use of quantum concepts might limit the accessibility of the algorithm to a wider audience due to its technical nature. 2. Generalization: The paper primarily focuses on text classification and while extensions are suggested the methods performance on nontextual data remains to be explored. 3. Empirical Clarity: More detailed empirical results including comparisons with existing quantuminspired methods and traditional approaches would enhance the papers credibility. 4. Practical Implementation: Detailed discussions on realworld scenarios where this method could offer significant advantages over existing techniques are missing. Questions: 1. Can the method handle largescale datasets efficiently given the complex nature of the algorithm 2. How does the performance of this algorithm compare to traditional machine learning approaches especially in scenarios with limited labeled data 3. Are there specific types of text datasets or domains where this quantuminspired method excels 4. Can the explanation provided by the algorithm be easily understood and leveraged by nonexperts in quantum physics or machine learning Limitations: 1. Textual Focus: The papers focus predominantly on text classification restricts the generalizability of the algorithm to other data types. 2. Interpretability: While explainability is highlighted as a strength the extent and impact of this feature have not been thoroughly explored. 3. Implementation: The practical implementation of the algorithm in realworld scenarios remains to be demonstrated comprehensively. 4. Scalability: The scalability of the algorithm to larger and more diverse datasets needs to be investigated for broader adoption. Overall the paper presents an intriguing approach to text classification with quantuminspired methodologies but there are opportunities for further research and exploration to enhance its applicability and impact across diverse domains.", "lgj33-O1Ely": " Summary: The paper introduces TotalSelfScan a method for reconstructing a detailed fullbody model from multiple monocular selfrotation videos focusing on the face hands and body. The approach utilizes a multipart representation to model separate body parts seamlessly fusing them into a unified human model for realistic rendering under novel poses. TotalSelfScan significantly improves the quality of reconstruction and rendering on the face and hands compared to existing methods.  Strengths: 1. Innovative Approach: The use of multiple partspecific videos to enhance the reconstruction and rendering quality is a novel and effective approach. 2. Comprehensive Methodology: The paper details a thorough methodology encompassing partspecific models part deformation part compositions training strategies and nonrigid ray transformation for rendering. 3. Experimental Validation: The paper provides detailed experimental results on newly created datasets showcasing the methods efficacy compared to baseline methods. 4. Ablation Studies: The ablation studies provide crucial insights into the effectiveness of different components of the proposed method.  Weaknesses: 1. Limited Generalization: The method might face challenges in modeling posedependent deformations due to the constraint of selfrotation videos. 2. Training Time: The method requires a relatively long training time suggesting potential areas for optimization. 3. Lack of Error Bars: The absence of error bars in the experimental results limits the quantification of result variability.  Questions: 1. How does the proposed method handle variations in lighting conditions across different partspecific videos during the fusion of appearance information 2. Could the method be applied to scenarios where the selfrotation video covers more diverse human motions to enhance posedependent deformation modeling 3. Are there plans to release the datasets created for evaluation to facilitate future research in this area  Limitations: 1. PoseDependent Deformations: The method may struggle with modeling complex posedependent deformations due to limited motion coverage in selfrotation videos. 2. Training Time: The relatively long training time could hinder scalability and practicality for realtime applications. 3. Generalization: The reliance on selfrotation videos may limit the methods ability to model diverse human motions beyond the covered rotations. In conclusion the paper presents an innovative approach for fullbody avatar creation demonstrating significant improvements in reconstruction and rendering quality. Addressing the limitations and potential questions posed could further enhance the robustness and applicability of the proposed method.", "rjDziEPQLQs": " Peer Review:  Summary: The paper presents a novel stepsize schedule for the Newton method that exhibits fast global and local convergence guarantees. The authors prove global convergence rates matching stateoftheart methods and also demonstrate competitive performance compared to existing baselines. The proposed algorithm referred to as AICN converges globally with an O(k2) rate and also shows fast local convergence. The algorithm is simple explicit and does not require solving subproblems making it practical and efficient for optimization tasks.  Strengths:  The paper addresses an important optimization problem by proposing a novel stepsize schedule for the Newton method with fast convergence guarantees.  The convergence proofs are solid matching the bestknown global and local rates of existing methods demonstrating the effectiveness of the proposed algorithm.  The algorithm is implementable efficient and performs competitively when compared to baseline methods.  The research is wellstructured providing clear explanations of the algorithm assumptions and convergence results.  Weaknesses:  The paper could benefit from more detailed comparisons with existing methods in terms of convergence speed complexity and performance on a wider range of optimization problems.  While the theoretical analysis is comprehensive practical experiments on a diverse set of datasets and optimization tasks could further strengthen the empirical validation of the proposed algorithm.  Questions: 1. How does the proposed AICN algorithm handle noisy or stochastic optimization settings where gradients may be approximated or noisy 2. Can the algorithm be extended to handle nonconvex optimization problems and if so how does its performance compare in such cases 3. Are there specific characteristics or properties of optimization problems where the AICN algorithm excels or underperforms compared to traditional Newton methods or other variants  Limitations:  The paper lacks extensive empirical validation on various datasets and optimization tasks limiting the generalizability of the proposed algorithms performance.  The focus on theoretical analysis might overshadow potential practical challenges or considerations that could arise when implementing the algorithm in realworld scenarios. Overall the paper presents a significant contribution to the field of optimization with its novel stepsize schedule for the Newton method. Further empirical validation and detailed comparisons with existing methods could enhance the impact and applicability of the proposed algorithm in practical optimization scenarios.", "kK200QKfvjB": "Peer Review 1. Summary: The paper focuses on analyzing the loss surface of DNNs with L2 regularization through two reformulations of the loss: one in terms of the layerwise activations of the training set and another in terms of covariances of hidden representations. The authors demonstrate a sparsity result for homogeneous DNNs showing that the local minimum of the L2regularized loss can be achieved with a limited number of neurons. The study provides theoretical insights into the dynamics of feature learning in DNNs for positively homogeneous nonlinearities. 2. Strengths:  The paper presents a thorough analysis of the loss surface of DNNs with L2 regularization offering unique reformulations for studying feature learning dynamics.  The theoretical study of the sparsity effect for homogeneous DNNs is insightful with clear implications for the number of neurons needed to reach a local minimum.  The work is wellstructured with clear explanations of the reformulations and their implications for feature optimization and covariance learning. 3. Weaknesses:  The paper could benefit from more concrete experimental validation to demonstrate the practical implications of the theoretical findings.  While the theoretical results are wellexplained the practical relevance to realworld applications could be further elaborated.  The complexity of the methods proposed may limit the practical applicability and discussion on computational efficiency would be valuable. 4. Questions:  How do the findings of this study translate to practical applications in deep learning tasks Are there specific scenarios where the observed sparsity effect can be leveraged for improved performance  Can the proposed reformulations be extended or adapted for different types of neural networks or regularization techniques and what implications would that have  What are the implications of the studys findings for the design and optimization of deep neural networks in realworld applications 5. Limitations:  The paper primarily focuses on theoretical analysis and lacks extensive empirical validation. Including experiments to validate the theoretical insights would strengthen the study.  The computational complexity of the methods proposed may hinder practical implementation in largescale deep learning scenarios.  The study is limited to specific types of neural networks and regularization techniques which may restrict the generalizability of the findings to diverse deep learning architectures. Overall the paper provides valuable theoretical insights into the sparsity effect of L2 regularization in deep neural networks but further empirical validation and discussions on practical implications would enhance the impact and applicability of the findings.", "u6MpfQPx9ck": " Peer Review Summary: The paper discusses ProbCover a new active learning algorithm for the low budget regime in deep active learning. The algorithm aims to maximize Probability Coverage in subset selection for annotation through a covering lens. The paper presents a thorough theoretical framework and empirical evaluation demonstrating ProbCovers effectiveness in improving the stateoftheart in image recognition benchmarks in the lowbudget regime especially benefiting semisupervised methods. Strengths: 1. Theoretical Framework: The paper provides a solid theoretical framework linking the Max Probability Cover problem to generalization error bounds showcasing a deep understanding of the lowbudget regime in active learning. 2. Novel Algorithm: Introducing ProbCover as a new active learning strategy for the lowbudget regime is a significant contribution to the field especially its demonstrated capability to outperform existing methods. 3. Empirical Evaluation: The empirical experiments are comprehensive covering various datasets and comparison with stateoftheart methods. The results show the superiority of ProbCover in the lowbudget regime. 4. Ablation Study: Inclusion of ablation study strengthens the evaluation providing insights into the key aspects contributing to ProbCover\u2019s performance. Weaknesses: 1. Complexity: The NPhardness of Max Probability Cover could be a limiting factor in practical applications. The paper would benefit from more discussion on potential computational challenges. 2. Generalizability: The paper extensively evaluates the algorithm on specific computer vision datasets. Additional experiments on diverse domains could enhance the generalizability of ProbCover. Questions: 1. Scalability: How does ProbCover scale with increasing dataset sizes and dimensionality 2. Effectiveness in Noisy Data: Has ProbCover been evaluated under noisy data conditions or data with mislabeled instances 3. Comparison with SemiSupervised Methods: Can the authors provide an indepth comparison of ProbCovers performance with a wider range of semisupervised methods Limitations: 1. Dataset Dependency: The evaluation focuses mainly on image recognition benchmarks. Additional experiments on other types of datasets would broaden the applicability of ProbCover. 2. Empirical Results: While the empirical evaluation is robust further insights into the algorithms behavior in challenging scenarios or on largerscale datasets could add depth to the findings. Overall the paper presents a novel active learning algorithm ProbCover with a strong theoretical foundation and promising empirical results. Addressing the highlighted weaknesses and questions could further enhance the impact and applicability of ProbCover in realworld scenarios.", "pd6ipu3jDw": " Peer Review of the Paper  1) Summary The paper introduces the Agent Transformer Memory (ATM) network for tackling the challenges of learning in realworld multiagent tasks with partial observability. The ATM leverages transformerbased working memory and EntityBound Action Layer to enhance multiagent reinforcement learning (MARL) algorithms. Experimental results on SMAC and LevelBased Foraging environments show that ATM significantly accelerates learning and improves performance.  2) Strengths and Weaknesses Strengths:  The paper addresses a significant topic in MARL with a novel approach combining memory mechanisms and action semantic inductive bias using a transformerbased framework.  The proposed ATM network demonstrates impressive learning acceleration and performance improvements across challenging environments.  The integration of transformerbased memory structure and EntityBound Action Layer is a crucial contribution.  Extensive experiments on SMAC and LevelBased Foraging environments validate the effectiveness of ATM in enhancing existing MARL algorithms. Weaknesses:  The paper lacks a detailed comparison with existing stateoftheart MARL techniques beyond GRUbased networks. More comparative analysis with different MARL algorithms would strengthen the evaluation.  The computational complexity of the ATM and potential scalability issues are not thoroughly discussed in relation to more complex multiagent tasks or different environments.  The explanation of the EntityBound Action Layer could be enhanced with clearer descriptions and possible limitations.  3) Questions  How does the ATM perform with a larger number of memory slots in more complex multiagent environments  Are there any specific scenarios where the computational complexity of ATM becomes a significant bottleneck  Can the proposed EntityBound Action Layer adapt to dynamic changes in the action space without manual configuration  4) Limitations  The paper focuses primarily on the proposed ATM network without providing a thorough comparison with a wider range of MARL algorithms.  The discussion on the scalability of ATM and potential limitations in extremely complex multiagent scenarios is limited.  The generalization of the EntityBound Action Layer to different environments with varying semantic meanings of actions may pose challenges. Overall the paper presents an innovative approach addressing partial observability in MARL through the ATM network. Enhancing the comparisons scalability analysis and detailed discussion on limitations would further strengthen the research contributions.", "r9b6T088_75": "Peer Review: 1) Summary: The paper presents a novel approach the DegradationAware Unfolding HalfShuffle Transformer (DAUHST) for hyperspectral image (HSI) reconstruction in coded aperture snapshot spectral compressive imaging systems. It combines a principled DegradationAware Unfolding Framework (DAUF) and a HalfShuffle Transformer (HST) to estimate degradation patterns illposedness and longrange dependencies for HSI reconstruction. Experimental results show that DAUHST outperforms stateoftheart methods in terms of performance and computational cost. 2) Strengths:  The paper addresses two critical issues in existing deep unfolding methods providing a principled framework for estimating key parameters that guide the iterative learning process.  The proposal of the HalfShuffle Transformer (HST) to capture both local and nonlocal dependencies in the denoising process is a significant contribution.  The thorough experimentation demonstrates the superior performance of DAUHST compared to existing methods while maintaining lower computational and memory costs.  The detailed explanation of the proposed framework and architecture as well as the experimental setup add clarity to the research. Weaknesses:  The paper could benefit from a more detailed discussion on the limitations of the proposed method such as potential failure scenarios or instances where the method may not perform optimally.  While the comparisons with existing methods are thorough a more indepth analysis of the tradeoffs between performance and computational cost could provide additional insights.  The paper could further elaborate on the practical implications and scalability of DAUHST including its potential for realworld applications beyond the described experimental settings. 4) Questions:  Could the proposed framework be extended to address additional challenges in HSI reconstruction beyond degradation patterns and longrange dependencies  How does the method adapt to varying levels of noise in different imaging scenarios and are there any limitations in noise handling  Can the models scalability and adaptability to different hardware configurations be discussed further especially in the context of realworld applications  How robust is the proposed method to variations in input data characteristics such as different spectral ranges or varying levels of dispersion in CASSI systems 5) Limitations:  The paper focuses heavily on the technical aspects of the proposed method potentially overshadowing discussions on broader societal impacts or future directions for the research.  The limitations section offers scope for further exploration into the generalizability and robustness of the method in diverse imaging scenarios.  While the experimental results are promising there could be additional validation on a wider range of datasets or in varied environmental conditions to evaluate the methods robustness. Overall the paper presents a novel approach to HSI reconstruction that addresses key limitations in existing methods. By emphasizing degradationaware unfolding and leveraging transformerbased techniques the proposed DAUHST shows promising results. Further discussions on practical implications scalability and potential extensions could enhance the papers impact and relevance to the scientific community.", "qmm__jMjMlL": " Peer Review: 1) Summary (Word Count: 100) The paper introduces TARGETVAE a novel translation and rotation groupequivariant variational autoencoder framework for learning semantic object representations that are invariant to pose and location in images in an unsupervised manner. The model disentangles semantic rotation and translation components of object representations demonstrating impressive results in accurately inferring object poses and locations. The experiments show significant improvements over existing methods enhancing clustering in the latent space and enabling accurate unsupervised pose and location inference. The paper provides detailed methodologies experimental setups results and applications in cryoEM image analysis. 2) Strengths and Weaknesses: Strengths:  Introduces a novel framework TARGETVAE addressing the problem of learning invariant object representations in an unsupervised manner.  Comprehensive experiments demonstrating the effectiveness of TARGETVAE in learning disentangled representations and accurate pose and location inference.  Structured around translation and rotation equivariance presenting innovative solutions to existing challenges in unsupervised object representation learning.  Implications for future applications in unsupervised object generation pose prediction and object detection.  Clear and detailed explanations provided regarding the model architecture training setups and experimental results. Weaknesses:  The paper could benefit from a more detailed discussion on the limitations and potential drawbacks of the proposed TARGETVAE framework.  While the experiments show promising results a more extensive comparison with a broader range of existing methods could strengthen the papers contributions.  It might be helpful to delve deeper into the practical implications and potential challenges of deploying the TARGETVAE model in realworld applications beyond the conducted experiments. 3) Questions:  Could you provide more insight into the computational complexity of the TARGETVAE framework compared to existing methods  How sensitive is the performance of TARGETVAE to variations in hyperparameters such as the number of discrete rotations latent space dimensionality etc.  Are there any specific scenarios or datasets where TARGETVAE might face challenges in learning accurate object representations  How generalizable is the proposed framework to other types of images beyond the datasets used in the experiments 4) Limitations:  The paper focuses mainly on 2D images containing single objects with rotation and translation. Extending the framework to handle more complex scenarios involving multiple objects or 3D environments could be a significant challenge.  The experiments primarily demonstrate the effectiveness of TARGETVAE on synthetic datasets and MNIST variations. Validation on larger more diverse datasets would further strengthen the models robustness and generalizability. Overall the paper presents a novel and promising approach to learning invariant object representations in images with potential applications across various domains. Further exploration and validation on diverse datasets and scenarios would enhance the papers impact and applicability in realworld settings.", "nyn2ewuF-g9": "Peer Review 1) Summary: The paper addresses the challenge in collaborative machine learning of fairly allocating rewards while considering tradeoffs between payoff and model rewards. It proposes a suitable allocation scheme based on desirable properties of payoffs and underlying payoff flows. The scheme is empirically demonstrated to be effective in various scenarios of collaborative ML with budget constraints. 2) Strengths:  The paper presents a comprehensive and wellstructured approach to fair reward allocation in collaborative ML.  The methodology proposed to balance between payoff and model rewards is welldefined and theoretically grounded.  The empirical demonstrations on the MNIST dataset showcase the effectiveness of the proposed scheme in various budget constraint scenarios.  The inclusion of both payoff and model reward allocation is a novel and valuable contribution to the field. 3) Weaknesses:  The theoretical derivations and properties defined may be complex for readers not deeply familiar with game theory or cooperative game theory.  The paper focuses more on the theoretical framework and less on the practical implementation aspects or realworld applications.  Although the experiments are illustrative more realworld diverse datasets could have been used for a broader evaluation of the proposed scheme. 4) Questions:  In practical collaborative ML scenarios what challenges could arise in implementing the proposed fair reward allocation scheme  How scalable is the proposed methodology to larger datasets or more parties in collaborative ML settings  Is there a way to extend the approach further to include dynamic changes in party budgets or contributions during the collaboration process 5) Limitations:  The theoretical nature of the paper may limit its immediate applicability in realworld collaborative ML projects without further practical guidelines.  Evaluating the proposed scheme on more complex tasks or industryspecific datasets could provide additional insights into its effectiveness and practical utility.  Generalizability of the proposed approach beyond the specific collaborative ML scenarios considered in the paper may need further investigation. Overall the paper presents a novel and technically sound approach to fair reward allocation in collaborative machine learning. Further exploration of practical implications and realworld applications would enhance the utility of the proposed scheme.", "q9XPBhFgL6z": " Summary: The paper presents a formal definition of harm in the context of autonomous systems using causal models and actual causality. It addresses the challenges in defining harm particularly in cases of preemption and failing to benefit. The authors highlight the importance of the choice of default utility in determining harm and provide examples to illustrate their frameworks effectiveness in resolving conflicting perspectives.  Strengths: 1. Clear Definition Approach: The paper provides a structured formal approach to defining harm in the context of autonomous systems using causal models and actual causality. 2. Addressing Key Challenges: The authors effectively address challenges such as preemption and failing to benefit in defining harm showcasing the utility of their approach. 3. Illustrative Examples: The inclusion of examples strengthens the understanding of the proposed framework and its application to realworld scenarios. 4. Comparative Analysis: The comparison with the approach of Richens Beard and Thompson (RBT) adds value by highlighting the unique aspects and benefits of the authors methodology.  Weaknesses: 1. Complexity: The paper tackles a complex subject using advanced concepts like causal models and actual causality which may be difficult for readers without a strong background in the field to grasp easily. 2. Assumptions: The reliance on specific assumptions such as default utility values may limit the generalizability of the framework in diverse contexts. 3. Lack of Empirical Validation: The paper lacks empirical validation of the framework which could strengthen the practical applicability of the proposed definition of harm.  Questions: 1. Generalizability: How transferable is the defined framework to diverse autonomous system contexts beyond the examples provided 2. Utility of Default Values: Can you elaborate on the rationale behind the choice of default utility values and their impact on determining harm in different scenarios 3. Practical Implementation: How feasible is the application of this formal definition in legal and regulatory frameworks for autonomous systems in the real world 4. Future Research: What are the potential areas for further research or refinement of the proposed definition of harm based on the outcomes of this study  Limitations: 1. Theoretical Focus: The paper is primarily theoretical and lacks empirical validation or practical application scenarios limiting its immediate realworld relevance. 2. Complexity Barrier: The advanced concepts and technical language used may hinder accessibility and understanding for a wider audience potentially excluding nonexperts in the field. 3. Assumption Sensitivity: The frameworks reliance on specific assumptions such as default utilities may restrict its adaptability across varied contexts where different values could apply.", "oPzICxVFqVM": " Peer Review: 1) Summary: The paper investigates the relationship between scorebased generative models and the Wasserstein distance. The authors prove that scorebased models minimize the Wasserstein distance between the data and generated distributions under certain assumptions. The novel application of optimal transport theory sheds light on the theoretical understanding of scorebased models. Numerical experiments are conducted to support the theoretical findings exploring the tightness of the upper bound on the Wasserstein distance. 2) Strengths:  The paper presents a novel theoretical result linking scorebased models to Wasserstein distance minimization.  The use of optimal transport theory provides a unique perspective on generative modeling.  The numerical experiments validate the theoretical findings and showcase the practical implications.  Clear and detailed explanations provided in the main text and notations for clarity. 3) Weaknesses:  The paper could benefit from a more detailed discussion on the implications of their findings for realworld applications.  The complexity of estimating the Lipschitz constant Ls is acknowledged but potential solutions or workarounds could be explored.  The manuscript could be enhanced by more visual aids or figures to help illustrate the concepts and experimental results. 4) Questions:  Could the findings of this research be extended to other types of generative models beyond scorebased models  How might the claimed convergence of the Wasserstein distance be practically useful in realworld applications of generative modeling  Are there plans to explore the applicability of the upper bound estimation to larger and more complex datasets in future research 5) Limitations:  The reliance on the onesided Lipschitz constant for estimation could hinder practical applications in highdimensional data.  The challenges associated with estimating Ls could limit the scalability and utility of the theoretical upper bound in realworld scenarios.  The paper primarily focuses on synthetic datasets further validation on realworld datasets would enhance the applicability of the findings. Overall the paper presents intriguing insights into the theoretical underpinnings of scorebased generative models and their relationship to the Wasserstein distance. Clear communication of concepts and thorough experimental validation enhance the credibility of the research. Further exploration of practical implications and avenues for overcoming computational challenges would strengthen the impact of the study.", "sYDX_OxNNjh": " Peer Review 1. Summary The paper introduces a novel approach Recurrent Skill Training (ReST) to address the exploration degradation issue in unsupervised skill discovery in reinforcement learning. The authors identify that parallel training approaches can limit state coverage and subsequent skill diversity. Through experiments on 2D navigation and robotic locomotion environments the proposed ReST method outperforms previous parallel training approaches showcasing improved state coverage and skill diversity. 2. Strengths and Weaknesses Strengths:  Identification of the exploration degradation issue and proposing a novel solution ReST to address it.  Meaningful experiments on challenging 2D navigation and robotic locomotion tasks demonstrate the efficacy of ReST in achieving better state coverage and skill divergence.  Comprehensive analysis and insights provided regarding the limitations of parallel training approaches in unsupervised skill discovery.  Clear formulation and experimental setup that allows for replicability. Weaknesses:  The sample complexity of ReST is noted to be higher compared to previous approaches which could be considered a limitation.  Computational complexity is another potential drawback as ReST requires computing intrinsic rewards based on predictions from all other skills.  The scalability of ReST is currently limited to discrete latent spaces which leaves room for improvement for continuous latent spaces in future work.  The paper could benefit from more detailed discussion on how ReST generalizes to downstream or hierarchical tasks. 3. Questions  How does the proposed ReST approach compare in terms of computational efficiency and sample complexity with existing methods in unsupervised skill discovery  Are there specific criteria or benchmarks used to evaluate the effectiveness of ReST in addressing the exploration degradation issue  Can ReST be extended or adapted to handle continuous latent spaces for potential applicability in more complex environments or tasks 4. Limitations  The study primarily focuses on discrete latent spaces limiting the generalizability of ReST to continuous latent spaces and more complex environments.  The computational complexity and sample efficiency of ReST could hinder its practical implementation in realworld applications.  The scalability and adaptability of ReST to downstream or hierarchical tasks are not extensively discussed warranting further exploration and clarification. In conclusion the paper presents a valuable contribution to reinforcement learning research by addressing the exploration degradation issue in unsupervised skill discovery. The proposed ReST method shows promising results in improving state coverage and skill diversity although there are areas for further investigation and refinement for broader applicability. Overall the paper provides a solid foundation for future research in this domain.  Rating:  OriginalityNovelty: 45  Technical Soundness: 45  Impact and Relevance: 45  ReadabilityClarity: 45", "kMiL9hWbD1z": " Peer Review of \"RTFormer: Efficient DualResolution Transformer for RealTime Semantic Segmentation\"  Summary: The paper introduces RTFormer an efficient dualresolution transformer for realtime semantic segmentation aiming to achieve a better tradeoff between performance and efficiency compared to CNNbased models. The proposed approach leverages GPUFriendly Attention with linear complexity and discards the multihead mechanism for inference efficiency on GPUlike devices. The paper extensively evaluates RTFormer on popular benchmarks and achieves stateoftheart results on Cityscapes CamVid and COCOStuff with promising performance on ADE20K.  Strengths: 1. Novel Contribution: The introduction of the RTFormer block and network architecture addresses the need for efficient global context modeling in realtime semantic segmentation tasks. 2. Efficiency: The utilization of GPUFriendly Attention and crossresolution attention demonstrates improved performance and efficiency making it competitive with CNNbased approaches. 3. Extensive Evaluation: The thorough evaluation on multiple datasets showcases the effectiveness and generalization capabilities of RTFormer. 4. Comparative Analysis: Ablation studies provide insights into the effectiveness of different attention mechanisms and network design choices.  Weaknesses: 1. Complexity: The paper is highly technical and may be challenging for readers not wellversed in transformer architectures and attention mechanisms. 2. Justification: More detailed justification for certain design choices such as the number of groups in grouped double normalization could strengthen the paper. 3. Accessible Implementation: Although code availability is provided more details on implementation and reproducibility could enhance the practical usability of the proposed method.  Questions: 1. Generalization: How does the proposed RTFormer perform on datasets beyond those mentioned in the paper Is there any empirical evidence on the scalability of the method to different domains 2. Impact of Hyperparameters: How sensitive is the performance of RTFormer to hyperparameter tuning such as setting reduction ratios or spatial sizes in attention mechanisms 3. RealTime Performance: Could you provide more insights into the realtime performance of RTFormer in terms of responsiveness in dynamic environments or scenarios with high computational demand  Limitations: 1. Parameter Efficiency: While the paper mentions the low parameter count of RTFormerSlim further discussion on optimizing parameters for edge devices could be beneficial. 2. Practical Implementation: It would be helpful to include practical considerations for implementing RTFormer in realworld applications such as memory usage and deployment challenges. Overall the paper presents a significant advancement in realtime semantic segmentation with transformerbased approaches. However addressing the weaknesses and limitations could further enhance the clarity and applicability of the proposed method for the broader research community.", "pMumil2EJh": "Peer Review 1) Summary (avg word length 100): The paper introduces a novel Temporal Polynomial Graph Neural Network (TPGNN) for accurate Multivariate Time Series (MTS) forecasting. TPGNN addresses the challenges of capturing dynamic variable dependence in MTS data by representing the correlation as a temporal matrix polynomial. The paper provides theoretical analysis experimental results on synthetic and benchmark datasets and ablation experiments demonstrating the effectiveness of TPGNN in achieving stateoftheart performance in MTS forecasting. 2) Strengths:  The paper addresses the critical issue of capturing dynamic variable dependence in MTS forecasting which is important for realworld applications.  TPGNN introduces a novel approach using temporal matrix polynomials to represent timevarying correlations which leads to stateoftheart performance on both shortterm and longterm MTS forecasting tasks.  The paper provides a comprehensive theoretical analysis experiments on synthetic data and comparisons with baselines demonstrating the effectiveness of the proposed method.  Ablation experiments help in understanding the importance of key components like the TPG module and timevarying coefficients in TPGNN.  The paper includes case studies on realworld datasets providing insights into how TPGNN captures variable dependence. Weaknesses:  The paper could benefit from clearer explanations in some sections especially regarding the theoretical analyses and the design choices in TPGNN.  The datasets used in experiments particularly the synthetic datasets could be elaborated on with more details about their characteristics and generation process.  The paper lacks discussion on potential limitations of TPGNN in handling complex MTS data scenarios including noise outliers and domainspecific factors. 3) Questions:  How does TPGNN handle outliers and noisy data in MTS forecasting Was this aspect tested in the experiments  Could you provide more insights on the computational complexity and time efficiency of TPGNN compared to existing methods especially for largescale MTS datasets  Is there potential for domainspecific adaptations of TPGNN for different application scenarios and how generalizable is the approach across diverse MTS data domains 4) Limitations:  The paper focuses on synthetic and benchmark datasets which may not fully capture the complexity of realworld MTS scenarios.  More detailed discussions on the limitations and challenges faced by TPGNN in handling diverse MTS datasets would enrich the paper. To enhance the paper addressing the weaknesses providing insights on limitations and further discussing the applicability of TPGNN in realworld scenarios would be valuable. Moreover clarifying the implementation details and computational aspects of TPGNN would strengthen the overall contribution of the work.", "vGQiU5sqUe3": " Peer Review  1) Summary: The paper explores the integration of contrastive representation learning with reinforcement learning (RL) for the purpose of directly acquiring good representations in RL algorithms particularly in goalconditioned RL tasks. The authors demonstrate that contrastive RL methods outperform prior noncontrastive methods in various RL tasks including offline RL settings without the need for data augmentation or auxiliary objectives. The paper provides a theoretical framework linking contrastive learning with RL introduces new algorithms based on contrastive RL and presents empirical results across different tasks.  2) Strengths:  The paper addresses an important issue in RL: the acquisition of good representations without relying on auxiliary perception losses.  The novel approach of utilizing contrastive representation learning as RL algorithms themselves is wellmotivated and supported by theoretical analysis.  The experiments cover a comprehensive evaluation comparing the proposed contrastive RL methods against prior methods in various RL tasks and demonstrating superior performance.  The paper is wellstructured with detailed explanations of the methods clear theoretical derivations and thorough experimental results.  3) Weaknesses:  The paper lacks a more indepth discussion on the potential limitations and challenges of contrastive RL such as scalability to largerscale problems and computationally intensive settings.  The experimental section could benefit from more insights into the hyperparameters selection process and sensitivity analysis to further understand the algorithms robustness.  4) Questions:  Can the proposed contrastive RL methods handle more complex RL tasks with highdimensional state and action spaces  How generalizable are the findings of this study to realworld applications and more diverse environments beyond the tasks presented in the paper  Are there any considerations for the interpretability of the learned representations in contrastive RL for practical RL applications  5) Limitations:  The paper primarily focuses on goalconditioned RL tasks limiting the scope of application to broader RL problems.  The absence of performance comparisons with more traditional RL algorithms may restrict the assessment of the general effectiveness of contrastive RL in broader RL settings.  The complexity of contrastive RL methods and the potential challenges of scaling the algorithms to more complex tasks are relatively underexplored. Overall the paper presents an innovative approach that shows promising results in enhancing representation learning in RL tasks. Further exploration of the scalability and generalizability of contrastive RL methods could enhance the impact of the work in the RL research community.", "uOdTKkg2FtP": " Peer Review: 1) Summary: The paper proposes two new methods offteam belief learning (OTBL) and offteam offbelief learning (OTOBL) to address the issues faced by offbelief learning (OBL) in the zeroshot coordination (ZSC) setting in the game Hanabi. OTBL aims to improve the accuracy of the belief model for a target policy by training offteam while OTOBL aims to reduce testtime covariate shift by training on broader distributions. The methods are evaluated in variants of Hanabi demonstrating improved performance in intraAXP and adhoc teamplay scenarios. 2) Strengths:  The paper addresses an important problem in multiagent reinforcement learning and proposes novel methods to improve performance.  The empirical evaluations in Hanabi demonstrate the effectiveness of the proposed methods in reducing covariate shift and enhancing coordination.  Clear explanations of the proposed methods and their applications in the ZSC setting are provided aiding in understanding the contributions. 3) Weaknesses:  The paper lacks a comparison with existing related works in multiagent reinforcement learning which could provide a better context for the proposed methods.  The theoretical underpinnings and mathematical formulations of OTBL and OTOBL could be further elaborated to enhance the technical depth of the paper.  The limitations and assumptions of the proposed methods are not extensively discussed which could affect the generalizability of the findings. 4) Questions:  How do OTBL and OTOBL address the specific issues of covariate shift in belief models and neural policies in the context of ZSC in Hanabi  Are there any scalability concerns when applying these methods to larger multiagent environments beyond Hanabi  How sensitive are the performance improvements achieved by OTBL and OTOBL to hyperparameter choices and varying conditions in different multiagent scenarios 5) Limitations:  The heuristics employed in OTBL and OTOBL are approximations and may not converge to optimal solutions in all cases.  The assumption that policies are on distribution when using OTBL may limit its effectiveness in scenarios with significant distributional discrepancies.  The generalizability of the findings to other environments and the robustness of the methods to varying conditions need further exploration. Overall the paper makes a valuable contribution to the field of multiagent reinforcement learning by introducing innovative methods to mitigate covariate shift in ZSC settings. Addressing the weaknesses and limitations identified above could further enhance the impact and applicability of the proposed methods.", "nxl-IjnDCRo": "Peer Review: 1) Summary: The paper investigates the behavior of Diffusionbased Deep Generative Models (DDGMs) by analyzing the transition point from generating to denoising during the backward diffusion process. The authors propose a new model called DAED which splits DDGMs into a denoiser and a generator. Experimental validation is provided on three datasets to assess the performance and transferability of DDGMs and DAED. 2) Strengths and weaknesses: Strengths:  The paper offers a comprehensive analysis of DDGMs and provides insights into the generative and denoising capabilities.  The introduction of DAED and the experimental validation on multiple datasets add valuable contributions to the field.  The paper is wellstructured and logically progresses from problem statement to methodology results and discussion.  The experiments are detailed and wellsupported providing clear evidence to support the proposed hypotheses.  Figures and tables are effectively used to present the results and comparisons. Weaknesses:  The paper could benefit from a more detailed discussion of the limitations and potential drawbacks of the proposed approach.  Some technical details might be challenging for readers unfamiliar with the topic requiring clarification or simplification.  Additional comparison with existing models or approaches could enhance the discussion regarding the novelty and significance of the proposed method. 3) Questions:  How does the proposed DAED model compare with other stateoftheart generative models in terms of performance metrics  Can the contribution of splitting DDGMs into denoising and generating parts be further enhanced by incorporating additional components or optimizations  How does the training complexity and computational requirements of the DAED model compare with traditional DDGMs 4) Limitations:  The study focuses on experimental validation on specific datasets further exploration on a broader range of datasets could enhance the generalizability of the findings.  The proposed approach of dividing DDGMs into denoising and generating parts may introduce challenges in model interpretability and training convergence that need to be addressed.  The paper highlights the benefits of DAED over standard DDGMs but further investigation into scenarios where DDGMs might outperform DAED could provide a more balanced perspective. In conclusion the paper presents a valuable investigation into the behavior of DDGMs and introduces an innovative approach with the DAED model. The thorough experimental validation and analysis contribute significantly to the understanding of DDGMs generative and denoising capabilities. Consideration of the limitations and potential future directions could further strengthen the impact of the research.", "oW4Zz0zlbFF": " Summary: The paper investigates the phenomenon of \"benign overfitting\" in gradientbased meta learning with overparameterized models. Specifically they focus on meta linear regression models in the context of meta learning scenarios where total data is less than model dimensions. The study provides theoretical analysis simulates numerical experiments and discusses the interplay of data heterogeneity adaptation and benign overfitting in gradientbased meta learning.  Strengths: 1. Novel Analysis: The paper delves into an intriguing aspect of benign overfitting in the context of overparameterized meta learning shedding light on a less explored area. 2. Theoretical Contribution: The study offers a comprehensive theoretical framework to understand generalization performance in meta learning emphasizing the nuances of data heterogeneity and model adaptation. 3. Practical Relevance: By focusing on linear models the findings are applicable to practical scenarios like fewshot learning enhancing the papers practical relevance.  Weaknesses: 1. Limited Scope: The analysis is confined to linear models limiting the generalizability of findings to more complex neural network architectures commonly used in meta learning. 2. Complexity: The technical content may be challenging for readers unfamiliar with the field necessitating a high level of expertise to grasp the detailed insights presented in the paper.  Questions: 1. How do the findings in meta linear regression models extend to more complex neural network architectures commonly used in practical meta learning scenarios 2. How can the insights on benign overfitting in gradientbased meta learning be leveraged to optimize hyperparameters in realworld applications 3. Are there specific realworld applications or datasets where the phenomenon of benign overfitting in meta learning has been observed or can be harnessed effectively  Limitations: 1. Model Restrictions: The focus on linear models may oversimplify the complexities of realworld meta learning applications that often involve nonlinear relationships and more sophisticated models. 2. Generalization: The findings and analysis provided may have limited generalizability to meta learning models beyond linear regression setups warranting further validation in diverse settings. Overall the paper presents a valuable contribution to understanding the delicate dynamics of overparameterized meta learning models and benign overfitting and it opens up avenues for future research in more diverse and complex scenarios.", "suplyBhTDjC": " Peer Review  Summary The paper introduces a novel update rule Clairvoyant Multiplicative Weights Update (CMWU) for convergence to Coarse Correlated Equilibria (CCE) in general games. CMWU is shown to achieve constant regret in all normalform games surpassing previous stateoftheart convergence rates for uncoupled online learning dynamics. The paper explores the implementation and convergence properties of CMWU highlighting its efficiency and simplicity.  Strengths and Weaknesses Strengths: 1. The paper presents a novel and promising algorithm for achieving convergence to CCE in general games. 2. The theoretical analysis and proofs establish the efficiency and computational feasibility of CMWU. 3. The comparison with existing stateoftheart techniques and the discussion of implications for game theory and machine learning are insightful. Weaknesses: 1. The paper could benefit from providing more detailed practical implications and applications of CMWU beyond theoretical analysis. 2. The discussion on the limitations and potential challenges in realworld implementations could help enhance the practical relevance of the proposed algorithm.  Questions 1. How does the computational efficiency and scalability of CMWU compare to existing online learning algorithms in realworld scenarios 2. Are there specific types of games or scenarios where CMWU may outperform other algorithms and if so what are the distinguishing factors 3. How does the proposed algorithm account for adversarial behavior or strategic manipulation in game settings especially in cases where agents might deviate from the update rule 4. Could the authors elaborate on the potential extensions or modifications to CMWU that could further enhance its performance or applicability in complex game environments  Limitations 1. The discussion on the practical implementation challenges especially in scenarios with limited computational resources or realtime decisionmaking requirements is lacking. 2. The paper focuses heavily on theoretical proofs and convergence guarantees but more empirical validation or experimental results could strengthen the practical relevance of CMWU. Overall the paper presents a significant contribution to the field of game theory and online learning algorithms with the introduction of CMWU. Addressing the raised questions and limitations could further enhance the impact and practical applicability of the proposed algorithm.", "sof8l4cki9": "Peer Review: \"Fast Equilibrium Conjecture for ScaleInvariant Neural Networks Trained by SGD with Weight Decay\" 1) Summary The paper presents a theoretical study on the Fast Equilibrium Conjecture for scaleinvariant neural networks trained by Stochastic Gradient Descent (SGD) with Weight Decay (WD). The authors introduce a novel time and parameter rescaling approach to investigate the hidden progress in neural networks equipped with normalization layers. They provide theoretical derivations supporting the Fast Equilibrium Conjecture showing that the limiting distribution of SGDWD on scaleinvariant loss converges to a unique equilibrium distribution for various initializations. Experimental verification is conducted on a simplified linear model and a PreResNet on CIFAR10 demonstrating the benefits of the mixing process for generalization. 2) Strengths and Weaknesses Strengths: The paper makes significant contributions by providing a theoretical framework for understanding the hidden progress in scaleinvariant neural networks trained by SGDWD. The rescaling approach and theoretical results are welldeveloped offering insights into the generalization benefits of the mixing process. The experimental validation supports the theoretical claims. The paper is wellstructured and offers detailed derivations and explanations. Weaknesses: The paper could benefit from more clarity in the presentation of complex mathematical derivations and assumptions. The experiments could be expanded to include more diverse datasets and models to enhance the generalizability of the findings. Providing more interpretation of the experimental results in the context of practical implications for deep learning applications would enrich the discussion. 3) Questions  Could the authors elaborate on the scalability of the proposed theoretical framework to larger and more complex neural networks beyond the simplified models considered in the experiments  How does the proposed rescaling approach compare to existing methodologies for analyzing the convergence properties of optimization algorithms in deep learning  What are the implications of the theoretical results for optimizing training procedures in largescale deep learning applications 4) Limitations The main limitation of the paper lies in the complexity of the mathematical derivations and assumptions which may make it challenging for nonexperts to understand the intricacies of the theoretical framework. Additionally while the experimental results provide evidence for the theoretical claims more extensive experiments on a variety of datasets and architectures would strengthen the practical relevance of the findings. Furthermore the direct translation of the theoretical results to practical applications in deep learning may require further analysis and validation. In conclusion the paper offers valuable insights into the theoretical understanding of hidden progress in neural networks trained using SGD with WD. The theoretical advancements and experimental validation provide a strong foundation for future research in understanding the implicit bias and generalization properties of deep learning models. With some enhancements in clarity and additional experiments the paper could further solidify its contributions to the field. Overall Rating: 4 stars", "zJNqte0b-xn": " Summary The paper investigates minmax optimization problems in the Riemannian setting focusing on the convergence rates of the Riemannian corrected extragradient (RCEG) method and Riemannian gradient descent ascent (RGDA) method. The study provides theoretical results on the lastiterate and averageiterate convergence rates for both smooth and nonsmooth cases considering both deterministic and stochastic scenarios. The experiments conducted on robust principal component analysis (RPCA) tasks verify the theoretical findings.  Strengths and Weaknesses  Strengths 1. The paper addresses an important and challenging problem of minmax optimization in the Riemannian setting. 2. The theoretical results on the convergence rates of RCEG and RGDA are detailed and wellsupported. 3. The experimental evaluation provides empirical evidence supporting the theoretical findings. 4. The paper is wellorganized and presents a systematic analysis of the Riemannian counterparts of Euclidean optimal firstorder methods.  Weaknesses 1. The paper might be challenging to grasp for readers not wellversed in advanced mathematical concepts and Riemannian geometry. 2. More explanation and intuition could be beneficial to clarify some complex concepts and results. 3. The limitations and assumptions of the proposed algorithms could be further discussed for a more comprehensive understanding.  Questions 1. How do the results of this study contribute to the broader field of machine learning particularly in applications involving robust optimization and optimization on manifolds 2. How sensitive are the algorithms proposed in this study to the choice of hyperparameters such as step sizes in practice 3. Can the results presented in the paper be extended to more general nonconvexnonconcave scenarios in Riemannian optimization 4. What are the implications of the observed differences in convergence rates between lastiterate and timeaverageiterate convergence in the experimental results 5. How do the findings of this study impact the design of optimization algorithms for practical machine learning tasks with inherent geometric constraints  Limitations 1. The complexity of the theoretical formalism and mathematical concepts may limit the accessibility of the paper to a wider audience. 2. The assumptions made regarding the Riemannian manifolds and curvature parameters could restrict the applicability of the proposed algorithms to certain specific scenarios only. 3. The experiments conducted only on RPCA tasks may not fully capture the generalizability of the proposed algorithms to diverse machine learning applications. 4. The paper lacks a detailed discussion on potential practical challenges and considerations when implementing the proposed methods in realworld machine learning settings. Overall the paper makes significant contributions to the field of optimization on Riemannian manifolds presenting novel theoretical results and showcasing promising empirical validation. However addressing the highlighted weaknesses and limitations could enhance the impact and practical relevance of the study.", "tIqzLFf3kk": " Peer Review: 1) Summary: This paper delves into the behavior of the rank of deep neural networks exploring the concept of rank deficiency and its implications. The study combines theoretical analysis with empirical validation shedding light on the monotonic decreasing behavior of network ranks with depth. The paper provides theoretical insights into rank diminishing introduces numerical tools to validate rank behavior and uncovers the phenomenon of independence deficit in multiclass classification networks. The findings are supported by empirical results from ResNets deep MLPs and Transformers on ImageNet aligning well with the proposed theoretical framework. 2) Strengths and weaknesses: Strengths:  The paper addresses an important aspect of deep neural networks the rank behavior with a comprehensive investigation combining theoretical analysis and empirical validation.  The theoretical framework proposed including the Principle of Rank Diminishing and Structural Impetus of Strictly Decreasing provides a solid foundation for understanding rank behavior in deep networks.  The development of numerical tools for validating rank behavior such as the measurement of partial ranks of Jacobians is a significant methodological contribution.  The identification of the independence deficit phenomenon in classification networks adds a novel dimension to the study highlighting the implications of rank deficiency on network performance.  The empirical validation on benchmark networks like ResNets MLPs and Transformers on ImageNet supports the theoretical claims enhancing the credibility of the findings. Weaknesses:  While the theoretical framework is robust and insightful the paper might benefit from clearer explanations or examples to aid in understanding complex concepts especially for readers less familiar with the subject.  The paper could provide more detailed explanations on the practical implications of rank deficiency in deep networks and how it relates to realworld applications or challenges.  Some components like the discussion on limiting behavior for infinitely deep networks could be further elaborated or simplified to make them more accessible to a wider audience.  The analyses and conclusions heavily focus on the theoretical and numerical aspects possibly overlooking broader implications or practical considerations that could enhance the relevance of the findings. 3) Questions:  How does the proposed numerical tools sensitivity to noise impact the validity of the results particularly in practical applications where noise is prevalent  Can the findings on rank deficiency and independence deficit be extrapolated to different types of datasets or applications beyond ImageNet  How might the concept of rank deficiency in deep networks influence the design or optimization of neural network architectures in future research  Are there specific factors or considerations that may limit the generalizability of the theoretical assertions to a broader range of deep learning models or applications 4) Limitations:  The overarching focus on theoretical analysis and numerical validation may limit the generalizability of the findings to a wider spectrum of deep learning applications beyond the specific benchmarks used in the study.  The complexity of some theoretical concepts or mathematical frameworks introduced in the paper might pose challenges for readers without a deep understanding of the subject matter.  The empirical validation primarily confined to ImageNet datasets might restrict the applicability of the findings to datasets with different characteristics or domainspecific challenges. In conclusion the paper presents a rigorous investigation into the rank behavior of deep neural networks offering valuable insights into a key structural aspect of machine learning models. Further clarifications on the practical implications of rank deficiency and independence deficit along with considerations for generalizability and realworld applications could enhance the impact and relevance of the study. Additionally efforts to simplify complex theoretical concepts without compromising depth could improve accessibility and understanding for a broader audience.", "lMrpZ-ycIaT": " Summary The paper introduces novel selfsupervised learning frameworks inspired by natural language processing for mental state decoding using neuroimaging data. The frameworks pretrain deep learning models on a largescale neuroimaging dataset and evaluate their performance on benchmark mental state decoding tasks. The results show that pretrained models generally outperform baseline models with the causal language modeling framework outperforming others.  Strengths and Weaknesses Strengths: 1. Novel application of selfsupervised learning techniques from NLP to neuroimaging data. 2. Utilization of a large diverse neuroimaging dataset for pretraining. 3. Clear evaluation of different frameworks on benchmark mental state decoding datasets. 4. Public availability of code training data and pretrained models. Weaknesses: 1. The work focuses on model performance in mental state decoding without exploring inferences about decoded mental states. 2. Lack of comparison with contrastive learning techniques. 3. Limited discussion on the interpretability of model decisions. 4. Training details such as hyperparameters and model architecture are briefly discussed.  Questions 1. How do the proposed frameworks handle the challenges of high dimensionality and low sample size inherent in neuroimaging datasets 2. Can the frameworks be generalized to other types of mental states beyond those evaluated in the benchmark datasets 3. What are the implications of the observed overfitting in the transformer model trained in the causal sequence modeling framework 4. How are the models performances affected by variations in individual datasets acquisition sites and experimental conditions  Limitations 1. Neglect of interpreting the association between decoded mental states and brain activity. 2. Lack of comparison with contrastive learning techniques. 3. Limited investigation into the impact of different acquisition sites and experimental conditions on model performance. 4. The paper does not explore potential negative social impacts comprehensively. Overall the paper presents innovative approaches integrating selfsupervised learning techniques from NLP into neuroimaging research showcasing promising results in mental state decoding. Further exploration of interpretability and comparison with alternative learning techniques would enhance the studys contributions to the field.", "rQ1cNbi07Vq": " Peer Review  1) Summary: The paper introduces the use of Shapley value in the frequency domain to quantify the impact of individual frequency components on convolutional neural networks (CNNs) in the context of adversarial robustness. The authors explore the disparity between frequency components at the instance level investigate the impact of frequency components on adversarial training and attacks and propose a Shapleyvalue guided data augmentation technique for improving robustness. Experimental results on image classification benchmarks demonstrate the effectiveness of the proposed method in improving robustness.  2) Strengths and weaknesses: Strengths:  Introduces a novel approach using Shapley value to quantify the impact of frequency components on CNNs.  Provides insights into instancelevel disparity in the contribution of frequency components.  Investigates adversarial training and attacks in the frequency domain leading to novel hypotheses.  Proposes a data augmentation technique based on Shapley value which is shown to improve robustness effectively.  Conducts thorough experiments on image classification benchmarks to validate the proposed method. Weaknesses:  The paper lacks a detailed explanation of the implementation of the proposed Shapleyvalue guided data augmentation technique.  The limitations and assumptions of the Shapley value approach in the frequency domain could be discussed further.  The paper could provide a more indepth comparison with existing adversarial defense methods beyond high frequency filtering.  The robustness of the proposed method to more advanced adversarial attacks could be tested.  3) Questions:  How did the authors validate the stability and reliability of the Shapley value calculations especially given the complexity of sampling for each sample  Can the proposed method be easily extended to other neural network architectures beyond CNNs as indicated in the future work section  What are the implications of the findings on the broader landscape of adversarial attacks and defenses in deep learning models  4) Limitations:  The study focuses mainly on CNNs limiting the generalizability of the findings to other neural network architectures.  The complexity of sampling for obtaining stable Shapley values for each sample may pose challenges in terms of scalability and practical implementation.  The lack of indepth comparison with existing adversarial defense methods may limit the full scope of the proposed methods effectiveness. In conclusion the paper presents a novel and insightful approach to quantifying the impact of frequency components on CNNs using Shapley value in the frequency domain. The method shows promise in improving adversarial robustness in image classification tasks. Addressing the outlined weaknesses and limitations could enhance the papers contribution in the field of adversarial attacks and defenses in deep learning models.", "p_BVHgrvHD4": " Peer Review: 1) Summary: The paper introduces a novel informationtheoretic framework to analyze the sample complexity of machine learning models particularly deep neural networks. By studying the sample complexity of learning processes generated by deep ReLU neural networks and deep networks with bounded sum of weights the paper establishes linear and quadratic sample complexity bounds respectively in terms of network depth. The framework allows for exploration of theoretical aspects that existing analyses fail to address presenting promising results. It bridges the gap between theoretical understanding and empirical observations in modern machine learning. 2) Strengths:  The paper introduces a novel informationtheoretic framework for analyzing sample complexity offering a fresh perspective in studying machine learning models.  It presents theoretical results on sample complexity for deep neural networks providing insights into the data requirements of such models.  The results on sample complexity for different types of deep networks are significant and suggest efficient learning processes contradicting previous highorder dependencies and exponential bounds.  The analysis encompasses a broad range of multilayer environments and demonstrates the potential of the proposed framework in addressing complex learning scenarios. 3) Weaknesses:  The paper focuses primarily on theoretical analysis lacking empirical validation or practical implementation of the proposed framework.  While the theoretical results are promising further discussion on the applicability and scalability of the framework in realworld scenarios would enhance the relevance of the study.  The readability of the paper could be improved by incorporating more illustrative examples or figures to aid in understanding the concepts and results presented.  The paper could benefit from discussing potential challenges or limitations associated with the proposed informationtheoretic framework and how they might impact practical implementations. 4) Questions:  How transferable are the sample complexity bounds derived in this study to realworld applications with diverse datasets and network architectures  Can the proposed informationtheoretic framework be extended to analyze different types of machine learning problems beyond deep neural networks  What are the computational implications of utilizing the framework in practical scenarios considering the complexity involved in informationtheoretic analyses 5) Limitations:  The paper heavily focuses on theoretical analysis neglecting practical implications or validations of the proposed framework in realworld scenarios.  The complexity and abstract nature of the informationtheoretic framework may limit its straightforward application by practitioners or researchers without a strong theoretical background.  The scalability of the framework to largescale datasets or complex network architectures might be challenging requiring further exploration and optimization for practical usability. Overall the paper presents a significant contribution with its informationtheoretic framework for analyzing sample complexity in machine learning particularly deep neural networks. However addressing the mentioned weaknesses and limitations would enhance the papers impact and applicability in the field of machine learning research.", "uuaMrewU9Kk": "Peer Review 1) Summary: The paper introduces a novel multitask reinforcement learning (RL) technique for offline RL with diverse datasets of varying quality. The proposed approach leverages skillbased decomposition of tasks joint learning of skills and tasks using Wasserstein autoencoder and data augmentation with imaginary trajectories. Experimental results on robotic manipulation and drone navigation tasks demonstrate the superiority of the proposed method compared to stateoftheart algorithms particularly in scenarios with mixedquality datasets. 2) Strengths:  Innovative Approach: The paper presents a novel approach that addresses the challenges of multitask RL with varyingquality datasets in offline settings.  Skillbased Decomposition: The use of skillbased decomposition for task representation is a strong foundation for achieving robust learning across tasks.  Qualityaware Regularization: The incorporation of qualityweighted loss for task decomposition helps in handling datasets with heterogeneous quality effectively.  Data Augmentation: The introduction of data augmentation with imaginary trajectories based on highquality skills is a valuable contribution to improving learning performance. 3) Weaknesses:  Theoretical Explanation: Certain parts of the theoretical explanations in the paper are complex and may be difficult for readers unfamiliar with the topic to understand. Simplifying certain sections would improve readability.  Evaluation: While the results are promising a deeper analysis of the performance metrics and comparison metrics used could further strengthen the experimental evaluation. 4) Questions:  Generalization: How well does the proposed model generalize to tasks beyond robotic manipulation and drone navigation  Scalability: Have scalability issues been considered when applying this technique to larger or more complex multitask scenarios  Sensitivity Analysis: How sensitive is the proposed method to hyperparameters and dataset configurations 5) Limitations:  Complexity: The complexity of the proposed method could introduce challenges in implementation and deployment in realworld settings.  Dataset Dependency: The effectiveness of the approach heavily relies on the quality and nature of the datasets used which might limit its applicability in diverse scenarios. In conclusion the paper introduces an innovative approach to multitask RL with diverse offline datasets showing promising results in robotic and drone tasks. The methods complexity and reliance on dataset quality are potential limitations that need to be further addressed. Overall Rating: 45", "toleacrf7Hv": " Peer Review:  Summary: The paper introduces a novel criterion the semidual Brenier formulation for selecting convex potentials in Optimal Transport (OT) models. The study showcases the application of this criterion in various OT models including Sinkhorn model Input Convex Neural Networks (ICNN) and Smooth Strongly Convex Nearest Brenier (SSNB) models. Experimental results on synthetic and Domain Adaptation (DA) datasets validate the effectiveness of the proposed criterion in selecting the most accurate potential models. The study provides theoretical guarantees for model selection and error minimization using the semidual Brenier formulation.  Strengths: 1. Novel Criterion: The introduction of the semidual Brenier formulation for selecting convex potentials in OT models is a significant contribution addressing the challenge of parameter tuning and model selection. 2. Theoretical Analysis: The inclusion of theoretical results stability guarantees and propositions provides a strong foundation for the proposed criterions efficacy. 3. Experimental Validation: The experiments on synthetic datasets and the DA task demonstrate the practical applicability and effectiveness of the semidual criterion in selecting models that minimize errors and approximate ground truth maps. 4. Comparison Across Models: By testing multiple OT models like Sinkhorn ICNN and SSNB the paper provides a comprehensive evaluation of the proposed criterions performance in various scenarios.  Weaknesses: 1. Clarity Concerns: The paper can be challenging to follow due to the complexity of the mathematical formulations and technical terms. Simplifying the language and providing more intuitive explanations could enhance readability. 2. Experiment Detail: While the paper reports results on synthetic and DA datasets additional insights into hyperparameter tuning convergence characteristics and computational efficiency could add depth to the experimental analysis. 3. Generalization: The paper focuses on convex potentials and specific OT models limiting the generalizability of the proposed criterion to a broader set of OT scenarios and models with nonconvex potentials.  Questions: 1. How sensitive is the semidual criterion to the choice of hyperparameters in different OT models Are there specific scenarios where the criterion may not perform optimally 2. The paper discusses regularization for the Sinkhorn model to address divergence issues. Could further insights be provided on the impact of regularization strength on model performance and criterion selection 3. Are there any computational challenges or limitations in applying the proposed semidual criterion in practice especially at scale or in scenarios with highdimensional data  Limitations: 1. Limited Scope: The focus on convex potentials and specific OT models may limit the applicability of the proposed criterion to a broader range of OT applications with diverse cost functions or nonconvex potentials. 2. Complexity: The intricate mathematical formulations and technical language may serve as a barrier for readers not deeply involved in OT research. 3. Validity in Realworld Data: While the experiments on synthetic and DA datasets are insightful the generalizability and performance of the semidual criterion on realworld datasets with more complex structures need further exploration. Overall the paper presents a valuable contribution to the field of OT by introducing a novel criterion for model selection. Further clarity in presentation and experimental detail could enhance the impact and applicability of the proposed semidual Brenier formulation.", "lbQTJN42uea": " Peer Review: Summary: The paper proposes the first subquadratictime algorithm for solving Kronecker regression problems efficiently thereby improving the running time of the alternating least squares (ALS) algorithm for Tucker decompositions. The technique combines leverage score sampling and iterative methods to handle Kronecker structured design matrices. The algorithm is further extended to handle blockdesign matrices achieving subquadratictime for Kronecker ridge regression and updating factor matrices in ALS. The paper presents theoretical results algorithms and experimental validations on synthetic data and realworld image tensors. Strengths: 1. Novel Approach: The paper introduces a new and innovative algorithm for solving Kronecker regression efficiently filling an important gap in existing literature. 2. Theoretical Contributions: The paper provides detailed theoretical results such as lemma on leverage score sampling algorithms convergence guarantees and novel methods for Kroneckermatrix multiplication. 3. Experimental Validation: The experimental section presents comprehensive results demonstrating the effectiveness of the proposed algorithm. 4. Detailed Explanations: The paper provides thorough explanations of the proposed methods and theoretical underpinnings making it accessible to a wide range of readers. Weaknesses: 1. Clarity: Some sections of the paper especially the formal mathematical notations and proofs may be difficult for readers not wellversed in the specific field of study. Providing more intuitive explanations or practical examples could help improve clarity. 2. Reproducibility: While the paper mentions that the code is available on GitHub providing more details on the implementation configuration setup and running instructions in the paper would enhance reproducibility. Questions: 1. How does the proposed algorithm compare to existing stateoftheart methods in terms of accuracy and scalability on realworld datasets 2. Are there any specific assumptions made in the theoretical analyses that may limit the applicability of the algorithm in certain scenarios 3. How sensitive is the performance of the algorithm to hyperparameter choices such as the error parameter \" and regularization strength Limitations: 1. Generalization: The papers focus on the specific problem of Kronecker regression and ALS for Tucker decompositions may limit its broader applicability to other tensor decomposition methods or regression problems. 2. Complexity: The complexity of the algorithm and theoretical formulations may hinder its adoption by practitioners not deeply familiar with the underlying mathematical concepts. In conclusion the paper presents a significant contribution in developing a subquadratictime algorithm for Kronecker regression showcasing promising results through both theoretical analyses and experimental validations. Improvements in accessibility reproducibility and comparisons with existing methods could further enhance the impact of this research.", "wiBEFdAvl8L": " Peer Review  1) Summary The paper presents GLIPv2 a unified model for grounded VisionLanguage (VL) understanding that combines localization tasks with VL understanding tasks. The model introduces novel pretraining tasks such as phrase grounding and interimage regionword contrastive learning to achieve mutual benefits between localization and understanding tasks. Experimental results demonstrate that GLIPv2 achieves near StateoftheArt (SoTA) performance on various localization and understanding tasks including zeroshot and fewshot adaptation. The model also shows superior grounding capability and strong transfer learning abilities. Overall GLIPv2 offers a unified framework for VL representation learning.  2) Strengths  The paper addresses an important challenge by unifying localization tasks and VL understanding tasks in a single model.  The introduction of novel pretraining tasks such as interimage regionword contrastive learning enhances the models performance.  GLIPv2 achieves competitive near SoTA performance on a wide range of tasks showcasing its effectiveness and potential for practical applications.  The experimental results demonstrate the models strong zeroshot and fewshot adaptation capabilities as well as its superior grounding ability on VL understanding tasks.  The detailed explanation of the model architecture pretraining process and transfer learning methods provide a comprehensive understanding of GLIPv2.  3) Weaknesses  The paper could benefit from clearer descriptions of the experimental setups and methodologies used for evaluation.  More insight into the computational complexity and efficiency of the model implementation would be helpful for practical considerations.  While the results are promising further analysis on the generalizability of the model to different datasets and scenarios would strengthen the papers conclusions.  Additional discussion on potential limitations or challenges in realworld applications could provide a more balanced perspective on the models performance.  4) Questions  How does the proposed interimage regionword contrastive learning task contribute to improving the models performance compared to traditional pretraining methods  Can you elaborate on the specific datasets used for pretraining and how they were selected to ensure diverse and comprehensive training data  What considerations were taken into account for ensuring privacy and bias mitigation in the web data used for pretraining  Are there plans to further validate the models performance in realworld applications or specific domains outside of the controlled experiments conducted  5) Limitations  The paper could benefit from a more indepth discussion on the scalability and deployment readiness of GLIPv2 in realworld environments.  There should be more transparency regarding potential biases or ethical concerns related to the web data used for pretraining.  Although the model achieves impressive results further investigation into interpretability and explainability aspects may be necessary for practical deployment. Overall the paper presents a novel approach to tackling the unification of localization and VL understanding tasks through the GLIPv2 model. The experimental results are promising but further analysis and validation are needed to fully assess the models practical implications and limitations. Additional clarity on the implementation details and considerations for realworld applications would enhance the impact and relevance of GLIPv2 in the field of VL understanding.", "s776AhRFm67": " Summary: The paper presents an oracleefficient algorithm RoBoost for boosting the adversarial robustness of barely robust learners. It explores the notion of barely robust learning showing its equivalence with strongly robust learning. The algorithm boosts the robustness of learners by repeatedly calling a blackbox learner. The paper provides theoretical results strategies and practical implications for boosting adversarial robustness.  Strengths: 1. The paper is wellstructured providing clear definitions theorems and algorithms for boosting robustness. 2. The theoretical framework is rigorous and provides insights into the relationship between barely robust learning and strongly robust learning. 3. The experiments conducted on synthetic and MNIST datasets showcase the practical utility of the proposed algorithm. 4. The paper addresses important questions in adversarial robustness and offers a fresh perspective on boosting algorithms for robust learning.  Weaknesses: 1. The complexity of the algorithms and theoretical concepts might make it less accessible to readers without a strong background in the field. 2. The paper primarily focuses on the realizable setting and exploring the agnostic setting further could strengthen the practical implications. 3. The experiments are limited to linear SVM as a baseline barely robust learner and more diverse experiments with different learners could have provided a broader perspective.  Questions: 1. Can the RoBoost algorithm be extended to nonlinear models or more complex learners beyond linear SVM 2. How do the results change in scenarios with different perturbation sets or multiclass classification tasks 3. Are there considerations for computational efficiency outlined for scaling the boosting algorithm in larger datasets or more complex models  Limitations: 1. The experiments are limited in scope with only linear SVM as the barely robust learner. More extensive experiments with diverse learners and datasets could provide a comprehensive evaluation. 2. The paper mainly focuses on the equivalence between barely robust learning and strongly robust learning in the realizable setting and exploring the agnostic setting further could enhance the practical implications. 3. The computational efficiency of the boosting algorithm and the practical feasibility in realworld scenarios remain to be deeply investigated. Further exploration into certification of robustness might be crucial for practical implementations.", "skgJy0CjAO": "Summary: The paper introduces LogiGAN an unsupervised adversarial pretraining framework designed to enhance the logical reasoning abilities of language models. By automatically identifying logical reasoning phenomena in a text corpus and training models to predict maskedout logical statements LogiGAN aims to improve models reasoning capabilities. The framework implements a novel sequential GAN approach that allows for largescale pretraining with longer target length. Experiments demonstrate performance improvements on 12 datasets requiring general reasoning abilities highlighting the importance of logic in broad reasoning and showcasing the effectiveness of LogiGAN. Strengths: 1. Innovative Framework: LogiGAN presents a novel approach to enhancing logical reasoning abilities in language models through adversarial pretraining which is a significant contribution to the field. 2. Performance Improvements: The experiments show clear performance enhancements on datasets requiring reasoning abilities providing empirical evidence of LogiGANs effectiveness. 3. Ablation Studies: The ablation studies conducted on LogiGAN components offer insights into the functionality of the framework shedding light on the relative orthogonality between linguistic and logic abilities. Weaknesses: 1. Limited Justification: While the paper presents strong empirical results the justification for the chosen approach and design decisions could be expanded to provide a deeper theoretical understanding. 2. Evaluation on Limited Dataset Variety: The paper experiments on a limited set of datasets which may raise concerns about the generalizability of the findings to a broader range of tasks. 3. Complexity: The adversarial training framework and the sequential GAN approach may introduce complexity that could hinder the practicality of the framework for wider adoption. Questions: 1. How does LogiGAN compare against existing methods for improving reasoning abilities in language models such as explicit rulebased approaches or knowledgeenhanced pretraining strategies 2. Can LogiGAN be extended to incorporate domainspecific logical reasoning and how might this impact the frameworks performance on specialized tasks 3. Given the computational feasibility of LogiGAN for largescale pretraining are there any limitations or tradeoffs in terms of model efficiency or training time Limitations: 1. Generalizability: The limited scope of datasets used for evaluation may restrict the generalizability of LogiGANs impact across a wider range of reasoning tasks. 2. Complexity: The intricate design of LogiGAN including the adversarial training and sequential GAN approach may pose challenges for implementation and practical deployment in realworld applications. 3. Domain Adaptation: The effectiveness of LogiGAN for domainspecific logical reasoning tasks remains unexplored raising questions about its adaptability to specialized domains.", "ufRSbXtgbOo": " Peer Review: 1) Summary: The paper focuses on multiagent performative prediction exploring the solution to the MultiPfD problem using a decentralized optimization approach. The authors introduce the DSGDGD scheme and analyze its convergence properties providing theoretical results and conducting numerical experiments to validate their findings. The research makes significant contributions to the field by addressing a scenario relevant to consensusseeking agents in a performative prediction setting. 2) Strengths:  Theoretical Contributions: The paper presents a rigorous theoretical analysis of the MultiPfD problem introducing novel concepts such as MultiPS solutions and consensusseeking behaviors.  Algorithm Development: The DSGDGD scheme is welldesigned and analyzed offering insights into its convergence towards the MultiPS solution.  Experimental Validation: The numerical experiments provide empirical evidence supporting the theoretical claims enhancing the credibility of the proposed approach. Weaknesses:  Clarity: The paper is dense with technical details and mathematical expressions which might be challenging for nonexperts to grasp without a deep understanding of the subject matter. Some parts may benefit from clearer explanations or additional illustrative examples.  Limited Comparison: While the experiments demonstrate the effectiveness of the DSGDGD scheme the paper could benefit from a more indepth comparison with existing methods or alternative decentralized approaches for performative prediction. Questions: 1. How does the proposed DSGDGD scheme compare to other decentralized optimization algorithms in terms of convergence speed and stability for multiagent performative prediction 2. Could different choices of step sizes impact the convergence behavior of the DSGDGD scheme and if so how sensitive is the convergence to these parameters 3. Are there any practical considerations or realworld applications where the proposed methodology could be particularly beneficial or challenging to implement Limitations:  Theoretical Assumptions: The reliance on specific assumptions such as strong convexity of loss functions and uniform step size rules may restrict the generalizability of the results.  Scalability: The scalability of the proposed DSGDGD scheme to a larger number of agents or more complex scenarios could be a potential limitation that needs further investigation. In conclusion the paper presents a thorough analysis of the MultiPfD problem and offers a novel decentralized optimization scheme with promising theoretical and empirical results. Addressing the limitations and expanding the practical implications of the proposed approach could further enhance the impact of this research in the field of multiagent performative prediction.", "sexfswCc7B": " Peer Review  1) Summary The paper investigates the issue of sentencelevel repetitions in largescale neural language models and proposes a training method named DITTO to mitigate this problem. Through quantitative experiments the authors study the relationship between repetitive tokens and their previous repetitions in context. The paper finds that language models exhibit a preference for repeating the previous sentence and have a selfreinforcement effect that leads to sentencelevel repetitions. DITTO shows effectiveness in reducing repetitions without sacrificing perplexity and improving generation quality in openended text generation and summarization tasks.  2) Strengths and Weaknesses Strengths:  The paper provides a comprehensive analysis of the reasons behind sentencelevel repetitions in language models through quantitative experiments.  The proposed training method DITTO effectively mitigates sentencelevel repetitions without affecting perplexity and improves generation quality.  The experiments conducted on openended text generation and summarization tasks demonstrate the generality and effectiveness of the proposed method.  The paper is wellstructured provides clear explanations and offers detailed experimental results. Weaknesses:  The paper lacks discussion on the broader implications of the findings beyond mitigating repetitions. Exploring the impact on downstream tasks or potential applications would enhance the paper.  The paper could benefit from a more indepth analysis of the underlying reasons why the model prefers consecutive repetitions such as exploring neural network architectures or embedding characteristics.  The evaluation could be further strengthened by comparing DITTO with a wider range of existing methods for mitigating repetitions.  3) Questions  How does DITTO compare to other stateoftheart methods for mitigating repetitions in language models especially in terms of scalability and generalizability  Are there any specific scenarios or datasets where DITTO may face challenges in effectively reducing sentencelevel repetitions  Have the authors considered the computational overhead of implementing DITTO in largescale language models and are there any tradeoffs in terms of training time or resource utilization when using the proposed method  4) Limitations  The paper focuses primarily on the issue of sentencelevel repetitions and the proposed method to address it. Exploring a wider range of repetition types (wordlevel phraselevel) and their mitigation could provide a more comprehensive analysis.  While the experiments demonstrate the effectiveness of DITTO in reducing repetitions further human evaluation or qualitative analysis of generated texts could offer additional insights into the quality of the generated content.  The paper does not discuss the potential implications of reducing repetition on other aspects of text generation such as diversity coherence or fluency. Overall the paper presents valuable insights into the reasons behind sentencelevel repetitions in language models and offers a practical solution in the form of the DITTO training method. Addressing the mentioned weaknesses and limitations could further strengthen the impact and relevance of the research.", "s_PJMEGIUfa": " Peer Review:  1) Summary: The paper introduces LanguageInterfaced FineTuning (LIFT) a method that leverages pretrained language models (LMs) for nonlanguage downstream tasks without altering the model architecture or loss function. The study provides empirical evidence through extensive experiments on various classification and regression tasks demonstrating LIFTs effectiveness and discussing its properties sample complexity robustness and applications. Results indicate that LIFT performs well on lowdimensional tasks and improvements are made using techniques like contextbased prompting twostage finetuning and data augmentation. The study raises questions about the adaptability of LMs inductive biases and the impact of pretraining on LIFTs performance.  2) Strengths and Weaknesses: Strengths:  Innovative Approach: Introducing LIFT for nonlanguage tasks without altering model architecture is a novel and potentially impactful approach.  Extensive Empirical Study: The paper conducts a thorough empirical study across various tasks to validate the effectiveness and limitations of LIFT.  Robustness Analysis: The analysis of LIFTs robustness against outliers and feature corruptions enhances the credibility of the findings.  Comprehensive Evaluation: The study investigates various aspects of LIFT including sample complexity calibration and data generation providing a holistic view of its capabilities. Weaknesses:  Scope Limitation: While the study covers a wide range of tasks the focus on lowdimensional tasks limits the generalizability of the findings to highdimensional or more complex problems.  Memory Inefficiency: The paper acknowledges the memory inefficiency of LIFTGPT which could be seen as a limitation from a practical implementation standpoint.  Generalization Concern: The claim of \"nocode machine learning\" might oversimplify the complexity of realworld applications that may require more intricate model adjustments.  3) Questions:  How does LIFTGPT handle the scalability and complexity of tasks with highdimensional feature spaces or a large number of classes  What are the computational requirements for implementing LIFTGPT at scale considering memory inefficiency concerns  Can LIFT be applied to tasks beyond classification and regression such as generation tasks or reinforcement learning  4) Limitations:  Model Limitation: The study focuses on lowdimensional tasks potentially limiting the applicability of LIFT to more complex or highdimensional problems.  Bias Consideration: The paper lacks a comprehensive discussion on potential biases embedded in pretraining language models and how they may impact LIFTs predictions in realworld applications.  Complexity Concern: The simplicity of the \"nocode machine learning\" claim may not fully encapsulate the intricacies and custom requirements of implementing LIFT in diverse tasks and scenarios. Overall the paper presents an innovative approach with promising results for using language models in nonlanguage tasks. It opens up avenues for further exploration and enhancements in leveraging LM capabilities for diverse applications beyond conventional language tasks. Considerations about scalability bias mitigation and feasibility in realworld settings could strengthen the impact and practicality of LIFT in future research.", "sQiEJLPt1Qh": " Peer Review of the Scientific Paper  1) Summary: The paper proposes improved bounds and a polynomial time algorithm to find a ReLU neural network representation for any Continuous Piecewise Linear (CPWL) function. By showing that the number of hidden neurons is bounded by much slower growth functions the paper offers a significant advancement in understanding the complexity of ReLU networks representing CPWL functions. The results indicate that the expressivity of ReLU neural networks can be achieved at a lower cost through improved bounds.  2) Strengths and Weaknesses: Strengths:  The paper provides novel results establishing tighter bounds for representing CPWL functions using ReLU neural networks.  The polynomial time algorithm for finding a network representation enhances practical applicability.  The introduction of bounds that scale with the complexity of the CPWL function helps in better understanding the relationships between ReLU networks and CPWL functions.  The papers comprehensive overview of existing literature proof sketches and detailed explanations enhance clarity for the readers. Weaknesses:  The paper lacks empirical validation and practical implementations of the proposed algorithm.  The complexity and mathematical nature of the results may limit accessibility to a broader audience.  The focus on ReLU networks may overlook potential implications for other types of activation functions or neural network architectures.  3) Questions:  Can the proposed polynomial time algorithm be further optimized for efficiency or scalability in realworld applications  How do the bounds introduced in the paper compare to empirical findings or experiments on actual CPWL functions  Are there any assumptions in the bounds or algorithm that could limit their applicability in certain scenarios or for specific types of CPWL functions  How do the results of this research impact the design and training of deep neural networks in practical machine learning tasks  4) Limitations:  The mathematical focus of the paper may limit the immediate practical applications in realworld scenarios without empirical validation.  The bounds proposed are theoretical and may need further validation or refinement through experiments or case studies.  The paper primarily addresses the neural complexity of ReLU networks representing CPWL functions potentially overlooking other important aspects of neural network design and optimization. Overall the paper presents valuable insights into the neural complexity of ReLU networks representing CPWL functions. Further validation practical implementations and comparisons with empirical results could enhance the impact and applicability of the proposed bounds and algorithm in realworld machine learning tasks.", "nYrFghNHzz": " Summary This paper introduces the Supervised Clustering approach using Adaptive Fusion (SCAF) method for clustering treatments with similar effects to optimize Individualized Treatment Rules (ITR) in precision medicine. The method formulates the problem as convex optimization simultaneously clustering treatments and estimating the ITR. The theoretical guarantee for consistency of estimated coefficients is provided and simulations and real data application on cancer treatment demonstrate the superior performance of SCAF.  Strengths and Weaknesses  Strengths:  Novel approach to cluster treatments with similar effects and estimate ITR.  Formulation as convex optimization allows simultaneous clustering and ITR estimation.  Theoretical guarantee for consistent estimated coefficients.  Superior performance demonstrated in simulations and real data application.  Weaknesses:  The method complexity may limit its applicability without specialized expertise.  Limited discussion on the computational efficiency and scalability of the proposed algorithm.  The approach may require extensive tuning of parameters for optimal performance and interpretation.  Questions 1. How does the SCAF method compare to other existing methods in terms of computational speed and scalability 2. Can the proposed method handle missing data or noisy measurements in the treatment space 3. How sensitive is the performance of SCAF to the choice of the tuning parameter and weight in the optimization problem 4. Are there any potential biases introduced by the clustering process that could impact the accuracy of the estimated ITR  Limitations  Lack of detailed discussion on potential biases introduced by clustering or estimation assumptions.  Limited exploration of the methods robustness to noisy or incomplete data in realworld applications.  The extensive parameter tuning process required for optimal performance may limit practical usability in clinical settings.  The generalizability of the approach to different disease types or treatment modalities is not fully explored. Overall the paper introduces a novel method with promising results but could benefit from further discussion on practical considerations limitations and realworld applicability. Further validation and exploration in diverse clinical settings will be crucial for establishing the methods robustness and utility in precision medicine.", "sjaQ2bHpELV": "Peer Review 1) Summary The paper addresses the problem of optimizing policies with mixed scopes in online reinforcement learning focusing on healthcare settings with combination therapy. It introduces novel algorithms for online learning and provides theoretical analysis for regret bounds. The study explores the use of causal diagrams and introduces a decomposition approach for model simplification. Simulations validate the proposed methods across different causal diagrams. 2) Strengths  The paper addresses an important and relevant issue in healthcare considering combination therapies in the context of policy optimization.  It introduces novel algorithms for learning optimal policies with mixed scopes providing theoretical analysis and regret bounds.  The use of causal diagrams and the decomposition approach for simplifying models is innovative and contributes to understanding the impact of actions in the healthcare environment.  The simulations validate the effectiveness of the proposed algorithms across various causal diagrams providing empirical evidence for the theoretical claims. 3) Weaknesses  The paper may benefit from a clearer explanation of the practical implications of the findings in an actual healthcare setting. How could the proposed methods be practically implemented and integrated with existing medical decisionmaking systems  While the theoretical developments are substantial the paper could elaborate on the scalability and computational efficiency of the algorithms in realworld scenarios with large datasets.  The paper could be enhanced by discussing potential limitations of the proposed algorithms and addressing possible challenges or drawbacks in implementation. 4) Questions  How do the proposed algorithms compare to existing policy optimization methods in terms of computational efficiency and scalability  How sensitive are the results to changes in the underlying assumptions or model structures Would the algorithms perform similarly with different types of causal diagrams  Can the findings of this study be extended to other domains beyond healthcare where combination therapies or mixed policy spaces are relevant 5) Limitations  The paper primarily focuses on theoretical developments and simulations which may require further validation in realworld healthcare applications.  The approach of using causal diagrams and decomposition methods may have limitations in complex healthcare scenarios with numerous variables or interactions.  Considering the abstract nature of the topic the paper could benefit from discussing the practical challenges of implementing the proposed algorithms in a clinical setting. Overall the paper presents valuable contributions to the field of policy optimization in healthcare. Clearer connections to practical applications discussions on scalability and limitations and potential extensions of the findings would enhance the impact and relevance of the study.", "lMMaNf6oxKM": " Peer Review:  1) Summary: The paper introduces a general powerful scalable (GPS) graph Transformer architecture with linear complexity and competitive results on various benchmarks. The proposed architecture emphasizes the importance of positional and structural encodings local message passing and global attention mechanisms. Key contributions include modular encodings balance between local and global mechanisms and empirical validation on 16 benchmarks. The ablations studies and experiments demonstrate superiority over existing models in terms of scalability expressivity and performance.  2) Strengths:  The paper addresses the need for a unified framework for graph Transformers offering a modular and scalable approach.  The methodology is wellstructured with clear definitions of positional and structural encodings and the incorporation of local and global mechanisms.  The experimental results demonstrate the superiority of the proposed GPS architecture on a diverse set of benchmarks.  A thorough ablation study helps in understanding the contributions of different components to the models performance.  3) Weaknesses:  The paper can benefit from more indepth explanations of the theoretical underpinnings and the reasoning behind model design choices.  While the empirical results are extensive further analysis on the failure cases or limitations of the proposed model could enhance the discussion.  The impact of hyperparameters on the model performance is highlighted but more insights into hyperparameter tuning strategies would be helpful.  4) Questions:  The paper mentions the ablation studies but does not delve into the specific results. Can the authors provide more details on the key findings from the ablation studies and their implications for the model design  How does the proposed GPS architecture compare to stateoftheart models in terms of computational efficiency and generalization across various graph structures  Can the authors elaborate on any realworld applications where the proposed GPS Transformer could have significant impact or unique advantages  5) Limitations:  The authors acknowledge the sensitivity of the model to hyperparameters. Have they experimented with different hyperparameter settings to understand their effects better  More detailed discussions on the situations in which the model may not perform optimally and potential areas for future research and improvement would enhance the papers completeness. Overall the paper presents a comprehensive study on the GPS graph Transformer architecture highlighting its strengths in scalability and performance across diverse benchmarks. Further insights into model interpretability hyperparameter tuning and realworld applications could enhance the impact of this work.", "ywxtmG1nU_6": "Peer Review Summary: The paper introduces Equivariant Graph Hierarchybased Networks (EGHNs) to model multibody physical systems utilizing three key components: Equivariant Matrix Message Passing (EMMP) EPool and EUnPool. The EMMP layer improves message passing expressivity EPool clusters nodes hierarchically and EUnPool updates lowlevel dynamics using highlevel information. Experimental evaluations show that EGHN outperforms existing models on tasks like multiobject dynamics simulation motion capture and protein dynamics modeling. Strengths: 1. Innovative Approach: The introduction of EGHNs incorporating hierarchical modeling for multibody systems is a novel and significant contribution to the field of Graph Neural Networks (GNNs). 2. Comprehensive Experiments: The paper provides thorough experimental evaluations on various tasks demonstrating the effectiveness and superiority of EGHN over existing methods. 3. Theoretical Foundation: The paper is wellgrounded in theory with detailed explanations of components like EMMP EPool and EUnPool backed by theoretical guarantees of E(n)equivariance. Weaknesses: 1. Complexity: The complexity of the model and the theoretical underpinnings might be challenging for readers unfamiliar with Graph Neural Networks and equivariance concepts. 2. Parameterization: While the effectiveness of EGHN is demonstrated through experiments the choice of hyperparameters such as the number of clusters (K) is not deeply explored which could impact the model performance. Questions: 1. Scalability: How does EGHN scale with larger datasets and higherdimensional inputs especially in realworld applications with complex systems 2. Generalization: How well does EGHN generalize to unseen data or domains and does it retain efficacy across a wide range of multibody systems 3. Interpretability: Can the hierarchical clustering produced by EPool provide insights into the underlying structures and dynamics of the physical systems being modeled Limitations: 1. Fixed Hyperparameters: The fixed number of clusters (K) in EPool as an empirical hyperparameter limits the adaptability of EGHN to different scales of systems. 2. Complexity vs. Interpretability Tradeoff: The complex nature of the model might hinder interpretability posing challenges in understanding the hierarchy and dynamics discovered by EGHN. In conclusion the paper presents a significant advancement with EGHNs for modeling physical systems backed by extensive experiments. Addressing scalability generalization and interpretability aspects along with finetuning hyperparameters for different scenarios could enhance the practical applicability and understanding of EGHNs in diverse realworld settings. Further elucidation on the limitations and tradeoffs of model complexity would enrich the discussion in future works.", "xatjGRWLRO": "Peer Review of \"SelfSupervised Pretraining for LargeScale 3D Point Clouds\" Summary: This paper introduces a novel selfsupervised pretraining method SSPL targeting largescale 3D scenes particularly outdoor LiDAR point clouds. The approach leverages local volume reasoning and a ClusterInfoNCE loss for contrastive learning. The study demonstrates superior performance in 3D object detection and semantic segmentation tasks compared to existing methods on both indoor and outdoor datasets. Strengths:  The paper addresses the gap in pretraining methods for large 3D scenes using a novel approach.  The proposed SSPL method outperforms baselines in object detection and semantic segmentation tasks.  Extensive experiments on different datasets validate the effectiveness of the approach.  Detailed descriptions of the method and the experimental setup provide clarity for replication.  A thoughtful discussion of feature analysis and ablation studies enhances the credibility of the proposed method. Weaknesses:  While the experiments are welldesigned the paper could provide more discussion on the generalizability of the approach to other 3D perception tasks beyond object detection and segmentation.  The comparisons with existing methods could be expanded to include a wider range of baseline models to provide a more comprehensive assessment.  The limitations section could be further developed to address potential challenges or drawbacks of the proposed method in practical applications. Questions: 1. How does the proposed SSPL method handle variations in point cloud density and noise levels in outdoor LiDAR point clouds 2. Can the proposed approach be extended to address challenges in 3D scene understanding tasks beyond object detection and semantic segmentation 3. What computational considerations are taken into account for scaling the proposed method to larger point cloud datasets or more complex scenes 4. How does the SSPL method compare to stateoftheart selfsupervised methods in 3D point cloud processing on computational efficiency and model complexity Limitations:  The study focuses primarily on object detection and semantic segmentation tasks limiting the exploration of other potential downstream applications.  The comparison with baseline methods though comprehensive could benefit from additional evaluation metrics to provide a more holistic assessment.  The proposed method may have limited applicability to diverse 3D sensor types and scene environments which could be addressed in future research. In conclusion the paper presents a novel selfsupervised pretraining method tailored for largescale 3D point clouds showcasing promising results in improving downstream tasks. However addressing the questions raised and expanding the scope of applicability could enhance the impact and significance of the proposed approach in the field of 3D perception.", "kZnGYt-3f_X": " Peer Review:  1) Summary: The paper proposes a novel Hilbert curvebased crossdimensionality distillation approach Hilbert Distillation (HD) to transfer structural information from 3D to 2D networks. Additionally a Variablelength Hilbert Distillation (VHD) method is introduced to dynamically adjust the stride based on activation and context areas. Experimental results on activity and medical imaging classification tasks demonstrate the effectiveness of the proposed methods in improving performance and scalability of 2D networks.  2) Strengths and weaknesses: Strengths:  The paper addresses an important problem of crossdimensionality distillation and introduces innovative methods to improve 2D network performance by leveraging 3D knowledge.  The use of Hilbert curves to preserve structural information and Variablelength Hilbert Distillation for efficient knowledge transfer are novel and promising.  Extensive experiments on activity and medical imaging datasets demonstrate superior performance compared to existing methods. Weaknesses:  The paper lacks comparison with more recent stateoftheart methods in crossdimensionality distillation potentially limiting the evaluation of the proposed approach.  The methods performance on different network architectures and datasets could provide more comprehensive insights into its generalizability.  The paper could benefit from a more detailed discussion on the computational efficiency and scalability of the proposed methods which are important considerations for realworld applications.  3) Questions:  Could the proposed Hilbert Distillation and Variablelength Hilbert Distillation methods be applied to other types of data beyond video and medical imaging  How do the proposed methods handle issues related to overfitting or generalization to unseen data in realworld scenarios  Are there specific scenarios where the proposed methods might not be as effective and what are the potential limitations in those cases  4) Limitations:  The paper focuses on specific classification tasks and might benefit from exploring broader applications or benchmarks to assess the methods versatility.  The lack of explanation on the scalability of the proposed methods could limit their practical implementation in largescale settings.  More clarity on the methods robustness to noise or variations in input data could enhance the understanding of its reliability in realworld scenarios. Overall the paper presents innovative methods for knowledge distillation in crossdimensionality tasks with promising results. Addressing the identified weaknesses and limitations could further strengthen the impact and practical applicability of the proposed approach.", "yb3HOXO3lX2": " Summary In the paper the researchers define \"reward hacking\" in the context of reinforcement learning where optimizing a proxy reward function can lead to unintended behavior as compared to the true reward function. They explore the concept of hackability looking at scenarios where improving a policy based on the proxy reward can result in worse performance based on the true reward. The paper delves into scenarios of \"simplification\" of reward functions highlighting how seemingly natural ways of simplifying reward functions may not prevent reward hacking. The research is presented through theoretical analysis and examples shedding light on the implications for aligning AI systems with human values.  Strengths  Clear Definition: The paper provides a clear and formal definition of reward hacking and hackability contributing to a better understanding of unintended outcomes in reinforcement learning.  Theoretical Analysis: The research includes rigorous theoretical analysis to establish necessary conditions for hackability and simplification of reward functions providing valuable insights into these phenomena.  Exploration of Examples: By providing concrete examples such as the cleaning robot scenario the paper enhances comprehension of the concepts being discussed.  Implications for AI Alignment: The discussion on the implications of the findings for aligning AI systems with human values adds depth to the research.  Weaknesses  Limited Exploration of Empirical Evidence: The paper heavily leans on theoretical analysis and lacks empirical validation. Including empirical studies could strengthen the conclusions drawn from the theoretical framework.  Opaque Definitions in Some Sections: Some sections especially concerning the technical conditions for hackability and simplification may be challenging for readers less familiar with the subject matter.  Questions  How do the proposed definitions and results in this paper compare with existing studies on reward hacking and specification problems in reinforcement learning  Could the theoretical findings in this paper be extended to more complex environments or scenarios beyond the scope of finite MDPs  Are there potential applications of these theoretical concepts in practical reinforcement learning systems and how might they be implemented in realworld AI systems  Limitations  Empirical Validation: The lack of empirical validation limits the generalizability of the findings to realworld scenarios.  Opaque Technical Language: Some sections containing technical details and definitions may be challenging for a broader audience to understand.  Focused on Finite Policy Sets: The studys focus on finite policy sets restricts the generalizability of the findings to infinite or more complex policy scenarios.", "uOQNvEfjpaC": " Peer Review: Summary: The paper presents a novel method for weaklysupervised object localization and phrase grounding in an openworld scenario where objects in images may not have been encountered during training. The key contributions involve combining CLIP and BLIP pretrained networks to generate bounding boxes and phrases for objects purely from images with no bounding boxes provided during training. The proposed method named \"What is Where by Looking\" (WWbL) outperforms stateoftheart methods in both weaklysupervised segmentation and phrase grounding tasks. The thorough evaluation on multiple datasets demonstrates the effectiveness of the method. Strengths: 1. The paper addresses an important and challenging problem in computer vision  weakly supervised object localization and phrase grounding in an openworld scenario. 2. The method is novel leveraging pretrained CLIP and BLIP networks for generating bounding boxes and phrases from images without any bounding box annotations during training. 3. The proposed method achieves stateoftheart performance in weaklysupervised segmentation and phrase grounding tasks. 4. The thorough experimental evaluation on multiple datasets adds credibility to the effectiveness of the proposed method. Weaknesses: 1. The paper could benefit from a more detailed explanation of the experimental setup training process and hyperparameter selection process to enable better reproducibility. 2. While the method is demonstrated on various datasets and tasks a comparison with more diverse baselines and methods could strengthen the evaluation. 3. The paper could elaborate more on the limitations and failure cases of the proposed method to provide a comprehensive understanding of its performance. Questions: 1. Could you provide more insights into the selection process and rationale behind the loss terms used in the training of the segmentation network g 2. How does the proposed method handle cases where multiple instances of the same object appear in an image Are there any strategies in place to address this scenario 3. How sensitive is the performance of the method to variations in the dataset used for training especially in terms of cultural bias and domainspecific knowledge Limitations: 1. The methods reliance on pretrained models and potentially bias in training data may limit its effectiveness in handling culturally diverse or domainspecific images. 2. The paper lacks a detailed analysis of the methods scalability and efficiency for practical applications especially in realtime scenarios or video processing. 3. The assumption of an openworld scenario may not fully capture the complexity of realworld images with diverse and unanticipated objects. Overall the paper presents a significant advancement in weakly supervised object localization and phrase grounding tasks. Enhancing the methods explanation addressing potential limitations and expanding the experimental evaluation could further strengthen the works impact and applicability in realworld scenarios.", "wjSHd5nDeo": "Peer Review 1) Summary This paper investigates the problem of lossy neural image compression (NIC) by proposing a multiplesample importance weighted autoencoder (IWAE) target. The authors identify special properties of the uniform posterior used in NIC and develop an enhanced IWAE target for NIC termed multiplesample NIC (MSNIC). Experimental results demonstrate the effectiveness of MSNIC in improving stateoftheart NIC methods and learning richer latent representations. The paper provides insights into NIC training from a gradient variance perspective and offers novel contributions to the field. 2) Strengths and Weaknesses Strengths:  The paper introduces a novel approach MSNIC for training NIC which outperforms existing methods.  Experimental results support the effectiveness of MSNIC in improving compression performance.  The paper provides valuable insights into the impact of the uniform posterior on gradient estimators and training tricks. Weaknesses:  The RD performance improvement achieved by MSNIC especially when applied to models with spatial context models is marginal.  While the paper acknowledges the limitations further evaluations on more recent stateoftheart methods could strengthen the claims.  The significant gain of IWAE over VAE occurs primarily when the variance is very small limiting the improvement in posterior collapse in NIC. 3) Questions  Can the authors provide more details on the experimental setup such as hyperparameter choices and model architecture configurations to ensure result reproducibility  How does MSNIC compare to other cuttingedge compression techniques like VQVAE or STE in terms of performance and computational efficiency  Has the proposed MSNIC method been tested on a wider variety of datasets beyond ImageNet and Kodak to evaluate its generalizability 4) Limitations  The primary limitation of the study lies in the modest improvements in RD performance especially in comparison to models with spatial context models like Cheng et al. [2020].  The insights provided focus on NIC specifically limiting the generalizability of the proposed method to other compression tasks or domains.  The explanation for the negative results observed on certain stateoftheart methods like Cheng et al. [2020] needs further clarification to assess the broader implications. In conclusion the paper presents an innovative approach to training NIC with multiple samples and offers valuable insights into improving compression performance. Further work should address the identified limitations and consider additional evaluations to establish the robustness and applicability of the proposed MSNIC method in various compression scenarios.", "w0QoqmUT9vJ": " Peer Review: 1) Summary: The paper introduces the kOSAN framework to investigate the expressive power of subgraphenhanced GNNs. It explores the relationship between various subgraphenhanced GNN approaches their expressive power compared to the kWL hierarchy and the impact of datadriven subgraph selection. The theoretical framework proposed sheds light on the upper bounds of subgraphenhanced GNNs and their relation to the kWL hierarchy. Empirical experiments demonstrate the superiority of datadriven architectures in improving prediction accuracy while reducing computation time. 2) Strengths:  The paper is wellstructured and offers a comprehensive theoretical framework for studying subgraphenhanced GNNs.  The proposed kOSAN framework provides insights into the expressive power of GNN architectures.  Empirical results demonstrate the effectiveness of datadriven architectures in enhancing prediction accuracy and efficiency. 3) Weaknesses:  The paper could benefit from more specific details in the introduction to help readers understand the context and significance of the study.  While the theoretical framework is robust the connection between theory and empirical results could be further emphasized for a clearer understanding.  The paper lacks specific details on the datasets used making it challenging to assess the generalizability of the findings. 4) Questions:  How does the proposed kOSAN framework address the limitations of existing subgraphenhanced GNNs in terms of scalability and expressivity  Can the datadriven subgraph selection approach be extended to handle more complex graph structures beyond the ones considered in the experiments  What are the implications of the results on subgraphenhanced GNNs for realworld applications in various domains such as bioinformatics or social networks 5) Limitations:  The paper could further discuss the robustness of the proposed framework in handling noisy or incomplete graph data that are common in realworld applications.  The impact of hyperparameter settings and model complexity on the performance of the proposed architecture could be explored to provide insights for practitioners.  The evaluation on additional diverse datasets with varying graph properties could strengthen the generalizability of the proposed framework. Overall the paper offers valuable insights into enhancing the expressive power of subgraphenhanced GNNs through a unified theoretical framework and datadriven subgraph selection. Addressing the above points could further enhance the impact and applicability of the proposed framework in realworld scenarios.", "o8H6h13Avjy": " Peer Review: Summary: The paper proposes a framework called MExMI that demonstrates how Model Extraction (ME) and Membership Inference (MI) attacks can reinforce each other through an iterative reaction significantly enhancing ME accuracy and MI efficiency. By utilizing a poolbased active model extraction approach the authors show how MI attacks can be exploited to guide ME and vice versa. The experiments conducted on CIFAR10 AGS NEWS and a reallife MLaaS model demonstrate the effectiveness of MExMI in improving attack fidelity and inference accuracy. The proposed framework achieves high performance in both ME and MI scenarios without requiring additional query costs. Strengths: 1. The paper addresses an important issue in adversarial machine learning by demonstrating how ME and MI attacks can be synergistically connected to enhance attack capabilities. 2. The MExMI framework is wellstructured and provides detailed explanations of its components including MI PreFilter PostFilter and semisupervised boosting. 3. The experimental results presented in the paper show a significant improvement in ME fidelity and MI accuracy compared to existing methods. 4. The proposed metricbased shadowmodel MI approach is innovative and contributes to improving attack accuracy with limited training samples. Weaknesses: 1. While the paper provides a comprehensive explanation of the MExMI framework some key technical details could be clarified further especially regarding the implementation of MI and ME attacks. 2. The paper could benefit from additional analysis on the potential limitations and realworld implications of the proposed MExMI framework including possible defense mechanisms and practical deployment considerations. 3. The impact of varying parameters and settings in the experiments on the final results should be clearly discussed to provide a deeper understanding of the frameworks robustness. 4. The paper could expand on the implications of the results for future research directions and practical applications in the field of adversarial machine learning. Questions: 1. How sensitive is the MExMI framework to variations in the composition and size of the adversary data pool and how does this impact the attack effectiveness 2. Can the application of the MExMI framework be extended to different types of ML models and datasets beyond the ones studied in the paper 3. What are the computational requirements and scalability considerations of implementing the MExMI framework in realworld scenarios involving largescale ML models 4. How do the proposed MI defenses such as differential privacy impact the performance of MExMI attacks and are there potential countermeasures that could mitigate the frameworks effectiveness Limitations: 1. The study primarily focuses on experimental evaluations without extensive discussion on the practical implications and ethical considerations of deploying adversarial attacks in realworld settings. 2. The generalizability of the MExMI framework to diverse ML models and datasets needs further investigation to assess its applicability across different scenarios. 3. The paper lacks indepth analysis of potential vulnerabilities or limitations of the MExMI framework particularly in the context of evolving MLaaS platforms and security mechanisms. 4. The theoretical foundations and implications of the proposed shadowmodel MI approach could be elaborated further to provide a more comprehensive understanding of its impact on attack accuracy. Overall the paper presents a novel MExMI framework that demonstrates promising results in enhancing ME and MI attacks through iterative interactions. While the experimental findings are encouraging additional insights on the frameworks robustness scalability and ethical considerations would strengthen the papers contributions to the field of adversarial machine learning.", "q5h7Ywx-sS": " Summary The paper introduces Stars a novel and highly scalable method for constructing extremely sparse yet highquality similarity graphs using twohop spanners. Stars reduces the number of pairwise similarity comparisons significantly leading to faster graph construction with minimal loss in quality. The theoretical guarantees and empirical results demonstrate the effectiveness of Stars in improving performance for clustering and graph learning tasks with large datasets.  Strengths and Weaknesses Strengths: 1. Stars introduces a novel approach to constructing similarity graphs improving efficiency and scalability. 2. The paper provides theoretical guarantees on the performance of Stars for various similarity measures. 3. Empirical evaluations demonstrate significant improvements in pairwise similarity comparisons and runtime. 4. Stars exhibits good performance in clustering tasks showcasing its practical utility. Weaknesses: 1. The paper could further elaborate on the tradeoffs between the sparsity of the graph and the quality of downstream tasks. 2. More detailed comparisons with existing stateoftheart methods would strengthen the evaluation of Stars. 3. The impact of parameter settings on the performance of Stars could be discussed in more detail.  Questions 1. How does Stars compare to existing graph construction methods in terms of scalability and performance on varying dataset sizes 2. Can Stars be applied to dynamic datasets where the underlying structure changes frequently 3. Are there specific scenarios or datasets where Stars may not perform as effectively compared to traditional methods  Limitations 1. The paper focuses heavily on theoretical guarantees and empirical results but lacks a detailed discussion on potential limitations of the Stars approach. 2. The scalability and efficiency of Stars on extremely large datasets are highlighted but it would be beneficial to explore potential bottlenecks in even largerscale scenarios. 3. The generalizability of Stars to diverse types of datasets and similarity measures could be further investigated. Overall the paper presents an innovative approach to similarity graph construction with promising results. Further exploration of the limitations and applicability of Stars could enhance the depth and breadth of its impact in the field of graph learning and clustering tasks.", "yJE7iQSAep": " Peer Review 1) Summary: The paper presents a novel approach called S4D which is a diagonal state space model proposed as a simple and effective alternative to the complex and difficulttoimplement S4 model. By systematically analyzing various design choices the authors demonstrate that S4D performs comparably to S4 in almost all settings achieving stateoftheart results across multiple domains. The study includes a mathematical analysis controlled empirical studies and ablations to support the proposed model. 2) Strengths:  The paper provides a detailed and thorough analysis of the proposed S4D model comparing it with the existing S4 model across various domains.  The systematic study of parameterization computation discretization and initialization of diagonal state matrices contributes significantly to understanding deep state space models.  The controlled empirical studies and ablations provide a strong basis for the efficacy and performance of S4D showcasing its competitiveness with traditional models. 3) Weaknesses:  Acknowledging the thoroughness of the study the paper could benefit from more clarity in explaining mathematical theories and models for readers less familiar with state space models.  The comparison with existing benchmarks and models could be expanded to include more diverse scenarios or applications to strengthen the claims of S4Ds effectiveness. 4) Questions:  What specific advantages does S4D offer over traditional deep learning models apart from its simplicity and effectiveness  How does the proposed model generalize to different types of datasets and tasks beyond the ones explored in the study  Are there any considerations for scalability and computational efficiency when implementing S4D in largescale applications or realworld scenarios 5) Limitations:  The paper primarily focuses on theoretical and empirical comparisons with existing models but further investigations into the robustness and generalizability of S4D across diverse datasets could provide deeper insights.  While the proposed S4D model shows promising results the limitations of the study lie in the restricted scope of applications and datasets considered necessitating broader validation and testing. Overall the paper contributes valuable insights into simplifying and improving state space models through the novel S4D approach but further validations and expansions of the studys findings could enhance its impact and practical applicability in the field of deep learning.", "qj-_HnxQxB": " Peer Review  Summary The paper proposes a new neural framework called Functional Indirection Neural Estimator (FINE) to improve outofdistribution generalization by performing analogymaking and indirection in functional spaces. FINE dynamically constructs backbone weights by mixing trainable basis weights stored in a semantic memory using inputoutput data pairs. Empirical results show FINE outperforms other models on IQ tasks involving geometric transformations even in small data scenarios.  Strengths  The paper addresses an important problem of outofdistribution generalization by proposing a novel approach of performing analogymaking and indirection in functional spaces.  FINEs design is innovative leveraging a memoryaugmented architecture and basis weight matrices to compose functions onthefly showcasing superior performance on IQ tasks involving geometric transformations.  The empirical results demonstrate FINEs effectiveness in outperforming competing models and adapting well in smallscale data scenarios.  Weaknesses  The paper lacks comparison with a broader range of existing models in the field particularly those focusing on outofdistribution generalization or IQ tasks.  While the proposed FINE framework shows promising results a more detailed discussion on the theoretical underpinnings and limitations of the approach could enhance the papers depth.  Questions 1. Could the paper elaborate more on the interpretability of FINEs model parameters and how they contribute to the analogymaking and indirection processes 2. How sensitive is FINE to hyperparameter tuning especially regarding the number of memories and layers in the backbone architecture 3. Are there specific scenarios or types of tasks where FINE might struggle or underperform compared to other models and under what conditions might its performance degrade  Limitations  The experimental evaluations are primarily focused on IQ tasks involving geometric transformations. Exploring a broader range of tasks to assess FINEs generalizability across different problem types could strengthen the papers findings.  The implementation details and scalability considerations especially in scenarios with larger datasets or complex transformations remain as potential limitations that could be further addressed. Overall the paper presents a novel and promising approach with FINE for tackling outofdistribution generalization in the context of IQ tasks. It provides valuable insights into the benefits of functional space transformations and memoryaugmented architectures contributing to the advancement of research in this area. Further refinements and comprehensive evaluations could enhance the impact and applicability of the proposed framework.", "lWq3KDEIXIE": " Summary The paper introduces a novel approach to improve tractable probabilistic models using local probability estimates even when the estimates are inconsistent. The method involves updating the parameters of the model via a gradient descent procedure that minimizes a combination of KL divergences between the model and the local estimates. Experimental results show significant improvements in both generative and predictive performance over models learned from small possibly noisy data.  Strengths 1. The paper addresses an important problem of incorporating inconsistent local estimates into probabilistic models. 2. The proposed method is clearly explained and the theoretical framework is welldefined. 3. The experimental evaluation is controlled and thorough offering insights into the performance of the approach across various datasets and noise levels. 4. The gradientbased algorithm for learning the parameters of cutset networks is wellstructured and computationally efficient.  Weaknesses 1. The paper could benefit from providing more detailed explanations of certain complex concepts and equations for readers less familiar with probabilistic graphical models. 2. The method is evaluated on a limited set of benchmark datasets potentially limiting the generalizability of the results. 3. The paper lacks a discussion on the practical implementation and potential realworld applications of the proposed approach.  Questions 1. How does the computational complexity of the proposed gradientbased algorithm compare to existing methods for learning probabilistic models 2. Can the approach handle more complex scenarios beyond binary variables and pairwise local statistics 3. Are there any assumptions made in the methodology that could limit its applicability to certain types of datasets or models  Limitations 1. The evaluation is focused on controlled experiments with synthetic data potentially lacking insights into the performance of the method in more diverse and realworld settings. 2. The paper does not address potential challenges or limitations of scaling the approach to larger more complex models or datasets. 3. While the results are promising the methods performance on extremely noisy or inconsistent local estimates is not explored in depth.", "nEJMdZd8cIi": " Summary The paper introduces a novel method PROJUNN for training neural networks using unitary matrices efficiently. By leveraging rankk updates the proposed method maintains performance while significantly reducing training runtime. Two variants of the method PROJUNND and PROJUNNT project lowrank gradients onto the closest unitary matrix or transport unitary matrices in the direction of the gradient. The paper demonstrates the effectiveness of PROJUNN in recurrent neural network and convolutional neural network settings showing promising results that match or exceed benchmarked algorithms.  Strengths and Weaknesses Strengths:  Introduces a novel method PROJUNN for efficient training with unitary matrices.  Demonstrates the efficacy of PROJUNN in both recurrent and convolutional neural network settings.  Provides theoretical analysis and practical experiments to support the proposed method.  Maintains performance while reducing training runtime by leveraging rankk updates. Weaknesses:  The paper could benefit from more detailed explanation and analysis of the experimental results.  The connection between theoretical constructs (such as Lie algebra and unitaryorthogonal groups) and practical implementations could be further elaborated.  Further comparison with other stateoftheart methods on a wider range of datasets and network architectures could strengthen the papers contribution.  Questions 1. How does the proposed method PROJUNN compare with other existing algorithms in terms of training stability and robustness 2. Can the paper provide more insights into the application of PROJUNN in largescale neural network architectures beyond the tested scenarios 3. Are there any specific characteristics of datasets or network architectures where PROJUNN demonstrates notable advantages or limitations  Limitations  The papers experimental scope could be expanded to include more diverse datasets and network architectures to provide a broader understanding of PROJUNNs applicability.  The discussion on the computational complexity and efficiency of PROJUNN could be more detailed to provide a comprehensive analysis of its practical implications.  The paper might benefit from elucidating potential challenges and limitations faced when scaling up the proposed method to larger and more complex models. Overall the paper presents a promising method for efficient training with unitary matrices in neural networks. Further exploration and validation in diverse settings could enhance the understanding of PROJUNNs capabilities and limitations.", "pGLFkjgVvVe": " Peer Review of the Scientific Paper:  Summary: The paper introduces a novel approach that combines parametric and nonparametric methods for uncertainty estimation in modelbased offline reinforcement learning. The proposed method utilizes a Riemannian metric based on generative models to estimate uncertainty and penalize model errors. The approach is demonstrated to outperform contemporary modelbased offline methods on continuous control and autonomous driving benchmarks.  Strengths: 1. Innovative approach: The combination of parametric and nonparametric methods for uncertainty estimation through a novel Riemannian metric is a novel contribution to the field. 2. Theoretical foundation: The paper provides a strong theoretical foundation by leveraging Riemannian geometry of generative models and building a pullback metric for uncertainty estimation. 3. Empirical validation: The proposed method is demonstrated to show significant improvement over contemporary approaches on various benchmarks supporting the effectiveness of the proposed metric.  Weaknesses: 1. Complexity: The method presented in the paper seems complex and computationally intensive especially in the computation of geodesic distances. This may limit the practicality of the approach in realworld applications. 2. Limited Discussion on Generalizability: The paper focuses primarily on the specific benchmarks used for evaluation without a detailed discussion on the generalizability of the proposed method to other domains or tasks. 3. Performance Overhead: The paper mentions a slowdown in performance due to the computation of the geodesic distance. Future work should aim to optimize this computation process to improve efficiency.  Questions: 1. Scalability: How scalable is the proposed method to larger and more complex environments with highdimensional stateaction spaces 2. Computational Resources: What computational resources are required for implementing the approach in practical settings Are there any limitations in terms of scalability to realworld applications 3. Robustness: How robust is the proposed method to variations in dataset sizes noise levels and different types of environments  Limitations: 1. Computational Intensity: The computation of geodesic distances and other related metrics may introduce significant computational overhead potentially limiting realtime application in practical scenarios. 2. Lack of Comparison: The paper lacks a detailed comparison with a wider range of baseline methods to showcase the superiority of the proposed approach more comprehensively. 3. DomainSpecific Focus: The evaluation seems focused on specific continuous control and autonomous driving tasks limiting the generalizability of the findings to broader reinforcement learning applications. Overall the paper presents an innovative method combining parametric and nonparametric techniques for uncertainty estimation in modelbased offline reinforcement learning. While the proposed approach shows promising results on benchmarks addressing the scalability and efficiency concerns would be crucial for practical implementation in realworld scenarios. Additionally expanding the discussions on generalizability and robustness would enhance the overall impact of the research.", "se2oxj-6Nz": " Peer Review  1) Summary: The paper addresses the issue of image restoration assisting object detectors in adverse imaging conditions by proposing a targeted adversarial attack in the restoration procedure to boost object detection performance after restoration. The proposed ADAMlike adversarial attack generates pseudo ground truth for restoration finetuning leading to restored images close to original sharp images and improved object detection results. Extensive experiments in image dehazing and low light enhancement demonstrate the superiority of the proposed method over conventional training and other domain adaptation methods.  2) Strengths and weaknesses: Strengths:  The paper introduces a novel approach using targeted adversarial attacks to generate pseudo ground truth for image restoration finetuning.  Extensive experiments are conducted in two restoration tasks dehazing and low light enhancement showcasing the effectiveness of the proposed method.  The method demonstrates improved object detection results while maintaining visually satisfactory restoration outcomes. Weaknesses:  The proposed method may face challenges in learning the pattern of the attacked pseudo ground truth perfectly leading to potential limitations in practical applications.  While the targeted adversarial attack provides a performance boost in detection on test sets the gains are not replicated in realworld restoration models indicating room for optimization.  The paper lacks a comprehensive comparison with stateoftheart methods in restoration and object detection limiting the assessment of the proposed approach against existing solutions.  3) Questions:  How does the proposed targeted adversarial attack method ensure robustness and generalizability across various image restoration tasks and object detection scenarios  What considerations were made in the experimental setup to ensure the validity and reliability of the results obtained from the conducted experiments  Could additional insights be provided on the impact of hyperparameter tuning especially concerning the slackness coefficients on the overall performance of the proposed finetuning pipeline  4) Limitations:  A noted limitation is the potential failure of restoration models to learn perfectly the pattern of the attacked pseudo ground truth leading to challenges in achieving significant improvement in detection. This limitation raises questions about the practical applicability and generalization of the proposed method.  The discrepancy between the significant boost in detection results seen with the adversarial attack on test sets compared to the actual results of restoration models learning the map to pseudo ground truth poses a limitation on the overall effectiveness and consistency of the proposed approach. Overall the paper introduces an innovative method using targeted adversarial attacks for image restoration finetuning to improve object detection performance. Addressing the identified limitations and questions could enhance the clarity and impact of the proposed approach in the field of image processing and object detection research. Further evaluation and comparisons with existing methods could strengthen the validation of the proposed method.", "rG7HZZtIc-": "Peer Review Summary: The paper introduces a novel approach called Decoupled Dynamic Neural Radiance Field (DNeRF) for segmenting and decoupling dynamic objects from the static background in monocular videos. The method utilizes separate neural radiance fields for dynamic and static components and introduces a shadow field network for handling dynamic shadows. Experimental results indicate superior performance compared to stateoftheart approaches in decoupling dynamic and static objects occlusion and shadow removal and image segmentation of moving objects. Strengths: 1. Innovative Approach: The introduction of DNeRF with separate radiance fields for dynamic and static components is a novel approach to address the problem of scene segmentation in monocular videos. 2. SelfSupervised Learning: The method is selfsupervised which is advantageous for learning scene representations without the need for additional supervision data improving practical applicability. 3. Comprehensive Evaluation: The paper presents detailed quantitative and qualitative evaluations on synthetic and realworld datasets showcasing the methods effectiveness in decoupling dynamic objects shadows and reconstructing static backgrounds. 4. Clear Explanations: The paper includes clear explanations of the methodology architectural details loss functions and evaluation metrics aiding in understanding the proposed approach. Weaknesses: 1. Complexity: The method involves several specialized components like skewed entropy loss and shadow field network which may make the implementation and understanding challenging for researchers without extensive expertise in neural rendering. 2. Limited Realworld Evaluation: While the method is evaluated on realworld datasets the absence of ground truth makes quantifying the performance challenging and hinders a comprehensive assessment of its effectiveness in realworld scenarios. 3. Dependency on Camera Registration: The effectiveness of the approach is reliant on accurate camera registration and high frequency viewdependent radiance changes can pose challenges in achieving accurate scene decoupling. 4. Ambiguity in Moving Shadows: Handling textureless moving shadows or repetitive motion shadows presents challenges and the paper acknowledges difficulties in accurately modeling such scenarios. Questions: 1. How does the method handle complex scenes with multiple dynamic objects especially those with overlapping dynamic shadows 2. Are there scalability considerations for applying the proposed approach to largescale scenes with significant motion variations 3. Could the method benefit from incorporating additional cues or priors to enhance the decoupling accuracy especially in challenging scenarios 4. How generalizable is the proposed method to different lighting conditions and environmental settings Limitations: 1. Evaluation Metrics: The lack of ground truth in realworld datasets limits the ability to provide precise quantitative evaluations and may introduce subjective biases in performance assessments. 2. Implementation Complexity: The specialized components and loss functions may increase the complexity of implementation and require careful tuning of hyperparameters potentially limiting ease of adoption by the broader community. 3. Realworld Applicability: The effectiveness of the method in realworld scenarios may be influenced by external factors such as lighting variations scene complexity and camera motion suggesting the need for further validation in diverse settings. 4. Dependency on Camera Pose: The reliance on known camera poses for accurate scene reconstruction may restrict the methods applicability to scenarios where precise camera information is not available. Overall the paper presents a notable advancement in scene segmentation and reconstruction from monocular videos. Addressing the limitations and complexities could enhance the methods usability and robustness across a wider range of applications and scenarios.", "pp7onaiM4VB": " Peer Review: 1) Summary (avg word length 100): The paper proposes a machine learning framework named TSMNet for EEG transfer learning by introducing domainspecific momentum batch normalization on the SPD manifold. The goal is to close the performance gap to stateoftheart domainspecific methods in intersession and subject transfer learning scenarios. The results demonstrate that TSMNet achieves or exceeds the performance of reference methods in various EEG datasets offering competitive results and interpretability. 2) Strengths and weaknesses: Strengths:  The paper presents a novel machine learning framework for EEG transfer learning with promising results.  The proposed TSMNet architecture achieves stateoftheart performance in intersession and subject transfer learning scenarios.  The inclusion of DSBN and SPDMBN contributes significantly to the success of TSMNet.  Comprehensive theoretical background and empirical validation with multiple EEG datasets.  Interpretable results with extracted patterns providing insights into the discriminative sources detected by TSMNet. Weaknesses:  The computational complexity of methods involving eigen decomposition may limit their application to highdimensional SPD features.  The focus was on offline evaluations further studies should explore online UDA scenarios.  The potential negative societal impacts of the proposed methods were discussed briefly but could be further elaborated. 3) Questions:  How does the proposed TSMNet architecture compare to more traditional EEG transfer learning methods in terms of computational efficiency  Can the proposed framework be extended to handle more complex EEG data or scenarios such as higherdimensional SPD features or longterm transfer learning  How generalizable are the findings of the study across different EEG datasets with varying characteristics and challenges  What are the key factors influencing the interpretability of the extracted patterns from TSMNet 4) Limitations:  The study primarily focuses on EEG datasets potentially limiting the generalizability of findings to other types of biological or nonbiological data.  The performance evaluation metrics are primarily focused on balanced accuracy and it would be beneficial to analyze additional metrics for a more comprehensive assessment.  The paper could benefit from a more detailed discussion of the practical implications and potential applications of the proposed method in realworld EEG neurotechnology settings. Overall the paper presents a valuable contribution to the field of EEG transfer learning showcasing a novel machine learning framework that offers competitive performance and interpretability. Further exploration of the proposed methods scalability applicability to diverse EEG datasets and potential realworld utility could enhance the impact of this research.", "mWaYC6CZf5": " Peer Review: 1. Summary The paper introduces a routing algorithm for Sparse MixtureofExperts (SMoE) models to address the issue of representation collapse in neural networks. The proposed method estimates routing scores on a lowdimensional hypersphere improving routing consistency and model performance. Extensive experiments on crosslingual language model pretraining and downstream tasks show consistent gains and alleviation of representation collapse. The paper also includes detailed analysis and comparisons with baseline methods. 2. Strengths and weaknesses Strengths:  The paper addresses an important issue of representation collapse in SMoE models and proposes a novel routing algorithm to mitigate this problem.  The experimental results are extensive and cover a wide range of multilingual benchmarks demonstrating consistent gains and improvements in routing behaviors.  Detailed analysis and comparison with baseline methods provide a thorough evaluation of the proposed method. Weaknesses:  The paper could provide more insight into the theoretical underpinnings of the proposed routing algorithm and its implications on model behavior.  While the experimental results are impressive more discussion on the limitations of the proposed method and potential failure cases could enhance the papers relevance and applicability. 3. Questions  Can you provide more details on the impact of the proposed routing algorithm on specific downstream tasks such as machine translation or image classification  How does the proposed method compare with stateoftheart approaches in addressing representation collapse in neural networks  Have you explored the scalability of the proposed method to larger models or different domains beyond language tasks 4. Limitations  While the paper extensively evaluates the proposed method on languagerelated tasks it would be beneficial to explore its applicability to other domains like computer vision or multimodal tasks.  The discussion on ethical considerations is brief expanding on potential societal impacts of model training and addressing mitigation strategies would enhance the papers relevance and completeness. Overall the paper presents a novel routing algorithm for SMoE models to alleviate representation collapse and achieve more consistent routing behaviors. The extensive experiments and analysis provide valuable insights into the effectiveness and limitations of the proposed method. Further exploration into the theoretical foundations and broader applications could strengthen the papers contributions to the field.", "nJJjv0JDJju": " Peer Review  Summary The paper introduces a novel method Manifold Constrained Gradient (MCG) to improve the performance of diffusion modelbased solvers for solving inverse problems. The proposed method leverages a denoising step inspired by Tweedies formula in combination with a manifold constraint to prevent sample paths from deviating from the data manifold. Extensive experiments demonstrate the superiority of the proposed method in tasks such as image inpainting colorization and computed tomography reconstruction compared to existing diffusion model solvers. The theoretical analysis supports the effectiveness of the algorithm in maintaining accuracy and stability during the inference process.  Strengths 1. Novel Contribution: The introduction of MCG as a modification to existing diffusion model solvers to improve performance is a significant and novel contribution. 2. Theoretical Analysis: The paper provides a detailed theoretical analysis supporting the proposed methods effectiveness enhancing the credibility of the findings. 3. Extensive Experiments: The method is validated through comprehensive experiments across various tasks showing consistent performance improvements over existing diffusion modelbased solvers.  Weaknesses 1. Complexity: The paper delves into theoretical concepts and mathematical formulations that may be challenging for readers without a strong background in diffusion models and inverse problems. 2. Lack of Comparison: While the paper compares the proposed method against existing diffusion model solvers additional comparison with traditional optimization methods or other unsupervised techniques would provide a broader perspective on the effectiveness of MCG. 3. Practical Implementation: The paper briefly mentions that the proposed method is straightforward to implement within a few lines of code but more practical guidance on implementation details potential pitfalls and parameter tuning would be valuable.  Questions 1. How does the proposed MCG method generalize to unseen datasets or noise levels not evaluated in the experiments 2. Are there specific limitations or challenges in applying the MCG approach to realworld applications beyond the experimental setups considered in the study 3. Can the authors elaborate on the computational overhead introduced by the MCG step compared to traditional diffusion model solvers  Limitations 1. Stochasticity: As noted the inherent stochastic nature of diffusion models may limit the robustness and reliability of the proposed method in certain scenarios. 2. Computational Complexity: The potential computational overhead of the MCG step could limit its practical utility in scenarios where realtime processing or largescale datasets are involved. The paper presents a promising advancement in diffusion model solvers for inverse problems with the introduction of the MCG method. Addressing the weaknesses outlined above and providing more clarity on practical implementation and generalizability would enhance the impact and applicability of the proposed method.", "kY1RbKE7DWE": " Peer Review  Summary: The paper introduces an Anchorchanging Regularized Natural Policy Gradient (ARNPG) framework for policy optimization in Markov decision processes (MDPs) with multiple reward value functions. The ARNPG framework is designed to incorporate firstorder methods for efficient policy optimization in multiobjective MDP scenarios. The paper provides theoretical guarantees on the convergence properties of algorithms based on the ARNPG framework and demonstrates superior performance in empirical evaluations compared to existing policy gradientbased approaches.  Strengths: 1. Innovative Framework: The introduction of the ARNPG framework is a novel approach to addressing policy optimization in multiobjective MDPs leveraging firstorder methods in a systematic manner. 2. Theoretical Contribution: The paper provides detailed theoretical analyses and guarantees on the convergence properties of the proposed algorithms which adds significant value to the research community. 3. Empirical Evaluation: The empirical results showcase the effectiveness of the ARNPGguided algorithms across various scenarios demonstrating superior performance compared to benchmark algorithms.  Weaknesses: 1. Complexity of Presentation: The paper delves deep into theoretical aspects and algorithmic details which may make it challenging for readers not familiar with the topic to grasp the concepts easily. 2. Clarity in Experimentation: While the empirical results are promising there could be more explanation on how the experiments were conducted including details on the setup and parameters used. 3. Extension to Larger Scale Scenarios: It would be beneficial to discuss the scalability and applicability of the proposed framework to larger and more complex environments beyond the examples provided.  Questions: 1. In the empirical evaluations are there specific criteria or metrics used to compare the performance of the ARNPGguided algorithms to existing methods 2. How does the ARNPG framework handle situations where the MDP is nonconvex in terms of the parameters 3. Can the ARNPG framework be extended or adapted for online or continuous learning tasks in MDPs 4. Are there any limitations or tradeoffs encountered when implementing the ARNPG framework in practical reinforcement learning scenarios  Limitations: 1. Generalizability: The paper primarily focuses on tabular and exact gradient scenarios with samplebased scenarios included to some extent. Further work may be needed to generalize the framework to more diverse environments. 2. Practical Implementation: While the theoretical guarantees are strong the practical implementation and computational complexity of ARNPG algorithms may need further exploration. 3. Scalability: It would be valuable to investigate how the ARNPG framework performs in largerscale and more complex MDP environments to assess its scalability and efficiency. Overall the paper presents a comprehensive framework for policy optimization in multiobjective MDPs but further research and experimentation may be required to validate its effectiveness in various challenging scenarios.", "wjClgX-muzB": " Peer Review: 1) Summary (avg word length 100): The paper introduces Support Decomposition Variational Inference (SDVI) as a novel variational inference approach for probabilistic programs with stochastic support. It addresses the challenges of constructing appropriate variational families by decomposing programs into subprograms with static support. SDVI provides significant improvements in inference performance and has been implemented in Pyro. The paper demonstrates the effectiveness of SDVI through experiments on synthetic and realworld data showcasing its superiority over existing techniques. 2) Strengths:  Innovative Approach: SDVI introduces a novel way of constructing variational guides by breaking down programs into subprograms with static support leading to improved inference performance.  Theoretical Framework: The paper provides a comprehensive theoretical framework for SDVI explaining the breakdown into straightline programs and the construction of variational guides.  Experimental Validation: The experiments conducted demonstrate the superiority of SDVI over existing variational inference approaches showcasing its effectiveness in diverse settings.  Implementation: The implementation of SDVI in Pyro makes it accessible to a wide audience and facilitates practical applications of the proposed method. 3) Weaknesses:  Complexity: The proposed method introduces a level of complexity that may limit its straightforward application in all scenarios especially when dealing with a large number of subprograms.  Guided Learning: While SDVI offers significant advantages the paper could have provided more insights into how to best construct customized variational guides within the framework for specific applications.  Resource Allocation: The paper discusses resource allocation strategies but does not delve deep into the potential challenges or tradeoffs associated with such strategies which could affect the practical implementation of SDVI. 4) Questions:  Generalization: How well does SDVI generalize across different types of probabilistic programs and datasets Are there specific scenarios where SDVI may not perform as effectively  Scalability: Can SDVI be easily scaled to handle highdimensional models and large datasets Are there any scalability limitations that researchers should be aware of  Realworld Applications: How applicable is SDVI to practical realworld problems in fields like natural language processing and statistical phylogenetics Have there been any case studies or applications in these domains 5) Limitations:  Complexity: The complexity introduced by the breakdown into subprograms and resource allocation strategies could make SDVI less accessible and more challenging to implement particularly in scenarios with limited computational resources.  Modelspecific Challenges: The effectiveness of SDVI could be influenced by the specific characteristics of the probabilistic models and the nature of the datasets which may limit its generalizability across diverse scenarios.  User Expertise: The successful implementation of SDVI may require a certain level of expertise and familiarity with variational inference and probabilistic programming potentially limiting its adoption among less experienced practitioners.  Overall Assessment: The paper presents a novel and innovative approach to variational inference for probabilistic programs with stochastic support. SDVI shows promise in improving inference performance and demonstrates advantages over existing techniques in experimental settings. However the complexity introduced by the method potential challenges in customization and scalability considerations may pose limitations to its widespread adoption. Further research on generalizability practical applications and userfriendly implementations could enhance the impact and accessibility of SDVI in the field of probabilistic programming and variational inference.", "uRTW_PgXvc7": " Peer Review Summary: The paper investigates a novel approach of parameterefficient imagetovideo transfer learning through a SpatioTemporal Adapter (STAdapter). The study focuses on the adaptation of large pretrained image models for video understanding tasks particularly action recognition. Extensive experiments demonstrate the effectiveness of the STAdapter in achieving superior video understanding with significantly fewer parameters outperforming existing finetuning methods and stateoftheart video models. Strengths: 1. The paper addresses an important and emerging research problem of crossmodality transfer learning from images to videos. 2. The proposed STAdapter is innovative in enabling pretrained image models to reason about spatial and temporal information efficiently. 3. Extensive benchmarking and experiments demonstrate the effectiveness and superiority of the STAdapter over existing finetuning methods and video models. 4. The work contributes to advancing the field of video understanding tasks particularly in action recognition by leveraging parameterefficient transfer learning strategies. Weaknesses: 1. The limitations of the study include the focus on a specific task of action recognition which may restrict the generalizability of the findings to other video understanding tasks. 2. The description of the STAdapter architecture and integration could be further elaborated for better understanding and reproducibility by the research community. 3. The comparisons with a wider range of stateoftheart video models and transfer learning methods could strengthen the papers contributions and implications. Questions: 1. How does the proposed STAdapter compare in terms of convergence speed and computational efficiency compared to full finetuning and existing video models 2. Can the STAdapter architecture be easily adapted for other video understanding tasks beyond action recognition 3. What are the implications of the findings for realworld applications requiring efficient transfer learning from images to videos Limitations: 1. The study primarily focuses on action recognition tasks and may not fully explore the performance implications of the STAdapter on other video understanding tasks. 2. The experimental setup could be further diversified with additional datasets and video understanding benchmarks to validate the generalizability of the STAdapter across different scenarios. 3. The generalizability of the proposed STAdapter beyond the specific architecture and pretrained models used in the study remains to be fully explored in diverse video understanding contexts. Overall the paper presents a significant contribution to the field of crossmodality transfer learning from images to videos particularly in the context of action recognition. Addressing the identified weaknesses and further research into the limitations and questions raised could enhance the impact and applicability of the proposed STAdapter in practical video analysis applications.", "x4JZ3xX5mtv": " Peer Review  1) Summary This paper proposes a novel singleimage view synthesis framework that mitigates the \"seesaw\" problem between explicit and implicit methods. By introducing a loss function that complements the outputs of both explicit and implicit renderers the method outperforms previous stateoftheart methods and speeds up image generation considerably. The experimental validation on RealEstate10K and ACID datasets demonstrates the effectiveness of the proposed approach.  2) Strengths and weaknesses Strengths:  The paper effectively addresses the \"seesaw\" problem in singleimage view synthesis through the use of explicit and implicit renderers with a novel loss function.  The proposed method demonstrates superior performance compared to existing autoregressivebased approaches achieving highquality results and faster image generation.  The thorough experimental validation on RealEstate10K and ACID datasets showcases the efficiency and effectiveness of the framework. Weaknesses:  While the paper discusses the limitations of autoregressive models more indepth comparison with existing approaches and discussions on potential drawbacks of the proposed method would enhance the papers completeness.  The lack of error bars in reporting experimental results could impact the robustness of the findings especially in terms of reproducibility. 4) Questions  How does the proposed loss function specifically address the tradeoff between preserving seen contents and generating unseen regions and how does it ensure a balanced use of explicit and implicit features  Can the authors provide additional insights into the potential negative societal impacts ethical considerations and privacy concerns associated with this work  In terms of practical applicability are there any specific realworld scenarios or applications where this method could have significant implications or benefits 5) Limitations  While the paper acknowledges the limitations in the conclusion a clearer delineation of the specific constraints or constraints of the proposed method such as scalability generalization to diverse scenes or computational requirements would strengthen the discussion.  The absence of reporting error bars for experimental results may diminish the confidence in the statistical significance and reliability of the findings. Overall the paper presents a novel framework for singleimage view synthesis with notable advancements in mitigating the \"seesaw\" problem. Addressing the provided questions and limitations would further enrich the discussion and improve the comprehensiveness and impact of the research. The paper is wellstructured and the experiments are sufficiently detailed providing a strong foundation. Further exploration of the proposed methods practical implications and societal implications could enhance the papers overall contribution to the field.  Checklists  [ ] The main contribution accurately reflects the abstract and introduction.  [X] The limitations of the work are discussed.  [X] Potential negative societal impacts are mentioned.  [X] The code data and training details for reproducibility are included.  [X] The paper adequately discusses the assets used including licenses.  [X] Compensation details for crowdsourcing or human subjects are not applicable.  [X] Consent and privacy considerations are appropriately addressed. This review aims to provide constructive feedback and suggestions to strengthen the credibility and impact of the research presented in the paper.", "nyCr6-0hinG": "Peer Review 1) Summary: The paper introduces MetaSchedule a domainspecific probabilistic programming language abstraction for constructing a rich search space of tensor programs. It empowers domain experts to customize and enhance the search space for optimization leading to a 48 speedup in endtoend deep learning workloads. The framework covers a diverse set of tensor programs and achieves competitive performance with existing stateoftheart frameworks. 2) Strengths and Weaknesses: Strengths:  Introduces MetaSchedule a novel domainspecific probabilistic language for constructing tensor program search spaces.  The paper presents an endtoend learningdriven framework for optimizing tensor programs.  Demonstrates the ability to cover a wide variety of tensor program optimization techniques.  Provides experimental results showing significant speedup and competitive performance on deep learning workloads. Weaknesses:  While the paper emphasizes the benefits of MetaSchedule the exact implementation details and code examples could have been more thoroughly elaborated.  The paper could benefit from more concrete examples and benchmarks to further illustrate the effectiveness of MetaSchedule.  The theoretical aspects of probabilistic programming could be presented in a more accessible manner for a broader audience. 3) Questions:  Could the authors provide more insights into how MetaSchedule compares to existing probabilistic programming languages in terms of expressiveness and ease of use  How does the proposed learningdriven framework handle scalability issues when dealing with a large number of tensor programs  Are there any limitations or assumptions made in the implementation of MetaSchedule that could impact its performance or applicability in realworld scenarios 4) Limitations:  The paper focuses heavily on the theoretical aspects and experimental results of MetaSchedule potentially leaving out practical challenges and limitations that may arise in realworld deployment.  The generalizability of MetaSchedule to varied hardware platforms and deep learning models could be further explored to understand potential limitations in complex scenarios.  The scalability and performance of MetaSchedule when dealing with rapidly evolving tensor program optimization techniques and hardware primitives might pose a limitation. Overall the paper presents a promising approach in the field of tensor program optimization through its innovative MetaSchedule framework. Addressing the raised questions and limitations could provide a more comprehensive understanding of the applicability and effectiveness of MetaSchedule in practical settings.", "tmer8WAEzV": "Peer Review 1) Summary: The paper introduces a novel algorithm Constrained Riemannian Hamiltonian Monte Carlo (CRHMC) designed to efficiently sample from illconditioned nonsmooth constrained distributions in high dimensions. The algorithm demonstrates improved performance compared to existing methods on benchmark datasets from systems biology and linear programming. The authors successfully address the challenge of maintaining sparsity and constraints while achieving a mixing rate independent of condition numbers showcasing significant speed enhancements even for large networks like RECON3D. The thorough experimental evaluation on realworld datasets further supports the efficacy of CRHMC. 2) Strengths and Weaknesses: Strengths:  Introduction of a novel algorithm CRHMC tailored for efficient sampling from illconditioned nonsmooth highdimensional constrained distributions.  The paper showcases significant speed improvements achieving orders of magnitude faster sampling rates on benchmark datasets compared to existing methods.  The experimental evaluations on diverse datasets from systems biology and linear programming demonstrate the algorithms practical utility and scalability.  The detailed explanations of the algorithm approach including the Riemannian framework constraints handling and sampling process provide clarity and insights into the methodology. Weaknesses:  While the paper is wellstructured and comprehensive the detailed technical explanations may be overwhelming for readers not wellversed in advanced sampling algorithms and mathematical concepts.  The limitation of focusing on a specific type of distribution (illconditioned nonsmooth constrained) may limit the generalizability of the algorithm to broader sampling scenarios.  The discussion on parameter tuning or potential further optimization strategies for CRHMC could enhance the completeness of the paper. 3) Questions:  How does CRHMC compare or complement existing stateoftheart sampling algorithms like MALA HMC NUTS in terms of handling constraints and preserving sparsity in highdimensional settings  Are there specific realworld applications beyond the mentioned examples from systems biology and linear programming where CRHMC could provide substantial benefits in sampling efficiency  Could the authors elaborate on the computational complexity considerations of CRHMC and potential scalability challenges when applied to even higher dimensions or more complex distribution scenarios 4) Limitations:  The focus on specific types of distributions (illconditioned nonsmooth constrained) limits the generalizability of CRHMC to a broader range of sampling scenarios.  The detailed technical presentation of the algorithm may hinder accessibility and applicability for researchers without a deep background in advanced sampling methodologies.  While the experimental results are promising further validation on a wider range of datasets and comparisons with additional sampling algorithms could strengthen the algorithms adaptability and robustness. In conclusion the paper presents a novel and promising algorithm CRHMC designed to efficiently sample from challenging distributions in high dimensions. The thorough experimental evaluations and detailed algorithmic descriptions enhance the understanding of the proposed method. Addressing the raised questions and potential limitations could further enhance the clarity applicability and impact of the algorithm in diverse research domains.", "wcBXsXIf-n9": " Peer Review  Summary: The paper introduces a novel deep neural network classifier that maximizes the margin in both Euclidean and angular spaces simultaneously. The proposed method enforces samples to cluster around classspecific centers chosen from the vertices of a regular simplex. Experimental results demonstrate the effectiveness of the method in achieving stateoftheart accuracies in both closedset and open set recognition tasks.  Strengths: 1. Innovation: The proposed method introduces a novel approach to maximizing margin in both Euclidean and angular spaces which is a unique contribution to the field of deep learning classifiers. 2. Simplicity: The proposed method does not require setting hyperparameters for classical classification problems making it userfriendly and easy to implement. 3. Performance: Experimental studies show that the method achieves stateoftheart accuracies in open set recognition and competitive results in closed set recognition tasks. 4. Clear Explanations: The paper provides a detailed explanation of the motivation behind the proposed method and the theoretical underpinnings supporting the approach.  Weaknesses: 1. Dimensional Constraints: The limitation that the feature dimension must be larger than or equal to the total number of classes minus one could restrict the applicability of the method in scenarios with a large number of classes. 2. Complexity in Largescale Datasets: While the Dimension Augmentation Module (DAM) helps overcome the dimension limitation the methods performance on largescale face recognition datasets is not stateoftheart.  Questions: 1. How does the proposed method handle outliers or noisy data in the training samples especially in the context of open set recognition 2. Could the Dimension Augmentation Module (DAM) be further improved or optimized to enhance the performance on largescale face recognition datasets 3. Are there any considerations or insights regarding the generalizability of the proposed method to different types of datasets beyond the ones tested in the experiments  Limitations: 1. Limited Scope of Datasets: The experiments in the paper are conducted on specific datasets and the generalizability of the proposed method to a broader range of datasets may need further exploration. 2. Hybrid Evaluation Metrics: While the paper provides AUC scores for open set recognition and closedset accuracies a unified metric for evaluating both tasks could help in better comparison and understanding of the methods performance. Overall the paper presents an innovative approach to deep learning classification with promising results. Addressing the limitations and further refining the proposed method could enhance its applicability and effectiveness across different scenarios.", "q-LMlivZrV": " Peer Review:  Summary: The paper addresses the \"explaining away\" problem in CLIP a popular contrastive learning model by proposing a new method called CLOOB that combines modern Hopfield networks with the InfoLOOB objective. The paper introduces the theoretical background methodology and experimental results showing that CLOOB outperforms CLIP in zeroshot transfer learning tasks on various datasets. Modern Hopfield networks are used to enrich covariance structures while InfoLOOB mitigates saturation effects observed with InfoNCE resulting in improved performance.  Strengths: 1. Innovation: The paper introduces a novel method CLOOB which effectively tackles the explaining away problem in CLIP using modern Hopfield networks and InfoLOOB. 2. Theoretical Justification: The paper provides a solid theoretical background for the proposed method explaining the mechanisms of Hopfield networks and the advantages of InfoLOOB over InfoNCE. 3. Experimental Results: Extensive experiments are conducted comparing CLOOB and CLIP on zeroshot transfer learning tasks demonstrating consistent performance improvements with CLOOB across different datasets and architectures.  Weaknesses: 1. Complexity: The paper delves into technical details which may be challenging for readers unfamiliar with the domain potentially limiting the accessibility of the work. 2. Evaluation Metrics: The evaluation metrics used in the experiments are not thoroughly discussed making it harder to understand the significance of the results. 3. Notation: The paper introduces various mathematical notations without sufficient explanation potentially confusing readers not wellversed in the specific terminology.  Questions: 1. How does the proposed CLOOB method compare in terms of computational efficiency compared to CLIP particularly considering the use of modern Hopfield networks 2. Could the authors provide more insight into the potential realworld applications of CLOOB beyond zeroshot transfer learning tasks 3. What are the implications of the suggested use of InfoLOOB for other contrastive learning methods beyond CLIP  Limitations: 1. Generalizability: The experiments focus on specific datasets and tasks the generalizability of CLOOB to a wider range of applications remains to be explored. 2. Validation: The significance and robustness of the experimental results could be strengthened by additional validation methods such as ablation studies or crossvalidation. 3. Resource Requirements: The introduction of modern Hopfield networks and the altered loss objective might increase computational demands a discussion on potential resource implications would be beneficial. Overall the paper presents a valuable contribution to contrastive learning methods addressing a known limitation in CLIP and showcasing improved performance with the proposed CLOOB method. Further exploration of the practical implications generalizability and comparative analyses with other similar approaches can enhance the impact and relevance of the research findings.", "xnI37HyfoP": "Peer Review 1) Summary (110 words): This paper introduces a novel algorithm for tensor completion of nonnegative tensors that achieves the informationtheoretic rate by utilizing a new norm and integer linear optimization. The algorithm converges in a linear number of oracle steps demonstrating effectiveness and scalability through experiments. The paper is wellstructured providing a comprehensive overview of the problem contributions norm definition complexity analysis and numerical experiments. 2) Strengths and Weaknesses: Strengths:  Novel algorithm for nonnegative tensor completion with a new norm.  Theoretical analysis with rigorous proofs and computational complexity assessments.  Empirical validation through experiments that demonstrate effectiveness and scalability. Weaknesses:  The paper is technically dense potentially challenging for readers without a strong background in tensor completion.  Limited discussion on practical implications and potential realworld applications of the proposed algorithm. 4) Questions:  Could the authors elaborate on potential realworld applications of their algorithm beyond the discussed examples in computer vision healthcare and regression  How does the proposed algorithm compare to existing stateoftheart methods in terms of computational efficiency and accuracy  Are there any limitations or constraints of the algorithm that might hinder its applicability in certain scenarios 5) Limitations:  The paper focuses heavily on theoretical aspects potentially lacking a more extensive discussion of practical implications and applications.  The empirical evaluations are limited in terms of the variety of datasets used and comparisons against other algorithms in the field. Overall the paper provides valuable contributions to the field of tensor completion offering a new algorithm for nonnegative tensors with theoretical guarantees and empirical validation. However the presentation could benefit from more clarity on practical relevance and comparative analyses with existing methods.", "lHuPdoHBxbg": " Peer Review: 1) Summary: The paper introduces the method of coarsetofine autoregressive networks (C2FAR) for modeling the probability distribution of univariate numeric random variables. C2FAR offers a hierarchical coarsetofine discretization of variable autoregressively which allows for an increase in precision without a proportional increase in complexity. C2FAR is applied to probabilistic forecasting using recurrent neural networks addressing challenges related to mixed and continuous time series data. The method is evaluated on benchmark forecasting datasets showing improvements over existing approaches. 2) Strengths and Weaknesses: Strengths:  Novel Method: The introduction of C2FAR offers a new approach to modeling distribution particularly suited to handling discrete and continuous time series data.  Improved Precision: C2FAR achieves an exponential increase in precision while maintaining a linear increase in complexity addressing the challenge of variable scales in time series data.  Performance: The method shows improvements over the stateoftheart on benchmark forecasting datasets.  Flexibility: C2FAR\u2019s flexibility enables a range of time series applications including anomaly detection interpolation and compression. Weaknesses:  Complexity: The complexity of the C2FAR model might be a drawback especially when compared to simpler approaches like flat binning methods.  Hyperparameter Tuning: The requirement for tuning hyperparameters in C2FAR might increase the complexity and computational cost.  Training Time: The paper does not provide a detailed analysis of the training time and computational resources required for implementing the C2FAR method which could be crucial for practical applications. 3) Questions:  How does the computational efficiency of C2FAR compare to existing methods especially in terms of training time and resource consumption  Can you elaborate on the practical implications of the hierarchical binning approach in terms of model interpretability and integration into realworld forecasting systems  Are there specific scenarios or types of time series data where C2FAR might not be as effective and how does the method address such challenges 4) Limitations:  The use of synthetic and benchmark datasets for evaluation may not fully represent the diversity and complexity of realworld time series data.  The paper does not address the potential limitations or challenges of implementing C2FAR in realworld forecasting applications such as data preprocessing requirements or integration with existing forecasting systems. In conclusion the paper presents a novel method for probabilistic forecasting with potential applications across various time series domains. Further insights into the practical implementation scalability and realworld performance of C2FAR would enhance the significance of this research for the scientific community.", "xqyEG7EhTZ": "Peer Review Summary: The paper introduces Tempo a novel approach aimed at optimizing accelerator memory allocation for training Transformerbased models in deep learning. The authors propose inplace replacements for GELU LayerNorm and Attention layers to reduce memory usage enhancing training efficiency. Experimental results on BERTLARGE GPT2 and RoBERTa models demonstrate significant improvements in training throughput and memory usage compared to baselines. Strengths and Weaknesses: Strengths: 1. Novel Approach: Tempo introduces unique techniques tailored specifically for Transformerbased models targeting memory footprint reduction without sacrificing performance. 2. Experimental Validation: The authors conduct thorough experiments across different model configurations hardware setups and sequences lengths to showcase the efficacy of Tempo. 3. Significant Results: Tempo shows up to 2x higher batch sizes and 26 higher training throughput over the baseline indicating substantial performance gains. Weaknesses: 1. Lack of Comparison: While the paper compares Tempo against baselines a comparison with other stateoftheart memory optimization techniques would provide a broader perspective on its effectiveness. 2. Limited Applications: The evaluation focuses mainly on pretraining and finetuning tasks on BERTLARGE GPT2 and RoBERTa with potential for broader application scope discussion missing. Questions: 1. Generalization to Other Architectures: How transferable are the proposed techniques in Tempo to models beyond Transformerbased architectures 2. Cost Implications: Are there tradeoffs in terms of additional computational costs or complexities introduced by implementing Tempo that need to be considered 3. Scalability: How does the performance of Tempo scale with increasingly complex models or larger datasets Limitations: 1. Limited Scope: The paper primarily focuses on memory footprint reduction for Transformerbased architectures potentially limiting the broad applicability of the proposed techniques. 2. Scalability Verification: While experiments demonstrate improvements further validation on largerscale datasets or with more diverse tasks could strengthen the generalizability of Tempo. Overall the paper presents a compelling methodology in Tempo for enhancing training throughput and memory efficiency of Transformerbased models. Addressing the current limitations and further exploring broader applications and scalability aspects would complement the promising results presented in the study.", "nE8IJLT7nW-": " Summary The paper introduces PerViT a model that incorporates peripheral position encoding into deep neural networks for visual recognition inspired by human peripheral vision. By partitioning the visual field into diverse peripheral regions and proposing Multihead Peripheral Attention (MPA) PerViT learns to provide attention that mimics human visual processing strategies capturing both finegrained details and coarse semantics. The study systematically investigates the models inner workings demonstrating improvements in image classification performance over baseline models across different sizes.  Strengths and Weaknesses Strengths: 1. Introduces a novel approach inspired by human peripheral vision to enhance visual recognition in deep neural networks. 2. Systematically analyzes the models learned attentions showcasing the adaptation of the network to mimic human visual processing. 3. Demonstrates consistent performance improvements in image classification over baseline models for different model sizes validating the effectiveness of the proposed method. Weaknesses: 1. The paper lacks a detailed comparison with more recent stateoftheart models beyond DeiT limiting the scope of evaluation. 2. The computational complexity considerations and scalability of the proposed PerViT model are not extensively discussed which could impact its practical utility. 3. While the study focuses on image classification extrapolating the findings to other vision tasks like object detection and segmentation is mentioned as potential future work but further exploration in this direction would add value.  Questions 1. Can the proposed PerViT model be easily extended or adapted for tasks beyond image classification such as object detection or image segmentation 2. How does the computational efficiency of PerViT compare to existing models especially with regard to handling largescale datasets and models 3. How does the training data size impact the effectiveness of PerViT especially in scenarios with limited labeled data  Limitations 1. The computational complexity of PerViT and its scalability to larger datasets and models are not extensively discussed. 2. The comparability of PerViT with the latest stateoftheart models beyond DeiT could provide a more comprehensive evaluation perspective. 3. The generalizability of the findings and performance of PerViT across a wider range of visual recognition tasks beyond image classification needs further exploration for a more comprehensive understanding of its utility in practical applications.", "ocg4JWjYZ96": " Peer Review:  Summary: The paper proposes a novel technique called empirical influence function (EIF) to address generalization errors in deep metric learning (DML) models. EIF efficiently identifies and quantifies training samples contributing to generalization errors and can recommend relabelling potentially noisy samples. The experimental results demonstrate EIFs superiority over traditional methods in identifying influential samples and improving model performance. The paper also conducts a field study on popular DML datasets revealing labeling issues and suggesting improvements. Overall the paper presents significant findings in understanding and enhancing DML approaches.  Strengths: 1. Novel Technique: Introducing EIF for debugging generalization errors in DML models is innovative. 2. Experimental Evaluation: The paper provides detailed quantitative and qualitative assessment of EIF showcasing its effectiveness. 3. Field Study: Conducting a field study on popular DML datasets adds practical relevance to the proposed technique. 4. Thorough Analysis: The paper offers a comprehensive analysis of dataset problems and provides actionable recommendations for improving DML performance. 5. Clear Presentation: The paper is wellstructured making it easy to follow the methodology results and conclusions.  Weaknesses: 1. Complexity: The papers technical details might be overwhelming for nonexperts in the field potentially limiting its accessibility. 2. Lack of Comparison: While EIF is compared with traditional methods a comparison with other stateoftheart techniques could strengthen the evaluation. 3. Ethical Considerations: The paper could benefit from discussing potential biases introduced by relabelling noisy samples and the ethical implications of dataset cleaning.  Questions: 1. How is the runtime performance of EIF influenced by the size of the dataset and the dimensionality of the embedding space 2. Can EIF effectively handle noisy samples that are not easily distinguishable from other samples in the training set 3. What are the potential implications of using EIF in realworld applications such as face recognition or clustering 4. Have the proposed relabelling recommendations been validated through user studies or realworld experiments 5. How does the proposed technique generalize to other domains beyond DML such as natural language processing or reinforcement learning  Limitations: 1. Dataset Specificity: The efficacy of EIF on other datasets outside of CUB200 CARS196 and InShop remains to be explored. 2. Scalability: The paper should address the scalability of EIF to larger datasets and more complex model architectures. 3. Overfitting Concerns: Relabelling noisy samples based on EIF recommendations may inadvertently lead to overfitting if not carefully handled. Overall the paper presents a valuable contribution to the understanding and improvement of DML models through the EIF technique. Addressing the weaknesses and limitations highlighted can enhance the papers impact and applicability in the research community.", "pfEIGgDstz0": " Peer Review of \"Hierarchical Neural Deformation Pyramid for NonRigid Point Cloud Registration\"  1) Summary: The paper proposes a novel approach called Neural Deformation Pyramid (NDP) for nonrigid point cloud registration. The method utilizes a hierarchical motion decomposition scheme based on a pyramid architecture with MultiLayer Perceptron (MLP) networks at each level. By gradually increasing the frequency of sinusoidal encoding the NDP achieves multilevel rigid to nonrigid motion decomposition and significantly speeds up the solving process compared to existing methods. The proposed method outperforms stateoftheart benchmarks in both nolearned and supervised settings demonstrating superior partialtopartial nonrigid registration results.  2) Strengths and weaknesses: Strengths:  Innovative approach: The hierarchical motion decomposition using MLPs in a pyramid structure is a novel and effective way to address nonrigid point cloud registration.  Superior performance: The NDP method achieves advanced registration results on challenging benchmarks under different settings.  Speed enhancement: Significantly faster solving times compared to existing approaches making it more efficient.  Extensive experimentation: The paper provides ablation studies and benchmarks to validate the effectiveness of the proposed method. Weaknesses:  Complexity: The methods effectiveness heavily depends on the hierarchical decomposition scheme which may limit its application in certain scenarios.  Lack of realtime performance: Although faster than existing methods NDP implementation is not realtime posing limitations in timesensitive applications.  3) Questions:  How does the NDP method handle cases with topological changes in the point clouds  Is there a proposed method to address the failure cases mentioned such as the nonisometric mappings observed in the results  Can the current approach handle dynamic changes in the point cloud structures such as continuous deformations or shape evolution over time  4) Limitations:  The method may not fully handle topological changes or nonisometric cases effectively.  Lack of realtime performance could be a limitation in applications requiring immediate responsiveness.  Challenges in addressing lowoverlap cases suggest limitations in handling scenarios with limited point cloud coverage. Overall the paper presents an innovative approach to hierarchical motion decomposition for nonrigid point cloud registration showcasing superior results compared to existing methods. Addressing the questions raised and mitigating the limitations identified could further enhance the practical applicability and robustness of the proposed NDP method.", "u4ihlSG240n": " Peer Review:  1) Summary: The paper introduces OmniVL a novel foundation model that unifies imagelanguage and videolanguage tasks using one universal architecture. It utilizes a transformerbased visual encoder for both image and video inputs enabling joint pretraining of imagelanguage and videolanguage. The model incorporates a decoupled joint pretraining approach a unified visionlanguage contrastive loss and supports various downstream tasks achieving stateoftheart or competitive results.  2) Strengths and Weaknesses: Strengths:  OmniVL introduces a novel approach of unifying imagelanguage and videolanguage tasks in one model.  The decoupled joint pretraining strategy aids in boosting performance on both image and video tasks.  The unified visionlanguage contrastive loss leverages a wide range of data types improving model performance.  The model demonstrates competitive results across various downstream tasks with similar model size and data scale. Weaknesses:  The paper lacks a detailed comparison with existing stateoftheart models in the field.  More insights could be provided on the computational efficiency and resource requirements of the proposed approach.  Further analysis on the robustness and generalization capabilities of OmniVL is warranted.  3) Questions:  Can you elaborate on how OmniVL balances efficiency in spatial and temporal representation learning during joint pretraining  How does the model handle potential biases and toxic language present in the largescale pretraining data  Are there plans to address limitations in the common sense reasoning capabilities of the model for better interaction tasks  4) Limitations:  The paper could benefit from additional experimental validation on larger and more diverse datasets to ensure generalizability.  A more extensive comparison with existing foundation models and their performance across a broader range of tasks would enhance the comprehensiveness of the study.  It is essential to consider the societal impact of deploying models pretrained on webcrawled data to address potential ethical concerns. Overall the paper presents a novel contribution in the visionlanguage domain with promising results on various tasks. Further exploration into model robustness interpretability and societal implications would enhance the impact and applicability of OmniVL in realworld scenarios.", "wmdbwZz65FM": " Peer Review: 1) Summary: The paper introduces a novel approach utilizing adversarial training to address the phenomenon of posterior collapse in autoregressive variational autoencoders (VAEs) for sequential data. The proposed method Adversarial Word Dropout (AWD) aims to drop words based on their information content ultimately improving both the latent structure and sequence modeling performance. Theoretical derivations and experiments on standard text benchmarks showcase the effectiveness of AWD in comparison to uniform dropout. 2) Strengths and weaknesses: Strengths:  The paper provides a clear explanation of the problem at hand and the proposed solution.  The theoretical basis for AWD and its implications are welldeveloped.  Experiments are conducted on standard text datasets demonstrating the effectiveness of the proposed method in enhancing both sentence modeling and the informativeness of the latent space. Weaknesses:  The paper could benefit from a more detailed discussion or comparison with existing methods addressing posterior collapse.  While the theoretical framework is wellexplained more clarity could be provided in relating it to the practical implementation of the AWD approach.  The qualitative analysis should be expanded further to provide deeper insights into the workings of the adversarial network. 3) Questions:  Could you elaborate on the computational complexity of the proposed method compared to traditional approaches  How sensitive is the performance of the AWD approach to the hyperparameter settings and how did you determine the optimal values  Has the qualitative analysis provided any unexpected or intriguing findings that might warrant further investigation 4) Limitations:  The paper would benefit from more diverse datasets to evaluate the generalizability of the AWD approach.  The discussion of potential ethical considerations or framework vulnerabilities related to sequence generation using the proposed method could add depth to the conclusions. In conclusion the paper presents a novel and theoretically grounded approach to address posterior collapse in sequence VAEs through the use of adversarial training. The empirical results demonstrate the effectiveness of the proposed method in improving both sequence modeling and latent space informativeness. Further refinement in the implementation and a more extensive evaluation across various domains could strengthen the overall impact of the paper.", "rcrY85WLAKU": " Peer Review:  1) Summary (avg word length 100): The paper introduces a novel approach for designing tensor network embeddings to efficiently sketch input data structures particularly in the context of tensor decomposition and kernel regression tasks. The proposed algorithm leverages Gaussian random tensors for reduced computational cost and increased efficiency. The work emphasizes the design of embeddings with low sketch sizes to optimize computational tasks. The paper includes theoretical analyses supported by experiments on CP decomposition and tensor train rounding showcasing the effectiveness of the proposed method in improving the accuracy and efficiency of these applications.  2) Strengths and Weaknesses: Strengths:  The paper provides a systematic approach to designing tensor network embeddings using Gaussian random tensors for efficient sketching of data structures.  The theoretical analyses offer a structured way to quantify the accuracy and optimize the sketch size for various tensor network structures.  The paper includes comprehensive experiments on CP decomposition and tensor train rounding to validate the proposed methods effectiveness.  The algorithm presented in the paper demonstrates promising results in terms of accuracy and computational efficiency for various tensorrelated applications. Weaknesses:  The paper could benefit from a more detailed discussion on the practical implications of the proposed method and how it compares to existing stateoftheart techniques.  While the theoretical analyses are thorough the paper could elaborate more on the limitations and assumptions underlying the proposed approach.  Further insights on the computational complexity and scalability of the algorithm could enhance the practical applicability of the method.  3) Questions:  How does the proposed tensor network embedding method compare to existing stateoftheart techniques in terms of accuracy and efficiency in diverse applications beyond CP decomposition and tensor train rounding  Can the proposed algorithm handle realworld datasets with varying complexities and sizes and how does its performance scale with larger datasets  Are there specific scenarios or data structures where the Gaussianrandomtensorbased embeddings may not be as effective and how might these limitations be addressed in future research  4) Limitations:  The evaluation of the proposed method predominantly focuses on CP decomposition and tensor train rounding leaving room for further exploration of its performance in a broader range of tensorrelated applications.  The practical implementation and scalability of the algorithm for handling largescale datasets and realworld scenarios could be areas where the method may face challenges that need to be addressed. Overall the paper introduces a novel approach to designing tensor network embeddings for efficient sketching of data structures showcasing promising results in experimental evaluations. Further research can build upon these findings to explore the methods applicability in diverse applications and address potential limitations encountered in more complex scenarios.", "nE8_DvxAqAB": "Peer Review Summary: The paper introduces Egocentric VideoLanguage Pretraining (VLP) and presents EgoClip a 1stperson videotext pretraining dataset EgoNCE pretraining objective and EgoMCQ benchmark. Experimental results demonstrate the effectiveness of Egocentric VLP on five tasks. The work addresses the domain gap between 3rdperson and 1stperson views contributing to egocentric video understanding. Strengths: 1. Novel Contributions: The creation of EgoClip dataset EgoNCE pretraining objective and EgoMCQ benchmark are significant contributions to advancing Egocentric VLP. 2. Thorough Methodology: The paper provides detailed explanations of dataset creation pretraining objectives model architecture and benchmarking tasks. 3. Comprehensive Experiments: The extensive experiments on five egocentric benchmarks demonstrate the effectiveness of the proposed approach. 4. Clear Results: The results section is wellorganized presenting comparison with stateoftheart methods across various tasks. Weaknesses: 1. Longwinded Sections: Some sections especially the description of dataset creation could benefit from more concise explanations to improve readability. 2. Evaluation Metrics: While the paper reports strong performance results a more detailed discussion on the choice of evaluation metrics and their implications could enhance the understanding of the experimental outcomes. Questions: 1. How does the proposed methodology generalize to unseen scenarios or domains not covered in the training datasets 2. What are the computational resources required for pretraining on EgoClip and are there potential scalability challenges 3. How does the model handle ambiguous situations where actions may have multiple interpretations based on the context 4. Can the proposed EgoNCE objective be applied to other egocentric video datasets and are there potential adaptations required for different datasets Limitations: 1. Temporal Dependencies: The paper acknowledges the limitation of not considering longterm temporal dependencies in Ego4D videos suggesting an avenue for future work. 2. Privacy and Bias Concerns: The use of participantcollected videos in Ego4D raises privacy and biasrelated issues which may require further investigation. In conclusion the paper presents a valuable contribution to the field of egocentric video understanding through the introduction of EgoClip EgoNCE and EgoMCQ. The experimental results support the effectiveness of Egocentric VLP but further discussions on generalization scalability and robustness would provide a more comprehensive evaluation of the proposed approach.", "wKd2XtSRsjl": " Peer Review  1) Summary The paper proposes a novel metric called Mutual Information Divergence (MID) to evaluate texttoimage generation and image captioning tasks using a negative crossmutual information approach with CLIP features. The method outperforms existing metrics demonstrating consistency across benchmarks sample parsimony and robustness with the CLIP model. The paper also presents theoretical analyses and experimental validations across multiple datasets and evaluation criteria. It provides comprehensive details on the proposed method empirical results and comparison with existing metrics. The work contributes a significant advancement in multimodal generative model evaluation methodologies.  2) Strengths and Weaknesses Strengths:  Novel approach proposing Mutual Information Divergence (MID) for evaluating multimodal generative models.  Theoretical analyses and thorough empirical validations across texttoimage generation and image captioning tasks.  Demonstrated significant performance improvement over existing metrics indicating consistency and robustness.  Detailed comparison with competitive methods and careful benchmarks on various datasets.  Clear explanation of the proposed method evaluation criteria and experimental results. Weaknesses:  Limited discussion on potential biases introduced by using CLIP models and implications for fairness accountability and transparency.  Possible overreliance on CLIP features and need to explore the performance with other feature extractors.  It may benefit from a more indepth analysis of the limitations of the proposed metric and areas for future research.  3) Questions  How does the proposed negative crossmutual information method address or control for potential biases in the CLIP features used for evaluation  Are there any known limitations or biases in the CLIP features that may affect the performance or generalizability of the proposed metric  Can the proposed Mutual Information Divergence (MID) be applied effectively to other multimodal generative tasks beyond texttoimage generation and image captioning  How would the proposed method perform when evaluated on a larger and more diverse set of datasets beyond those considered in this study  4) Limitations  The paper could provide more insights into the potential biases or limitations introduced by the use of CLIP features for evaluation especially in terms of social biases and fairness considerations.  The generalizability of the proposed metric across different types of multimodal generative tasks and datasets should be further investigated.  The performance of the proposed metric with other feature extractors or generative models should be explored to understand its robustness and applicability beyond CLIPbased models. Overall the paper presents a valuable contribution to the field of multimodal generative model evaluation with the introduction of Mutual Information Divergence (MID). Further investigations into potential biases generalizability and robustness of the proposed metric would enhance the impact and applicability of the research findings.", "nLGRGuzjtoR": " Peer Review of the Paper \"Limitations of Posthoc and Adversarial Concept Removal Methods: A Theoretical and Empirical Analysis\"  1. Summary: The paper analyzes the limitations of posthoc and adversarial concept removal methods in neural network models trained on text data. The authors show through theoretical and empirical analysis that these methods are counterproductive and fail to remove unwanted concepts entirely. They outline the use of a probing classifier as a proxy for concept removal and propose a spuriousness metric to evaluate the quality of the final classifier. The paper provides insights into the challenges of concept removal in text models and recommends caution in using these methods for sensitive applications.  2. Strengths and Weaknesses:  Strengths:  Theoretical Analysis: The paper provides a rigorous theoretical analysis of the limitations of posthoc and adversarial concept removal methods demonstrating the challenges in completely removing unwanted concepts.  Empirical Validation: The empirical results on synthetic and realworld datasets support the theoretical claims showing that these methods fail to achieve effective concept removal.  Novel Metric: The introduction of the spuriousness metric is a valuable contribution for evaluating the reliance of classifiers on spurious features providing a practical tool for assessing concept removal methods.  Weaknesses:  Assumptions: The reliance on certain assumptions in the theoretical analysis such as the frozen latent representation limits the generalizability of the results to all types of models and datasets.  Scope: The paper focuses on posthoc and adversarial methods but could benefit from a discussion on other potential approaches for concept removal or mitigation.  Clarity: The complexity of the theoretical analysis and technical details may pose a barrier for some readers to fully grasp the implications of the research findings.  3. Questions:  How do the limitations identified in posthoc and adversarial concept removal methods impact the broader field of machine learning interpretability and fairness  Are there potential modifications or refinements to the existing concept removal methods that could mitigate the identified failure modes  How do the findings of the paper relate to practical applications of concept removal in realworld scenarios especially in sensitive domains like fairness and bias mitigation  4. Limitations:  Generalization: The theoretical analysis and empirical results are based on specific assumptions and datasets which may not fully capture the complexity of concept removal across all text classification tasks.  Evaluation Metrics: While the spuriousness metric provides an important measure it might not capture all aspects of concept removal effectiveness especially in scenarios where the concept is deeply intertwined with the main task features.  Algorithmic Dependencies: The reliance on specific methods like null space projection and adversarial training may limit the papers conclusions to these specific methodologies potentially overlooking alternative approaches.  5. Overall Assessment: The paper presents a detailed and insightful analysis of the limitations of posthoc and adversarial concept removal methods in neural network models trained on text data. The combination of theoretical rigor and empirical validation strengthens the credibility of the findings. The introduction of the spuriousness metric is a valuable contribution to the field. Addressing the limitations and potential areas for refinement could further enhance the impact of the research. Overall the paper makes a significant contribution to understanding the challenges of concept removal in text models and provides important considerations for future research in this area.", "vMQ1V_z0TxU": " Peer Review:  1) Summary: The paper introduces an informative hierarchical VAE to address the issue of \"posterior collapse\" in hierarchical VAEs for unsupervised OOD detection. The authors propose a novel Adaptive Likelihood Ratio score function for OOD detection. Experimental results demonstrate that the proposed method significantly outperforms existing unsupervised OOD detection approaches across various benchmarks.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important issue of \"posterior collapse\" in hierarchical VAEs for OOD detection providing a novel solution.  The proposed Adaptive Likelihood Ratio score function shows promising results in outperforming existing approaches.  Thorough theoretical explanations and detailed analysis provide a strong foundation for the proposed methods.  Comparative experiments and evaluations with various benchmarks add credibility to the findings. Weaknesses:  The presentation of the theoretical concepts could be further simplified to enhance understanding for readers not wellversed in the subject matter.  The paper could benefit from more detailed discussions on potential practical applications and implications of the proposed method.  3) Questions:  Could the authors provide more insights into potential realworld applications or scenarios where the proposed method could be particularly beneficial  Are there any limitations or challenges in implementing the proposed informative hierarchical VAE and Adaptive Likelihood Ratio score function in more complex or larger datasets  4) Limitations:  The paper focuses largely on the technical aspects of the proposed method with limited discussion on practical implementation challenges that researchers or practitioners might face.  The comprehensive theoretical analysis while insightful may be difficult for readers with limited background in this area to fully grasp.  The evaluation of the models primarily relies on standard metrics additional qualitative analysis or user studies could enhance the understanding of the proposed methods effectiveness. Overall the paper presents a novel approach to address a significant challenge in unsupervised OOD detection using hierarchical VAEs. The proposed methods show promising results in outperforming existing approaches but the paper could be further improved by providing more practical insights and addressing potential implementation challenges.", "mMT8bhVBoUa": " Peer Review 1) Summary: The paper introduces a novel methodology termed Gaussian Wasserstein Inference (GWI) that leverages the Wasserstein distance between Gaussian measures in infinitedimensional function spaces for variational inference. The method is applied to deep neural networks combining their predictive performance with principled uncertainty quantification akin to Gaussian processes. The work showcases stateoftheart results on regression datasets and image classification tasks. 2) Strengths: The paper is wellwritten and comprehensive presenting a novel methodology and demonstrating its efficacy on benchmark datasets. The development of GWI combining Gaussian measures and Wasserstein distance is a significant contribution to the field. The discussion on different parameterizations of the variational mean and kernel is insightful providing a holistic view of the method. The experiments are thorough and show promising results especially in outofdistribution detection tasks. Weaknesses: While the paper introduces an innovative method there are some areas that could be further enhanced. The paper lacks a clear comparison with existing stateoftheart methods making it difficult to assess the methods advancement over current techniques. The limitations section could be expanded to address potential issues and further elaborate on challenges faced during implementation. The discussion on realworld applicability and scalability of GWI could be more indepth. 4) Questions:  Could you provide a more detailed discussion on how GWI compares to existing Bayesian deep learning approaches particularly in terms of scalability and inference time  How does the performance of GWI vary with different choices of the prior and variational kernel functions Are there specific criteria for selecting these functions  Can you elaborate on the computational complexity of GWI compared to traditional methods such as Gaussian processes or Bayesian neural networks 5) Limitations: The limitations section could be further enriched by addressing the following points:  Explore the implications of the unimodal nature of the posterior distribution modeled by Gaussian measures on GWIs applicability to scenarios with multimodal uncertainty.  Discuss the potential impact of numerical instabilities arising from the inversion of the kernel matrix on GWIs performance and scalability in largescale datasets.  Provide insights into the generalization of GWI to different types of kernels and the accuracy of approximations in cases of less rapid spectral decay. Overall the paper presents an innovative and promising method for variational inference in function spaces but additional discussions on comparisons with existing methods and further exploration of limitations would enhance the clarity and depth of the contribution.", "zrAUoI2JA2": " Summary: The paper introduces uHuBERT a selfsupervised pretraining framework for speech that can incorporate both unimodal and multimodal speech data. By leveraging modality dropout during pretraining uHuBERT achieves performance on par or better than stateoftheart modalityspecific models. The model demonstrates zeroshot modality generalization for various speech processing tasks including speech recognition and translation across different input modalities.  Strengths: 1. Innovative Approach: The introduction of uHuBERT utilizing modality dropout and shared target spaces presents a novel approach to address the challenges of utilizing multiple speech modalities. 2. Performance: The paper shows that uHuBERT achieves competitive performance compared to stateoftheart modalityspecific models and showcases zeroshot modality generalization. 3. Extensive Experiments: The paper includes detailed experiments and ablation studies on various speech tasks providing thorough analysis and insights. 4. Relevance: The research addresses the significant issue of limited labeled and unlabeled data for alternative speech modalities showing potential for realworld applications.  Weaknesses: 1. Complexity: The model and methodology introduced in the paper may be complex for readers unfamiliar with speech processing and selfsupervised learning. 2. Data Availability: The paper focuses on specific datasets and the generalizability of the approach to diverse datasets needs to be further explored. 3. Ethical Considerations: The paper briefly mentions ethical discussions but a deeper exploration of ethical implications could enhance the impact of the research.  Questions: 1. Transfer Learning: How does the model handle transfer learning to different domains outside the pretraining data especially in lowresource settings 2. Model Scalability: Can the uHuBERT framework be easily adapted to scale to larger datasets and be applied to realworld scenarios beyond the experimental setup 3. Noise Robustness: How does uHuBERT perform in scenarios with varying levels of noise and are there specific strategies employed to enhance noise robustness  Limitations: 1. Generalization: The generalization of the proposed framework to a broader range of speech modalities and tasks may require further validation. 2. Dataset Dependency: The performance of uHuBERT may be influenced by the specific datasets used for pretraining and finetuning limiting the models applicability in diverse settings. 3. Model Interpretability: The complexity of the model architecture and training methodology could hinder interpretability and practical implementation for users without extensive expertise in the field. Overall the paper introduces a promising framework for multimodal speech processing but further validation on diverse datasets and realworld applications would enhance the impact of the research.", "p62j5eqi_g2": " Peer Review: 1) Summary: The paper investigates blackbox adversarial attacks against deep clustering models utilizing a Generative Adversarial Network (GAN) approach. The study highlights the vulnerability of deep clustering models to adversarial attacks showcasing a sophisticated attack paradigm that significantly impacts deep clustering model performance. The experiments demonstrate the attacks effectiveness across diverse stateoftheart deep clustering models and realworld datasets. Additionally the study evaluates defense mechanisms such as robust deep clustering models and deep learningbased anomaly detection but finds them inadequate in mitigating the adversarial attacks. The attacks transferability across models and datasets is also explored concluding that deep clustering models lack robustness to blackbox adversarial attacks. Moreover the study extends the experiment by attacking a productionlevel face clustering API emphasizing the realworld applicability of the attacks and the need for enhanced model defenses. Overall the paper aims to provoke the development of more robust deep clustering models in the face of blackbox adversarial threats. 2) Strengths:  The paper addresses a significant gap in the research domain by exploring adversarial attacks against deep clustering models offering valuable insights into the vulnerability of such models to sophisticated attacks.  The study showcases a wellstructured methodology employing a GANbased blackbox attack strategy that proves highly effective in disrupting deep clustering model performances across various datasets and models.  The incorporation of empirical analyses defense mechanisms evaluation and realworld application of the attacks against a productionlevel API enriches the studys comprehensiveness and practical relevance.  The experimental replication of attacks on opensource and realworld models adds credibility to the findings and demonstrates the attacks impact across different use cases. Weaknesses:  The paper could benefit from providing more detailed discussions on the implications of the attack outcomes specifically in terms of the potential disruptions and implications of weakened clustering performances in practical settings.  The limitations section could be enhanced by discussing the potential implications of the attack approach on ethical considerations privacy concerns and societal impacts to provide a more holistic perspective on the studys implications.  Including a comparison with existing attack methodologies in other domains or similar vulnerabilities in related unsupervised learning models could help in establishing the uniqueness and significance of the proposed blackbox adversarial attacks against deep clustering. 3) Questions:  How does the attacks success rate vary across different types of datasets beyond image datasets such as text or numerical data  Can the attack effectiveness be influenced by the size of the dataset or the complexity of the deep clustering model architecture  Have the researchers considered the potential remedial strategies or countermeasures that can be implemented to enhance the robustness of deep clustering models against blackbox adversarial attacks in future work  How does the computational overhead of the GANbased attack impact its practicality and scalability in realworld applications especially when deploying defenses against adversarial threats 4) Limitations:  The study focuses predominantly on image datasets and deep neural networkbased models potentially limiting the generalizability of the findings to other data types and model architectures.  The attacks reliance on a GAN architecture might introduce computational complexities affecting the feasibility of the approach in resourceconstrained environments or for largescale applications.  The threat model assumptions could impose constraints on the attacks adaptability to realworld scenarios where complete knowledge of the training dataset or model architecture may not be accessible raising questions about the attacks practical applicability in diverse settings.  The study lacks indepth discussions on the potential societal impacts ethical considerations and implications of the proposed blackbox adversarial attacks on deep clustering models warranting further exploration in future research. Overall the paper presents a comprehensive study on blackbox adversarial attacks against deep clustering models showcasing a novel GANbased attack strategy and demonstrating the vulnerabilities of deep clustering models to adversarial threats. The findings contribute significantly to the understanding of adversarial vulnerabilities in unsupervised learning paradigms and call for enhanced defenses to bolster the robustness of deep clustering models in the face of blackbox attacks. The limitations and future research directions indicated in the study pave the way for broader explorations into the security and resilience of deep clustering models against adversarial threats.", "ripJhpwlA2v": " Peer Review  1) Summary: The paper introduces a new group invariant learning method SCILL to address the insufficiency of existing methods in preventing classifiers from depending on spurious correlations in the training set. The theoretical analysis reveals necessary conditions for groupIL to survive spurious correlations namely falsity exposure and label balance criteria. SCILL constructs groups with statistical independence tests and reweights samples to meet these criteria. Experimental results demonstrate the effectiveness of SCILL on synthetic and real data in generalizing to spurious correlation shifts.  2) Strengths and Weaknesses: Strengths:  Theoretical Contribution: The paper introduces two novel criteria for evaluating group invariant learning methods and provides a detailed theoretical analysis.  New Method: SCILL presents a practical solution to the shortcomings of existing groupIL methods by addressing spurious correlations effectively.  Empirical Performance: Extensive experiments on synthetic and real data demonstrate the superiority of SCILL over existing methods in generalizing to spurious correlation shifts. Weaknesses:  Assumptions: The central assumption of conditional independence between Xinv and Xsp given Y may limit the generalizability of the findings.  Comparative Analysis: The comparison with methods beyond groupIL and the assessment of SCILL against a wider range of approaches could enhance the papers completeness.  4) Questions:  Model Selection Strategies: How robust are the findings to the different model selection strategies used in the experiments  Generalizability: How transferable are the insights and methods proposed in this paper to other types of tasks and datasets  Scalability: Would SCILL be applicable to larger and more complex datasets beyond the ones tested in the experiments  5) Limitations:  Causal Assumptions: The assumption of conditional independence between Xinv and Xsp given Y may not hold in all scenarios potentially limiting the implications of the results.  Comparative Analysis: While SCILL is shown to outperform existing groupIL methods the paper could benefit from comparing SCILL with a broader range of debiasing and feature invariance methods.  Realworld Applicability: The practical implementation and computational efficiency of SCILL in realworld settings need to be explored further to assess its usability in complex applications. Overall the paper presents a valuable theoretical contribution and introduces a new practical method SCILL to address spurious correlations in group invariant learning. Further investigations into the generalizability and scalability of SCILL as well as comparisons with a wider array of techniques would enhance the impact and relevance of the research findings.", "uyEYNg2HHFQ": "Peer Review 1) Summary: The paper addresses the generation of novel model weights using hyperrepresentations learned from a model zoo without relying on original image data or labels. The proposed method includes layerwise loss normalization and sampling methods to produce diverse and highperforming models for various downstream tasks. Experimental results demonstrate the effectiveness of the approach in model initialization ensemble sampling and transfer learning across different image datasets. The study provides insights into the potential of hyperrepresentations for generative use in neural networks. 2) Strengths and weaknesses: Strengths:  The paper proposes a novel method for generating model weights from hyperrepresentations trained on a model zoo without requiring original image data.  The experimentation is thorough covering model initialization ensemble sampling transfer learning and unseen architectures demonstrating the versatility and potential of the proposed approach.  The utilization of layerwise loss normalization and sampling methods leads to diverse and highperforming models compared to baseline methods across multiple downstream tasks.  The discussion on robustness and smoothness of hyperrepresentations enhancement of initialization via LWLN and the ability to generate ensembles efficiently are noteworthy contributions. Weaknesses:  Despite the extensive experimentation the generalization to larger architectures or more complex models is limited. The study primarily focuses on simpler models due to the scale of the model zoos utilized.  The paper could provide more indepth analysis on the limitations of the approach especially regarding the scalability and applicability to more diverse architectures.  The method relies on previously trained model zoos which may raise concerns about generalizability to new and unseen tasks without preexisting model populations. 3) Questions:  Did the study consider the computational overhead of training hyperrepresentations and generating samples for various downstream tasks How does the efficiency of the proposed method compare to traditional approaches in terms of computational resources  How robust is the method to variations in model zoo composition and size Can the approach still perform effectively with smaller or larger model populations  What are the implications of applying the proposed method to realworld scenarios where access to extensive pretrained model zoos may be limited or impractical  Are there potential biases introduced by utilizing hyperrepresentations to generate model weights especially when transferring knowledge across datasets or unseen architectures 4) Limitations:  The study is conducted using model zoos of limited scale potentially limiting the generalizability of the findings to larger more complex architectures commonly used in realworld applications.  The reliance on existing model zoos for hyperrepresentation learning may hinder the applicability of the approach in scenarios where such pretrained model populations are unavailable or challenging to construct.  While the method shows promise in generating diverse and highperforming model weights the scalability and robustness of the approach to different model configurations and datasets remain areas for further investigation. Overall the paper presents an innovative approach to generating model weights using hyperrepresentations showcasing promising results across various downstream tasks. Further research could focus on addressing the limitations identified and exploring the applicability of the method to broader architectural settings and realworld use cases.", "kQgLvIFLyIu": "Peer Review Summary: The paper presents a novel approach to tackle the linesets kmedian clustering problem by introducing the concept of \u03b5coresets. These coresets provide approximate solutions to the problem with a small number of weighted linesets. The authors provide algorithms for constructing these coresets and highlight their applications through experimentation on synthetic and real datasets. The work also delves into related problems like fair clustering and colored pointsets clustering. Overall the paper contributes a significant advancement in the realm of clustering problems with specific shapes and structures. Strengths and weaknesses: Strengths: 1. The paper introduces a novel concept of \u03b5coresets for linesets kmedian clustering which provides efficient approximate solutions. 2. The detailed explanations of algorithms and strategies used for constructing coresets are welldefined and easy to follow. 3. Experimental results on synthetic and real datasets exhibit the effectiveness of the proposed approach over existing methods. 4. The exploration of related problems and the theoretical foundations laid out in the paper are strong contributions. 5. The inclusion of opensource code for replication and experimentation adds value to the research. Weaknesses: 1. Some technical details in the algorithms might be challenging for readers unfamiliar with the subject to grasp easily. Providing more intuitive explanations could help in this regard. 2. The comparison with only one baseline method could limit the understanding of how the proposed coreset technique fares against a broader range of approaches. 3. The study could benefit from further elaboration on the limitations of the proposed algorithms and potential areas for future research. Questions: 1. Have you considered expanding the comparison to include additional baseline methods to further benchmark the performance of your proposed coresets 2. How generalizable are your coreset constructions to different types of data distributions and dimensionalities 3. In what scenarios or domains do you foresee the proposed \u03b5coresets being most beneficial compared to traditional clustering methods 4. Can you elaborate on any unexpected findings or challenges encountered during the implementation and experimentation phases of your research Limitations: 1. Although the experimental results are promising the evaluation on a wider variety of datasets with different characteristics could enhance the generalizability of the proposed approach. 2. The assumptions and constraints made in the construction of coresets and their scalability to larger datasets should be thoroughly examined. 3. The paper could discuss the potential impact of noise in the data and its effect on the accuracy of the coresets. Overall the paper presents a significant advancement in the field of clustering problems and coresets construction. Addressing the highlighted weaknesses and limitations could further strengthen the research and contribute to its practical applicability and impact.", "uCXNOeL0TG": " Summary The paper introduces the MultiWorker Restless MultiArmed Bandit (MWRMAB) problem addressing intervention scheduling with heterogeneous workers. The proposed framework combines Whittle indices and a balanced allocation strategy to achieve fairness and efficiency. Experimental results demonstrate the frameworks effectiveness in achieving fairness while maintaining comparable rewards to baseline methods.  Strengths and Weaknesses  Strengths: 1. Novel Contribution: Introducing MWRMAB problem and workercentric fairness constraint. 2. Methodological Approach: Combining Whittle indices and balanced allocation to achieve fairness. 3. Empirical Evaluation: Comprehensive evaluation across various experimental settings demonstrating fairness and efficiency compared to baselines. 4. Scalability: Demonstrating scalability of the approach even for larger instances.  Weaknesses: 1. Complexity: The proposed method may be complex due to its multiworker considerations and balancing fairness. 2. Theoretical Assumptions: The paper relies on certain assumptions for theoretical proofs which might not hold in all realworld scenarios. 3. Baseline Comparison: The comparison with baselines could be extended to include more diverse algorithms for a comprehensive evaluation.  Questions 1. Can you elaborate on the potential realworld applications where the proposed MWRMAB framework could be most beneficial 2. How sensitive is the fairness aspect of the framework to changes in the model parameters such as costs and transition probabilities 3. In the empirical evaluations were there any specific scenarios where the proposed framework struggled or showed limitations 4. How generalizable is the proposed framework to other related optimization problems beyond the intervention planning context  Limitations The limitations include the inherent complexity of the MWRMAB problem reliance on specific assumptions for theoretical proofs and a scope that could be broadened to include more baseline comparisons and diverse scenarios in the empirical evaluation. Moreover practical implementation aspects such as platform dependencies resource requirements and potential challenges in deploying the framework in realworld settings could provide valuable insights for future research.", "p9_Z4m2Vyvr": " Peer Review  1) Summary The paper introduces a novel approach called clusterwise Amortized Mixing Coupling Processes (AMCP) for efficient clustering by using intracluster mixing and intercluster coupling strategies. AMCP improves traditional clustering methods by reducing computational complexity and adapting to changing data distributions. The proposed method is evaluated on synthetic and realworld datasets showing competitive performance in terms of clustering accuracy computational efficiency and adaptivity.  2) Strengths and Weaknesses  Strengths:  Novel Approach: Introduces AMCP a clusterwise amortized clustering method using mixing and coupling strategies for efficient clustering.  Efficiency: Reduces computational complexity from quadratic to linear by avoiding pairwise calculations of distances between clusters.  Adaptivity: Can dynamically adjust clusters without handcrafted intervention enabling unsupervised parameter learning.  Experimental Results: Shows superior clustering performance and computational efficiency compared to stateoftheart methods on synthetic and realworld datasets.  Weaknesses:  Complexity: The proposed method involves intricate steps like IntraCluster Mixing and InterCluster Coupling which may be challenging to implement and understand.  Evaluation: While the experimental results highlight the effectiveness of AMCP additional details on the hyperparameters selection and convergence analysis would enhance the completeness of the evaluation.  3) Questions  Optimization Methods: How sensitive is the AMCP to hyperparameter selection and was there any parameter tuning involved in achieving the reported results  Scalability: How would the performance of AMCP scale with largescale datasets in terms of both computational efficiency and clustering accuracy  Generalization: How well does the proposed method generalize to datasets with varied data distributions and cluster complexities beyond the ones used in the experiments  4) Limitations  Interpretability: While the methodology is novel and effective the paper could benefit from more thorough explanations and intuitive visual representations to aid in understanding the proposed process.  Comparison: The comparison with baselines seems limited indepth analysis further insights into the strengths and weaknesses of AMCP compared to existing methods would enhance the papers discussion section. Overall the paper demonstrates a promising approach for efficient amortized clustering with competitive performance. However further clarifications on the methodology thorough scalability analysis and comprehensive comparison with existing methods would strengthen the papers contribution to the field of clustering algorithms.", "pz2UcXyX0Cj": " Peer Review: 1) Summary: The paper introduces CDHRL a causalitydriven hierarchical reinforcement learning framework that aims to overcome the limitations of existing HRL methods in discovering highquality hierarchical structures for complex environments. By leveraging causality to guide the construction of subgoal hierarchies CDHRL significantly improves exploration efficiency in tasks with sparse rewards. The proposed framework is validated through experiments in 2DMinecraft and Eden showcasing superior performance compared to stateoftheart HRL methods. 2) Strengths and Weaknesses: Strengths:  Innovative Approach: CDHRL introduces a novel methodology of using causality to drive the discovery of hierarchical structures addressing a key challenge in HRL.  Experimental Validation: The paper provides detailed experiments in two complex environments demonstrating the effectiveness of CDHRL in enhancing exploration efficiency.  Iterative Boosting Framework: The iterative process of causality discovery and subgoal hierarchy construction appears welldesigned and shows promising results in enhancing learning efficiency. Weaknesses:  Complexity of Implementation: The proposed framework involves intricate concepts such as causal discovery in the context of MDP which may require additional clarity for readers unfamiliar with these topics.  Limitation to Discrete Variables: CDHRL is limited to environments with discrete variables restricting its applicability in continuous state spaces. 3) Questions:  How does CDHRL handle highdimensional state spaces in environments with complex observations such as imagebased inputs  Can CDHRL be extended to work with continuous action spaces or is it specifically designed for discrete actions  Are there any theoretical guarantees or formal analysis of the convergence properties of the causalitydriven exploration paradigm in CDHRL 4) Limitations:  Generalization to Continuous Spaces: The reliance on discrete variables limits the generalizability of CDHRL to environments with continuous state spaces which are prevalent in many realworld applications.  Scalability Concerns: The effectiveness of CDHRL in larger and more complex environments with a multitude of causal relationships between variables remains to be explored.  Application Scope: The paper acknowledges the specific application scope of CDHRL in environments with discrete variables posing a challenge for broader adoption. Overall the paper presents a novel approach in addressing hierarchical reinforcement learning challenges through a causalitydriven paradigm. While the framework shows promise in enhancing exploration efficiency further investigation into scalability applicability to continuous spaces and theoretical underpinnings would strengthen the contribution of CDHRL in the field of reinforcement learning.", "nax3ATLrovW": "Peer Review 1) Summary: The paper proposes a novel approach to integrate multiple information sources in Electronic Design Automation (EDA) using deep learning techniques. The proposed method Circuit GNN aims to handle diverse tasks in circuit design by creating a unified heterogeneous graph called Circuit Graph. The paper demonstrates the effectiveness of Circuit GNN in logic synthesis and placement stages achieving stateoftheart performance and significant speedup in congestion prediction. The experiments show promising results in circuit representation and prediction tasks. 2) Strengths:  Novelty: The paper introduces a unique approach to integrate topological and geometrical information in a unified Circuit Graph enabling efficient handling of diverse EDA tasks.  Performance: The experimental results demonstrate the superiority of Circuit GNN in logic synthesis global placement and congestion prediction compared to existing methods.  Efficiency: The proposed method shows a 10x speedup in congestion prediction indicating efficient utilization of heterogeneous information sources.  Clarity: The paper is wellstructured clearly explaining the problem methodology and results of the study. Weaknesses:  Evaluation Scope: The evaluation focuses on specific tasks in EDA and the paper could benefit from discussing the generalizability and applicability of Circuit GNN to broader domains.  Comparison Baselines: While the paper compares Circuit GNN with existing methods additional comparisons with more recent stateoftheart techniques in deep learning for EDA could enhance the discussion and validation of the proposed approach.  Explanation of Results: Some parts of the experimental results and comparisons could be further elaborated to provide deeper insights into the performance of Circuit GNN in different scenarios. 3) Questions:  How does the proposed Circuit GNN approach handle noise or variations in the input data especially in realworld practical scenarios where circuit designs may have imperfections  Are there specific limitations or challenges faced during the implementation or experimentation of the Circuit GNN method that the authors encountered and addressed  Can the approach be easily adapted or extended to handle emerging challenges in EDA such as security or reliability considerations in circuit design 4) Limitations:  Generalization: The proposed approachs generalizability to different types of circuit designs or EDA tasks beyond logic synthesis and placement stages is not extensively discussed.  Scalability: The scalability of the Circuit GNN method to larger and more complex circuits or datasets may pose challenges that need to be addressed in future research.  Commercial Adoption: While the paper highlights the benefits of the proposed method challenges related to practical implementation and adoption in commercial EDA tools are not fully explored. In conclusion the paper introduces an innovative approach for circuit design using deep learning and heterogeneous graph integration. The results demonstrate significant improvements in performance and efficiency making Circuit GNN a promising technique for diverse EDA tasks. Further investigations could delve into scalability robustness and realworld applicability of the proposed method in the field of Electronic Design Automation. Overall Rating: 3.55", "rTvH1_SRyXs": " Summary: The paper addresses the lack of a common foundational goal among post hoc explanation methods by unifying eight popular methods under a local function approximation (LFA) framework. It demonstrates that these methods perform local function approximation of blackbox models differing only in the neighbourhoods and loss functions used. The paper presents a no free lunch theorem for explanation methods showing that no method can perform optimally across all neighbourhoods. It provides a guiding principle for selecting among methods based on faithfulness to the blackbox model and empirically validates the theoretical results using realworld datasets.  Strengths:  The paper addresses a critical issue in post hoc explainability by unifying diverse explanation methods under a common framework.  The formalization of the local function approximation framework provides a theoretical basis for understanding and comparing explanation methods.  The paper demonstrates the practical implications of the LFA framework through empirical evaluation on various datasets model classes and prediction tasks.  The no free lunch theorem adds a valuable insight into the limitations of explanation methods across different neighbourhoods.  Weaknesses:  The paper could provide more indepth discussion on the practical implications of the results and how they can be translated into realworld applications.  The empirical evaluation while thorough could benefit from additional experiments on a broader range of datasets and model architectures.  It would be beneficial to discuss the limitations of the proposed framework more explicitly and address potential challenges in its application.  Questions: 1. How does the LFA framework handle scenarios where the blackbox model f is highly complex or nonlinear 2. Are there specific types of datasets or model classes where the proposed guiding principle for selecting among explanation methods may not be as effective 3. Can the LFA framework be extended to incorporate considerations for interpretability beyond faithfulness to the blackbox model  Limitations:  The findings are limited to the specific set of explanation methods and datasets evaluated in the paper and generalizability to other contexts may vary.  The evaluation focuses on the faithfulness of explanations and additional aspects of interpretability (e.g. human understanding) are not extensively explored.  The theoretical results and guiding principle provided in the paper may require further validation across diverse domains and scenarios to assess their robustness.", "zAuiZpZ478l": " Summary The paper introduces the hierarchical lattice layer (HLL) as an extension of the TensorFlow Lattice library for partially monotone regression in neural networks. HLL aims to address the limitations of the lattice layer specifically the high memory consumption and the need for the projected gradient descent algorithm with many constraints. The experimental results show that HLL maintains prediction performance while supporting highdimensional input vectors with a small m indicating its effectiveness in handling monotonicity constraints.  Strengths  Innovative Approach: Introducing the HLL as an extension of the lattice layer to enable training using a standard stochastic gradient descent algorithm.  Experimental Validation: Conducting experiments to compare HLL with existing models demonstrating its ability to maintain prediction performance on real datasets.  Theoretical Analysis: Providing proof of monotonocity constraints and complexity analysis.  Clear Explanations: Providing detailed descriptions of the HLL structure training algorithm and inference process.  Applicability: Addressing a practical problem in machine learning applications related to monotonocity.  Weaknesses  Limited Scope: The experiments focus on comparing HLL with existing models without exploring more advanced optimization techniques or application scenarios.  Theoretical Complexity: Some parts of the theoretical explanations might be challenging for readers with limited background in neural networks or machine learning algorithms.  Practical Application: While the proposed HLL shows promising results there is a lack of discussion on the realworld applications and scalability.  Questions 1. How does the HLL layer address potential overfitting issues given its design focused on monotonicity constraints 2. Are there any insights on how the HLL layer performs compared to other models on regression tasks outside of the monotonocity constraints context 3. Were there any specific limitations or challenges encountered during the experimentation phase that were not explicitly addressed in the paper  Limitations  Experimental Setup: The paper lacks indepth exploration of hyperparameter tuning strategies or robustness testing.  Dataset Diversity: The experiments are conducted using a limited set of datasets additional datasets could provide more comprehensive insights.  Scalability: While HLL shows promise for highdimensional input vectors with a small m scalability to larger dimensions may pose challenges that need further investigation.  RealWorld Applications: The practical implications and realworld applicability of HLL need to be further explored across a wider range of machine learning tasks. Overall the paper presents a novel approach in partially monotone regression with the introduction of HLL showcasing its potential to address existing limitations in neural network models. Further research and validation across diverse datasets and practical scenarios would contribute to a more thorough understanding of HLLs capabilities and limitations.", "xbJAITw9Z6t": " Peer Review of the Scientific Paper 1. Summary: The paper introduces a Stacked Unsupervised Learning (SUL) algorithm that is applied to MNIST digit clustering. The algorithm involves multiple convolutional energy layers and KSubspaces clustering with a focus on learning invariances for pattern recognition. The authors demonstrate the effectiveness of the algorithm in achieving high accuracy comparable to supervised methods but without the need for labeled data following a biologically plausible approach. 2. Strengths:  The paper addresses an important and relevant topic in the field of unsupervised learning focusing on the biologically inspired SUL algorithm.  The use of a novel SUL algorithm for unsupervised clustering on the MNIST dataset showcases competitive accuracy compared to supervised methods.  The inclusion of detailed network architecture algorithms and convergence analysis adds depth to the study.  The incorporation of metalearning for hyperparameter tuning provides insights into improving performance.  Experimental results and comparisons with existing algorithms are welldocumented providing a comprehensive evaluation. 3. Weaknesses:  The paper lacks information on potential societal impacts or ethical considerations related to the research.  The methodological rigor may be enhanced with error bars on experimental results and a detailed description of the computational resources used.  The convergence analysis could be complemented with more theoretical insights and proofs for better clarity.  Although the automated tuning of hyperparameters is highlighted additional details on the specific effect of each hyperparameter on performance would be beneficial. 4. Questions:  How sensitive is the SUL algorithm to different types of datasets beyond MNIST Have you considered testing the algorithm on larger and more diverse datasets  Can the learned features from the SUL algorithm be interpreted or visualized for better understanding and model interpretability  How does the proposed SUL algorithm compare to stateoftheart unsupervised methods that utilize more complex architectures or training strategies  Is the proposed SUL algorithm computationally efficient and scalable for handling larger datasets and more complex tasks 5. Limitations:  Lack of detailed discussions on potential societal implications and ethical considerations.  The methodology could be further strengthened with additional theoretical analyses and robustness testing on various datasets.  The generalization of the proposed SUL algorithm beyond MNIST and its scalability to more intricate tasks are areas for further exploration. Overall the paper presents an innovative approach to unsupervised learning with a strong focus on biologically plausible algorithms. The findings provide valuable insights into the capabilities and limitations of stacked unsupervised learning models. Further improvements in methodology theoretical foundations and broader applicability can strengthen the impact of the research in the field of machine learning.", "rJjJda5q0E": " Peer Review:  Summary: The paper focuses on studying the Bayesian regret of Thompson Sampling algorithm in contextual bandits with binary losses particularly in the presence of adversariallyselected contexts. It introduces a novel perspective by adjusting the information ratio to iterate the tradeoff between incurring low regret and gaining information about the hidden parameter \\xce\\xb8 rather than the optimal action. The main results provide bounds on the Bayesian regret of Thompson Sampling in various settings including logistic bandits with Lipschitz logits. The paper also presents concrete regret bounds theoretical analysis techniques and explores extensions beyond binary losses to include linear bandits with Gaussian noise.  Strengths and Weaknesses: Strengths: 1. Novel Perspective: The adjustment of the information ratio to focus on gaining information about the hidden parameter \\xce\\xb8 rather than the optimal action in contextual bandits is a significant contribution. 2. Theoretical Advances: The paper provides new theoretical frameworks and concrete regret bounds in logistic bandits and linear bandits with Gaussian noise expanding the current understanding in the field. 3. Technical Rigor: The mathematical proofs are detailed and the analysis is wellstructured providing a comprehensive theoretical foundation for the results. Weaknesses: 1. Complexity: The paper delves into advanced theoretical concepts which may make it challenging for readers less familiar with the topics covered. 2. Practical Applicability: While the theoretical results are robust the practical implementation and implications of the findings are not extensively discussed.  Questions: 1. How does the proposed adjustment in the information ratio impact the practical implementation of Thompson Sampling in contextual bandits 2. Can the findings and analysis techniques in the paper be extended to other types of bandit problems beyond those discussed in the context of the study 3. Are there any practical scenarios or realworld applications where the novel perspectives introduced in the paper can be directly applied or tested  Limitations: 1. The paper focuses primarily on theoretical analysis and may benefit from further exploration of practical implications and applications of the proposed concepts. 2. The complexity of the mathematical proofs and concepts presented may limit the accessibility of the paper to a broader audience including researchers new to the field. Overall the paper presents valuable insights into Bayesian regret analysis in contextual bandits with novel theoretical advancements and concrete regret bounds. Consideration of practical implications and broader applications could enhance the impact of the results. This peer review aims to provide constructive feedback and areas for further exploration in the research study.", "z0M3qHDqH20": " Peer Review  Summary: The paper introduces HUMUSNet a novel hybrid architecture combining Transformers and convolutions for accelerated MRI reconstruction. The proposed network achieves stateoftheart performance on the fastMRI dataset and demonstrates improvements over existing methods on other MRI datasets. The authors conducted extensive experiments and ablation studies to validate their design choices highlighting the efficacy of the proposed HUMUSNet in addressing challenges in Transformerbased architectures for lowlevel vision tasks like MRI reconstruction.  Strengths: 1. Novel Architecture: The integration of Transformer blocks with convolutional features in HUMUSNet is a significant contribution to the field of MRI reconstruction. 2. StateoftheArt Results: HUMUSNet establishes new benchmarks on the fastMRI dataset and demonstrates improved performance over existing methods on other MRI datasets. 3. Extensive Experimental Validation: The inclusion of benchmark experiments ablation studies and direct comparisons with existing models provides a thorough evaluation of the proposed architecture. 4. Reproducibility: The availability of source code and reproducible results enhances the credibility of the study.  Weaknesses: 1. Complexity: The paper dives quickly into technical details making it challenging for readers not wellversed in MRI reconstruction and deep learning to follow along. 2. Limited Discussion: The limitations of the proposed model and potential challenges in practical implementation are not extensively discussed. 3. Lack of Performance Metrics: The paper mainly focuses on SSIM comparisons lacking a broader range of evaluation metrics for a comprehensive analysis of model performance.  Questions: 1. Scalability: How does the proposed network scale with respect to input resolutions beyond the limitations discussed in the paper 2. Generalizability: Have the authors tested the performance of HUMUSNet on datasets outside of MRI reconstructions to evaluate its applicability to broader image restoration tasks 3. Computational Efficiency: How does the computational efficiency of HUMUSNet compare to existing methods especially in terms of inference time on different hardware platforms  Limitations: 1. FixedSize Inputs: The current architecture requires fixedsize inputs limiting its flexibility in handling images of varying dimensions. 2. Evaluation Metrics: The paper could benefit from including a diverse set of evaluation metrics to provide a more comprehensive assessment of model performance. Overall the paper presents a novel approach to MRI reconstruction with HUMUSNet backed by substantial experimental validation. Addressing the weaknesses and addressing the questions raised would enhance the clarity and impact of the study.", "yjWir-w3gki": "Peer Review: 1) Summary: The paper introduces a novel theory for continuoustime modelbased reinforcement learning with nonexponential discount functions aiming to generalize RL to different discounting schemes observed in human decisionmaking. The proposed approach is applied to two simulated problems showing promising results. The work also delves into inverse reinforcement learning to infer discount functions from decision data. Overall the study is wellstructured combining theoretical derivations with practical applications. 2) Strengths:  Originality and Innovation: The paper offers a novel approach by incorporating nonexponential discount functions in continuoustime RL bridging the gap between traditional exponential discounting and hyperbolic discounting observed in human decisionmaking.  Theoretical Contributions: The derivation of the Hamilton\u2013Jacobi\u2013Bellman equation for general discount functions and the exploration of solving it with a collocation method are significant contributions to the field.  Applicability and Simulations: The experiments conducted on two simulated tasks demonstrate the practical feasibility and effectiveness of the proposed method showcasing its potential for realworld applications.  Inverse Reinforcement Learning: The inclusion of an inverse reinforcement learning approach to infer discount functions from behavioral data is a valuable addition enabling the analysis of human decisionmaking under uncertainty. 3) Weaknesses:  Clarity and Accessibility: While the paper presents a comprehensive theoretical framework the dense technical language and complex mathematical derivations may pose challenges for readers not wellversed in reinforcement learning and optimization.  Assumptions and Limitations: The paper assumes a known model and discrete action spaces limiting the scalability of the proposed method to more complex and continuous control problems. Addressing these limitations and discussing potential solutions could enhance the practicality of the approach. 4) Questions:  Can the proposed method handle highdimensional state spaces efficiently especially in cases where the model is unknown  How sensitive is the approach to the selection of discount function parameters especially in scenarios with noisy data or uncertainties in parameter estimation  Have you considered extensions of the method to incorporate continuous control or learn discount functions from trajectory data rather than action switch points 5) Limitations:  The assumption of a known model restricts the generalizability of the method to realworld applications where models may be imperfect or unknown.  The discrete action space assumption limits the applicability to tasks with continuous controls requiring further exploration for broader usage scenarios.  The complexity of the theoretical framework and mathematical derivations may hinder the adoption of the method by researchers with limited expertise in the field. Overall the paper presents a novel and promising approach to modelbased reinforcement learning with nonexponential discount functions contributing to the understanding of human decisionmaking processes. Addressing the outlined weaknesses and limitations could further enhance the significance and applicability of the proposed method.  Please let me know if you need further elaboration on any specific aspect of the review.", "mrt90D00aQX": " Peer Review of the Scientific Paper  1. Summary: The paper introduces FedSR a novel representation learning framework integrating Domain Generalization into Federated Learning. The proposed method aims to learn a simple representation of data by incorporating L2norm and conditional mutual information regularizers. The theoretical and empirical results demonstrate the superiority of FedSR over existing methods in domain generalization tasks across various datasets. The paper provides a clear problem statement detailed methodology and extensive experimentation results validating the effectiveness of the proposed approach.  2. Strengths and Weaknesses: Strengths:  Clear Problem Statement: The paper effectively highlights the issue of Domain Generalization in Federated Learning providing a comprehensive background.  Novel Methodology: The introduction of FedSR leveraging L2norm regularizer and conditional mutual information is a significant contribution to the field.  Theoretical Alignment: The connection between regularizers and representation alignment in domain generalization is well explained.  Extensive Experiments: The performance of FedSR is extensively evaluated on various datasets showcasing its superiority over baselines. Weaknesses:  Privacy Concerns: The paper lacks a detailed discussion on how FedSR maintains data privacy in the decentralized FL setting.  Experimental Comparison: While comparing with centralized methods is valuable a more direct comparison with FL methods in a decentralized setup could further strengthen the analysis.  RealWorld Applicability: More insights on the practical implementation and scalability of FedSR in realworld scenarios could enhance the papers relevance.  3. Questions:  How does FedSR address potential privacy breaches in a decentralized FL setting considering the constraints on data sharing among clients  Could the paper elaborate on the computational complexity of FedSR especially in terms of scalability and convergence speed in largescale FL scenarios  Are there plans to extend the evaluation of FedSR to other diverse domains or realworld applications to showcase its versatility  4. Limitations:  Limited Privacy Discussion: The paper could provide more insight into how FedSR ensures data privacy and confidentiality in a decentralized FL environment.  Scalability Concerns: The scalability of FedSR in largescale FL scenarios particularly in terms of communication overhead and convergence time warrants further exploration.  Generalization to Unseen Domains: While FedSR demonstrates domain generalization effectiveness the ability to generalize to unseen target domains remains a potential limitation that could be further addressed. Overall the paper presents a valuable contribution to the Federated Learning domain by addressing the challenge of Domain Generalization. The proposed methodology FedSR showcases promising results and opens avenues for future research in decentralized privacypreserving machine learning frameworks.", "msBC-W9Elaa": " Peer Review 1. Summary The paper introduces a novel covering technique for localized trajectories of Stochastic Gradient Descent (SGD) to derive generalization bounds applicable to nonconvex functions. The results demonstrate independence from dimension and improvement over existing stateoftheart rates in various machine learning models. Notably the approach allows for estimation without early stopping or decaying step size. 2. Strengths  The paper provides a comprehensive analysis and theoretical framework for deriving dimensionindependent generalization bounds for nonconvex optimization problems.  The results are wellstructured and clearly presented showcasing the localized covering technique and its implications in multiple learning scenarios indicating a significant contribution to the field.  The extension of results to multiindex linear models support vector machines and Kmeans clustering allows for a broad application of the proposed method.  The discussion section provides insights into potential future directions and acknowledges limitations adding depth to the research. 3. Weaknesses  The paper is technically dense and may pose challenges for readers unfamiliar with advanced optimization theory requiring a strong mathematical background.  While the method is effective for certain scenarios the limitations regarding varying step sizes and the translation of generalization error bounds to excess risk could be further addressed or explored in future research. 4. Questions  How do the proposed generalization bounds compare in practical scenarios with datasets of varying complexity and noise levels  Can the method be extended to incorporate more complex optimization algorithms beyond SGD and what implications would this have on the generalization bounds 5. Limitations  The results mainly focus on theoretical analysis and may benefit from additional empirical validation on realworld datasets to confirm the effectiveness of the proposed technique.  The applicability of the presented method to deep learning architectures or models with highdimensional data sets could be a potential limitation and requires further investigation. Overall the paper presents a promising approach to addressing the generalization problem in nonconvex optimization offering valuable theoretical insights and potential directions for future research. The authors are encouraged to consider addressing the identified weaknesses and questions to enhance the robustness and practical applicability of their findings.", "lKULHf7oFDo": " Summary The paper introduces a new fairness notion in federated learning called corestable fairness which addresses the challenges of heterogeneity and fairness among local agents. The authors propose a federated learning protocol CoreFed to optimize a corestable predictor using concepts from game theory. They demonstrate the existence of corestable predictors in various scenarios including linear regression logistic regression and deep neural networks by leveraging Kakutanis fixed point theorem. Empirical evaluations on realworld datasets show that CoreFed achieves higher corestable fairness compared to FedAvg while maintaining similar accuracy.  Strengths  The paper introduces a novel concept corestable fairness incorporating game theory into federated learning to address fairness issues.  The proposed CoreFed protocol is welldesigned and theoretically grounded providing a solution to optimize corestable predictors in various scenarios.  The theoretical proofs using Kakutanis fixed point theorem provide a robust foundation for the existence of corestable predictors in federated learning.  Empirical evaluations on realworld datasets demonstrate the effectiveness of CoreFed in achieving higher corestable fairness while maintaining utility.  Weaknesses  The paper lacks a detailed comparison with existing fairness notions in federated learning such as egalitarian and proportional fairness which could provide better context and comparison.  The experimental section could be expanded to include more datasets and a broader range of models to further validate the effectiveness of CoreFed.  The paper could benefit from a more detailed discussion on the limitations and potential future directions of research based on the proposed corestable fairness concept.  Questions 1. How does the proposed corestable fairness concept compare with other wellknown fairness notions in federated learning such as egalitarian and proportional fairness 2. Could the paper discuss the computational complexity of the CoreFed protocol compared to traditional federated learning methods like FedAvg 3. Did the authors consider any potential tradeoffs or drawbacks of implementing corestable fairness in federated learning systems  Limitations  The paper could be more explicit in discussing the limitations of the corestable fairness concept and the proposed CoreFed protocol.  The evaluation on realworld datasets might not capture all possible scenarios and could be further expanded for a more comprehensive analysis.  The generalizability of CoreFed across different types of datasets and models could be a potential limitation that needs further investigation.", "mjVmifxpKqS": " Peer Review of the Paper  1) Summary: The paper presents a study on multibatch reinforcement learning addressing the explorationexploitation tradeoff using a multibatch framework with a constraint on the number of batches. They propose an algorithm that achieves nearoptimal regret bounds and batch complexity. The key contributions include a design scheme for exploration an algorithm for efficient exploration and batch learning for distributed deployment. The study provides theoretical analysis and practical implications for efficient policy learning.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important problem in reinforcement learning by introducing a multibatch framework for efficient exploration.  The proposed algorithm shows nearoptimal regret bounds and batch complexity demonstrating significant technical contributions.  The theoretical analysis coupled with practical implications enhances the credibility and applicability of the study. Weaknesses:  The paper lacks empirical validation or experimentation to validate the proposed algorithms efficiency and performance under practical scenarios.  Some sections of the paper especially in the technical details could be challenging for readers without a strong background in reinforcement learning theory.  The impact and scalability of the proposed algorithm to realworld applications require more exploration and discussion.  3) Questions:  How does the proposed algorithm perform compared to existing RL algorithms in terms of computational efficiency and scalability especially in largescale applications  Can the algorithm handle more complex environments with higher state and action spaces and how does it generalize to those scenarios  What are the practical implications of the study and how can the proposed framework be applied in realworld applications beyond theoretical analysis  4) Limitations:  The paper lacks empirical evaluation which is crucial to demonstrate the effectiveness of the proposed algorithm in practical scenarios.  The theoretical focus might limit the papers accessibility and practical relevance for researchers or practitioners interested in applied reinforcement learning.  The discussion on broader impact could be further elaborated to highlight the potential applications and implications of the research. Overall the paper presents a comprehensive study on multibatch reinforcement learning with significant theoretical contributions. However incorporating empirical validation and expanding the discussion on practical applications could enhance the papers impact and relevance in the field of reinforcement learning.", "xnuN2vGmZA0": "Summary The paper introduces VITA a novel paradigm for offline Video Instance Segmentation based on objectoriented information. VITA uses object tokens processed by a Transformerbased image instance segmentation model to achieve videolevel understanding without referencing spatiotemporal backbone features. VITA outperforms existing methods on VIS benchmarks handling long and highresolution videos efficiently. It shows practical benefits such as processing long videos on a single GPU and training on a frozen image object detector. Strengths and Weaknesses Strengths: 1. VITA achieves stateoftheart performance on VIS benchmarks surpassing existing offline methods. 2. The approach handles long and highresolution videos efficiently showing practical advantages. 3. By using object tokens and a unique architecture VITA reduces the need for dense spatiotemporal features improving inference speed and memory efficiency. 4. VITA can be trained on a frozen image object detector making it applicable to scenarios where separate models are not viable. Weaknesses: 1. The paper does not extensively discuss the challenges or limitations faced during the development of VITA. 2. While VITA performs well on benchmarks its performance in realworld scenarios with diverse and complex video data remains to be seen. 3. The paper lacks a comprehensive comparison with online methods in terms of performance and efficiency. 4. The exact mechanism of object token aggregation and impact on different video domains is not deeply explored. Questions 1. How does VITA handle occlusions and complex object interactions especially in scenarios with multiple objects and overlapping instances 2. Can VITA adapt to realtime processing requirements or is it mainly focused on offline video instance segmentation tasks 3. What strategies are employed to ensure VITAs scalability and efficiency when processing extremely long video sequences 4. How sensitive is VITAs performance to variations in the number of object tokens or window sizes in the Object Encoder Limitations 1. While VITA has demonstrated high performance on benchmarks its effectiveness in handling complex and dynamic scenes that span over very long sequences could be a limitation. 2. The reliance on object tokens without explicit temporal information utilization may restrict VITAs ability to capture intricate temporal dynamics in videos. In conclusion the paper presents a promising approach in the field of offline Video Instance Segmentation with significant performance improvements and practical benefits. However further investigation is needed to assess its adaptability to diverse video scenarios and potential challenges in processing extremely long video sequences.", "xNeAhc2CNAl": "Peer Review 1) Summary: The paper presents a comprehensive investigation into extreme compression methods particularly focusing on ultralow bit precision quantization for NLP models. The study identifies the undertraining of previous baselines and proposes a novel compression pipeline named XTC. Key findings include the effectiveness of skipping pretraining knowledge distillation and achieving better performance with extreme quantization plus layer reduction. The experimental results highlight significant improvements in model compression and accuracy surpassing previous stateoftheart methods. 2) Strengths and Weaknesses: Strengths:  The paper provides a detailed analysis of various hyperparameters and training strategies in extreme quantization methods.  The introduction of XTC as a simpler yet effective compression pipeline showcases significant improvements in model compression and accuracy.  The systematic study and extensive experiments offer valuable insights into the effectiveness of different optimization strategies.  The comparison with existing methods and demonstration of new stateoftheart results on GLUE tasks enhance the credibility and impact of the proposed method. Weaknesses:  The paper lacks clarity in some sections particularly in explaining the methodologies and results. Some parts require detailed explanations for better understanding.  The study could benefit from more thorough analysis and discussion of the limitations and potential challenges of the proposed method.  While the experimental results are promising additional comparison with a broader range of existing methods and datasets could provide a more holistic evaluation of the proposed approach.  The paper would benefit from a more concise and focused discussion to enhance readability and clarity of the contributions. 3) Questions:  Could the authors elaborate on the specific criteria used for selecting hyperparameters and training strategies for the comparative study  What are the key insights gained from the observed performance differences between binarized and ternarized networks in the extreme quantization process  How does the proposed XTC method address the challenges related to model instability and optimization issues encountered in extreme compression 4) Limitations:  The paper focuses primarily on BERTbase models potentially limiting the generalizability of the findings to other transformer models. Future work could explore the application of the proposed method across a wider range of models.  The study lacks indepth exploration of the impact of extreme compression on tasks beyond GLUE which could provide a more comprehensive evaluation of the methods effectiveness.  The complexity and computational cost of certain optimization strategies in extreme quantization methods could hinder practical applicability warranting further investigation into more efficient techniques.  The paper would benefit from a more comprehensive discussion on the tradeoffs between extreme compression and model performance to provide a clearer understanding of the limitations of the proposed approach. Overall the paper presents a significant contribution to the field of extreme compression for NLP models offering a novel compression pipeline and achieving stateoftheart results on GLUE tasks. Addressing the identified weaknesses and limitations could further enhance the impact and relevance of the proposed method in practical applications.", "xijYyYFlRIf": "Peer Review of \"GAUDI: Generative Model for 3D Scene Generation\" Summary: The paper introduces GAUDI a generative model for capturing complex 3D scenes and rendering them immersively. GAUDI disentangles radiance fields and camera poses enabling unconditional and conditional generation of 3D scenes. The model scales to thousands of indoor scenes across datasets and achieves stateoftheart performance. The approach addresses challenges faced by existing generative models and shows promise for various machine learning and computer vision applications. Strengths: 1. The paper addresses an important challenge in generative modeling for 3D scenes extending the scope beyond single objects to complex scenes. 2. GAUDIs disentangled representation of radiance fields and camera poses is a novel and scalable approach that allows for unconditional and conditional generation. 3. The model demonstrates stateoftheart performance across multiple datasets emphasizing its effectiveness in generating diverse and realistic 3D scenes. 4. The paper includes comprehensive experiments on various datasets evaluating reconstruction quality generative tasks and conditional modeling showcasing the versatility and effectiveness of GAUDI. Weaknesses: 1. The paper lacks comparative analysis with more recent approaches in 3D scene generation such as those utilizing deep reinforcement learning or graph neural networks. 2. While the performance metrics are provided a qualitative analysis of generated scenes compared to ground truth could enhance the evaluation. 3. The technical details and optimization process may be challenging for readers less familiar with advanced generative modeling techniques potentially hindering understanding. 4. The paper could benefit from a more detailed discussion on the implications of mode collapse and how GAUDI mitigates such issues. Questions: 1. How does GAUDI handle challenging datasets with varying complexities and scene structures and does its performance degrade with increased dataset complexity 2. Could the disentangled representation of radiance fields and camera poses in GAUDI lead to interpretability of generated scenes or aid in downstream visual tasks like object recognition 3. How robust is GAUDI to perturbations in camera poses or ambient lighting conditions that might affect the scene generation process 4. Given the emphasis on conditional inference how does GAUDI handle cases where the conditioning variables are ambiguous or noisy potentially leading to deviations in scene generation Limitations: 1. The papers discussion on the scalability and performance of GAUDI could be further supported by insights into model interpretability and generalization across diverse 3D scene datasets. 2. The complexity of the optimization process and architecture design might limit the practical applicability of GAUDI in realworld scenarios without extensive computational resources. 3. As with many generative models the paper could explore potential ethical considerations or biases that may arise in using GAUDI for applications like content creation or reinforcement learning. Overall the paper presents a compelling generative model for 3D scene generation with notable strengths in scalability performance and versatility. Addressing the identified weaknesses and questions could further enhance the impact and understanding of GAUDI in the field of generative modeling and computer vision.", "lzZstLVGVGW": " Peer Review  Summary: The paper proposes Earthformer a spacetime Transformer for Earth system forecasting based on a generic efficient building block called Cuboid Attention. The research explores the design and effectiveness of spacetime attention using Earth observation data and demonstrates its performance on synthetic datasets (MovingMNIST and Nbody MNIST) as well as realworld benchmarks (SEVIR precipitation nowcasting and ICARENSO sea surface temperature anomalies forecasting). The experiments show that Earthformer achieves stateoftheart performance in various forecasting tasks.  Strengths: 1. Novel Approach: The introduction of Earthformer utilizing Cuboid Attention is a novel and promising approach for Earth system forecasting. 2. Experimental Rigor: The paper conducts extensive experiments on synthetic and realworld datasets demonstrating the effectiveness and performance of the proposed model. 3. Comparative Analysis: The comparison with existing DL models including UNet ConvLSTM PredRNN PhyDNet E3DLSTM and Rainformer provides comprehensive insights into Earthformers superiority. 4. Hierarchical Architecture: The hierarchical encoderdecoder architecture stacked cuboid attention layers and the inclusion of global vectors are innovative design choices that contribute to the models performance.  Weaknesses: 1. Limited Evaluation Metrics: While the paper evaluates the model using MSE and Critical Success Index (CSI) on SEVIR and correlation skill on ICARENSO additional metrics or comparisons with other stateoftheart models could strengthen the evaluation. 2. Uncertainty Modeling: The paper acknowledges the lack of uncertainty modeling in Earthformer which is an essential aspect of forecasting in chaotic systems. Incorporating probabilistic forecasting capabilities can enhance the models reliability. 3. DataDriven Approach: The paper mentions that Earthformer is purely datadriven and does not incorporate physical knowledge of the Earth system. Integrating physical constraints or ensembling with physicsbased models could enhance the models interpretability and generalization.  Questions: 1. Generalization: How does Earthformer perform on datasets with variations in spatial and temporal resolutions and how sensitive is its performance to changes in data characteristics 2. Robustness: How robust is Earthformer to noisy or incomplete Earth observation data and are there strategies to mitigate these challenges in realworld scenarios 3. Scalability: Given the computational complexity of Transformers how scalable is Earthformer for larger datasets or increased spatialtemporal dimensions in Earth system forecasting tasks 4. Interpretability: How interpretable are the predictions generated by Earthformer especially in complex scenarios like extreme weather events or abrupt system changes  Limitations: 1. Deterministic Model: Earthformer lacks uncertainty modeling which may limit its ability to provide probabilistic forecasts or capture nuanced variations in Earth system dynamics. 2. DataDriven Approach: The model relies solely on datadriven methods without incorporating domainspecific physical knowledge potentially limiting its adaptability to evolving Earth system dynamics. 3. Computational Complexity: The computational demands of the proposed model especially in highdimensional Earth observation data might pose challenges in deployment and realtime forecasting scenarios. Overall the paper presents a novel and promising approach in Earth system forecasting with Earthformer and Cuboid Attention. Addressing uncertainties incorporating physical constraints and enhancing interpretability could further strengthen the models applicability in realworld scenarios. The comprehensive experiments and comparative analyses provide valuable insights into the models effectiveness and potential improvements.", "tX_dIvk4j-s": "Peer Review: 1) Summary: The paper introduces \"VisCo Grids\" a gridbased surface reconstruction method that aims to overcome the limitations associated with Implicit Neural Representations (INRs). By utilizing simple grid functions and introducing two novel geometric priors  Viscosity and Coarea the authors demonstrate comparable results to the best performing INRs in terms of accuracy instant inference and improved training times. The methodology involves optimizing the loss function with these novel priors to reconstruct 3D surfaces from sparse point clouds. Experimental results show promising outcomes on standard reconstruction datasets and ablation studies validate the effectiveness of the proposed priors. 2) Strengths and weaknesses: The strengths of this paper lie in its innovative approach towards surface reconstruction departing from complex neural network architectures to gridbased representations with geometric priors. The methodology is welldescribed and the experiments are thorough demonstrating the efficacy of VisCo Grids compared to existing INR methods. The incorporation of Viscosity and Coarea priors to regulate the smoothness and area of reconstructed surfaces is a significant contribution. The ablation studies provide insightful analyses of each components impact on reconstruction quality. However the paper could benefit from providing more details on the implementation of the proposed method particularly in terms of parameter tuning and convergence criteria. Additionally while the comparisons with existing methods are informative a more comprehensive analysis of the tradeoffs between VisCo Grids and INRs in terms of memory and computational requirements would strengthen the paper. 3) Questions: 1. How does the proposed method perform when dealing with noisy or incomplete input point clouds 2. Are there scalability issues when applying VisCo Grids to larger and more complex datasets 3. Can the authors elaborate on the computational efficiency of VisCo Grids compared to existing INR methods in terms of inference speed and memory usage 4) Limitations: One limitation of the paper is the lack of a detailed discussion on the scalability and computational efficiency of VisCo Grids when handling larger or more complex datasets. Additionally the reliance on defining the degrees of freedom a priori in gridbased methods compared to the flexibility of neural networks could limit the adaptability of VisCo Grids in certain scenarios. Overall the paper presents a novel approach to surface reconstruction through gridbased representations with geometric priors offering an alternative to traditional INRs with promising results. The methodology is wellsupported by experiments and thorough analyses showcasing the effectiveness of the VisCo Grids method in 3D surface reconstruction tasks. Further exploration of the limitations and potential improvements could enhance the impact and applicability of the proposed approach in realworld scenarios.", "oDRQGo8I7P": " Peer Review 1) Summary Scorebased generative models (SGMs) have shown remarkable empirical performance by modeling generative processes as denoising processes using diffusions. However existing SGMs are limited to Euclidean spaces neglecting Riemannian manifolds present in various scientific domains. This paper introduces Riemannian Scorebased Generative Models (RSGMs) extending SGMs to Riemannian manifolds. The proposed method is demonstrated on various manifolds including earth and climate science spherical data showcasing improved performance over existing baselines. 2) Strengths:  The paper creatively addresses the limitation of SGMs by introducing RSGMs for Riemannian manifolds broadening the applicability of generative models.  The theoretical analysis convergence bounds and empirical results validate the effectiveness of RSGMs on various manifolds.  Methodological extensions like Geodesic Random Walks for sampling and score approximation on manifolds add depth to the proposed approach.  The experiments on earth and climate science datasets synthetic tori data SO3(R) and hyperbolic space exhibit the robustness and scalability of RSGMs compared to alternative models. Weaknesses:  The paper lacks a detailed comparison with existing methods on Riemannian manifolds limiting the context for evaluating the novelty and superiority of RSGMs.  The explanation of RSGM algorithm score approximation methods and convergence results could be made more accessible for readers unfamiliar with the specific terminology and techniques of manifoldbased generative modeling.  The description of experiments could be enhanced by providing more clarity on datasets used evaluation metrics and significance of results. 4) Questions:  Can the proposed RSGMs be applied to realworld datasets in addition to synthetic data to demonstrate practical utility  How robust are RSGMs to hyperparameters tuning and what strategies are employed to optimize the model performance on complex datasets  Could further insights be provided on the computational complexity of RSGMs compared to existing methods especially in scenarios with largescale highdimensional datasets  Are there any plans to make the implementation or source code of RSGMs available to the research community to promote reproducibility and further research in this domain 5) Limitations:  The paper could benefit from more extensive validation on diverse realworld datasets to confirm the generalizability and effectiveness of RSGMs.  An indepth comparison with stateoftheart manifoldbased generative models would enhance the understanding of where RSGMs stand in the current research landscape.  The theoretical analysis could be expanded to include more indepth discussions on potential limitations and challenges faced while implementing RSGMs in practical scenarios. Overall the paper introduces an innovative approach of RSGMs for generative modeling on Riemannian manifolds demonstrating promising results through theoretical analysis and empirical validation. Further experimental validation and comparison with existing methods will be beneficial for a more comprehensive evaluation of the proposed RSGMs.", "zz0FC7qBpkh": "Peer Review of the Scientific Paper: \"An Analysis and Comparison of Invariant Risk Minimization (IRM) and Mirror Reflected IRM (MRI) Formulations\" Summary: The paper presents a comprehensive analysis and comparison of Invariant Risk Minimization (IRM) and Mirror Reflected IRM (MRI) formulations for achieving invariance in machine learning models. The study identifies a fundamental design flaw in the IRM formulation and proposes the MRI formulation as a complementary and robust alternative. The authors introduce MRIv1 as a practical version of MRI and demonstrate its effectiveness in achieving invariant predictors particularly in nonlinear imagebased problems. They provide theoretical underpinnings as well as empirical results to validate the superior performance of MRIv1 over IRMv1. Strengths: 1. Novel Contribution: The paper addresses an important issue in machine learning regarding the generalization of models to outofdistribution data by proposing a new formulation (MRI) to achieve invariance without the limitations observed in IRM. 2. Comprehensive Analysis: The study includes theoretical proofs experimental demonstrations and comparison with existing methods to establish the effectiveness of MRIv1 in achieving nearoptimal OOD generalization. 3. Empirical Results: The paper provides empirical evidence supporting the superiority of MRIv1 over IRMv1 in imagebased nonlinear problems demonstrating its practical relevance and performance. 4. Clarity and Structure: The paper is wellstructured with clear delineation of concepts formulations and results. The explanation of methodologies and findings is detailed and accessible. Weaknesses: 1. Ambiguity in Constraints: The paper could benefit from clarifying the constraints used in the formulations (IRMv1 and MRIv1) especially in the context of practical implementation for realworld applications. 2. Scope of Applicability: The study predominantly focuses on linear problems and imagebased datasets. Further exploration of the efficacy of MRI in diverse domains or applications could enhance the generalizability of the findings. Questions: 1. What are the implications of the proposed MRI formulation in realworld scenarios beyond imagebased problems How scalable and adaptable is the MRI approach to different types of datasets and models 2. Can the authors elaborate on the potential computational overhead or complexity associated with implementing MRIv1 in comparison to traditional methods like IRMv1 How practical is MRI in largescale machine learning applications Limitations: 1. Domain Specificity: The study primarily focuses on imagebased nonlinear problems potentially limiting the generalizability of the results to other domains or types of datasets. 2. Theoretical Emphasis: While the theoretical proofs offer valuable insights the practical implementation and realworld performance of MRIv1 across a broader range of machine learning applications warrant further investigation. In conclusion the paper presents a significant advancement in addressing invariance in machine learning models through the introduction of MRI formulation. While the study demonstrates promising results further exploration of the practical implications and scalability of MRI in diverse applications would enhance the utility and impact of this research. The authors contributions in exposing the limitations of IRM and proposing a more robust alternative in the form of MRI are commendable and pave the way for future research in achieving effective OOD generalization.", "m_JSC3r9td7": " Peer Review of \"DeepMed: Deep Neural Networks for Semiparametric Mediation Analysis\"  Summary: The paper introduces DeepMed a novel method for semiparametric mediation analysis using deep neural networks (DNNs) to estimate Natural Direct and Indirect Effects. The authors provide theoretical results demonstrating that DeepMed can achieve semiparametric efficiency without the need for sparse DNN architectures. Extensive synthetic experiments support the findings showcasing improved performance over other methods in various scenarios. Real data analysis on fairness also aligns with prior findings. Overall the paper advances DNNbased causal inference addressing limitations in current literature.  Strengths: 1. Novel Methodology: DeepMed introduces a new approach for mediation analysis using DNNs addressing limitations in existing literature. 2. Theoretical Advances: Theoretical results show that DeepMed can achieve semiparametric efficiency without sparse DNN architectures facilitating practical applications. 3. Extensive Experiments: Comprehensive synthetic experiments provide a strong empirical foundation demonstrating the effectiveness of DeepMed in various scenarios. 4. Real Data Analysis: Application to real data on fairness reinforces the applicability of DeepMed and aligns with prior research. 5. UserFriendly Implementation: Providing an R package enhances reproducibility and usability for practitioners.  Weaknesses: 1. Algorithmic Considerations: The paper lacks detailed discussions on the training process and algorithmic aspects of DeepMed. Incorporating insights into the training process could enhance understanding. 2. Empirical Performance Discrepancies: Remark 10 highlights discrepancies between theoretical guarantees and empirical observations in certain scenarios. Further investigation and mitigation strategies could strengthen the methods reliability.  Questions: 1. Implicit Regularization Concerns: Remark 10 raises the issue of implicit regularization potentially impacting the curse of implicit biases in causal effect estimates. Could this be further investigated or mitigated in future research 2. Handling Unmeasured Confounding: How does DeepMed handle issues of unmeasured confounding in realworld data analysis and are there plans to address this in future developments 3. Complex PathSpecific Effects: Are there plans to extend DeepMed to handle more complex pathspecific effects and interactions in causal pathways 4. Hyperparameter Tuning Strategies: How does the choice of hyperparameters impact DeepMeds performance and are there alternative strategies beyond crossvalidation that could be explored  Limitations: 1. Algorithmic Transparency: Lack of detailed algorithmic considerations limits a comprehensive understanding of the training process and potential biases introduced. 2. Synthetic Experiment Complexity: The use of highly complex nuisance functions in synthetic experiments may limit the generalizability and practical implications of the findings. 3. Real Data Generalizability: While the real data analysis supports prior findings further applications across diverse datasets may be necessary to establish robustness. In conclusion the paper presents a significant contribution to causal inference using deep neural networks. Addressing the identified weaknesses and questions through future research can enhance the methods effectiveness and applicability in realworld settings. Further studies focusing on algorithmic aspects robustness evaluations and broader applications would strengthen the impact of DeepMed in the field of causal mediation analysis.", "wiGXs_kS_X": " Peer Review  1) Summary The paper introduces Logical Credal Networks (LCNs) a probabilistic logic model that allows probability and conditional probability bounds on arbitrary propositional and firstorder logic formulas without restrictive conditions. LCNs are evaluated through experiments on random networks Mastermind games with uncertainty and credit card fraud detection showing promising results by outperforming existing approaches in aggregating multiple sources of imprecise information.  2) Strengths  The paper presents a novel approach LCNs that allows for flexible handling of imprecise information in propositional and firstorder logic formulas without requiring acyclicity.  The introduction of a generalized Markov condition enables the modeling of implicit independence relations between logical atoms similar to those in probabilistic databases.  The evaluation on benchmark problems demonstrates the superior performance of LCNs in aggregating multiple sources of imprecise information compared to existing methods.  3) Weaknesses  While the paper highlights the advantages of LCNs in aggregating imprecise information a more detailed comparison with existing methods could enhance the understanding of their relative strengths and weaknesses.  The complexity of exact inference in LCNs is mentioned but further discussion on potential scalability issues and strategies for addressing them would be beneficial.  The application scenarios considered in the experiments are limited to random networks Mastermind games and credit card fraud detection. Exploring a broader range of realworld applications would provide a more comprehensive assessment of LCNs effectiveness.  4) Questions  How do LCNs handle scalability issues in exact inference especially when the number of interpretations grows significantly  Can the performance of LCNs be further optimized for faster runtime in realworld applications considering the computational complexity mentioned in the paper  Are there any plans to extend the evaluation of LCNs to additional application domains beyond the ones explored in the paper to further validate their flexibility and effectiveness  5) Limitations  The paper primarily focuses on the theoretical aspects and experimental evaluations of LCNs on specific benchmark problems limiting the exploration of their applicability across diverse domains.  The discussion on the limitations and potential challenges faced by LCNs such as scalability runtime optimization and algorithmic improvements could be more detailed to provide a comprehensive understanding of the practical implications. Overall the paper presents a novel probabilistic logic model that shows promise in handling imprecise information and aggregating multiple sources of knowledge. Further investigation into scalability optimization strategies and broader application scenarios could contribute to a more comprehensive assessment of Logical Credal Networks.", "x56v-UN7BjD": " Peer Review  1) Summary: The paper investigates privacypreserving nonconvex Empirical Risk Minimization (ERM) algorithms by introducing two new algorithms based on Normalized SGD with momentum utilizing treeaggregation techniques. The first algorithm achieves an asymptotic bound of O\\xcc\\x83 ( d14\\xe2\\x88\\x9a N ) and overcomes challenges of large batch sizes and privacy amplification requirements. The second algorithm further reduces sensitivity achieving a utility bound of O\\xcc\\x83 ( d13 ( N)23 ) which is the bestknown rate for private nonconvex ERM.  2) Strengths and Weaknesses:  Strengths:  The paper addresses a significant problem in machine learning by developing privacypreserving algorithms for nonconvex ERM.  The use of Normalized SGD with momentum and treeaggregation techniques is innovative and provides a new approach to privacypreserving optimization.  The theoretical analysis and guarantees for privacy and utility bounds in both algorithms are rigorously presented.  Weaknesses:  The paper could benefit from practical implementations or empirical validation to demonstrate the effectiveness of the proposed algorithms in realworld scenarios.  While the theoretical analysis is detailed some explanations could be clearer to help readers understand the algorithms and their implications more easily.  3) Questions:  How do the proposed algorithms in the paper compare to existing stateoftheart privacypreserving nonconvex ERM algorithms in terms of performance and efficiency  Are there specific scenarios or datasets where the proposed algorithms are particularly wellsuited and are there any limitations in terms of scalability to larger datasets  4) Limitations: The limitations section in the paper acknowledges certain areas for improvement such as the potential incorporation of privacy amplification by shuffling for further performance enhancement and reducing the runtime of the second algorithm for more efficient processing.  Overall Assessment: The paper provides a valuable contribution to the field of privacypreserving machine learning particularly in the context of nonconvex ERM. The theoretical developments in the proposed algorithms along with the sensitivity reduction techniques offer promising avenues for enhancing privacy while maintaining utility in optimization tasks. Further empirical evaluation and exploration of the limitations highlighted for future research could strengthen the practical relevance and impact of the presented work.", "mE1QoOe5juz": "Peer Review: 1) Summary The paper proposes a new learning framework for reinforcement learning in userinteraction applications with a tiered structure. The framework involves maintaining two policies one for risktolerant users and one for riskaverse users and investigates the advantages of separating policies for different user groups. The key contributions include theoretical analyses on regret minimization and benefits of leveraging the tiered structure. 2) Strengths and Weaknesses Strengths:  The paper addresses an interesting and practical problem of tiered user interactions in reinforcement learning.  The theoretical analyses and technical results presented are welldeveloped and thorough.  The proposed algorithm framework and separation of policies demonstrate potential benefits from the tiered structure. Weaknesses:  The paper lacks empirical validation and practical experiments to support the theoretical findings.  Some assumptions made such as identical dynamics and rewards for different user groups could limit the generalizability of the results.  The discussion on the practical applicability and realworld implementation of the proposed framework could be further elaborated. 3) Questions  How sensitive are the proposed results to deviations from assumptions such as identical dynamics and rewards for different user groups  Has the proposed tiered RL framework been tested on realworld datasets or simulations to demonstrate its effectiveness  What are the potential challenges in implementing the suggested tiered structure in practical applications and how can they be addressed 4) Limitations  The lack of empirical validation limits the practical applicability and generalizability of the proposed tiered RL framework.  The assumption of identical dynamics and rewards may not hold in realworld scenarios affecting the transferability of the findings to practical applications.  The paper focuses on a binary tiered structure extending the framework to multiple tiers could provide additional insights but is not explored. Overall the paper offers valuable theoretical insights into tiered user interactions in reinforcement learning. Addressing the limitations and uncertainties through empirical validation and realworld applications could strengthen the impact and relevance of the proposed framework in practical settings.", "tjFaqsSK2I3": "Peer Review Summary: The paper proposes a unified approach to address multiple core computer vision tasks by formulating them in a shared pixeltosequence interface. By converting task inputs and outputs into sequences of discrete tokens the model can be trained with a single architecture and objective function eliminating the need for taskspecific customization. Through experiments on the COCO dataset the model achieves competitive performance on tasks like object detection instance segmentation keypoint detection and image captioning. The approach demonstrates promising results in unifying vision tasks under a common framework. Strengths: 1. Novel Approach: The paper introduces a novel method to unify core computer vision tasks through a shared pixeltosequence interface showcasing the potential for crossdomain task sharing. 2. Experimental Validation: The approach is rigorously evaluated on the COCO dataset demonstrating competitive performance across diverse tasks without the need for specialized architectures or loss functions. 3. Unified Interface: By treating various vision tasks as sequences of tokens the model simplifies multitask learning enabling efficient scalability and adaptation to new tasks. 4. Detailed Experiments: The paper provides detailed experimental setups training strategies and results across tasks offering insights into the models performance and generalization capabilities. Weaknesses: 1. Lack of Comparison Metrics: While the paper compares the proposed approach with specialized baselines it could benefit from additional quantitative metrics or benchmarks to further evaluate the models performance and generalizability. 2. Inference Speed: The potential limitations in inference speed due to autoregressive modeling could be elaborated upon along with considerations for efficiency improvements. 3. Extension to More Tasks: The generalizability of the approach to additional vision tasks beyond the four considered in the study is briefly mentioned but not extensively explored. Further investigations into the scalability of the model to a broader range of tasks could enhance the papers impact. Questions: 1. Could the paper elaborate on the scalability of the proposed approach to encompass a wider variety of vision tasks beyond the four core tasks evaluated in the study 2. How does the model handle complexity and scalability when incorporating larger datasets for pretraining or using larger model sizes especially with regards to training efficiency 3. What are the computational requirements and constraints associated with the proposed unified interface approach particularly in scenarios involving tasks with longer sequences or higherdimensional outputs Limitations: 1. Experimental Scope: The limitations of the study primarily stem from the focused experimental scope on a restricted set of tasks and datasets potentially limiting the generalizability of the proposed approach. 2. Inference Efficiency: The autoregressive nature of the model may pose challenges in optimizing inference speed for tasks with longer sequences suggesting a need for further optimization strategies. 3. Architectural Constraints: As the approach diverges from conventional taskspecific models there could be architectural limitations or complexities that require further exploration to enhance model performance. Overall the paper presents an innovative solution for unifying vision tasks under a shared interface with promising results on foundational tasks. Addressing the outlined weaknesses and questions could strengthen the papers contribution to the field of computer vision and artificial intelligence research.", "qwjrO7Rewqy": " Peer Review  Summary The paper proposes a novel graph neural network approach to efficiently estimate extended persistence diagrams (EPDs) on graphs by leveraging algorithmic insights. By transforming the EPD computation into an edgewise prediction problem and designing a graph neural network architecture to approximate the UnionFind algorithm the proposed model demonstrates promising results in approximating EPDs and downstream graph representation learning tasks. The method proves to be efficient on large and dense graphs providing a faster alternative to direct EPD computation.  Strengths and Weaknesses  Strengths: 1. Innovative Approach: The paper introduces a novel learning framework guided by insights into EPD computation offering a fresh perspective on approximating EPDs efficiently. 2. Technical Contributions: The model redefines the EPD computation as an edgewise prediction problem leading to better supervision and representation learning efficiency. 3. Experimental Validation: The empirical results demonstrate the effectiveness of the proposed method in approximating EPDs and its utility in downstream graph tasks. 4. Efficiency Improvement: The model significantly accelerates EPD computation on large and dense graphs addressing a critical bottleneck in graph learning pipelines.  Weaknesses: 1. Theoretical Explanation: While the paper presents the model architecture and experimental results a more comprehensive theoretical analysis of the frameworks principles and algorithms would enhance the understanding of the method. 2. Comparative Analysis: While the proposed method is compared against baselines a more extensive comparison with a wider range of related methods could provide additional insights into the models performance. 3. Scalability: The scalability of the proposed method to even larger graphs or more complex structures could be explored further to assess its applicability in diverse settings.  Questions 1. Can you elaborate on the limitations encountered during the practical implementation of the proposed model on diverse datasets and graph structures 2. How does the proposed method generalize to realworld graphs with varying topological complexities and sizes 3. Have you considered the interpretability of the models predictions especially in relation to the underlying topological features captured by EPDs  Limitations 1. Algorithmic Complexity: The paper could delve deeper into the algorithmic complexity of the proposed model and its implications on scalability for even larger graphs. 2. Interpretability: The interpretability of the model outputs in terms of understanding the reasoning behind specific EPD predictions could be further discussed. 3. Generalization: While the model shows promise in transferring to unseen graphs a more detailed analysis of its generalization capacity across different graph types would be valuable. Overall the paper presents a significant advancement in approximating EPDs using a neural network approach providing a more efficient solution for graph representation learning tasks. Further exploration of the methods scalability interpretability and generalization capabilities could enhance its applicability in diverse graph analysis scenarios.", "pkzwYftNcqY": "Peer Review 1) Summary (avg word length 100) The paper introduces efficient nonparametric tests based on Maximum Mean Discrepancy (MMD) Hilbert Schmidt Independence Criterion (HSIC) and Kernel Stein Discrepancy (KSD) using incomplete Ustatistics. The proposed tests provide a computationally efficient solution to the kernel selection problem aggregating over multiple kernel bandwidths. The work includes theoretical analyses deriving quantile bounds for wild bootstrapped Ustatistics proving uniform separation rates and demonstrating the tradeoff between computational efficiency and achievable rates through experiments. Notably the lineartime versions of the proposed tests perform well compared to stateoftheart tests in various testing frameworks. 2) Strengths and Weaknesses Strengths:  Introduces computationally efficient nonparametric tests based on incomplete Ustatistics and kernel aggregation.  Theoretical analyses provide quantile bounds and uniform separation rates.  Employs both theoretical investigations and numerical experiments to demonstrate effectiveness.  Shows superior performance of the proposed tests even in linear time compared to existing stateoftheart tests. Weaknesses:  Complexity and technical depth may pose challenges for readers not wellversed in the field.  Extensive theoretical derivations and analyses may be difficult for nonspecialists to grasp.  Theoretical contributions are highlighted more emphasis on practical implications and realworld applications could be beneficial.  Limited discussion on potential practical limitations or challenges in implementing the proposed tests. 3) Questions  Can the proposed approach be generalized to other types of data than the ones considered in the experiments  How sensitive is the performance of the tests to the choice of design size and the number of bootstraps in the experiments  Are there scenarios where the lineartime versions of the tests may underperform compared to quadratictime tests  Is there a plan to extend the research to address potential limitations in practical applications or incorporate additional realworld datasets 4) Limitations  The paper predominantly focuses on theoretical derivations and numerical experiments greater emphasis on practical implications and generalizability could enhance the impact.  The assumption of the superiority of the proposed tests in various scenarios should be further substantiated with more extensive experiments and comparisons. Overall the paper presents a significant contribution to nonparametric testing methods with a thorough theoretical foundation and promising empirical results. The authors are encouraged to address potential limitations and expand discussions on practical implications to further enhance the impact of their research.", "v6NNlubbSQ": "Peer Review 1) Summary: The paper introduces a novel method called Nonparametric Uncertainty Quantification (NUQ) for uncertainty quantification in machine learning models. The method aims to disentangle aleatoric and epistemic uncertainties by leveraging the NadarayaWatson estimator of the conditional label distribution in feature space. The proposed approach is showcased through experiments on various datasets like MNIST SVHN CIFAR100 and ImageNet demonstrating strong performance in uncertainty estimation tasks. Additionally the method is compared favorably to existing uncertainty estimation techniques. 2) Strengths and weaknesses: Strengths:  The paper addresses a crucial aspect of machine learning by proposing a method for quantifying uncertainty in predictions effectively.  The approach is theoretically grounded making it potentially reliable for practical applications.  The disentanglement of aleatoric and epistemic uncertainties enhances the interpretability of uncertainty estimates.  The method shows robust performance across image and text classification tasks.  The experiments are thorough and cover a wide range of datasets providing comprehensive insights into the methods efficacy. Weaknesses:  While the method shows promise the paper could provide more details on the computational complexity and scalability of the NUQ method especially for large datasets.  The discussion on the limitations and future work could be more detailed to guide further research in the area.  The paper lacks comparison with more recent stateoftheart methods in the field of uncertainty quantification which could help position NUQ in relation to cuttingedge approaches. 3) Questions:  Can the proposed method be generalized to other types of machine learning models beyond neural networks  How sensitive is the NUQ method to changes in hyperparameters and are there any guidelines for hyperparameter tuning  Are there any specific domain applications where NUQ would particularly excel compared to other uncertainty quantification techniques 4) Limitations:  The paper mainly focuses on the technical aspects of the NUQ method without diving deeply into realworld applications or case studies that could provide more context for understanding its practical implications.  The evaluation of the method is based on experiments with specific datasets insights into the generalizability of the approach to diverse application domains could be beneficial. Overall the paper introduces an innovative method for uncertainty quantification in machine learning models and showcases its performance through extensive experiments. The methods ability to disentangle different types of uncertainties and provide efficient uncertainty estimates makes it a valuable contribution to the field. To enhance the papers impact addressing the mentioned weaknesses and providing more context on applications and comparisons with stateoftheart methods would be beneficial.", "w6tBOjPCrIO": "Peer Review: 1) Summary: The paper introduces a novel Modelbased Counterfactual Data Augmentation (MOCODA) framework to address the challenge of generalization in reinforcement learning (RL) tasks with limited empirical training data. The key idea is to leverage a locally factored dynamics model to predict and generate outofdistribution transition data for RL agents. The empirical results demonstrate the effectiveness of MOCODA in enabling agents to learn policies that generalize to unseen states and actions even in outofdistribution scenarios. 2) Strengths and weaknesses: Strengths:  The paper addresses an important challenge in RL  generalization to unseen states and actions in complex domains with limited empirical data.  Proposes a novel Modelbased Counterfactual Data Augmentation (MOCODA) framework leveraging a locally factored dynamics model for data augmentation.  The experiments on 2D Navigation and HookSweep2 environments demonstrate the effectiveness of MOCODA in improving the generalization performance of RL agents.  The paper is wellstructured with clear explanations of key concepts and the proposed framework. Weaknesses:  Some parts of the paper could benefit from more concrete examples or illustrations to aid in understanding complex concepts such as local factorization and dynamics model architecture.  The discussion on limitations and future work could be expanded to provide a more comprehensive view of potential challenges and areas for improvement.  It would be helpful to include comparisons with existing stateoftheart methods or frameworks in RL to establish the novelty and superiority of MOCODA more clearly. 3) Questions:  Could the authors provide more details or examples to help readers understand how the locally factored dynamics model functions in generating and predicting outofdistribution transitions  How sensitive is the MOCODA framework to the choice of hyperparameters or model configurations and how were these parameters selected in the experiments  Are there any considerations or challenges related to the scalability of MOCODA to larger and more complex environments or tasks 4) Limitations:  The experiments are conducted on specific controlled environments it would be beneficial to explore the performance of MOCODA in more diverse and complex realworld settings.  The paper could further discuss the computational efficiency and potential scalability challenges of MOCODA in handling highdimensional state spaces or largescale problems.  The discussion on the potential biases or fairness implications of asserting independence relationships in locally factored representations should be expanded to address ethical considerations in RL applications. Overall the paper presents a promising framework MOCODA for addressing generalization challenges in RL tasks with limited training data. By addressing the identified weaknesses and limitations the impact and applicability of MOCODA can be further enhanced.", "o8nYuR8ekFm": "Peer Review 1) Summary: The paper presents a novel method for compressing deep learning models to obtain stateoftheart generalization bounds focusing on understanding the principles underlying deep learning generalization. The compression approach is based on quantizing neural network parameters in a linear subspace leading to significant improvements in generalization properties. The study delves into topics like model size equivariance implicit biases of optimization and the role of compression in generalization. 2) Strengths and weaknesses: Strengths:  The paper addresses an important issue in deep learning by providing a novel compression approach for improved generalization bounds.  The study showcases significant advancements in understanding the generalization properties of neural networks particularly regarding model compression.  The incorporation of different datasets and tasks including transfer learning enhances the applicability and relevance of the findings.  Develops a new method for adaptive training of compressed neural networks to match the problem difficulty showcasing versatility and practicality.  Cleary structured and detailed explanation of the methods and results making it accessible to readers. Weaknesses:  The study focuses heavily on the technical aspects of the compression approach which might make it challenging for nonexperts to grasp the implications and significance of the findings.  While the generalization bounds are improved significantly the paper falls slightly short in clearly linking these bounds to realworld application scenarios. 3) Questions:  How does the novel compression approach compare to existing methods in terms of computational efficiency and feasibility for realworld applications  Can the findings be extended to different types of neural network architectures beyond the ones considered in the study  Could there be potential tradeoffs between extreme model compression and other performance metrics like inference speed or robustness against adversarial attacks 4) Limitations:  The study focuses heavily on theoretical developments and generalization bounds but application scenarios and practical implications could be further elaborated.  The explanations around the compression approach and its relation to generalization while detailed might require further simplification for broader understanding.  The direct link between compression of models and deep learning phenomena can be emphasized more clearly for enhanced understanding and practical insights. Overall the paper presents a significant contribution to the field of deep learning generalization through the development of a compression approach but could benefit from further clarification on realworld implications and practical applicability of the methods.", "wuunqp9KVw": " Peer Review 1) Summary The paper addresses the issue of pronounced constraints and lack of interpretability in pluralistic image completion methods. It proposes an endtoend probabilistic approach that utilizes a unified probabilistic graph model and a Gaussian Mixture Model (GMM) to achieve visually realistic and diverse image completion results. The approach is evaluated through comprehensive experiments on benchmark datasets demonstrating superior performance compared to stateoftheart methods. 2) Strengths and weaknesses Strengths:  The paper introduces a novel approach for pluralistic image completion addressing the interpretability and constraint issues in prior methods.  The use of a unified probabilistic graph model and GMM provides a structured and taskrelated way to enforce constraints and control result diversity.  The proposed method achieves superior performance in terms of both visual quality and result diversity compared to existing methods as evidenced by qualitative and quantitative comparisons. Weaknesses:  The paper could provide more indepth discussion on the limitations and potential drawbacks of the proposed method.  While the paper highlights the benefits of the proposed approach more comparative analysis with a wider range of baselines could further demonstrate the methods effectiveness and generalizability. 3) Questions  Could the authors elaborate on the practical implications of the proposed method in realworld applications such as photo restoration object removal and error concealment  How does the proposed endtoend probabilistic approach handle complex scenarios where pluralistic image completion is required for intricate images or patterns  Could the authors discuss any specific challenges or limitations encountered during the implementation or experimental evaluation of the method 4) Limitations  The evaluation on a limited set of benchmark datasets may restrict the generalizability of the proposed method to diverse realworld scenarios. Including a wider range of datasets with varying complexities could enhance the robustness of the approach.  The paper lacks a detailed discussion on potential scalability issues or computational efficiency concerns when applying the proposed method to largescale or highresolution image completion tasks.  While the paper emphasizes the interpretability and effectiveness of the constraints enforced in the method further exploration on the tradeoffs between interpretability and performance optimization could provide additional insights. Overall the paper presents a novel and promising approach to pluralistic image completion offering valuable contributions to the field. Further elaboration on the practical implications scalability considerations and addressing potential limitations would enhance the completeness and impact of the research findings.", "u6OfmaGIya1": " Peer Review of \"SECOND THOUGHTS: A New Learning Paradigm for Language Models to Realign with Human Values\"  Summary: The paper introduces SECOND THOUGHTS a novel learning paradigm that enhances language models (LMs) ability to align with human values. The method utilizes chainofedits modeling LM finetuning and reinforcement learning to achieve superior performance in three value alignment benchmark datasets. SECOND THOUGHTS also demonstrates strong humanvalue transfer learning in fewshot scenarios providing better interpretability and ease for interactive error correction. Extensive human evaluations confirm the effectiveness of the proposed approach.  Strengths: 1. Innovative Approach: SECOND THOUGHTS offers a fresh perspective on value alignment in language models focusing on text edits inspired by utilitarian ethics which sets it apart from existing methods. 2. Comprehensive Evaluation: The paper includes thorough human evaluations and comparisons with existing methods providing empirical evidence of SECOND THOUGHTS effectiveness. 3. Transfer Learning Capability: The study demonstrates the transferability of learned alignment across different human values tasks showcasing the generalization ability of SECOND THOUGHTS.  Weaknesses: 1. Model Limitations: The paper acknowledges limitations such as the need for retraining as human values evolve and the impact of the LMs max sequence length on the chainofedits. 2. Human Evaluation Bias: The potential bias in human evaluation due to demographic factors like gender education and ideology could affect the generalizability of the findings. 3. Missing Perspective on Ethics: While the paper briefly touches on ethics and broader impact a more detailed discussion on potential ethical implications and considerations would strengthen the study.  Questions: 1. Training Efficiency: How does the training time and computational resources required for SECOND THOUGHTS compare to existing methods especially in scaling to larger models 2. Scalability: Can SECOND THOUGHTS be easily scaled to larger datasets or different languages and how might it perform in multilingual settings 3. Robustness: How does SECOND THOUGHTS perform in handling adversarial attacks or scenarios where the text is deliberately obscure in terms of human values alignment  Limitations: 1. Model Interpretation: While SECOND THOUGHTS offers interpretability through editing steps a more detailed analysis of the models decisionmaking process and interpretability on a larger scale would add value. 2. Generalization to Realworld Scenarios: The study focuses on benchmark datasets and its applicability and performance in realworld settings with dynamic contexts might need further exploration. 3. Human Factor Impact: Although the paper briefly touches on the impact of human factors on value judgments a more indepth analysis of these factors and potential biases would be insightful for the reader. Overall the study presents a promising new approach to enhancing LM alignment with human values. Further research into scalability robustness and realworld applicability will be valuable to solidify the impact of SECOND THOUGHTS in the field of value alignment in language models.", "yoBaCtx_a3": "Peer Review: 1) Summary: The paper presents an algorithm for learning switched linear dynamical systems from noisy observations of the systems full state or output. This algorithm improves complexity to be linear in the number of data points while remaining exponential in the number of state variables and desired modes. The approach is demonstrated on microbenchmarks and realworld applications showing promising results compared to offtheshelf solvers. 2) Strengths:  The paper addresses a significant problem in learning switched linear systems with practical applications.  The novel algorithm improves time complexity to be linear in the number of data points.  The experimental evaluation on microbenchmarks and realworld datasets demonstrates the effectiveness of the proposed approach.  The theoretical underpinnings algorithm design and complexity analysis are welldeveloped and clearly presented.  Comprehensive related work analysis provides context and comparison with existing approaches. 3) Weaknesses:  While the paper offers a novel algorithmic solution more comparative analysis against a broader range of existing methodologies could strengthen the evaluation.  Evaluation metrics could be expanded to include additional performance measures beyond runtime efficiency such as model quality and robustness.  The papers current focus on switched linear systems could be broadened to consider more complex or varied models for a wider range of applications. 4) Questions:  How does the proposed algorithm handle cases of high noise in the data and the resulting impact on model accuracy  Is there potential scalability issues when applying the algorithm to larger and more complex systems with higher dimensions and modes  How does the algorithms performance degrade when faced with incomplete data or missing observations  Can the algorithm be easily adapted to accommodate realtime learning or adaptive modeling requirements 5) Limitations:  Theoretical complexity improvements may not directly translate to practical gains in all scenarios.  The focus on a specific problem domain limits the generalizability of the algorithm across diverse applications.  The restricted evaluation scope and dataset diversity may not fully capture the algorithms performance in realworld scenarios. Overall the paper addresses an important problem and presents a promising algorithmic approach with theoretical foundations. The strengths lie in innovative algorithm design complexity analysis and initial empirical validation. Addressing the weaknesses through further comparative analysis diverse evaluation metrics and broader applicability considerations could enhance the papers impact and relevance. Additional Comments:  The papers structure is wellorganized with clear definitions methodologies and experimental results.  The indepth exploration of the algorithms theoretical framework and practical implementation is commendable.  The paper provides valuable insights into the challenges and solutions in learning switched linear dynamical systems contributing significantly to the research field.", "nSe94hrIWhb": " Peer Review of the Paper  1. Summary: The paper introduces two novel algorithms CoralTDA and PrunIT to address the computational challenges in applying Topological Data Analysis (TDA) to large complex networks. CoralTDA focuses on reducing the graph size by considering the (k  1)core of a graph for kth persistence diagrams while PrunIT removes dominated vertices without affecting the persistence diagrams. The experiments showcase significant reductions in computational costs making TDA more accessible for large network analysis.  2. Strengths:  Innovative Algorithms: The CoralTDA and PrunIT algorithms offer effective solutions to reduce computational costs in computing persistence diagrams for large networks.  Experimental Validation: The experiments demonstrate substantial reductions in graph size computational time and edge counts validating the effectiveness of the proposed algorithms.  Bridge between Graph Theory and TDA: The paper successfully connects graph theory with TDA offering new possibilities for applying TDA to large networks.  Accessible Implementation: Providing the implementation on GitHub allows for reproducibility and practical application of the algorithms.  3. Weaknesses:  Algorithm Complexity: While the paper claims reduction in computational costs a detailed analysis of the time complexity of the algorithms could provide more insight into their efficiency.  Theoretical Justification: While the algorithms are empirically validated more theoretical justifications and theorems could enhance the understanding of the proposed approaches.  Comparison with Existing Methods: Comparison with other stateoftheart methods for reducing computational costs in TDA could provide a broader perspective on the novelty of the proposed algorithms.  4. Questions:  How do the CoralTDA and PrunIT algorithms compare to existing techniques for reducing computational costs in TDA on graphs  Could the paper provide more detailed insights into the theoretical foundations behind the algorithms to enhance understanding and trust in their efficacy  Are there any specific realworld applications or case studies where the proposed algorithms have been successfully applied beyond the experimental validation mentioned in the paper  5. Limitations:  Generalizability: The effectiveness of the algorithms across a wide range of datasets and network types could be further explored to understand their generalizability.  Scalability: The scalability of the algorithms to even larger networks beyond the datasets tested should be discussed to understand their limitations in extreme cases.  Assumption Validity: The assumptions made such as the dominated vertices and core structures in networks need further validation to ensure applicability across diverse network models.  Additional Comments: The paper presents innovative solutions to tackle the computational challenges of applying TDA to large networks. By addressing these limitations the proposed algorithms have the potential to facilitate broader applications of TDA in realworld studies. Further elaboration on theoretical underpinnings comparative analysis and realworld use cases could strengthen the papers impact and relevance in the field of graphbased TDA.", "pV7f1Rq71I5": " Peer Review:  1) Summary The paper proposed a new constant memory scheme for estimating the entropy of a distribution using a lowmemory streaming algorithm. By reducing the sample complexity dependency on the error margin the authors achieved an improvement over the previous memoryefficient bound. The algorithm employs a bucketing scheme and correction techniques to estimate the entropy with \u00b1\\xe2\\x9c\\x8f error using O(k log4(1\\xe2\\x9c\\x8f)\\xe2\\x9c\\x8f2) i.i.d. samples. The approach shows promise in reducing the sample complexity while maintaining a constant memory requirement.  2) Strengths and Weaknesses  Strengths:  The paper addresses a relevant and challenging problem in lowmemory streaming algorithms for statistical inference.  The proposed algorithm significantly improves the sample complexity bound compared to previous work by reducing the dependency on the error margin.  The use of bias correction techniques and bucketing scheme shows a novel approach to estimate entropy in a streaming setting.  The theoretical analysis of the algorithm provides insights into the sample complexity and error bounds.  Weaknesses:  The paper lacks experimental validation or empirical evaluation of the proposed algorithms performance.  The complexity of the algorithm might make it challenging to implement and deploy in practical scenarios.  The theoretical nature of the paper may limit its accessibility to a broader audience outside the specific research domain.  3) Questions  How does the proposed algorithm compare in terms of computational efficiency with existing approaches for entropy estimation in streaming algorithms  Is there a practical application scenario where the proposed algorithm could be utilized and if so what are the practical challenges in implementing it  Could the algorithm be extended or adapted for estimating other statistical properties of distributions apart from entropy in streaming settings  4) Limitations  The lack of empirical evaluation limits the practical utility assessment of the proposed algorithm.  The algorithms complexity and memory requirements might hinder its realworld applicability.  The theoretical contributions might not directly translate into practical solutions without extensive optimization and validation. In conclusion the paper presents a novel algorithm for entropy estimation in streaming algorithms with significant improvements in sample complexity bounds. While the theoretical contributions are valuable practical considerations and empirical validation are necessary to assess the algorithms realworld effectiveness. Further clarity on the algorithms computational complexity and potential applications would enhance the papers impact and relevance in the research community.", "swIARHfCaUB": " Paper Review  1) Summary: The paper proposes a delayed variant of the online FrankWolfe (OFW) method for online convex optimization with arbitrary delays in receiving gradients. The delayed OFW achieves O(T 34  dT 14) regret bounds for convex losses and O(T 23  d log T) regret bounds for strongly convex losses. The theoretical results are supported by simulation experiments.  2) Strengths and weaknesses: Strengths:  The paper addresses the practical issue of delayed gradient feedback in online convex optimization using a delayed variant of OFW.  The proposed delayed OFW achieves competitive regret bounds for convex and strongly convex losses outperforming existing methods in the delayed setting.  The theoretical results are supported by simulation experiments providing empirical validation.  The paper is wellstructured providing clear insights into the proposed algorithm and its theoretical guarantees. Weaknesses:  The paper could benefit from more detailed explanations and illustrations of the proofs for the theoretical guarantees to make them more accessible to a wider audience.  The empirical validation could be further expanded to include a broader range of scenarios or datasets to ensure the algorithms robustness.  3) Questions:  How does the delayed OFW algorithm handle the variability in delay values and their impact on regret bounds  Are there any particular applications or domains where the delayed OFW algorithm could be especially beneficial  What are the implications of the proposed delayed variant on computational complexity compared to traditional OFW  4) Limitations:  The paper focuses primarily on theoretical analysis and experimental validation but further discussions on practical implementation challenges or realworld application scenarios could enhance the impact of the research.  The paper assumes certain conditions and parameter settings for theoretical guarantees further exploration of sensitivity to such assumptions would be beneficial. Overall the paper presents a significant contribution to the field of online convex optimization by addressing the impact of delayed feedback on regret bounds using a tailored variant of the OFW algorithm. The thorough theoretical analysis and empirical evaluations provide valuable insights into the effectiveness of the proposed approach. Further exploration of practical implications and extensions to different algorithms or settings could enhance the impact and applicability of the research.", "v9Wjc2OWjz": " Summary: The paper addresses the problem of estimating a rank1 signal corrupted by structured rotationally invariant noise when the noise statistics are unknown. The study evaluates the performance of inference algorithms considering the mismatch between assumed Gaussian noise and unknown noise statistics. Rigorous analyses of the Bayes estimator and the Approximate Message Passing (AMP) algorithm are provided revealing surprising results from numerical simulations.  Strengths: 1. Novelty: The paper addresses an important problem in machine learning with practical implications. 2. Rigorous Analysis: The rigorous analysis of the Bayes estimator and AMP algorithm enhances the understanding of the mismatched setting. 3. Theoretical Contributions: The closedform expressions for meansquare error and overlap provide valuable insights into the performance of inference methods. 4. Experimental Validation: The numerical simulations provide a rich and surprising phenomenology highlighting unexpected behaviors in inference algorithms. 5. State Evolution Results: The state evolution analysis of the Gaussian AMP algorithm is a strong aspect of the paper.  Weaknesses: 1. Clarity: The paper is quite technical and some parts could be challenging for nonexperts to follow without sufficient background knowledge. 2. Complexity: The analysis and results presented in the paper may be too complex for a broad audience.  Questions: 1. Model Comparisons: How do the performance of the Bayes estimator and AMP algorithm compare with other stateoftheart methods for this specific problem 2. Impact of Mismatch: How does the performance degradation due to mismatch impact realworld applications in machine learning and data analysis 3. Generalization: Can the findings in this paper be generalized to other types of structured noise beyond rotationally invariant matrices  Limitations: 1. RealWorld Applicability: The study focuses on theoretical analysis and simulation realworld applicability in various settings may require further investigation. 2. Sensitivity: The performance conclusions drawn from simulated data may be sensitive to specific choices of parameters and conditions. 3. Scope: The study primarily focuses on the specific scenario outlined extending the analysis to variations of the problem could be beneficial. In conclusion the paper provides valuable insights into the impact of unknown noise statistics on inference algorithms offering novel theoretical contributions and surprising results from numerical simulations. Enhancing the clarity and relevance to practical applications could make the findings even more impactful.", "pnSyqRXx73": " Peer Review  1) Summary: The paper introduces the concept of efficiency ordering for stochastic gradient descent (SGD) algorithms driven by different stochastic input sequences. It explores the relationship between convergence properties and efficiency of input sequences based on asymptotic analysis and covariance matrices. The study demonstrates the utility of this efficiency ordering in comparing SGD variants and presents empirical validation through numerical simulations. The findings suggest that processes with smaller asymptotic covariance matrices lead to improved performance in SGD algorithms.  2) Strengths and Weaknesses: Strengths:  The paper addresses an important concept of efficiency ordering in the context of SGD algorithms providing valuable insights for optimizing algorithm performance.  It incorporates theoretical analysis supported by rigorous mathematical formulations enhancing the credibility of the findings.  The empirical validation through numerical experiments on various objective functions and input sequences strengthens the practical applicability of the theoretical results.  The paper is wellstructured with clear sections definitions and results making it easy to follow for readers interested in stochastic optimization algorithms. Weaknesses:  While the theoretical analysis is thorough the paper could benefit from more detailed explanations in certain sections especially for readers less familiar with the underlying mathematical concepts.  The limitations of the study particularly regarding the assumptions made and the generalizability of the efficiency ordering concept to different SGD variants could be further elaborated.  The limitations and implications of the findings could be explored in more depth to provide a comprehensive view of the applicability of the efficiency ordering in realworld scenarios.  3) Questions:  How does the concept of efficiency ordering proposed in the paper compare to existing optimization metrics used in stochastic optimization algorithms  Can the efficiency ordering concept be extended to more complex optimization problems beyond SGD variants and if so what challenges may arise in such extensions  What are the practical implications of the findings on efficiency ordering for realworld applications of stochastic optimization algorithms in machine learning and other domains  4) Limitations:  The study primarily focuses on efficiency ordering in the context of SGD algorithms with specific assumptions and conditions. Generalizing the concept to different optimization algorithms or scenarios may require further investigation.  The empirical validation is based on numerical experiments with limited datasets and objective functions. Consideration of a wider range of scenarios could provide more robust validation of the efficiency ordering concept.  The scalability and computational complexity implications of incorporating efficiency ordering in largescale optimization problems are not discussed which could be crucial for practical implementation. Overall the paper presents a novel concept of efficiency ordering for SGD algorithms supported by theoretical analysis and empirical validation. Further exploration of the implications limitations and practical applications of the efficiency ordering concept could enhance the impact of the study in the field of stochastic optimization.", "yoLGaLPEPo_": " Peer Review: 1) Summary: The paper proposes a hyperbolic feature augmentation method to combat overfitting in the hyperbolic space due to limited data availability. The method uses a wrapped normal distribution to model augmented features and employs a neural ordinary differential equation module for distribution estimation. An upper bound of the augmentation loss is derived for efficient training with infinite augmentations. Experiments on fewshot learning and continual learning tasks show significant performance improvements with the proposed method. 2) Strengths:  The paper addresses an important problem of overfitting in hyperbolic spaces with limited data offering a novel solution through hyperbolic feature augmentation.  The proposed method incorporates an interesting use of a wrapped normal distribution and neural ordinary differential equation module for distribution estimation providing a unique approach to combat overfitting.  Experiment results demonstrate the effectiveness of the method showcasing significant performance improvements in fewshot learning and continual learning tasks compared to existing algorithms.  The derivations and detailed explanations provide a comprehensive understanding of the proposed method making it accessible to readers interested in hyperbolic algorithms. Weaknesses:  While the method shows improved performance it would be beneficial to provide more comprehensive comparisons with a broader range of existing hyperbolic algorithms and data augmentation techniques across various datasets.  The paper could benefit from a more indepth discussion on the limitations and potential failure modes of the proposed method to ensure the robustness of the approach in practical scenarios.  The methods practical applicability outside of the specific tasks studied (fewshot learning continual learning) should be discussed to demonstrate its versatility and potential in other areas of machine learning.  Questions: 1) How does the proposed hyperbolic feature augmentation method compare to existing data augmentation techniques in nonhyperbolic spaces in terms of performance and computational efficiency 2) Can the method be easily extended to handle more complex hierarchical structures or different types of datasets beyond the ones tested in the experiments 3) Have the authors considered exploring the impact of hyperparameter settings on the performance of the method and how sensitive is the performance to these hyperparameters  Limitations:  The paper focuses primarily on fewshot learning and continual learning tasks limiting the generalizability of the proposed method to other machine learning domains and tasks.  The proposed methods assumption of a uniform hierarchical structure in data might limit its effectiveness in capturing more complex and varied data structures encountered in realworld datasets. Overall the paper presents a novel approach to addressing overfitting in hyperbolic spaces with limited data and demonstrates promising results through experiments. Further exploration and refinement of the methods applicability and robustness to varying conditions could enhance its impact in the field of hyperbolic algorithms.", "zvNMzjOizmn": " Peer Review  Summary: The paper introduces a novel method amortized Langevin dynamics (ALD) for efficient posterior sampling in deep latent variable models. The ALD approach replaces datapointwise MCMC iterations with an encoder and demonstrates its validity in approximating intractable posteriors. The study presents the Langevin autoencoder (LAE) based on ALD and shows its outperformance compared to variational inference and existing MCMCbased methods on various datasets.  Strengths: 1. Novelty and Contribution: The introduction of ALD and LAE provides a fresh perspective on efficient posterior sampling and deep generative modeling. The proposed methods show promise in overcoming limitations of existing MCMC and VI techniques. 2. Theoretical Foundation: The study is wellsupported by theoretical analysis including the validity of ALD as an MCMC algorithm and the conditions under which ALD converges to the true posterior. Theorems provide a solid basis for the proposed algorithms. 3. Experimental Evaluation: The experiments conducted on toy examples and image generation tasks demonstrate the effectiveness of ALD and LAE. Results showcase the superior performance of LAE over baseline methods validating the theoretical claims.  Weaknesses: 1. Complexity: The study involves intricate mathematical and algorithmic details which might make it challenging for readers without a strong background in MCMC variational inference and deep learning to grasp the concepts fully. Simplification of certain technical aspects could enhance accessibility. 2. Limited Generalizability: While the methodology is evaluated on various datasets the generalizability and scalability of ALD and LAE to largescale complex models remain to be thoroughly investigated. Addressing this aspect would strengthen the practical applicability of the proposed methods.  Questions: 1. Scalability: How do the computational requirements of ALD and LAE scale with increasing dataset sizes and model complexities Have you considered the potential bottlenecks in scaling these techniques to realworld applications 2. Robustness: How sensitive are ALD and LAE to initialization parameters and hyperparameters Have you explored strategies to ensure robust convergence and performance across different settings 3. Comparison Metrics: Apart from test likelihood are there other metrics or benchmarks where ALD and LAE exhibit advantages over existing methods Additional comparisons could provide further insights into the strengths of the proposed techniques.  Limitations: 1. Theoretical Constraints: The constraints on the structure of the encoder as discussed in Theorem 1 could pose limitations on the applicability of ALD and LAE especially in scenarios with hierarchical latent variable structures. Strategies to relax or address these constraints would enhance the versatility of the methods. 2. MCMC Iteration Selection: The study mentions the challenge in selecting the number of MCMC iterations. A discussion on strategies to determine optimal iteration counts or sensitivity analyses to evaluate the impact of iteration choices would add depth to the analysis. Overall the paper presents a theoretically grounded and empirically validated approach towards efficient posterior sampling in deep latent variable models. Addressing the identified weaknesses and limitations would further solidify the contributions and applicability of ALD and LAE in the field of probabilistic modeling and generative modeling.", "yCJVkELVT9d": "Peer Review: 1) Summary: The paper addresses the critical issue of evaluating the robustness of Graph Neural Network (GNN) defenses by highlighting the shortcomings of existing methodologies that rely on nonadaptive attacks leading to inflated robustness estimates. Through a comprehensive analysis of popular defenses using adaptive attacks the authors demonstrate a significant drop in robustness estimates compared to nonadaptive attacks. The study provides valuable insights into the challenges and lessons learned in designing strong adaptive attacks for GNNs. 2) Strengths and weaknesses: Strengths:  Thorough analysis: The paper provides a detailed and systematic analysis of GNN defenses using adaptive attacks offering a more realistic evaluation of their robustness.  Clear methodology: The authors outline a transparent methodology for designing adaptive attacks enhancing reproducibility.  Insightful findings: The results reveal substantial discrepancies between nonadaptive and adaptive attack evaluations shedding light on the need for more rigorous robustness assessments in GNN research. Weaknesses:  Limited datasets: The reliance on Cora ML and Citeseer datasets may limit the generalizability of the findings. It would be beneficial to explore the performance of defenses on a broader range of datasets.  Lack of discussion on defense tuning: While the paper addresses tuning hyperparameters for attacking defenses it would be useful to include a discussion on how defense hyperparameters may impact robustness evaluations. 3) Questions:  How do the findings of this study impact the broader field of GNN research and the development of more robust defenses  Can the lessons learned from designing adaptive attacks for GNNs be applied to other domains or network structures  What are the implications of the observed datasetdependency of defense effectiveness for realworld applications of GNNs 4) Limitations:  Dataset limitations: The reliance on a limited number of datasets may restrict the generalizability of the findings to a broader range of GNN applications.  Evaluation metrics: While the study introduces valuable metrics like (R)AUC further discussion on the limitations of these metrics as well as potential alternative evaluation approaches could enhance the paper. In conclusion the paper presents a valuable contribution to the field by highlighting the necessity of adaptive attacks for robustness evaluations in GNN research. Addressing the identified limitations and offering insights into the broader implications of the findings would strengthen the impact of the study. [Total word count: 964]", "kcQiIrvA_nz": " Peer Review  1) Summary (Avg word length: 100) The paper explores untargeted backdoor watermarking scheme for dataset ownership verification in deep neural networks (DNNs). It addresses the limitations of existing methods in protecting copyrights of opensourced datasets and proposes the untargeted backdoor watermark under poisonedlabel and cleanlabel settings. The effectiveness and stealthiness of the proposed method are verified through experiments on benchmark datasets. The paper introduces a hypothesistestbased method for dataset ownership verification and discusses its applicability in different scenarios. Furthermore the resistance of the untargeted backdoor watermark to existing backdoor defenses is also investigated.  2) Strengths and weaknesses Strengths:  The paper addresses an important issue of protecting dataset copyrights in the context of opensourced datasets.  The proposed untargeted backdoor watermarking scheme provides a novel approach to dataset ownership verification.  The experiments conducted on benchmark datasets validate the effectiveness and resistance of the proposed method to existing backdoor defenses.  The detailed theoretical formulation and methodology provide a comprehensive understanding of the untargeted backdoor watermark. Weaknesses:  The paper could benefit from a more indepth discussion on potential realworld applications and implications of the proposed method.  The paper lacks a detailed discussion on the computational efficiency and scalability of the proposed approach.  The limitations of the research and potential future directions could be further elaborated to guide future research efforts.  3) Questions  How does the untargeted backdoor watermarking scheme compare to existing targeted watermarking methods in terms of computational overhead and training complexity  Are there specific use cases or domains where the proposed untargeted backdoor watermarking scheme may be more suitable or effective  Could the authors provide more insight into the interpretability of the dispersibility metrics and their practical implications for dataset ownership verification  How does the proposed method handle scenarios where attackers attempt to bypass the untargeted backdoor watermark through more sophisticated attacks  4) Limitations  The paper primarily focuses on theoretical formulations and experimental validation without extensive realworld deployment scenarios.  The scalability and generalizability of the proposed untargeted backdoor watermarking method to diverse datasets and model architectures could be a limitation.  The lack of a comparative analysis with other stateoftheart watermarking or ownership verification methods limits the context of the proposed research.  Conclusion In conclusion the paper introduces a novel untargeted backdoor watermarking scheme for dataset ownership verification in DNNs. While the research addresses an important aspect of safeguarding dataset copyrights more discussion on practical applications scalability and potential vulnerabilities could enhance the impact and relevance of the proposed method. Further exploration of these aspects could provide valuable insights for future research in this domain.  Overall Rating: The paper presents a novel approach to dataset ownership verification using untargeted backdoor watermarking in DNNs supported by theoretical formulations and experimental validation. However additional insights into the practical implications scalability and comparative analysis could enhance the depth and applicability of the research.", "uloenYmLCAo": " Summary: The paper introduces the BlockRecurrent Transformer a novel architecture combining attention and recurrence to handle long sequences efficiently. The recurrent cell processes blocks of tokens in parallel addressing limitations of traditional transformers. Experiment results show significant improvement in language modeling perplexity over long sequences compared to Transformer XL. The study includes various gate configurations ablations scaling experiments and comparisons against baselines and similar models.  Strengths: 1. Innovative Architecture: The BlockRecurrent Transformer effectively combines attention and recurrence improving perplexity on long sequences. 2. Efficiency: Linear complexity with regard to sequence length and comparable cost to conventional transformers in terms of computation and parameter count. 3. Empirical Results: Outperforms baselines and similar models across different datasets demonstrating the effectiveness of the approach. 4. Character Study: Qualitative and quantitative analyses offer insights into how the recurrent model utilizes longrange dependencies.  Weaknesses: 1. Lack of Downstream Tasks Evaluation: While language modeling perplexity is significantly improved the paper lacks evaluation on downstream tasks to demonstrate realworld applicability. 2. Limited Discussion on Gate Configurations: More indepth analysis on gate configurations and their impact on model performance could provide additional insights. 3. Ethical Considerations: The discussion on potential negative social impacts is cursory a more thorough examination and mitigation strategies would enhance the paper.  Questions: 1. Scalability: How does the performance of the BlockRecurrent Transformer scale with further increases in model size or complexity 2. Generalization: Can the benefits of the recurrent model be extended to tasks beyond language modeling such as natural language understanding or generation 3. Interpretability: Is there any analysis in the paper regarding the interpretability of the models decisions particularly in capturing longrange dependencies 4. Comparison to Transformer XL: How does the BlockRecurrent Transformer compare to Transformer XL in terms of model interpretability and training stability  Limitations: 1. Scope of Applications: The focus on language modeling tasks limits the exploration of the models performance across a wider range of NLP tasks. 2. Complexity of Model: Detailed insights into the effects of different gate configurations and recurrent layer placements could provide a deeper understanding of the models behavior. 3. Potential Societal Impacts: A more comprehensive discussion on the models ethical implications and strategies to mitigate misuse would strengthen the papers societal relevance. Overall the paper presents an intriguing architecture with promising results but further evaluations and analyses would enhance the depth and applicability of the findings."}